id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/broadinstitute/gatk/issues/4369:4489,Integrability,protocol,protocol,4489,"ocketImpl.doConnect(AbstractPlainSocketImpl.java:350); 	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206); 	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188); 	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392); 	at java.net.Socket.connect(Socket.java:589); 	at sun.net.NetworkClient.doConnect(NetworkClient.java:175); 	at sun.net.www.http.HttpClient.openServer(HttpClient.java:463); 	at sun.net.www.http.HttpClient.openServer(HttpClient.java:558); 	at sun.net.www.http.HttpClient.<init>(HttpClient.java:242); 	at sun.net.www.http.HttpClient.New(HttpClient.java:339); 	at sun.net.www.http.HttpClient.New(HttpClient.java:357); 	at sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:1202); 	at sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1138); 	at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1032); 	at sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:966); 	at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:93); 	at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:981); 	at com.google.cloud.hadoop.util.CredentialFactory$ComputeCredentialWithRetry.executeRefreshToken(CredentialFactory.java:158); 	at com.google.api.client.auth.oauth2.Credential.refreshToken(Credential.java:489); 	at com.google.cloud.hadoop.util.CredentialFactory.getCredentialFromMetadataServiceAccount(CredentialFactory.java:206); 	... 31 more; ```; Followed by repetitions of the following stacktrace:; ```; Feb 07, 2018 12:41:59 PM com.google.api.client.http.HttpRequest execute; WARNING: exception thrown while executing request; java.net.SocketTimeoutException: connect timed out; 	at java.net.PlainSocketImpl.socketConnect(Native Method); 	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350); 	at java.net.AbstractPlainSocketImpl.connectToAdd",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4369
https://github.com/broadinstitute/gatk/issues/4369:6108,Integrability,protocol,protocol,6108,"acktrace:; ```; Feb 07, 2018 12:41:59 PM com.google.api.client.http.HttpRequest execute; WARNING: exception thrown while executing request; java.net.SocketTimeoutException: connect timed out; 	at java.net.PlainSocketImpl.socketConnect(Native Method); 	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350); 	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206); 	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188); 	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392); 	at java.net.Socket.connect(Socket.java:589); 	at sun.net.NetworkClient.doConnect(NetworkClient.java:175); 	at sun.net.www.http.HttpClient.openServer(HttpClient.java:463); 	at sun.net.www.http.HttpClient.openServer(HttpClient.java:558); 	at sun.net.www.http.HttpClient.<init>(HttpClient.java:242); 	at sun.net.www.http.HttpClient.New(HttpClient.java:339); 	at sun.net.www.http.HttpClient.New(HttpClient.java:357); 	at sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:1202); 	at sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1138); 	at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1032); 	at sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:966); 	at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:93); 	at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:981); 	at com.google.cloud.hadoop.util.CredentialFactory$ComputeCredentialWithRetry.executeRefreshToken(CredentialFactory.java:158); 	at com.google.api.client.auth.oauth2.Credential.refreshToken(Credential.java:489); 	at com.google.cloud.hadoop.util.CredentialFactory.getCredentialFromMetadataServiceAccount(CredentialFactory.java:206); 	at com.google.cloud.hadoop.util.CredentialConfiguration.getCredential(CredentialConfiguration.java:70); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4369
https://github.com/broadinstitute/gatk/issues/4369:6203,Integrability,protocol,protocol,6203,G: exception thrown while executing request; java.net.SocketTimeoutException: connect timed out; 	at java.net.PlainSocketImpl.socketConnect(Native Method); 	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350); 	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206); 	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188); 	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392); 	at java.net.Socket.connect(Socket.java:589); 	at sun.net.NetworkClient.doConnect(NetworkClient.java:175); 	at sun.net.www.http.HttpClient.openServer(HttpClient.java:463); 	at sun.net.www.http.HttpClient.openServer(HttpClient.java:558); 	at sun.net.www.http.HttpClient.<init>(HttpClient.java:242); 	at sun.net.www.http.HttpClient.New(HttpClient.java:339); 	at sun.net.www.http.HttpClient.New(HttpClient.java:357); 	at sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:1202); 	at sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1138); 	at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1032); 	at sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:966); 	at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:93); 	at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:981); 	at com.google.cloud.hadoop.util.CredentialFactory$ComputeCredentialWithRetry.executeRefreshToken(CredentialFactory.java:158); 	at com.google.api.client.auth.oauth2.Credential.refreshToken(Credential.java:489); 	at com.google.cloud.hadoop.util.CredentialFactory.getCredentialFromMetadataServiceAccount(CredentialFactory.java:206); 	at com.google.cloud.hadoop.util.CredentialConfiguration.getCredential(CredentialConfiguration.java:70); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.configure(GoogleHadoopFileSystemBase.java:1825); 	at com.google.cloud.hadoop.fs.gcs.GoogleHado,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4369
https://github.com/broadinstitute/gatk/issues/4369:6295,Integrability,protocol,protocol,6295,out; 	at java.net.PlainSocketImpl.socketConnect(Native Method); 	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350); 	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206); 	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188); 	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392); 	at java.net.Socket.connect(Socket.java:589); 	at sun.net.NetworkClient.doConnect(NetworkClient.java:175); 	at sun.net.www.http.HttpClient.openServer(HttpClient.java:463); 	at sun.net.www.http.HttpClient.openServer(HttpClient.java:558); 	at sun.net.www.http.HttpClient.<init>(HttpClient.java:242); 	at sun.net.www.http.HttpClient.New(HttpClient.java:339); 	at sun.net.www.http.HttpClient.New(HttpClient.java:357); 	at sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:1202); 	at sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1138); 	at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1032); 	at sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:966); 	at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:93); 	at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:981); 	at com.google.cloud.hadoop.util.CredentialFactory$ComputeCredentialWithRetry.executeRefreshToken(CredentialFactory.java:158); 	at com.google.api.client.auth.oauth2.Credential.refreshToken(Credential.java:489); 	at com.google.cloud.hadoop.util.CredentialFactory.getCredentialFromMetadataServiceAccount(CredentialFactory.java:206); 	at com.google.cloud.hadoop.util.CredentialConfiguration.getCredential(CredentialConfiguration.java:70); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.configure(GoogleHadoopFileSystemBase.java:1825); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:1012); 	at com.google.cloud.hado,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4369
https://github.com/broadinstitute/gatk/issues/4369:6386,Integrability,protocol,protocol,6386,ocketImpl.doConnect(AbstractPlainSocketImpl.java:350); 	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206); 	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188); 	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392); 	at java.net.Socket.connect(Socket.java:589); 	at sun.net.NetworkClient.doConnect(NetworkClient.java:175); 	at sun.net.www.http.HttpClient.openServer(HttpClient.java:463); 	at sun.net.www.http.HttpClient.openServer(HttpClient.java:558); 	at sun.net.www.http.HttpClient.<init>(HttpClient.java:242); 	at sun.net.www.http.HttpClient.New(HttpClient.java:339); 	at sun.net.www.http.HttpClient.New(HttpClient.java:357); 	at sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:1202); 	at sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1138); 	at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1032); 	at sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:966); 	at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:93); 	at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:981); 	at com.google.cloud.hadoop.util.CredentialFactory$ComputeCredentialWithRetry.executeRefreshToken(CredentialFactory.java:158); 	at com.google.api.client.auth.oauth2.Credential.refreshToken(Credential.java:489); 	at com.google.cloud.hadoop.util.CredentialFactory.getCredentialFromMetadataServiceAccount(CredentialFactory.java:206); 	at com.google.cloud.hadoop.util.CredentialConfiguration.getCredential(CredentialConfiguration.java:70); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.configure(GoogleHadoopFileSystemBase.java:1825); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:1012); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:975); 	at o,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4369
https://github.com/broadinstitute/gatk/issues/4369:684,Modifiability,config,configure,684,"I have noticed that when running spark tools (e.g. CountReadsSpark or MarkDuplicatesSpark) that running with an input in the form ""CountReadsSpark -I gs://my-bucket-dir/my-file.bam."" The tool crashes with the following unhelpful stacktraces:. ```; java.io.IOException: Error getting access token from metadata server at: http://metadata/computeMetadata/v1/instance/service-accounts/default/token; 	at com.google.cloud.hadoop.util.CredentialFactory.getCredentialFromMetadataServiceAccount(CredentialFactory.java:208); 	at com.google.cloud.hadoop.util.CredentialConfiguration.getCredential(CredentialConfiguration.java:70); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.configure(GoogleHadoopFileSystemBase.java:1825); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:1012); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:975); 	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2653); 	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:92); 	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2687); 	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2669); 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:371); 	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:295); 	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths(FileInputFormat.java:500); 	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths(FileInputFormat.java:469); 	at org.apache.spark.SparkContext$$anonfun$newAPIHadoopFile$2.apply(SparkContext.scala:1084); 	at org.apache.spark.SparkContext$$anonfun$newAPIHadoopFile$2.apply(SparkContext.scala:1072); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.SparkContext.withScope(SparkContext.scala:679); 	at org.ap",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4369
https://github.com/broadinstitute/gatk/issues/4369:7114,Modifiability,config,configure,7114,nection.getNewHttpClient(HttpURLConnection.java:1202); 	at sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1138); 	at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1032); 	at sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:966); 	at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:93); 	at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:981); 	at com.google.cloud.hadoop.util.CredentialFactory$ComputeCredentialWithRetry.executeRefreshToken(CredentialFactory.java:158); 	at com.google.api.client.auth.oauth2.Credential.refreshToken(Credential.java:489); 	at com.google.cloud.hadoop.util.CredentialFactory.getCredentialFromMetadataServiceAccount(CredentialFactory.java:206); 	at com.google.cloud.hadoop.util.CredentialConfiguration.getCredential(CredentialConfiguration.java:70); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.configure(GoogleHadoopFileSystemBase.java:1825); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:1012); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:975); 	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2653); 	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:92); 	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2687); 	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2669); 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:371); 	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:295); 	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths(FileInputFormat.java:500); 	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths(FileInputFormat.java:469); 	at org.apache.spark.SparkContext$$anonfun$newAPIHadoopFile$2.apply(SparkContext.scala:1084); 	at org.apache.spark.SparkContext$$anonfun$,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4369
https://github.com/broadinstitute/gatk/issues/4369:1136,Performance,Cache,Cache,1136,"untReadsSpark -I gs://my-bucket-dir/my-file.bam."" The tool crashes with the following unhelpful stacktraces:. ```; java.io.IOException: Error getting access token from metadata server at: http://metadata/computeMetadata/v1/instance/service-accounts/default/token; 	at com.google.cloud.hadoop.util.CredentialFactory.getCredentialFromMetadataServiceAccount(CredentialFactory.java:208); 	at com.google.cloud.hadoop.util.CredentialConfiguration.getCredential(CredentialConfiguration.java:70); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.configure(GoogleHadoopFileSystemBase.java:1825); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:1012); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:975); 	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2653); 	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:92); 	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2687); 	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2669); 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:371); 	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:295); 	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths(FileInputFormat.java:500); 	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths(FileInputFormat.java:469); 	at org.apache.spark.SparkContext$$anonfun$newAPIHadoopFile$2.apply(SparkContext.scala:1084); 	at org.apache.spark.SparkContext$$anonfun$newAPIHadoopFile$2.apply(SparkContext.scala:1072); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.SparkContext.withScope(SparkContext.scala:679); 	at org.apache.spark.SparkContext.newAPIHadoopFile(SparkContext.scala:1072); 	at org.apache.spark.api.java.JavaSparkContext.newAPIHadoopFile(Ja",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4369
https://github.com/broadinstitute/gatk/issues/4369:1213,Performance,Cache,Cache,1213,ollowing unhelpful stacktraces:. ```; java.io.IOException: Error getting access token from metadata server at: http://metadata/computeMetadata/v1/instance/service-accounts/default/token; 	at com.google.cloud.hadoop.util.CredentialFactory.getCredentialFromMetadataServiceAccount(CredentialFactory.java:208); 	at com.google.cloud.hadoop.util.CredentialConfiguration.getCredential(CredentialConfiguration.java:70); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.configure(GoogleHadoopFileSystemBase.java:1825); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:1012); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:975); 	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2653); 	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:92); 	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2687); 	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2669); 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:371); 	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:295); 	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths(FileInputFormat.java:500); 	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths(FileInputFormat.java:469); 	at org.apache.spark.SparkContext$$anonfun$newAPIHadoopFile$2.apply(SparkContext.scala:1084); 	at org.apache.spark.SparkContext$$anonfun$newAPIHadoopFile$2.apply(SparkContext.scala:1072); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.SparkContext.withScope(SparkContext.scala:679); 	at org.apache.spark.SparkContext.newAPIHadoopFile(SparkContext.scala:1072); 	at org.apache.spark.api.java.JavaSparkContext.newAPIHadoopFile(JavaSparkContext.scala:474); 	at org.broadinstitute.hellbender.engine.spark.dat,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4369
https://github.com/broadinstitute/gatk/issues/4369:7566,Performance,Cache,Cache,7566,ient.http.HttpRequest.execute(HttpRequest.java:981); 	at com.google.cloud.hadoop.util.CredentialFactory$ComputeCredentialWithRetry.executeRefreshToken(CredentialFactory.java:158); 	at com.google.api.client.auth.oauth2.Credential.refreshToken(Credential.java:489); 	at com.google.cloud.hadoop.util.CredentialFactory.getCredentialFromMetadataServiceAccount(CredentialFactory.java:206); 	at com.google.cloud.hadoop.util.CredentialConfiguration.getCredential(CredentialConfiguration.java:70); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.configure(GoogleHadoopFileSystemBase.java:1825); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:1012); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:975); 	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2653); 	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:92); 	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2687); 	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2669); 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:371); 	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:295); 	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths(FileInputFormat.java:500); 	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths(FileInputFormat.java:469); 	at org.apache.spark.SparkContext$$anonfun$newAPIHadoopFile$2.apply(SparkContext.scala:1084); 	at org.apache.spark.SparkContext$$anonfun$newAPIHadoopFile$2.apply(SparkContext.scala:1072); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.SparkContext.withScope(SparkContext.scala:679); 	at org.apache.spark.SparkContext.newAPIHadoopFile(SparkContext.scala:1072); 	at org.apache.spark.api.java.JavaSparkContext.newAPIHadoopFile(Ja,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4369
https://github.com/broadinstitute/gatk/issues/4369:7643,Performance,Cache,Cache,7643,oop.util.CredentialFactory$ComputeCredentialWithRetry.executeRefreshToken(CredentialFactory.java:158); 	at com.google.api.client.auth.oauth2.Credential.refreshToken(Credential.java:489); 	at com.google.cloud.hadoop.util.CredentialFactory.getCredentialFromMetadataServiceAccount(CredentialFactory.java:206); 	at com.google.cloud.hadoop.util.CredentialConfiguration.getCredential(CredentialConfiguration.java:70); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.configure(GoogleHadoopFileSystemBase.java:1825); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:1012); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:975); 	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2653); 	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:92); 	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2687); 	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2669); 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:371); 	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:295); 	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths(FileInputFormat.java:500); 	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths(FileInputFormat.java:469); 	at org.apache.spark.SparkContext$$anonfun$newAPIHadoopFile$2.apply(SparkContext.scala:1084); 	at org.apache.spark.SparkContext$$anonfun$newAPIHadoopFile$2.apply(SparkContext.scala:1072); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.SparkContext.withScope(SparkContext.scala:679); 	at org.apache.spark.SparkContext.newAPIHadoopFile(SparkContext.scala:1072); 	at org.apache.spark.api.java.JavaSparkContext.newAPIHadoopFile(JavaSparkContext.scala:474); 	at org.broadinstitute.hellbender.engine.spark.dat,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4369
https://github.com/broadinstitute/gatk/issues/4369:283,Security,access,access,283,"I have noticed that when running spark tools (e.g. CountReadsSpark or MarkDuplicatesSpark) that running with an input in the form ""CountReadsSpark -I gs://my-bucket-dir/my-file.bam."" The tool crashes with the following unhelpful stacktraces:. ```; java.io.IOException: Error getting access token from metadata server at: http://metadata/computeMetadata/v1/instance/service-accounts/default/token; 	at com.google.cloud.hadoop.util.CredentialFactory.getCredentialFromMetadataServiceAccount(CredentialFactory.java:208); 	at com.google.cloud.hadoop.util.CredentialConfiguration.getCredential(CredentialConfiguration.java:70); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.configure(GoogleHadoopFileSystemBase.java:1825); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:1012); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:975); 	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2653); 	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:92); 	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2687); 	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2669); 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:371); 	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:295); 	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths(FileInputFormat.java:500); 	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths(FileInputFormat.java:469); 	at org.apache.spark.SparkContext$$anonfun$newAPIHadoopFile$2.apply(SparkContext.scala:1084); 	at org.apache.spark.SparkContext$$anonfun$newAPIHadoopFile$2.apply(SparkContext.scala:1072); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.SparkContext.withScope(SparkContext.scala:679); 	at org.ap",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4369
https://github.com/broadinstitute/gatk/issues/4369:1068,Security,access,access,1068,"catesSpark) that running with an input in the form ""CountReadsSpark -I gs://my-bucket-dir/my-file.bam."" The tool crashes with the following unhelpful stacktraces:. ```; java.io.IOException: Error getting access token from metadata server at: http://metadata/computeMetadata/v1/instance/service-accounts/default/token; 	at com.google.cloud.hadoop.util.CredentialFactory.getCredentialFromMetadataServiceAccount(CredentialFactory.java:208); 	at com.google.cloud.hadoop.util.CredentialConfiguration.getCredential(CredentialConfiguration.java:70); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.configure(GoogleHadoopFileSystemBase.java:1825); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:1012); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:975); 	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2653); 	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:92); 	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2687); 	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2669); 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:371); 	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:295); 	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths(FileInputFormat.java:500); 	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths(FileInputFormat.java:469); 	at org.apache.spark.SparkContext$$anonfun$newAPIHadoopFile$2.apply(SparkContext.scala:1084); 	at org.apache.spark.SparkContext$$anonfun$newAPIHadoopFile$2.apply(SparkContext.scala:1072); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.SparkContext.withScope(SparkContext.scala:679); 	at org.apache.spark.SparkContext.newAPIHadoopFile(SparkContext.scala:1072); 	at org.apa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4369
https://github.com/broadinstitute/gatk/issues/4369:7498,Security,access,access,7498,execute(NetHttpRequest.java:93); 	at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:981); 	at com.google.cloud.hadoop.util.CredentialFactory$ComputeCredentialWithRetry.executeRefreshToken(CredentialFactory.java:158); 	at com.google.api.client.auth.oauth2.Credential.refreshToken(Credential.java:489); 	at com.google.cloud.hadoop.util.CredentialFactory.getCredentialFromMetadataServiceAccount(CredentialFactory.java:206); 	at com.google.cloud.hadoop.util.CredentialConfiguration.getCredential(CredentialConfiguration.java:70); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.configure(GoogleHadoopFileSystemBase.java:1825); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:1012); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:975); 	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2653); 	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:92); 	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2687); 	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2669); 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:371); 	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:295); 	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths(FileInputFormat.java:500); 	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths(FileInputFormat.java:469); 	at org.apache.spark.SparkContext$$anonfun$newAPIHadoopFile$2.apply(SparkContext.scala:1084); 	at org.apache.spark.SparkContext$$anonfun$newAPIHadoopFile$2.apply(SparkContext.scala:1072); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.SparkContext.withScope(SparkContext.scala:679); 	at org.apache.spark.SparkContext.newAPIHadoopFile(SparkContext.scala:1072); 	at org.apa,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4369
https://github.com/broadinstitute/gatk/issues/4369:9859,Testability,test,tests,9859,oop.mapreduce.lib.input.FileInputFormat.setInputPaths(FileInputFormat.java:469); 	at org.apache.spark.SparkContext$$anonfun$newAPIHadoopFile$2.apply(SparkContext.scala:1084); 	at org.apache.spark.SparkContext$$anonfun$newAPIHadoopFile$2.apply(SparkContext.scala:1072); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.SparkContext.withScope(SparkContext.scala:679); 	at org.apache.spark.SparkContext.newAPIHadoopFile(SparkContext.scala:1072); 	at org.apache.spark.api.java.JavaSparkContext.newAPIHadoopFile(JavaSparkContext.scala:474); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getParallelReads(ReadsSparkSource.java:112); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.getUnfilteredReads(GATKSparkTool.java:254); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.getReads(GATKSparkTool.java:220); 	at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark.runTool(MarkDuplicatesSpark.java:72); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:387); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:136); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:152); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:195); 	at org.broadinstitute.hellbender.Main.main(Main.java:275); ```. Notable is the fact that I do not have a service key setup when executing these tests but rather have logged in using my google account.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4369
https://github.com/broadinstitute/gatk/issues/4369:9881,Testability,log,logged,9881,oop.mapreduce.lib.input.FileInputFormat.setInputPaths(FileInputFormat.java:469); 	at org.apache.spark.SparkContext$$anonfun$newAPIHadoopFile$2.apply(SparkContext.scala:1084); 	at org.apache.spark.SparkContext$$anonfun$newAPIHadoopFile$2.apply(SparkContext.scala:1072); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.SparkContext.withScope(SparkContext.scala:679); 	at org.apache.spark.SparkContext.newAPIHadoopFile(SparkContext.scala:1072); 	at org.apache.spark.api.java.JavaSparkContext.newAPIHadoopFile(JavaSparkContext.scala:474); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getParallelReads(ReadsSparkSource.java:112); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.getUnfilteredReads(GATKSparkTool.java:254); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.getReads(GATKSparkTool.java:220); 	at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark.runTool(MarkDuplicatesSpark.java:72); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:387); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:136); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:152); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:195); 	at org.broadinstitute.hellbender.Main.main(Main.java:275); ```. Notable is the fact that I do not have a service key setup when executing these tests but rather have logged in using my google account.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4369
https://github.com/broadinstitute/gatk/issues/4370:85,Testability,test,test,85,"Hi there,. When I run a very basic command such as:. `gatk CollectMultipleMetrics -I test.bam -O ALIGNSUMM --PROGRAM CollectAlignmentSummaryMetrics`. I expect the --PROGRAM argument will override the default value assigned, which is currently five of the metrics tools. Instead, the PROGRAM option includes the specified metrics tool on top of the default selection. So, the above command line will still output metrics from the five default metrics tools. Please let me know if this is the intended behavior so I can adjust our workflow accordingly. Thanks!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4370
https://github.com/broadinstitute/gatk/issues/4371:212,Safety,predict,predicts,212,"After extensive QC, Ryan Collins with the Talkowski lab has a set of ~20 samples that he believes to have sex chromosome genotypes that are not XX or XY. It would be great to run our tool on them and see what it predicts. Normals could probably come from any of the same projects: G100345, G68758, G81032, G94818, etc.; Case data has already been copied to gs: //broad-dsde-methods/testdata/aneuploidy_samples/",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4371
https://github.com/broadinstitute/gatk/issues/4371:382,Testability,test,testdata,382,"After extensive QC, Ryan Collins with the Talkowski lab has a set of ~20 samples that he believes to have sex chromosome genotypes that are not XX or XY. It would be great to run our tool on them and see what it predicts. Normals could probably come from any of the same projects: G100345, G68758, G81032, G94818, etc.; Case data has already been copied to gs: //broad-dsde-methods/testdata/aneuploidy_samples/",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4371
https://github.com/broadinstitute/gatk/issues/4374:488,Testability,test,testing,488,"In `IntervalListMetadata`, the contig list is sorted lexicographically:. ```self.contig_list = sorted(list(self.contig_set))```. This causes, e.g., the output of DetermineGermlineContigPloidy to be sorted lexicographically. Let's remove this and assume that incoming data from temporary files is always sorted according to the sequence dictionary. We should double check that this didn't affect any previous results in any unexpected way. We should also add some basic checks on the Java testing side. One easy modification would be to use contigs 1, 2, and 10 (along with X and Y) for the PostprocessGermlineCNVCalls test. The expected VCF should be in the correct order.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4374
https://github.com/broadinstitute/gatk/issues/4374:618,Testability,test,test,618,"In `IntervalListMetadata`, the contig list is sorted lexicographically:. ```self.contig_list = sorted(list(self.contig_set))```. This causes, e.g., the output of DetermineGermlineContigPloidy to be sorted lexicographically. Let's remove this and assume that incoming data from temporary files is always sorted according to the sequence dictionary. We should double check that this didn't affect any previous results in any unexpected way. We should also add some basic checks on the Java testing side. One easy modification would be to use contigs 1, 2, and 10 (along with X and Y) for the PostprocessGermlineCNVCalls test. The expected VCF should be in the correct order.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4374
https://github.com/broadinstitute/gatk/issues/4375:62,Performance,perform,perform,62,"Right now we check that the simulated data runs, but we don't perform any further checks on the results.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4375
https://github.com/broadinstitute/gatk/issues/4376:18,Deployability,pipeline,pipeline,18,"Running the reads pipeline on latest master on GCP on an exome shows a major performance regression. At the end of last year this took 40 minutes for the whole job, now using master BQSR is taking 1.4 hours, and HC is taking well over an hour (it hasn't finished yet). #4278 is the likely culprit for the HC slowdown. Not sure about BQSR. cc @lbergelson @droazen",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4376
https://github.com/broadinstitute/gatk/issues/4376:77,Performance,perform,performance,77,"Running the reads pipeline on latest master on GCP on an exome shows a major performance regression. At the end of last year this took 40 minutes for the whole job, now using master BQSR is taking 1.4 hours, and HC is taking well over an hour (it hasn't finished yet). #4278 is the likely culprit for the HC slowdown. Not sure about BQSR. cc @lbergelson @droazen",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4376
https://github.com/broadinstitute/gatk/pull/4379:16,Deployability,release,release,16,The earlier GKL release did not have all updates merged. Hence submitting another request to include the latest GKL.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4379
https://github.com/broadinstitute/gatk/pull/4379:41,Deployability,update,updates,41,The earlier GKL release did not have all updates merged. Hence submitting another request to include the latest GKL.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4379
https://github.com/broadinstitute/gatk/issues/4380:194,Availability,error,errors,194,There needs to be a validation tool for data sources to ensure that they conform to their formats properly. This tool is envisioned to be run just prior to data source release to fix any silent errors.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4380
https://github.com/broadinstitute/gatk/issues/4380:168,Deployability,release,release,168,There needs to be a validation tool for data sources to ensure that they conform to their formats properly. This tool is envisioned to be run just prior to data source release to fix any silent errors.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4380
https://github.com/broadinstitute/gatk/issues/4380:20,Security,validat,validation,20,There needs to be a validation tool for data sources to ensure that they conform to their formats properly. This tool is envisioned to be run just prior to data source release to fix any silent errors.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4380
https://github.com/broadinstitute/gatk/pull/4381:72,Testability,test,tests,72,This is the bug fix for @eitanbanks's pon. @LeeTL1220 can you review if tests pass? If they fail I will fix ASAP.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4381
https://github.com/broadinstitute/gatk/pull/4385:207,Usability,learn,learning,207,"I've tried to organize these changes into three separate commits with one issue per commit.; 1) Running the new ExtractSVEvidenceSpark tool via manage_sv_pipeline.sh; 2) Dumping more useful info for machine learning; 3) Reinstating option to not copy fastq files. Hopefully this is sensible, it's my first time trying to organize a PR this way. This should resolve issue #4332",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4385
https://github.com/broadinstitute/gatk/pull/4386:5,Performance,optimiz,optimization,5,This optimization came out of performance testing associated with https://github.com/broadinstitute/gatk/issues/3706,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4386
https://github.com/broadinstitute/gatk/pull/4386:30,Performance,perform,performance,30,This optimization came out of performance testing associated with https://github.com/broadinstitute/gatk/issues/3706,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4386
https://github.com/broadinstitute/gatk/pull/4386:42,Testability,test,testing,42,This optimization came out of performance testing associated with https://github.com/broadinstitute/gatk/issues/3706,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4386
https://github.com/broadinstitute/gatk/issues/4387:207,Modifiability,plugin,plugin,207,"This worked until we kebabified. The Freemarker template looks for the hardcoded string ""readFilter"", but the doc system populates the Freemarker map using the display name self-reported by the (ReadFilter) plugin, which is in turn derived from the standard argument name for read filters. These matched when the arg was ""readFilter"". But after kebabification, its now ""read-filter"". We should probably change the plugins to use a fixed display.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4387
https://github.com/broadinstitute/gatk/issues/4387:414,Modifiability,plugin,plugins,414,"This worked until we kebabified. The Freemarker template looks for the hardcoded string ""readFilter"", but the doc system populates the Freemarker map using the display name self-reported by the (ReadFilter) plugin, which is in turn derived from the standard argument name for read filters. These matched when the arg was ""readFilter"". But after kebabification, its now ""read-filter"". We should probably change the plugins to use a fixed display.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4387
https://github.com/broadinstitute/gatk/issues/4388:203,Availability,reliab,reliably,203,"Frequently we find our pipeline detecting STR expansions whose size >50, i.e. in the SV domain, but we cannot fully assemble the expanded allele, as judged by PacBio calls.; We need a strategy on how to reliably report ; * what is found and; * how likely it is that we have assembled the full allele, or only part of the expansion (lower bound estimate).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4388
https://github.com/broadinstitute/gatk/issues/4388:23,Deployability,pipeline,pipeline,23,"Frequently we find our pipeline detecting STR expansions whose size >50, i.e. in the SV domain, but we cannot fully assemble the expanded allele, as judged by PacBio calls.; We need a strategy on how to reliably report ; * what is found and; * how likely it is that we have assembled the full allele, or only part of the expansion (lower bound estimate).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4388
https://github.com/broadinstitute/gatk/issues/4388:32,Safety,detect,detecting,32,"Frequently we find our pipeline detecting STR expansions whose size >50, i.e. in the SV domain, but we cannot fully assemble the expanded allele, as judged by PacBio calls.; We need a strategy on how to reliably report ; * what is found and; * how likely it is that we have assembled the full allele, or only part of the expansion (lower bound estimate).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4388
https://github.com/broadinstitute/gatk/issues/4390:99,Availability,error,error,99,"Hello - I was running the exact command with GATK-Mutect2 4.0.0.0 and then switched to 4.0.1.2; an error message was returned:. ```**BETA FEATURE - WORK IN PROGRESS**; USAGE: Mutect2 [arguments]; Call somatic SNVs and indels via local assembly of haplotypes; Version:4.0.1.2; ...; ***********************************************************************; A USER ERROR has occurred: dbsnp is not a recognized option; ***********************************************************************; ```. The exact command works with 4.0.0.0:; ```12:56:44.435 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/group/bioinformatics/software/GATK/4.0.0.0/gatk-package-4.0.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 12:56:44.750 INFO Mutect2 - ------------------------------------------------------------; 12:56:44.750 INFO Mutect2 - The Genome Analysis Toolkit (GATK) v4.0.0.0; 12:56:44.750 INFO Mutect2 - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:56:44.751 INFO Mutect2 - Executing as rbao@cri16in002 on Linux v2.6.32-573.12.1.el6.x86_64 amd64; 12:56:44.751 INFO Mutect2 - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_92-b14; 12:56:44.751 INFO Mutect2 - Start Date/Time: February 11, 2018 12:56:44 PM CST; 12:56:44.751 INFO Mutect2 - ------------------------------------------------------------; 12:56:44.751 INFO Mutect2 - ------------------------------------------------------------; 12:56:44.751 INFO Mutect2 - HTSJDK Version: 2.13.2; 12:56:44.751 INFO Mutect2 - Picard Version: 2.17.2; 12:56:44.752 INFO Mutect2 - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 12:56:44.752 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 12:56:44.752 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 12:56:44.752 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 12:56:44.752 INFO Mutect2 - Deflater: IntelDeflater; 12:56:44.752 INFO Mutect2 - Inflater: IntelInflater; ```. I was ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4390
https://github.com/broadinstitute/gatk/issues/4390:361,Availability,ERROR,ERROR,361,"Hello - I was running the exact command with GATK-Mutect2 4.0.0.0 and then switched to 4.0.1.2; an error message was returned:. ```**BETA FEATURE - WORK IN PROGRESS**; USAGE: Mutect2 [arguments]; Call somatic SNVs and indels via local assembly of haplotypes; Version:4.0.1.2; ...; ***********************************************************************; A USER ERROR has occurred: dbsnp is not a recognized option; ***********************************************************************; ```. The exact command works with 4.0.0.0:; ```12:56:44.435 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/group/bioinformatics/software/GATK/4.0.0.0/gatk-package-4.0.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 12:56:44.750 INFO Mutect2 - ------------------------------------------------------------; 12:56:44.750 INFO Mutect2 - The Genome Analysis Toolkit (GATK) v4.0.0.0; 12:56:44.750 INFO Mutect2 - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:56:44.751 INFO Mutect2 - Executing as rbao@cri16in002 on Linux v2.6.32-573.12.1.el6.x86_64 amd64; 12:56:44.751 INFO Mutect2 - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_92-b14; 12:56:44.751 INFO Mutect2 - Start Date/Time: February 11, 2018 12:56:44 PM CST; 12:56:44.751 INFO Mutect2 - ------------------------------------------------------------; 12:56:44.751 INFO Mutect2 - ------------------------------------------------------------; 12:56:44.751 INFO Mutect2 - HTSJDK Version: 2.13.2; 12:56:44.751 INFO Mutect2 - Picard Version: 2.17.2; 12:56:44.752 INFO Mutect2 - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 12:56:44.752 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 12:56:44.752 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 12:56:44.752 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 12:56:44.752 INFO Mutect2 - Deflater: IntelDeflater; 12:56:44.752 INFO Mutect2 - Inflater: IntelInflater; ```. I was ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4390
https://github.com/broadinstitute/gatk/issues/4390:105,Integrability,message,message,105,"Hello - I was running the exact command with GATK-Mutect2 4.0.0.0 and then switched to 4.0.1.2; an error message was returned:. ```**BETA FEATURE - WORK IN PROGRESS**; USAGE: Mutect2 [arguments]; Call somatic SNVs and indels via local assembly of haplotypes; Version:4.0.1.2; ...; ***********************************************************************; A USER ERROR has occurred: dbsnp is not a recognized option; ***********************************************************************; ```. The exact command works with 4.0.0.0:; ```12:56:44.435 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/group/bioinformatics/software/GATK/4.0.0.0/gatk-package-4.0.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 12:56:44.750 INFO Mutect2 - ------------------------------------------------------------; 12:56:44.750 INFO Mutect2 - The Genome Analysis Toolkit (GATK) v4.0.0.0; 12:56:44.750 INFO Mutect2 - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:56:44.751 INFO Mutect2 - Executing as rbao@cri16in002 on Linux v2.6.32-573.12.1.el6.x86_64 amd64; 12:56:44.751 INFO Mutect2 - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_92-b14; 12:56:44.751 INFO Mutect2 - Start Date/Time: February 11, 2018 12:56:44 PM CST; 12:56:44.751 INFO Mutect2 - ------------------------------------------------------------; 12:56:44.751 INFO Mutect2 - ------------------------------------------------------------; 12:56:44.751 INFO Mutect2 - HTSJDK Version: 2.13.2; 12:56:44.751 INFO Mutect2 - Picard Version: 2.17.2; 12:56:44.752 INFO Mutect2 - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 12:56:44.752 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 12:56:44.752 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 12:56:44.752 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 12:56:44.752 INFO Mutect2 - Deflater: IntelDeflater; 12:56:44.752 INFO Mutect2 - Inflater: IntelInflater; ```. I was ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4390
https://github.com/broadinstitute/gatk/issues/4390:575,Performance,Load,Loading,575,"Hello - I was running the exact command with GATK-Mutect2 4.0.0.0 and then switched to 4.0.1.2; an error message was returned:. ```**BETA FEATURE - WORK IN PROGRESS**; USAGE: Mutect2 [arguments]; Call somatic SNVs and indels via local assembly of haplotypes; Version:4.0.1.2; ...; ***********************************************************************; A USER ERROR has occurred: dbsnp is not a recognized option; ***********************************************************************; ```. The exact command works with 4.0.0.0:; ```12:56:44.435 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/group/bioinformatics/software/GATK/4.0.0.0/gatk-package-4.0.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 12:56:44.750 INFO Mutect2 - ------------------------------------------------------------; 12:56:44.750 INFO Mutect2 - The Genome Analysis Toolkit (GATK) v4.0.0.0; 12:56:44.750 INFO Mutect2 - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:56:44.751 INFO Mutect2 - Executing as rbao@cri16in002 on Linux v2.6.32-573.12.1.el6.x86_64 amd64; 12:56:44.751 INFO Mutect2 - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_92-b14; 12:56:44.751 INFO Mutect2 - Start Date/Time: February 11, 2018 12:56:44 PM CST; 12:56:44.751 INFO Mutect2 - ------------------------------------------------------------; 12:56:44.751 INFO Mutect2 - ------------------------------------------------------------; 12:56:44.751 INFO Mutect2 - HTSJDK Version: 2.13.2; 12:56:44.751 INFO Mutect2 - Picard Version: 2.17.2; 12:56:44.752 INFO Mutect2 - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 12:56:44.752 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 12:56:44.752 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 12:56:44.752 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 12:56:44.752 INFO Mutect2 - Deflater: IntelDeflater; 12:56:44.752 INFO Mutect2 - Inflater: IntelInflater; ```. I was ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4390
https://github.com/broadinstitute/gatk/issues/4393:56,Deployability,pipeline,pipeline,56,"Hello,. I'm using the latest docker image to test GATK4 pipeline (v 4.0.1.2) and I'm struggling to get it done. Apparently it has to do with the task manager service, please find attached the full log of one of my runs.; [65670_gatk_hc.log](https://github.com/broadinstitute/gatk/files/1717363/65670_gatk_hc.log). Best,; Pedro",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4393
https://github.com/broadinstitute/gatk/issues/4393:45,Testability,test,test,45,"Hello,. I'm using the latest docker image to test GATK4 pipeline (v 4.0.1.2) and I'm struggling to get it done. Apparently it has to do with the task manager service, please find attached the full log of one of my runs.; [65670_gatk_hc.log](https://github.com/broadinstitute/gatk/files/1717363/65670_gatk_hc.log). Best,; Pedro",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4393
https://github.com/broadinstitute/gatk/issues/4393:197,Testability,log,log,197,"Hello,. I'm using the latest docker image to test GATK4 pipeline (v 4.0.1.2) and I'm struggling to get it done. Apparently it has to do with the task manager service, please find attached the full log of one of my runs.; [65670_gatk_hc.log](https://github.com/broadinstitute/gatk/files/1717363/65670_gatk_hc.log). Best,; Pedro",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4393
https://github.com/broadinstitute/gatk/issues/4393:236,Testability,log,log,236,"Hello,. I'm using the latest docker image to test GATK4 pipeline (v 4.0.1.2) and I'm struggling to get it done. Apparently it has to do with the task manager service, please find attached the full log of one of my runs.; [65670_gatk_hc.log](https://github.com/broadinstitute/gatk/files/1717363/65670_gatk_hc.log). Best,; Pedro",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4393
https://github.com/broadinstitute/gatk/issues/4393:308,Testability,log,log,308,"Hello,. I'm using the latest docker image to test GATK4 pipeline (v 4.0.1.2) and I'm struggling to get it done. Apparently it has to do with the task manager service, please find attached the full log of one of my runs.; [65670_gatk_hc.log](https://github.com/broadinstitute/gatk/files/1717363/65670_gatk_hc.log). Best,; Pedro",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4393
https://github.com/broadinstitute/gatk/pull/4396:847,Deployability,Update,Updated,847,"This PR adds segments VCF writing to `PostprocessGermlineCNVCalls`. Segmentation (Viterbi) and segment quality calculation are performed by `gcnvkernel`. This PR introduces the following additional features:; - Calls and model shards are not required to be provided in sorted order anymore; - The user can specify the ref copy-number state for autosomal contigs, as well as allosomal contigs; - For both intervals and segments VCF output: now we use either `<DUP>` or `<DEL>` alleles (in place of `CN_x` alleles), depending on whether the most likely copy-number call is below or above the ; contig baseline. The contig baseline state is whatever the user has specified for autosomal contigs, and the contig ploidy state on sex chromosomes (from the output of `DetermineGermlineContigPloidy`).; - Fail-fast validations and better test coverage; - Updated cohort and case WDL scripts and WDL tests",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4396
https://github.com/broadinstitute/gatk/pull/4396:514,Integrability,depend,depending,514,"This PR adds segments VCF writing to `PostprocessGermlineCNVCalls`. Segmentation (Viterbi) and segment quality calculation are performed by `gcnvkernel`. This PR introduces the following additional features:; - Calls and model shards are not required to be provided in sorted order anymore; - The user can specify the ref copy-number state for autosomal contigs, as well as allosomal contigs; - For both intervals and segments VCF output: now we use either `<DUP>` or `<DEL>` alleles (in place of `CN_x` alleles), depending on whether the most likely copy-number call is below or above the ; contig baseline. The contig baseline state is whatever the user has specified for autosomal contigs, and the contig ploidy state on sex chromosomes (from the output of `DetermineGermlineContigPloidy`).; - Fail-fast validations and better test coverage; - Updated cohort and case WDL scripts and WDL tests",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4396
https://github.com/broadinstitute/gatk/pull/4396:127,Performance,perform,performed,127,"This PR adds segments VCF writing to `PostprocessGermlineCNVCalls`. Segmentation (Viterbi) and segment quality calculation are performed by `gcnvkernel`. This PR introduces the following additional features:; - Calls and model shards are not required to be provided in sorted order anymore; - The user can specify the ref copy-number state for autosomal contigs, as well as allosomal contigs; - For both intervals and segments VCF output: now we use either `<DUP>` or `<DEL>` alleles (in place of `CN_x` alleles), depending on whether the most likely copy-number call is below or above the ; contig baseline. The contig baseline state is whatever the user has specified for autosomal contigs, and the contig ploidy state on sex chromosomes (from the output of `DetermineGermlineContigPloidy`).; - Fail-fast validations and better test coverage; - Updated cohort and case WDL scripts and WDL tests",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4396
https://github.com/broadinstitute/gatk/pull/4396:807,Security,validat,validations,807,"This PR adds segments VCF writing to `PostprocessGermlineCNVCalls`. Segmentation (Viterbi) and segment quality calculation are performed by `gcnvkernel`. This PR introduces the following additional features:; - Calls and model shards are not required to be provided in sorted order anymore; - The user can specify the ref copy-number state for autosomal contigs, as well as allosomal contigs; - For both intervals and segments VCF output: now we use either `<DUP>` or `<DEL>` alleles (in place of `CN_x` alleles), depending on whether the most likely copy-number call is below or above the ; contig baseline. The contig baseline state is whatever the user has specified for autosomal contigs, and the contig ploidy state on sex chromosomes (from the output of `DetermineGermlineContigPloidy`).; - Fail-fast validations and better test coverage; - Updated cohort and case WDL scripts and WDL tests",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4396
https://github.com/broadinstitute/gatk/pull/4396:830,Testability,test,test,830,"This PR adds segments VCF writing to `PostprocessGermlineCNVCalls`. Segmentation (Viterbi) and segment quality calculation are performed by `gcnvkernel`. This PR introduces the following additional features:; - Calls and model shards are not required to be provided in sorted order anymore; - The user can specify the ref copy-number state for autosomal contigs, as well as allosomal contigs; - For both intervals and segments VCF output: now we use either `<DUP>` or `<DEL>` alleles (in place of `CN_x` alleles), depending on whether the most likely copy-number call is below or above the ; contig baseline. The contig baseline state is whatever the user has specified for autosomal contigs, and the contig ploidy state on sex chromosomes (from the output of `DetermineGermlineContigPloidy`).; - Fail-fast validations and better test coverage; - Updated cohort and case WDL scripts and WDL tests",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4396
https://github.com/broadinstitute/gatk/pull/4396:891,Testability,test,tests,891,"This PR adds segments VCF writing to `PostprocessGermlineCNVCalls`. Segmentation (Viterbi) and segment quality calculation are performed by `gcnvkernel`. This PR introduces the following additional features:; - Calls and model shards are not required to be provided in sorted order anymore; - The user can specify the ref copy-number state for autosomal contigs, as well as allosomal contigs; - For both intervals and segments VCF output: now we use either `<DUP>` or `<DEL>` alleles (in place of `CN_x` alleles), depending on whether the most likely copy-number call is below or above the ; contig baseline. The contig baseline state is whatever the user has specified for autosomal contigs, and the contig ploidy state on sex chromosomes (from the output of `DetermineGermlineContigPloidy`).; - Fail-fast validations and better test coverage; - Updated cohort and case WDL scripts and WDL tests",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4396
https://github.com/broadinstitute/gatk/pull/4401:11,Testability,test,tests,11,* updating tests because a toString changed in htsjdk and it's value is written directly into the outputs of some tools; * fix two trivial mistakes in ExampleVariantWalkerSpark that I noticed,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4401
https://github.com/broadinstitute/gatk/issues/4403:208,Availability,avail,available,208,"At least -I and -O, to be consistent with ModelSegments pipeline. I think our general rule for the CNV tools is that we do not introduce any CNV-specific short names, but we can use the standard ones already available.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4403
https://github.com/broadinstitute/gatk/issues/4403:56,Deployability,pipeline,pipeline,56,"At least -I and -O, to be consistent with ModelSegments pipeline. I think our general rule for the CNV tools is that we do not introduce any CNV-specific short names, but we can use the standard ones already available.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4403
https://github.com/broadinstitute/gatk/issues/4407:203,Deployability,update,updated,203,"For INDELs, ReferenceContext is rendered with too many bases. This is because of how the VCF format stores INDELs (vs the MAF format). The logic for creating the ReferenceContext base string needs to be updated to account for the extra bases in the VCF format.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4407
https://github.com/broadinstitute/gatk/issues/4407:139,Testability,log,logic,139,"For INDELs, ReferenceContext is rendered with too many bases. This is because of how the VCF format stores INDELs (vs the MAF format). The logic for creating the ReferenceContext base string needs to be updated to account for the extra bases in the VCF format.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4407
https://github.com/broadinstitute/gatk/issues/4409:45,Deployability,install,installAll,45,"The current docker build script runs `gradle installAll` in addition to running `localJar`. This causes the `gatk` script in our docker image to prefer running with the unpackaged set of jars, instead of the fully packaged jar. This, in turn, can cause us to run out of file handles in certain tools, since we need to open all of the jars for our dependencies individually at once. We should just run something like `gradle clean localJar sparkJar createPythonPackageArchive` in our `Dockerfile`, and avoid `installAll`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4409
https://github.com/broadinstitute/gatk/issues/4409:508,Deployability,install,installAll,508,"The current docker build script runs `gradle installAll` in addition to running `localJar`. This causes the `gatk` script in our docker image to prefer running with the unpackaged set of jars, instead of the fully packaged jar. This, in turn, can cause us to run out of file handles in certain tools, since we need to open all of the jars for our dependencies individually at once. We should just run something like `gradle clean localJar sparkJar createPythonPackageArchive` in our `Dockerfile`, and avoid `installAll`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4409
https://github.com/broadinstitute/gatk/issues/4409:347,Integrability,depend,dependencies,347,"The current docker build script runs `gradle installAll` in addition to running `localJar`. This causes the `gatk` script in our docker image to prefer running with the unpackaged set of jars, instead of the fully packaged jar. This, in turn, can cause us to run out of file handles in certain tools, since we need to open all of the jars for our dependencies individually at once. We should just run something like `gradle clean localJar sparkJar createPythonPackageArchive` in our `Dockerfile`, and avoid `installAll`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4409
https://github.com/broadinstitute/gatk/issues/4409:501,Safety,avoid,avoid,501,"The current docker build script runs `gradle installAll` in addition to running `localJar`. This causes the `gatk` script in our docker image to prefer running with the unpackaged set of jars, instead of the fully packaged jar. This, in turn, can cause us to run out of file handles in certain tools, since we need to open all of the jars for our dependencies individually at once. We should just run something like `gradle clean localJar sparkJar createPythonPackageArchive` in our `Dockerfile`, and avoid `installAll`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4409
https://github.com/broadinstitute/gatk/issues/4413:15,Availability,error,error,15,"Hi all,. Below error occurs trying to access Funcotator data source directory installed on lustre file system. We have a non-lustre mounted fs for cases like this, but I thought it was worth bringing up. ```; org.broadinstitute.hellbender.exceptions.GATKException: Unable to query the database for geneName: null; 	at org.broadinstitute.hellbender.tools.funcotator.dataSources.cosmic.CosmicFuncotationFactory.createFuncotations(CosmicFuncotationFactory.java:244); 	at org.broadinstitute.hellbender.tools.funcotator.Funcotator.enqueueAndHandleVariant(Funcotator.java:404); 	at org.broadinstitute.hellbender.tools.funcotator.Funcotator.apply(Funcotator.java:316); 	at org.broadinstitute.hellbender.engine.VariantWalkerBase.lambda$traverse$0(VariantWalkerBase.java:110); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); 	at org.broadinstitute.hellbender.engine.VariantWalkerBase.traverse(VariantWalkerBase.java:108); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:893); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:136); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgr",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4413
https://github.com/broadinstitute/gatk/issues/4413:2312,Availability,error,error,2312, 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); 	at org.broadinstitute.hellbender.engine.VariantWalkerBase.traverse(VariantWalkerBase.java:108); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:893); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:136); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:152); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:195); 	at org.broadinstitute.hellbender.Main.main(Main.java:275); Caused by: org.sqlite.SQLiteException: [SQLITE_IOERR_LOCK] I/O error in the advisory file locking logic (disk I/O error); 	at org.sqlite.core.DB.newSQLException(DB.java:909); 	at org.sqlite.core.DB.newSQLException(DB.java:921); 	at org.sqlite.core.DB.throwex(DB.java:886); 	at org.sqlite.core.NativeDB.prepare_utf8(Native Method); 	at org.sqlite.core.NativeDB.prepare(NativeDB.java:127); 	at org.sqlite.core.DB.prepare(DB.java:227); 	at org.sqlite.jdbc3.JDBC3Statement.executeQuery(JDBC3Statement.java:81); 	at org.broadinstitute.hellbender.tools.funcotator.dataSources.cosmic.CosmicFuncotationFactory.createFuncotations(CosmicFuncotationFactory.java:215); 	... 21 more. ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4413
https://github.com/broadinstitute/gatk/issues/4413:2363,Availability,error,error,2363, 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); 	at org.broadinstitute.hellbender.engine.VariantWalkerBase.traverse(VariantWalkerBase.java:108); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:893); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:136); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:152); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:195); 	at org.broadinstitute.hellbender.Main.main(Main.java:275); Caused by: org.sqlite.SQLiteException: [SQLITE_IOERR_LOCK] I/O error in the advisory file locking logic (disk I/O error); 	at org.sqlite.core.DB.newSQLException(DB.java:909); 	at org.sqlite.core.DB.newSQLException(DB.java:921); 	at org.sqlite.core.DB.throwex(DB.java:886); 	at org.sqlite.core.NativeDB.prepare_utf8(Native Method); 	at org.sqlite.core.NativeDB.prepare(NativeDB.java:127); 	at org.sqlite.core.DB.prepare(DB.java:227); 	at org.sqlite.jdbc3.JDBC3Statement.executeQuery(JDBC3Statement.java:81); 	at org.broadinstitute.hellbender.tools.funcotator.dataSources.cosmic.CosmicFuncotationFactory.createFuncotations(CosmicFuncotationFactory.java:215); 	... 21 more. ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4413
https://github.com/broadinstitute/gatk/issues/4413:78,Deployability,install,installed,78,"Hi all,. Below error occurs trying to access Funcotator data source directory installed on lustre file system. We have a non-lustre mounted fs for cases like this, but I thought it was worth bringing up. ```; org.broadinstitute.hellbender.exceptions.GATKException: Unable to query the database for geneName: null; 	at org.broadinstitute.hellbender.tools.funcotator.dataSources.cosmic.CosmicFuncotationFactory.createFuncotations(CosmicFuncotationFactory.java:244); 	at org.broadinstitute.hellbender.tools.funcotator.Funcotator.enqueueAndHandleVariant(Funcotator.java:404); 	at org.broadinstitute.hellbender.tools.funcotator.Funcotator.apply(Funcotator.java:316); 	at org.broadinstitute.hellbender.engine.VariantWalkerBase.lambda$traverse$0(VariantWalkerBase.java:110); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); 	at org.broadinstitute.hellbender.engine.VariantWalkerBase.traverse(VariantWalkerBase.java:108); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:893); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:136); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgr",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4413
https://github.com/broadinstitute/gatk/issues/4413:1186,Integrability,wrap,wrapAndCopyInto,1186,. ```; org.broadinstitute.hellbender.exceptions.GATKException: Unable to query the database for geneName: null; 	at org.broadinstitute.hellbender.tools.funcotator.dataSources.cosmic.CosmicFuncotationFactory.createFuncotations(CosmicFuncotationFactory.java:244); 	at org.broadinstitute.hellbender.tools.funcotator.Funcotator.enqueueAndHandleVariant(Funcotator.java:404); 	at org.broadinstitute.hellbender.tools.funcotator.Funcotator.apply(Funcotator.java:316); 	at org.broadinstitute.hellbender.engine.VariantWalkerBase.lambda$traverse$0(VariantWalkerBase.java:110); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); 	at org.broadinstitute.hellbender.engine.VariantWalkerBase.traverse(VariantWalkerBase.java:108); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:893); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:136); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:152); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:195); 	at org.broadi,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4413
https://github.com/broadinstitute/gatk/issues/4413:38,Security,access,access,38,"Hi all,. Below error occurs trying to access Funcotator data source directory installed on lustre file system. We have a non-lustre mounted fs for cases like this, but I thought it was worth bringing up. ```; org.broadinstitute.hellbender.exceptions.GATKException: Unable to query the database for geneName: null; 	at org.broadinstitute.hellbender.tools.funcotator.dataSources.cosmic.CosmicFuncotationFactory.createFuncotations(CosmicFuncotationFactory.java:244); 	at org.broadinstitute.hellbender.tools.funcotator.Funcotator.enqueueAndHandleVariant(Funcotator.java:404); 	at org.broadinstitute.hellbender.tools.funcotator.Funcotator.apply(Funcotator.java:316); 	at org.broadinstitute.hellbender.engine.VariantWalkerBase.lambda$traverse$0(VariantWalkerBase.java:110); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); 	at org.broadinstitute.hellbender.engine.VariantWalkerBase.traverse(VariantWalkerBase.java:108); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:893); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:136); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgr",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4413
https://github.com/broadinstitute/gatk/issues/4413:2347,Testability,log,logic,2347, 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); 	at org.broadinstitute.hellbender.engine.VariantWalkerBase.traverse(VariantWalkerBase.java:108); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:893); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:136); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:152); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:195); 	at org.broadinstitute.hellbender.Main.main(Main.java:275); Caused by: org.sqlite.SQLiteException: [SQLITE_IOERR_LOCK] I/O error in the advisory file locking logic (disk I/O error); 	at org.sqlite.core.DB.newSQLException(DB.java:909); 	at org.sqlite.core.DB.newSQLException(DB.java:921); 	at org.sqlite.core.DB.throwex(DB.java:886); 	at org.sqlite.core.NativeDB.prepare_utf8(Native Method); 	at org.sqlite.core.NativeDB.prepare(NativeDB.java:127); 	at org.sqlite.core.DB.prepare(DB.java:227); 	at org.sqlite.jdbc3.JDBC3Statement.executeQuery(JDBC3Statement.java:81); 	at org.broadinstitute.hellbender.tools.funcotator.dataSources.cosmic.CosmicFuncotationFactory.createFuncotations(CosmicFuncotationFactory.java:215); 	... 21 more. ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4413
https://github.com/broadinstitute/gatk/pull/4414:37,Testability,test,tests,37,"Fixes #3983 . - Added two tumor-only tests to the CNV WDL test. I did not add all four possible combinations, since I did not see the value. I just wanted to make sure that the WGS and WES cases were covered.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4414
https://github.com/broadinstitute/gatk/pull/4414:58,Testability,test,test,58,"Fixes #3983 . - Added two tumor-only tests to the CNV WDL test. I did not add all four possible combinations, since I did not see the value. I just wanted to make sure that the WGS and WES cases were covered.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4414
https://github.com/broadinstitute/gatk/issues/4418:94,Deployability,upgrade,upgrade,94,"There are too many WDL conventions that work in cromwell 29 and not in 30. Therefore, time to upgrade.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4418
https://github.com/broadinstitute/gatk/issues/4420:117,Testability,log,logic,117,The `transcriptPosition` annotation is being computed incorrectly in the `GencodeFuncotationFactory`. This erroneous logic must be replaced with logic that is correct for the definition of the field as found here:. https://wiki.nci.nih.gov/display/TCGA/Mutation+Annotation+Format+(MAF)+Specification,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4420
https://github.com/broadinstitute/gatk/issues/4420:145,Testability,log,logic,145,The `transcriptPosition` annotation is being computed incorrectly in the `GencodeFuncotationFactory`. This erroneous logic must be replaced with logic that is correct for the definition of the field as found here:. https://wiki.nci.nih.gov/display/TCGA/Mutation+Annotation+Format+(MAF)+Specification,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4420
https://github.com/broadinstitute/gatk/issues/4421:138,Deployability,configurat,configuration,138,"Lee, just letting you know I've tagged you in a forum question. ---; The oncotated maf output includes many rejected mutations (using the configuration from the public spaces). This is bad practice. The unfiltered VCF (or preferably a tsv) should have these sites but we should not be annotating them or putting them in final outputs. . This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/11440/m2-gatk4-oncotated-maf-output-includes-rejected-mutations/p1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4421
https://github.com/broadinstitute/gatk/issues/4421:138,Modifiability,config,configuration,138,"Lee, just letting you know I've tagged you in a forum question. ---; The oncotated maf output includes many rejected mutations (using the configuration from the public spaces). This is bad practice. The unfiltered VCF (or preferably a tsv) should have these sites but we should not be annotating them or putting them in final outputs. . This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/11440/m2-gatk4-oncotated-maf-output-includes-rejected-mutations/p1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4421
https://github.com/broadinstitute/gatk/issues/4422:96,Deployability,Update,Update,96,"Many users do not want mutations filtered in the VCF to show up in the MAF file. Two things:. - Update the default M2 WDL to use `broadinstitute/oncotator:1.9.7.0` docker image. - Add an optional flag to prune filtered mutations to the Oncotator task. If this is set to True, add the following to the oncotator command: `--collapse-filter-cols --prune-filter-cols`. This is unnecessary for the CNV WDL.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4422
https://github.com/broadinstitute/gatk/pull/4424:22,Testability,test,test,22,"Add path support, and test.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4424
https://github.com/broadinstitute/gatk/issues/4425:53,Deployability,update,update,53,Build command is: . ./gradlew gatkDoc -PphpDoc. Will update this issue with the location for pushing the docs to the website shortly.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4425
https://github.com/broadinstitute/gatk/issues/4427:303,Availability,error,error,303,"@chandrans commented on [Tue Feb 20 2018](https://github.com/broadinstitute/gsaweb/issues/89). ## Feature request. ### Tool(s) involved; CollectRnaSeqMetrics. ### Description; In Picard standalone version 2.16.0, --IGNORE_SEQUENCE is not required, but in GATK4 version, it is required and fails with an error if not provided. This should be an optional argument. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/comment/46016#Comment_46016",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4427
https://github.com/broadinstitute/gatk/pull/4428:167,Deployability,upgrade,upgraded,167,This reverts commit 8a366c7ba570c61338f7109b86c3284b80d5cf47. We noticed a major performance regression in `BaseRecalibratorSpark` and `HaplotypeCallerSpark` after we upgraded our ADAM dependency (see https://github.com/broadinstitute/gatk/issues/4376). This PR reverts that upgrade for now until we understand the underlying cause.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4428
https://github.com/broadinstitute/gatk/pull/4428:275,Deployability,upgrade,upgrade,275,This reverts commit 8a366c7ba570c61338f7109b86c3284b80d5cf47. We noticed a major performance regression in `BaseRecalibratorSpark` and `HaplotypeCallerSpark` after we upgraded our ADAM dependency (see https://github.com/broadinstitute/gatk/issues/4376). This PR reverts that upgrade for now until we understand the underlying cause.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4428
https://github.com/broadinstitute/gatk/pull/4428:185,Integrability,depend,dependency,185,This reverts commit 8a366c7ba570c61338f7109b86c3284b80d5cf47. We noticed a major performance regression in `BaseRecalibratorSpark` and `HaplotypeCallerSpark` after we upgraded our ADAM dependency (see https://github.com/broadinstitute/gatk/issues/4376). This PR reverts that upgrade for now until we understand the underlying cause.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4428
https://github.com/broadinstitute/gatk/pull/4428:81,Performance,perform,performance,81,This reverts commit 8a366c7ba570c61338f7109b86c3284b80d5cf47. We noticed a major performance regression in `BaseRecalibratorSpark` and `HaplotypeCallerSpark` after we upgraded our ADAM dependency (see https://github.com/broadinstitute/gatk/issues/4376). This PR reverts that upgrade for now until we understand the underlying cause.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4428
https://github.com/broadinstitute/gatk/pull/4429:139,Integrability,message,message,139,"Fixes https://github.com/broadinstitute/gatk/issues/4367. Includes a sleep for 5 seconds; not sure thats a good idea, although the warning message scrolls off the screen pretty quickly, especially with Spark apps. This will need to be replicated in Picard as well. Experimental:. <img width=""679"" alt=""screen shot 2018-02-20 at 3 37 06 pm"" src=""https://user-images.githubusercontent.com/10062863/36447994-16fe058c-1654-11e8-94e7-efc6e90b8f3e.png"">. Beta:. <img width=""653"" alt=""screen shot 2018-02-20 at 3 37 22 pm"" src=""https://user-images.githubusercontent.com/10062863/36447984-13245560-1654-11e8-8776-9345319e37ae.png"">",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4429
https://github.com/broadinstitute/gatk/pull/4430:24,Deployability,integrat,integration,24,"This method is for unit/integration testing purposes only, and should not be called from tools.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4430
https://github.com/broadinstitute/gatk/pull/4430:24,Integrability,integrat,integration,24,"This method is for unit/integration testing purposes only, and should not be called from tools.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4430
https://github.com/broadinstitute/gatk/pull/4430:36,Testability,test,testing,36,"This method is for unit/integration testing purposes only, and should not be called from tools.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4430
https://github.com/broadinstitute/gatk/pull/4431:0,Modifiability,refactor,refactoring,0,refactoring code to remove confusion and unnecessary call to calculateChromosomeCounts,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4431
https://github.com/broadinstitute/gatk/issues/4433:178,Availability,error,error,178,"I think this user report sums it up nicely. ----; User Report; ----. In my BASH scripts I often use ""$?"" to monitor the exit status of a process and normally stop if there is an error. However, I am using the latest version of GATK (4.0.0.0) and some tools return 0 exit status even if they fail. Instead, they display the following message to STDOUT:; ; Tool returned:; 1. Though inconvenient for error handling in BASH scripts, this might be an intended behaviour, but not all tools exhibit it. To mention a few, MarkDuplicates, CollectMultipleMetrics, CollectGcBiasMetrics always have a 0 exit status, whereas VariantsToTable or CountVariants do return 1 if they encounter an error. . A similar issue had been reported in the past for previous versions of GATK (https://gatkforums.broadinstitute.org/gatk/discussion/8618/error-handling-end-exit-codes-in-gatk). Best regards,. Roger. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/11414/exit-codes-in-gatk-4-0/p1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4433
https://github.com/broadinstitute/gatk/issues/4433:398,Availability,error,error,398,"I think this user report sums it up nicely. ----; User Report; ----. In my BASH scripts I often use ""$?"" to monitor the exit status of a process and normally stop if there is an error. However, I am using the latest version of GATK (4.0.0.0) and some tools return 0 exit status even if they fail. Instead, they display the following message to STDOUT:; ; Tool returned:; 1. Though inconvenient for error handling in BASH scripts, this might be an intended behaviour, but not all tools exhibit it. To mention a few, MarkDuplicates, CollectMultipleMetrics, CollectGcBiasMetrics always have a 0 exit status, whereas VariantsToTable or CountVariants do return 1 if they encounter an error. . A similar issue had been reported in the past for previous versions of GATK (https://gatkforums.broadinstitute.org/gatk/discussion/8618/error-handling-end-exit-codes-in-gatk). Best regards,. Roger. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/11414/exit-codes-in-gatk-4-0/p1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4433
https://github.com/broadinstitute/gatk/issues/4433:679,Availability,error,error,679,"I think this user report sums it up nicely. ----; User Report; ----. In my BASH scripts I often use ""$?"" to monitor the exit status of a process and normally stop if there is an error. However, I am using the latest version of GATK (4.0.0.0) and some tools return 0 exit status even if they fail. Instead, they display the following message to STDOUT:; ; Tool returned:; 1. Though inconvenient for error handling in BASH scripts, this might be an intended behaviour, but not all tools exhibit it. To mention a few, MarkDuplicates, CollectMultipleMetrics, CollectGcBiasMetrics always have a 0 exit status, whereas VariantsToTable or CountVariants do return 1 if they encounter an error. . A similar issue had been reported in the past for previous versions of GATK (https://gatkforums.broadinstitute.org/gatk/discussion/8618/error-handling-end-exit-codes-in-gatk). Best regards,. Roger. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/11414/exit-codes-in-gatk-4-0/p1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4433
https://github.com/broadinstitute/gatk/issues/4433:824,Availability,error,error-handling-end-exit-codes-in-gatk,824,"I think this user report sums it up nicely. ----; User Report; ----. In my BASH scripts I often use ""$?"" to monitor the exit status of a process and normally stop if there is an error. However, I am using the latest version of GATK (4.0.0.0) and some tools return 0 exit status even if they fail. Instead, they display the following message to STDOUT:; ; Tool returned:; 1. Though inconvenient for error handling in BASH scripts, this might be an intended behaviour, but not all tools exhibit it. To mention a few, MarkDuplicates, CollectMultipleMetrics, CollectGcBiasMetrics always have a 0 exit status, whereas VariantsToTable or CountVariants do return 1 if they encounter an error. . A similar issue had been reported in the past for previous versions of GATK (https://gatkforums.broadinstitute.org/gatk/discussion/8618/error-handling-end-exit-codes-in-gatk). Best regards,. Roger. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/11414/exit-codes-in-gatk-4-0/p1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4433
https://github.com/broadinstitute/gatk/issues/4433:108,Energy Efficiency,monitor,monitor,108,"I think this user report sums it up nicely. ----; User Report; ----. In my BASH scripts I often use ""$?"" to monitor the exit status of a process and normally stop if there is an error. However, I am using the latest version of GATK (4.0.0.0) and some tools return 0 exit status even if they fail. Instead, they display the following message to STDOUT:; ; Tool returned:; 1. Though inconvenient for error handling in BASH scripts, this might be an intended behaviour, but not all tools exhibit it. To mention a few, MarkDuplicates, CollectMultipleMetrics, CollectGcBiasMetrics always have a 0 exit status, whereas VariantsToTable or CountVariants do return 1 if they encounter an error. . A similar issue had been reported in the past for previous versions of GATK (https://gatkforums.broadinstitute.org/gatk/discussion/8618/error-handling-end-exit-codes-in-gatk). Best regards,. Roger. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/11414/exit-codes-in-gatk-4-0/p1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4433
https://github.com/broadinstitute/gatk/issues/4433:333,Integrability,message,message,333,"I think this user report sums it up nicely. ----; User Report; ----. In my BASH scripts I often use ""$?"" to monitor the exit status of a process and normally stop if there is an error. However, I am using the latest version of GATK (4.0.0.0) and some tools return 0 exit status even if they fail. Instead, they display the following message to STDOUT:; ; Tool returned:; 1. Though inconvenient for error handling in BASH scripts, this might be an intended behaviour, but not all tools exhibit it. To mention a few, MarkDuplicates, CollectMultipleMetrics, CollectGcBiasMetrics always have a 0 exit status, whereas VariantsToTable or CountVariants do return 1 if they encounter an error. . A similar issue had been reported in the past for previous versions of GATK (https://gatkforums.broadinstitute.org/gatk/discussion/8618/error-handling-end-exit-codes-in-gatk). Best regards,. Roger. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/11414/exit-codes-in-gatk-4-0/p1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4433
https://github.com/broadinstitute/gatk/issues/4434:68,Availability,down,downloaded,68,"When using the command `./gradlew bundle` to build the GATK project downloaded from GitHub I receive this error:. ```; Starting a Gradle Daemon, 1 incompatible Daemon could not be reused, use --status for details; :createPythonPackageArchive; Creating GATK Python package archive...; Created GATK Python package archive in /datadrive/NGS-SparkGATK/gatk/build/gatkPythonPackageArchive.zip; :compileJava UP-TO-DATE; :processResources; :classes; :gatkTabComplete; /datadrive/NGS-SparkGATK/gatk/build/tmp/expandedArchives/picard-2.17.2-sources.jar_2eu0sptfiz8othzntm7sqhvx4/picard/util/LiftoverUtils.java:332: error: unmappable character for encoding ASCII; * Based on Adrian Tan, Gon??alo R. Abecasis and Hyun Min Kang. (2015); ^; /datadrive/NGS-SparkGATK/gatk/build/tmp/expandedArchives/picard-2.17.2-sources.jar_2eu0sptfiz8othzntm7sqhvx4/picard/util/LiftoverUtils.java:332: error: unmappable character for encoding ASCII; * Based on Adrian Tan, Gon??alo R. Abecasis and Hyun Min Kang. (2015); ^; 2 errors; :gatkTabComplete FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':gatkTabComplete'.; > Javadoc generation failed. Generated Javadoc options file (useful for troubleshooting): '/datadrive/NGS-SparkGATK/gatk/build/tmp/gatkTabComplete/javadoc.options'. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 9.873 secs; ```; Can you please ""rewrite"" the name _Gonalo_? It is always cause of exceptions when building the GATK project.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4434
https://github.com/broadinstitute/gatk/issues/4434:106,Availability,error,error,106,"When using the command `./gradlew bundle` to build the GATK project downloaded from GitHub I receive this error:. ```; Starting a Gradle Daemon, 1 incompatible Daemon could not be reused, use --status for details; :createPythonPackageArchive; Creating GATK Python package archive...; Created GATK Python package archive in /datadrive/NGS-SparkGATK/gatk/build/gatkPythonPackageArchive.zip; :compileJava UP-TO-DATE; :processResources; :classes; :gatkTabComplete; /datadrive/NGS-SparkGATK/gatk/build/tmp/expandedArchives/picard-2.17.2-sources.jar_2eu0sptfiz8othzntm7sqhvx4/picard/util/LiftoverUtils.java:332: error: unmappable character for encoding ASCII; * Based on Adrian Tan, Gon??alo R. Abecasis and Hyun Min Kang. (2015); ^; /datadrive/NGS-SparkGATK/gatk/build/tmp/expandedArchives/picard-2.17.2-sources.jar_2eu0sptfiz8othzntm7sqhvx4/picard/util/LiftoverUtils.java:332: error: unmappable character for encoding ASCII; * Based on Adrian Tan, Gon??alo R. Abecasis and Hyun Min Kang. (2015); ^; 2 errors; :gatkTabComplete FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':gatkTabComplete'.; > Javadoc generation failed. Generated Javadoc options file (useful for troubleshooting): '/datadrive/NGS-SparkGATK/gatk/build/tmp/gatkTabComplete/javadoc.options'. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 9.873 secs; ```; Can you please ""rewrite"" the name _Gonalo_? It is always cause of exceptions when building the GATK project.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4434
https://github.com/broadinstitute/gatk/issues/4434:606,Availability,error,error,606,"When using the command `./gradlew bundle` to build the GATK project downloaded from GitHub I receive this error:. ```; Starting a Gradle Daemon, 1 incompatible Daemon could not be reused, use --status for details; :createPythonPackageArchive; Creating GATK Python package archive...; Created GATK Python package archive in /datadrive/NGS-SparkGATK/gatk/build/gatkPythonPackageArchive.zip; :compileJava UP-TO-DATE; :processResources; :classes; :gatkTabComplete; /datadrive/NGS-SparkGATK/gatk/build/tmp/expandedArchives/picard-2.17.2-sources.jar_2eu0sptfiz8othzntm7sqhvx4/picard/util/LiftoverUtils.java:332: error: unmappable character for encoding ASCII; * Based on Adrian Tan, Gon??alo R. Abecasis and Hyun Min Kang. (2015); ^; /datadrive/NGS-SparkGATK/gatk/build/tmp/expandedArchives/picard-2.17.2-sources.jar_2eu0sptfiz8othzntm7sqhvx4/picard/util/LiftoverUtils.java:332: error: unmappable character for encoding ASCII; * Based on Adrian Tan, Gon??alo R. Abecasis and Hyun Min Kang. (2015); ^; 2 errors; :gatkTabComplete FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':gatkTabComplete'.; > Javadoc generation failed. Generated Javadoc options file (useful for troubleshooting): '/datadrive/NGS-SparkGATK/gatk/build/tmp/gatkTabComplete/javadoc.options'. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 9.873 secs; ```; Can you please ""rewrite"" the name _Gonalo_? It is always cause of exceptions when building the GATK project.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4434
https://github.com/broadinstitute/gatk/issues/4434:873,Availability,error,error,873,"When using the command `./gradlew bundle` to build the GATK project downloaded from GitHub I receive this error:. ```; Starting a Gradle Daemon, 1 incompatible Daemon could not be reused, use --status for details; :createPythonPackageArchive; Creating GATK Python package archive...; Created GATK Python package archive in /datadrive/NGS-SparkGATK/gatk/build/gatkPythonPackageArchive.zip; :compileJava UP-TO-DATE; :processResources; :classes; :gatkTabComplete; /datadrive/NGS-SparkGATK/gatk/build/tmp/expandedArchives/picard-2.17.2-sources.jar_2eu0sptfiz8othzntm7sqhvx4/picard/util/LiftoverUtils.java:332: error: unmappable character for encoding ASCII; * Based on Adrian Tan, Gon??alo R. Abecasis and Hyun Min Kang. (2015); ^; /datadrive/NGS-SparkGATK/gatk/build/tmp/expandedArchives/picard-2.17.2-sources.jar_2eu0sptfiz8othzntm7sqhvx4/picard/util/LiftoverUtils.java:332: error: unmappable character for encoding ASCII; * Based on Adrian Tan, Gon??alo R. Abecasis and Hyun Min Kang. (2015); ^; 2 errors; :gatkTabComplete FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':gatkTabComplete'.; > Javadoc generation failed. Generated Javadoc options file (useful for troubleshooting): '/datadrive/NGS-SparkGATK/gatk/build/tmp/gatkTabComplete/javadoc.options'. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 9.873 secs; ```; Can you please ""rewrite"" the name _Gonalo_? It is always cause of exceptions when building the GATK project.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4434
https://github.com/broadinstitute/gatk/issues/4434:997,Availability,error,errors,997,"When using the command `./gradlew bundle` to build the GATK project downloaded from GitHub I receive this error:. ```; Starting a Gradle Daemon, 1 incompatible Daemon could not be reused, use --status for details; :createPythonPackageArchive; Creating GATK Python package archive...; Created GATK Python package archive in /datadrive/NGS-SparkGATK/gatk/build/gatkPythonPackageArchive.zip; :compileJava UP-TO-DATE; :processResources; :classes; :gatkTabComplete; /datadrive/NGS-SparkGATK/gatk/build/tmp/expandedArchives/picard-2.17.2-sources.jar_2eu0sptfiz8othzntm7sqhvx4/picard/util/LiftoverUtils.java:332: error: unmappable character for encoding ASCII; * Based on Adrian Tan, Gon??alo R. Abecasis and Hyun Min Kang. (2015); ^; /datadrive/NGS-SparkGATK/gatk/build/tmp/expandedArchives/picard-2.17.2-sources.jar_2eu0sptfiz8othzntm7sqhvx4/picard/util/LiftoverUtils.java:332: error: unmappable character for encoding ASCII; * Based on Adrian Tan, Gon??alo R. Abecasis and Hyun Min Kang. (2015); ^; 2 errors; :gatkTabComplete FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':gatkTabComplete'.; > Javadoc generation failed. Generated Javadoc options file (useful for troubleshooting): '/datadrive/NGS-SparkGATK/gatk/build/tmp/gatkTabComplete/javadoc.options'. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 9.873 secs; ```; Can you please ""rewrite"" the name _Gonalo_? It is always cause of exceptions when building the GATK project.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4434
https://github.com/broadinstitute/gatk/issues/4434:1030,Availability,FAILURE,FAILURE,1030,"When using the command `./gradlew bundle` to build the GATK project downloaded from GitHub I receive this error:. ```; Starting a Gradle Daemon, 1 incompatible Daemon could not be reused, use --status for details; :createPythonPackageArchive; Creating GATK Python package archive...; Created GATK Python package archive in /datadrive/NGS-SparkGATK/gatk/build/gatkPythonPackageArchive.zip; :compileJava UP-TO-DATE; :processResources; :classes; :gatkTabComplete; /datadrive/NGS-SparkGATK/gatk/build/tmp/expandedArchives/picard-2.17.2-sources.jar_2eu0sptfiz8othzntm7sqhvx4/picard/util/LiftoverUtils.java:332: error: unmappable character for encoding ASCII; * Based on Adrian Tan, Gon??alo R. Abecasis and Hyun Min Kang. (2015); ^; /datadrive/NGS-SparkGATK/gatk/build/tmp/expandedArchives/picard-2.17.2-sources.jar_2eu0sptfiz8othzntm7sqhvx4/picard/util/LiftoverUtils.java:332: error: unmappable character for encoding ASCII; * Based on Adrian Tan, Gon??alo R. Abecasis and Hyun Min Kang. (2015); ^; 2 errors; :gatkTabComplete FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':gatkTabComplete'.; > Javadoc generation failed. Generated Javadoc options file (useful for troubleshooting): '/datadrive/NGS-SparkGATK/gatk/build/tmp/gatkTabComplete/javadoc.options'. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 9.873 secs; ```; Can you please ""rewrite"" the name _Gonalo_? It is always cause of exceptions when building the GATK project.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4434
https://github.com/broadinstitute/gatk/issues/4434:1481,Modifiability,rewrite,rewrite,1481,"When using the command `./gradlew bundle` to build the GATK project downloaded from GitHub I receive this error:. ```; Starting a Gradle Daemon, 1 incompatible Daemon could not be reused, use --status for details; :createPythonPackageArchive; Creating GATK Python package archive...; Created GATK Python package archive in /datadrive/NGS-SparkGATK/gatk/build/gatkPythonPackageArchive.zip; :compileJava UP-TO-DATE; :processResources; :classes; :gatkTabComplete; /datadrive/NGS-SparkGATK/gatk/build/tmp/expandedArchives/picard-2.17.2-sources.jar_2eu0sptfiz8othzntm7sqhvx4/picard/util/LiftoverUtils.java:332: error: unmappable character for encoding ASCII; * Based on Adrian Tan, Gon??alo R. Abecasis and Hyun Min Kang. (2015); ^; /datadrive/NGS-SparkGATK/gatk/build/tmp/expandedArchives/picard-2.17.2-sources.jar_2eu0sptfiz8othzntm7sqhvx4/picard/util/LiftoverUtils.java:332: error: unmappable character for encoding ASCII; * Based on Adrian Tan, Gon??alo R. Abecasis and Hyun Min Kang. (2015); ^; 2 errors; :gatkTabComplete FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':gatkTabComplete'.; > Javadoc generation failed. Generated Javadoc options file (useful for troubleshooting): '/datadrive/NGS-SparkGATK/gatk/build/tmp/gatkTabComplete/javadoc.options'. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 9.873 secs; ```; Can you please ""rewrite"" the name _Gonalo_? It is always cause of exceptions when building the GATK project.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4434
https://github.com/broadinstitute/gatk/issues/4434:1410,Testability,log,log,1410,"When using the command `./gradlew bundle` to build the GATK project downloaded from GitHub I receive this error:. ```; Starting a Gradle Daemon, 1 incompatible Daemon could not be reused, use --status for details; :createPythonPackageArchive; Creating GATK Python package archive...; Created GATK Python package archive in /datadrive/NGS-SparkGATK/gatk/build/gatkPythonPackageArchive.zip; :compileJava UP-TO-DATE; :processResources; :classes; :gatkTabComplete; /datadrive/NGS-SparkGATK/gatk/build/tmp/expandedArchives/picard-2.17.2-sources.jar_2eu0sptfiz8othzntm7sqhvx4/picard/util/LiftoverUtils.java:332: error: unmappable character for encoding ASCII; * Based on Adrian Tan, Gon??alo R. Abecasis and Hyun Min Kang. (2015); ^; /datadrive/NGS-SparkGATK/gatk/build/tmp/expandedArchives/picard-2.17.2-sources.jar_2eu0sptfiz8othzntm7sqhvx4/picard/util/LiftoverUtils.java:332: error: unmappable character for encoding ASCII; * Based on Adrian Tan, Gon??alo R. Abecasis and Hyun Min Kang. (2015); ^; 2 errors; :gatkTabComplete FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':gatkTabComplete'.; > Javadoc generation failed. Generated Javadoc options file (useful for troubleshooting): '/datadrive/NGS-SparkGATK/gatk/build/tmp/gatkTabComplete/javadoc.options'. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 9.873 secs; ```; Can you please ""rewrite"" the name _Gonalo_? It is always cause of exceptions when building the GATK project.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4434
https://github.com/broadinstitute/gatk/issues/4435:392,Modifiability,variab,variables,392,"Users are supposed to enable/disable async I/O options by passing java option flags to the gatk launch shell script:. `; ./gatk-4.0.1.2/gatk --java-options ""-Dsamjdk.use_async_io_read_samtools=true -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=true"" <Tool and its flags>; `. It appears that no matter how these flags are passed, and even if one hard codes the variables in the actual shell script (change the defaults within the script), they are ignored. **Three examples follow:**. (Irrelevant sections are omitted with "". . ."" in their place for readability) ; (These examples use BaseRecalibrator as an example tool). **Example 1: No Flags Passed (but -Xmx12G)** ; `; ./gatk-4.0.1.2/gatk --java-options ""-Xmx12G"" <Tool specific commands>; `. ```; Using GATK jar /projects/bioinformatics/TestingGATK4/gatk-4.0.1.2/gatk-package-4.0.1.2-local.jar ; Running: ; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Xmx12G -jar /projects/bioinformatics/TestingGATK4/gatk-4.0.1.2/gatk-package-4.0.1.2-local.jar BaseRecalibrator -I /projects/bioinformatics/TestingGATK4/hg38_GATK4/async_options_run/newOutputs/hg38_aligned.sorted.dedupped.bam --known-sites /projects/bioinformatics/DataPacks/human/NA12878_fullsize_bundle_GATK4_copy_2/dbsnp_hg38/dbsnp_146.hg38.vcf -O /projects/bioinformatics/TestingGATK4/hg38_GATK4/async_options_run/newOutputs/recal.table -R /projects/bioinformatics/DataPacks/human/NA12878_fullsize_bundle_GATK4_copy_2/reference_hg38/Homo_sapiens_assembly38.fasta; 10:45:49.790 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/projects/bioinformatics/TestingGATK4/gatk-4.0.1.2/gatk-package-4.0.1.2-local.jar!/com/intel/gkl/native/libgkl_compression.so; 10:45:49.887 INFO BaseRecalibrator - ------------------------------------------------------------ ; 10:45:49.888 INFO BaseRecalibrator - The Genome Analysis Toolkit (GAT",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4435
https://github.com/broadinstitute/gatk/issues/4435:1661,Performance,Load,Loading,1661,"mple 1: No Flags Passed (but -Xmx12G)** ; `; ./gatk-4.0.1.2/gatk --java-options ""-Xmx12G"" <Tool specific commands>; `. ```; Using GATK jar /projects/bioinformatics/TestingGATK4/gatk-4.0.1.2/gatk-package-4.0.1.2-local.jar ; Running: ; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Xmx12G -jar /projects/bioinformatics/TestingGATK4/gatk-4.0.1.2/gatk-package-4.0.1.2-local.jar BaseRecalibrator -I /projects/bioinformatics/TestingGATK4/hg38_GATK4/async_options_run/newOutputs/hg38_aligned.sorted.dedupped.bam --known-sites /projects/bioinformatics/DataPacks/human/NA12878_fullsize_bundle_GATK4_copy_2/dbsnp_hg38/dbsnp_146.hg38.vcf -O /projects/bioinformatics/TestingGATK4/hg38_GATK4/async_options_run/newOutputs/recal.table -R /projects/bioinformatics/DataPacks/human/NA12878_fullsize_bundle_GATK4_copy_2/reference_hg38/Homo_sapiens_assembly38.fasta; 10:45:49.790 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/projects/bioinformatics/TestingGATK4/gatk-4.0.1.2/gatk-package-4.0.1.2-local.jar!/com/intel/gkl/native/libgkl_compression.so; 10:45:49.887 INFO BaseRecalibrator - ------------------------------------------------------------ ; 10:45:49.888 INFO BaseRecalibrator - The Genome Analysis Toolkit (GATK) v4.0.1.2 ; . . .; 10:45:49.889 INFO BaseRecalibrator - HTSJDK Version: 2.14.1 ; 10:45:49.889 INFO BaseRecalibrator - Picard Version: 2.17.2 ; 10:45:49.889 INFO BaseRecalibrator - HTSJDK Defaults.COMPRESSION_LEVEL : 1 ; 10:45:49.889 INFO BaseRecalibrator - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false ; 10:45:49.889 INFO BaseRecalibrator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true ; 10:45:49.889 INFO BaseRecalibrator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false ; ```. **Result:** No flags were passed, so the default values are passed to java at runtime (seen in the second line). **Example 2: Flags Passed (Along wit",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4435
https://github.com/broadinstitute/gatk/issues/4435:3881,Performance,Load,Loading,3881,"Using GATK jar /projects/bioinformatics/TestingGATK4/gatk-4.0.1.2/gatk-package-4.0.1.2-local.jar ; Running: ; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Xmx12G -Dsamjdk.use_async_io_read_samtools=true -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=true -jar /projects/bioinformatics/TestingGATK4/gatk-4.0.1.2/gatk-package-4.0.1.2-local.jar BaseRecalibrator -I /projects/bioinformatics/TestingGATK4/hg38_GATK4/async_options_run/newOutputs/hg38_aligned.sorted.dedupped.bam --known-sites /projects/bioinformatics/DataPacks/human/NA12878_fullsize_bundle_GATK4_copy_2/dbsnp_hg38/dbsnp_146.hg38.vcf -O /projects/bioinformatics/TestingGATK4/hg38_GATK4/async_options_run/newOutputs/recal.table -R /projects/bioinformatics/DataPacks/human/NA12878_fullsize_bundle_GATK4_copy_2/reference_hg38/Homo_sapiens_assembly38.fasta; 11:13:22.175 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/projects/bioinformatics/TestingGATK4/gatk-4.0.1.2/gatk-package-4.0.1.2-local.jar!/com/intel/gkl/native/libgkl_compression.so; 11:13:22.277 INFO BaseRecalibrator - ------------------------------------------------------------ ; 11:13:22.277 INFO BaseRecalibrator - The Genome Analysis Toolkit (GATK) v4.0.1.2 ; . . .; 11:13:22.278 INFO BaseRecalibrator - HTSJDK Version: 2.14.1 ; 11:13:22.278 INFO BaseRecalibrator - Picard Version: 2.17.2 ; 11:13:22.278 INFO BaseRecalibrator - HTSJDK Defaults.COMPRESSION_LEVEL : 1 ; 11:13:22.278 INFO BaseRecalibrator - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false ; 11:13:22.278 INFO BaseRecalibrator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true ; 11:13:22.278 INFO BaseRecalibrator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; ```; **Result:** The flags are passed to java, but only after the default flags are also passed. Regardless, the actual behavior of the tool is unchanged (",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4435
https://github.com/broadinstitute/gatk/issues/4435:6189,Performance,Load,Loading,6189,"e"", ; ""-Dsamjdk.use_async_io_write_tribble=true"", ; ""-Dsamjdk.compression_level=1""]; . . .; ```; `; ./gatk-4.0.1.2/gatk --java-options ""-Xmx12G"" <Tool specific commands>; `; ```; Using GATK jar /projects/bioinformatics/TestingGATK4/gatk-4.0.1.2/gatk-package-4.0.1.2-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=true -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=true -Dsamjdk.compression_level=1 -Xmx12G -jar /projects/bioinformatics/TestingGATK4/gatk-4.0.1.2/gatk-package-4.0.1.2-local.jar BaseRecalibrator -I /projects/bioinformatics/TestingGATK4/hg38_GATK4/async_options_run/newOutputs/hg38_aligned.sorted.dedupped.bam --known-sites /projects/bioinformatics/DataPacks/human/NA12878_fullsize_bundle_GATK4_copy_2/dbsnp_hg38/dbsnp_146.hg38.vcf -O /projects/bioinformatics/TestingGATK4/hg38_GATK4/async_options_run/newOutputs/recal.table -R /projects/bioinformatics/DataPacks/human/NA12878_fullsize_bundle_GATK4_copy_2/reference_hg38/Homo_sapiens_assembly38.fasta; 11:23:11.500 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/projects/bioinformatics/TestingGATK4/gatk-4.0.1.2/gatk-package-4.0.1.2-local.jar!/com/intel/gkl/native/libgkl_compression.so; 11:23:11.637 INFO BaseRecalibrator - ------------------------------------------------------------; 11:23:11.637 INFO BaseRecalibrator - The Genome Analysis Toolkit (GATK) v4.0.1.2; . . .; 11:23:11.638 INFO BaseRecalibrator - HTSJDK Version: 2.14.1; 11:23:11.639 INFO BaseRecalibrator - Picard Version: 2.17.2; 11:23:11.639 INFO BaseRecalibrator - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 11:23:11.639 INFO BaseRecalibrator - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 11:23:11.639 INFO BaseRecalibrator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 11:23:11.639 INFO BaseRecalibrator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; ```; **Result:** Now the changed values show up and are passed to Java, but the tool still runs in the same way",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4435
https://github.com/broadinstitute/gatk/issues/4436:282,Deployability,release,release,282,Parsing the GATK config file currently overrides any command-line specified config options for system-level parameters. Options explicitly specified on the command-line should override what is in the config file. The `GATKConfig.properties` file is missing from the packaged binary release (as created by`gradle bundle`). Gradle must be updated to include it.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4436
https://github.com/broadinstitute/gatk/issues/4436:337,Deployability,update,updated,337,Parsing the GATK config file currently overrides any command-line specified config options for system-level parameters. Options explicitly specified on the command-line should override what is in the config file. The `GATKConfig.properties` file is missing from the packaged binary release (as created by`gradle bundle`). Gradle must be updated to include it.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4436
https://github.com/broadinstitute/gatk/issues/4436:17,Modifiability,config,config,17,Parsing the GATK config file currently overrides any command-line specified config options for system-level parameters. Options explicitly specified on the command-line should override what is in the config file. The `GATKConfig.properties` file is missing from the packaged binary release (as created by`gradle bundle`). Gradle must be updated to include it.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4436
https://github.com/broadinstitute/gatk/issues/4436:76,Modifiability,config,config,76,Parsing the GATK config file currently overrides any command-line specified config options for system-level parameters. Options explicitly specified on the command-line should override what is in the config file. The `GATKConfig.properties` file is missing from the packaged binary release (as created by`gradle bundle`). Gradle must be updated to include it.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4436
https://github.com/broadinstitute/gatk/issues/4436:200,Modifiability,config,config,200,Parsing the GATK config file currently overrides any command-line specified config options for system-level parameters. Options explicitly specified on the command-line should override what is in the config file. The `GATKConfig.properties` file is missing from the packaged binary release (as created by`gradle bundle`). Gradle must be updated to include it.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4436
https://github.com/broadinstitute/gatk/pull/4438:82,Modifiability,extend,extend,82,"This PR attempts to eliminate long-running, useless assemblies that significantly extend runtime on some samples:. - Conducts a scan over the genome to find intervals of excessive depth, defined as an interval where coverage is greater than a lower factor times the average coverage of the sample and containing a coverage peak greater than an upper factor times the average coverage.; - Nearby high-coverage regions within one read-length of each other are merged together.; - Excludes reads that map exclusively inside high coverage regions from evidence gathering.; - Excludes reads that map exclusively inside high coverage regions from QName finding for seeding assemblies. In addition, after observing that many long-running assemblies occur on non-primary reference contigs, we also exclude reads that map to non-primary contigs (as defined by the ""cross-contig to ignore set"") from evidence gathering. Runtime on the CHM mix sample with this change is approximately 38 minutes, and our NA19238 snapshot now takes only 22 minutes, a significant drop in runtime. There are a few changes in the resulting call set but they appear to be minimal.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4438
https://github.com/broadinstitute/gatk/issues/4439:391,Safety,avoid,avoid-the-overlap-of-adjacent-intervals,391,"## Feature request. ### Tool(s) involved; CollectHsMetrics and perhaps all Picard tools. ### Description; When running on multiple intervals that are adjacent, the tool merges the intervals. Users would like an option to not have them merged. . This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/11439/gatk4-collecthsmetrics-how-to-avoid-the-overlap-of-adjacent-intervals/p1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4439
https://github.com/broadinstitute/gatk/issues/4440:245,Availability,down,downstream,245,"VariantEval compares input VCF(s) to comparison tracks(s). One can execute the tool without providing comparison tracks. In GATK3, when no comps are provided, the tool creates a dummy RODBinding, not backed by a file. This is passed through the downstream code in the tool. In GATK4, RODBinding has become FeatureInput. Is there a way to replicate this dummy FeatureInput behavior in GATK4? I dont see a way to do this in GATK4, since FeatureInput needs a valid filepath. I think I can rework VariantEval to come up w/ a different solution than this dummy binding; however, thought I'd ask here first.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4440
https://github.com/broadinstitute/gatk/issues/4447:552,Usability,simpl,simply,552,"I am looking into porting VariantEval to GATK4. In GATK3, this used RodWalker, which basically iterated all covered sites, driven by a ROD file. A VCF is typically a pretty sparse file, so you end up with large stretches of non-covered positions of the genome. In GATK3, RodWalker / AlignmentContext explicitly tracked getSkippedBases(), which seems designed to allow tools to track stretches of REF sites. VariantEval in GATK3 relied on AlignmentContext.getSkippedBases() to count the number of loci it processed, for example. In GATK4, VariantWalker simply iterates over the sites in the VCF. Am I missing an analog, and/or is there a design reason why GATK4 does not track something akin to GATK3's getSkippedBases()?. I dont fully know the implications for parallelization, but if VariantWalker tracked something like lastProcessedSite, then this would effectively provide the same information as GATK3's AlignmentContext.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4447
https://github.com/broadinstitute/gatk/issues/4448:146,Availability,reliab,reliable,146,"Is there a way to specify the number of threads or cores to use ? Specifically for HaplotypeCaller ?; In GATK3, we had -nt and -nct (more of less reliable for what I read) but it doesn't seems to be here anymore.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4448
https://github.com/broadinstitute/gatk/issues/4450:312,Testability,test,tests,312,There were a number of methods that got ported in VariantAnnotator to make the RankSumTest output match the gatk3 output when computed on variants based on the pileup. Most of this code is buggy and in many cases simply wrong. We should reevaluate this code and fix it to do the correct thing and write stronger tests for the changed behavior in VariantAnnotator. . Specifically the methods `getNumClippedBasesAtEnd`/`getNumClippedBasesAtStart` make incorrect assumptions about how clipping works.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4450
https://github.com/broadinstitute/gatk/issues/4450:213,Usability,simpl,simply,213,There were a number of methods that got ported in VariantAnnotator to make the RankSumTest output match the gatk3 output when computed on variants based on the pileup. Most of this code is buggy and in many cases simply wrong. We should reevaluate this code and fix it to do the correct thing and write stronger tests for the changed behavior in VariantAnnotator. . Specifically the methods `getNumClippedBasesAtEnd`/`getNumClippedBasesAtStart` make incorrect assumptions about how clipping works.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4450
https://github.com/broadinstitute/gatk/pull/4451:67,Energy Efficiency,efficient,efficient,67,"* some classes were missing registration in kryo which causes less efficient serialization; * adding registrations for a number of classes that MarkDuplicatesSpark needs that weren't registered yet. * notably, BAMRecord wasn't registered to use the correct serializer which could cause major inefficiencies; * it's not clear what circumstances we're serializing BAMRecord instead of SAMRecordToGATKReadAdapter so how much this will help is not obvious",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4451
https://github.com/broadinstitute/gatk/pull/4451:319,Usability,clear,clear,319,"* some classes were missing registration in kryo which causes less efficient serialization; * adding registrations for a number of classes that MarkDuplicatesSpark needs that weren't registered yet. * notably, BAMRecord wasn't registered to use the correct serializer which could cause major inefficiencies; * it's not clear what circumstances we're serializing BAMRecord instead of SAMRecordToGATKReadAdapter so how much this will help is not obvious",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4451
https://github.com/broadinstitute/gatk/issues/4452:116,Availability,avail,available,116,"In keeping concordance with gatk3, when computing the RankSumAnnotation for pileups when the likelihoods map is not available we attempt to match the bases of the reads to the allele in the variant context. This is problematic if the variant is not a snp. For most other annotations we explicitly only compute the annotation for SNPs in this case, but in order to mimic gatk3 behavior it will still attempt to compute the rank sum over indels. This should be evaluated and changed. . Related to #4450, #3803",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4452
https://github.com/broadinstitute/gatk/issues/4453:28,Testability,test,test,28,"I noticed that in the GATK3 test data, the dictionary built into the test file noGenotypes.vcf and human_g1k_v37.dict do not line up. They're close, but the VCF's header dictionary has hs37d5 and NC_007605. On both GATK3 and GATK4 this seems to be allowed. Should it?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4453
https://github.com/broadinstitute/gatk/issues/4453:69,Testability,test,test,69,"I noticed that in the GATK3 test data, the dictionary built into the test file noGenotypes.vcf and human_g1k_v37.dict do not line up. They're close, but the VCF's header dictionary has hs37d5 and NC_007605. On both GATK3 and GATK4 this seems to be allowed. Should it?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4453
https://github.com/broadinstitute/gatk/pull/4455:85,Deployability,patch,patch,85,"The -contamination argument was not hooked up properly in the HaplotypeCaller.; This patch fixes the tool argument, and adds tests on artificially contaminated; data to demonstrate that the feature works as intended. Resolves #4312",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4455
https://github.com/broadinstitute/gatk/pull/4455:125,Testability,test,tests,125,"The -contamination argument was not hooked up properly in the HaplotypeCaller.; This patch fixes the tool argument, and adds tests on artificially contaminated; data to demonstrate that the feature works as intended. Resolves #4312",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4455
https://github.com/broadinstitute/gatk/issues/4456:984,Deployability,update,update,984,"I'm trying to figure out the best way to replicate GATK3 behavior in GATK4. The GATK4 VariantWalker iterates all variants from VCF(s), calling apply() once per variant. If the input VCFs has duplicates at a given location, apply() is called multiple times for the same locus. In GATK3, VariantEval iterates each locus, and generates a list of variants at that site. I'm trying to figure out the most efficient way to do this in GATK4. One solution is to override traverse(), and add some kind of groupingBy step, for example:. StreamSupport.stream(getSpliteratorForDrivingVariants(), false); .filter(variantfilter); .collect(Collectors.groupingBy(x -> new SimpleInterval(x))); .values(); .forEach(variantList -> {; final SimpleInterval variantInterval = new SimpleInterval(variantList.get(0));; apply(variantList,; new ReadsContext(reads, variantInterval, readFilter),; new ReferenceContext(reference, variantInterval),; new FeatureContext(features, variantInterval));. progressMeter.update(variantInterval);; });. This will get me the right end result (like of variants per site); however, it's not clear to me if this is the most efficient route, and I'm not sure if it's aware of the sorted input. Because the input data are sorted, I could iterate, track the previous location and maintain a list of track variants per site. Each time we hit a new location I call apply() with that list. Are the places in GATK4 that already do this type of per-locus grouping?. Thanks.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4456
https://github.com/broadinstitute/gatk/issues/4456:400,Energy Efficiency,efficient,efficient,400,"I'm trying to figure out the best way to replicate GATK3 behavior in GATK4. The GATK4 VariantWalker iterates all variants from VCF(s), calling apply() once per variant. If the input VCFs has duplicates at a given location, apply() is called multiple times for the same locus. In GATK3, VariantEval iterates each locus, and generates a list of variants at that site. I'm trying to figure out the most efficient way to do this in GATK4. One solution is to override traverse(), and add some kind of groupingBy step, for example:. StreamSupport.stream(getSpliteratorForDrivingVariants(), false); .filter(variantfilter); .collect(Collectors.groupingBy(x -> new SimpleInterval(x))); .values(); .forEach(variantList -> {; final SimpleInterval variantInterval = new SimpleInterval(variantList.get(0));; apply(variantList,; new ReadsContext(reads, variantInterval, readFilter),; new ReferenceContext(reference, variantInterval),; new FeatureContext(features, variantInterval));. progressMeter.update(variantInterval);; });. This will get me the right end result (like of variants per site); however, it's not clear to me if this is the most efficient route, and I'm not sure if it's aware of the sorted input. Because the input data are sorted, I could iterate, track the previous location and maintain a list of track variants per site. Each time we hit a new location I call apply() with that list. Are the places in GATK4 that already do this type of per-locus grouping?. Thanks.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4456
https://github.com/broadinstitute/gatk/issues/4456:1132,Energy Efficiency,efficient,efficient,1132,"I'm trying to figure out the best way to replicate GATK3 behavior in GATK4. The GATK4 VariantWalker iterates all variants from VCF(s), calling apply() once per variant. If the input VCFs has duplicates at a given location, apply() is called multiple times for the same locus. In GATK3, VariantEval iterates each locus, and generates a list of variants at that site. I'm trying to figure out the most efficient way to do this in GATK4. One solution is to override traverse(), and add some kind of groupingBy step, for example:. StreamSupport.stream(getSpliteratorForDrivingVariants(), false); .filter(variantfilter); .collect(Collectors.groupingBy(x -> new SimpleInterval(x))); .values(); .forEach(variantList -> {; final SimpleInterval variantInterval = new SimpleInterval(variantList.get(0));; apply(variantList,; new ReadsContext(reads, variantInterval, readFilter),; new ReferenceContext(reference, variantInterval),; new FeatureContext(features, variantInterval));. progressMeter.update(variantInterval);; });. This will get me the right end result (like of variants per site); however, it's not clear to me if this is the most efficient route, and I'm not sure if it's aware of the sorted input. Because the input data are sorted, I could iterate, track the previous location and maintain a list of track variants per site. Each time we hit a new location I call apply() with that list. Are the places in GATK4 that already do this type of per-locus grouping?. Thanks.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4456
https://github.com/broadinstitute/gatk/issues/4456:1142,Integrability,rout,route,1142,"I'm trying to figure out the best way to replicate GATK3 behavior in GATK4. The GATK4 VariantWalker iterates all variants from VCF(s), calling apply() once per variant. If the input VCFs has duplicates at a given location, apply() is called multiple times for the same locus. In GATK3, VariantEval iterates each locus, and generates a list of variants at that site. I'm trying to figure out the most efficient way to do this in GATK4. One solution is to override traverse(), and add some kind of groupingBy step, for example:. StreamSupport.stream(getSpliteratorForDrivingVariants(), false); .filter(variantfilter); .collect(Collectors.groupingBy(x -> new SimpleInterval(x))); .values(); .forEach(variantList -> {; final SimpleInterval variantInterval = new SimpleInterval(variantList.get(0));; apply(variantList,; new ReadsContext(reads, variantInterval, readFilter),; new ReferenceContext(reference, variantInterval),; new FeatureContext(features, variantInterval));. progressMeter.update(variantInterval);; });. This will get me the right end result (like of variants per site); however, it's not clear to me if this is the most efficient route, and I'm not sure if it's aware of the sorted input. Because the input data are sorted, I could iterate, track the previous location and maintain a list of track variants per site. Each time we hit a new location I call apply() with that list. Are the places in GATK4 that already do this type of per-locus grouping?. Thanks.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4456
https://github.com/broadinstitute/gatk/issues/4456:656,Usability,Simpl,SimpleInterval,656,"I'm trying to figure out the best way to replicate GATK3 behavior in GATK4. The GATK4 VariantWalker iterates all variants from VCF(s), calling apply() once per variant. If the input VCFs has duplicates at a given location, apply() is called multiple times for the same locus. In GATK3, VariantEval iterates each locus, and generates a list of variants at that site. I'm trying to figure out the most efficient way to do this in GATK4. One solution is to override traverse(), and add some kind of groupingBy step, for example:. StreamSupport.stream(getSpliteratorForDrivingVariants(), false); .filter(variantfilter); .collect(Collectors.groupingBy(x -> new SimpleInterval(x))); .values(); .forEach(variantList -> {; final SimpleInterval variantInterval = new SimpleInterval(variantList.get(0));; apply(variantList,; new ReadsContext(reads, variantInterval, readFilter),; new ReferenceContext(reference, variantInterval),; new FeatureContext(features, variantInterval));. progressMeter.update(variantInterval);; });. This will get me the right end result (like of variants per site); however, it's not clear to me if this is the most efficient route, and I'm not sure if it's aware of the sorted input. Because the input data are sorted, I could iterate, track the previous location and maintain a list of track variants per site. Each time we hit a new location I call apply() with that list. Are the places in GATK4 that already do this type of per-locus grouping?. Thanks.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4456
https://github.com/broadinstitute/gatk/issues/4456:721,Usability,Simpl,SimpleInterval,721,"I'm trying to figure out the best way to replicate GATK3 behavior in GATK4. The GATK4 VariantWalker iterates all variants from VCF(s), calling apply() once per variant. If the input VCFs has duplicates at a given location, apply() is called multiple times for the same locus. In GATK3, VariantEval iterates each locus, and generates a list of variants at that site. I'm trying to figure out the most efficient way to do this in GATK4. One solution is to override traverse(), and add some kind of groupingBy step, for example:. StreamSupport.stream(getSpliteratorForDrivingVariants(), false); .filter(variantfilter); .collect(Collectors.groupingBy(x -> new SimpleInterval(x))); .values(); .forEach(variantList -> {; final SimpleInterval variantInterval = new SimpleInterval(variantList.get(0));; apply(variantList,; new ReadsContext(reads, variantInterval, readFilter),; new ReferenceContext(reference, variantInterval),; new FeatureContext(features, variantInterval));. progressMeter.update(variantInterval);; });. This will get me the right end result (like of variants per site); however, it's not clear to me if this is the most efficient route, and I'm not sure if it's aware of the sorted input. Because the input data are sorted, I could iterate, track the previous location and maintain a list of track variants per site. Each time we hit a new location I call apply() with that list. Are the places in GATK4 that already do this type of per-locus grouping?. Thanks.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4456
https://github.com/broadinstitute/gatk/issues/4456:758,Usability,Simpl,SimpleInterval,758,"I'm trying to figure out the best way to replicate GATK3 behavior in GATK4. The GATK4 VariantWalker iterates all variants from VCF(s), calling apply() once per variant. If the input VCFs has duplicates at a given location, apply() is called multiple times for the same locus. In GATK3, VariantEval iterates each locus, and generates a list of variants at that site. I'm trying to figure out the most efficient way to do this in GATK4. One solution is to override traverse(), and add some kind of groupingBy step, for example:. StreamSupport.stream(getSpliteratorForDrivingVariants(), false); .filter(variantfilter); .collect(Collectors.groupingBy(x -> new SimpleInterval(x))); .values(); .forEach(variantList -> {; final SimpleInterval variantInterval = new SimpleInterval(variantList.get(0));; apply(variantList,; new ReadsContext(reads, variantInterval, readFilter),; new ReferenceContext(reference, variantInterval),; new FeatureContext(features, variantInterval));. progressMeter.update(variantInterval);; });. This will get me the right end result (like of variants per site); however, it's not clear to me if this is the most efficient route, and I'm not sure if it's aware of the sorted input. Because the input data are sorted, I could iterate, track the previous location and maintain a list of track variants per site. Each time we hit a new location I call apply() with that list. Are the places in GATK4 that already do this type of per-locus grouping?. Thanks.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4456
https://github.com/broadinstitute/gatk/issues/4456:1100,Usability,clear,clear,1100,"I'm trying to figure out the best way to replicate GATK3 behavior in GATK4. The GATK4 VariantWalker iterates all variants from VCF(s), calling apply() once per variant. If the input VCFs has duplicates at a given location, apply() is called multiple times for the same locus. In GATK3, VariantEval iterates each locus, and generates a list of variants at that site. I'm trying to figure out the most efficient way to do this in GATK4. One solution is to override traverse(), and add some kind of groupingBy step, for example:. StreamSupport.stream(getSpliteratorForDrivingVariants(), false); .filter(variantfilter); .collect(Collectors.groupingBy(x -> new SimpleInterval(x))); .values(); .forEach(variantList -> {; final SimpleInterval variantInterval = new SimpleInterval(variantList.get(0));; apply(variantList,; new ReadsContext(reads, variantInterval, readFilter),; new ReferenceContext(reference, variantInterval),; new FeatureContext(features, variantInterval));. progressMeter.update(variantInterval);; });. This will get me the right end result (like of variants per site); however, it's not clear to me if this is the most efficient route, and I'm not sure if it's aware of the sorted input. Because the input data are sorted, I could iterate, track the previous location and maintain a list of track variants per site. Each time we hit a new location I call apply() with that list. Are the places in GATK4 that already do this type of per-locus grouping?. Thanks.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4456
https://github.com/broadinstitute/gatk/issues/4457:1972,Performance,Load,Loading,1972,"+00 --active_class_padding_hybrid_mode=50000 --enable_bias_factors=True --disable_bias_factors_in_active_class=False --p_alt=1.000000e-06 --cnv_coherence_length=1.000000e+04 --max_copy_number=5 --p_active=0.010000 --class_coherence_length=10000.000000 --learning_rate=5.000000e-02 --adamax_beta1=9.000000e-01 --adamax_beta2=9.900000e-01 --log_emission_samples_per_round=50 --log_emission_sampling_rounds=10 --log_emission_sampling_median_rel_error=5.000000e-03 --max_advi_iter_first_epoch=100 --max_advi_iter_subsequent_epochs=100 --min_training_epochs=10 --max_training_epochs=50 --initial_temperature=2.000000e+00 --num_thermal_epochs=20 --convergence_snr_averaging_window=500 --convergence_snr_trigger_threshold=1.000000e-01 --convergence_snr_countdown_window=10 --max_calling_iters=10 --caller_update_convergence_threshold=1.000000e-03 --caller_admixing_rate=7.500000e-01 --disable_caller=false --disable_sampler=false --disable_annealing=false; Stdout: 10:35:09.182 INFO cohort_denoising_calling - Loading 4 read counts file(s)...; 10:35:12.176 INFO gcnvkernel.io.io_metadata - Loading germline contig ploidy and global read depth metadata...; sample_names. Stderr: Traceback (most recent call last):; File ""/tmp/wujh/cohort_denoising_calling.7794651839449939395.py"", line 114, in <module>; n_st, sample_names, sample_metadata_collection); File ""/opt/NfsDir/BioDir/Anaconda3/lib/python3.6/site-packages/gcnvkernel/models/model_denoising_calling.py"", line 379, in __init__; sample_metadata_collection, sample_names, self.contig_list); File ""/opt/NfsDir/BioDir/Anaconda3/lib/python3.6/site-packages/gcnvkernel/models/model_denoising_calling.py"", line 586, in _get_baseline_copy_number_and_read_depth; ""Some samples do not have read depth metadata""; AssertionError: Some samples do not have read depth metadata. at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75); at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCura",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4457
https://github.com/broadinstitute/gatk/issues/4457:2052,Performance,Load,Loading,2052,"ass=False --p_alt=1.000000e-06 --cnv_coherence_length=1.000000e+04 --max_copy_number=5 --p_active=0.010000 --class_coherence_length=10000.000000 --learning_rate=5.000000e-02 --adamax_beta1=9.000000e-01 --adamax_beta2=9.900000e-01 --log_emission_samples_per_round=50 --log_emission_sampling_rounds=10 --log_emission_sampling_median_rel_error=5.000000e-03 --max_advi_iter_first_epoch=100 --max_advi_iter_subsequent_epochs=100 --min_training_epochs=10 --max_training_epochs=50 --initial_temperature=2.000000e+00 --num_thermal_epochs=20 --convergence_snr_averaging_window=500 --convergence_snr_trigger_threshold=1.000000e-01 --convergence_snr_countdown_window=10 --max_calling_iters=10 --caller_update_convergence_threshold=1.000000e-03 --caller_admixing_rate=7.500000e-01 --disable_caller=false --disable_sampler=false --disable_annealing=false; Stdout: 10:35:09.182 INFO cohort_denoising_calling - Loading 4 read counts file(s)...; 10:35:12.176 INFO gcnvkernel.io.io_metadata - Loading germline contig ploidy and global read depth metadata...; sample_names. Stderr: Traceback (most recent call last):; File ""/tmp/wujh/cohort_denoising_calling.7794651839449939395.py"", line 114, in <module>; n_st, sample_names, sample_metadata_collection); File ""/opt/NfsDir/BioDir/Anaconda3/lib/python3.6/site-packages/gcnvkernel/models/model_denoising_calling.py"", line 379, in __init__; sample_metadata_collection, sample_names, self.contig_list); File ""/opt/NfsDir/BioDir/Anaconda3/lib/python3.6/site-packages/gcnvkernel/models/model_denoising_calling.py"", line 586, in _get_baseline_copy_number_and_read_depth; ""Some samples do not have read depth metadata""; AssertionError: Some samples do not have read depth metadata. at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75); at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCuratedArgs(ScriptExecutor.java:126); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.execut",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4457
https://github.com/broadinstitute/gatk/issues/4457:2721,Testability,Assert,AssertionError,2721,"--convergence_snr_countdown_window=10 --max_calling_iters=10 --caller_update_convergence_threshold=1.000000e-03 --caller_admixing_rate=7.500000e-01 --disable_caller=false --disable_sampler=false --disable_annealing=false; Stdout: 10:35:09.182 INFO cohort_denoising_calling - Loading 4 read counts file(s)...; 10:35:12.176 INFO gcnvkernel.io.io_metadata - Loading germline contig ploidy and global read depth metadata...; sample_names. Stderr: Traceback (most recent call last):; File ""/tmp/wujh/cohort_denoising_calling.7794651839449939395.py"", line 114, in <module>; n_st, sample_names, sample_metadata_collection); File ""/opt/NfsDir/BioDir/Anaconda3/lib/python3.6/site-packages/gcnvkernel/models/model_denoising_calling.py"", line 379, in __init__; sample_metadata_collection, sample_names, self.contig_list); File ""/opt/NfsDir/BioDir/Anaconda3/lib/python3.6/site-packages/gcnvkernel/models/model_denoising_calling.py"", line 586, in _get_baseline_copy_number_and_read_depth; ""Some samples do not have read depth metadata""; AssertionError: Some samples do not have read depth metadata. at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75); at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCuratedArgs(ScriptExecutor.java:126); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeArgs(PythonScriptExecutor.java:170); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeScript(PythonScriptExecutor.java:151); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeScript(PythonScriptExecutor.java:121); at org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller.executeGermlineCNVCallerPythonScript(GermlineCNVCaller.java:416); at org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller.doWork(GermlineCNVCaller.java:255); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:136); at org.broadinstitute.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4457
https://github.com/broadinstitute/gatk/issues/4458:4474,Availability,down,down,4474,"validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations$BreakpointsInference.getLeftJustifiedBreakpoints(NovelAdjacencyReferenceLocations.java:86); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.leftJustifyBreakpoints(NovelAdjacencyReferenceLocations.java:301); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.<init>(NovelAdjacencyReferenceLocations.java:46); ... 18 more. 8/02/23 23:06:24 INFO storage.BlockManagerMaster: BlockManagerMaster stopped; 18/02/23 23:06:24 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/02/23 23:06:24 INFO spark.SparkContext: Successfully stopped SparkContext; 23:06:24.240 INFO StructuralVariationDiscoveryPipelineSpark - Shutting down engine; [February 23, 2018 11:06:24 PM EST] org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDiscoveryPipelineSpark done. Elapsed time: 73.92 minutes.; Runtime.totalMemory()=10458497024; org.apache.spark.SparkException: Job aborted due to stage failure: Task 27 in stage 15.0 failed 4 times, most recent failure: Lost task 27.3 in stage 15.0 (TID 29483, scc-q15.scc.bu.edu, executor 13): org.broadinstitute.hellbender.exc eptions.GATKException: Erred when inferring breakpoint location and event type from chimeric alignment:; asm010450:tig00000 1_189_chrUn_JTFH01000312v1_decoy:663-851_-_189M512H_60_8_149_O 153_701_chrUn_JTFH01000312v1_decoy:1-549_+_152S549M_60_0_549_O; at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.<init>(NovelAdjacencyReferenceLocations.java:51); at org.broadinstitute.hellbender.tools.spark.sv.discovery.DiscoverVariantsFromContigAlignmentsSAMSpark.lambda$null$0(DiscoverVariantsFromContigAlignment",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458
https://github.com/broadinstitute/gatk/issues/4458:4739,Availability,failure,failure,4739,"iedBreakpoints(NovelAdjacencyReferenceLocations.java:86); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.leftJustifyBreakpoints(NovelAdjacencyReferenceLocations.java:301); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.<init>(NovelAdjacencyReferenceLocations.java:46); ... 18 more. 8/02/23 23:06:24 INFO storage.BlockManagerMaster: BlockManagerMaster stopped; 18/02/23 23:06:24 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/02/23 23:06:24 INFO spark.SparkContext: Successfully stopped SparkContext; 23:06:24.240 INFO StructuralVariationDiscoveryPipelineSpark - Shutting down engine; [February 23, 2018 11:06:24 PM EST] org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDiscoveryPipelineSpark done. Elapsed time: 73.92 minutes.; Runtime.totalMemory()=10458497024; org.apache.spark.SparkException: Job aborted due to stage failure: Task 27 in stage 15.0 failed 4 times, most recent failure: Lost task 27.3 in stage 15.0 (TID 29483, scc-q15.scc.bu.edu, executor 13): org.broadinstitute.hellbender.exc eptions.GATKException: Erred when inferring breakpoint location and event type from chimeric alignment:; asm010450:tig00000 1_189_chrUn_JTFH01000312v1_decoy:663-851_-_189M512H_60_8_149_O 153_701_chrUn_JTFH01000312v1_decoy:1-549_+_152S549M_60_0_549_O; at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.<init>(NovelAdjacencyReferenceLocations.java:51); at org.broadinstitute.hellbender.tools.spark.sv.discovery.DiscoverVariantsFromContigAlignmentsSAMSpark.lambda$null$0(DiscoverVariantsFromContigAlignmentsSAMSpark.java:175); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.ArrayList$ArrayListSpliterator.tryAdvance(ArrayList.java:1351); at java.util.stream.StreamSpliterators$WrappingSpliterator.lambda$initPartialTraversalS",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458
https://github.com/broadinstitute/gatk/issues/4458:4798,Availability,failure,failure,4798,"org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.leftJustifyBreakpoints(NovelAdjacencyReferenceLocations.java:301); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.<init>(NovelAdjacencyReferenceLocations.java:46); ... 18 more. 8/02/23 23:06:24 INFO storage.BlockManagerMaster: BlockManagerMaster stopped; 18/02/23 23:06:24 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/02/23 23:06:24 INFO spark.SparkContext: Successfully stopped SparkContext; 23:06:24.240 INFO StructuralVariationDiscoveryPipelineSpark - Shutting down engine; [February 23, 2018 11:06:24 PM EST] org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDiscoveryPipelineSpark done. Elapsed time: 73.92 minutes.; Runtime.totalMemory()=10458497024; org.apache.spark.SparkException: Job aborted due to stage failure: Task 27 in stage 15.0 failed 4 times, most recent failure: Lost task 27.3 in stage 15.0 (TID 29483, scc-q15.scc.bu.edu, executor 13): org.broadinstitute.hellbender.exc eptions.GATKException: Erred when inferring breakpoint location and event type from chimeric alignment:; asm010450:tig00000 1_189_chrUn_JTFH01000312v1_decoy:663-851_-_189M512H_60_8_149_O 153_701_chrUn_JTFH01000312v1_decoy:1-549_+_152S549M_60_0_549_O; at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.<init>(NovelAdjacencyReferenceLocations.java:51); at org.broadinstitute.hellbender.tools.spark.sv.discovery.DiscoverVariantsFromContigAlignmentsSAMSpark.lambda$null$0(DiscoverVariantsFromContigAlignmentsSAMSpark.java:175); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.ArrayList$ArrayListSpliterator.tryAdvance(ArrayList.java:1351); at java.util.stream.StreamSpliterators$WrappingSpliterator.lambda$initPartialTraversalState$0(StreamSpliterators.java:294); at java.util.stream.Stre",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458
https://github.com/broadinstitute/gatk/issues/4458:127,Deployability,pipeline,pipeline,127,"When running StructuralVariationDiscoveryPipelineSpark (GATK 4.0.1.1) on a hadoop cluster, the following exception occurs. The pipeline has been running fine on other cram files. **Caused by: java.lang.IllegalArgumentException: Invalid interval. Contig:chrUn_JTFH01000312v1_decoy start:0 end:0**. Below is the stack and other details. . The pipeline looks like it is running through all the contigs and is not limited to chr1-chr22, ChrY, ChrX and ChrM. Would it help running the software if the extra 3000+ contig names in the GRCh38 reference are excluded? If so, what is the best way to exclude processing the extra contigs?. ```; 18/02/23 23:06:22 INFO scheduler.TaskSetManager: Finished task 24.0 in stage 15.0 (TID 29435) in 2906 ms on scc-q04.scc.bu.edu (executor 1) (48/70); 18/02/23 23:06:23 INFO scheduler.TaskSetManager: Finished task 52.0 in stage 15.0 (TID 29463) in 2354 ms on scc-q04.scc.bu.edu (executor 1) (49/70); 18/02/23 23:06:23 INFO scheduler.TaskSetManager: Finished task 37.0 in stage 15.0 (TID 29448) in 2653 ms on scc-q03.scc.bu.edu (executor 6) (50/70); 18/02/23 23:06:23 WARN scheduler.TaskSetManager: Lost task 27.0 in stage 15.0 (TID 29438, scc-q13.scc.bu.edu, executor 7): org.broadinstitute.hellbender.exceptions.GATKException: Erred when inferring breakpoint location and event type from chimeric alignment:; asm010450:tig00000 1_189_chrUn_JTFH01000312v1_decoy:663-851_-_189M512H_60_8_149_O 153_701_chrUn_JTFH01000312v1_decoy:1-549_+_152S549M_60_0_549_O; at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.<init>(NovelAdjacencyReferenceLocations.java:51); at org.broadinstitute.hellbender.tools.spark.sv.discovery.DiscoverVariantsFromContigAlignmentsSAMSpark.lambda$null$0(DiscoverVariantsFromContigAlignmentsSAMSpark.java:175); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.ArrayList$ArrayListSpliterator.tryAdvance(ArrayList.java:1351); at java.util.stream.StreamSplitera",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458
https://github.com/broadinstitute/gatk/issues/4458:341,Deployability,pipeline,pipeline,341,"When running StructuralVariationDiscoveryPipelineSpark (GATK 4.0.1.1) on a hadoop cluster, the following exception occurs. The pipeline has been running fine on other cram files. **Caused by: java.lang.IllegalArgumentException: Invalid interval. Contig:chrUn_JTFH01000312v1_decoy start:0 end:0**. Below is the stack and other details. . The pipeline looks like it is running through all the contigs and is not limited to chr1-chr22, ChrY, ChrX and ChrM. Would it help running the software if the extra 3000+ contig names in the GRCh38 reference are excluded? If so, what is the best way to exclude processing the extra contigs?. ```; 18/02/23 23:06:22 INFO scheduler.TaskSetManager: Finished task 24.0 in stage 15.0 (TID 29435) in 2906 ms on scc-q04.scc.bu.edu (executor 1) (48/70); 18/02/23 23:06:23 INFO scheduler.TaskSetManager: Finished task 52.0 in stage 15.0 (TID 29463) in 2354 ms on scc-q04.scc.bu.edu (executor 1) (49/70); 18/02/23 23:06:23 INFO scheduler.TaskSetManager: Finished task 37.0 in stage 15.0 (TID 29448) in 2653 ms on scc-q03.scc.bu.edu (executor 6) (50/70); 18/02/23 23:06:23 WARN scheduler.TaskSetManager: Lost task 27.0 in stage 15.0 (TID 29438, scc-q13.scc.bu.edu, executor 7): org.broadinstitute.hellbender.exceptions.GATKException: Erred when inferring breakpoint location and event type from chimeric alignment:; asm010450:tig00000 1_189_chrUn_JTFH01000312v1_decoy:663-851_-_189M512H_60_8_149_O 153_701_chrUn_JTFH01000312v1_decoy:1-549_+_152S549M_60_0_549_O; at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.<init>(NovelAdjacencyReferenceLocations.java:51); at org.broadinstitute.hellbender.tools.spark.sv.discovery.DiscoverVariantsFromContigAlignmentsSAMSpark.lambda$null$0(DiscoverVariantsFromContigAlignmentsSAMSpark.java:175); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.ArrayList$ArrayListSpliterator.tryAdvance(ArrayList.java:1351); at java.util.stream.StreamSplitera",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458
https://github.com/broadinstitute/gatk/issues/4458:11295,Deployability,deploy,deploy,11295,ine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:387); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:136); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:153); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:195); at org.broadinstitute.hellbender.Main.main(Main.java:277); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: org.broadinstitute.hellbender.exceptions.GATKException: Erred when inferring breakpoint location and event type from chimeric alignment:; asm010450:tig00000 1_189_chrUn_JTFH01000312v1_decoy:663-851_-_189M512H_60_8_149_O 153_701_chrUn_JTFH01000312v1_decoy:1-549_+_152S549M_60_0_549_O; at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.<init>(NovelAdjacencyReferenceLocations.java:51); at org.broadinstitute.hellbender.tools.spark.sv.discovery.DiscoverVariantsFromContigAlignmentsSAMSpark.lambda$null$0(DiscoverVariantsFromContigAlignmentsSAMSpark.java:175); at java.util.str,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458
https://github.com/broadinstitute/gatk/issues/4458:11332,Deployability,deploy,deploy,11332,Tool.java:387); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:136); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:153); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:195); at org.broadinstitute.hellbender.Main.main(Main.java:277); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: org.broadinstitute.hellbender.exceptions.GATKException: Erred when inferring breakpoint location and event type from chimeric alignment:; asm010450:tig00000 1_189_chrUn_JTFH01000312v1_decoy:663-851_-_189M512H_60_8_149_O 153_701_chrUn_JTFH01000312v1_decoy:1-549_+_152S549M_60_0_549_O; at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.<init>(NovelAdjacencyReferenceLocations.java:51); at org.broadinstitute.hellbender.tools.spark.sv.discovery.DiscoverVariantsFromContigAlignmentsSAMSpark.lambda$null$0(DiscoverVariantsFromContigAlignmentsSAMSpark.java:175); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePip,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458
https://github.com/broadinstitute/gatk/issues/4458:11404,Deployability,deploy,deploy,11404,arkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:136); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:153); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:195); at org.broadinstitute.hellbender.Main.main(Main.java:277); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: org.broadinstitute.hellbender.exceptions.GATKException: Erred when inferring breakpoint location and event type from chimeric alignment:; asm010450:tig00000 1_189_chrUn_JTFH01000312v1_decoy:663-851_-_189M512H_60_8_149_O 153_701_chrUn_JTFH01000312v1_decoy:1-549_+_152S549M_60_0_549_O; at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.<init>(NovelAdjacencyReferenceLocations.java:51); at org.broadinstitute.hellbender.tools.spark.sv.discovery.DiscoverVariantsFromContigAlignmentsSAMSpark.lambda$null$0(DiscoverVariantsFromContigAlignmentsSAMSpark.java:175); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.ArrayList$ArrayListSpliterator.try,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458
https://github.com/broadinstitute/gatk/issues/4458:11480,Deployability,deploy,deploy,11480,nstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:136); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:153); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:195); at org.broadinstitute.hellbender.Main.main(Main.java:277); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: org.broadinstitute.hellbender.exceptions.GATKException: Erred when inferring breakpoint location and event type from chimeric alignment:; asm010450:tig00000 1_189_chrUn_JTFH01000312v1_decoy:663-851_-_189M512H_60_8_149_O 153_701_chrUn_JTFH01000312v1_decoy:1-549_+_152S549M_60_0_549_O; at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.<init>(NovelAdjacencyReferenceLocations.java:51); at org.broadinstitute.hellbender.tools.spark.sv.discovery.DiscoverVariantsFromContigAlignmentsSAMSpark.lambda$null$0(DiscoverVariantsFromContigAlignmentsSAMSpark.java:175); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.ArrayList$ArrayListSpliterator.tryAdvance(ArrayList.java:1351); at java.util.stream.StreamSpliterators$Wrappin,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458
https://github.com/broadinstitute/gatk/issues/4458:11551,Deployability,deploy,deploy,11551,am.java:136); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:153); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:195); at org.broadinstitute.hellbender.Main.main(Main.java:277); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: org.broadinstitute.hellbender.exceptions.GATKException: Erred when inferring breakpoint location and event type from chimeric alignment:; asm010450:tig00000 1_189_chrUn_JTFH01000312v1_decoy:663-851_-_189M512H_60_8_149_O 153_701_chrUn_JTFH01000312v1_decoy:1-549_+_152S549M_60_0_549_O; at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.<init>(NovelAdjacencyReferenceLocations.java:51); at org.broadinstitute.hellbender.tools.spark.sv.discovery.DiscoverVariantsFromContigAlignmentsSAMSpark.lambda$null$0(DiscoverVariantsFromContigAlignmentsSAMSpark.java:175); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.ArrayList$ArrayListSpliterator.tryAdvance(ArrayList.java:1351); at java.util.stream.StreamSpliterators$WrappingSpliterator.lambda$initPartialTraversalState$0(StreamSpliterators.java,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458
https://github.com/broadinstitute/gatk/issues/4458:11620,Deployability,deploy,deploy,11620,gram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:153); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:195); at org.broadinstitute.hellbender.Main.main(Main.java:277); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: org.broadinstitute.hellbender.exceptions.GATKException: Erred when inferring breakpoint location and event type from chimeric alignment:; asm010450:tig00000 1_189_chrUn_JTFH01000312v1_decoy:663-851_-_189M512H_60_8_149_O 153_701_chrUn_JTFH01000312v1_decoy:1-549_+_152S549M_60_0_549_O; at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.<init>(NovelAdjacencyReferenceLocations.java:51); at org.broadinstitute.hellbender.tools.spark.sv.discovery.DiscoverVariantsFromContigAlignmentsSAMSpark.lambda$null$0(DiscoverVariantsFromContigAlignmentsSAMSpark.java:175); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.ArrayList$ArrayListSpliterator.tryAdvance(ArrayList.java:1351); at java.util.stream.StreamSpliterators$WrappingSpliterator.lambda$initPartialTraversalState$0(StreamSpliterators.java:294); at java.util.stream.StreamSpliterators$AbstractWrappingSpliter,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458
https://github.com/broadinstitute/gatk/issues/4458:657,Energy Efficiency,schedul,scheduler,657,"When running StructuralVariationDiscoveryPipelineSpark (GATK 4.0.1.1) on a hadoop cluster, the following exception occurs. The pipeline has been running fine on other cram files. **Caused by: java.lang.IllegalArgumentException: Invalid interval. Contig:chrUn_JTFH01000312v1_decoy start:0 end:0**. Below is the stack and other details. . The pipeline looks like it is running through all the contigs and is not limited to chr1-chr22, ChrY, ChrX and ChrM. Would it help running the software if the extra 3000+ contig names in the GRCh38 reference are excluded? If so, what is the best way to exclude processing the extra contigs?. ```; 18/02/23 23:06:22 INFO scheduler.TaskSetManager: Finished task 24.0 in stage 15.0 (TID 29435) in 2906 ms on scc-q04.scc.bu.edu (executor 1) (48/70); 18/02/23 23:06:23 INFO scheduler.TaskSetManager: Finished task 52.0 in stage 15.0 (TID 29463) in 2354 ms on scc-q04.scc.bu.edu (executor 1) (49/70); 18/02/23 23:06:23 INFO scheduler.TaskSetManager: Finished task 37.0 in stage 15.0 (TID 29448) in 2653 ms on scc-q03.scc.bu.edu (executor 6) (50/70); 18/02/23 23:06:23 WARN scheduler.TaskSetManager: Lost task 27.0 in stage 15.0 (TID 29438, scc-q13.scc.bu.edu, executor 7): org.broadinstitute.hellbender.exceptions.GATKException: Erred when inferring breakpoint location and event type from chimeric alignment:; asm010450:tig00000 1_189_chrUn_JTFH01000312v1_decoy:663-851_-_189M512H_60_8_149_O 153_701_chrUn_JTFH01000312v1_decoy:1-549_+_152S549M_60_0_549_O; at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.<init>(NovelAdjacencyReferenceLocations.java:51); at org.broadinstitute.hellbender.tools.spark.sv.discovery.DiscoverVariantsFromContigAlignmentsSAMSpark.lambda$null$0(DiscoverVariantsFromContigAlignmentsSAMSpark.java:175); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.ArrayList$ArrayListSpliterator.tryAdvance(ArrayList.java:1351); at java.util.stream.StreamSplitera",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458
https://github.com/broadinstitute/gatk/issues/4458:806,Energy Efficiency,schedul,scheduler,806,"When running StructuralVariationDiscoveryPipelineSpark (GATK 4.0.1.1) on a hadoop cluster, the following exception occurs. The pipeline has been running fine on other cram files. **Caused by: java.lang.IllegalArgumentException: Invalid interval. Contig:chrUn_JTFH01000312v1_decoy start:0 end:0**. Below is the stack and other details. . The pipeline looks like it is running through all the contigs and is not limited to chr1-chr22, ChrY, ChrX and ChrM. Would it help running the software if the extra 3000+ contig names in the GRCh38 reference are excluded? If so, what is the best way to exclude processing the extra contigs?. ```; 18/02/23 23:06:22 INFO scheduler.TaskSetManager: Finished task 24.0 in stage 15.0 (TID 29435) in 2906 ms on scc-q04.scc.bu.edu (executor 1) (48/70); 18/02/23 23:06:23 INFO scheduler.TaskSetManager: Finished task 52.0 in stage 15.0 (TID 29463) in 2354 ms on scc-q04.scc.bu.edu (executor 1) (49/70); 18/02/23 23:06:23 INFO scheduler.TaskSetManager: Finished task 37.0 in stage 15.0 (TID 29448) in 2653 ms on scc-q03.scc.bu.edu (executor 6) (50/70); 18/02/23 23:06:23 WARN scheduler.TaskSetManager: Lost task 27.0 in stage 15.0 (TID 29438, scc-q13.scc.bu.edu, executor 7): org.broadinstitute.hellbender.exceptions.GATKException: Erred when inferring breakpoint location and event type from chimeric alignment:; asm010450:tig00000 1_189_chrUn_JTFH01000312v1_decoy:663-851_-_189M512H_60_8_149_O 153_701_chrUn_JTFH01000312v1_decoy:1-549_+_152S549M_60_0_549_O; at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.<init>(NovelAdjacencyReferenceLocations.java:51); at org.broadinstitute.hellbender.tools.spark.sv.discovery.DiscoverVariantsFromContigAlignmentsSAMSpark.lambda$null$0(DiscoverVariantsFromContigAlignmentsSAMSpark.java:175); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.ArrayList$ArrayListSpliterator.tryAdvance(ArrayList.java:1351); at java.util.stream.StreamSplitera",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458
https://github.com/broadinstitute/gatk/issues/4458:955,Energy Efficiency,schedul,scheduler,955,"When running StructuralVariationDiscoveryPipelineSpark (GATK 4.0.1.1) on a hadoop cluster, the following exception occurs. The pipeline has been running fine on other cram files. **Caused by: java.lang.IllegalArgumentException: Invalid interval. Contig:chrUn_JTFH01000312v1_decoy start:0 end:0**. Below is the stack and other details. . The pipeline looks like it is running through all the contigs and is not limited to chr1-chr22, ChrY, ChrX and ChrM. Would it help running the software if the extra 3000+ contig names in the GRCh38 reference are excluded? If so, what is the best way to exclude processing the extra contigs?. ```; 18/02/23 23:06:22 INFO scheduler.TaskSetManager: Finished task 24.0 in stage 15.0 (TID 29435) in 2906 ms on scc-q04.scc.bu.edu (executor 1) (48/70); 18/02/23 23:06:23 INFO scheduler.TaskSetManager: Finished task 52.0 in stage 15.0 (TID 29463) in 2354 ms on scc-q04.scc.bu.edu (executor 1) (49/70); 18/02/23 23:06:23 INFO scheduler.TaskSetManager: Finished task 37.0 in stage 15.0 (TID 29448) in 2653 ms on scc-q03.scc.bu.edu (executor 6) (50/70); 18/02/23 23:06:23 WARN scheduler.TaskSetManager: Lost task 27.0 in stage 15.0 (TID 29438, scc-q13.scc.bu.edu, executor 7): org.broadinstitute.hellbender.exceptions.GATKException: Erred when inferring breakpoint location and event type from chimeric alignment:; asm010450:tig00000 1_189_chrUn_JTFH01000312v1_decoy:663-851_-_189M512H_60_8_149_O 153_701_chrUn_JTFH01000312v1_decoy:1-549_+_152S549M_60_0_549_O; at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.<init>(NovelAdjacencyReferenceLocations.java:51); at org.broadinstitute.hellbender.tools.spark.sv.discovery.DiscoverVariantsFromContigAlignmentsSAMSpark.lambda$null$0(DiscoverVariantsFromContigAlignmentsSAMSpark.java:175); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.ArrayList$ArrayListSpliterator.tryAdvance(ArrayList.java:1351); at java.util.stream.StreamSplitera",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458
https://github.com/broadinstitute/gatk/issues/4458:1104,Energy Efficiency,schedul,scheduler,1104,"uster, the following exception occurs. The pipeline has been running fine on other cram files. **Caused by: java.lang.IllegalArgumentException: Invalid interval. Contig:chrUn_JTFH01000312v1_decoy start:0 end:0**. Below is the stack and other details. . The pipeline looks like it is running through all the contigs and is not limited to chr1-chr22, ChrY, ChrX and ChrM. Would it help running the software if the extra 3000+ contig names in the GRCh38 reference are excluded? If so, what is the best way to exclude processing the extra contigs?. ```; 18/02/23 23:06:22 INFO scheduler.TaskSetManager: Finished task 24.0 in stage 15.0 (TID 29435) in 2906 ms on scc-q04.scc.bu.edu (executor 1) (48/70); 18/02/23 23:06:23 INFO scheduler.TaskSetManager: Finished task 52.0 in stage 15.0 (TID 29463) in 2354 ms on scc-q04.scc.bu.edu (executor 1) (49/70); 18/02/23 23:06:23 INFO scheduler.TaskSetManager: Finished task 37.0 in stage 15.0 (TID 29448) in 2653 ms on scc-q03.scc.bu.edu (executor 6) (50/70); 18/02/23 23:06:23 WARN scheduler.TaskSetManager: Lost task 27.0 in stage 15.0 (TID 29438, scc-q13.scc.bu.edu, executor 7): org.broadinstitute.hellbender.exceptions.GATKException: Erred when inferring breakpoint location and event type from chimeric alignment:; asm010450:tig00000 1_189_chrUn_JTFH01000312v1_decoy:663-851_-_189M512H_60_8_149_O 153_701_chrUn_JTFH01000312v1_decoy:1-549_+_152S549M_60_0_549_O; at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.<init>(NovelAdjacencyReferenceLocations.java:51); at org.broadinstitute.hellbender.tools.spark.sv.discovery.DiscoverVariantsFromContigAlignmentsSAMSpark.lambda$null$0(DiscoverVariantsFromContigAlignmentsSAMSpark.java:175); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.ArrayList$ArrayListSpliterator.tryAdvance(ArrayList.java:1351); at java.util.stream.StreamSpliterators$WrappingSpliterator.lambda$initPartialTraversalState$0(StreamSpliterators.java:",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458
https://github.com/broadinstitute/gatk/issues/4458:2750,Energy Efficiency,schedul,scheduler,2750,(DiscoverVariantsFromContigAlignmentsSAMSpark.java:175); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.ArrayList$ArrayListSpliterator.tryAdvance(ArrayList.java:1351); at java.util.stream.StreamSpliterators$WrappingSpliterator.lambda$initPartialTraversalState$0(StreamSpliterators.java:294); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.fillBuffer(StreamSpliterators.java:206); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.doAdvance(StreamSpliterators.java:161); at java.util.stream.StreamSpliterators$WrappingSpliterator.tryAdvance(StreamSpliterators.java:300); at java.util.Spliterators$1Adapter.hasNext(Spliterators.java:681); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.IllegalArgumentException: Invalid interval. Contig:chrUn_JTFH01000312v1_decoy start:0 end:0; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations$BreakpointsInference.getLeftJustifiedBreakpoints(NovelAd,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458
https://github.com/broadinstitute/gatk/issues/4458:2829,Energy Efficiency,schedul,scheduler,2829,ferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.ArrayList$ArrayListSpliterator.tryAdvance(ArrayList.java:1351); at java.util.stream.StreamSpliterators$WrappingSpliterator.lambda$initPartialTraversalState$0(StreamSpliterators.java:294); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.fillBuffer(StreamSpliterators.java:206); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.doAdvance(StreamSpliterators.java:161); at java.util.stream.StreamSpliterators$WrappingSpliterator.tryAdvance(StreamSpliterators.java:300); at java.util.Spliterators$1Adapter.hasNext(Spliterators.java:681); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.IllegalArgumentException: Invalid interval. Contig:chrUn_JTFH01000312v1_decoy start:0 end:0; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations$BreakpointsInference.getLeftJustifiedBreakpoints(NovelAdjacencyReferenceLocations.java:86); at org.broadinstitute.hellbender.tools.spar,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458
https://github.com/broadinstitute/gatk/issues/4458:2908,Energy Efficiency,schedul,scheduler,2908,ArrayListSpliterator.tryAdvance(ArrayList.java:1351); at java.util.stream.StreamSpliterators$WrappingSpliterator.lambda$initPartialTraversalState$0(StreamSpliterators.java:294); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.fillBuffer(StreamSpliterators.java:206); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.doAdvance(StreamSpliterators.java:161); at java.util.stream.StreamSpliterators$WrappingSpliterator.tryAdvance(StreamSpliterators.java:300); at java.util.Spliterators$1Adapter.hasNext(Spliterators.java:681); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.IllegalArgumentException: Invalid interval. Contig:chrUn_JTFH01000312v1_decoy start:0 end:0; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations$BreakpointsInference.getLeftJustifiedBreakpoints(NovelAdjacencyReferenceLocations.java:86); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.leftJustifyBreakpoint,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458
https://github.com/broadinstitute/gatk/issues/4458:4224,Energy Efficiency,schedul,scheduler,4224,"lang.Thread.run(Thread.java:745); Caused by: java.lang.IllegalArgumentException: Invalid interval. Contig:chrUn_JTFH01000312v1_decoy start:0 end:0; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations$BreakpointsInference.getLeftJustifiedBreakpoints(NovelAdjacencyReferenceLocations.java:86); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.leftJustifyBreakpoints(NovelAdjacencyReferenceLocations.java:301); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.<init>(NovelAdjacencyReferenceLocations.java:46); ... 18 more. 8/02/23 23:06:24 INFO storage.BlockManagerMaster: BlockManagerMaster stopped; 18/02/23 23:06:24 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/02/23 23:06:24 INFO spark.SparkContext: Successfully stopped SparkContext; 23:06:24.240 INFO StructuralVariationDiscoveryPipelineSpark - Shutting down engine; [February 23, 2018 11:06:24 PM EST] org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDiscoveryPipelineSpark done. Elapsed time: 73.92 minutes.; Runtime.totalMemory()=10458497024; org.apache.spark.SparkException: Job aborted due to stage failure: Task 27 in stage 15.0 failed 4 times, most recent failure: Lost task 27.3 in stage 15.0 (TID 29483, scc-q15.scc.bu.edu, executor 13): org.broadinstitute.hellbender.exc eptions.GATKException: Erred when inferring breakpoint location and event type from chimeric alignment:; asm010450:tig00000 1_189_chrUn_JTFH01000312v1_decoy:663-851_-_189M512H_60_8_149_O 153_701_chrUn_JTFH01000312v1_decoy:1-549_+_152S549M_60_0_549_O; at org.broadinstitute.hellb",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458
https://github.com/broadinstitute/gatk/issues/4458:6429,Energy Efficiency,schedul,scheduler,6429,(DiscoverVariantsFromContigAlignmentsSAMSpark.java:175); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.ArrayList$ArrayListSpliterator.tryAdvance(ArrayList.java:1351); at java.util.stream.StreamSpliterators$WrappingSpliterator.lambda$initPartialTraversalState$0(StreamSpliterators.java:294); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.fillBuffer(StreamSpliterators.java:206); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.doAdvance(StreamSpliterators.java:161); at java.util.stream.StreamSpliterators$WrappingSpliterator.tryAdvance(StreamSpliterators.java:300); at java.util.Spliterators$1Adapter.hasNext(Spliterators.java:681); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.IllegalArgumentException: Invalid interval. Contig:chrUn_JTFH01000312v1_decoy start:0 end:0; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations$BreakpointsInference.getLeftJustifiedBreakpoints(NovelAd,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458
https://github.com/broadinstitute/gatk/issues/4458:6508,Energy Efficiency,schedul,scheduler,6508,ferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.ArrayList$ArrayListSpliterator.tryAdvance(ArrayList.java:1351); at java.util.stream.StreamSpliterators$WrappingSpliterator.lambda$initPartialTraversalState$0(StreamSpliterators.java:294); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.fillBuffer(StreamSpliterators.java:206); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.doAdvance(StreamSpliterators.java:161); at java.util.stream.StreamSpliterators$WrappingSpliterator.tryAdvance(StreamSpliterators.java:300); at java.util.Spliterators$1Adapter.hasNext(Spliterators.java:681); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.IllegalArgumentException: Invalid interval. Contig:chrUn_JTFH01000312v1_decoy start:0 end:0; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations$BreakpointsInference.getLeftJustifiedBreakpoints(NovelAdjacencyReferenceLocations.java:86); at org.broadinstitute.hellbender.tools.spar,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458
https://github.com/broadinstitute/gatk/issues/4458:6587,Energy Efficiency,schedul,scheduler,6587,ArrayListSpliterator.tryAdvance(ArrayList.java:1351); at java.util.stream.StreamSpliterators$WrappingSpliterator.lambda$initPartialTraversalState$0(StreamSpliterators.java:294); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.fillBuffer(StreamSpliterators.java:206); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.doAdvance(StreamSpliterators.java:161); at java.util.stream.StreamSpliterators$WrappingSpliterator.tryAdvance(StreamSpliterators.java:300); at java.util.Spliterators$1Adapter.hasNext(Spliterators.java:681); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.IllegalArgumentException: Invalid interval. Contig:chrUn_JTFH01000312v1_decoy start:0 end:0; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations$BreakpointsInference.getLeftJustifiedBreakpoints(NovelAdjacencyReferenceLocations.java:86); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.leftJustifyBreakpoint,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458
https://github.com/broadinstitute/gatk/issues/4458:7842,Energy Efficiency,schedul,scheduler,7842,cutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.IllegalArgumentException: Invalid interval. Contig:chrUn_JTFH01000312v1_decoy start:0 end:0; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations$BreakpointsInference.getLeftJustifiedBreakpoints(NovelAdjacencyReferenceLocations.java:86); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.leftJustifyBreakpoints(NovelAdjacencyReferenceLocations.java:301); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.<init>(NovelAdjacencyReferenceLocations.java:46); ... 18 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.s,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458
https://github.com/broadinstitute/gatk/issues/4458:7882,Energy Efficiency,schedul,scheduler,7882,Caused by: java.lang.IllegalArgumentException: Invalid interval. Contig:chrUn_JTFH01000312v1_decoy start:0 end:0; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations$BreakpointsInference.getLeftJustifiedBreakpoints(NovelAdjacencyReferenceLocations.java:86); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.leftJustifyBreakpoints(NovelAdjacencyReferenceLocations.java:301); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.<init>(NovelAdjacencyReferenceLocations.java:46); ... 18 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSchedule,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458
https://github.com/broadinstitute/gatk/issues/4458:7980,Energy Efficiency,schedul,scheduler,7980,TFH01000312v1_decoy start:0 end:0; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations$BreakpointsInference.getLeftJustifiedBreakpoints(NovelAdjacencyReferenceLocations.java:86); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.leftJustifyBreakpoints(NovelAdjacencyReferenceLocations.java:301); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.<init>(NovelAdjacencyReferenceLocations.java:46); ... 18 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onRec,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458
https://github.com/broadinstitute/gatk/issues/4458:8077,Energy Efficiency,schedul,scheduler,8077,.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations$BreakpointsInference.getLeftJustifiedBreakpoints(NovelAdjacencyReferenceLocations.java:86); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.leftJustifyBreakpoints(NovelAdjacencyReferenceLocations.java:301); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.<init>(NovelAdjacencyReferenceLocations.java:46); ... 18 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458
https://github.com/broadinstitute/gatk/issues/4458:8328,Energy Efficiency,schedul,scheduler,8328,y.inference.NovelAdjacencyReferenceLocations$BreakpointsInference.getLeftJustifiedBreakpoints(NovelAdjacencyReferenceLocations.java:86); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.leftJustifyBreakpoints(NovelAdjacencyReferenceLocations.java:301); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.<init>(NovelAdjacencyReferenceLocations.java:46); ... 18 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931); at org.apache.spark.SparkContext.runJob(Sp,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458
https://github.com/broadinstitute/gatk/issues/4458:8408,Energy Efficiency,schedul,scheduler,8408,edBreakpoints(NovelAdjacencyReferenceLocations.java:86); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.leftJustifyBreakpoints(NovelAdjacencyReferenceLocations.java:301); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.<init>(NovelAdjacencyReferenceLocations.java:46); ... 18 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944); at org.apache.spark.SparkContext.runJob(SparkContext.sca,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458
https://github.com/broadinstitute/gatk/issues/4458:8513,Energy Efficiency,schedul,scheduler,8513,discovery.inference.NovelAdjacencyReferenceLocations.leftJustifyBreakpoints(NovelAdjacencyReferenceLocations.java:301); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.<init>(NovelAdjacencyReferenceLocations.java:46); ... 18 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1958); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:935); at org.apache.spark.rdd.RD,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458
https://github.com/broadinstitute/gatk/issues/4458:8661,Energy Efficiency,schedul,scheduler,8661,nder.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.<init>(NovelAdjacencyReferenceLocations.java:46); ... 18 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1958); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:935); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.a,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458
https://github.com/broadinstitute/gatk/issues/4458:8749,Energy Efficiency,schedul,scheduler,8749,acencyReferenceLocations.java:46); ... 18 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1958); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:935); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.collect(RDD.sc,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458
https://github.com/broadinstitute/gatk/issues/4458:8846,Energy Efficiency,schedul,scheduler,8846,.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1958); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:935); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.collect(RDD.scala:934); at org.apache.spark.api.java.JavaRDDLike$class.collect(JavaRDDLike.scala:361); at org.a,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458
https://github.com/broadinstitute/gatk/issues/4458:8941,Energy Efficiency,schedul,scheduler,8941,.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1958); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:935); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.collect(RDD.scala:934); at org.apache.spark.api.java.JavaRDDLike$class.collect(JavaRDDLike.scala:361); at org.apache.spark.api.java.AbstractJavaRDDLike.collect(JavaRDDLike.scala:45); at org.broadinstitute.h,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458
https://github.com/broadinstitute/gatk/issues/4458:9104,Energy Efficiency,schedul,scheduler,9104,abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1958); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:935); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.collect(RDD.scala:934); at org.apache.spark.api.java.JavaRDDLike$class.collect(JavaRDDLike.scala:361); at org.apache.spark.api.java.AbstractJavaRDDLike.collect(JavaRDDLike.scala:45); at org.broadinstitute.hellbender.tools.spark.sv.discovery.DiscoverVariantsFromContigAlignmentsSAMSpark.discoverVariantsFromChimeras(DiscoverVariantsFromContigAlignmentsSAMSpark.java:183),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458
https://github.com/broadinstitute/gatk/issues/4458:13221,Energy Efficiency,schedul,scheduler,13221,(DiscoverVariantsFromContigAlignmentsSAMSpark.java:175); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.ArrayList$ArrayListSpliterator.tryAdvance(ArrayList.java:1351); at java.util.stream.StreamSpliterators$WrappingSpliterator.lambda$initPartialTraversalState$0(StreamSpliterators.java:294); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.fillBuffer(StreamSpliterators.java:206); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.doAdvance(StreamSpliterators.java:161); at java.util.stream.StreamSpliterators$WrappingSpliterator.tryAdvance(StreamSpliterators.java:300); at java.util.Spliterators$1Adapter.hasNext(Spliterators.java:681); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.IllegalArgumentException: Invalid interval. Contig:chrUn_JTFH01000312v1_decoy start:0 end:0; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations$BreakpointsInference.getLeftJustifiedBreakpoints(NovelAd,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458
https://github.com/broadinstitute/gatk/issues/4458:13300,Energy Efficiency,schedul,scheduler,13300,ferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.ArrayList$ArrayListSpliterator.tryAdvance(ArrayList.java:1351); at java.util.stream.StreamSpliterators$WrappingSpliterator.lambda$initPartialTraversalState$0(StreamSpliterators.java:294); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.fillBuffer(StreamSpliterators.java:206); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.doAdvance(StreamSpliterators.java:161); at java.util.stream.StreamSpliterators$WrappingSpliterator.tryAdvance(StreamSpliterators.java:300); at java.util.Spliterators$1Adapter.hasNext(Spliterators.java:681); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.IllegalArgumentException: Invalid interval. Contig:chrUn_JTFH01000312v1_decoy start:0 end:0; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations$BreakpointsInference.getLeftJustifiedBreakpoints(NovelAdjacencyReferenceLocations.java:86); at org.broadinstitute.hellbender.tools.spar,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458
https://github.com/broadinstitute/gatk/issues/4458:13379,Energy Efficiency,schedul,scheduler,13379,ArrayListSpliterator.tryAdvance(ArrayList.java:1351); at java.util.stream.StreamSpliterators$WrappingSpliterator.lambda$initPartialTraversalState$0(StreamSpliterators.java:294); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.fillBuffer(StreamSpliterators.java:206); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.doAdvance(StreamSpliterators.java:161); at java.util.stream.StreamSpliterators$WrappingSpliterator.tryAdvance(StreamSpliterators.java:300); at java.util.Spliterators$1Adapter.hasNext(Spliterators.java:681); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.IllegalArgumentException: Invalid interval. Contig:chrUn_JTFH01000312v1_decoy start:0 end:0; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations$BreakpointsInference.getLeftJustifiedBreakpoints(NovelAdjacencyReferenceLocations.java:86); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.leftJustifyBreakpoint,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458
https://github.com/broadinstitute/gatk/issues/4458:2006,Integrability,Wrap,WrappingSpliterator,2006,"ge 15.0 (TID 29448) in 2653 ms on scc-q03.scc.bu.edu (executor 6) (50/70); 18/02/23 23:06:23 WARN scheduler.TaskSetManager: Lost task 27.0 in stage 15.0 (TID 29438, scc-q13.scc.bu.edu, executor 7): org.broadinstitute.hellbender.exceptions.GATKException: Erred when inferring breakpoint location and event type from chimeric alignment:; asm010450:tig00000 1_189_chrUn_JTFH01000312v1_decoy:663-851_-_189M512H_60_8_149_O 153_701_chrUn_JTFH01000312v1_decoy:1-549_+_152S549M_60_0_549_O; at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.<init>(NovelAdjacencyReferenceLocations.java:51); at org.broadinstitute.hellbender.tools.spark.sv.discovery.DiscoverVariantsFromContigAlignmentsSAMSpark.lambda$null$0(DiscoverVariantsFromContigAlignmentsSAMSpark.java:175); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.ArrayList$ArrayListSpliterator.tryAdvance(ArrayList.java:1351); at java.util.stream.StreamSpliterators$WrappingSpliterator.lambda$initPartialTraversalState$0(StreamSpliterators.java:294); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.fillBuffer(StreamSpliterators.java:206); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.doAdvance(StreamSpliterators.java:161); at java.util.stream.StreamSpliterators$WrappingSpliterator.tryAdvance(StreamSpliterators.java:300); at java.util.Spliterators$1Adapter.hasNext(Spliterators.java:681); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.sc",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458
https://github.com/broadinstitute/gatk/issues/4458:2345,Integrability,Wrap,WrappingSpliterator,2345,010450:tig00000 1_189_chrUn_JTFH01000312v1_decoy:663-851_-_189M512H_60_8_149_O 153_701_chrUn_JTFH01000312v1_decoy:1-549_+_152S549M_60_0_549_O; at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.<init>(NovelAdjacencyReferenceLocations.java:51); at org.broadinstitute.hellbender.tools.spark.sv.discovery.DiscoverVariantsFromContigAlignmentsSAMSpark.lambda$null$0(DiscoverVariantsFromContigAlignmentsSAMSpark.java:175); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.ArrayList$ArrayListSpliterator.tryAdvance(ArrayList.java:1351); at java.util.stream.StreamSpliterators$WrappingSpliterator.lambda$initPartialTraversalState$0(StreamSpliterators.java:294); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.fillBuffer(StreamSpliterators.java:206); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.doAdvance(StreamSpliterators.java:161); at java.util.stream.StreamSpliterators$WrappingSpliterator.tryAdvance(StreamSpliterators.java:300); at java.util.Spliterators$1Adapter.hasNext(Spliterators.java:681); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.IllegalArgumentException: Invalid interval. Contig:chrUn_JTFH01000312v1_decoy start:0 end:0; at or,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458
https://github.com/broadinstitute/gatk/issues/4458:2501,Integrability,Wrap,Wrappers,2501,bender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.<init>(NovelAdjacencyReferenceLocations.java:51); at org.broadinstitute.hellbender.tools.spark.sv.discovery.DiscoverVariantsFromContigAlignmentsSAMSpark.lambda$null$0(DiscoverVariantsFromContigAlignmentsSAMSpark.java:175); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.ArrayList$ArrayListSpliterator.tryAdvance(ArrayList.java:1351); at java.util.stream.StreamSpliterators$WrappingSpliterator.lambda$initPartialTraversalState$0(StreamSpliterators.java:294); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.fillBuffer(StreamSpliterators.java:206); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.doAdvance(StreamSpliterators.java:161); at java.util.stream.StreamSpliterators$WrappingSpliterator.tryAdvance(StreamSpliterators.java:300); at java.util.Spliterators$1Adapter.hasNext(Spliterators.java:681); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.IllegalArgumentException: Invalid interval. Contig:chrUn_JTFH01000312v1_decoy start:0 end:0; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458
https://github.com/broadinstitute/gatk/issues/4458:2535,Integrability,Wrap,Wrappers,2535,.discovery.inference.NovelAdjacencyReferenceLocations.<init>(NovelAdjacencyReferenceLocations.java:51); at org.broadinstitute.hellbender.tools.spark.sv.discovery.DiscoverVariantsFromContigAlignmentsSAMSpark.lambda$null$0(DiscoverVariantsFromContigAlignmentsSAMSpark.java:175); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.ArrayList$ArrayListSpliterator.tryAdvance(ArrayList.java:1351); at java.util.stream.StreamSpliterators$WrappingSpliterator.lambda$initPartialTraversalState$0(StreamSpliterators.java:294); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.fillBuffer(StreamSpliterators.java:206); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.doAdvance(StreamSpliterators.java:161); at java.util.stream.StreamSpliterators$WrappingSpliterator.tryAdvance(StreamSpliterators.java:300); at java.util.Spliterators$1Adapter.hasNext(Spliterators.java:681); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.IllegalArgumentException: Invalid interval. Contig:chrUn_JTFH01000312v1_decoy start:0 end:0; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.he,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458
https://github.com/broadinstitute/gatk/issues/4458:5685,Integrability,Wrap,WrappingSpliterator,5685,"apache.spark.SparkException: Job aborted due to stage failure: Task 27 in stage 15.0 failed 4 times, most recent failure: Lost task 27.3 in stage 15.0 (TID 29483, scc-q15.scc.bu.edu, executor 13): org.broadinstitute.hellbender.exc eptions.GATKException: Erred when inferring breakpoint location and event type from chimeric alignment:; asm010450:tig00000 1_189_chrUn_JTFH01000312v1_decoy:663-851_-_189M512H_60_8_149_O 153_701_chrUn_JTFH01000312v1_decoy:1-549_+_152S549M_60_0_549_O; at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.<init>(NovelAdjacencyReferenceLocations.java:51); at org.broadinstitute.hellbender.tools.spark.sv.discovery.DiscoverVariantsFromContigAlignmentsSAMSpark.lambda$null$0(DiscoverVariantsFromContigAlignmentsSAMSpark.java:175); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.ArrayList$ArrayListSpliterator.tryAdvance(ArrayList.java:1351); at java.util.stream.StreamSpliterators$WrappingSpliterator.lambda$initPartialTraversalState$0(StreamSpliterators.java:294); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.fillBuffer(StreamSpliterators.java:206); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.doAdvance(StreamSpliterators.java:161); at java.util.stream.StreamSpliterators$WrappingSpliterator.tryAdvance(StreamSpliterators.java:300); at java.util.Spliterators$1Adapter.hasNext(Spliterators.java:681); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.sc",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458
https://github.com/broadinstitute/gatk/issues/4458:6024,Integrability,Wrap,WrappingSpliterator,6024,010450:tig00000 1_189_chrUn_JTFH01000312v1_decoy:663-851_-_189M512H_60_8_149_O 153_701_chrUn_JTFH01000312v1_decoy:1-549_+_152S549M_60_0_549_O; at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.<init>(NovelAdjacencyReferenceLocations.java:51); at org.broadinstitute.hellbender.tools.spark.sv.discovery.DiscoverVariantsFromContigAlignmentsSAMSpark.lambda$null$0(DiscoverVariantsFromContigAlignmentsSAMSpark.java:175); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.ArrayList$ArrayListSpliterator.tryAdvance(ArrayList.java:1351); at java.util.stream.StreamSpliterators$WrappingSpliterator.lambda$initPartialTraversalState$0(StreamSpliterators.java:294); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.fillBuffer(StreamSpliterators.java:206); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.doAdvance(StreamSpliterators.java:161); at java.util.stream.StreamSpliterators$WrappingSpliterator.tryAdvance(StreamSpliterators.java:300); at java.util.Spliterators$1Adapter.hasNext(Spliterators.java:681); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.IllegalArgumentException: Invalid interval. Contig:chrUn_JTFH01000312v1_decoy start:0 end:0; at or,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458
https://github.com/broadinstitute/gatk/issues/4458:6180,Integrability,Wrap,Wrappers,6180,bender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.<init>(NovelAdjacencyReferenceLocations.java:51); at org.broadinstitute.hellbender.tools.spark.sv.discovery.DiscoverVariantsFromContigAlignmentsSAMSpark.lambda$null$0(DiscoverVariantsFromContigAlignmentsSAMSpark.java:175); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.ArrayList$ArrayListSpliterator.tryAdvance(ArrayList.java:1351); at java.util.stream.StreamSpliterators$WrappingSpliterator.lambda$initPartialTraversalState$0(StreamSpliterators.java:294); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.fillBuffer(StreamSpliterators.java:206); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.doAdvance(StreamSpliterators.java:161); at java.util.stream.StreamSpliterators$WrappingSpliterator.tryAdvance(StreamSpliterators.java:300); at java.util.Spliterators$1Adapter.hasNext(Spliterators.java:681); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.IllegalArgumentException: Invalid interval. Contig:chrUn_JTFH01000312v1_decoy start:0 end:0; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458
https://github.com/broadinstitute/gatk/issues/4458:6214,Integrability,Wrap,Wrappers,6214,.discovery.inference.NovelAdjacencyReferenceLocations.<init>(NovelAdjacencyReferenceLocations.java:51); at org.broadinstitute.hellbender.tools.spark.sv.discovery.DiscoverVariantsFromContigAlignmentsSAMSpark.lambda$null$0(DiscoverVariantsFromContigAlignmentsSAMSpark.java:175); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.ArrayList$ArrayListSpliterator.tryAdvance(ArrayList.java:1351); at java.util.stream.StreamSpliterators$WrappingSpliterator.lambda$initPartialTraversalState$0(StreamSpliterators.java:294); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.fillBuffer(StreamSpliterators.java:206); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.doAdvance(StreamSpliterators.java:161); at java.util.stream.StreamSpliterators$WrappingSpliterator.tryAdvance(StreamSpliterators.java:300); at java.util.Spliterators$1Adapter.hasNext(Spliterators.java:681); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.IllegalArgumentException: Invalid interval. Contig:chrUn_JTFH01000312v1_decoy start:0 end:0; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.he,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458
https://github.com/broadinstitute/gatk/issues/4458:12477,Integrability,Wrap,WrappingSpliterator,12477,rk.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: org.broadinstitute.hellbender.exceptions.GATKException: Erred when inferring breakpoint location and event type from chimeric alignment:; asm010450:tig00000 1_189_chrUn_JTFH01000312v1_decoy:663-851_-_189M512H_60_8_149_O 153_701_chrUn_JTFH01000312v1_decoy:1-549_+_152S549M_60_0_549_O; at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.<init>(NovelAdjacencyReferenceLocations.java:51); at org.broadinstitute.hellbender.tools.spark.sv.discovery.DiscoverVariantsFromContigAlignmentsSAMSpark.lambda$null$0(DiscoverVariantsFromContigAlignmentsSAMSpark.java:175); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.ArrayList$ArrayListSpliterator.tryAdvance(ArrayList.java:1351); at java.util.stream.StreamSpliterators$WrappingSpliterator.lambda$initPartialTraversalState$0(StreamSpliterators.java:294); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.fillBuffer(StreamSpliterators.java:206); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.doAdvance(StreamSpliterators.java:161); at java.util.stream.StreamSpliterators$WrappingSpliterator.tryAdvance(StreamSpliterators.java:300); at java.util.Spliterators$1Adapter.hasNext(Spliterators.java:681); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.sc,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458
https://github.com/broadinstitute/gatk/issues/4458:12816,Integrability,Wrap,WrappingSpliterator,12816,010450:tig00000 1_189_chrUn_JTFH01000312v1_decoy:663-851_-_189M512H_60_8_149_O 153_701_chrUn_JTFH01000312v1_decoy:1-549_+_152S549M_60_0_549_O; at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.<init>(NovelAdjacencyReferenceLocations.java:51); at org.broadinstitute.hellbender.tools.spark.sv.discovery.DiscoverVariantsFromContigAlignmentsSAMSpark.lambda$null$0(DiscoverVariantsFromContigAlignmentsSAMSpark.java:175); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.ArrayList$ArrayListSpliterator.tryAdvance(ArrayList.java:1351); at java.util.stream.StreamSpliterators$WrappingSpliterator.lambda$initPartialTraversalState$0(StreamSpliterators.java:294); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.fillBuffer(StreamSpliterators.java:206); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.doAdvance(StreamSpliterators.java:161); at java.util.stream.StreamSpliterators$WrappingSpliterator.tryAdvance(StreamSpliterators.java:300); at java.util.Spliterators$1Adapter.hasNext(Spliterators.java:681); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.IllegalArgumentException: Invalid interval. Contig:chrUn_JTFH01000312v1_decoy start:0 end:0; at or,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458
https://github.com/broadinstitute/gatk/issues/4458:12972,Integrability,Wrap,Wrappers,12972,bender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.<init>(NovelAdjacencyReferenceLocations.java:51); at org.broadinstitute.hellbender.tools.spark.sv.discovery.DiscoverVariantsFromContigAlignmentsSAMSpark.lambda$null$0(DiscoverVariantsFromContigAlignmentsSAMSpark.java:175); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.ArrayList$ArrayListSpliterator.tryAdvance(ArrayList.java:1351); at java.util.stream.StreamSpliterators$WrappingSpliterator.lambda$initPartialTraversalState$0(StreamSpliterators.java:294); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.fillBuffer(StreamSpliterators.java:206); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.doAdvance(StreamSpliterators.java:161); at java.util.stream.StreamSpliterators$WrappingSpliterator.tryAdvance(StreamSpliterators.java:300); at java.util.Spliterators$1Adapter.hasNext(Spliterators.java:681); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.IllegalArgumentException: Invalid interval. Contig:chrUn_JTFH01000312v1_decoy start:0 end:0; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458
https://github.com/broadinstitute/gatk/issues/4458:13006,Integrability,Wrap,Wrappers,13006,.discovery.inference.NovelAdjacencyReferenceLocations.<init>(NovelAdjacencyReferenceLocations.java:51); at org.broadinstitute.hellbender.tools.spark.sv.discovery.DiscoverVariantsFromContigAlignmentsSAMSpark.lambda$null$0(DiscoverVariantsFromContigAlignmentsSAMSpark.java:175); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.ArrayList$ArrayListSpliterator.tryAdvance(ArrayList.java:1351); at java.util.stream.StreamSpliterators$WrappingSpliterator.lambda$initPartialTraversalState$0(StreamSpliterators.java:294); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.fillBuffer(StreamSpliterators.java:206); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.doAdvance(StreamSpliterators.java:161); at java.util.stream.StreamSpliterators$WrappingSpliterator.tryAdvance(StreamSpliterators.java:300); at java.util.Spliterators$1Adapter.hasNext(Spliterators.java:681); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.IllegalArgumentException: Invalid interval. Contig:chrUn_JTFH01000312v1_decoy start:0 end:0; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.he,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458
https://github.com/broadinstitute/gatk/issues/4458:3030,Performance,concurren,concurrent,3030,itPartialTraversalState$0(StreamSpliterators.java:294); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.fillBuffer(StreamSpliterators.java:206); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.doAdvance(StreamSpliterators.java:161); at java.util.stream.StreamSpliterators$WrappingSpliterator.tryAdvance(StreamSpliterators.java:300); at java.util.Spliterators$1Adapter.hasNext(Spliterators.java:681); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.IllegalArgumentException: Invalid interval. Contig:chrUn_JTFH01000312v1_decoy start:0 end:0; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations$BreakpointsInference.getLeftJustifiedBreakpoints(NovelAdjacencyReferenceLocations.java:86); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.leftJustifyBreakpoints(NovelAdjacencyReferenceLocations.java:301); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdja,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458
https://github.com/broadinstitute/gatk/issues/4458:3114,Performance,concurren,concurrent,3114,literators$AbstractWrappingSpliterator.fillBuffer(StreamSpliterators.java:206); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.doAdvance(StreamSpliterators.java:161); at java.util.stream.StreamSpliterators$WrappingSpliterator.tryAdvance(StreamSpliterators.java:300); at java.util.Spliterators$1Adapter.hasNext(Spliterators.java:681); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.IllegalArgumentException: Invalid interval. Contig:chrUn_JTFH01000312v1_decoy start:0 end:0; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations$BreakpointsInference.getLeftJustifiedBreakpoints(NovelAdjacencyReferenceLocations.java:86); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.leftJustifyBreakpoints(NovelAdjacencyReferenceLocations.java:301); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.<init>(NovelAdjacencyReferenceLocations.java:46); ... 18 mor,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458
https://github.com/broadinstitute/gatk/issues/4458:6709,Performance,concurren,concurrent,6709,itPartialTraversalState$0(StreamSpliterators.java:294); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.fillBuffer(StreamSpliterators.java:206); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.doAdvance(StreamSpliterators.java:161); at java.util.stream.StreamSpliterators$WrappingSpliterator.tryAdvance(StreamSpliterators.java:300); at java.util.Spliterators$1Adapter.hasNext(Spliterators.java:681); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.IllegalArgumentException: Invalid interval. Contig:chrUn_JTFH01000312v1_decoy start:0 end:0; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations$BreakpointsInference.getLeftJustifiedBreakpoints(NovelAdjacencyReferenceLocations.java:86); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.leftJustifyBreakpoints(NovelAdjacencyReferenceLocations.java:301); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdja,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458
https://github.com/broadinstitute/gatk/issues/4458:6793,Performance,concurren,concurrent,6793,literators$AbstractWrappingSpliterator.fillBuffer(StreamSpliterators.java:206); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.doAdvance(StreamSpliterators.java:161); at java.util.stream.StreamSpliterators$WrappingSpliterator.tryAdvance(StreamSpliterators.java:300); at java.util.Spliterators$1Adapter.hasNext(Spliterators.java:681); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.IllegalArgumentException: Invalid interval. Contig:chrUn_JTFH01000312v1_decoy start:0 end:0; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations$BreakpointsInference.getLeftJustifiedBreakpoints(NovelAdjacencyReferenceLocations.java:86); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.leftJustifyBreakpoints(NovelAdjacencyReferenceLocations.java:301); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.<init>(NovelAdjacencyReferenceLocations.java:46); ... 18 mor,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458
https://github.com/broadinstitute/gatk/issues/4458:13501,Performance,concurren,concurrent,13501,itPartialTraversalState$0(StreamSpliterators.java:294); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.fillBuffer(StreamSpliterators.java:206); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.doAdvance(StreamSpliterators.java:161); at java.util.stream.StreamSpliterators$WrappingSpliterator.tryAdvance(StreamSpliterators.java:300); at java.util.Spliterators$1Adapter.hasNext(Spliterators.java:681); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.IllegalArgumentException: Invalid interval. Contig:chrUn_JTFH01000312v1_decoy start:0 end:0; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations$BreakpointsInference.getLeftJustifiedBreakpoints(NovelAdjacencyReferenceLocations.java:86); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.leftJustifyBreakpoints(NovelAdjacencyReferenceLocations.java:301); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdja,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458
https://github.com/broadinstitute/gatk/issues/4458:13585,Performance,concurren,concurrent,13585,literators$AbstractWrappingSpliterator.fillBuffer(StreamSpliterators.java:206); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.doAdvance(StreamSpliterators.java:161); at java.util.stream.StreamSpliterators$WrappingSpliterator.tryAdvance(StreamSpliterators.java:300); at java.util.Spliterators$1Adapter.hasNext(Spliterators.java:681); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.IllegalArgumentException: Invalid interval. Contig:chrUn_JTFH01000312v1_decoy start:0 end:0; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations$BreakpointsInference.getLeftJustifiedBreakpoints(NovelAdjacencyReferenceLocations.java:86); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.leftJustifyBreakpoints(NovelAdjacencyReferenceLocations.java:301); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.<init>(NovelAdjacencyReferenceLocations.java:46); ... 18 mor,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458
https://github.com/broadinstitute/gatk/issues/4458:4718,Safety,abort,aborted,4718,"iedBreakpoints(NovelAdjacencyReferenceLocations.java:86); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.leftJustifyBreakpoints(NovelAdjacencyReferenceLocations.java:301); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.<init>(NovelAdjacencyReferenceLocations.java:46); ... 18 more. 8/02/23 23:06:24 INFO storage.BlockManagerMaster: BlockManagerMaster stopped; 18/02/23 23:06:24 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/02/23 23:06:24 INFO spark.SparkContext: Successfully stopped SparkContext; 23:06:24.240 INFO StructuralVariationDiscoveryPipelineSpark - Shutting down engine; [February 23, 2018 11:06:24 PM EST] org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDiscoveryPipelineSpark done. Elapsed time: 73.92 minutes.; Runtime.totalMemory()=10458497024; org.apache.spark.SparkException: Job aborted due to stage failure: Task 27 in stage 15.0 failed 4 times, most recent failure: Lost task 27.3 in stage 15.0 (TID 29483, scc-q15.scc.bu.edu, executor 13): org.broadinstitute.hellbender.exc eptions.GATKException: Erred when inferring breakpoint location and event type from chimeric alignment:; asm010450:tig00000 1_189_chrUn_JTFH01000312v1_decoy:663-851_-_189M512H_60_8_149_O 153_701_chrUn_JTFH01000312v1_decoy:1-549_+_152S549M_60_0_549_O; at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.<init>(NovelAdjacencyReferenceLocations.java:51); at org.broadinstitute.hellbender.tools.spark.sv.discovery.DiscoverVariantsFromContigAlignmentsSAMSpark.lambda$null$0(DiscoverVariantsFromContigAlignmentsSAMSpark.java:175); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.ArrayList$ArrayListSpliterator.tryAdvance(ArrayList.java:1351); at java.util.stream.StreamSpliterators$WrappingSpliterator.lambda$initPartialTraversalS",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458
https://github.com/broadinstitute/gatk/issues/4458:8012,Safety,abort,abortStage,8012,art:0 end:0; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations$BreakpointsInference.getLeftJustifiedBreakpoints(NovelAdjacencyReferenceLocations.java:86); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.leftJustifyBreakpoints(NovelAdjacencyReferenceLocations.java:301); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.<init>(NovelAdjacencyReferenceLocations.java:46); ... 18 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458
https://github.com/broadinstitute/gatk/issues/4458:8109,Safety,abort,abortStage,8109,adinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations$BreakpointsInference.getLeftJustifiedBreakpoints(NovelAdjacencyReferenceLocations.java:86); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.leftJustifyBreakpoints(NovelAdjacencyReferenceLocations.java:301); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.<init>(NovelAdjacencyReferenceLocations.java:46); ... 18 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.s,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458
https://github.com/broadinstitute/gatk/issues/4458:8351,Safety,abort,abortStage,8351,renceLocations$BreakpointsInference.getLeftJustifiedBreakpoints(NovelAdjacencyReferenceLocations.java:86); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.leftJustifyBreakpoints(NovelAdjacencyReferenceLocations.java:301); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.<init>(NovelAdjacencyReferenceLocations.java:46); ... 18 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944); at org,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458
https://github.com/broadinstitute/gatk/issues/4458:3386,Security,validat,validateArg,3386,java:300); at java.util.Spliterators$1Adapter.hasNext(Spliterators.java:681); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.IllegalArgumentException: Invalid interval. Contig:chrUn_JTFH01000312v1_decoy start:0 end:0; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations$BreakpointsInference.getLeftJustifiedBreakpoints(NovelAdjacencyReferenceLocations.java:86); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.leftJustifyBreakpoints(NovelAdjacencyReferenceLocations.java:301); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.<init>(NovelAdjacencyReferenceLocations.java:46); ... 18 more. 8/02/23 23:06:24 INFO storage.BlockManagerMaster: BlockManagerMaster stopped; 18/02/23 23:06:24 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/02/23 23:06:24 INFO spark.SparkContext: Successfully stopped SparkC,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458
https://github.com/broadinstitute/gatk/issues/4458:3469,Security,validat,validatePositions,3469,lection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.IllegalArgumentException: Invalid interval. Contig:chrUn_JTFH01000312v1_decoy start:0 end:0; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations$BreakpointsInference.getLeftJustifiedBreakpoints(NovelAdjacencyReferenceLocations.java:86); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.leftJustifyBreakpoints(NovelAdjacencyReferenceLocations.java:301); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.<init>(NovelAdjacencyReferenceLocations.java:46); ... 18 more. 8/02/23 23:06:24 INFO storage.BlockManagerMaster: BlockManagerMaster stopped; 18/02/23 23:06:24 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/02/23 23:06:24 INFO spark.SparkContext: Successfully stopped SparkContext; 23:06:24.240 INFO StructuralVariationDiscoveryPipelineSpark - Shutting down engine;,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458
https://github.com/broadinstitute/gatk/issues/4458:7065,Security,validat,validateArg,7065,java:300); at java.util.Spliterators$1Adapter.hasNext(Spliterators.java:681); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.IllegalArgumentException: Invalid interval. Contig:chrUn_JTFH01000312v1_decoy start:0 end:0; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations$BreakpointsInference.getLeftJustifiedBreakpoints(NovelAdjacencyReferenceLocations.java:86); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.leftJustifyBreakpoints(NovelAdjacencyReferenceLocations.java:301); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.<init>(NovelAdjacencyReferenceLocations.java:46); ... 18 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spa,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458
https://github.com/broadinstitute/gatk/issues/4458:7148,Security,validat,validatePositions,7148,lection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.IllegalArgumentException: Invalid interval. Contig:chrUn_JTFH01000312v1_decoy start:0 end:0; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations$BreakpointsInference.getLeftJustifiedBreakpoints(NovelAdjacencyReferenceLocations.java:86); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.leftJustifyBreakpoints(NovelAdjacencyReferenceLocations.java:301); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.<init>(NovelAdjacencyReferenceLocations.java:46); ... 18 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.co,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458
https://github.com/broadinstitute/gatk/issues/4458:13857,Security,validat,validateArg,13857,java:300); at java.util.Spliterators$1Adapter.hasNext(Spliterators.java:681); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.IllegalArgumentException: Invalid interval. Contig:chrUn_JTFH01000312v1_decoy start:0 end:0; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations$BreakpointsInference.getLeftJustifiedBreakpoints(NovelAdjacencyReferenceLocations.java:86); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.leftJustifyBreakpoints(NovelAdjacencyReferenceLocations.java:301); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.<init>(NovelAdjacencyReferenceLocations.java:46); ... 18 more; 18/02/23 23:06:24 INFO util.ShutdownHookManager: Shutdown hook called; 18/02/23 23:06:24 INFO util.ShutdownHookManager: Deleting directory /restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/tmp/farrell/spark-94fa6743-3d29-4748-b8f8-d13a52dfed31; ```. The command line is:. ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458
https://github.com/broadinstitute/gatk/issues/4458:13940,Security,validat,validatePositions,13940,"lection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.IllegalArgumentException: Invalid interval. Contig:chrUn_JTFH01000312v1_decoy start:0 end:0; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations$BreakpointsInference.getLeftJustifiedBreakpoints(NovelAdjacencyReferenceLocations.java:86); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.leftJustifyBreakpoints(NovelAdjacencyReferenceLocations.java:301); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.<init>(NovelAdjacencyReferenceLocations.java:46); ... 18 more; 18/02/23 23:06:24 INFO util.ShutdownHookManager: Shutdown hook called; 18/02/23 23:06:24 INFO util.ShutdownHookManager: Deleting directory /restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/tmp/farrell/spark-94fa6743-3d29-4748-b8f8-d13a52dfed31; ```. The command line is:. ```; gatk --java-options ""-Djava.io.tmpdir=tmp"" StructuralVariationDiscoveryPipelineSpark \; -",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458
https://github.com/broadinstitute/gatk/issues/4458:3454,Usability,Simpl,SimpleInterval,3454,.java:681); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.IllegalArgumentException: Invalid interval. Contig:chrUn_JTFH01000312v1_decoy start:0 end:0; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations$BreakpointsInference.getLeftJustifiedBreakpoints(NovelAdjacencyReferenceLocations.java:86); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.leftJustifyBreakpoints(NovelAdjacencyReferenceLocations.java:301); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.<init>(NovelAdjacencyReferenceLocations.java:46); ... 18 more. 8/02/23 23:06:24 INFO storage.BlockManagerMaster: BlockManagerMaster stopped; 18/02/23 23:06:24 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/02/23 23:06:24 INFO spark.SparkContext: Successfully stopped SparkContext; 23:06:24.240 INFO StructuralVariationDiscoveryPipelineSpark,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458
https://github.com/broadinstitute/gatk/issues/4458:3487,Usability,Simpl,SimpleInterval,3487,lection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.IllegalArgumentException: Invalid interval. Contig:chrUn_JTFH01000312v1_decoy start:0 end:0; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations$BreakpointsInference.getLeftJustifiedBreakpoints(NovelAdjacencyReferenceLocations.java:86); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.leftJustifyBreakpoints(NovelAdjacencyReferenceLocations.java:301); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.<init>(NovelAdjacencyReferenceLocations.java:46); ... 18 more. 8/02/23 23:06:24 INFO storage.BlockManagerMaster: BlockManagerMaster stopped; 18/02/23 23:06:24 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/02/23 23:06:24 INFO spark.SparkContext: Successfully stopped SparkContext; 23:06:24.240 INFO StructuralVariationDiscoveryPipelineSpark - Shutting down engine;,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458
https://github.com/broadinstitute/gatk/issues/4458:3551,Usability,Simpl,SimpleInterval,3551,"scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.IllegalArgumentException: Invalid interval. Contig:chrUn_JTFH01000312v1_decoy start:0 end:0; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations$BreakpointsInference.getLeftJustifiedBreakpoints(NovelAdjacencyReferenceLocations.java:86); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.leftJustifyBreakpoints(NovelAdjacencyReferenceLocations.java:301); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.<init>(NovelAdjacencyReferenceLocations.java:46); ... 18 more. 8/02/23 23:06:24 INFO storage.BlockManagerMaster: BlockManagerMaster stopped; 18/02/23 23:06:24 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/02/23 23:06:24 INFO spark.SparkContext: Successfully stopped SparkContext; 23:06:24.240 INFO StructuralVariationDiscoveryPipelineSpark - Shutting down engine; [February 23, 2018 11:06:24 PM EST] org.broadinstitute.hellbender.tools.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458
https://github.com/broadinstitute/gatk/issues/4458:3573,Usability,Simpl,SimpleInterval,3573,"erator$$anon$12.hasNext(Iterator.scala:438); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.IllegalArgumentException: Invalid interval. Contig:chrUn_JTFH01000312v1_decoy start:0 end:0; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations$BreakpointsInference.getLeftJustifiedBreakpoints(NovelAdjacencyReferenceLocations.java:86); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.leftJustifyBreakpoints(NovelAdjacencyReferenceLocations.java:301); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.<init>(NovelAdjacencyReferenceLocations.java:46); ... 18 more. 8/02/23 23:06:24 INFO storage.BlockManagerMaster: BlockManagerMaster stopped; 18/02/23 23:06:24 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/02/23 23:06:24 INFO spark.SparkContext: Successfully stopped SparkContext; 23:06:24.240 INFO StructuralVariationDiscoveryPipelineSpark - Shutting down engine; [February 23, 2018 11:06:24 PM EST] org.broadinstitute.hellbender.tools.spark.sv.Structura",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458
https://github.com/broadinstitute/gatk/issues/4458:7133,Usability,Simpl,SimpleInterval,7133,.java:681); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.IllegalArgumentException: Invalid interval. Contig:chrUn_JTFH01000312v1_decoy start:0 end:0; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations$BreakpointsInference.getLeftJustifiedBreakpoints(NovelAdjacencyReferenceLocations.java:86); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.leftJustifyBreakpoints(NovelAdjacencyReferenceLocations.java:301); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.<init>(NovelAdjacencyReferenceLocations.java:46); ... 18 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458
https://github.com/broadinstitute/gatk/issues/4458:7166,Usability,Simpl,SimpleInterval,7166,lection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.IllegalArgumentException: Invalid interval. Contig:chrUn_JTFH01000312v1_decoy start:0 end:0; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations$BreakpointsInference.getLeftJustifiedBreakpoints(NovelAdjacencyReferenceLocations.java:86); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.leftJustifyBreakpoints(NovelAdjacencyReferenceLocations.java:301); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.<init>(NovelAdjacencyReferenceLocations.java:46); ... 18 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.co,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458
https://github.com/broadinstitute/gatk/issues/4458:7230,Usability,Simpl,SimpleInterval,7230,scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.IllegalArgumentException: Invalid interval. Contig:chrUn_JTFH01000312v1_decoy start:0 end:0; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations$BreakpointsInference.getLeftJustifiedBreakpoints(NovelAdjacencyReferenceLocations.java:86); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.leftJustifyBreakpoints(NovelAdjacencyReferenceLocations.java:301); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.<init>(NovelAdjacencyReferenceLocations.java:46); ... 18 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); a,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458
https://github.com/broadinstitute/gatk/issues/4458:7252,Usability,Simpl,SimpleInterval,7252,erator$$anon$12.hasNext(Iterator.scala:438); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.IllegalArgumentException: Invalid interval. Contig:chrUn_JTFH01000312v1_decoy start:0 end:0; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations$BreakpointsInference.getLeftJustifiedBreakpoints(NovelAdjacencyReferenceLocations.java:86); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.leftJustifyBreakpoints(NovelAdjacencyReferenceLocations.java:301); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.<init>(NovelAdjacencyReferenceLocations.java:46); ... 18 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458
https://github.com/broadinstitute/gatk/issues/4458:13925,Usability,Simpl,SimpleInterval,13925,".java:681); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.IllegalArgumentException: Invalid interval. Contig:chrUn_JTFH01000312v1_decoy start:0 end:0; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations$BreakpointsInference.getLeftJustifiedBreakpoints(NovelAdjacencyReferenceLocations.java:86); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.leftJustifyBreakpoints(NovelAdjacencyReferenceLocations.java:301); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.<init>(NovelAdjacencyReferenceLocations.java:46); ... 18 more; 18/02/23 23:06:24 INFO util.ShutdownHookManager: Shutdown hook called; 18/02/23 23:06:24 INFO util.ShutdownHookManager: Deleting directory /restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/tmp/farrell/spark-94fa6743-3d29-4748-b8f8-d13a52dfed31; ```. The command line is:. ```; gatk --java-options ""-Djava.io.tmpdir=tmp"" StructuralVariationDis",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458
https://github.com/broadinstitute/gatk/issues/4458:13958,Usability,Simpl,SimpleInterval,13958,"lection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.IllegalArgumentException: Invalid interval. Contig:chrUn_JTFH01000312v1_decoy start:0 end:0; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations$BreakpointsInference.getLeftJustifiedBreakpoints(NovelAdjacencyReferenceLocations.java:86); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.leftJustifyBreakpoints(NovelAdjacencyReferenceLocations.java:301); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.<init>(NovelAdjacencyReferenceLocations.java:46); ... 18 more; 18/02/23 23:06:24 INFO util.ShutdownHookManager: Shutdown hook called; 18/02/23 23:06:24 INFO util.ShutdownHookManager: Deleting directory /restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/tmp/farrell/spark-94fa6743-3d29-4748-b8f8-d13a52dfed31; ```. The command line is:. ```; gatk --java-options ""-Djava.io.tmpdir=tmp"" StructuralVariationDiscoveryPipelineSpark \; -",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458
https://github.com/broadinstitute/gatk/issues/4458:14022,Usability,Simpl,SimpleInterval,14022,"scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.IllegalArgumentException: Invalid interval. Contig:chrUn_JTFH01000312v1_decoy start:0 end:0; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations$BreakpointsInference.getLeftJustifiedBreakpoints(NovelAdjacencyReferenceLocations.java:86); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.leftJustifyBreakpoints(NovelAdjacencyReferenceLocations.java:301); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.<init>(NovelAdjacencyReferenceLocations.java:46); ... 18 more; 18/02/23 23:06:24 INFO util.ShutdownHookManager: Shutdown hook called; 18/02/23 23:06:24 INFO util.ShutdownHookManager: Deleting directory /restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/tmp/farrell/spark-94fa6743-3d29-4748-b8f8-d13a52dfed31; ```. The command line is:. ```; gatk --java-options ""-Djava.io.tmpdir=tmp"" StructuralVariationDiscoveryPipelineSpark \; -R $REF \; --aligner-index-image GRCh38_full_analysis_set_plus_decoy_hla.f",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458
https://github.com/broadinstitute/gatk/issues/4458:14044,Usability,Simpl,SimpleInterval,14044,"erator$$anon$12.hasNext(Iterator.scala:438); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.IllegalArgumentException: Invalid interval. Contig:chrUn_JTFH01000312v1_decoy start:0 end:0; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations$BreakpointsInference.getLeftJustifiedBreakpoints(NovelAdjacencyReferenceLocations.java:86); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.leftJustifyBreakpoints(NovelAdjacencyReferenceLocations.java:301); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.<init>(NovelAdjacencyReferenceLocations.java:46); ... 18 more; 18/02/23 23:06:24 INFO util.ShutdownHookManager: Shutdown hook called; 18/02/23 23:06:24 INFO util.ShutdownHookManager: Deleting directory /restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/tmp/farrell/spark-94fa6743-3d29-4748-b8f8-d13a52dfed31; ```. The command line is:. ```; gatk --java-options ""-Djava.io.tmpdir=tmp"" StructuralVariationDiscoveryPipelineSpark \; -R $REF \; --aligner-index-image GRCh38_full_analysis_set_plus_decoy_hla.fa.img \; --kmers-t",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458
https://github.com/broadinstitute/gatk/issues/4459:32,Availability,error,error,32,"Hi,; I recently started getting error messages when running a Nextflow pipeline for WGS analysis. I am using GATK 4.0.1.2 and was wondering whether:. /bin/env python - too many levels of symbolic links. may have to do with a broken conda environment (which GATK seems to use)? This happens for tools such as GenomicsDBImport. If I run the job in question outside of Nextflow, it seems to start just fine. But as far as I know Nextflow does not use python, so doesn't look like the obvious culprit.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4459
https://github.com/broadinstitute/gatk/issues/4459:71,Deployability,pipeline,pipeline,71,"Hi,; I recently started getting error messages when running a Nextflow pipeline for WGS analysis. I am using GATK 4.0.1.2 and was wondering whether:. /bin/env python - too many levels of symbolic links. may have to do with a broken conda environment (which GATK seems to use)? This happens for tools such as GenomicsDBImport. If I run the job in question outside of Nextflow, it seems to start just fine. But as far as I know Nextflow does not use python, so doesn't look like the obvious culprit.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4459
https://github.com/broadinstitute/gatk/issues/4459:38,Integrability,message,messages,38,"Hi,; I recently started getting error messages when running a Nextflow pipeline for WGS analysis. I am using GATK 4.0.1.2 and was wondering whether:. /bin/env python - too many levels of symbolic links. may have to do with a broken conda environment (which GATK seems to use)? This happens for tools such as GenomicsDBImport. If I run the job in question outside of Nextflow, it seems to start just fine. But as far as I know Nextflow does not use python, so doesn't look like the obvious culprit.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4459
https://github.com/broadinstitute/gatk/issues/4460:118,Availability,error,error,118,"In GATK4 (specifically 4.0.1.2), you can no longer give SelectVariants a file of variant IDs to keep. There isn't any error message, but the output VCF is empty. (It works in GATK3)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4460
https://github.com/broadinstitute/gatk/issues/4460:124,Integrability,message,message,124,"In GATK4 (specifically 4.0.1.2), you can no longer give SelectVariants a file of variant IDs to keep. There isn't any error message, but the output VCF is empty. (It works in GATK3)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4460
https://github.com/broadinstitute/gatk/issues/4462:98,Integrability,interface,interfaces,98,"As part of the implementation of VariantAnnotator, to avoid significant changes to the annotation interfaces a subclass of ReadLikelihoods was made to support partial data that was backed up by read pileups. Unfortunately since functionality is entangled with its superclass, this subclass is going to be subject to issues as more likelihoods functionality is added. A common interface between the two objects along the lines of a ""ReadEvidence"" object would help alleviate these problems.; Related to #4450, #3803",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4462
https://github.com/broadinstitute/gatk/issues/4462:376,Integrability,interface,interface,376,"As part of the implementation of VariantAnnotator, to avoid significant changes to the annotation interfaces a subclass of ReadLikelihoods was made to support partial data that was backed up by read pileups. Unfortunately since functionality is entangled with its superclass, this subclass is going to be subject to issues as more likelihoods functionality is added. A common interface between the two objects along the lines of a ""ReadEvidence"" object would help alleviate these problems.; Related to #4450, #3803",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4462
https://github.com/broadinstitute/gatk/issues/4462:54,Safety,avoid,avoid,54,"As part of the implementation of VariantAnnotator, to avoid significant changes to the annotation interfaces a subclass of ReadLikelihoods was made to support partial data that was backed up by read pileups. Unfortunately since functionality is entangled with its superclass, this subclass is going to be subject to issues as more likelihoods functionality is added. A common interface between the two objects along the lines of a ""ReadEvidence"" object would help alleviate these problems.; Related to #4450, #3803",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4462
https://github.com/broadinstitute/gatk/pull/4463:170,Availability,avail,available,170,"Fixes #4274 and #4303. Relies on https://github.com/HadoopGenomics/Hadoop-BAM/pull/194, so this won't pass (and shouldn't be merged) until a new release of Hadoop-BAM is available.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4463
https://github.com/broadinstitute/gatk/pull/4463:145,Deployability,release,release,145,"Fixes #4274 and #4303. Relies on https://github.com/HadoopGenomics/Hadoop-BAM/pull/194, so this won't pass (and shouldn't be merged) until a new release of Hadoop-BAM is available.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4463
https://github.com/broadinstitute/gatk/issues/4467:880,Availability,down,down,880,"Hello,. I am using the latest gatk4 from bioconda (4.0.2.0-0). I have genotyped 40 bacterial samples with HaplotypeCaller and -ERC BP_RESOLUTION. I have then merged the gvcfs files in a genomic db using GenomicsDBImport. However, when I try genotyping this genomics db, the genotyper starts but hangs rather rapidly like so : . ```; 07:18:31.127 INFO ProgressMeter - NC_016854.1:20000 0.2 20000 86132.6; 07:18:42.173 INFO ProgressMeter - NC_016854.1:58000 0.4 58000 139322.6; 07:21:16.150 INFO ProgressMeter - NC_016854.1:82000 3.0 82000 27493.1; 07:21:27.087 INFO ProgressMeter - NC_016854.1:99000 3.2 99000 31280.9; 07:24:06.157 INFO ProgressMeter - NC_016854.1:102000 5.8 102000 17537.7; 07:24:16.220 INFO ProgressMeter - NC_016854.1:138000 6.0 138000 23062.5; 07:24:29.116 INFO ProgressMeter - NC_016854.1:175000 6.2 175000 28231.8; 07:43:58.742 INFO GenotypeGVCFs - Shutting down engine; GENOMICSDB_TIMER,GenomicsDB iterator next() timer,Wall-clock time(s),29.644886145999088,Cpu time(s),29.480321756000397; Using GATK jar /opt/conda/envs/789546e2/share/gatk4-4.0.1.1-0/gatk-package-4.0.1.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -jar /opt/conda/envs/789546e2/share/gatk4-4.0.1.1-0/gatk-package-4.0.1.1-local.jar GenotypeGVCFs -ploidy 1 -R references/359488/genome_fasta.fasta --annotate-with-num-discovered-alleles true --annotations-to-exclude InbreedingCoeff -V gendb://typing/gatk_gvcfs/full_genome/359488/bwa/genomics_db -O typing/gatk_gvcfs/full_genome/359488/bwa/all_samples.vcf; ```; In between the last ProgressMeter and the Shutting down of the engine, I see the java process still running with top. Do you know what could be causing the problem ? Could it be related to -ERC BP_RESOLUTION ? I used to use -ERC GVCF before but I would rather keep the information of the coverage for post filtering, and I am not sure how to use --GVCFGQBands to",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4467
https://github.com/broadinstitute/gatk/issues/4467:1690,Availability,down,down,1690,"7:24:16.220 INFO ProgressMeter - NC_016854.1:138000 6.0 138000 23062.5; 07:24:29.116 INFO ProgressMeter - NC_016854.1:175000 6.2 175000 28231.8; 07:43:58.742 INFO GenotypeGVCFs - Shutting down engine; GENOMICSDB_TIMER,GenomicsDB iterator next() timer,Wall-clock time(s),29.644886145999088,Cpu time(s),29.480321756000397; Using GATK jar /opt/conda/envs/789546e2/share/gatk4-4.0.1.1-0/gatk-package-4.0.1.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -jar /opt/conda/envs/789546e2/share/gatk4-4.0.1.1-0/gatk-package-4.0.1.1-local.jar GenotypeGVCFs -ploidy 1 -R references/359488/genome_fasta.fasta --annotate-with-num-discovered-alleles true --annotations-to-exclude InbreedingCoeff -V gendb://typing/gatk_gvcfs/full_genome/359488/bwa/genomics_db -O typing/gatk_gvcfs/full_genome/359488/bwa/all_samples.vcf; ```; In between the last ProgressMeter and the Shutting down of the engine, I see the java process still running with top. Do you know what could be causing the problem ? Could it be related to -ERC BP_RESOLUTION ? I used to use -ERC GVCF before but I would rather keep the information of the coverage for post filtering, and I am not sure how to use --GVCFGQBands to match my criteria for coverage filtering. Thanks a lot for your help !. Edit: sorry with the latest version of gatk I get a new message error :; ```; 08:22:54.446 INFO ProgressMeter - NC_016854.1:20000 0.2 20000 87450.8; 08:23:04.942 INFO ProgressMeter - NC_016854.1:58000 0.4 58000 143694.8; 08:25:25.155 INFO ProgressMeter - NC_016854.1:82000 2.7 82000 29921.4; 08:25:35.161 INFO ProgressMeter - NC_016854.1:100000 2.9 100000 34396.6; 08:28:02.395 INFO ProgressMeter - NC_016854.1:102000 5.4 102000 19025.7; 08:28:13.248 INFO ProgressMeter - NC_016854.1:140000 5.5 140000 25261.3; 08:28:24.027 INFO ProgressMeter - NC_016854.1:175000 5.7 175000 30585.2; 08:46:13.574 INFO GenotypeGVCFs - S",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4467
https://github.com/broadinstitute/gatk/issues/4467:2138,Availability,error,error,2138,"java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -jar /opt/conda/envs/789546e2/share/gatk4-4.0.1.1-0/gatk-package-4.0.1.1-local.jar GenotypeGVCFs -ploidy 1 -R references/359488/genome_fasta.fasta --annotate-with-num-discovered-alleles true --annotations-to-exclude InbreedingCoeff -V gendb://typing/gatk_gvcfs/full_genome/359488/bwa/genomics_db -O typing/gatk_gvcfs/full_genome/359488/bwa/all_samples.vcf; ```; In between the last ProgressMeter and the Shutting down of the engine, I see the java process still running with top. Do you know what could be causing the problem ? Could it be related to -ERC BP_RESOLUTION ? I used to use -ERC GVCF before but I would rather keep the information of the coverage for post filtering, and I am not sure how to use --GVCFGQBands to match my criteria for coverage filtering. Thanks a lot for your help !. Edit: sorry with the latest version of gatk I get a new message error :; ```; 08:22:54.446 INFO ProgressMeter - NC_016854.1:20000 0.2 20000 87450.8; 08:23:04.942 INFO ProgressMeter - NC_016854.1:58000 0.4 58000 143694.8; 08:25:25.155 INFO ProgressMeter - NC_016854.1:82000 2.7 82000 29921.4; 08:25:35.161 INFO ProgressMeter - NC_016854.1:100000 2.9 100000 34396.6; 08:28:02.395 INFO ProgressMeter - NC_016854.1:102000 5.4 102000 19025.7; 08:28:13.248 INFO ProgressMeter - NC_016854.1:140000 5.5 140000 25261.3; 08:28:24.027 INFO ProgressMeter - NC_016854.1:175000 5.7 175000 30585.2; 08:46:13.574 INFO GenotypeGVCFs - Shutting down engine; GENOMICSDB_TIMER,GenomicsDB iterator next() timer,Wall-clock time(s),29.232685148998623,Cpu time(s),29.09919726900138; [February 28, 2018 8:46:13 AM UTC] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 23.59 minutes.; Runtime.totalMemory()=5588910080; Exception in thread ""main"" java.lang.OutOfMemoryError: GC overhead limit exceeded; 	at org.broadinstitute.hellbender.tool",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4467
https://github.com/broadinstitute/gatk/issues/4467:2701,Availability,down,down,2701,"rocess still running with top. Do you know what could be causing the problem ? Could it be related to -ERC BP_RESOLUTION ? I used to use -ERC GVCF before but I would rather keep the information of the coverage for post filtering, and I am not sure how to use --GVCFGQBands to match my criteria for coverage filtering. Thanks a lot for your help !. Edit: sorry with the latest version of gatk I get a new message error :; ```; 08:22:54.446 INFO ProgressMeter - NC_016854.1:20000 0.2 20000 87450.8; 08:23:04.942 INFO ProgressMeter - NC_016854.1:58000 0.4 58000 143694.8; 08:25:25.155 INFO ProgressMeter - NC_016854.1:82000 2.7 82000 29921.4; 08:25:35.161 INFO ProgressMeter - NC_016854.1:100000 2.9 100000 34396.6; 08:28:02.395 INFO ProgressMeter - NC_016854.1:102000 5.4 102000 19025.7; 08:28:13.248 INFO ProgressMeter - NC_016854.1:140000 5.5 140000 25261.3; 08:28:24.027 INFO ProgressMeter - NC_016854.1:175000 5.7 175000 30585.2; 08:46:13.574 INFO GenotypeGVCFs - Shutting down engine; GENOMICSDB_TIMER,GenomicsDB iterator next() timer,Wall-clock time(s),29.232685148998623,Cpu time(s),29.09919726900138; [February 28, 2018 8:46:13 AM UTC] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 23.59 minutes.; Runtime.totalMemory()=5588910080; Exception in thread ""main"" java.lang.OutOfMemoryError: GC overhead limit exceeded; 	at org.broadinstitute.hellbender.tools.walkers.genotyper.afcalc.GeneralPloidyExactAFCalculator$CombinedPoolLikelihoods.getLikelihoodOfConformation(GeneralPloidyExactAFCalculator.java:61); 	at org.broadinstitute.hellbender.tools.walkers.genotyper.afcalc.GeneralPloidyExactAFCalculator.computeLofK(GeneralPloidyExactAFCalculator.java:283); 	at org.broadinstitute.hellbender.tools.walkers.genotyper.afcalc.GeneralPloidyExactAFCalculator.calculateACConformationAndUpdateQueue(GeneralPloidyExactAFCalculator.java:187); 	at org.broadinstitute.hellbender.tools.walkers.genotyper.afcalc.GeneralPloidyExactAFCalculator.fastCombineMultiallelicPool(GeneralPl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4467
https://github.com/broadinstitute/gatk/issues/4467:2130,Integrability,message,message,2130,"java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -jar /opt/conda/envs/789546e2/share/gatk4-4.0.1.1-0/gatk-package-4.0.1.1-local.jar GenotypeGVCFs -ploidy 1 -R references/359488/genome_fasta.fasta --annotate-with-num-discovered-alleles true --annotations-to-exclude InbreedingCoeff -V gendb://typing/gatk_gvcfs/full_genome/359488/bwa/genomics_db -O typing/gatk_gvcfs/full_genome/359488/bwa/all_samples.vcf; ```; In between the last ProgressMeter and the Shutting down of the engine, I see the java process still running with top. Do you know what could be causing the problem ? Could it be related to -ERC BP_RESOLUTION ? I used to use -ERC GVCF before but I would rather keep the information of the coverage for post filtering, and I am not sure how to use --GVCFGQBands to match my criteria for coverage filtering. Thanks a lot for your help !. Edit: sorry with the latest version of gatk I get a new message error :; ```; 08:22:54.446 INFO ProgressMeter - NC_016854.1:20000 0.2 20000 87450.8; 08:23:04.942 INFO ProgressMeter - NC_016854.1:58000 0.4 58000 143694.8; 08:25:25.155 INFO ProgressMeter - NC_016854.1:82000 2.7 82000 29921.4; 08:25:35.161 INFO ProgressMeter - NC_016854.1:100000 2.9 100000 34396.6; 08:28:02.395 INFO ProgressMeter - NC_016854.1:102000 5.4 102000 19025.7; 08:28:13.248 INFO ProgressMeter - NC_016854.1:140000 5.5 140000 25261.3; 08:28:24.027 INFO ProgressMeter - NC_016854.1:175000 5.7 175000 30585.2; 08:46:13.574 INFO GenotypeGVCFs - Shutting down engine; GENOMICSDB_TIMER,GenomicsDB iterator next() timer,Wall-clock time(s),29.232685148998623,Cpu time(s),29.09919726900138; [February 28, 2018 8:46:13 AM UTC] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 23.59 minutes.; Runtime.totalMemory()=5588910080; Exception in thread ""main"" java.lang.OutOfMemoryError: GC overhead limit exceeded; 	at org.broadinstitute.hellbender.tool",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4467
https://github.com/broadinstitute/gatk/issues/4467:5363,Integrability,wrap,wrapAndCopyInto,5363,ngEngine.calculateGenotypes(GenotypingEngine.java:210); 	at org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs.calculateGenotypes(GenotypeGVCFs.java:266); 	at org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs.regenotypeVC(GenotypeGVCFs.java:222); 	at org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs.apply(GenotypeGVCFs.java:201); 	at org.broadinstitute.hellbender.engine.VariantWalkerBase.lambda$traverse$0(VariantWalkerBase.java:110); 	at org.broadinstitute.hellbender.engine.VariantWalkerBase$$Lambda$82/1457352442.accept(Unknown Source); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); 	at org.broadinstitute.hellbender.engine.VariantWalkerBase.traverse(VariantWalkerBase.java:108); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:893); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:135); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:180); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:199); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:159); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:201); 	at org.broadi,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4467
https://github.com/broadinstitute/gatk/pull/4472:657,Deployability,Update,Updated,657,"Added in argument for MAF out.; Added more ""required"" MAF fields.; Added Funcotation::getDataSourceName; Added contig name overrides for hg19 VS B37; Added in code to convert VCF INDEL positions to MAF INDEL positions.; Added start/stop positions for IGRs.; Added argument to ignore filtered variants at the front of processing to save time.; Added a script to fully retrieve the COSMIC data sources. Fixed how the MafOutputRenderer handles mapping fields to values.; Fixed a bug in LocatableXsv and COSMIC parsers (was missing name and; version).; In Gencode: Now TumorSeqAllele1 is the refAllele, not the AltAllele.; Fixed some problems with VCF output.; Updated VCF outputs to have better header info.; Refactored header output for OutputRenderers.; Changed the logic for creating alt protein sequences.; Fixed a bug in the LocatableXsvFuncotationFactory that caused annotations to be incorrectly associated with a factory.; Fixed several bugs in the GencodeFuncotationFactory.; Fixed bugx in the handling of UTR variants.; Fixed the Transcript Selection Mode ordering.; Fixed an issue with splice sites. Minor speed fix to GencodeFuncotationFactory. Now CosmicFuncotationFactory opens the database in read-only mode. Bugfix - now LocatableXsvFuncotationFactories use overrides. Now the reference should properly align with ALL indels regardless of; length. ReferenceContext now always rendered on + strand. Now will create funcotations for transcripts without fasta sequences. Minor changes to FuncotatorIntegrationTest. - Added in more integration test files. These are as yet unused - must; refactor the files themselves to actually reflect what should be correct; as far as produced funcotations. - Updated LocatableXsvFuncotationFactoryUnitTest.java and SimpleKeyXsvFuncotationFactoryUnitTest.java; to reflect the change to funcotation factories to always produce the; expected funcotations (rather than only producing funcotations when; there are data that match the target variant). Fixed an",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4472
https://github.com/broadinstitute/gatk/pull/4472:1541,Deployability,integrat,integration,1541,"Added contig name overrides for hg19 VS B37; Added in code to convert VCF INDEL positions to MAF INDEL positions.; Added start/stop positions for IGRs.; Added argument to ignore filtered variants at the front of processing to save time.; Added a script to fully retrieve the COSMIC data sources. Fixed how the MafOutputRenderer handles mapping fields to values.; Fixed a bug in LocatableXsv and COSMIC parsers (was missing name and; version).; In Gencode: Now TumorSeqAllele1 is the refAllele, not the AltAllele.; Fixed some problems with VCF output.; Updated VCF outputs to have better header info.; Refactored header output for OutputRenderers.; Changed the logic for creating alt protein sequences.; Fixed a bug in the LocatableXsvFuncotationFactory that caused annotations to be incorrectly associated with a factory.; Fixed several bugs in the GencodeFuncotationFactory.; Fixed bugx in the handling of UTR variants.; Fixed the Transcript Selection Mode ordering.; Fixed an issue with splice sites. Minor speed fix to GencodeFuncotationFactory. Now CosmicFuncotationFactory opens the database in read-only mode. Bugfix - now LocatableXsvFuncotationFactories use overrides. Now the reference should properly align with ALL indels regardless of; length. ReferenceContext now always rendered on + strand. Now will create funcotations for transcripts without fasta sequences. Minor changes to FuncotatorIntegrationTest. - Added in more integration test files. These are as yet unused - must; refactor the files themselves to actually reflect what should be correct; as far as produced funcotations. - Updated LocatableXsvFuncotationFactoryUnitTest.java and SimpleKeyXsvFuncotationFactoryUnitTest.java; to reflect the change to funcotation factories to always produce the; expected funcotations (rather than only producing funcotations when; there are data that match the target variant). Fixed an issue with the new VariantClassification code. Fixed issue #4410. Fixed #4022. Fixed #4420. Fixed #3922",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4472
https://github.com/broadinstitute/gatk/pull/4472:1706,Deployability,Update,Updated,1706,"Added contig name overrides for hg19 VS B37; Added in code to convert VCF INDEL positions to MAF INDEL positions.; Added start/stop positions for IGRs.; Added argument to ignore filtered variants at the front of processing to save time.; Added a script to fully retrieve the COSMIC data sources. Fixed how the MafOutputRenderer handles mapping fields to values.; Fixed a bug in LocatableXsv and COSMIC parsers (was missing name and; version).; In Gencode: Now TumorSeqAllele1 is the refAllele, not the AltAllele.; Fixed some problems with VCF output.; Updated VCF outputs to have better header info.; Refactored header output for OutputRenderers.; Changed the logic for creating alt protein sequences.; Fixed a bug in the LocatableXsvFuncotationFactory that caused annotations to be incorrectly associated with a factory.; Fixed several bugs in the GencodeFuncotationFactory.; Fixed bugx in the handling of UTR variants.; Fixed the Transcript Selection Mode ordering.; Fixed an issue with splice sites. Minor speed fix to GencodeFuncotationFactory. Now CosmicFuncotationFactory opens the database in read-only mode. Bugfix - now LocatableXsvFuncotationFactories use overrides. Now the reference should properly align with ALL indels regardless of; length. ReferenceContext now always rendered on + strand. Now will create funcotations for transcripts without fasta sequences. Minor changes to FuncotatorIntegrationTest. - Added in more integration test files. These are as yet unused - must; refactor the files themselves to actually reflect what should be correct; as far as produced funcotations. - Updated LocatableXsvFuncotationFactoryUnitTest.java and SimpleKeyXsvFuncotationFactoryUnitTest.java; to reflect the change to funcotation factories to always produce the; expected funcotations (rather than only producing funcotations when; there are data that match the target variant). Fixed an issue with the new VariantClassification code. Fixed issue #4410. Fixed #4022. Fixed #4420. Fixed #3922",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4472
https://github.com/broadinstitute/gatk/pull/4472:1541,Integrability,integrat,integration,1541,"Added contig name overrides for hg19 VS B37; Added in code to convert VCF INDEL positions to MAF INDEL positions.; Added start/stop positions for IGRs.; Added argument to ignore filtered variants at the front of processing to save time.; Added a script to fully retrieve the COSMIC data sources. Fixed how the MafOutputRenderer handles mapping fields to values.; Fixed a bug in LocatableXsv and COSMIC parsers (was missing name and; version).; In Gencode: Now TumorSeqAllele1 is the refAllele, not the AltAllele.; Fixed some problems with VCF output.; Updated VCF outputs to have better header info.; Refactored header output for OutputRenderers.; Changed the logic for creating alt protein sequences.; Fixed a bug in the LocatableXsvFuncotationFactory that caused annotations to be incorrectly associated with a factory.; Fixed several bugs in the GencodeFuncotationFactory.; Fixed bugx in the handling of UTR variants.; Fixed the Transcript Selection Mode ordering.; Fixed an issue with splice sites. Minor speed fix to GencodeFuncotationFactory. Now CosmicFuncotationFactory opens the database in read-only mode. Bugfix - now LocatableXsvFuncotationFactories use overrides. Now the reference should properly align with ALL indels regardless of; length. ReferenceContext now always rendered on + strand. Now will create funcotations for transcripts without fasta sequences. Minor changes to FuncotatorIntegrationTest. - Added in more integration test files. These are as yet unused - must; refactor the files themselves to actually reflect what should be correct; as far as produced funcotations. - Updated LocatableXsvFuncotationFactoryUnitTest.java and SimpleKeyXsvFuncotationFactoryUnitTest.java; to reflect the change to funcotation factories to always produce the; expected funcotations (rather than only producing funcotations when; there are data that match the target variant). Fixed an issue with the new VariantClassification code. Fixed issue #4410. Fixed #4022. Fixed #4420. Fixed #3922",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4472
https://github.com/broadinstitute/gatk/pull/4472:706,Modifiability,Refactor,Refactored,706,"Added in argument for MAF out.; Added more ""required"" MAF fields.; Added Funcotation::getDataSourceName; Added contig name overrides for hg19 VS B37; Added in code to convert VCF INDEL positions to MAF INDEL positions.; Added start/stop positions for IGRs.; Added argument to ignore filtered variants at the front of processing to save time.; Added a script to fully retrieve the COSMIC data sources. Fixed how the MafOutputRenderer handles mapping fields to values.; Fixed a bug in LocatableXsv and COSMIC parsers (was missing name and; version).; In Gencode: Now TumorSeqAllele1 is the refAllele, not the AltAllele.; Fixed some problems with VCF output.; Updated VCF outputs to have better header info.; Refactored header output for OutputRenderers.; Changed the logic for creating alt protein sequences.; Fixed a bug in the LocatableXsvFuncotationFactory that caused annotations to be incorrectly associated with a factory.; Fixed several bugs in the GencodeFuncotationFactory.; Fixed bugx in the handling of UTR variants.; Fixed the Transcript Selection Mode ordering.; Fixed an issue with splice sites. Minor speed fix to GencodeFuncotationFactory. Now CosmicFuncotationFactory opens the database in read-only mode. Bugfix - now LocatableXsvFuncotationFactories use overrides. Now the reference should properly align with ALL indels regardless of; length. ReferenceContext now always rendered on + strand. Now will create funcotations for transcripts without fasta sequences. Minor changes to FuncotatorIntegrationTest. - Added in more integration test files. These are as yet unused - must; refactor the files themselves to actually reflect what should be correct; as far as produced funcotations. - Updated LocatableXsvFuncotationFactoryUnitTest.java and SimpleKeyXsvFuncotationFactoryUnitTest.java; to reflect the change to funcotation factories to always produce the; expected funcotations (rather than only producing funcotations when; there are data that match the target variant). Fixed an",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4472
https://github.com/broadinstitute/gatk/pull/4472:1597,Modifiability,refactor,refactor,1597,"Added contig name overrides for hg19 VS B37; Added in code to convert VCF INDEL positions to MAF INDEL positions.; Added start/stop positions for IGRs.; Added argument to ignore filtered variants at the front of processing to save time.; Added a script to fully retrieve the COSMIC data sources. Fixed how the MafOutputRenderer handles mapping fields to values.; Fixed a bug in LocatableXsv and COSMIC parsers (was missing name and; version).; In Gencode: Now TumorSeqAllele1 is the refAllele, not the AltAllele.; Fixed some problems with VCF output.; Updated VCF outputs to have better header info.; Refactored header output for OutputRenderers.; Changed the logic for creating alt protein sequences.; Fixed a bug in the LocatableXsvFuncotationFactory that caused annotations to be incorrectly associated with a factory.; Fixed several bugs in the GencodeFuncotationFactory.; Fixed bugx in the handling of UTR variants.; Fixed the Transcript Selection Mode ordering.; Fixed an issue with splice sites. Minor speed fix to GencodeFuncotationFactory. Now CosmicFuncotationFactory opens the database in read-only mode. Bugfix - now LocatableXsvFuncotationFactories use overrides. Now the reference should properly align with ALL indels regardless of; length. ReferenceContext now always rendered on + strand. Now will create funcotations for transcripts without fasta sequences. Minor changes to FuncotatorIntegrationTest. - Added in more integration test files. These are as yet unused - must; refactor the files themselves to actually reflect what should be correct; as far as produced funcotations. - Updated LocatableXsvFuncotationFactoryUnitTest.java and SimpleKeyXsvFuncotationFactoryUnitTest.java; to reflect the change to funcotation factories to always produce the; expected funcotations (rather than only producing funcotations when; there are data that match the target variant). Fixed an issue with the new VariantClassification code. Fixed issue #4410. Fixed #4022. Fixed #4420. Fixed #3922",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4472
https://github.com/broadinstitute/gatk/pull/4472:765,Testability,log,logic,765,"Added in argument for MAF out.; Added more ""required"" MAF fields.; Added Funcotation::getDataSourceName; Added contig name overrides for hg19 VS B37; Added in code to convert VCF INDEL positions to MAF INDEL positions.; Added start/stop positions for IGRs.; Added argument to ignore filtered variants at the front of processing to save time.; Added a script to fully retrieve the COSMIC data sources. Fixed how the MafOutputRenderer handles mapping fields to values.; Fixed a bug in LocatableXsv and COSMIC parsers (was missing name and; version).; In Gencode: Now TumorSeqAllele1 is the refAllele, not the AltAllele.; Fixed some problems with VCF output.; Updated VCF outputs to have better header info.; Refactored header output for OutputRenderers.; Changed the logic for creating alt protein sequences.; Fixed a bug in the LocatableXsvFuncotationFactory that caused annotations to be incorrectly associated with a factory.; Fixed several bugs in the GencodeFuncotationFactory.; Fixed bugx in the handling of UTR variants.; Fixed the Transcript Selection Mode ordering.; Fixed an issue with splice sites. Minor speed fix to GencodeFuncotationFactory. Now CosmicFuncotationFactory opens the database in read-only mode. Bugfix - now LocatableXsvFuncotationFactories use overrides. Now the reference should properly align with ALL indels regardless of; length. ReferenceContext now always rendered on + strand. Now will create funcotations for transcripts without fasta sequences. Minor changes to FuncotatorIntegrationTest. - Added in more integration test files. These are as yet unused - must; refactor the files themselves to actually reflect what should be correct; as far as produced funcotations. - Updated LocatableXsvFuncotationFactoryUnitTest.java and SimpleKeyXsvFuncotationFactoryUnitTest.java; to reflect the change to funcotation factories to always produce the; expected funcotations (rather than only producing funcotations when; there are data that match the target variant). Fixed an",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4472
https://github.com/broadinstitute/gatk/pull/4472:1553,Testability,test,test,1553,"Added contig name overrides for hg19 VS B37; Added in code to convert VCF INDEL positions to MAF INDEL positions.; Added start/stop positions for IGRs.; Added argument to ignore filtered variants at the front of processing to save time.; Added a script to fully retrieve the COSMIC data sources. Fixed how the MafOutputRenderer handles mapping fields to values.; Fixed a bug in LocatableXsv and COSMIC parsers (was missing name and; version).; In Gencode: Now TumorSeqAllele1 is the refAllele, not the AltAllele.; Fixed some problems with VCF output.; Updated VCF outputs to have better header info.; Refactored header output for OutputRenderers.; Changed the logic for creating alt protein sequences.; Fixed a bug in the LocatableXsvFuncotationFactory that caused annotations to be incorrectly associated with a factory.; Fixed several bugs in the GencodeFuncotationFactory.; Fixed bugx in the handling of UTR variants.; Fixed the Transcript Selection Mode ordering.; Fixed an issue with splice sites. Minor speed fix to GencodeFuncotationFactory. Now CosmicFuncotationFactory opens the database in read-only mode. Bugfix - now LocatableXsvFuncotationFactories use overrides. Now the reference should properly align with ALL indels regardless of; length. ReferenceContext now always rendered on + strand. Now will create funcotations for transcripts without fasta sequences. Minor changes to FuncotatorIntegrationTest. - Added in more integration test files. These are as yet unused - must; refactor the files themselves to actually reflect what should be correct; as far as produced funcotations. - Updated LocatableXsvFuncotationFactoryUnitTest.java and SimpleKeyXsvFuncotationFactoryUnitTest.java; to reflect the change to funcotation factories to always produce the; expected funcotations (rather than only producing funcotations when; there are data that match the target variant). Fixed an issue with the new VariantClassification code. Fixed issue #4410. Fixed #4022. Fixed #4420. Fixed #3922",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4472
https://github.com/broadinstitute/gatk/pull/4472:1762,Usability,Simpl,SimpleKeyXsvFuncotationFactoryUnitTest,1762,"Added contig name overrides for hg19 VS B37; Added in code to convert VCF INDEL positions to MAF INDEL positions.; Added start/stop positions for IGRs.; Added argument to ignore filtered variants at the front of processing to save time.; Added a script to fully retrieve the COSMIC data sources. Fixed how the MafOutputRenderer handles mapping fields to values.; Fixed a bug in LocatableXsv and COSMIC parsers (was missing name and; version).; In Gencode: Now TumorSeqAllele1 is the refAllele, not the AltAllele.; Fixed some problems with VCF output.; Updated VCF outputs to have better header info.; Refactored header output for OutputRenderers.; Changed the logic for creating alt protein sequences.; Fixed a bug in the LocatableXsvFuncotationFactory that caused annotations to be incorrectly associated with a factory.; Fixed several bugs in the GencodeFuncotationFactory.; Fixed bugx in the handling of UTR variants.; Fixed the Transcript Selection Mode ordering.; Fixed an issue with splice sites. Minor speed fix to GencodeFuncotationFactory. Now CosmicFuncotationFactory opens the database in read-only mode. Bugfix - now LocatableXsvFuncotationFactories use overrides. Now the reference should properly align with ALL indels regardless of; length. ReferenceContext now always rendered on + strand. Now will create funcotations for transcripts without fasta sequences. Minor changes to FuncotatorIntegrationTest. - Added in more integration test files. These are as yet unused - must; refactor the files themselves to actually reflect what should be correct; as far as produced funcotations. - Updated LocatableXsvFuncotationFactoryUnitTest.java and SimpleKeyXsvFuncotationFactoryUnitTest.java; to reflect the change to funcotation factories to always produce the; expected funcotations (rather than only producing funcotations when; there are data that match the target variant). Fixed an issue with the new VariantClassification code. Fixed issue #4410. Fixed #4022. Fixed #4420. Fixed #3922",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4472
https://github.com/broadinstitute/gatk/pull/4474:155,Integrability,interface,interface,155,"Can you review this one @droazen - this is the last part of the temporary solution for #3998 (mentioned on ##4468). I think that in this case, because the interface is just a marker, it is easier to just modify `GATKDocOnlyArgumentCollection` for doc-only top level arguments and does not modify the API. Thanks!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4474
https://github.com/broadinstitute/gatk/issues/4475:161,Availability,FAILURE,FAILURE,161,"Issue while Building GATK4. sudo ./gradlew bundle; Starting a Gradle Daemon, 1 incompatible and 1 stopped Daemons could not be reused, use --status for details. FAILURE: Build failed with an exception. * Where:; Build file '/home/rafay/gatk-4.0.2.0/build.gradle' line: 289. * What went wrong:; A problem occurred evaluating root project 'gatk'.; > Cannot find '.git' directory. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 24.349 secs",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4475
https://github.com/broadinstitute/gatk/issues/4475:485,Testability,log,log,485,"Issue while Building GATK4. sudo ./gradlew bundle; Starting a Gradle Daemon, 1 incompatible and 1 stopped Daemons could not be reused, use --status for details. FAILURE: Build failed with an exception. * Where:; Build file '/home/rafay/gatk-4.0.2.0/build.gradle' line: 289. * What went wrong:; A problem occurred evaluating root project 'gatk'.; > Cannot find '.git' directory. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 24.349 secs",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4475
https://github.com/broadinstitute/gatk/pull/4476:76,Integrability,wrap,wrapper,76,"There is currently an issue with spark stderr output if running through the wrapper script, this is a workaround to that.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4476
https://github.com/broadinstitute/gatk/issues/4479:147,Performance,Perform,Performance,147,"I don't know if it is an issue or I am doing something wrong, but I report my experience in case might be useful for GATK developers:. I took some Performance Analysis for the tool BQSRPipelineSpark (when GATK 4.0 was still in Beta, for understanding there was still `gatk-launch` as command to execute the tool), processing a Whole Exome Sequencing Genome of about 14 GB (obviously after applying FastqToSam and BwaAndMarkDuplicatesPipelineSpark) and it took about 70 minutes. I tried to use the same tool in the same VM, with the same input data and now takes 626,95 minutes (a considerable difference of execution time). Is it normal or am I doing something wrong?. To be sure of what I am saying, I re-executed the old version tool with `gatk-launch` and it takes 65 minutes for example",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4479
https://github.com/broadinstitute/gatk/issues/4480:446,Availability,redundant,redundant,446,"FeatureManager dynamically discovers all `FeatureInput` arguments in a tool by accepting a (pre-populated) CommandLineProgram object, which it then passes to static methods in Barclay. Those methods perform the same `@Argument`/`@ArgumentCollection` discovery already implemented by the parser, but using separate, out-of-date code paths that currently don't discover `@PositionalArguments` or plugin descriptor arguments. Rather than fixing the redundant code paths and static methods in Barclay, they can be eliminated and replaced with an instance method on the parser. Since FeatureManager already requires that the parser have been run on the tool, the parser already has the state necessary to just collect the results. However, this means that FeatureManager would require a CommandLineParser object instead of the tool itself. (Alternatively, we could extract the results from the parser and pass them in directly to FeatureManager instead of the parser). @droazen do you have any opinion on this before I refactor this part of the parser ?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4480
https://github.com/broadinstitute/gatk/issues/4480:394,Modifiability,plugin,plugin,394,"FeatureManager dynamically discovers all `FeatureInput` arguments in a tool by accepting a (pre-populated) CommandLineProgram object, which it then passes to static methods in Barclay. Those methods perform the same `@Argument`/`@ArgumentCollection` discovery already implemented by the parser, but using separate, out-of-date code paths that currently don't discover `@PositionalArguments` or plugin descriptor arguments. Rather than fixing the redundant code paths and static methods in Barclay, they can be eliminated and replaced with an instance method on the parser. Since FeatureManager already requires that the parser have been run on the tool, the parser already has the state necessary to just collect the results. However, this means that FeatureManager would require a CommandLineParser object instead of the tool itself. (Alternatively, we could extract the results from the parser and pass them in directly to FeatureManager instead of the parser). @droazen do you have any opinion on this before I refactor this part of the parser ?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4480
https://github.com/broadinstitute/gatk/issues/4480:1014,Modifiability,refactor,refactor,1014,"FeatureManager dynamically discovers all `FeatureInput` arguments in a tool by accepting a (pre-populated) CommandLineProgram object, which it then passes to static methods in Barclay. Those methods perform the same `@Argument`/`@ArgumentCollection` discovery already implemented by the parser, but using separate, out-of-date code paths that currently don't discover `@PositionalArguments` or plugin descriptor arguments. Rather than fixing the redundant code paths and static methods in Barclay, they can be eliminated and replaced with an instance method on the parser. Since FeatureManager already requires that the parser have been run on the tool, the parser already has the state necessary to just collect the results. However, this means that FeatureManager would require a CommandLineParser object instead of the tool itself. (Alternatively, we could extract the results from the parser and pass them in directly to FeatureManager instead of the parser). @droazen do you have any opinion on this before I refactor this part of the parser ?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4480
https://github.com/broadinstitute/gatk/issues/4480:199,Performance,perform,perform,199,"FeatureManager dynamically discovers all `FeatureInput` arguments in a tool by accepting a (pre-populated) CommandLineProgram object, which it then passes to static methods in Barclay. Those methods perform the same `@Argument`/`@ArgumentCollection` discovery already implemented by the parser, but using separate, out-of-date code paths that currently don't discover `@PositionalArguments` or plugin descriptor arguments. Rather than fixing the redundant code paths and static methods in Barclay, they can be eliminated and replaced with an instance method on the parser. Since FeatureManager already requires that the parser have been run on the tool, the parser already has the state necessary to just collect the results. However, this means that FeatureManager would require a CommandLineParser object instead of the tool itself. (Alternatively, we could extract the results from the parser and pass them in directly to FeatureManager instead of the parser). @droazen do you have any opinion on this before I refactor this part of the parser ?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4480
https://github.com/broadinstitute/gatk/issues/4480:446,Safety,redund,redundant,446,"FeatureManager dynamically discovers all `FeatureInput` arguments in a tool by accepting a (pre-populated) CommandLineProgram object, which it then passes to static methods in Barclay. Those methods perform the same `@Argument`/`@ArgumentCollection` discovery already implemented by the parser, but using separate, out-of-date code paths that currently don't discover `@PositionalArguments` or plugin descriptor arguments. Rather than fixing the redundant code paths and static methods in Barclay, they can be eliminated and replaced with an instance method on the parser. Since FeatureManager already requires that the parser have been run on the tool, the parser already has the state necessary to just collect the results. However, this means that FeatureManager would require a CommandLineParser object instead of the tool itself. (Alternatively, we could extract the results from the parser and pass them in directly to FeatureManager instead of the parser). @droazen do you have any opinion on this before I refactor this part of the parser ?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4480
https://github.com/broadinstitute/gatk/issues/4481:912,Security,hash,hashtags,912,"Just some thoughts for @samuelklee. For example, CollectFragmentCounts produces the following hybrid-type `@RG` line:. ![screenshot 2018-02-22 15 05 48](https://user-images.githubusercontent.com/11543866/36908820-66b90938-1e0a-11e8-8830-793ff3f71e96.png). ```; @RG ID:GATKCopyNumber SM:HCC1143_tumor; ```; Official format specifications are at https://samtools.github.io/hts-specs/. Let me briefly describe the #choices. ---; If we are to follow conventions used in the alignment world (SAM specs, for interval lists), then... We note data transformations using `@PG` program groups. These can be added successively to the same data file, given unique `@PG ID` fields, and collectively these lines showcase the history of data transformations for a dataset. The `@RG` group is reserved for lane level data and yes, does unify based on the sample or library. ---; If we examine VCFs, the convention is to use `#` hashtags to denote header rows (VCF specs). Double hashtags `##` denote all metadata lines and a single hashtag `#` denotes the line with the column labels. Here are some select rows from an M2 VCF header:; ```; ##fileformat=VCFv4.2; ##FILTER=<ID=artifact_in_normal,Description=""artifact_in_normal"">; ...; ##FORMAT=<ID=AD,Number=R,Type=Integer,Description=""Allelic depths for the ref and alt alleles in the order listed"">; ...; ##GATKCommandLine=<ID=FilterMutectCalls,CommandLine=""FilterMutectCalls...; ...; ##GATKCommandLine=<ID=Mutect2,CommandLine=""Mutect2 --tumor-sample HCC1143_tumor ...; ...; ##INFO=<ID=TLOD,Number=A,Type=Float,Description=""Tumor LOD score"">; ##Mutect Version=2.1-beta; ##command=FilterByOrientationBias --output hcc1143_T_clean-filtered.vcf...; ...; ##contig=<ID=chr1,length=248956422>; ##contig=<ID=chr2,length=242193529>; ...; ##contig=<ID=HLA-DRB1*16:02:01,length=11005>; ##filtering_status=These calls have been filtered by FilterMutectCalls to label false positives with a list of failed filters and true positives with PASS.; ##normal_sample=HCC1143_normal; #",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4481
https://github.com/broadinstitute/gatk/issues/4481:963,Security,hash,hashtags,963,"houghts for @samuelklee. For example, CollectFragmentCounts produces the following hybrid-type `@RG` line:. ![screenshot 2018-02-22 15 05 48](https://user-images.githubusercontent.com/11543866/36908820-66b90938-1e0a-11e8-8830-793ff3f71e96.png). ```; @RG ID:GATKCopyNumber SM:HCC1143_tumor; ```; Official format specifications are at https://samtools.github.io/hts-specs/. Let me briefly describe the #choices. ---; If we are to follow conventions used in the alignment world (SAM specs, for interval lists), then... We note data transformations using `@PG` program groups. These can be added successively to the same data file, given unique `@PG ID` fields, and collectively these lines showcase the history of data transformations for a dataset. The `@RG` group is reserved for lane level data and yes, does unify based on the sample or library. ---; If we examine VCFs, the convention is to use `#` hashtags to denote header rows (VCF specs). Double hashtags `##` denote all metadata lines and a single hashtag `#` denotes the line with the column labels. Here are some select rows from an M2 VCF header:; ```; ##fileformat=VCFv4.2; ##FILTER=<ID=artifact_in_normal,Description=""artifact_in_normal"">; ...; ##FORMAT=<ID=AD,Number=R,Type=Integer,Description=""Allelic depths for the ref and alt alleles in the order listed"">; ...; ##GATKCommandLine=<ID=FilterMutectCalls,CommandLine=""FilterMutectCalls...; ...; ##GATKCommandLine=<ID=Mutect2,CommandLine=""Mutect2 --tumor-sample HCC1143_tumor ...; ...; ##INFO=<ID=TLOD,Number=A,Type=Float,Description=""Tumor LOD score"">; ##Mutect Version=2.1-beta; ##command=FilterByOrientationBias --output hcc1143_T_clean-filtered.vcf...; ...; ##contig=<ID=chr1,length=248956422>; ##contig=<ID=chr2,length=242193529>; ...; ##contig=<ID=HLA-DRB1*16:02:01,length=11005>; ##filtering_status=These calls have been filtered by FilterMutectCalls to label false positives with a list of failed filters and true positives with PASS.; ##normal_sample=HCC1143_normal; ##orientatio",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4481
https://github.com/broadinstitute/gatk/issues/4481:1016,Security,hash,hashtag,1016,"houghts for @samuelklee. For example, CollectFragmentCounts produces the following hybrid-type `@RG` line:. ![screenshot 2018-02-22 15 05 48](https://user-images.githubusercontent.com/11543866/36908820-66b90938-1e0a-11e8-8830-793ff3f71e96.png). ```; @RG ID:GATKCopyNumber SM:HCC1143_tumor; ```; Official format specifications are at https://samtools.github.io/hts-specs/. Let me briefly describe the #choices. ---; If we are to follow conventions used in the alignment world (SAM specs, for interval lists), then... We note data transformations using `@PG` program groups. These can be added successively to the same data file, given unique `@PG ID` fields, and collectively these lines showcase the history of data transformations for a dataset. The `@RG` group is reserved for lane level data and yes, does unify based on the sample or library. ---; If we examine VCFs, the convention is to use `#` hashtags to denote header rows (VCF specs). Double hashtags `##` denote all metadata lines and a single hashtag `#` denotes the line with the column labels. Here are some select rows from an M2 VCF header:; ```; ##fileformat=VCFv4.2; ##FILTER=<ID=artifact_in_normal,Description=""artifact_in_normal"">; ...; ##FORMAT=<ID=AD,Number=R,Type=Integer,Description=""Allelic depths for the ref and alt alleles in the order listed"">; ...; ##GATKCommandLine=<ID=FilterMutectCalls,CommandLine=""FilterMutectCalls...; ...; ##GATKCommandLine=<ID=Mutect2,CommandLine=""Mutect2 --tumor-sample HCC1143_tumor ...; ...; ##INFO=<ID=TLOD,Number=A,Type=Float,Description=""Tumor LOD score"">; ##Mutect Version=2.1-beta; ##command=FilterByOrientationBias --output hcc1143_T_clean-filtered.vcf...; ...; ##contig=<ID=chr1,length=248956422>; ##contig=<ID=chr2,length=242193529>; ...; ##contig=<ID=HLA-DRB1*16:02:01,length=11005>; ##filtering_status=These calls have been filtered by FilterMutectCalls to label false positives with a list of failed filters and true positives with PASS.; ##normal_sample=HCC1143_normal; ##orientatio",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4481
https://github.com/broadinstitute/gatk/issues/4482:125,Availability,Error,Error,125,@thebkaufman1995 encountered the following warning when trying to use TSV count files in the gCNV pipeline:. ```; HDF5-DIAG: Error detected in HDF5 (1.8.14) thread 0:; #000: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5F.c line 604 in H5Fopen(): unable to open file; major: File accessibilty; minor: Unable to open file; #001: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Fint.c line 1085 in H5F_open(): unable to read superblock; major: File accessibilty; minor: Read failed; #002: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Fsuper.c line 277 in H5F_super_read(): file signature not found; major: File accessibilty; minor: Not an HDF5 file; ```. My guess is this is because we first try to open counts files as HDF5 and then fall back to TSV (catching the relevant exception). Perhaps slightly different versions of the HDF5 library result in these warnings being emitted.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4482
https://github.com/broadinstitute/gatk/issues/4482:98,Deployability,pipeline,pipeline,98,@thebkaufman1995 encountered the following warning when trying to use TSV count files in the gCNV pipeline:. ```; HDF5-DIAG: Error detected in HDF5 (1.8.14) thread 0:; #000: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5F.c line 604 in H5Fopen(): unable to open file; major: File accessibilty; minor: Unable to open file; #001: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Fint.c line 1085 in H5F_open(): unable to read superblock; major: File accessibilty; minor: Read failed; #002: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Fsuper.c line 277 in H5F_super_read(): file signature not found; major: File accessibilty; minor: Not an HDF5 file; ```. My guess is this is because we first try to open counts files as HDF5 and then fall back to TSV (catching the relevant exception). Perhaps slightly different versions of the HDF5 library result in these warnings being emitted.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4482
https://github.com/broadinstitute/gatk/issues/4482:131,Safety,detect,detected,131,@thebkaufman1995 encountered the following warning when trying to use TSV count files in the gCNV pipeline:. ```; HDF5-DIAG: Error detected in HDF5 (1.8.14) thread 0:; #000: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5F.c line 604 in H5Fopen(): unable to open file; major: File accessibilty; minor: Unable to open file; #001: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Fint.c line 1085 in H5F_open(): unable to read superblock; major: File accessibilty; minor: Read failed; #002: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Fsuper.c line 277 in H5F_super_read(): file signature not found; major: File accessibilty; minor: Not an HDF5 file; ```. My guess is this is because we first try to open counts files as HDF5 and then fall back to TSV (catching the relevant exception). Perhaps slightly different versions of the HDF5 library result in these warnings being emitted.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4482
https://github.com/broadinstitute/gatk/issues/4482:307,Security,access,accessibilty,307,@thebkaufman1995 encountered the following warning when trying to use TSV count files in the gCNV pipeline:. ```; HDF5-DIAG: Error detected in HDF5 (1.8.14) thread 0:; #000: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5F.c line 604 in H5Fopen(): unable to open file; major: File accessibilty; minor: Unable to open file; #001: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Fint.c line 1085 in H5F_open(): unable to read superblock; major: File accessibilty; minor: Read failed; #002: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Fsuper.c line 277 in H5F_super_read(): file signature not found; major: File accessibilty; minor: Not an HDF5 file; ```. My guess is this is because we first try to open counts files as HDF5 and then fall back to TSV (catching the relevant exception). Perhaps slightly different versions of the HDF5 library result in these warnings being emitted.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4482
https://github.com/broadinstitute/gatk/issues/4482:499,Security,access,accessibilty,499,@thebkaufman1995 encountered the following warning when trying to use TSV count files in the gCNV pipeline:. ```; HDF5-DIAG: Error detected in HDF5 (1.8.14) thread 0:; #000: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5F.c line 604 in H5Fopen(): unable to open file; major: File accessibilty; minor: Unable to open file; #001: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Fint.c line 1085 in H5F_open(): unable to read superblock; major: File accessibilty; minor: Read failed; #002: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Fsuper.c line 277 in H5F_super_read(): file signature not found; major: File accessibilty; minor: Not an HDF5 file; ```. My guess is this is because we first try to open counts files as HDF5 and then fall back to TSV (catching the relevant exception). Perhaps slightly different versions of the HDF5 library result in these warnings being emitted.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4482
https://github.com/broadinstitute/gatk/issues/4482:689,Security,access,accessibilty,689,@thebkaufman1995 encountered the following warning when trying to use TSV count files in the gCNV pipeline:. ```; HDF5-DIAG: Error detected in HDF5 (1.8.14) thread 0:; #000: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5F.c line 604 in H5Fopen(): unable to open file; major: File accessibilty; minor: Unable to open file; #001: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Fint.c line 1085 in H5F_open(): unable to read superblock; major: File accessibilty; minor: Read failed; #002: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Fsuper.c line 277 in H5F_super_read(): file signature not found; major: File accessibilty; minor: Not an HDF5 file; ```. My guess is this is because we first try to open counts files as HDF5 and then fall back to TSV (catching the relevant exception). Perhaps slightly different versions of the HDF5 library result in these warnings being emitted.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4482
https://github.com/broadinstitute/gatk/issues/4483:319,Modifiability,refactor,refactor,319,"Currently, `StandardCallerArgumentCollection`, which contains the `-contamination-file` argument, requires that the contamination file be loaded externally, and its contents then passed back in to the argument collection via `setSampleContamination()`. This is rather poor design, and an invitation for bugs. We should refactor so that `StandardCallerArgumentCollection` handles the loading of the contamination file internally, and remove the `setSampleContamination()` method.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4483
https://github.com/broadinstitute/gatk/issues/4483:138,Performance,load,loaded,138,"Currently, `StandardCallerArgumentCollection`, which contains the `-contamination-file` argument, requires that the contamination file be loaded externally, and its contents then passed back in to the argument collection via `setSampleContamination()`. This is rather poor design, and an invitation for bugs. We should refactor so that `StandardCallerArgumentCollection` handles the loading of the contamination file internally, and remove the `setSampleContamination()` method.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4483
https://github.com/broadinstitute/gatk/issues/4483:383,Performance,load,loading,383,"Currently, `StandardCallerArgumentCollection`, which contains the `-contamination-file` argument, requires that the contamination file be loaded externally, and its contents then passed back in to the argument collection via `setSampleContamination()`. This is rather poor design, and an invitation for bugs. We should refactor so that `StandardCallerArgumentCollection` handles the loading of the contamination file internally, and remove the `setSampleContamination()` method.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4483
https://github.com/broadinstitute/gatk/issues/4484:27,Modifiability,config,config,27,"We should rely on the GATK config file for the default values for these system properties, rather than hardcoding them into the `gatk` frontend script.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4484
https://github.com/broadinstitute/gatk/issues/4486:165,Availability,error,error,165,Currently if the number of headers do not match the number of rows it manifests as:`AssertionError: Some contigs do not have ploidy priors`. It would be good if the error message could more explicitly state how the ploidy prior file is malformed. In the event where it's actually the case it might also be worth explicitly stating which contig was found not to have a ploidy prior.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4486
https://github.com/broadinstitute/gatk/issues/4486:171,Integrability,message,message,171,Currently if the number of headers do not match the number of rows it manifests as:`AssertionError: Some contigs do not have ploidy priors`. It would be good if the error message could more explicitly state how the ploidy prior file is malformed. In the event where it's actually the case it might also be worth explicitly stating which contig was found not to have a ploidy prior.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4486
https://github.com/broadinstitute/gatk/issues/4486:84,Testability,Assert,AssertionError,84,Currently if the number of headers do not match the number of rows it manifests as:`AssertionError: Some contigs do not have ploidy priors`. It would be good if the error message could more explicitly state how the ploidy prior file is malformed. In the event where it's actually the case it might also be worth explicitly stating which contig was found not to have a ploidy prior.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4486
https://github.com/broadinstitute/gatk/issues/4487:831,Usability,simpl,simply,831,"A user originally reported a `RuntimeIOException: Attempt to add record to closed writer in SplitNCigarReads`. It seems setting TMP_DIR helps even though user had Djava.io.tmpdir set. Is this expected? The user post below should help as well. ----; User Report; ----. Hi @Sheila and @gerzs,; I figured out the issue (at least for me). It stems from where SplitNCigarReads is writing the temporary files. For me, it's writing them to the cluster which has very limited disk space. When I redirected this using `--TMP_DIR /my/scratch/space` everything went smoothly. The part that still confuses me is that I had already set `export _JAVA_OPTIONS=-Djava.io.tmpdir=/my/scratch/space`. This is not getting picked up by SplitNCigarReads in GATK4 as I would have expected. After much experimenting I started with a clean environment and simply set `--TMP_DIR /my/scratch/space` only which worked. . This seems a bit ""buggy"" to me and it would be great if the GATK development team could look into it and pass `Djava.io.tmpdir` to `--TMP_DIR` if possible. Thanks,. Stephen . This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/comment/46418#Comment_46418",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4487
https://github.com/broadinstitute/gatk/issues/4488:33,Deployability,pipeline,pipeline,33,version: gatk 4.0.2.1; I use the pipeline :BwaAndMarkDuplicatesPipelineSpark-BQSRPipelineSpark-HaplotypeCallerSparkand I get the bad resultby testingHaplotypeCallerSpark lose a lot of variable sites and HaplotypeCallerSpark 'result jitter to the same input bam,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4488
https://github.com/broadinstitute/gatk/issues/4488:189,Modifiability,variab,variable,189,version: gatk 4.0.2.1; I use the pipeline :BwaAndMarkDuplicatesPipelineSpark-BQSRPipelineSpark-HaplotypeCallerSparkand I get the bad resultby testingHaplotypeCallerSpark lose a lot of variable sites and HaplotypeCallerSpark 'result jitter to the same input bam,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4488
https://github.com/broadinstitute/gatk/issues/4488:146,Testability,test,testing,146,version: gatk 4.0.2.1; I use the pipeline :BwaAndMarkDuplicatesPipelineSpark-BQSRPipelineSpark-HaplotypeCallerSparkand I get the bad resultby testingHaplotypeCallerSpark lose a lot of variable sites and HaplotypeCallerSpark 'result jitter to the same input bam,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4488
https://github.com/broadinstitute/gatk/issues/4489:91,Modifiability,config,config,91,"Currently, the only way to specify locatable columns for a locatable xsv file is through a config file (and only a sibling config file if tribble compatibility is required). . In the future, we should have structured comments that specify the names of the locatable columns (i.e. contig, start, end). For example:; ```; # This is my annotated segment file; #_ContigHeader=CONTIG;; #_StartHeader=START;; #_EndHeader=END;; CONTIG START END Annotation1 Annotation2 ; 1 100 200 foo bar; ```. These comments should be parsed and the specified columns used. If the structured comments are not there, then fallback on a specified config file, then fallback a default config file in the jar itself.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4489
https://github.com/broadinstitute/gatk/issues/4489:123,Modifiability,config,config,123,"Currently, the only way to specify locatable columns for a locatable xsv file is through a config file (and only a sibling config file if tribble compatibility is required). . In the future, we should have structured comments that specify the names of the locatable columns (i.e. contig, start, end). For example:; ```; # This is my annotated segment file; #_ContigHeader=CONTIG;; #_StartHeader=START;; #_EndHeader=END;; CONTIG START END Annotation1 Annotation2 ; 1 100 200 foo bar; ```. These comments should be parsed and the specified columns used. If the structured comments are not there, then fallback on a specified config file, then fallback a default config file in the jar itself.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4489
https://github.com/broadinstitute/gatk/issues/4489:623,Modifiability,config,config,623,"Currently, the only way to specify locatable columns for a locatable xsv file is through a config file (and only a sibling config file if tribble compatibility is required). . In the future, we should have structured comments that specify the names of the locatable columns (i.e. contig, start, end). For example:; ```; # This is my annotated segment file; #_ContigHeader=CONTIG;; #_StartHeader=START;; #_EndHeader=END;; CONTIG START END Annotation1 Annotation2 ; 1 100 200 foo bar; ```. These comments should be parsed and the specified columns used. If the structured comments are not there, then fallback on a specified config file, then fallback a default config file in the jar itself.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4489
https://github.com/broadinstitute/gatk/issues/4489:660,Modifiability,config,config,660,"Currently, the only way to specify locatable columns for a locatable xsv file is through a config file (and only a sibling config file if tribble compatibility is required). . In the future, we should have structured comments that specify the names of the locatable columns (i.e. contig, start, end). For example:; ```; # This is my annotated segment file; #_ContigHeader=CONTIG;; #_StartHeader=START;; #_EndHeader=END;; CONTIG START END Annotation1 Annotation2 ; 1 100 200 foo bar; ```. These comments should be parsed and the specified columns used. If the structured comments are not there, then fallback on a specified config file, then fallback a default config file in the jar itself.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4489
https://github.com/broadinstitute/gatk/issues/4490:0,Deployability,Update,Update,0,Update the Owner version with the latest release from Maven.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4490
https://github.com/broadinstitute/gatk/issues/4490:41,Deployability,release,release,41,Update the Owner version with the latest release from Maven.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4490
https://github.com/broadinstitute/gatk/pull/4493:405,Deployability,configurat,configuration,405,"The current initialization action for dataproc workers puts the reference image in different places depending on whether or not an SSD is mounted. Preemptible dataproc workers don't have SSDs, so a mixed cluster will have references mounted on different paths depending on the worker. This change symlinks the SSD mount point onto the HDD so that paths can be consistent. . Also increases several cluster configuration parameters relating to retries, which I saw recommended if using preemptible workers.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4493
https://github.com/broadinstitute/gatk/pull/4493:100,Integrability,depend,depending,100,"The current initialization action for dataproc workers puts the reference image in different places depending on whether or not an SSD is mounted. Preemptible dataproc workers don't have SSDs, so a mixed cluster will have references mounted on different paths depending on the worker. This change symlinks the SSD mount point onto the HDD so that paths can be consistent. . Also increases several cluster configuration parameters relating to retries, which I saw recommended if using preemptible workers.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4493
https://github.com/broadinstitute/gatk/pull/4493:260,Integrability,depend,depending,260,"The current initialization action for dataproc workers puts the reference image in different places depending on whether or not an SSD is mounted. Preemptible dataproc workers don't have SSDs, so a mixed cluster will have references mounted on different paths depending on the worker. This change symlinks the SSD mount point onto the HDD so that paths can be consistent. . Also increases several cluster configuration parameters relating to retries, which I saw recommended if using preemptible workers.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4493
https://github.com/broadinstitute/gatk/pull/4493:405,Modifiability,config,configuration,405,"The current initialization action for dataproc workers puts the reference image in different places depending on whether or not an SSD is mounted. Preemptible dataproc workers don't have SSDs, so a mixed cluster will have references mounted on different paths depending on the worker. This change symlinks the SSD mount point onto the HDD so that paths can be consistent. . Also increases several cluster configuration parameters relating to retries, which I saw recommended if using preemptible workers.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4493
https://github.com/broadinstitute/gatk/pull/4495:351,Availability,down,downstream,351,"This is part of the effort to port GATK3 VariantEval to GATK4. Because VariantEval is a pretty big tool, I thought it would help to separate changes in the VariantEval package from core GATK changes. If you would prefer I could split this PR into several, based on what they target. However, here is a summary:. 1) I added a method to FeatureInput so downstream code and determine if the name is actually user-supplied or whether this is the default one assigned by GATK. 2) I split MultiVariantWalker into a base class that does not have arguments specified. This is comparable to what already exists for VariantWalkerBase - this is a pretty minor change. 3) I added a method to VariantWalkerBase to return the set of intervals potentially being iterated. The intent is to return either the full genome, or the user-supplied intervals. . 4) I restored some code to MendelianViolation that is used by components of VariantEval. This is very close to identical from GATK3. 5) Likewise to SampleDBBuilder (code had to be tweaked for GATK4). 6) IntervalUtils/MathUtils/Utils: I restored from GATK3 methods not ported to GATK4. These should be exact copies of GATK3 unless a change was needed. Once these are closed out, I will make a separate PR with VariantEval itself.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4495
https://github.com/broadinstitute/gatk/pull/4496:454,Modifiability,config,configured,454,"* Shows the flags needed for sites only query and for producing GT fields; * https://github.com/broadinstitute/gatk/issues/3688; * https://github.com/Intel-HLS/GenomicsDB/issues/161; * Currently, hard coded - need to discuss how these flags will be passed in.; * FYI after https://github.com/Intel-HLS/GenomicsDB/pull/165 is merged in, we will not need to have long argument lists for GenomicsDB. A Protobuf object will be the input parameter and can be configured as needed.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4496
https://github.com/broadinstitute/gatk/issues/4500:208,Testability,test,test,208,"The new model must. - [ ] Work for high depth low allele fraction site; - [ ] Handle the cases where both ref and alt reads are strand biased, not just alt reads; - [ ] Beat the model based on Fisher's exact test",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4500
https://github.com/broadinstitute/gatk/pull/4501:76,Integrability,wrap,wrapper,76,"There is currently an issue with spark stderr output if running through the wrapper script, this should make it a little clearer what spark is doing after it finishes with the tools work.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4501
https://github.com/broadinstitute/gatk/pull/4501:121,Usability,clear,clearer,121,"There is currently an issue with spark stderr output if running through the wrapper script, this should make it a little clearer what spark is doing after it finishes with the tools work.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4501
https://github.com/broadinstitute/gatk/pull/4503:127,Safety,avoid,avoid,127,"This will require less tuning of `minimum-interval-median-percentile` to filter out completely uncovered intervals, which will avoid e.g. https://gatkforums.broadinstitute.org/gatk/discussion/11461/gatk-4-0-1-2-no-non-zero-singular-values-were-found-in-creating-a-panel-of-normals-for-somatic-cnv. For example, currently if 10% of my intervals are completely uncovered and I set the relevant parameter to 5%, the 5th percentile is then equal to zero. However, because no intervals are strictly less than zero, none are filtered. Changing this to filter on equality then gets rid of all 10% of the intervals as one would want to do in practice.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4503
https://github.com/broadinstitute/gatk/issues/4504:94,Availability,error,error,94,"Hi . I used CollectAllelicCounts with a bed file for the `-I` parameter and got the following error message:. `A USER ERROR has occurred: Badly formed genome unclippedLoc: Parameters to GenomeLocParser are incorrect:The stop position 29430911 is less than start 29430912 in contig chr2`. The only entry in my bed file dealing with this position is the following:. `chr2	 29430911	29430911	ENSE00001695104	3	ENST00000431873	ENSG00000171094	ALK`. There seems to be a problem, that CollectAllelicCounts add 1 base to the start of each interval. I tried it with a padding of 10 (`-ip 10`) but did not get it to work. Am I on the wrong way? Isn't it a problem with the bed file?`; Thanks in advance; Stefan",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4504
https://github.com/broadinstitute/gatk/issues/4504:118,Availability,ERROR,ERROR,118,"Hi . I used CollectAllelicCounts with a bed file for the `-I` parameter and got the following error message:. `A USER ERROR has occurred: Badly formed genome unclippedLoc: Parameters to GenomeLocParser are incorrect:The stop position 29430911 is less than start 29430912 in contig chr2`. The only entry in my bed file dealing with this position is the following:. `chr2	 29430911	29430911	ENSE00001695104	3	ENST00000431873	ENSG00000171094	ALK`. There seems to be a problem, that CollectAllelicCounts add 1 base to the start of each interval. I tried it with a padding of 10 (`-ip 10`) but did not get it to work. Am I on the wrong way? Isn't it a problem with the bed file?`; Thanks in advance; Stefan",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4504
https://github.com/broadinstitute/gatk/issues/4504:100,Integrability,message,message,100,"Hi . I used CollectAllelicCounts with a bed file for the `-I` parameter and got the following error message:. `A USER ERROR has occurred: Badly formed genome unclippedLoc: Parameters to GenomeLocParser are incorrect:The stop position 29430911 is less than start 29430912 in contig chr2`. The only entry in my bed file dealing with this position is the following:. `chr2	 29430911	29430911	ENSE00001695104	3	ENST00000431873	ENSG00000171094	ALK`. There seems to be a problem, that CollectAllelicCounts add 1 base to the start of each interval. I tried it with a padding of 10 (`-ip 10`) but did not get it to work. Am I on the wrong way? Isn't it a problem with the bed file?`; Thanks in advance; Stefan",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4504
https://github.com/broadinstitute/gatk/pull/4505:548,Availability,error,error,548,"We should be able to use this single implementation to satisfy both https://github.com/broadinstitute/gatk/issues/4347 and https://github.com/broadinstitute/gatk/issues/4351. Questions: @vdauwera What other tools that were dropped from GATK3 should be added to the list now? GATK3 also has a deprecated annotations list, which is included here, but is empty. Are there any annotations that should be listed ? I can't really implement/test that part unless I populate it with something. Also, trying to run a missing tool is currently handled as an error, and surfaces in the context of a usage message. Perhaps that hides it too much:. <img width=""918"" alt=""screen shot 2018-03-07 at 11 25 40 am"" src=""https://user-images.githubusercontent.com/10062863/37104403-a15635ae-21fa-11e8-985a-94ff0e203cf8.png"">",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4505
https://github.com/broadinstitute/gatk/pull/4505:594,Integrability,message,message,594,"We should be able to use this single implementation to satisfy both https://github.com/broadinstitute/gatk/issues/4347 and https://github.com/broadinstitute/gatk/issues/4351. Questions: @vdauwera What other tools that were dropped from GATK3 should be added to the list now? GATK3 also has a deprecated annotations list, which is included here, but is empty. Are there any annotations that should be listed ? I can't really implement/test that part unless I populate it with something. Also, trying to run a missing tool is currently handled as an error, and surfaces in the context of a usage message. Perhaps that hides it too much:. <img width=""918"" alt=""screen shot 2018-03-07 at 11 25 40 am"" src=""https://user-images.githubusercontent.com/10062863/37104403-a15635ae-21fa-11e8-985a-94ff0e203cf8.png"">",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4505
https://github.com/broadinstitute/gatk/pull/4505:434,Testability,test,test,434,"We should be able to use this single implementation to satisfy both https://github.com/broadinstitute/gatk/issues/4347 and https://github.com/broadinstitute/gatk/issues/4351. Questions: @vdauwera What other tools that were dropped from GATK3 should be added to the list now? GATK3 also has a deprecated annotations list, which is included here, but is empty. Are there any annotations that should be listed ? I can't really implement/test that part unless I populate it with something. Also, trying to run a missing tool is currently handled as an error, and surfaces in the context of a usage message. Perhaps that hides it too much:. <img width=""918"" alt=""screen shot 2018-03-07 at 11 25 40 am"" src=""https://user-images.githubusercontent.com/10062863/37104403-a15635ae-21fa-11e8-985a-94ff0e203cf8.png"">",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4505
https://github.com/broadinstitute/gatk/issues/4506:85,Availability,error,error,85,"Should the CreateHadoopBamSplittingIndex tool also work on a cram? I am getting this error below which suggests not. What are the benefits of a SplittingIndex for a spark job? On average-how long should it take a spark job to get the splits for a 30x bam or cram? . ```; gatk CreateHadoopBamSplittingIndex -I adni/cram/ADNI_002_S_0413.hg38.realign.bqsr.cram; Using GATK jar /share/pkg/gatk/4.0.1.1/install/bin/gatk-package-4.0.1.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -jar /share/pkg/gatk/4.0.1.1/install/bin/gatk-package-4.0.1.1-local.jar CreateHadoopBamSplittingIndex -I adni/cram/ADNI_002_S_0413.hg38.realign.bqsr.cram; 11:47:53.243 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg/gatk/4.0.1.1/install/bin/gatk-package-4.0.1.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 11:47:53.455 INFO CreateHadoopBamSplittingIndex - ------------------------------------------------------------; 11:47:53.455 INFO CreateHadoopBamSplittingIndex - The Genome Analysis Toolkit (GATK) v4.0.1.1; 11:47:53.455 INFO CreateHadoopBamSplittingIndex - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:47:53.455 INFO CreateHadoopBamSplittingIndex - Executing as farrell@scc-hadoop.bu.edu on Linux v2.6.32-696.10.3.el6.x86_64 amd64; 11:47:53.456 INFO CreateHadoopBamSplittingIndex - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_151-b12; 11:47:53.456 INFO CreateHadoopBamSplittingIndex - Start Date/Time: March 7, 2018 11:47:52 AM EST; 11:47:53.456 INFO CreateHadoopBamSplittingIndex - ------------------------------------------------------------; 11:47:53.456 INFO CreateHadoopBamSplittingIndex - ------------------------------------------------------------; 11:47:53.457 INFO CreateHadoopBamSplittingIndex - HTSJDK Version: 2.14.1; 11:47:53.457 INFO CreateHadoopBamSplittingIndex - Picard Ve",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4506
https://github.com/broadinstitute/gatk/issues/4506:3052,Availability,down,down,3052,"eHadoopBamSplittingIndex - ------------------------------------------------------------; 11:47:53.456 INFO CreateHadoopBamSplittingIndex - ------------------------------------------------------------; 11:47:53.457 INFO CreateHadoopBamSplittingIndex - HTSJDK Version: 2.14.1; 11:47:53.457 INFO CreateHadoopBamSplittingIndex - Picard Version: 2.17.2; 11:47:53.457 INFO CreateHadoopBamSplittingIndex - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 11:47:53.457 INFO CreateHadoopBamSplittingIndex - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 11:47:53.457 INFO CreateHadoopBamSplittingIndex - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 11:47:53.457 INFO CreateHadoopBamSplittingIndex - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 11:47:53.457 INFO CreateHadoopBamSplittingIndex - Deflater: IntelDeflater; 11:47:53.457 INFO CreateHadoopBamSplittingIndex - Inflater: IntelInflater; 11:47:53.457 INFO CreateHadoopBamSplittingIndex - GCS max retries/reopens: 20; 11:47:53.457 INFO CreateHadoopBamSplittingIndex - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 11:47:53.457 INFO CreateHadoopBamSplittingIndex - Initializing engine; 11:47:53.458 INFO CreateHadoopBamSplittingIndex - Done initializing engine; 11:47:53.463 INFO CreateHadoopBamSplittingIndex - Shutting down engine; [March 7, 2018 11:47:53 AM EST] org.broadinstitute.hellbender.tools.spark.CreateHadoopBamSplittingIndex done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=1115160576; ***********************************************************************. A USER ERROR has occurred: Bad input: A splitting index is only relevant for a bam file, but a file with extension cram was specified. ***********************************************************************; Set the system property GATK_STACKTRACE_ON_USER_EXCEPTION (--java-options '-DGATK_STACKTRACE_ON_USER_EXCEPTION=true') to print the stack trace.; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4506
https://github.com/broadinstitute/gatk/issues/4506:3318,Availability,ERROR,ERROR,3318,"eHadoopBamSplittingIndex - ------------------------------------------------------------; 11:47:53.456 INFO CreateHadoopBamSplittingIndex - ------------------------------------------------------------; 11:47:53.457 INFO CreateHadoopBamSplittingIndex - HTSJDK Version: 2.14.1; 11:47:53.457 INFO CreateHadoopBamSplittingIndex - Picard Version: 2.17.2; 11:47:53.457 INFO CreateHadoopBamSplittingIndex - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 11:47:53.457 INFO CreateHadoopBamSplittingIndex - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 11:47:53.457 INFO CreateHadoopBamSplittingIndex - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 11:47:53.457 INFO CreateHadoopBamSplittingIndex - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 11:47:53.457 INFO CreateHadoopBamSplittingIndex - Deflater: IntelDeflater; 11:47:53.457 INFO CreateHadoopBamSplittingIndex - Inflater: IntelInflater; 11:47:53.457 INFO CreateHadoopBamSplittingIndex - GCS max retries/reopens: 20; 11:47:53.457 INFO CreateHadoopBamSplittingIndex - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 11:47:53.457 INFO CreateHadoopBamSplittingIndex - Initializing engine; 11:47:53.458 INFO CreateHadoopBamSplittingIndex - Done initializing engine; 11:47:53.463 INFO CreateHadoopBamSplittingIndex - Shutting down engine; [March 7, 2018 11:47:53 AM EST] org.broadinstitute.hellbender.tools.spark.CreateHadoopBamSplittingIndex done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=1115160576; ***********************************************************************. A USER ERROR has occurred: Bad input: A splitting index is only relevant for a bam file, but a file with extension cram was specified. ***********************************************************************; Set the system property GATK_STACKTRACE_ON_USER_EXCEPTION (--java-options '-DGATK_STACKTRACE_ON_USER_EXCEPTION=true') to print the stack trace.; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4506
https://github.com/broadinstitute/gatk/issues/4506:398,Deployability,install,install,398,"Should the CreateHadoopBamSplittingIndex tool also work on a cram? I am getting this error below which suggests not. What are the benefits of a SplittingIndex for a spark job? On average-how long should it take a spark job to get the splits for a 30x bam or cram? . ```; gatk CreateHadoopBamSplittingIndex -I adni/cram/ADNI_002_S_0413.hg38.realign.bqsr.cram; Using GATK jar /share/pkg/gatk/4.0.1.1/install/bin/gatk-package-4.0.1.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -jar /share/pkg/gatk/4.0.1.1/install/bin/gatk-package-4.0.1.1-local.jar CreateHadoopBamSplittingIndex -I adni/cram/ADNI_002_S_0413.hg38.realign.bqsr.cram; 11:47:53.243 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg/gatk/4.0.1.1/install/bin/gatk-package-4.0.1.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 11:47:53.455 INFO CreateHadoopBamSplittingIndex - ------------------------------------------------------------; 11:47:53.455 INFO CreateHadoopBamSplittingIndex - The Genome Analysis Toolkit (GATK) v4.0.1.1; 11:47:53.455 INFO CreateHadoopBamSplittingIndex - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:47:53.455 INFO CreateHadoopBamSplittingIndex - Executing as farrell@scc-hadoop.bu.edu on Linux v2.6.32-696.10.3.el6.x86_64 amd64; 11:47:53.456 INFO CreateHadoopBamSplittingIndex - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_151-b12; 11:47:53.456 INFO CreateHadoopBamSplittingIndex - Start Date/Time: March 7, 2018 11:47:52 AM EST; 11:47:53.456 INFO CreateHadoopBamSplittingIndex - ------------------------------------------------------------; 11:47:53.456 INFO CreateHadoopBamSplittingIndex - ------------------------------------------------------------; 11:47:53.457 INFO CreateHadoopBamSplittingIndex - HTSJDK Version: 2.14.1; 11:47:53.457 INFO CreateHadoopBamSplittingIndex - Picard Ve",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4506
https://github.com/broadinstitute/gatk/issues/4506:641,Deployability,install,install,641,"Should the CreateHadoopBamSplittingIndex tool also work on a cram? I am getting this error below which suggests not. What are the benefits of a SplittingIndex for a spark job? On average-how long should it take a spark job to get the splits for a 30x bam or cram? . ```; gatk CreateHadoopBamSplittingIndex -I adni/cram/ADNI_002_S_0413.hg38.realign.bqsr.cram; Using GATK jar /share/pkg/gatk/4.0.1.1/install/bin/gatk-package-4.0.1.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -jar /share/pkg/gatk/4.0.1.1/install/bin/gatk-package-4.0.1.1-local.jar CreateHadoopBamSplittingIndex -I adni/cram/ADNI_002_S_0413.hg38.realign.bqsr.cram; 11:47:53.243 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg/gatk/4.0.1.1/install/bin/gatk-package-4.0.1.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 11:47:53.455 INFO CreateHadoopBamSplittingIndex - ------------------------------------------------------------; 11:47:53.455 INFO CreateHadoopBamSplittingIndex - The Genome Analysis Toolkit (GATK) v4.0.1.1; 11:47:53.455 INFO CreateHadoopBamSplittingIndex - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:47:53.455 INFO CreateHadoopBamSplittingIndex - Executing as farrell@scc-hadoop.bu.edu on Linux v2.6.32-696.10.3.el6.x86_64 amd64; 11:47:53.456 INFO CreateHadoopBamSplittingIndex - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_151-b12; 11:47:53.456 INFO CreateHadoopBamSplittingIndex - Start Date/Time: March 7, 2018 11:47:52 AM EST; 11:47:53.456 INFO CreateHadoopBamSplittingIndex - ------------------------------------------------------------; 11:47:53.456 INFO CreateHadoopBamSplittingIndex - ------------------------------------------------------------; 11:47:53.457 INFO CreateHadoopBamSplittingIndex - HTSJDK Version: 2.14.1; 11:47:53.457 INFO CreateHadoopBamSplittingIndex - Picard Ve",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4506
https://github.com/broadinstitute/gatk/issues/4506:875,Deployability,install,install,875,"Should the CreateHadoopBamSplittingIndex tool also work on a cram? I am getting this error below which suggests not. What are the benefits of a SplittingIndex for a spark job? On average-how long should it take a spark job to get the splits for a 30x bam or cram? . ```; gatk CreateHadoopBamSplittingIndex -I adni/cram/ADNI_002_S_0413.hg38.realign.bqsr.cram; Using GATK jar /share/pkg/gatk/4.0.1.1/install/bin/gatk-package-4.0.1.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -jar /share/pkg/gatk/4.0.1.1/install/bin/gatk-package-4.0.1.1-local.jar CreateHadoopBamSplittingIndex -I adni/cram/ADNI_002_S_0413.hg38.realign.bqsr.cram; 11:47:53.243 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg/gatk/4.0.1.1/install/bin/gatk-package-4.0.1.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 11:47:53.455 INFO CreateHadoopBamSplittingIndex - ------------------------------------------------------------; 11:47:53.455 INFO CreateHadoopBamSplittingIndex - The Genome Analysis Toolkit (GATK) v4.0.1.1; 11:47:53.455 INFO CreateHadoopBamSplittingIndex - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:47:53.455 INFO CreateHadoopBamSplittingIndex - Executing as farrell@scc-hadoop.bu.edu on Linux v2.6.32-696.10.3.el6.x86_64 amd64; 11:47:53.456 INFO CreateHadoopBamSplittingIndex - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_151-b12; 11:47:53.456 INFO CreateHadoopBamSplittingIndex - Start Date/Time: March 7, 2018 11:47:52 AM EST; 11:47:53.456 INFO CreateHadoopBamSplittingIndex - ------------------------------------------------------------; 11:47:53.456 INFO CreateHadoopBamSplittingIndex - ------------------------------------------------------------; 11:47:53.457 INFO CreateHadoopBamSplittingIndex - HTSJDK Version: 2.14.1; 11:47:53.457 INFO CreateHadoopBamSplittingIndex - Picard Ve",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4506
https://github.com/broadinstitute/gatk/issues/4506:2726,Deployability,patch,patch,2726,"eHadoopBamSplittingIndex - ------------------------------------------------------------; 11:47:53.456 INFO CreateHadoopBamSplittingIndex - ------------------------------------------------------------; 11:47:53.457 INFO CreateHadoopBamSplittingIndex - HTSJDK Version: 2.14.1; 11:47:53.457 INFO CreateHadoopBamSplittingIndex - Picard Version: 2.17.2; 11:47:53.457 INFO CreateHadoopBamSplittingIndex - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 11:47:53.457 INFO CreateHadoopBamSplittingIndex - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 11:47:53.457 INFO CreateHadoopBamSplittingIndex - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 11:47:53.457 INFO CreateHadoopBamSplittingIndex - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 11:47:53.457 INFO CreateHadoopBamSplittingIndex - Deflater: IntelDeflater; 11:47:53.457 INFO CreateHadoopBamSplittingIndex - Inflater: IntelInflater; 11:47:53.457 INFO CreateHadoopBamSplittingIndex - GCS max retries/reopens: 20; 11:47:53.457 INFO CreateHadoopBamSplittingIndex - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 11:47:53.457 INFO CreateHadoopBamSplittingIndex - Initializing engine; 11:47:53.458 INFO CreateHadoopBamSplittingIndex - Done initializing engine; 11:47:53.463 INFO CreateHadoopBamSplittingIndex - Shutting down engine; [March 7, 2018 11:47:53 AM EST] org.broadinstitute.hellbender.tools.spark.CreateHadoopBamSplittingIndex done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=1115160576; ***********************************************************************. A USER ERROR has occurred: Bad input: A splitting index is only relevant for a bam file, but a file with extension cram was specified. ***********************************************************************; Set the system property GATK_STACKTRACE_ON_USER_EXCEPTION (--java-options '-DGATK_STACKTRACE_ON_USER_EXCEPTION=true') to print the stack trace.; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4506
https://github.com/broadinstitute/gatk/issues/4506:807,Performance,Load,Loading,807,"Should the CreateHadoopBamSplittingIndex tool also work on a cram? I am getting this error below which suggests not. What are the benefits of a SplittingIndex for a spark job? On average-how long should it take a spark job to get the splits for a 30x bam or cram? . ```; gatk CreateHadoopBamSplittingIndex -I adni/cram/ADNI_002_S_0413.hg38.realign.bqsr.cram; Using GATK jar /share/pkg/gatk/4.0.1.1/install/bin/gatk-package-4.0.1.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -jar /share/pkg/gatk/4.0.1.1/install/bin/gatk-package-4.0.1.1-local.jar CreateHadoopBamSplittingIndex -I adni/cram/ADNI_002_S_0413.hg38.realign.bqsr.cram; 11:47:53.243 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg/gatk/4.0.1.1/install/bin/gatk-package-4.0.1.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 11:47:53.455 INFO CreateHadoopBamSplittingIndex - ------------------------------------------------------------; 11:47:53.455 INFO CreateHadoopBamSplittingIndex - The Genome Analysis Toolkit (GATK) v4.0.1.1; 11:47:53.455 INFO CreateHadoopBamSplittingIndex - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:47:53.455 INFO CreateHadoopBamSplittingIndex - Executing as farrell@scc-hadoop.bu.edu on Linux v2.6.32-696.10.3.el6.x86_64 amd64; 11:47:53.456 INFO CreateHadoopBamSplittingIndex - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_151-b12; 11:47:53.456 INFO CreateHadoopBamSplittingIndex - Start Date/Time: March 7, 2018 11:47:52 AM EST; 11:47:53.456 INFO CreateHadoopBamSplittingIndex - ------------------------------------------------------------; 11:47:53.456 INFO CreateHadoopBamSplittingIndex - ------------------------------------------------------------; 11:47:53.457 INFO CreateHadoopBamSplittingIndex - HTSJDK Version: 2.14.1; 11:47:53.457 INFO CreateHadoopBamSplittingIndex - Picard Ve",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4506
https://github.com/broadinstitute/gatk/issues/4507:26,Availability,error,error,26,"I _believe_ that the user error text didn't get updated when the `--sequence-dictionary` argument got added. It would be great to mention that argument in GATKTool::initializeIntervals(), which currently says `""We require a sequence dictionary from a reference, a source of reads, or a source of variants to process intervals. "" +; ""Since reference and reads files generally contain sequence dictionaries, this error most commonly occurs "" +; ""for VariantWalkers that do not require a reference or reads. You can fix the problem by passing a reference file with a sequence dictionary "" +; ""via the -R argument or you can run the tool UpdateVCFSequenceDictionary on your vcf.""`. I would change the last sentence to:; `You can fix the problem by passing a reference file with a sequence dictionary via the -R argument, a *.dict dictionary file via the --sequence-dictionary argument, or you can run the tool UpdateVCFSequenceDictionary on your vcf.""`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4507
https://github.com/broadinstitute/gatk/issues/4507:411,Availability,error,error,411,"I _believe_ that the user error text didn't get updated when the `--sequence-dictionary` argument got added. It would be great to mention that argument in GATKTool::initializeIntervals(), which currently says `""We require a sequence dictionary from a reference, a source of reads, or a source of variants to process intervals. "" +; ""Since reference and reads files generally contain sequence dictionaries, this error most commonly occurs "" +; ""for VariantWalkers that do not require a reference or reads. You can fix the problem by passing a reference file with a sequence dictionary "" +; ""via the -R argument or you can run the tool UpdateVCFSequenceDictionary on your vcf.""`. I would change the last sentence to:; `You can fix the problem by passing a reference file with a sequence dictionary via the -R argument, a *.dict dictionary file via the --sequence-dictionary argument, or you can run the tool UpdateVCFSequenceDictionary on your vcf.""`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4507
https://github.com/broadinstitute/gatk/issues/4507:48,Deployability,update,updated,48,"I _believe_ that the user error text didn't get updated when the `--sequence-dictionary` argument got added. It would be great to mention that argument in GATKTool::initializeIntervals(), which currently says `""We require a sequence dictionary from a reference, a source of reads, or a source of variants to process intervals. "" +; ""Since reference and reads files generally contain sequence dictionaries, this error most commonly occurs "" +; ""for VariantWalkers that do not require a reference or reads. You can fix the problem by passing a reference file with a sequence dictionary "" +; ""via the -R argument or you can run the tool UpdateVCFSequenceDictionary on your vcf.""`. I would change the last sentence to:; `You can fix the problem by passing a reference file with a sequence dictionary via the -R argument, a *.dict dictionary file via the --sequence-dictionary argument, or you can run the tool UpdateVCFSequenceDictionary on your vcf.""`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4507
https://github.com/broadinstitute/gatk/issues/4507:634,Deployability,Update,UpdateVCFSequenceDictionary,634,"I _believe_ that the user error text didn't get updated when the `--sequence-dictionary` argument got added. It would be great to mention that argument in GATKTool::initializeIntervals(), which currently says `""We require a sequence dictionary from a reference, a source of reads, or a source of variants to process intervals. "" +; ""Since reference and reads files generally contain sequence dictionaries, this error most commonly occurs "" +; ""for VariantWalkers that do not require a reference or reads. You can fix the problem by passing a reference file with a sequence dictionary "" +; ""via the -R argument or you can run the tool UpdateVCFSequenceDictionary on your vcf.""`. I would change the last sentence to:; `You can fix the problem by passing a reference file with a sequence dictionary via the -R argument, a *.dict dictionary file via the --sequence-dictionary argument, or you can run the tool UpdateVCFSequenceDictionary on your vcf.""`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4507
https://github.com/broadinstitute/gatk/issues/4507:906,Deployability,Update,UpdateVCFSequenceDictionary,906,"I _believe_ that the user error text didn't get updated when the `--sequence-dictionary` argument got added. It would be great to mention that argument in GATKTool::initializeIntervals(), which currently says `""We require a sequence dictionary from a reference, a source of reads, or a source of variants to process intervals. "" +; ""Since reference and reads files generally contain sequence dictionaries, this error most commonly occurs "" +; ""for VariantWalkers that do not require a reference or reads. You can fix the problem by passing a reference file with a sequence dictionary "" +; ""via the -R argument or you can run the tool UpdateVCFSequenceDictionary on your vcf.""`. I would change the last sentence to:; `You can fix the problem by passing a reference file with a sequence dictionary via the -R argument, a *.dict dictionary file via the --sequence-dictionary argument, or you can run the tool UpdateVCFSequenceDictionary on your vcf.""`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4507
https://github.com/broadinstitute/gatk/issues/4508:154,Availability,error,errors,154,"We can do this in the GenomicsDBImport walker -- it doesn't have to be in GenomicsDB. Not every version is backward compatible, so when there are cryptic errors it would be nice to know what version was used to create the GDB.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4508
https://github.com/broadinstitute/gatk/pull/4509:209,Security,validat,validations,209,"@takutosato This uses minor allele fraction segmentation, which was already done internally in `CalculateContamination`, to improve tumor-only calling a lot. I also sw modest improvements in some tumor-normal validations. Also, @chandrans @sooheelee this hopefully does away with the problems with `af-of-alleles-not-in-resource` by deriving a defensible default that doesn't result in all calls in tumor-only mode getting filtered.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4509
https://github.com/broadinstitute/gatk/issues/4511:82,Availability,down,download,82,"Hi, I'm interested in your keras model for variant calling. Is there anyway I can download it to study it?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4511
https://github.com/broadinstitute/gatk/issues/4512:375,Performance,perform,performance-troubleshooting-tips-for-genotypegvcfs,375,"## Bug Report. ### Affected tool(s); GenotypeGVCFs . ### Affected version(s); 4.0.2.0. ### Description ; After running GenomicsDBImport which takes a short time, GenotypeGVCFs takes a really long time to genotype a short interval. It should not take so long. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/11471/performance-troubleshooting-tips-for-genotypegvcfs/p1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4512
https://github.com/broadinstitute/gatk/issues/4513:220,Availability,error,errors,220,"gatk silently sets TMP_DIR globally readable/writable for all users. This is problematic for admins trying to maintain a secure multi-user environment. It would be better (imho) if gatk tests if TMP_DIR is writeable and errors out when it is not instead of just globally making it writeable by all users. src/main/java/org/broadinstitute/hellbender/cmdline/CommandLineProgram.java. for (final File f : TMP_DIR) {; // Intentionally not checking the return values, because it may be that the program does not; // need a tmp_dir. If this fails, the problem will be discovered downstream.; if (!f.exists()) f.mkdirs();; f.setReadable(true, false);; f.setWritable(true, false);",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4513
https://github.com/broadinstitute/gatk/issues/4513:573,Availability,down,downstream,573,"gatk silently sets TMP_DIR globally readable/writable for all users. This is problematic for admins trying to maintain a secure multi-user environment. It would be better (imho) if gatk tests if TMP_DIR is writeable and errors out when it is not instead of just globally making it writeable by all users. src/main/java/org/broadinstitute/hellbender/cmdline/CommandLineProgram.java. for (final File f : TMP_DIR) {; // Intentionally not checking the return values, because it may be that the program does not; // need a tmp_dir. If this fails, the problem will be discovered downstream.; if (!f.exists()) f.mkdirs();; f.setReadable(true, false);; f.setWritable(true, false);",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4513
https://github.com/broadinstitute/gatk/issues/4513:121,Security,secur,secure,121,"gatk silently sets TMP_DIR globally readable/writable for all users. This is problematic for admins trying to maintain a secure multi-user environment. It would be better (imho) if gatk tests if TMP_DIR is writeable and errors out when it is not instead of just globally making it writeable by all users. src/main/java/org/broadinstitute/hellbender/cmdline/CommandLineProgram.java. for (final File f : TMP_DIR) {; // Intentionally not checking the return values, because it may be that the program does not; // need a tmp_dir. If this fails, the problem will be discovered downstream.; if (!f.exists()) f.mkdirs();; f.setReadable(true, false);; f.setWritable(true, false);",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4513
https://github.com/broadinstitute/gatk/issues/4513:186,Testability,test,tests,186,"gatk silently sets TMP_DIR globally readable/writable for all users. This is problematic for admins trying to maintain a secure multi-user environment. It would be better (imho) if gatk tests if TMP_DIR is writeable and errors out when it is not instead of just globally making it writeable by all users. src/main/java/org/broadinstitute/hellbender/cmdline/CommandLineProgram.java. for (final File f : TMP_DIR) {; // Intentionally not checking the return values, because it may be that the program does not; // need a tmp_dir. If this fails, the problem will be discovered downstream.; if (!f.exists()) f.mkdirs();; f.setReadable(true, false);; f.setWritable(true, false);",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4513
https://github.com/broadinstitute/gatk/issues/4514:86,Availability,error,error,86,"Hi,. I don't know if it's a bug, but whenever I try to do a GenomicsDBImport I get an error, no matter which file I use. The command I run is (using GATK 4.0):. gatk GenomicsDBImport -V AD0616.10.g.vcf.gz --genomicsdb-workspace-path gdbworkspace-gatk -L 10. And the error I get is:. Using GATK jar /apps/GATK/4.0/gatk-package-4.0.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -jar /apps/GATK/4.0/gatk-package-4.0.0.0-local.jar GenomicsDBImport -V /scratch/production/cluengo/genomicsdb/AD0616.10.g.vcf.gz --genomicsdb-workspace-path gdbworkspace-gatk -L 10; 17:00:53.658 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/apps/GATK/4.0/gatk-package-4.0.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 17:00:53.770 INFO GenomicsDBImport - ------------------------------------------------------------; 17:00:53.771 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.0.0.0; 17:00:53.771 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 17:00:53.771 INFO GenomicsDBImport - Executing as cluengo@login1 on Linux v2.6.32-696.13.2.el6.Bull.128.x86_64 amd64; 17:00:53.771 INFO GenomicsDBImport - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_102-b14; 17:00:53.771 INFO GenomicsDBImport - Start Date/Time: March 8, 2018 5:00:53 PM CET; 17:00:53.771 INFO GenomicsDBImport - ------------------------------------------------------------; 17:00:53.771 INFO GenomicsDBImport - ------------------------------------------------------------; 17:00:53.772 INFO GenomicsDBImport - HTSJDK Version: 2.13.2; 17:00:53.772 INFO GenomicsDBImport - Picard Version: 2.17.2; 17:00:53.772 INFO GenomicsDBImport - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 17:00:53.772 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:00:53.772 INFO GenomicsDBImport - HTS",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4514
https://github.com/broadinstitute/gatk/issues/4514:266,Availability,error,error,266,"Hi,. I don't know if it's a bug, but whenever I try to do a GenomicsDBImport I get an error, no matter which file I use. The command I run is (using GATK 4.0):. gatk GenomicsDBImport -V AD0616.10.g.vcf.gz --genomicsdb-workspace-path gdbworkspace-gatk -L 10. And the error I get is:. Using GATK jar /apps/GATK/4.0/gatk-package-4.0.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -jar /apps/GATK/4.0/gatk-package-4.0.0.0-local.jar GenomicsDBImport -V /scratch/production/cluengo/genomicsdb/AD0616.10.g.vcf.gz --genomicsdb-workspace-path gdbworkspace-gatk -L 10; 17:00:53.658 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/apps/GATK/4.0/gatk-package-4.0.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 17:00:53.770 INFO GenomicsDBImport - ------------------------------------------------------------; 17:00:53.771 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.0.0.0; 17:00:53.771 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 17:00:53.771 INFO GenomicsDBImport - Executing as cluengo@login1 on Linux v2.6.32-696.13.2.el6.Bull.128.x86_64 amd64; 17:00:53.771 INFO GenomicsDBImport - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_102-b14; 17:00:53.771 INFO GenomicsDBImport - Start Date/Time: March 8, 2018 5:00:53 PM CET; 17:00:53.771 INFO GenomicsDBImport - ------------------------------------------------------------; 17:00:53.771 INFO GenomicsDBImport - ------------------------------------------------------------; 17:00:53.772 INFO GenomicsDBImport - HTSJDK Version: 2.13.2; 17:00:53.772 INFO GenomicsDBImport - Picard Version: 2.17.2; 17:00:53.772 INFO GenomicsDBImport - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 17:00:53.772 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:00:53.772 INFO GenomicsDBImport - HTS",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4514
https://github.com/broadinstitute/gatk/issues/4514:5154,Availability,error,error,5154,"genomicsdb_array; 17:00:54.470 INFO ProgressMeter - Starting traversal; 17:00:54.470 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 17:00:54.488 INFO GenomicsDBImport - Importing batch 1 with 1 samples; terminate called after throwing an instance of 'FileBasedVidMapperException'; what(): FileBasedVidMapperException : type_index_iter != VidMapper::m_typename_string_to_type_index.end() && ""Unhandled field type""; [login1:01909] *** Process received signal ***; [login1:01909] Signal: Aborted (6); [login1:01909] Signal code: (-6); [login1:01909] [ 0] /lib64/libpthread.so.0[0x30bfa0f7e0]; [login1:01909] [ 1] /lib64/libc.so.6(gsignal+0x35)[0x30bf6325e5]; [login1:01909] [ 2] /lib64/libc.so.6(abort+0x175)[0x30bf633dc5]; [login1:01909] [ 3] /apps/GCC/6.3.0/lib64/libstdc++.so.6(_ZN9__gnu_cxx27__verbose_terminate_handlerEv+0x15d)[0x7f507f7018ed]; [login1:01909] [ 4] /apps/GCC/6.3.0/lib64/libstdc++.so.6(+0x8e8a6)[0x7f507f6ff8a6]; [login1:01909] [ 5] /apps/GCC/6.3.0/lib64/libstdc++.so.6(+0x8e8f1)[0x7f507f6ff8f1]; [login1:01909] [ 6] /apps/GCC/6.3.0/lib64/libstdc++.so.6(+0x8eb08)[0x7f507f6ffb08]; [login1:01909] [ 7] /apps/GENOMICSDB/0.9.2/lib/libtiledbgenomicsdb.so(+0x151df1)[0x7f507fb56df1]; [login1:01909] [ 8] /apps/GENOMICSDB/0.9.2/lib/libtiledbgenomicsdb.so(+0x1434d9)[0x7f507fb484d9]; [login1:01909] [ 9] /apps/GENOMICSDB/0.9.2/lib/libtiledbgenomicsdb.so(+0x1489f9)[0x7f507fb4d9f9]; [login1:01909] [10] /apps/GENOMICSDB/0.9.2/lib/libtiledbgenomicsdb.so(+0x12c78e)[0x7f507fb3178e]; [login1:01909] [11] /apps/GENOMICSDB/0.9.2/lib/libtiledbgenomicsdb.so(+0x12d7b4)[0x7f507fb327b4]; [login1:01909] [12] /apps/GENOMICSDB/0.9.2/lib/libtiledbgenomicsdb.so(+0x11f80d)[0x7f507fb2480d]; [login1:01909] [13] /apps/GENOMICSDB/0.9.2/lib/libtiledbgenomicsdb.so(Java_com_intel_genomicsdb_GenomicsDBImporter_jniSetupGenomicsDBLoader+0x98)[0x7f507fb99c78]; [login1:01909] [14] [0x7f50f1015814]; [login1:01909] *** End of error message ***. Thanks in advance!. Cristina.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4514
https://github.com/broadinstitute/gatk/issues/4514:2398,Deployability,patch,patch,2398," GenomicsDBImport - Start Date/Time: March 8, 2018 5:00:53 PM CET; 17:00:53.771 INFO GenomicsDBImport - ------------------------------------------------------------; 17:00:53.771 INFO GenomicsDBImport - ------------------------------------------------------------; 17:00:53.772 INFO GenomicsDBImport - HTSJDK Version: 2.13.2; 17:00:53.772 INFO GenomicsDBImport - Picard Version: 2.17.2; 17:00:53.772 INFO GenomicsDBImport - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 17:00:53.772 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:00:53.772 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 17:00:53.772 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:00:53.772 INFO GenomicsDBImport - Deflater: IntelDeflater; 17:00:53.772 INFO GenomicsDBImport - Inflater: IntelInflater; 17:00:53.772 INFO GenomicsDBImport - GCS max retries/reopens: 20; 17:00:53.772 INFO GenomicsDBImport - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 17:00:53.772 INFO GenomicsDBImport - Initializing engine; 17:00:54.197 INFO IntervalArgumentCollection - Processing 135534747 bp from intervals; 17:00:54.200 INFO GenomicsDBImport - Done initializing engine; Created workspace /scratch/production/cluengo/genomicsdb/gdbworkspace-gatk; 17:00:54.418 INFO GenomicsDBImport - Vid Map JSON file will be written to gdbworkspace-gatk/vidmap.json; 17:00:54.418 INFO GenomicsDBImport - Callset Map JSON file will be written to gdbworkspace-gatk/callset.json; 17:00:54.418 INFO GenomicsDBImport - Complete VCF Header will be written to gdbworkspace-gatk/vcfheader.vcf; 17:00:54.418 INFO GenomicsDBImport - Importing to array - gdbworkspace-gatk/genomicsdb_array; 17:00:54.470 INFO ProgressMeter - Starting traversal; 17:00:54.470 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 17:00:54.488 INFO GenomicsDBImport",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4514
https://github.com/broadinstitute/gatk/issues/4514:5160,Integrability,message,message,5160,"genomicsdb_array; 17:00:54.470 INFO ProgressMeter - Starting traversal; 17:00:54.470 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 17:00:54.488 INFO GenomicsDBImport - Importing batch 1 with 1 samples; terminate called after throwing an instance of 'FileBasedVidMapperException'; what(): FileBasedVidMapperException : type_index_iter != VidMapper::m_typename_string_to_type_index.end() && ""Unhandled field type""; [login1:01909] *** Process received signal ***; [login1:01909] Signal: Aborted (6); [login1:01909] Signal code: (-6); [login1:01909] [ 0] /lib64/libpthread.so.0[0x30bfa0f7e0]; [login1:01909] [ 1] /lib64/libc.so.6(gsignal+0x35)[0x30bf6325e5]; [login1:01909] [ 2] /lib64/libc.so.6(abort+0x175)[0x30bf633dc5]; [login1:01909] [ 3] /apps/GCC/6.3.0/lib64/libstdc++.so.6(_ZN9__gnu_cxx27__verbose_terminate_handlerEv+0x15d)[0x7f507f7018ed]; [login1:01909] [ 4] /apps/GCC/6.3.0/lib64/libstdc++.so.6(+0x8e8a6)[0x7f507f6ff8a6]; [login1:01909] [ 5] /apps/GCC/6.3.0/lib64/libstdc++.so.6(+0x8e8f1)[0x7f507f6ff8f1]; [login1:01909] [ 6] /apps/GCC/6.3.0/lib64/libstdc++.so.6(+0x8eb08)[0x7f507f6ffb08]; [login1:01909] [ 7] /apps/GENOMICSDB/0.9.2/lib/libtiledbgenomicsdb.so(+0x151df1)[0x7f507fb56df1]; [login1:01909] [ 8] /apps/GENOMICSDB/0.9.2/lib/libtiledbgenomicsdb.so(+0x1434d9)[0x7f507fb484d9]; [login1:01909] [ 9] /apps/GENOMICSDB/0.9.2/lib/libtiledbgenomicsdb.so(+0x1489f9)[0x7f507fb4d9f9]; [login1:01909] [10] /apps/GENOMICSDB/0.9.2/lib/libtiledbgenomicsdb.so(+0x12c78e)[0x7f507fb3178e]; [login1:01909] [11] /apps/GENOMICSDB/0.9.2/lib/libtiledbgenomicsdb.so(+0x12d7b4)[0x7f507fb327b4]; [login1:01909] [12] /apps/GENOMICSDB/0.9.2/lib/libtiledbgenomicsdb.so(+0x11f80d)[0x7f507fb2480d]; [login1:01909] [13] /apps/GENOMICSDB/0.9.2/lib/libtiledbgenomicsdb.so(Java_com_intel_genomicsdb_GenomicsDBImporter_jniSetupGenomicsDBLoader+0x98)[0x7f507fb99c78]; [login1:01909] [14] [0x7f50f1015814]; [login1:01909] *** End of error message ***. Thanks in advance!. Cristina.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4514
https://github.com/broadinstitute/gatk/issues/4514:737,Performance,Load,Loading,737,"Hi,. I don't know if it's a bug, but whenever I try to do a GenomicsDBImport I get an error, no matter which file I use. The command I run is (using GATK 4.0):. gatk GenomicsDBImport -V AD0616.10.g.vcf.gz --genomicsdb-workspace-path gdbworkspace-gatk -L 10. And the error I get is:. Using GATK jar /apps/GATK/4.0/gatk-package-4.0.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -jar /apps/GATK/4.0/gatk-package-4.0.0.0-local.jar GenomicsDBImport -V /scratch/production/cluengo/genomicsdb/AD0616.10.g.vcf.gz --genomicsdb-workspace-path gdbworkspace-gatk -L 10; 17:00:53.658 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/apps/GATK/4.0/gatk-package-4.0.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 17:00:53.770 INFO GenomicsDBImport - ------------------------------------------------------------; 17:00:53.771 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.0.0.0; 17:00:53.771 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 17:00:53.771 INFO GenomicsDBImport - Executing as cluengo@login1 on Linux v2.6.32-696.13.2.el6.Bull.128.x86_64 amd64; 17:00:53.771 INFO GenomicsDBImport - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_102-b14; 17:00:53.771 INFO GenomicsDBImport - Start Date/Time: March 8, 2018 5:00:53 PM CET; 17:00:53.771 INFO GenomicsDBImport - ------------------------------------------------------------; 17:00:53.771 INFO GenomicsDBImport - ------------------------------------------------------------; 17:00:53.772 INFO GenomicsDBImport - HTSJDK Version: 2.13.2; 17:00:53.772 INFO GenomicsDBImport - Picard Version: 2.17.2; 17:00:53.772 INFO GenomicsDBImport - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 17:00:53.772 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:00:53.772 INFO GenomicsDBImport - HTS",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4514
https://github.com/broadinstitute/gatk/issues/4514:3725,Safety,Abort,Aborted,3725,"ng engine; Created workspace /scratch/production/cluengo/genomicsdb/gdbworkspace-gatk; 17:00:54.418 INFO GenomicsDBImport - Vid Map JSON file will be written to gdbworkspace-gatk/vidmap.json; 17:00:54.418 INFO GenomicsDBImport - Callset Map JSON file will be written to gdbworkspace-gatk/callset.json; 17:00:54.418 INFO GenomicsDBImport - Complete VCF Header will be written to gdbworkspace-gatk/vcfheader.vcf; 17:00:54.418 INFO GenomicsDBImport - Importing to array - gdbworkspace-gatk/genomicsdb_array; 17:00:54.470 INFO ProgressMeter - Starting traversal; 17:00:54.470 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 17:00:54.488 INFO GenomicsDBImport - Importing batch 1 with 1 samples; terminate called after throwing an instance of 'FileBasedVidMapperException'; what(): FileBasedVidMapperException : type_index_iter != VidMapper::m_typename_string_to_type_index.end() && ""Unhandled field type""; [login1:01909] *** Process received signal ***; [login1:01909] Signal: Aborted (6); [login1:01909] Signal code: (-6); [login1:01909] [ 0] /lib64/libpthread.so.0[0x30bfa0f7e0]; [login1:01909] [ 1] /lib64/libc.so.6(gsignal+0x35)[0x30bf6325e5]; [login1:01909] [ 2] /lib64/libc.so.6(abort+0x175)[0x30bf633dc5]; [login1:01909] [ 3] /apps/GCC/6.3.0/lib64/libstdc++.so.6(_ZN9__gnu_cxx27__verbose_terminate_handlerEv+0x15d)[0x7f507f7018ed]; [login1:01909] [ 4] /apps/GCC/6.3.0/lib64/libstdc++.so.6(+0x8e8a6)[0x7f507f6ff8a6]; [login1:01909] [ 5] /apps/GCC/6.3.0/lib64/libstdc++.so.6(+0x8e8f1)[0x7f507f6ff8f1]; [login1:01909] [ 6] /apps/GCC/6.3.0/lib64/libstdc++.so.6(+0x8eb08)[0x7f507f6ffb08]; [login1:01909] [ 7] /apps/GENOMICSDB/0.9.2/lib/libtiledbgenomicsdb.so(+0x151df1)[0x7f507fb56df1]; [login1:01909] [ 8] /apps/GENOMICSDB/0.9.2/lib/libtiledbgenomicsdb.so(+0x1434d9)[0x7f507fb484d9]; [login1:01909] [ 9] /apps/GENOMICSDB/0.9.2/lib/libtiledbgenomicsdb.so(+0x1489f9)[0x7f507fb4d9f9]; [login1:01909] [10] /apps/GENOMICSDB/0.9.2/lib/libtiledbgenomicsdb.so(+0x12c78e)[0x",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4514
https://github.com/broadinstitute/gatk/issues/4514:3933,Safety,abort,abort,3933,"ile will be written to gdbworkspace-gatk/callset.json; 17:00:54.418 INFO GenomicsDBImport - Complete VCF Header will be written to gdbworkspace-gatk/vcfheader.vcf; 17:00:54.418 INFO GenomicsDBImport - Importing to array - gdbworkspace-gatk/genomicsdb_array; 17:00:54.470 INFO ProgressMeter - Starting traversal; 17:00:54.470 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 17:00:54.488 INFO GenomicsDBImport - Importing batch 1 with 1 samples; terminate called after throwing an instance of 'FileBasedVidMapperException'; what(): FileBasedVidMapperException : type_index_iter != VidMapper::m_typename_string_to_type_index.end() && ""Unhandled field type""; [login1:01909] *** Process received signal ***; [login1:01909] Signal: Aborted (6); [login1:01909] Signal code: (-6); [login1:01909] [ 0] /lib64/libpthread.so.0[0x30bfa0f7e0]; [login1:01909] [ 1] /lib64/libc.so.6(gsignal+0x35)[0x30bf6325e5]; [login1:01909] [ 2] /lib64/libc.so.6(abort+0x175)[0x30bf633dc5]; [login1:01909] [ 3] /apps/GCC/6.3.0/lib64/libstdc++.so.6(_ZN9__gnu_cxx27__verbose_terminate_handlerEv+0x15d)[0x7f507f7018ed]; [login1:01909] [ 4] /apps/GCC/6.3.0/lib64/libstdc++.so.6(+0x8e8a6)[0x7f507f6ff8a6]; [login1:01909] [ 5] /apps/GCC/6.3.0/lib64/libstdc++.so.6(+0x8e8f1)[0x7f507f6ff8f1]; [login1:01909] [ 6] /apps/GCC/6.3.0/lib64/libstdc++.so.6(+0x8eb08)[0x7f507f6ffb08]; [login1:01909] [ 7] /apps/GENOMICSDB/0.9.2/lib/libtiledbgenomicsdb.so(+0x151df1)[0x7f507fb56df1]; [login1:01909] [ 8] /apps/GENOMICSDB/0.9.2/lib/libtiledbgenomicsdb.so(+0x1434d9)[0x7f507fb484d9]; [login1:01909] [ 9] /apps/GENOMICSDB/0.9.2/lib/libtiledbgenomicsdb.so(+0x1489f9)[0x7f507fb4d9f9]; [login1:01909] [10] /apps/GENOMICSDB/0.9.2/lib/libtiledbgenomicsdb.so(+0x12c78e)[0x7f507fb3178e]; [login1:01909] [11] /apps/GENOMICSDB/0.9.2/lib/libtiledbgenomicsdb.so(+0x12d7b4)[0x7f507fb327b4]; [login1:01909] [12] /apps/GENOMICSDB/0.9.2/lib/libtiledbgenomicsdb.so(+0x11f80d)[0x7f507fb2480d]; [login1:01909] [13] /apps/GENOMICSD",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4514
https://github.com/broadinstitute/gatk/issues/4515:116,Availability,error,error,116,"Hello, i'm trying to run GATK4 BaseRecalibratorSpark in local in a nextflow pipeline, but I got every time the same error with this command :. **gatk-launch BaseRecalibratorSpark --spark-runner LOCAL --spark-master local[23] -R $indexbit -I ${input_bam} --known-sites ${params.dbsnpAll} -L ${params.targetBed} -O ${output_name}-recalibration_report.grp**. Here is the full error output, basically, the failure seems to be linked to an absence of activity of the BaseRecalibratorSpark during 12s which kill the spark heartbeat and then the processus :. ```; 18/03/09 09:22:08 WARN Executor: Issue communicating with driver in heartbeater; java.lang.NullPointerException; at org.apache.spark.storage.memory.MemoryStore.getSize(MemoryStore.scala:127); at org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$getCurrentBlockStatus(BlockManager.scala:387); at org.apache.spark.storage.BlockManager$$anonfun$reportAllBlocks$3.apply(BlockManager.scala:218); at org.apache.spark.storage.BlockManager$$anonfun$reportAllBlocks$3.apply(BlockManager.scala:217); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at org.apache.spark.storage.BlockManager.reportAllBlocks(BlockManager.scala:217); at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:236); at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:522); at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply$mcV$sp(Executor.scala:547); at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:547); at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:547); at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1953); at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:547); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4515
https://github.com/broadinstitute/gatk/issues/4515:373,Availability,error,error,373,"Hello, i'm trying to run GATK4 BaseRecalibratorSpark in local in a nextflow pipeline, but I got every time the same error with this command :. **gatk-launch BaseRecalibratorSpark --spark-runner LOCAL --spark-master local[23] -R $indexbit -I ${input_bam} --known-sites ${params.dbsnpAll} -L ${params.targetBed} -O ${output_name}-recalibration_report.grp**. Here is the full error output, basically, the failure seems to be linked to an absence of activity of the BaseRecalibratorSpark during 12s which kill the spark heartbeat and then the processus :. ```; 18/03/09 09:22:08 WARN Executor: Issue communicating with driver in heartbeater; java.lang.NullPointerException; at org.apache.spark.storage.memory.MemoryStore.getSize(MemoryStore.scala:127); at org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$getCurrentBlockStatus(BlockManager.scala:387); at org.apache.spark.storage.BlockManager$$anonfun$reportAllBlocks$3.apply(BlockManager.scala:218); at org.apache.spark.storage.BlockManager$$anonfun$reportAllBlocks$3.apply(BlockManager.scala:217); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at org.apache.spark.storage.BlockManager.reportAllBlocks(BlockManager.scala:217); at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:236); at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:522); at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply$mcV$sp(Executor.scala:547); at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:547); at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:547); at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1953); at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:547); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4515
https://github.com/broadinstitute/gatk/issues/4515:402,Availability,failure,failure,402,"Hello, i'm trying to run GATK4 BaseRecalibratorSpark in local in a nextflow pipeline, but I got every time the same error with this command :. **gatk-launch BaseRecalibratorSpark --spark-runner LOCAL --spark-master local[23] -R $indexbit -I ${input_bam} --known-sites ${params.dbsnpAll} -L ${params.targetBed} -O ${output_name}-recalibration_report.grp**. Here is the full error output, basically, the failure seems to be linked to an absence of activity of the BaseRecalibratorSpark during 12s which kill the spark heartbeat and then the processus :. ```; 18/03/09 09:22:08 WARN Executor: Issue communicating with driver in heartbeater; java.lang.NullPointerException; at org.apache.spark.storage.memory.MemoryStore.getSize(MemoryStore.scala:127); at org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$getCurrentBlockStatus(BlockManager.scala:387); at org.apache.spark.storage.BlockManager$$anonfun$reportAllBlocks$3.apply(BlockManager.scala:218); at org.apache.spark.storage.BlockManager$$anonfun$reportAllBlocks$3.apply(BlockManager.scala:217); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at org.apache.spark.storage.BlockManager.reportAllBlocks(BlockManager.scala:217); at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:236); at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:522); at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply$mcV$sp(Executor.scala:547); at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:547); at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:547); at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1953); at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:547); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4515
https://github.com/broadinstitute/gatk/issues/4515:516,Availability,heartbeat,heartbeat,516,"Hello, i'm trying to run GATK4 BaseRecalibratorSpark in local in a nextflow pipeline, but I got every time the same error with this command :. **gatk-launch BaseRecalibratorSpark --spark-runner LOCAL --spark-master local[23] -R $indexbit -I ${input_bam} --known-sites ${params.dbsnpAll} -L ${params.targetBed} -O ${output_name}-recalibration_report.grp**. Here is the full error output, basically, the failure seems to be linked to an absence of activity of the BaseRecalibratorSpark during 12s which kill the spark heartbeat and then the processus :. ```; 18/03/09 09:22:08 WARN Executor: Issue communicating with driver in heartbeater; java.lang.NullPointerException; at org.apache.spark.storage.memory.MemoryStore.getSize(MemoryStore.scala:127); at org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$getCurrentBlockStatus(BlockManager.scala:387); at org.apache.spark.storage.BlockManager$$anonfun$reportAllBlocks$3.apply(BlockManager.scala:218); at org.apache.spark.storage.BlockManager$$anonfun$reportAllBlocks$3.apply(BlockManager.scala:217); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at org.apache.spark.storage.BlockManager.reportAllBlocks(BlockManager.scala:217); at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:236); at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:522); at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply$mcV$sp(Executor.scala:547); at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:547); at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:547); at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1953); at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:547); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4515
https://github.com/broadinstitute/gatk/issues/4515:625,Availability,heartbeat,heartbeater,625,"Hello, i'm trying to run GATK4 BaseRecalibratorSpark in local in a nextflow pipeline, but I got every time the same error with this command :. **gatk-launch BaseRecalibratorSpark --spark-runner LOCAL --spark-master local[23] -R $indexbit -I ${input_bam} --known-sites ${params.dbsnpAll} -L ${params.targetBed} -O ${output_name}-recalibration_report.grp**. Here is the full error output, basically, the failure seems to be linked to an absence of activity of the BaseRecalibratorSpark during 12s which kill the spark heartbeat and then the processus :. ```; 18/03/09 09:22:08 WARN Executor: Issue communicating with driver in heartbeater; java.lang.NullPointerException; at org.apache.spark.storage.memory.MemoryStore.getSize(MemoryStore.scala:127); at org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$getCurrentBlockStatus(BlockManager.scala:387); at org.apache.spark.storage.BlockManager$$anonfun$reportAllBlocks$3.apply(BlockManager.scala:218); at org.apache.spark.storage.BlockManager$$anonfun$reportAllBlocks$3.apply(BlockManager.scala:217); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at org.apache.spark.storage.BlockManager.reportAllBlocks(BlockManager.scala:217); at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:236); at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:522); at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply$mcV$sp(Executor.scala:547); at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:547); at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:547); at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1953); at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:547); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4515
https://github.com/broadinstitute/gatk/issues/4515:2719,Availability,down,down,2719,"ly(Executor.scala:547); at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1953); at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:547); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308); at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180); at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); 18/03/09 09:22:08 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/03/09 09:22:08 INFO SparkContext: Successfully stopped SparkContext; 09:22:08.389 INFO BaseRecalibratorSpark - Shutting down engine; [March 9, 2018 9:22:08 AM UTC] org.broadinstitute.hellbender.tools.spark.BaseRecalibratorSpark done. Elapsed time: 61.53 minutes.; Runtime.totalMemory()=16815489024; org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 0.0 failed 1 times, most recent failure: Lost task 8.0 in stage 0.0 (TID 8, localhost): ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 126542 ms; Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4515
https://github.com/broadinstitute/gatk/issues/4515:2956,Availability,failure,failure,2956,"utors.java:511); at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308); at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180); at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); 18/03/09 09:22:08 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/03/09 09:22:08 INFO SparkContext: Successfully stopped SparkContext; 09:22:08.389 INFO BaseRecalibratorSpark - Shutting down engine; [March 9, 2018 9:22:08 AM UTC] org.broadinstitute.hellbender.tools.spark.BaseRecalibratorSpark done. Elapsed time: 61.53 minutes.; Runtime.totalMemory()=16815489024; org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 0.0 failed 1 times, most recent failure: Lost task 8.0 in stage 0.0 (TID 8, localhost): ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 126542 ms; Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$han",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4515
https://github.com/broadinstitute/gatk/issues/4515:3013,Availability,failure,failure,3013,"eset(FutureTask.java:308); at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180); at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); 18/03/09 09:22:08 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/03/09 09:22:08 INFO SparkContext: Successfully stopped SparkContext; 09:22:08.389 INFO BaseRecalibratorSpark - Shutting down engine; [March 9, 2018 9:22:08 AM UTC] org.broadinstitute.hellbender.tools.spark.BaseRecalibratorSpark done. Elapsed time: 61.53 minutes.; Runtime.totalMemory()=16815489024; org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 0.0 failed 1 times, most recent failure: Lost task 8.0 in stage 0.0 (TID 8, localhost): ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 126542 ms; Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.O",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4515
https://github.com/broadinstitute/gatk/issues/4515:3166,Availability,heartbeat,heartbeat,3166,"xecutor.java:180); at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); 18/03/09 09:22:08 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/03/09 09:22:08 INFO SparkContext: Successfully stopped SparkContext; 09:22:08.389 INFO BaseRecalibratorSpark - Shutting down engine; [March 9, 2018 9:22:08 AM UTC] org.broadinstitute.hellbender.tools.spark.BaseRecalibratorSpark done. Elapsed time: 61.53 minutes.; Runtime.totalMemory()=16815489024; org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 0.0 failed 1 times, most recent failure: Lost task 8.0 in stage 0.0 (TID 8, localhost): ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 126542 ms; Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.ap",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4515
https://github.com/broadinstitute/gatk/issues/4515:6898,Availability,heartbeat,heartbeatInterval,6898,"he.spark.rdd.RDD.treeAggregate(RDD.scala:1104); at org.apache.spark.api.java.JavaRDDLike$class.treeAggregate(JavaRDDLike.scala:438); at org.apache.spark.api.java.AbstractJavaRDDLike.treeAggregate(JavaRDDLike.scala:45); at org.broadinstitute.hellbender.tools.spark.transforms.BaseRecalibratorSparkFn.apply(BaseRecalibratorSparkFn.java:39); at org.broadinstitute.hellbender.tools.spark.BaseRecalibratorSpark.runTool(BaseRecalibratorSpark.java:159); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:387); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:135); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:180); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:199); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:159); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:201); at org.broadinstitute.hellbender.Main.main(Main.java:287); 18/03/09 09:22:08 INFO ShutdownHookManager: Shutdown hook called; 18/03/09 09:22:08 INFO ShutdownHookManager: Deleting directory /tmp/qtestard/spark-af28085b-4317-4d0f-85dd-a966b1d26d9d; [command.txt](https://github.com/broadinstitute/gatk/files/1796441/command.txt). ```. I got the same one when I try to execute it outside of nextflow. I also tried to run it with --conf spark.executor.heartbeatInterval=120, but it seems it is useless, i'm not sure it is the good syntax for a local execution of spark. Here is attached, the complete log file. [command.txt](https://github.com/broadinstitute/gatk/files/1796471/command.txt). Can you help me ?. Also posted on the [GATK forum](https://gatkforums.broadinstitute.org/gatk/discussion/11587/gatk4-baserecalibratorspark-executor-heartbeat-timed-out-after-x-ms#latest). Best regards.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4515
https://github.com/broadinstitute/gatk/issues/4515:7286,Availability,heartbeat,heartbeat-timed-out-after-x-ms,7286,"he.spark.rdd.RDD.treeAggregate(RDD.scala:1104); at org.apache.spark.api.java.JavaRDDLike$class.treeAggregate(JavaRDDLike.scala:438); at org.apache.spark.api.java.AbstractJavaRDDLike.treeAggregate(JavaRDDLike.scala:45); at org.broadinstitute.hellbender.tools.spark.transforms.BaseRecalibratorSparkFn.apply(BaseRecalibratorSparkFn.java:39); at org.broadinstitute.hellbender.tools.spark.BaseRecalibratorSpark.runTool(BaseRecalibratorSpark.java:159); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:387); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:135); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:180); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:199); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:159); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:201); at org.broadinstitute.hellbender.Main.main(Main.java:287); 18/03/09 09:22:08 INFO ShutdownHookManager: Shutdown hook called; 18/03/09 09:22:08 INFO ShutdownHookManager: Deleting directory /tmp/qtestard/spark-af28085b-4317-4d0f-85dd-a966b1d26d9d; [command.txt](https://github.com/broadinstitute/gatk/files/1796441/command.txt). ```. I got the same one when I try to execute it outside of nextflow. I also tried to run it with --conf spark.executor.heartbeatInterval=120, but it seems it is useless, i'm not sure it is the good syntax for a local execution of spark. Here is attached, the complete log file. [command.txt](https://github.com/broadinstitute/gatk/files/1796471/command.txt). Can you help me ?. Also posted on the [GATK forum](https://gatkforums.broadinstitute.org/gatk/discussion/11587/gatk4-baserecalibratorspark-executor-heartbeat-timed-out-after-x-ms#latest). Best regards.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4515
https://github.com/broadinstitute/gatk/issues/4515:76,Deployability,pipeline,pipeline,76,"Hello, i'm trying to run GATK4 BaseRecalibratorSpark in local in a nextflow pipeline, but I got every time the same error with this command :. **gatk-launch BaseRecalibratorSpark --spark-runner LOCAL --spark-master local[23] -R $indexbit -I ${input_bam} --known-sites ${params.dbsnpAll} -L ${params.targetBed} -O ${output_name}-recalibration_report.grp**. Here is the full error output, basically, the failure seems to be linked to an absence of activity of the BaseRecalibratorSpark during 12s which kill the spark heartbeat and then the processus :. ```; 18/03/09 09:22:08 WARN Executor: Issue communicating with driver in heartbeater; java.lang.NullPointerException; at org.apache.spark.storage.memory.MemoryStore.getSize(MemoryStore.scala:127); at org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$getCurrentBlockStatus(BlockManager.scala:387); at org.apache.spark.storage.BlockManager$$anonfun$reportAllBlocks$3.apply(BlockManager.scala:218); at org.apache.spark.storage.BlockManager$$anonfun$reportAllBlocks$3.apply(BlockManager.scala:217); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at org.apache.spark.storage.BlockManager.reportAllBlocks(BlockManager.scala:217); at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:236); at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:522); at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply$mcV$sp(Executor.scala:547); at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:547); at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:547); at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1953); at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:547); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4515
https://github.com/broadinstitute/gatk/issues/4515:2059,Energy Efficiency,Schedul,ScheduledThreadPoolExecutor,2059,"collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at org.apache.spark.storage.BlockManager.reportAllBlocks(BlockManager.scala:217); at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:236); at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:522); at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply$mcV$sp(Executor.scala:547); at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:547); at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:547); at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1953); at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:547); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308); at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180); at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); 18/03/09 09:22:08 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/03/09 09:22:08 INFO SparkContext: Successfully stopped SparkContext; 09:22:08.389 INFO BaseRecalibratorSpark - Shutting down engine; [March 9, 2018 9:22:08 AM UTC] org.broadinstitute.hellbender.tools.spark.BaseRecalibratorSpark done. Elapsed time: 61.53 minutes.; Runtime.totalMemory()=16815489024; org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 0.0 failed 1 times, most recent failure: Lost task 8.0 in stage 0.0 (TID 8, localhost): ExecutorLostFa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4515
https://github.com/broadinstitute/gatk/issues/4515:2087,Energy Efficiency,Schedul,ScheduledFutureTask,2087,"collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at org.apache.spark.storage.BlockManager.reportAllBlocks(BlockManager.scala:217); at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:236); at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:522); at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply$mcV$sp(Executor.scala:547); at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:547); at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:547); at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1953); at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:547); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308); at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180); at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); 18/03/09 09:22:08 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/03/09 09:22:08 INFO SparkContext: Successfully stopped SparkContext; 09:22:08.389 INFO BaseRecalibratorSpark - Shutting down engine; [March 9, 2018 9:22:08 AM UTC] org.broadinstitute.hellbender.tools.spark.BaseRecalibratorSpark done. Elapsed time: 61.53 minutes.; Runtime.totalMemory()=16815489024; org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 0.0 failed 1 times, most recent failure: Lost task 8.0 in stage 0.0 (TID 8, localhost): ExecutorLostFa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4515
https://github.com/broadinstitute/gatk/issues/4515:2118,Energy Efficiency,Schedul,ScheduledThreadPoolExecutor,2118,"scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at org.apache.spark.storage.BlockManager.reportAllBlocks(BlockManager.scala:217); at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:236); at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:522); at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply$mcV$sp(Executor.scala:547); at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:547); at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:547); at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1953); at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:547); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308); at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180); at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); 18/03/09 09:22:08 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/03/09 09:22:08 INFO SparkContext: Successfully stopped SparkContext; 09:22:08.389 INFO BaseRecalibratorSpark - Shutting down engine; [March 9, 2018 9:22:08 AM UTC] org.broadinstitute.hellbender.tools.spark.BaseRecalibratorSpark done. Elapsed time: 61.53 minutes.; Runtime.totalMemory()=16815489024; org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 0.0 failed 1 times, most recent failure: Lost task 8.0 in stage 0.0 (TID 8, localhost): ExecutorLostFailure (executor driver exited caused by one ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4515
https://github.com/broadinstitute/gatk/issues/4515:2181,Energy Efficiency,Schedul,ScheduledThreadPoolExecutor,2181,"at org.apache.spark.storage.BlockManager.reportAllBlocks(BlockManager.scala:217); at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:236); at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:522); at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply$mcV$sp(Executor.scala:547); at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:547); at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:547); at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1953); at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:547); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308); at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180); at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); 18/03/09 09:22:08 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/03/09 09:22:08 INFO SparkContext: Successfully stopped SparkContext; 09:22:08.389 INFO BaseRecalibratorSpark - Shutting down engine; [March 9, 2018 9:22:08 AM UTC] org.broadinstitute.hellbender.tools.spark.BaseRecalibratorSpark done. Elapsed time: 61.53 minutes.; Runtime.totalMemory()=16815489024; org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 0.0 failed 1 times, most recent failure: Lost task 8.0 in stage 0.0 (TID 8, localhost): ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 126542 ms; Dr",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4515
https://github.com/broadinstitute/gatk/issues/4515:2209,Energy Efficiency,Schedul,ScheduledFutureTask,2209,"at org.apache.spark.storage.BlockManager.reportAllBlocks(BlockManager.scala:217); at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:236); at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:522); at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply$mcV$sp(Executor.scala:547); at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:547); at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:547); at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1953); at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:547); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308); at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180); at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); 18/03/09 09:22:08 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/03/09 09:22:08 INFO SparkContext: Successfully stopped SparkContext; 09:22:08.389 INFO BaseRecalibratorSpark - Shutting down engine; [March 9, 2018 9:22:08 AM UTC] org.broadinstitute.hellbender.tools.spark.BaseRecalibratorSpark done. Elapsed time: 61.53 minutes.; Runtime.totalMemory()=16815489024; org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 0.0 failed 1 times, most recent failure: Lost task 8.0 in stage 0.0 (TID 8, localhost): ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 126542 ms; Dr",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4515
https://github.com/broadinstitute/gatk/issues/4515:2233,Energy Efficiency,Schedul,ScheduledThreadPoolExecutor,2233,".reportAllBlocks(BlockManager.scala:217); at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:236); at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:522); at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply$mcV$sp(Executor.scala:547); at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:547); at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:547); at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1953); at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:547); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308); at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180); at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); 18/03/09 09:22:08 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/03/09 09:22:08 INFO SparkContext: Successfully stopped SparkContext; 09:22:08.389 INFO BaseRecalibratorSpark - Shutting down engine; [March 9, 2018 9:22:08 AM UTC] org.broadinstitute.hellbender.tools.spark.BaseRecalibratorSpark done. Elapsed time: 61.53 minutes.; Runtime.totalMemory()=16815489024; org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 0.0 failed 1 times, most recent failure: Lost task 8.0 in stage 0.0 (TID 8, localhost): ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 126542 ms; Driver stacktrace:; at org.apache.spark.sc",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4515
https://github.com/broadinstitute/gatk/issues/4515:3243,Energy Efficiency,schedul,scheduler,3243,"PoolExecutor.java:294); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); 18/03/09 09:22:08 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/03/09 09:22:08 INFO SparkContext: Successfully stopped SparkContext; 09:22:08.389 INFO BaseRecalibratorSpark - Shutting down engine; [March 9, 2018 9:22:08 AM UTC] org.broadinstitute.hellbender.tools.spark.BaseRecalibratorSpark done. Elapsed time: 61.53 minutes.; Runtime.totalMemory()=16815489024; org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 0.0 failed 1 times, most recent failure: Lost task 8.0 in stage 0.0 (TID 8, localhost): ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 126542 ms; Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.s",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4515
https://github.com/broadinstitute/gatk/issues/4515:3283,Energy Efficiency,schedul,scheduler,3283,"xecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); 18/03/09 09:22:08 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/03/09 09:22:08 INFO SparkContext: Successfully stopped SparkContext; 09:22:08.389 INFO BaseRecalibratorSpark - Shutting down engine; [March 9, 2018 9:22:08 AM UTC] org.broadinstitute.hellbender.tools.spark.BaseRecalibratorSpark done. Elapsed time: 61.53 minutes.; Runtime.totalMemory()=16815489024; org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 0.0 failed 1 times, most recent failure: Lost task 8.0 in stage 0.0 (TID 8, localhost): ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 126542 ms; Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSchedule",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4515
https://github.com/broadinstitute/gatk/issues/4515:3381,Energy Efficiency,schedul,scheduler,3381,"PoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); 18/03/09 09:22:08 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/03/09 09:22:08 INFO SparkContext: Successfully stopped SparkContext; 09:22:08.389 INFO BaseRecalibratorSpark - Shutting down engine; [March 9, 2018 9:22:08 AM UTC] org.broadinstitute.hellbender.tools.spark.BaseRecalibratorSpark done. Elapsed time: 61.53 minutes.; Runtime.totalMemory()=16815489024; org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 0.0 failed 1 times, most recent failure: Lost task 8.0 in stage 0.0 (TID 8, localhost): ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 126542 ms; Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onRec",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4515
https://github.com/broadinstitute/gatk/issues/4515:3478,Energy Efficiency,schedul,scheduler,3478,"8/03/09 09:22:08 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/03/09 09:22:08 INFO SparkContext: Successfully stopped SparkContext; 09:22:08.389 INFO BaseRecalibratorSpark - Shutting down engine; [March 9, 2018 9:22:08 AM UTC] org.broadinstitute.hellbender.tools.spark.BaseRecalibratorSpark done. Elapsed time: 61.53 minutes.; Runtime.totalMemory()=16815489024; org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 0.0 failed 1 times, most recent failure: Lost task 8.0 in stage 0.0 (TID 8, localhost): ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 126542 ms; Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4515
https://github.com/broadinstitute/gatk/issues/4515:3729,Energy Efficiency,schedul,scheduler,3729,"arch 9, 2018 9:22:08 AM UTC] org.broadinstitute.hellbender.tools.spark.BaseRecalibratorSpark done. Elapsed time: 61.53 minutes.; Runtime.totalMemory()=16815489024; org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 0.0 failed 1 times, most recent failure: Lost task 8.0 in stage 0.0 (TID 8, localhost): ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 126542 ms; Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1936); at org.apache.spark.rdd.RDD$$anonfun$reduc",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4515
https://github.com/broadinstitute/gatk/issues/4515:3809,Energy Efficiency,schedul,scheduler,3809,"ibratorSpark done. Elapsed time: 61.53 minutes.; Runtime.totalMemory()=16815489024; org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 0.0 failed 1 times, most recent failure: Lost task 8.0 in stage 0.0 (TID 8, localhost): ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 126542 ms; Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1936); at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1002); at org.apache.spark.rdd.RDDOperationScope$.withScope(",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4515
https://github.com/broadinstitute/gatk/issues/4515:3914,Energy Efficiency,schedul,scheduler,3914,"kException: Job aborted due to stage failure: Task 8 in stage 0.0 failed 1 times, most recent failure: Lost task 8.0 in stage 0.0 (TID 8, localhost): ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 126542 ms; Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1936); at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1002); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scal",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4515
https://github.com/broadinstitute/gatk/issues/4515:4062,Energy Efficiency,schedul,scheduler,4062,: ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 126542 ms; Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1936); at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1002); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); at org.apache.spark.rdd.RDD.reduce(RDD.scala:984); at org.apache.spark.rdd.RDD$$anonfu,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4515
https://github.com/broadinstitute/gatk/issues/4515:4150,Energy Efficiency,schedul,scheduler,4150,: Executor heartbeat timed out after 126542 ms; Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1936); at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1002); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); at org.apache.spark.rdd.RDD.reduce(RDD.scala:984); at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1127); at org.apache.spark.rdd.RDDOperationScope$.with,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4515
https://github.com/broadinstitute/gatk/issues/4515:4247,Energy Efficiency,schedul,scheduler,4247,.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1936); at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1002); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); at org.apache.spark.rdd.RDD.reduce(RDD.scala:984); at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1127); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOpera,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4515
https://github.com/broadinstitute/gatk/issues/4515:4342,Energy Efficiency,schedul,scheduler,4342,.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1936); at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1002); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); at org.apache.spark.rdd.RDD.reduce(RDD.scala:984); at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1127); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); at org.apache.spark,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4515
https://github.com/broadinstitute/gatk/issues/4515:4505,Energy Efficiency,schedul,scheduler,4505,abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1936); at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1002); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); at org.apache.spark.rdd.RDD.reduce(RDD.scala:984); at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1127); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); at org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1104); at org.apache.spark.api.java.JavaRDDLike$class.treeAggregate(JavaRDDLike.scala:438); at org.apache.spark.api.java.AbstractJ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4515
https://github.com/broadinstitute/gatk/issues/4515:4729,Energy Efficiency,reduce,reduce,4729,k.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1936); at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1002); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); at org.apache.spark.rdd.RDD.reduce(RDD.scala:984); at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1127); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); at org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1104); at org.apache.spark.api.java.JavaRDDLike$class.treeAggregate(JavaRDDLike.scala:438); at org.apache.spark.api.java.AbstractJavaRDDLike.treeAggregate(JavaRDDLike.scala:45); at org.broadinstitute.hellbender.tools.spark.transforms.BaseRecalibratorSparkFn.apply(BaseRecalibratorSparkFn.java:39); at org.broadinstitute.hellbender.tools.spark.Base,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4515
https://github.com/broadinstitute/gatk/issues/4515:5009,Energy Efficiency,reduce,reduce,5009,.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1936); at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1002); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); at org.apache.spark.rdd.RDD.reduce(RDD.scala:984); at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1127); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); at org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1104); at org.apache.spark.api.java.JavaRDDLike$class.treeAggregate(JavaRDDLike.scala:438); at org.apache.spark.api.java.AbstractJavaRDDLike.treeAggregate(JavaRDDLike.scala:45); at org.broadinstitute.hellbender.tools.spark.transforms.BaseRecalibratorSparkFn.apply(BaseRecalibratorSparkFn.java:39); at org.broadinstitute.hellbender.tools.spark.BaseRecalibratorSpark.runTool(BaseRecalibratorSpark.java:159); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:387); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); at org.broadinstitute,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4515
https://github.com/broadinstitute/gatk/issues/4515:1903,Performance,concurren,concurrent,1903,"kManager$$anonfun$reportAllBlocks$3.apply(BlockManager.scala:218); at org.apache.spark.storage.BlockManager$$anonfun$reportAllBlocks$3.apply(BlockManager.scala:217); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at org.apache.spark.storage.BlockManager.reportAllBlocks(BlockManager.scala:217); at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:236); at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:522); at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply$mcV$sp(Executor.scala:547); at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:547); at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:547); at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1953); at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:547); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308); at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180); at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); 18/03/09 09:22:08 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/03/09 09:22:08 INFO SparkContext: Successfully stopped SparkContext; 09:22:08.389 INFO BaseRecalibratorSpark - Shutting down engine; [March 9, 2018 9:22:08 AM UTC] org.broadinstitute.hellbender.tools.spark.BaseRecalibratorSpark done. Elapsed time: 61.53 minutes.; Runtime.totalMemory()=16815489024; org.apache.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4515
https://github.com/broadinstitute/gatk/issues/4515:1979,Performance,concurren,concurrent,1979,"ache.spark.storage.BlockManager$$anonfun$reportAllBlocks$3.apply(BlockManager.scala:217); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at org.apache.spark.storage.BlockManager.reportAllBlocks(BlockManager.scala:217); at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:236); at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:522); at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply$mcV$sp(Executor.scala:547); at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:547); at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:547); at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1953); at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:547); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308); at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180); at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); 18/03/09 09:22:08 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/03/09 09:22:08 INFO SparkContext: Successfully stopped SparkContext; 09:22:08.389 INFO BaseRecalibratorSpark - Shutting down engine; [March 9, 2018 9:22:08 AM UTC] org.broadinstitute.hellbender.tools.spark.BaseRecalibratorSpark done. Elapsed time: 61.53 minutes.; Runtime.totalMemory()=16815489024; org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 0.0 ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4515
https://github.com/broadinstitute/gatk/issues/4515:2048,Performance,concurren,concurrent,2048,"kManager.scala:217); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at org.apache.spark.storage.BlockManager.reportAllBlocks(BlockManager.scala:217); at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:236); at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:522); at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply$mcV$sp(Executor.scala:547); at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:547); at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:547); at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1953); at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:547); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308); at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180); at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); 18/03/09 09:22:08 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/03/09 09:22:08 INFO SparkContext: Successfully stopped SparkContext; 09:22:08.389 INFO BaseRecalibratorSpark - Shutting down engine; [March 9, 2018 9:22:08 AM UTC] org.broadinstitute.hellbender.tools.spark.BaseRecalibratorSpark done. Elapsed time: 61.53 minutes.; Runtime.totalMemory()=16815489024; org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 0.0 failed 1 times, most recent failure: Lost task 8.0 in stage 0.0 (TID ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4515
https://github.com/broadinstitute/gatk/issues/4515:2170,Performance,concurren,concurrent,2170,"foreach(Iterator.scala:1336); at org.apache.spark.storage.BlockManager.reportAllBlocks(BlockManager.scala:217); at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:236); at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:522); at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply$mcV$sp(Executor.scala:547); at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:547); at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:547); at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1953); at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:547); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308); at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180); at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); 18/03/09 09:22:08 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/03/09 09:22:08 INFO SparkContext: Successfully stopped SparkContext; 09:22:08.389 INFO BaseRecalibratorSpark - Shutting down engine; [March 9, 2018 9:22:08 AM UTC] org.broadinstitute.hellbender.tools.spark.BaseRecalibratorSpark done. Elapsed time: 61.53 minutes.; Runtime.totalMemory()=16815489024; org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 0.0 failed 1 times, most recent failure: Lost task 8.0 in stage 0.0 (TID 8, localhost): ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4515
https://github.com/broadinstitute/gatk/issues/4515:2285,Performance,concurren,concurrent,2285,"org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:236); at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:522); at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply$mcV$sp(Executor.scala:547); at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:547); at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:547); at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1953); at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:547); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308); at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180); at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); 18/03/09 09:22:08 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/03/09 09:22:08 INFO SparkContext: Successfully stopped SparkContext; 09:22:08.389 INFO BaseRecalibratorSpark - Shutting down engine; [March 9, 2018 9:22:08 AM UTC] org.broadinstitute.hellbender.tools.spark.BaseRecalibratorSpark done. Elapsed time: 61.53 minutes.; Runtime.totalMemory()=16815489024; org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 0.0 failed 1 times, most recent failure: Lost task 8.0 in stage 0.0 (TID 8, localhost): ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 126542 ms; Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$schedule",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4515
https://github.com/broadinstitute/gatk/issues/4515:2369,Performance,concurren,concurrent,2369,"che.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:522); at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply$mcV$sp(Executor.scala:547); at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:547); at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:547); at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1953); at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:547); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308); at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180); at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); 18/03/09 09:22:08 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/03/09 09:22:08 INFO SparkContext: Successfully stopped SparkContext; 09:22:08.389 INFO BaseRecalibratorSpark - Shutting down engine; [March 9, 2018 9:22:08 AM UTC] org.broadinstitute.hellbender.tools.spark.BaseRecalibratorSpark done. Elapsed time: 61.53 minutes.; Runtime.totalMemory()=16815489024; org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 0.0 failed 1 times, most recent failure: Lost task 8.0 in stage 0.0 (TID 8, localhost): ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 126542 ms; Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4515
https://github.com/broadinstitute/gatk/issues/4515:2935,Safety,abort,aborted,2935,"utors.java:511); at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308); at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180); at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); 18/03/09 09:22:08 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/03/09 09:22:08 INFO SparkContext: Successfully stopped SparkContext; 09:22:08.389 INFO BaseRecalibratorSpark - Shutting down engine; [March 9, 2018 9:22:08 AM UTC] org.broadinstitute.hellbender.tools.spark.BaseRecalibratorSpark done. Elapsed time: 61.53 minutes.; Runtime.totalMemory()=16815489024; org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 0.0 failed 1 times, most recent failure: Lost task 8.0 in stage 0.0 (TID 8, localhost): ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 126542 ms; Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$han",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4515
https://github.com/broadinstitute/gatk/issues/4515:3413,Safety,abort,abortStage,3413,"n(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); 18/03/09 09:22:08 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/03/09 09:22:08 INFO SparkContext: Successfully stopped SparkContext; 09:22:08.389 INFO BaseRecalibratorSpark - Shutting down engine; [March 9, 2018 9:22:08 AM UTC] org.broadinstitute.hellbender.tools.spark.BaseRecalibratorSpark done. Elapsed time: 61.53 minutes.; Runtime.totalMemory()=16815489024; org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 0.0 failed 1 times, most recent failure: Lost task 8.0 in stage 0.0 (TID 8, localhost): ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 126542 ms; Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4515
https://github.com/broadinstitute/gatk/issues/4515:3510,Safety,abort,abortStage,3510,"OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/03/09 09:22:08 INFO SparkContext: Successfully stopped SparkContext; 09:22:08.389 INFO BaseRecalibratorSpark - Shutting down engine; [March 9, 2018 9:22:08 AM UTC] org.broadinstitute.hellbender.tools.spark.BaseRecalibratorSpark done. Elapsed time: 61.53 minutes.; Runtime.totalMemory()=16815489024; org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 0.0 failed 1 times, most recent failure: Lost task 8.0 in stage 0.0 (TID 8, localhost): ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 126542 ms; Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.s",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4515
https://github.com/broadinstitute/gatk/issues/4515:3752,Safety,abort,abortStage,3752,"rg.broadinstitute.hellbender.tools.spark.BaseRecalibratorSpark done. Elapsed time: 61.53 minutes.; Runtime.totalMemory()=16815489024; org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 0.0 failed 1 times, most recent failure: Lost task 8.0 in stage 0.0 (TID 8, localhost): ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 126542 ms; Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1936); at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1002); at ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4515
https://github.com/broadinstitute/gatk/issues/4515:2107,Security,access,access,2107,"scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at org.apache.spark.storage.BlockManager.reportAllBlocks(BlockManager.scala:217); at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:236); at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:522); at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply$mcV$sp(Executor.scala:547); at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:547); at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:547); at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1953); at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:547); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308); at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180); at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); 18/03/09 09:22:08 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/03/09 09:22:08 INFO SparkContext: Successfully stopped SparkContext; 09:22:08.389 INFO BaseRecalibratorSpark - Shutting down engine; [March 9, 2018 9:22:08 AM UTC] org.broadinstitute.hellbender.tools.spark.BaseRecalibratorSpark done. Elapsed time: 61.53 minutes.; Runtime.totalMemory()=16815489024; org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 0.0 failed 1 times, most recent failure: Lost task 8.0 in stage 0.0 (TID 8, localhost): ExecutorLostFailure (executor driver exited caused by one ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4515
https://github.com/broadinstitute/gatk/issues/4515:1778,Testability,log,logUncaughtExceptions,1778,"g$apache$spark$storage$BlockManager$$getCurrentBlockStatus(BlockManager.scala:387); at org.apache.spark.storage.BlockManager$$anonfun$reportAllBlocks$3.apply(BlockManager.scala:218); at org.apache.spark.storage.BlockManager$$anonfun$reportAllBlocks$3.apply(BlockManager.scala:217); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at org.apache.spark.storage.BlockManager.reportAllBlocks(BlockManager.scala:217); at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:236); at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:522); at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply$mcV$sp(Executor.scala:547); at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:547); at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:547); at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1953); at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:547); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308); at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180); at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); 18/03/09 09:22:08 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/03/09 09:22:08 INFO SparkContext: Successfully stopped SparkContext; 09:22:08.389 INFO BaseRecalibratorSpark - Shutting down engine; [March 9, 2018 9:22:08 AM UTC] org.broadinstitute.hellbender",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4515
https://github.com/broadinstitute/gatk/issues/4515:7047,Testability,log,log,7047,"he.spark.rdd.RDD.treeAggregate(RDD.scala:1104); at org.apache.spark.api.java.JavaRDDLike$class.treeAggregate(JavaRDDLike.scala:438); at org.apache.spark.api.java.AbstractJavaRDDLike.treeAggregate(JavaRDDLike.scala:45); at org.broadinstitute.hellbender.tools.spark.transforms.BaseRecalibratorSparkFn.apply(BaseRecalibratorSparkFn.java:39); at org.broadinstitute.hellbender.tools.spark.BaseRecalibratorSpark.runTool(BaseRecalibratorSpark.java:159); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:387); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:135); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:180); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:199); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:159); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:201); at org.broadinstitute.hellbender.Main.main(Main.java:287); 18/03/09 09:22:08 INFO ShutdownHookManager: Shutdown hook called; 18/03/09 09:22:08 INFO ShutdownHookManager: Deleting directory /tmp/qtestard/spark-af28085b-4317-4d0f-85dd-a966b1d26d9d; [command.txt](https://github.com/broadinstitute/gatk/files/1796441/command.txt). ```. I got the same one when I try to execute it outside of nextflow. I also tried to run it with --conf spark.executor.heartbeatInterval=120, but it seems it is useless, i'm not sure it is the good syntax for a local execution of spark. Here is attached, the complete log file. [command.txt](https://github.com/broadinstitute/gatk/files/1796471/command.txt). Can you help me ?. Also posted on the [GATK forum](https://gatkforums.broadinstitute.org/gatk/discussion/11587/gatk4-baserecalibratorspark-executor-heartbeat-timed-out-after-x-ms#latest). Best regards.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4515
https://github.com/broadinstitute/gatk/issues/4518:128,Availability,error,error,128,"About 3% of our GATK 4.0.0.0 GenotypeGVCFs runs (with a GenomicsDB as input) are failing with a `__pthread_tpp_change_priority` error and exiting with status -6. The stderr of such a run ends like this:; ```; 2018-03-10T07:14:27.165578644Z GENOMICSDB_TIMER,GenomicsDB iterator next() timer,Wall-clock time(s),74.01474455399772,Cpu time(s),62.96424693700022; 2018-03-10T07:14:27.168329699Z java: tpp.c:84: __pthread_tpp_change_priority: Assertion `new_prio == -1 || (new_prio >= fifo_min_prio && new_prio <= fifo_max_prio)' failed.; ```. Stdout from the same run:; ```; 2018-03-09T13:13:41.095913747Z 13:13:41.095 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk.jar!/com/intel/gkl/native/libgkl_compression.so; 2018-03-09T13:13:41.329888610Z 13:13:41.327 INFO GenotypeGVCFs - ------------------------------------------------------------; 2018-03-09T13:13:41.329934964Z 13:13:41.327 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.0.0.0; 2018-03-09T13:13:41.329942970Z 13:13:41.327 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 2018-03-09T13:13:41.329952404Z 13:13:41.328 INFO GenotypeGVCFs - Executing as root@localhost on Linux v4.4.0-112-generic amd64; 2018-03-09T13:13:41.329960555Z 13:13:41.328 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_151-8u151-b12-1~deb9u1-b12; 2018-03-09T13:13:41.329988865Z 13:13:41.328 INFO GenotypeGVCFs - Start Date/Time: March 9, 2018 1:13:41 PM UTC; 2018-03-09T13:13:41.329995417Z 13:13:41.328 INFO GenotypeGVCFs - ------------------------------------------------------------; 2018-03-09T13:13:41.330000910Z 13:13:41.328 INFO GenotypeGVCFs - ------------------------------------------------------------; 2018-03-09T13:13:41.330011002Z 13:13:41.328 INFO GenotypeGVCFs - HTSJDK Version: 2.13.2; 2018-03-09T13:13:41.330022980Z 13:13:41.328 INFO GenotypeGVCFs - Picard Version: 2.17.2; 2018-03-09T13:13:41.330030226Z 13:13:41.329 INFO GenotypeGVCFs - ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4518
https://github.com/broadinstitute/gatk/issues/4518:4759,Availability,down,down,4759,1bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 2018-03-09T13:13:41.330127806Z 13:13:41.329 INFO GenotypeGVCFs - Initializing engine; 2018-03-09T13:13:44.528605497Z 13:13:44.528 INFO GenotypeGVCFs - Done initializing engine; 2018-03-09T13:13:45.237843760Z 13:13:45.235 INFO ProgressMeter - Starting traversal; 2018-03-09T13:13:45.237903383Z 13:13:45.235 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 2018-03-09T13:13:56.665869885Z 13:13:56.662 INFO ProgressMeter - chr13:82078938 0.2 13000 68265.4; 2018-03-09T13:14:07.517952300Z 13:14:07.517 INFO ProgressMeter - chr13:82096938 0.4 31000 83475.5; 2018-03-09T13:14:17.546110604Z 13:14:17.545 INFO ProgressMeter - chr13:82123938 0.5 58000 107706.6; 2018-03-09T13:14:28.760222694Z 13:14:28.759 INFO ProgressMeter - chr13:82144938 0.7 79000 108905.4; 2018-03-09T13:14:39.292149466Z 13:14:39.289 INFO ProgressMeter - chr13:82169938 0.9 104000 115440.1; 2018-03-09T13:14:49.877851947Z 13:14:49.873 INFO ProgressMeter - chr13:82193938 1.1 128000 118815.6; 2018-03-09T13:15:01.109839332Z 13:15:01.106 INFO ProgressMeter - chr13:82224938 1.3 159000 125741.4; 2018-03-09T13:15:11.237868051Z 13:15:11.234 INFO ProgressMeter - chr13:82250938 1.4 185000 129071.3; 2018-03-09T13:15:22.457899462Z 13:15:22.455 INFO ProgressMeter - chr13:82284938 1.6 219000 135157.4; 2018-03-09T13:15:32.561846058Z 13:15:32.556 INFO ProgressMeter - chr13:82319938 1.8 254000 142003.9; 2018-03-09T13:15:42.624917849Z 13:15:42.624 INFO ProgressMeter - chr13:82348938 2.0 283000 144647.3; 2018-03-09T13:32:20.050317641Z 13:32:20.049 INFO ProgressMeter - chr13:82369938 18.6 304000 16361.5; 2018-03-09T13:47:24.213333084Z 13:47:24.212 INFO ProgressMeter - chr13:82373938 33.6 308000 9153.2; 2018-03-09T14:21:34.139657997Z 14:21:34.139 INFO ProgressMeter - chr13:82377938 67.8 312000 4600.7; 2018-03-10T07:14:27.162870189Z 07:14:27.161 INFO GenotypeGVCFs - Shutting down engine; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4518
https://github.com/broadinstitute/gatk/issues/4518:2766,Deployability,patch,patch,2766,41.330011002Z 13:13:41.328 INFO GenotypeGVCFs - HTSJDK Version: 2.13.2; 2018-03-09T13:13:41.330022980Z 13:13:41.328 INFO GenotypeGVCFs - Picard Version: 2.17.2; 2018-03-09T13:13:41.330030226Z 13:13:41.329 INFO GenotypeGVCFs - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 2018-03-09T13:13:41.330036559Z 13:13:41.329 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 2018-03-09T13:13:41.330045071Z 13:13:41.329 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 2018-03-09T13:13:41.330051564Z 13:13:41.329 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 2018-03-09T13:13:41.330068542Z 13:13:41.329 INFO GenotypeGVCFs - Deflater: IntelDeflater; 2018-03-09T13:13:41.330102470Z 13:13:41.329 INFO GenotypeGVCFs - Inflater: IntelInflater; 2018-03-09T13:13:41.330111084Z 13:13:41.329 INFO GenotypeGVCFs - GCS max retries/reopens: 20; 2018-03-09T13:13:41.330117286Z 13:13:41.329 INFO GenotypeGVCFs - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 2018-03-09T13:13:41.330127806Z 13:13:41.329 INFO GenotypeGVCFs - Initializing engine; 2018-03-09T13:13:44.528605497Z 13:13:44.528 INFO GenotypeGVCFs - Done initializing engine; 2018-03-09T13:13:45.237843760Z 13:13:45.235 INFO ProgressMeter - Starting traversal; 2018-03-09T13:13:45.237903383Z 13:13:45.235 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 2018-03-09T13:13:56.665869885Z 13:13:56.662 INFO ProgressMeter - chr13:82078938 0.2 13000 68265.4; 2018-03-09T13:14:07.517952300Z 13:14:07.517 INFO ProgressMeter - chr13:82096938 0.4 31000 83475.5; 2018-03-09T13:14:17.546110604Z 13:14:17.545 INFO ProgressMeter - chr13:82123938 0.5 58000 107706.6; 2018-03-09T13:14:28.760222694Z 13:14:28.759 INFO ProgressMeter - chr13:82144938 0.7 79000 108905.4; 2018-03-09T13:14:39.292149466Z 13:14:39.289 INFO ProgressMeter - chr13:82169938 0.9 104000 115440.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4518
https://github.com/broadinstitute/gatk/issues/4518:640,Performance,Load,Loading,640,"About 3% of our GATK 4.0.0.0 GenotypeGVCFs runs (with a GenomicsDB as input) are failing with a `__pthread_tpp_change_priority` error and exiting with status -6. The stderr of such a run ends like this:; ```; 2018-03-10T07:14:27.165578644Z GENOMICSDB_TIMER,GenomicsDB iterator next() timer,Wall-clock time(s),74.01474455399772,Cpu time(s),62.96424693700022; 2018-03-10T07:14:27.168329699Z java: tpp.c:84: __pthread_tpp_change_priority: Assertion `new_prio == -1 || (new_prio >= fifo_min_prio && new_prio <= fifo_max_prio)' failed.; ```. Stdout from the same run:; ```; 2018-03-09T13:13:41.095913747Z 13:13:41.095 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk.jar!/com/intel/gkl/native/libgkl_compression.so; 2018-03-09T13:13:41.329888610Z 13:13:41.327 INFO GenotypeGVCFs - ------------------------------------------------------------; 2018-03-09T13:13:41.329934964Z 13:13:41.327 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.0.0.0; 2018-03-09T13:13:41.329942970Z 13:13:41.327 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 2018-03-09T13:13:41.329952404Z 13:13:41.328 INFO GenotypeGVCFs - Executing as root@localhost on Linux v4.4.0-112-generic amd64; 2018-03-09T13:13:41.329960555Z 13:13:41.328 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_151-8u151-b12-1~deb9u1-b12; 2018-03-09T13:13:41.329988865Z 13:13:41.328 INFO GenotypeGVCFs - Start Date/Time: March 9, 2018 1:13:41 PM UTC; 2018-03-09T13:13:41.329995417Z 13:13:41.328 INFO GenotypeGVCFs - ------------------------------------------------------------; 2018-03-09T13:13:41.330000910Z 13:13:41.328 INFO GenotypeGVCFs - ------------------------------------------------------------; 2018-03-09T13:13:41.330011002Z 13:13:41.328 INFO GenotypeGVCFs - HTSJDK Version: 2.13.2; 2018-03-09T13:13:41.330022980Z 13:13:41.328 INFO GenotypeGVCFs - Picard Version: 2.17.2; 2018-03-09T13:13:41.330030226Z 13:13:41.329 INFO GenotypeGVCFs - ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4518
https://github.com/broadinstitute/gatk/issues/4518:436,Testability,Assert,Assertion,436,"About 3% of our GATK 4.0.0.0 GenotypeGVCFs runs (with a GenomicsDB as input) are failing with a `__pthread_tpp_change_priority` error and exiting with status -6. The stderr of such a run ends like this:; ```; 2018-03-10T07:14:27.165578644Z GENOMICSDB_TIMER,GenomicsDB iterator next() timer,Wall-clock time(s),74.01474455399772,Cpu time(s),62.96424693700022; 2018-03-10T07:14:27.168329699Z java: tpp.c:84: __pthread_tpp_change_priority: Assertion `new_prio == -1 || (new_prio >= fifo_min_prio && new_prio <= fifo_max_prio)' failed.; ```. Stdout from the same run:; ```; 2018-03-09T13:13:41.095913747Z 13:13:41.095 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk.jar!/com/intel/gkl/native/libgkl_compression.so; 2018-03-09T13:13:41.329888610Z 13:13:41.327 INFO GenotypeGVCFs - ------------------------------------------------------------; 2018-03-09T13:13:41.329934964Z 13:13:41.327 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.0.0.0; 2018-03-09T13:13:41.329942970Z 13:13:41.327 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 2018-03-09T13:13:41.329952404Z 13:13:41.328 INFO GenotypeGVCFs - Executing as root@localhost on Linux v4.4.0-112-generic amd64; 2018-03-09T13:13:41.329960555Z 13:13:41.328 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_151-8u151-b12-1~deb9u1-b12; 2018-03-09T13:13:41.329988865Z 13:13:41.328 INFO GenotypeGVCFs - Start Date/Time: March 9, 2018 1:13:41 PM UTC; 2018-03-09T13:13:41.329995417Z 13:13:41.328 INFO GenotypeGVCFs - ------------------------------------------------------------; 2018-03-09T13:13:41.330000910Z 13:13:41.328 INFO GenotypeGVCFs - ------------------------------------------------------------; 2018-03-09T13:13:41.330011002Z 13:13:41.328 INFO GenotypeGVCFs - HTSJDK Version: 2.13.2; 2018-03-09T13:13:41.330022980Z 13:13:41.328 INFO GenotypeGVCFs - Picard Version: 2.17.2; 2018-03-09T13:13:41.330030226Z 13:13:41.329 INFO GenotypeGVCFs - ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4518
https://github.com/broadinstitute/gatk/issues/4519:1734,Availability,recover,recovered,1734,"aluated. Using a small set of WGS SV tandem-duplication calls from @mwalker174 as a truth set, I did some experimenting with changing the count-collection strategy. (We initially thought we were missing some of these simply due to over-denoising/filtering by the PoN, but as we'll see below, the count-collection strategy plays a non-trivial role.). Subsetting to chr3, I built a small PoN of 12 normals (including the case normal) at 100bp and denoised using bin medians only (i.e., `--number-of-eigensamples 0`) to avoid denoising away common events. In chr3, the case sample had three events:. ````; chr3	8559423		8560126; chr3	64547471	64549936; chr3	90414457	90415989; ````. I tried the following, running `ModelSegments` using fairly sensitive parameters (`--number-of-changepoints-penalty-factor 0.1 --maximum-number-of-segments-per-chromosome 10000 --window-size 16 --window-size 32 --maximum-number-of-smoothing-iterations 0` in copy-ratio-only mode:. 1) CollectFragmentCounts. This only recovered event 2.; 2) CollectReadCounts - same as CollectFragmentCounts, but removing the properly-paired and first-of-pair filters and adding a count for each read to the bin containing its start. This recovered all 3 events.; 3) CollectFragmentOverlaps - same filters as CollectFragmentCounts, but adding counts to all bins overlapping each fragment. Note that we need to implement a filter on maximum fragment length, otherwise we get some strange artifacts from (incorrectly mapped?) extremely long fragments; I arbitrarily chose a cutoff of 10000bp. This recovered events 1 and 2. Event 3 seemed to be the most difficult to recover. Plotting the copy ratios surrounding this event (which spans ~15 100bp bins) yields some insights:. CollectFragmentCounts:; ![image](https://user-images.githubusercontent.com/11076296/37244188-317a7f1e-2453-11e8-937d-f7239354316e.png). CollectReadCounts:; ![image](https://user-images.githubusercontent.com/11076296/37244228-ad24908c-2453-11e8-91dd-a978578e77f4.png",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4519
https://github.com/broadinstitute/gatk/issues/4519:1938,Availability,recover,recovered,1938,"f these simply due to over-denoising/filtering by the PoN, but as we'll see below, the count-collection strategy plays a non-trivial role.). Subsetting to chr3, I built a small PoN of 12 normals (including the case normal) at 100bp and denoised using bin medians only (i.e., `--number-of-eigensamples 0`) to avoid denoising away common events. In chr3, the case sample had three events:. ````; chr3	8559423		8560126; chr3	64547471	64549936; chr3	90414457	90415989; ````. I tried the following, running `ModelSegments` using fairly sensitive parameters (`--number-of-changepoints-penalty-factor 0.1 --maximum-number-of-segments-per-chromosome 10000 --window-size 16 --window-size 32 --maximum-number-of-smoothing-iterations 0` in copy-ratio-only mode:. 1) CollectFragmentCounts. This only recovered event 2.; 2) CollectReadCounts - same as CollectFragmentCounts, but removing the properly-paired and first-of-pair filters and adding a count for each read to the bin containing its start. This recovered all 3 events.; 3) CollectFragmentOverlaps - same filters as CollectFragmentCounts, but adding counts to all bins overlapping each fragment. Note that we need to implement a filter on maximum fragment length, otherwise we get some strange artifacts from (incorrectly mapped?) extremely long fragments; I arbitrarily chose a cutoff of 10000bp. This recovered events 1 and 2. Event 3 seemed to be the most difficult to recover. Plotting the copy ratios surrounding this event (which spans ~15 100bp bins) yields some insights:. CollectFragmentCounts:; ![image](https://user-images.githubusercontent.com/11076296/37244188-317a7f1e-2453-11e8-937d-f7239354316e.png). CollectReadCounts:; ![image](https://user-images.githubusercontent.com/11076296/37244228-ad24908c-2453-11e8-91dd-a978578e77f4.png). CollectFragmentOverlaps:; ![image](https://user-images.githubusercontent.com/11076296/37244230-b25b9cee-2453-11e8-8646-f9c95365b355.png). The increased statistical noise in the CollectFragmentCounts result ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4519
https://github.com/broadinstitute/gatk/issues/4519:2295,Availability,recover,recovered,2295,"ase sample had three events:. ````; chr3	8559423		8560126; chr3	64547471	64549936; chr3	90414457	90415989; ````. I tried the following, running `ModelSegments` using fairly sensitive parameters (`--number-of-changepoints-penalty-factor 0.1 --maximum-number-of-segments-per-chromosome 10000 --window-size 16 --window-size 32 --maximum-number-of-smoothing-iterations 0` in copy-ratio-only mode:. 1) CollectFragmentCounts. This only recovered event 2.; 2) CollectReadCounts - same as CollectFragmentCounts, but removing the properly-paired and first-of-pair filters and adding a count for each read to the bin containing its start. This recovered all 3 events.; 3) CollectFragmentOverlaps - same filters as CollectFragmentCounts, but adding counts to all bins overlapping each fragment. Note that we need to implement a filter on maximum fragment length, otherwise we get some strange artifacts from (incorrectly mapped?) extremely long fragments; I arbitrarily chose a cutoff of 10000bp. This recovered events 1 and 2. Event 3 seemed to be the most difficult to recover. Plotting the copy ratios surrounding this event (which spans ~15 100bp bins) yields some insights:. CollectFragmentCounts:; ![image](https://user-images.githubusercontent.com/11076296/37244188-317a7f1e-2453-11e8-937d-f7239354316e.png). CollectReadCounts:; ![image](https://user-images.githubusercontent.com/11076296/37244228-ad24908c-2453-11e8-91dd-a978578e77f4.png). CollectFragmentOverlaps:; ![image](https://user-images.githubusercontent.com/11076296/37244230-b25b9cee-2453-11e8-8646-f9c95365b355.png). The increased statistical noise in the CollectFragmentCounts result (due to the lower overall count because of the pairing of reads) probably causes us to miss this event. Also, although CollectFragmentOverlaps initially looks pretty good, I think the bin-to-bin correlations that are evident here negatively affect segmentation. This is not an extremely rigorous evaluation, but it suggests that we should consider switching ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4519
https://github.com/broadinstitute/gatk/issues/4519:2364,Availability,recover,recover,2364,"559423		8560126; chr3	64547471	64549936; chr3	90414457	90415989; ````. I tried the following, running `ModelSegments` using fairly sensitive parameters (`--number-of-changepoints-penalty-factor 0.1 --maximum-number-of-segments-per-chromosome 10000 --window-size 16 --window-size 32 --maximum-number-of-smoothing-iterations 0` in copy-ratio-only mode:. 1) CollectFragmentCounts. This only recovered event 2.; 2) CollectReadCounts - same as CollectFragmentCounts, but removing the properly-paired and first-of-pair filters and adding a count for each read to the bin containing its start. This recovered all 3 events.; 3) CollectFragmentOverlaps - same filters as CollectFragmentCounts, but adding counts to all bins overlapping each fragment. Note that we need to implement a filter on maximum fragment length, otherwise we get some strange artifacts from (incorrectly mapped?) extremely long fragments; I arbitrarily chose a cutoff of 10000bp. This recovered events 1 and 2. Event 3 seemed to be the most difficult to recover. Plotting the copy ratios surrounding this event (which spans ~15 100bp bins) yields some insights:. CollectFragmentCounts:; ![image](https://user-images.githubusercontent.com/11076296/37244188-317a7f1e-2453-11e8-937d-f7239354316e.png). CollectReadCounts:; ![image](https://user-images.githubusercontent.com/11076296/37244228-ad24908c-2453-11e8-91dd-a978578e77f4.png). CollectFragmentOverlaps:; ![image](https://user-images.githubusercontent.com/11076296/37244230-b25b9cee-2453-11e8-8646-f9c95365b355.png). The increased statistical noise in the CollectFragmentCounts result (due to the lower overall count because of the pairing of reads) probably causes us to miss this event. Also, although CollectFragmentOverlaps initially looks pretty good, I think the bin-to-bin correlations that are evident here negatively affect segmentation. This is not an extremely rigorous evaluation, but it suggests that we should consider switching over to a CollectReadCounts-like strategy",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4519
https://github.com/broadinstitute/gatk/issues/4519:102,Deployability,pipeline,pipeline,102,"In the process of unifying CalculateTargetCoverage / SparkGenomeReadCounts for the rewrite of the CNV pipeline, we decided to experiment with switching over to fragment-based counts due to a request from CGA. For each fragment, CollectFragmentCounts adds a count to *the bin that overlaps with the fragment center*. We filter to properly-paired, first-of-pair reads in order to have well formed fragments and avoid double counting. We also filter out duplicates. In contrast, CalculateTargetCoverage added a count to *all bins that overlapped with a read* and SparkGenomeReadCounts added a count to *the bin that contained the read start*. These tools kept duplicates. However, none of these collection strategies have been rigorously evaluated. Using a small set of WGS SV tandem-duplication calls from @mwalker174 as a truth set, I did some experimenting with changing the count-collection strategy. (We initially thought we were missing some of these simply due to over-denoising/filtering by the PoN, but as we'll see below, the count-collection strategy plays a non-trivial role.). Subsetting to chr3, I built a small PoN of 12 normals (including the case normal) at 100bp and denoised using bin medians only (i.e., `--number-of-eigensamples 0`) to avoid denoising away common events. In chr3, the case sample had three events:. ````; chr3	8559423		8560126; chr3	64547471	64549936; chr3	90414457	90415989; ````. I tried the following, running `ModelSegments` using fairly sensitive parameters (`--number-of-changepoints-penalty-factor 0.1 --maximum-number-of-segments-per-chromosome 10000 --window-size 16 --window-size 32 --maximum-number-of-smoothing-iterations 0` in copy-ratio-only mode:. 1) CollectFragmentCounts. This only recovered event 2.; 2) CollectReadCounts - same as CollectFragmentCounts, but removing the properly-paired and first-of-pair filters and adding a count for each read to the bin containing its start. This recovered all 3 events.; 3) CollectFragmentOverlaps - same filt",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4519
https://github.com/broadinstitute/gatk/issues/4519:3520,Deployability,toggle,toggle,3520,"llectFragmentOverlaps - same filters as CollectFragmentCounts, but adding counts to all bins overlapping each fragment. Note that we need to implement a filter on maximum fragment length, otherwise we get some strange artifacts from (incorrectly mapped?) extremely long fragments; I arbitrarily chose a cutoff of 10000bp. This recovered events 1 and 2. Event 3 seemed to be the most difficult to recover. Plotting the copy ratios surrounding this event (which spans ~15 100bp bins) yields some insights:. CollectFragmentCounts:; ![image](https://user-images.githubusercontent.com/11076296/37244188-317a7f1e-2453-11e8-937d-f7239354316e.png). CollectReadCounts:; ![image](https://user-images.githubusercontent.com/11076296/37244228-ad24908c-2453-11e8-91dd-a978578e77f4.png). CollectFragmentOverlaps:; ![image](https://user-images.githubusercontent.com/11076296/37244230-b25b9cee-2453-11e8-8646-f9c95365b355.png). The increased statistical noise in the CollectFragmentCounts result (due to the lower overall count because of the pairing of reads) probably causes us to miss this event. Also, although CollectFragmentOverlaps initially looks pretty good, I think the bin-to-bin correlations that are evident here negatively affect segmentation. This is not an extremely rigorous evaluation, but it suggests that we should consider switching over to a CollectReadCounts-like strategy. To appease CGA (in case they still feel strongly, which they might not), we can make this a separate tool, or perhaps just have a single tool called CollectCounts that can toggle between the two (we just have to be careful about filters in the latter case). We should evaluate further once more rigorous automatic evaluations are in place. @mbabadi @LeeTL1220 @asmirnov239 @sooheelee might find this of interest. Also, I have a question for engine team, @droazen. Is there a read filter I should be using for fragment length, and if not, can we add one (or is there already a preferred way to do this type of filtering)?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4519
https://github.com/broadinstitute/gatk/issues/4519:83,Modifiability,rewrite,rewrite,83,"In the process of unifying CalculateTargetCoverage / SparkGenomeReadCounts for the rewrite of the CNV pipeline, we decided to experiment with switching over to fragment-based counts due to a request from CGA. For each fragment, CollectFragmentCounts adds a count to *the bin that overlaps with the fragment center*. We filter to properly-paired, first-of-pair reads in order to have well formed fragments and avoid double counting. We also filter out duplicates. In contrast, CalculateTargetCoverage added a count to *all bins that overlapped with a read* and SparkGenomeReadCounts added a count to *the bin that contained the read start*. These tools kept duplicates. However, none of these collection strategies have been rigorously evaluated. Using a small set of WGS SV tandem-duplication calls from @mwalker174 as a truth set, I did some experimenting with changing the count-collection strategy. (We initially thought we were missing some of these simply due to over-denoising/filtering by the PoN, but as we'll see below, the count-collection strategy plays a non-trivial role.). Subsetting to chr3, I built a small PoN of 12 normals (including the case normal) at 100bp and denoised using bin medians only (i.e., `--number-of-eigensamples 0`) to avoid denoising away common events. In chr3, the case sample had three events:. ````; chr3	8559423		8560126; chr3	64547471	64549936; chr3	90414457	90415989; ````. I tried the following, running `ModelSegments` using fairly sensitive parameters (`--number-of-changepoints-penalty-factor 0.1 --maximum-number-of-segments-per-chromosome 10000 --window-size 16 --window-size 32 --maximum-number-of-smoothing-iterations 0` in copy-ratio-only mode:. 1) CollectFragmentCounts. This only recovered event 2.; 2) CollectReadCounts - same as CollectFragmentCounts, but removing the properly-paired and first-of-pair filters and adding a count for each read to the bin containing its start. This recovered all 3 events.; 3) CollectFragmentOverlaps - same filt",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4519
https://github.com/broadinstitute/gatk/issues/4519:409,Safety,avoid,avoid,409,"In the process of unifying CalculateTargetCoverage / SparkGenomeReadCounts for the rewrite of the CNV pipeline, we decided to experiment with switching over to fragment-based counts due to a request from CGA. For each fragment, CollectFragmentCounts adds a count to *the bin that overlaps with the fragment center*. We filter to properly-paired, first-of-pair reads in order to have well formed fragments and avoid double counting. We also filter out duplicates. In contrast, CalculateTargetCoverage added a count to *all bins that overlapped with a read* and SparkGenomeReadCounts added a count to *the bin that contained the read start*. These tools kept duplicates. However, none of these collection strategies have been rigorously evaluated. Using a small set of WGS SV tandem-duplication calls from @mwalker174 as a truth set, I did some experimenting with changing the count-collection strategy. (We initially thought we were missing some of these simply due to over-denoising/filtering by the PoN, but as we'll see below, the count-collection strategy plays a non-trivial role.). Subsetting to chr3, I built a small PoN of 12 normals (including the case normal) at 100bp and denoised using bin medians only (i.e., `--number-of-eigensamples 0`) to avoid denoising away common events. In chr3, the case sample had three events:. ````; chr3	8559423		8560126; chr3	64547471	64549936; chr3	90414457	90415989; ````. I tried the following, running `ModelSegments` using fairly sensitive parameters (`--number-of-changepoints-penalty-factor 0.1 --maximum-number-of-segments-per-chromosome 10000 --window-size 16 --window-size 32 --maximum-number-of-smoothing-iterations 0` in copy-ratio-only mode:. 1) CollectFragmentCounts. This only recovered event 2.; 2) CollectReadCounts - same as CollectFragmentCounts, but removing the properly-paired and first-of-pair filters and adding a count for each read to the bin containing its start. This recovered all 3 events.; 3) CollectFragmentOverlaps - same filt",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4519
https://github.com/broadinstitute/gatk/issues/4519:1254,Safety,avoid,avoid,1254," a count to *the bin that overlaps with the fragment center*. We filter to properly-paired, first-of-pair reads in order to have well formed fragments and avoid double counting. We also filter out duplicates. In contrast, CalculateTargetCoverage added a count to *all bins that overlapped with a read* and SparkGenomeReadCounts added a count to *the bin that contained the read start*. These tools kept duplicates. However, none of these collection strategies have been rigorously evaluated. Using a small set of WGS SV tandem-duplication calls from @mwalker174 as a truth set, I did some experimenting with changing the count-collection strategy. (We initially thought we were missing some of these simply due to over-denoising/filtering by the PoN, but as we'll see below, the count-collection strategy plays a non-trivial role.). Subsetting to chr3, I built a small PoN of 12 normals (including the case normal) at 100bp and denoised using bin medians only (i.e., `--number-of-eigensamples 0`) to avoid denoising away common events. In chr3, the case sample had three events:. ````; chr3	8559423		8560126; chr3	64547471	64549936; chr3	90414457	90415989; ````. I tried the following, running `ModelSegments` using fairly sensitive parameters (`--number-of-changepoints-penalty-factor 0.1 --maximum-number-of-segments-per-chromosome 10000 --window-size 16 --window-size 32 --maximum-number-of-smoothing-iterations 0` in copy-ratio-only mode:. 1) CollectFragmentCounts. This only recovered event 2.; 2) CollectReadCounts - same as CollectFragmentCounts, but removing the properly-paired and first-of-pair filters and adding a count for each read to the bin containing its start. This recovered all 3 events.; 3) CollectFragmentOverlaps - same filters as CollectFragmentCounts, but adding counts to all bins overlapping each fragment. Note that we need to implement a filter on maximum fragment length, otherwise we get some strange artifacts from (incorrectly mapped?) extremely long fragments; I arb",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4519
https://github.com/broadinstitute/gatk/issues/4519:1734,Safety,recover,recovered,1734,"aluated. Using a small set of WGS SV tandem-duplication calls from @mwalker174 as a truth set, I did some experimenting with changing the count-collection strategy. (We initially thought we were missing some of these simply due to over-denoising/filtering by the PoN, but as we'll see below, the count-collection strategy plays a non-trivial role.). Subsetting to chr3, I built a small PoN of 12 normals (including the case normal) at 100bp and denoised using bin medians only (i.e., `--number-of-eigensamples 0`) to avoid denoising away common events. In chr3, the case sample had three events:. ````; chr3	8559423		8560126; chr3	64547471	64549936; chr3	90414457	90415989; ````. I tried the following, running `ModelSegments` using fairly sensitive parameters (`--number-of-changepoints-penalty-factor 0.1 --maximum-number-of-segments-per-chromosome 10000 --window-size 16 --window-size 32 --maximum-number-of-smoothing-iterations 0` in copy-ratio-only mode:. 1) CollectFragmentCounts. This only recovered event 2.; 2) CollectReadCounts - same as CollectFragmentCounts, but removing the properly-paired and first-of-pair filters and adding a count for each read to the bin containing its start. This recovered all 3 events.; 3) CollectFragmentOverlaps - same filters as CollectFragmentCounts, but adding counts to all bins overlapping each fragment. Note that we need to implement a filter on maximum fragment length, otherwise we get some strange artifacts from (incorrectly mapped?) extremely long fragments; I arbitrarily chose a cutoff of 10000bp. This recovered events 1 and 2. Event 3 seemed to be the most difficult to recover. Plotting the copy ratios surrounding this event (which spans ~15 100bp bins) yields some insights:. CollectFragmentCounts:; ![image](https://user-images.githubusercontent.com/11076296/37244188-317a7f1e-2453-11e8-937d-f7239354316e.png). CollectReadCounts:; ![image](https://user-images.githubusercontent.com/11076296/37244228-ad24908c-2453-11e8-91dd-a978578e77f4.png",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4519
https://github.com/broadinstitute/gatk/issues/4519:1938,Safety,recover,recovered,1938,"f these simply due to over-denoising/filtering by the PoN, but as we'll see below, the count-collection strategy plays a non-trivial role.). Subsetting to chr3, I built a small PoN of 12 normals (including the case normal) at 100bp and denoised using bin medians only (i.e., `--number-of-eigensamples 0`) to avoid denoising away common events. In chr3, the case sample had three events:. ````; chr3	8559423		8560126; chr3	64547471	64549936; chr3	90414457	90415989; ````. I tried the following, running `ModelSegments` using fairly sensitive parameters (`--number-of-changepoints-penalty-factor 0.1 --maximum-number-of-segments-per-chromosome 10000 --window-size 16 --window-size 32 --maximum-number-of-smoothing-iterations 0` in copy-ratio-only mode:. 1) CollectFragmentCounts. This only recovered event 2.; 2) CollectReadCounts - same as CollectFragmentCounts, but removing the properly-paired and first-of-pair filters and adding a count for each read to the bin containing its start. This recovered all 3 events.; 3) CollectFragmentOverlaps - same filters as CollectFragmentCounts, but adding counts to all bins overlapping each fragment. Note that we need to implement a filter on maximum fragment length, otherwise we get some strange artifacts from (incorrectly mapped?) extremely long fragments; I arbitrarily chose a cutoff of 10000bp. This recovered events 1 and 2. Event 3 seemed to be the most difficult to recover. Plotting the copy ratios surrounding this event (which spans ~15 100bp bins) yields some insights:. CollectFragmentCounts:; ![image](https://user-images.githubusercontent.com/11076296/37244188-317a7f1e-2453-11e8-937d-f7239354316e.png). CollectReadCounts:; ![image](https://user-images.githubusercontent.com/11076296/37244228-ad24908c-2453-11e8-91dd-a978578e77f4.png). CollectFragmentOverlaps:; ![image](https://user-images.githubusercontent.com/11076296/37244230-b25b9cee-2453-11e8-8646-f9c95365b355.png). The increased statistical noise in the CollectFragmentCounts result ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4519
https://github.com/broadinstitute/gatk/issues/4519:2295,Safety,recover,recovered,2295,"ase sample had three events:. ````; chr3	8559423		8560126; chr3	64547471	64549936; chr3	90414457	90415989; ````. I tried the following, running `ModelSegments` using fairly sensitive parameters (`--number-of-changepoints-penalty-factor 0.1 --maximum-number-of-segments-per-chromosome 10000 --window-size 16 --window-size 32 --maximum-number-of-smoothing-iterations 0` in copy-ratio-only mode:. 1) CollectFragmentCounts. This only recovered event 2.; 2) CollectReadCounts - same as CollectFragmentCounts, but removing the properly-paired and first-of-pair filters and adding a count for each read to the bin containing its start. This recovered all 3 events.; 3) CollectFragmentOverlaps - same filters as CollectFragmentCounts, but adding counts to all bins overlapping each fragment. Note that we need to implement a filter on maximum fragment length, otherwise we get some strange artifacts from (incorrectly mapped?) extremely long fragments; I arbitrarily chose a cutoff of 10000bp. This recovered events 1 and 2. Event 3 seemed to be the most difficult to recover. Plotting the copy ratios surrounding this event (which spans ~15 100bp bins) yields some insights:. CollectFragmentCounts:; ![image](https://user-images.githubusercontent.com/11076296/37244188-317a7f1e-2453-11e8-937d-f7239354316e.png). CollectReadCounts:; ![image](https://user-images.githubusercontent.com/11076296/37244228-ad24908c-2453-11e8-91dd-a978578e77f4.png). CollectFragmentOverlaps:; ![image](https://user-images.githubusercontent.com/11076296/37244230-b25b9cee-2453-11e8-8646-f9c95365b355.png). The increased statistical noise in the CollectFragmentCounts result (due to the lower overall count because of the pairing of reads) probably causes us to miss this event. Also, although CollectFragmentOverlaps initially looks pretty good, I think the bin-to-bin correlations that are evident here negatively affect segmentation. This is not an extremely rigorous evaluation, but it suggests that we should consider switching ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4519
https://github.com/broadinstitute/gatk/issues/4519:2364,Safety,recover,recover,2364,"559423		8560126; chr3	64547471	64549936; chr3	90414457	90415989; ````. I tried the following, running `ModelSegments` using fairly sensitive parameters (`--number-of-changepoints-penalty-factor 0.1 --maximum-number-of-segments-per-chromosome 10000 --window-size 16 --window-size 32 --maximum-number-of-smoothing-iterations 0` in copy-ratio-only mode:. 1) CollectFragmentCounts. This only recovered event 2.; 2) CollectReadCounts - same as CollectFragmentCounts, but removing the properly-paired and first-of-pair filters and adding a count for each read to the bin containing its start. This recovered all 3 events.; 3) CollectFragmentOverlaps - same filters as CollectFragmentCounts, but adding counts to all bins overlapping each fragment. Note that we need to implement a filter on maximum fragment length, otherwise we get some strange artifacts from (incorrectly mapped?) extremely long fragments; I arbitrarily chose a cutoff of 10000bp. This recovered events 1 and 2. Event 3 seemed to be the most difficult to recover. Plotting the copy ratios surrounding this event (which spans ~15 100bp bins) yields some insights:. CollectFragmentCounts:; ![image](https://user-images.githubusercontent.com/11076296/37244188-317a7f1e-2453-11e8-937d-f7239354316e.png). CollectReadCounts:; ![image](https://user-images.githubusercontent.com/11076296/37244228-ad24908c-2453-11e8-91dd-a978578e77f4.png). CollectFragmentOverlaps:; ![image](https://user-images.githubusercontent.com/11076296/37244230-b25b9cee-2453-11e8-8646-f9c95365b355.png). The increased statistical noise in the CollectFragmentCounts result (due to the lower overall count because of the pairing of reads) probably causes us to miss this event. Also, although CollectFragmentOverlaps initially looks pretty good, I think the bin-to-bin correlations that are evident here negatively affect segmentation. This is not an extremely rigorous evaluation, but it suggests that we should consider switching over to a CollectReadCounts-like strategy",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4519
https://github.com/broadinstitute/gatk/issues/4519:954,Usability,simpl,simply,954,"In the process of unifying CalculateTargetCoverage / SparkGenomeReadCounts for the rewrite of the CNV pipeline, we decided to experiment with switching over to fragment-based counts due to a request from CGA. For each fragment, CollectFragmentCounts adds a count to *the bin that overlaps with the fragment center*. We filter to properly-paired, first-of-pair reads in order to have well formed fragments and avoid double counting. We also filter out duplicates. In contrast, CalculateTargetCoverage added a count to *all bins that overlapped with a read* and SparkGenomeReadCounts added a count to *the bin that contained the read start*. These tools kept duplicates. However, none of these collection strategies have been rigorously evaluated. Using a small set of WGS SV tandem-duplication calls from @mwalker174 as a truth set, I did some experimenting with changing the count-collection strategy. (We initially thought we were missing some of these simply due to over-denoising/filtering by the PoN, but as we'll see below, the count-collection strategy plays a non-trivial role.). Subsetting to chr3, I built a small PoN of 12 normals (including the case normal) at 100bp and denoised using bin medians only (i.e., `--number-of-eigensamples 0`) to avoid denoising away common events. In chr3, the case sample had three events:. ````; chr3	8559423		8560126; chr3	64547471	64549936; chr3	90414457	90415989; ````. I tried the following, running `ModelSegments` using fairly sensitive parameters (`--number-of-changepoints-penalty-factor 0.1 --maximum-number-of-segments-per-chromosome 10000 --window-size 16 --window-size 32 --maximum-number-of-smoothing-iterations 0` in copy-ratio-only mode:. 1) CollectFragmentCounts. This only recovered event 2.; 2) CollectReadCounts - same as CollectFragmentCounts, but removing the properly-paired and first-of-pair filters and adding a count for each read to the bin containing its start. This recovered all 3 events.; 3) CollectFragmentOverlaps - same filt",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4519
https://github.com/broadinstitute/gatk/issues/4521:645,Availability,Down,Downloading,645,"From ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/funcotator/ the file `gencode/hg38/gencode.config` makes reference to `gencode.v27.pc_transcripts.fasta` . ```; # Required field for GENCODE files.; # Path to the FASTA file from which to load the sequences for GENCODE transcripts:; gencode_fasta_path = gencode.v27.pc_transcripts.fasta; ```. which is not present in the directory. . ```; $ ls; gencode.config; gencode.v27.chr_patch_hapl_scaff.annotation.REORDERED.gtf; gencode.v27.chr_patch_hapl_scaff.annotation.REORDERED.gtf.idx; gencode.v27.transcripts.dict; gencode.v27.transcripts.fasta; gencode.v27.transcripts.fasta.fai; ```. Downloading the pc_transcript.fasta from gencode does not fix the errors thrown when trying to use the reference. The error message is a fairly uninformative null pointer exception. . ```; 00:27:35.745 INFO Funcotator - Done initializing engine; 00:27:35.758 INFO Funcotator - Shutting down engine; [March 12, 2018 12:27:35 AM UTC] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=2258108416; java.lang.NullPointerException; 	at org.broadinstitute.hellbender.tools.funcotator.Funcotator.closeTool(Funcotator.java:330); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:897); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:135); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:180); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:199); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:159); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:202); 	at org.broadinstitute.hellbender.Main.main(Main.java:288); ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4521
https://github.com/broadinstitute/gatk/issues/4521:711,Availability,error,errors,711,"From ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/funcotator/ the file `gencode/hg38/gencode.config` makes reference to `gencode.v27.pc_transcripts.fasta` . ```; # Required field for GENCODE files.; # Path to the FASTA file from which to load the sequences for GENCODE transcripts:; gencode_fasta_path = gencode.v27.pc_transcripts.fasta; ```. which is not present in the directory. . ```; $ ls; gencode.config; gencode.v27.chr_patch_hapl_scaff.annotation.REORDERED.gtf; gencode.v27.chr_patch_hapl_scaff.annotation.REORDERED.gtf.idx; gencode.v27.transcripts.dict; gencode.v27.transcripts.fasta; gencode.v27.transcripts.fasta.fai; ```. Downloading the pc_transcript.fasta from gencode does not fix the errors thrown when trying to use the reference. The error message is a fairly uninformative null pointer exception. . ```; 00:27:35.745 INFO Funcotator - Done initializing engine; 00:27:35.758 INFO Funcotator - Shutting down engine; [March 12, 2018 12:27:35 AM UTC] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=2258108416; java.lang.NullPointerException; 	at org.broadinstitute.hellbender.tools.funcotator.Funcotator.closeTool(Funcotator.java:330); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:897); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:135); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:180); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:199); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:159); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:202); 	at org.broadinstitute.hellbender.Main.main(Main.java:288); ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4521
https://github.com/broadinstitute/gatk/issues/4521:763,Availability,error,error,763,"From ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/funcotator/ the file `gencode/hg38/gencode.config` makes reference to `gencode.v27.pc_transcripts.fasta` . ```; # Required field for GENCODE files.; # Path to the FASTA file from which to load the sequences for GENCODE transcripts:; gencode_fasta_path = gencode.v27.pc_transcripts.fasta; ```. which is not present in the directory. . ```; $ ls; gencode.config; gencode.v27.chr_patch_hapl_scaff.annotation.REORDERED.gtf; gencode.v27.chr_patch_hapl_scaff.annotation.REORDERED.gtf.idx; gencode.v27.transcripts.dict; gencode.v27.transcripts.fasta; gencode.v27.transcripts.fasta.fai; ```. Downloading the pc_transcript.fasta from gencode does not fix the errors thrown when trying to use the reference. The error message is a fairly uninformative null pointer exception. . ```; 00:27:35.745 INFO Funcotator - Done initializing engine; 00:27:35.758 INFO Funcotator - Shutting down engine; [March 12, 2018 12:27:35 AM UTC] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=2258108416; java.lang.NullPointerException; 	at org.broadinstitute.hellbender.tools.funcotator.Funcotator.closeTool(Funcotator.java:330); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:897); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:135); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:180); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:199); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:159); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:202); 	at org.broadinstitute.hellbender.Main.main(Main.java:288); ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4521
https://github.com/broadinstitute/gatk/issues/4521:931,Availability,down,down,931,"From ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/funcotator/ the file `gencode/hg38/gencode.config` makes reference to `gencode.v27.pc_transcripts.fasta` . ```; # Required field for GENCODE files.; # Path to the FASTA file from which to load the sequences for GENCODE transcripts:; gencode_fasta_path = gencode.v27.pc_transcripts.fasta; ```. which is not present in the directory. . ```; $ ls; gencode.config; gencode.v27.chr_patch_hapl_scaff.annotation.REORDERED.gtf; gencode.v27.chr_patch_hapl_scaff.annotation.REORDERED.gtf.idx; gencode.v27.transcripts.dict; gencode.v27.transcripts.fasta; gencode.v27.transcripts.fasta.fai; ```. Downloading the pc_transcript.fasta from gencode does not fix the errors thrown when trying to use the reference. The error message is a fairly uninformative null pointer exception. . ```; 00:27:35.745 INFO Funcotator - Done initializing engine; 00:27:35.758 INFO Funcotator - Shutting down engine; [March 12, 2018 12:27:35 AM UTC] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=2258108416; java.lang.NullPointerException; 	at org.broadinstitute.hellbender.tools.funcotator.Funcotator.closeTool(Funcotator.java:330); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:897); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:135); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:180); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:199); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:159); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:202); 	at org.broadinstitute.hellbender.Main.main(Main.java:288); ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4521
https://github.com/broadinstitute/gatk/issues/4521:769,Integrability,message,message,769,"From ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/funcotator/ the file `gencode/hg38/gencode.config` makes reference to `gencode.v27.pc_transcripts.fasta` . ```; # Required field for GENCODE files.; # Path to the FASTA file from which to load the sequences for GENCODE transcripts:; gencode_fasta_path = gencode.v27.pc_transcripts.fasta; ```. which is not present in the directory. . ```; $ ls; gencode.config; gencode.v27.chr_patch_hapl_scaff.annotation.REORDERED.gtf; gencode.v27.chr_patch_hapl_scaff.annotation.REORDERED.gtf.idx; gencode.v27.transcripts.dict; gencode.v27.transcripts.fasta; gencode.v27.transcripts.fasta.fai; ```. Downloading the pc_transcript.fasta from gencode does not fix the errors thrown when trying to use the reference. The error message is a fairly uninformative null pointer exception. . ```; 00:27:35.745 INFO Funcotator - Done initializing engine; 00:27:35.758 INFO Funcotator - Shutting down engine; [March 12, 2018 12:27:35 AM UTC] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=2258108416; java.lang.NullPointerException; 	at org.broadinstitute.hellbender.tools.funcotator.Funcotator.closeTool(Funcotator.java:330); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:897); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:135); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:180); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:199); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:159); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:202); 	at org.broadinstitute.hellbender.Main.main(Main.java:288); ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4521
https://github.com/broadinstitute/gatk/issues/4521:104,Modifiability,config,config,104,"From ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/funcotator/ the file `gencode/hg38/gencode.config` makes reference to `gencode.v27.pc_transcripts.fasta` . ```; # Required field for GENCODE files.; # Path to the FASTA file from which to load the sequences for GENCODE transcripts:; gencode_fasta_path = gencode.v27.pc_transcripts.fasta; ```. which is not present in the directory. . ```; $ ls; gencode.config; gencode.v27.chr_patch_hapl_scaff.annotation.REORDERED.gtf; gencode.v27.chr_patch_hapl_scaff.annotation.REORDERED.gtf.idx; gencode.v27.transcripts.dict; gencode.v27.transcripts.fasta; gencode.v27.transcripts.fasta.fai; ```. Downloading the pc_transcript.fasta from gencode does not fix the errors thrown when trying to use the reference. The error message is a fairly uninformative null pointer exception. . ```; 00:27:35.745 INFO Funcotator - Done initializing engine; 00:27:35.758 INFO Funcotator - Shutting down engine; [March 12, 2018 12:27:35 AM UTC] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=2258108416; java.lang.NullPointerException; 	at org.broadinstitute.hellbender.tools.funcotator.Funcotator.closeTool(Funcotator.java:330); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:897); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:135); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:180); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:199); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:159); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:202); 	at org.broadinstitute.hellbender.Main.main(Main.java:288); ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4521
https://github.com/broadinstitute/gatk/issues/4521:414,Modifiability,config,config,414,"From ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/funcotator/ the file `gencode/hg38/gencode.config` makes reference to `gencode.v27.pc_transcripts.fasta` . ```; # Required field for GENCODE files.; # Path to the FASTA file from which to load the sequences for GENCODE transcripts:; gencode_fasta_path = gencode.v27.pc_transcripts.fasta; ```. which is not present in the directory. . ```; $ ls; gencode.config; gencode.v27.chr_patch_hapl_scaff.annotation.REORDERED.gtf; gencode.v27.chr_patch_hapl_scaff.annotation.REORDERED.gtf.idx; gencode.v27.transcripts.dict; gencode.v27.transcripts.fasta; gencode.v27.transcripts.fasta.fai; ```. Downloading the pc_transcript.fasta from gencode does not fix the errors thrown when trying to use the reference. The error message is a fairly uninformative null pointer exception. . ```; 00:27:35.745 INFO Funcotator - Done initializing engine; 00:27:35.758 INFO Funcotator - Shutting down engine; [March 12, 2018 12:27:35 AM UTC] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=2258108416; java.lang.NullPointerException; 	at org.broadinstitute.hellbender.tools.funcotator.Funcotator.closeTool(Funcotator.java:330); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:897); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:135); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:180); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:199); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:159); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:202); 	at org.broadinstitute.hellbender.Main.main(Main.java:288); ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4521
https://github.com/broadinstitute/gatk/issues/4521:249,Performance,load,load,249,"From ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/funcotator/ the file `gencode/hg38/gencode.config` makes reference to `gencode.v27.pc_transcripts.fasta` . ```; # Required field for GENCODE files.; # Path to the FASTA file from which to load the sequences for GENCODE transcripts:; gencode_fasta_path = gencode.v27.pc_transcripts.fasta; ```. which is not present in the directory. . ```; $ ls; gencode.config; gencode.v27.chr_patch_hapl_scaff.annotation.REORDERED.gtf; gencode.v27.chr_patch_hapl_scaff.annotation.REORDERED.gtf.idx; gencode.v27.transcripts.dict; gencode.v27.transcripts.fasta; gencode.v27.transcripts.fasta.fai; ```. Downloading the pc_transcript.fasta from gencode does not fix the errors thrown when trying to use the reference. The error message is a fairly uninformative null pointer exception. . ```; 00:27:35.745 INFO Funcotator - Done initializing engine; 00:27:35.758 INFO Funcotator - Shutting down engine; [March 12, 2018 12:27:35 AM UTC] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=2258108416; java.lang.NullPointerException; 	at org.broadinstitute.hellbender.tools.funcotator.Funcotator.closeTool(Funcotator.java:330); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:897); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:135); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:180); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:199); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:159); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:202); 	at org.broadinstitute.hellbender.Main.main(Main.java:288); ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4521
https://github.com/broadinstitute/gatk/pull/4523:28,Deployability,upgrade,upgrade,28,Will fail until the Barclay upgrade.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4523
https://github.com/broadinstitute/gatk/pull/4524:23,Availability,error,error,23,User pointed out minor error.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4524
https://github.com/broadinstitute/gatk/issues/4525:300,Security,validat,validate,300,"## Bug Report. ### Affected tool(s); CombineGVCFs. ### Affected version(s); 4.0.2.0. ### Description ; CombineGVCFs fails with an IllegalArgumentException when trying to combine GVCFs. However, when trying to combine a different subset of the GVCFs, it fails with a ClassCastException. The GVCFs all validate with ValidateVariants and were produced with GATK4 HaplotypeCaller. #### Steps to reproduce; Details in https://github.com/broadinstitute/dsde-docs/issues/2990. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/11503/combinegvcfs-java-lang-illegalargumentexception-unexpected-base-in-allele-bases-aacc/p1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4525
https://github.com/broadinstitute/gatk/issues/4525:314,Security,Validat,ValidateVariants,314,"## Bug Report. ### Affected tool(s); CombineGVCFs. ### Affected version(s); 4.0.2.0. ### Description ; CombineGVCFs fails with an IllegalArgumentException when trying to combine GVCFs. However, when trying to combine a different subset of the GVCFs, it fails with a ClassCastException. The GVCFs all validate with ValidateVariants and were produced with GATK4 HaplotypeCaller. #### Steps to reproduce; Details in https://github.com/broadinstitute/dsde-docs/issues/2990. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/11503/combinegvcfs-java-lang-illegalargumentexception-unexpected-base-in-allele-bases-aacc/p1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4525
https://github.com/broadinstitute/gatk/issues/4526:165,Availability,error,error,165,"The following exception was thrown, which looks a lot like a user exception:. ```; htsjdk.tribble.TribbleException$MalformedFeatureFile: Unable to parse header with error: /storageNGS/ngs3/projects/other1/KinderKlinik/Exomes/04_GRCh8_GATK4_cohort/GATK4_testrun/cromwell-executions/HaplotypeCallerGvcf_GATK4/3e5a2ae0-5529-4344-b187-9ceac771e1ed/call-MergeGVCFs/execution/pa.hg38.g.vcf.gz, for input source: file:///storageNGS/ngs3/projects/other1/KinderKlinik/Exomes/04_GRCh8_GATK4_cohort/GATK4_testrun/cromwell-executions/HaplotypeCallerGvcf_GATK4/3e5a2ae0-5529-4344-b187-9ceac771e1ed/call-MergeGVCFs/execution/pa.hg38.g.vcf.gz; at htsjdk.tribble.TribbleIndexedFeatureReader.readHeader(TribbleIndexedFeatureReader.java:254); at htsjdk.tribble.TribbleIndexedFeatureReader.<init>(TribbleIndexedFeatureReader.java:101); at htsjdk.tribble.TribbleIndexedFeatureReader.<init>(TribbleIndexedFeatureReader.java:126); at htsjdk.tribble.AbstractFeatureReader.getFeatureReader(AbstractFeatureReader.java:110); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getReaderFromPath(GenomicsDBImport.java:620); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getHeaderFromPath(GenomicsDBImport.java:355); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.initializeHeaderAndSampleMappings(GenomicsDBImport.java:341); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.onStartup(GenomicsDBImport.java:296); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:152); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:195); at org.broadinstitute.hellbender.Main.main(Main.java:275) Caused by: java.nio.file.No",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4526
https://github.com/broadinstitute/gatk/issues/4526:3431,Availability,Error,Error,3431,"dLineProgram.runTool(CommandLineProgram.java:134); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:152); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:195); at org.broadinstitute.hellbender.Main.main(Main.java:275) Caused by: java.nio.file.NoSuchFileException: /storageNGS/ngs3/projects/other1/KinderKlinik/Exomes/04_GRCh8_GATK4_cohort/GATK4_testrun/cromwell-executions/HaplotypeCallerGvcf_GATK4/3e5a2ae0-5529-4344-b187-9ceac771e1ed/call-MergeGVCFs/execution/pa.hg38.g.vcf.gz; at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86); at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102); at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107); at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214); at java.nio.file.Files.newByteChannel(Files.java:361); at java.nio.file.Files.newByteChannel(Files.java:407); at htsjdk.samtools.seekablestream.SeekablePathStream.<init>(SeekablePathStream.java:41); at htsjdk.tribble.util.ParsingUtils.openInputStream(ParsingUtils.java:110); at htsjdk.tribble.TribbleIndexedFeatureReader.readHeader(TribbleIndexedFeatureReader.java:244) ... 13; ```. It looks like we intended to catch and convert to user exception in:; `at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getHeaderFromPath(GenomicsDBImport.java:355)` but we weren't expecting a `TribbleException`. ```; private VCFHeader getHeaderFromPath(final Path variantPath) {; try(final AbstractFeatureReader<VariantContext, LineIterator> reader = getReaderFromPath(variantPath)) {; return (VCFHeader) reader.getHeader();; } catch (final IOException e) {; throw new UserException(""Error while reading vcf header from "" + variantPath.toUri(), e);; }; }; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4526
https://github.com/broadinstitute/gatk/pull/4528:99,Availability,down,down,99,"This enables PoNs to be built with a number of intervals greater than 16777216, which lets us push down to bin sizes <175bp if really necessary. At some point, it is better to scatter and build a separate PoN for each contig so we don't run into memory issues. However, the official WDL is not set up to do the scatter, so one would have to write a custom WDL. @mwalker174 Do you mind taking a look?. Closes #4365.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4528
https://github.com/broadinstitute/gatk/pull/4530:579,Performance,load,load,579,"This adds a new folder for large runtime resources to git-lfs. Unlike the large test resources, which are only accessed via a volume mount when tests are run on the Docker image, the runtime resources need to be accessible to the GATK build during the Docker build process, since they're included in the jar. AFAICT there is no `docker build` equivalent to `docker run -v`. So for now the runtime resources are git pulled into the Docker staging area, and thus onto the Docker image. We need this for @lucidtronix 's CNN branch (and possibly for @TedBrookings) if we're going to load models for resources, but longer term, we need a better solution.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4530
https://github.com/broadinstitute/gatk/pull/4530:111,Security,access,accessed,111,"This adds a new folder for large runtime resources to git-lfs. Unlike the large test resources, which are only accessed via a volume mount when tests are run on the Docker image, the runtime resources need to be accessible to the GATK build during the Docker build process, since they're included in the jar. AFAICT there is no `docker build` equivalent to `docker run -v`. So for now the runtime resources are git pulled into the Docker staging area, and thus onto the Docker image. We need this for @lucidtronix 's CNN branch (and possibly for @TedBrookings) if we're going to load models for resources, but longer term, we need a better solution.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4530
https://github.com/broadinstitute/gatk/pull/4530:212,Security,access,accessible,212,"This adds a new folder for large runtime resources to git-lfs. Unlike the large test resources, which are only accessed via a volume mount when tests are run on the Docker image, the runtime resources need to be accessible to the GATK build during the Docker build process, since they're included in the jar. AFAICT there is no `docker build` equivalent to `docker run -v`. So for now the runtime resources are git pulled into the Docker staging area, and thus onto the Docker image. We need this for @lucidtronix 's CNN branch (and possibly for @TedBrookings) if we're going to load models for resources, but longer term, we need a better solution.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4530
https://github.com/broadinstitute/gatk/pull/4530:80,Testability,test,test,80,"This adds a new folder for large runtime resources to git-lfs. Unlike the large test resources, which are only accessed via a volume mount when tests are run on the Docker image, the runtime resources need to be accessible to the GATK build during the Docker build process, since they're included in the jar. AFAICT there is no `docker build` equivalent to `docker run -v`. So for now the runtime resources are git pulled into the Docker staging area, and thus onto the Docker image. We need this for @lucidtronix 's CNN branch (and possibly for @TedBrookings) if we're going to load models for resources, but longer term, we need a better solution.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4530
https://github.com/broadinstitute/gatk/pull/4530:144,Testability,test,tests,144,"This adds a new folder for large runtime resources to git-lfs. Unlike the large test resources, which are only accessed via a volume mount when tests are run on the Docker image, the runtime resources need to be accessible to the GATK build during the Docker build process, since they're included in the jar. AFAICT there is no `docker build` equivalent to `docker run -v`. So for now the runtime resources are git pulled into the Docker staging area, and thus onto the Docker image. We need this for @lucidtronix 's CNN branch (and possibly for @TedBrookings) if we're going to load models for resources, but longer term, we need a better solution.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4530
https://github.com/broadinstitute/gatk/issues/4531:123,Deployability,pipeline,pipeline,123,"When running StructuralVariationDiscoveryPipelineSpark, we found the local storage on our Hadoop Datanodes filling up. The pipeline was generating 2 GB of output and filling up the /var storage on the datanodes. The job would then fail. Is there a way to turnoff this extensive logging info for this pipeline and make less extensive logging the default? . Here is an example of one of the files.....; container_e2417_1520821377253_0060_01_000052/stdout. writing; 10000139 2/2 76b aligned read.; writing; 1000014 1/2 76b aligned read.; writing; 1000014 2/2 76b aligned read.; writing; 10000140 2/2 76b aligned read.; writing; 10000140 1/2 76b aligned read.; writing; 10000141 1/2 76b aligned read.; writing; 10000141 2/2 76b aligned read.; writing; 10000142 1/2 76b aligned read.; writing; 10000142 2/2 76b aligned read.; writing; 10000143 1/2 76b aligned read.; writing; 10000143 2/2 76b aligned read.; writing; 10000144 2/2 76b aligned read.; writing; 10000144 1/2 76b aligned read.; writing; 10000145 2/2 76b aligned read.; writing; 10000145 1/2 76b aligned read.; writing; 10000146 2/2 76b aligned read.; writing; 10000146 1/2 76b aligned read.; writing; 10000147 1/2 76b aligned read.; writing; 10000147 2/2 76b aligned read.; writing; 10000148 2/2 76b aligned read.; writing; 10000148 1/2 76b aligned read.; writing; 10000149 2/2 76b aligned read.; writing; 10000149 1/2 76b aligned read.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4531
https://github.com/broadinstitute/gatk/issues/4531:300,Deployability,pipeline,pipeline,300,"When running StructuralVariationDiscoveryPipelineSpark, we found the local storage on our Hadoop Datanodes filling up. The pipeline was generating 2 GB of output and filling up the /var storage on the datanodes. The job would then fail. Is there a way to turnoff this extensive logging info for this pipeline and make less extensive logging the default? . Here is an example of one of the files.....; container_e2417_1520821377253_0060_01_000052/stdout. writing; 10000139 2/2 76b aligned read.; writing; 1000014 1/2 76b aligned read.; writing; 1000014 2/2 76b aligned read.; writing; 10000140 2/2 76b aligned read.; writing; 10000140 1/2 76b aligned read.; writing; 10000141 1/2 76b aligned read.; writing; 10000141 2/2 76b aligned read.; writing; 10000142 1/2 76b aligned read.; writing; 10000142 2/2 76b aligned read.; writing; 10000143 1/2 76b aligned read.; writing; 10000143 2/2 76b aligned read.; writing; 10000144 2/2 76b aligned read.; writing; 10000144 1/2 76b aligned read.; writing; 10000145 2/2 76b aligned read.; writing; 10000145 1/2 76b aligned read.; writing; 10000146 2/2 76b aligned read.; writing; 10000146 1/2 76b aligned read.; writing; 10000147 1/2 76b aligned read.; writing; 10000147 2/2 76b aligned read.; writing; 10000148 2/2 76b aligned read.; writing; 10000148 1/2 76b aligned read.; writing; 10000149 2/2 76b aligned read.; writing; 10000149 1/2 76b aligned read.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4531
https://github.com/broadinstitute/gatk/issues/4531:278,Testability,log,logging,278,"When running StructuralVariationDiscoveryPipelineSpark, we found the local storage on our Hadoop Datanodes filling up. The pipeline was generating 2 GB of output and filling up the /var storage on the datanodes. The job would then fail. Is there a way to turnoff this extensive logging info for this pipeline and make less extensive logging the default? . Here is an example of one of the files.....; container_e2417_1520821377253_0060_01_000052/stdout. writing; 10000139 2/2 76b aligned read.; writing; 1000014 1/2 76b aligned read.; writing; 1000014 2/2 76b aligned read.; writing; 10000140 2/2 76b aligned read.; writing; 10000140 1/2 76b aligned read.; writing; 10000141 1/2 76b aligned read.; writing; 10000141 2/2 76b aligned read.; writing; 10000142 1/2 76b aligned read.; writing; 10000142 2/2 76b aligned read.; writing; 10000143 1/2 76b aligned read.; writing; 10000143 2/2 76b aligned read.; writing; 10000144 2/2 76b aligned read.; writing; 10000144 1/2 76b aligned read.; writing; 10000145 2/2 76b aligned read.; writing; 10000145 1/2 76b aligned read.; writing; 10000146 2/2 76b aligned read.; writing; 10000146 1/2 76b aligned read.; writing; 10000147 1/2 76b aligned read.; writing; 10000147 2/2 76b aligned read.; writing; 10000148 2/2 76b aligned read.; writing; 10000148 1/2 76b aligned read.; writing; 10000149 2/2 76b aligned read.; writing; 10000149 1/2 76b aligned read.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4531
https://github.com/broadinstitute/gatk/issues/4531:333,Testability,log,logging,333,"When running StructuralVariationDiscoveryPipelineSpark, we found the local storage on our Hadoop Datanodes filling up. The pipeline was generating 2 GB of output and filling up the /var storage on the datanodes. The job would then fail. Is there a way to turnoff this extensive logging info for this pipeline and make less extensive logging the default? . Here is an example of one of the files.....; container_e2417_1520821377253_0060_01_000052/stdout. writing; 10000139 2/2 76b aligned read.; writing; 1000014 1/2 76b aligned read.; writing; 1000014 2/2 76b aligned read.; writing; 10000140 2/2 76b aligned read.; writing; 10000140 1/2 76b aligned read.; writing; 10000141 1/2 76b aligned read.; writing; 10000141 2/2 76b aligned read.; writing; 10000142 1/2 76b aligned read.; writing; 10000142 2/2 76b aligned read.; writing; 10000143 1/2 76b aligned read.; writing; 10000143 2/2 76b aligned read.; writing; 10000144 2/2 76b aligned read.; writing; 10000144 1/2 76b aligned read.; writing; 10000145 2/2 76b aligned read.; writing; 10000145 1/2 76b aligned read.; writing; 10000146 2/2 76b aligned read.; writing; 10000146 1/2 76b aligned read.; writing; 10000147 1/2 76b aligned read.; writing; 10000147 2/2 76b aligned read.; writing; 10000148 2/2 76b aligned read.; writing; 10000148 1/2 76b aligned read.; writing; 10000149 2/2 76b aligned read.; writing; 10000149 1/2 76b aligned read.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4531
https://github.com/broadinstitute/gatk/issues/4532:345,Availability,error,error,345,build.gradle finds the tool provider with the following line:. ```; final javadocJDKFiles = files(((URLClassLoader) ToolProvider.getSystemToolClassLoader()).getURLs()); ```; ToolPrivider.getSystemToolClassLoader() returns null on jre and certain other java installations. This causes a confusing null pointer exceptions. We should have a better error message when this happens.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4532
https://github.com/broadinstitute/gatk/issues/4532:257,Deployability,install,installations,257,build.gradle finds the tool provider with the following line:. ```; final javadocJDKFiles = files(((URLClassLoader) ToolProvider.getSystemToolClassLoader()).getURLs()); ```; ToolPrivider.getSystemToolClassLoader() returns null on jre and certain other java installations. This causes a confusing null pointer exceptions. We should have a better error message when this happens.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4532
https://github.com/broadinstitute/gatk/issues/4532:351,Integrability,message,message,351,build.gradle finds the tool provider with the following line:. ```; final javadocJDKFiles = files(((URLClassLoader) ToolProvider.getSystemToolClassLoader()).getURLs()); ```; ToolPrivider.getSystemToolClassLoader() returns null on jre and certain other java installations. This causes a confusing null pointer exceptions. We should have a better error message when this happens.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4532
https://github.com/broadinstitute/gatk/issues/4533:169,Integrability,depend,dependency,169,CNNVariantWriteTensors currently has a thin Java front end that is an argument pass-through to Python. This should be rewritten with I/O done in Java and minimal Python dependency.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4533
https://github.com/broadinstitute/gatk/issues/4534:179,Integrability,depend,dependency,179,"CNNVariantTrain currently has a thin Java front end that is an argument pass-through to Python. If possible, this should be rewritten with I/O done in Java and the minimal Python dependency.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4534
https://github.com/broadinstitute/gatk/issues/4536:25,Deployability,integrat,integration,25,"CNNVariantScore has a 2d integration test but the vcf and bam inputs have no overlapping genomic territory, so no reads data is ever transmitted to the python code",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4536
https://github.com/broadinstitute/gatk/issues/4536:25,Integrability,integrat,integration,25,"CNNVariantScore has a 2d integration test but the vcf and bam inputs have no overlapping genomic territory, so no reads data is ever transmitted to the python code",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4536
https://github.com/broadinstitute/gatk/issues/4536:37,Testability,test,test,37,"CNNVariantScore has a 2d integration test but the vcf and bam inputs have no overlapping genomic territory, so no reads data is ever transmitted to the python code",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4536
https://github.com/broadinstitute/gatk/issues/4537:152,Security,validat,validation,152,"The CNNPipelineIntegration tests for CNNVariantWriteTensors, CNNVariantTrain and FilterVariantTranches executes tool code, but does no expected results validation.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4537
https://github.com/broadinstitute/gatk/issues/4537:27,Testability,test,tests,27,"The CNNPipelineIntegration tests for CNNVariantWriteTensors, CNNVariantTrain and FilterVariantTranches executes tool code, but does no expected results validation.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4537
https://github.com/broadinstitute/gatk/issues/4539:176,Integrability,depend,dependency,176,"The CNN tools currently use the FilterVariantTranches class to locate class-relative python resources, but FilterVariantTranches is the one CNN tool that doesn't need a Python dependency. If the individual (CNNVariantWriteTensors and CNNVariantTrain) tools still load python files as resources after https://github.com/broadinstitute/gatk/issues/4533 and https://github.com/broadinstitute/gatk/issues/4534 are done, the resources should be moved to live with the respective code for the respective tools.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4539
https://github.com/broadinstitute/gatk/issues/4539:263,Performance,load,load,263,"The CNN tools currently use the FilterVariantTranches class to locate class-relative python resources, but FilterVariantTranches is the one CNN tool that doesn't need a Python dependency. If the individual (CNNVariantWriteTensors and CNNVariantTrain) tools still load python files as resources after https://github.com/broadinstitute/gatk/issues/4533 and https://github.com/broadinstitute/gatk/issues/4534 are done, the resources should be moved to live with the respective code for the respective tools.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4539
https://github.com/broadinstitute/gatk/issues/4540:504,Integrability,protocol,protocol,504,"Minor things that should be fixed in CNNScoreVariants:; - the header should include the lines returned from getDefaultToolVCFHeaderLines; - the code currently (unnecessarily?) adds standard VQSR header lines via addVQSRStandardHeaderLines; - we should consider writing the reads data as separate lines in the FIFO for the 2d case to eliminate the need to create and then parse very long lines in python to find variant and read boundaries. longer term we may want to use a more structured format such as protocol buffers; - writeOutputVCFWithScores does a second (post-traversal) traversal, but should use whatever mechanism results from https://github.com/broadinstitute/gatk/issues/4316",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4540
https://github.com/broadinstitute/gatk/pull/4542:19,Integrability,interface,interface,19,removing an unused interface that was carried over from GATK3; removing check for file subtypes that is also no longer relevant,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4542
https://github.com/broadinstitute/gatk/issues/4544:289,Performance,load,loading,289,"Several people are reporting unreasonable memory usage in GenotypeGVCFs. ; See https://gatkforums.broadinstitute.org/gatk/discussion/11634/genotypegvcfs-resource-problems#latest; and https://github.com/broadinstitute/gatk/issues/4467. We should investigate what's going on, probably we're loading to many lines into memory at once somehow. It's possible it's related to running on a base pair resolution file. Possible that https://github.com/broadinstitute/gatk/pull/3480 might help?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4544
https://github.com/broadinstitute/gatk/pull/4545:111,Testability,test,tests,111,adding a --sort-order option to SortSamSpark to let users specify the what order to sort in; enabling disabled tests; fixing the tests which weren't actually asserting anything. closes #1260,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4545
https://github.com/broadinstitute/gatk/pull/4545:129,Testability,test,tests,129,adding a --sort-order option to SortSamSpark to let users specify the what order to sort in; enabling disabled tests; fixing the tests which weren't actually asserting anything. closes #1260,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4545
https://github.com/broadinstitute/gatk/pull/4545:158,Testability,assert,asserting,158,adding a --sort-order option to SortSamSpark to let users specify the what order to sort in; enabling disabled tests; fixing the tests which weren't actually asserting anything. closes #1260,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4545
https://github.com/broadinstitute/gatk/issues/4549:62,Availability,down,downloading,62,"We should have an installer script checked in to the repo for downloading the Funcotator datasources. Ideally the script would have a trivial Java frontend in the form of a simple GATK tool, to make it more discoverable by users.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4549
https://github.com/broadinstitute/gatk/issues/4549:18,Deployability,install,installer,18,"We should have an installer script checked in to the repo for downloading the Funcotator datasources. Ideally the script would have a trivial Java frontend in the form of a simple GATK tool, to make it more discoverable by users.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4549
https://github.com/broadinstitute/gatk/issues/4549:173,Usability,simpl,simple,173,"We should have an installer script checked in to the repo for downloading the Funcotator datasources. Ideally the script would have a trivial Java frontend in the form of a simple GATK tool, to make it more discoverable by users.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4549
https://github.com/broadinstitute/gatk/issues/4550:84,Modifiability,refactor,refactored,84,"As discovered in one of the recent `Funcotator` PRs, `ArgumentsBuilder` needs to be refactored.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4550
https://github.com/broadinstitute/gatk/issues/4551:1426,Integrability,depend,depending,1426,"plied_ to the reference to generate the SV genome. Paired-end reads w/ equal lengths (100bp) and insert sizes (500bp) were _uniformly_ generated from the SV genome and was mapped to chr22 using BWA-MEM (default arguments). Coverage on 100bp uniform bins were collected using `CollectFragmentCounts` (default arguments: MQ > 30, both mates aligned, and only innies). The coverage was studied case-by-case on a few SVs. **Case-by-base study**:; ; _Balanced translocation:_; <img width=""1440"" alt=""baltr-1-igv"" src=""https://user-images.githubusercontent.com/15305869/37737056-0a811466-2d29-11e8-84ef-4f31f030d05b.png"">. Here, an event in shown where a ~ 3kb region of chr22 is translocated to another region. Ideally, there should be no coverage loss. The IGV inspection shows excess coverage on the left side and depletion on the right side. Upon inspecting the conjugate translocation site, a similar scenario is seen. This situation is hardly avoidable -- depending on the mappability of the two loci, one captures the chimeric fragment of the other with higher probability (right?). The situation is worse for `CollectFragmentCounts` because chimeras are ignored altogether:. ![baltr-1](https://user-images.githubusercontent.com/15305869/37738066-2bb27a5a-2d2c-11e8-905d-ea553b93741b.png). _Deletion:_; For deletions, both coverage collection strategies work well. The deletion region is not quite captured perfectly by either method. <img width=""1440"" alt=""del-1-igv"" src=""https://user-images.githubusercontent.com/15305869/37738404-48dec038-2d2d-11e8-9d7e-625ff8e453e7.png"">. ![del-1](https://user-images.githubusercontent.com/15305869/37739544-ceeb7376-2d30-11e8-9480-3fb2a408e48a.png). _Tandem Duplication_:; For tandem duplications, neglecting FF and RR fagments leads to an underestimation of the size of the duplicated region by `CollectFragmentCounts`. IGV does not seem to get it quite right either (@cwhelan does the IGV plot make sense to you? could it be there's a bug in SVGen in genera",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4551
https://github.com/broadinstitute/gatk/issues/4551:51,Performance,perform,performance,51,"Pursuing issue #4519, I have tried to evaluate the performance of `CollectFragmentCounts` (the current coverage collection tool for somatic and germline CNV caller) on synthetic dataset. **Methodology**: [SVGen](https://github.com/WGLab/SVGen) was used to generate a set of random canonical SVs (deletion, tandem duplication, inversion, balanced translocation, unbalanced translocation) on chr22 of hg19 + random SNPs at observed population frequencies. The SVs were _applied_ to the reference to generate the SV genome. Paired-end reads w/ equal lengths (100bp) and insert sizes (500bp) were _uniformly_ generated from the SV genome and was mapped to chr22 using BWA-MEM (default arguments). Coverage on 100bp uniform bins were collected using `CollectFragmentCounts` (default arguments: MQ > 30, both mates aligned, and only innies). The coverage was studied case-by-case on a few SVs. **Case-by-base study**:; ; _Balanced translocation:_; <img width=""1440"" alt=""baltr-1-igv"" src=""https://user-images.githubusercontent.com/15305869/37737056-0a811466-2d29-11e8-84ef-4f31f030d05b.png"">. Here, an event in shown where a ~ 3kb region of chr22 is translocated to another region. Ideally, there should be no coverage loss. The IGV inspection shows excess coverage on the left side and depletion on the right side. Upon inspecting the conjugate translocation site, a similar scenario is seen. This situation is hardly avoidable -- depending on the mappability of the two loci, one captures the chimeric fragment of the other with higher probability (right?). The situation is worse for `CollectFragmentCounts` because chimeras are ignored altogether:. ![baltr-1](https://user-images.githubusercontent.com/15305869/37738066-2bb27a5a-2d2c-11e8-905d-ea553b93741b.png). _Deletion:_; For deletions, both coverage collection strategies work well. The deletion region is not quite captured perfectly by either method. <img width=""1440"" alt=""del-1-igv"" src=""https://user-images.githubusercontent.com/15305869/37738",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4551
https://github.com/broadinstitute/gatk/issues/4551:2772,Performance,perform,performs,2772,"erage collection strategies work well. The deletion region is not quite captured perfectly by either method. <img width=""1440"" alt=""del-1-igv"" src=""https://user-images.githubusercontent.com/15305869/37738404-48dec038-2d2d-11e8-9d7e-625ff8e453e7.png"">. ![del-1](https://user-images.githubusercontent.com/15305869/37739544-ceeb7376-2d30-11e8-9480-3fb2a408e48a.png). _Tandem Duplication_:; For tandem duplications, neglecting FF and RR fagments leads to an underestimation of the size of the duplicated region by `CollectFragmentCounts`. IGV does not seem to get it quite right either (@cwhelan does the IGV plot make sense to you? could it be there's a bug in SVGen in generating tandem duplications? ). <img width=""1440"" alt=""dup-1-igv"" src=""https://user-images.githubusercontent.com/15305869/37738572-ced05ce2-2d2d-11e8-9194-b5d9a0c15eaa.png"">. ![dup-1](https://user-images.githubusercontent.com/15305869/37738573-cee0f174-2d2d-11e8-9e7c-56ad20be73c4.png). _Inversion_:; IGV performs well, with very little coverage depletion at the boundaries. `CollectFragmentCounts` shows significant coverage depletion at the boundaries + random dropouts (why?). <img width=""1440"" alt=""inv-1-igv"" src=""https://user-images.githubusercontent.com/15305869/37738780-7918dc92-2d2e-11e8-8b6d-9edd34f7a65e.png"">. ![inv-1](https://user-images.githubusercontent.com/15305869/37739610-0fe8c752-2d31-11e8-8eb7-d22477ce00db.png). Here's another example in a less mappable region (the IGV track should [GMA](https://sourceforge.net/p/gma-bio/wiki/Home/) Illumina mappability track):. <img width=""1440"" alt=""inv-2-igv"" src=""https://user-images.githubusercontent.com/15305869/37738896-da832852-2d2e-11e8-8866-c46ea024d586.png"">. ![inv-2](https://user-images.githubusercontent.com/15305869/37739616-14feccbe-2d31-11e8-9217-ea8d19515001.png). Again, IGV does a much better job. In general, keeping only FR pairs seems to lead to noisy coverage, especially in low mappability regions. _Unbalanced Translocation:_; A clear win for I",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4551
https://github.com/broadinstitute/gatk/issues/4551:1413,Safety,avoid,avoidable,1413,"plied_ to the reference to generate the SV genome. Paired-end reads w/ equal lengths (100bp) and insert sizes (500bp) were _uniformly_ generated from the SV genome and was mapped to chr22 using BWA-MEM (default arguments). Coverage on 100bp uniform bins were collected using `CollectFragmentCounts` (default arguments: MQ > 30, both mates aligned, and only innies). The coverage was studied case-by-case on a few SVs. **Case-by-base study**:; ; _Balanced translocation:_; <img width=""1440"" alt=""baltr-1-igv"" src=""https://user-images.githubusercontent.com/15305869/37737056-0a811466-2d29-11e8-84ef-4f31f030d05b.png"">. Here, an event in shown where a ~ 3kb region of chr22 is translocated to another region. Ideally, there should be no coverage loss. The IGV inspection shows excess coverage on the left side and depletion on the right side. Upon inspecting the conjugate translocation site, a similar scenario is seen. This situation is hardly avoidable -- depending on the mappability of the two loci, one captures the chimeric fragment of the other with higher probability (right?). The situation is worse for `CollectFragmentCounts` because chimeras are ignored altogether:. ![baltr-1](https://user-images.githubusercontent.com/15305869/37738066-2bb27a5a-2d2c-11e8-905d-ea553b93741b.png). _Deletion:_; For deletions, both coverage collection strategies work well. The deletion region is not quite captured perfectly by either method. <img width=""1440"" alt=""del-1-igv"" src=""https://user-images.githubusercontent.com/15305869/37738404-48dec038-2d2d-11e8-9d7e-625ff8e453e7.png"">. ![del-1](https://user-images.githubusercontent.com/15305869/37739544-ceeb7376-2d30-11e8-9480-3fb2a408e48a.png). _Tandem Duplication_:; For tandem duplications, neglecting FF and RR fagments leads to an underestimation of the size of the duplicated region by `CollectFragmentCounts`. IGV does not seem to get it quite right either (@cwhelan does the IGV plot make sense to you? could it be there's a bug in SVGen in genera",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4551
https://github.com/broadinstitute/gatk/issues/4551:3782,Usability,clear,clear,3782,"es. `CollectFragmentCounts` shows significant coverage depletion at the boundaries + random dropouts (why?). <img width=""1440"" alt=""inv-1-igv"" src=""https://user-images.githubusercontent.com/15305869/37738780-7918dc92-2d2e-11e8-8b6d-9edd34f7a65e.png"">. ![inv-1](https://user-images.githubusercontent.com/15305869/37739610-0fe8c752-2d31-11e8-8eb7-d22477ce00db.png). Here's another example in a less mappable region (the IGV track should [GMA](https://sourceforge.net/p/gma-bio/wiki/Home/) Illumina mappability track):. <img width=""1440"" alt=""inv-2-igv"" src=""https://user-images.githubusercontent.com/15305869/37738896-da832852-2d2e-11e8-8866-c46ea024d586.png"">. ![inv-2](https://user-images.githubusercontent.com/15305869/37739616-14feccbe-2d31-11e8-9217-ea8d19515001.png). Again, IGV does a much better job. In general, keeping only FR pairs seems to lead to noisy coverage, especially in low mappability regions. _Unbalanced Translocation:_; A clear win for IGV, and a good reason for keeping split reads (notice the coverage loss on the left side of the event in `CollectFragmentCounts`). <img width=""1440"" alt=""unbtr-1-igv"" src=""https://user-images.githubusercontent.com/15305869/37738983-1b13bd6e-2d2f-11e8-8568-f473f8d3880b.png"">. ![unbtr-1](https://user-images.githubusercontent.com/15305869/37739626-1b50c964-2d31-11e8-8e8b-46d882e59810.png). An example is in a low complexity region:. <img width=""1440"" alt=""unbtr-2-igv"" src=""https://user-images.githubusercontent.com/15305869/37739176-bec4847a-2d2f-11e8-9ab1-64d2a3857459.png"">. ![unbtr-2](https://user-images.githubusercontent.com/15305869/37739630-1f0f4d28-2d31-11e8-9703-a67af5a442bc.png). No good strategy here -- it's better to blacklist such regions altogether for CNV calling. Another win for IGV. I do not understand the reason for coverage dropouts in `CollectFragmentCounts`. It might have something to do with the SNPs (though, all reads have high MQ). <img width=""1440"" alt=""unbtr-3-igv"" src=""https://user-images.githubusercontent.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4551
https://github.com/broadinstitute/gatk/issues/4553:13,Availability,down,down,13,"This can cut down the size of the PoN. However, see #4554.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4553
https://github.com/broadinstitute/gatk/issues/4556:18,Deployability,release,release,18,"The latest Picard release introduces a dependency on the Google Cloud NIO library that conflicts with GATK's dependency. We are going to have to blacklist the Picard NIO dependency for now. . Longer term, we might want to consider having both projects depend upon a build of htsjdk that comes with the NIO plugin.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4556
https://github.com/broadinstitute/gatk/issues/4556:39,Integrability,depend,dependency,39,"The latest Picard release introduces a dependency on the Google Cloud NIO library that conflicts with GATK's dependency. We are going to have to blacklist the Picard NIO dependency for now. . Longer term, we might want to consider having both projects depend upon a build of htsjdk that comes with the NIO plugin.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4556
https://github.com/broadinstitute/gatk/issues/4556:109,Integrability,depend,dependency,109,"The latest Picard release introduces a dependency on the Google Cloud NIO library that conflicts with GATK's dependency. We are going to have to blacklist the Picard NIO dependency for now. . Longer term, we might want to consider having both projects depend upon a build of htsjdk that comes with the NIO plugin.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4556
https://github.com/broadinstitute/gatk/issues/4556:170,Integrability,depend,dependency,170,"The latest Picard release introduces a dependency on the Google Cloud NIO library that conflicts with GATK's dependency. We are going to have to blacklist the Picard NIO dependency for now. . Longer term, we might want to consider having both projects depend upon a build of htsjdk that comes with the NIO plugin.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4556
https://github.com/broadinstitute/gatk/issues/4556:252,Integrability,depend,depend,252,"The latest Picard release introduces a dependency on the Google Cloud NIO library that conflicts with GATK's dependency. We are going to have to blacklist the Picard NIO dependency for now. . Longer term, we might want to consider having both projects depend upon a build of htsjdk that comes with the NIO plugin.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4556
https://github.com/broadinstitute/gatk/issues/4556:306,Modifiability,plugin,plugin,306,"The latest Picard release introduces a dependency on the Google Cloud NIO library that conflicts with GATK's dependency. We are going to have to blacklist the Picard NIO dependency for now. . Longer term, we might want to consider having both projects depend upon a build of htsjdk that comes with the NIO plugin.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4556
https://github.com/broadinstitute/gatk/issues/4557:76,Integrability,depend,depends,76,This will prevent any accidental collision with the version on which Picard depends.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4557
https://github.com/broadinstitute/gatk/issues/4558:977,Availability,error,errors,977,"As a step toward productionizing gCNV, I explore the idea of aggressive filtering of reads (or targets, or bins) based on known annotations, such as genome mappability, complexity, SINEs, LINEs, repeats, etc. This post is about mappability filtering, and in particular, for WGS samples. Let us consider coverage data collected by `CollectFragmentCounts` (default tool args: well-formed paired, FR orientation, MQ > 30) for a several WGS samples (hg38, BWA-MEM). Here's how the coverage distribution looks like for a random sample:; ![image](https://user-images.githubusercontent.com/15305869/37785053-8c9ca298-2dcf-11e8-9eb4-20192af80647.png). Notice the main peak at ~ 90 fragments/bin, the diffuse counts below 90, and the peak at 0. To proceed, I calculated the average mappability score of each bin using the recently published [multi-read Umap track](https://bismap.hoffmanlab.org/). It is not an ideal mappability scoring scheme (since it does not consider Illumina-like errors and paired-end reads), but it is the best I could find for hg38. I filtered out all bins that had a mappability score short of 100% (very aggressive). Here's how the filtered coverage distribution looks like:; ![image](https://user-images.githubusercontent.com/15305869/37785397-6c8052ba-2dd0-11e8-9fed-f8f1aa4406cf.png). Evidently, the diffusive low coverage part of the distribution has completely disappeared, along with the peak at 0. Here's a plot of the number of _retained bins_ after filtering with various mean mappability cutoffs:; ![image](https://user-images.githubusercontent.com/15305869/37785471-9d155920-2dd0-11e8-87bf-b8fb579250ff.png). Except for chrY, the most aggressive filtering only gets rid of 20% of the genome. Let us fit different distributions to the filtered coverage data:; ![image](https://user-images.githubusercontent.com/15305869/37785506-b80a4790-2dd0-11e8-97e4-9bff775a93ce.png). As we expect, negative binomial is almost a _perfect_ fit! this is very reassuring, because gCNV mode",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4558
https://github.com/broadinstitute/gatk/issues/4558:4033,Availability,degraded,degraded,4033," at the coverage mode (~ 100 fragments/bin), and a mode that bifurcates to lower values. **I conjecture that the bimodality results from heterogeneity of mappability scores at different positions in the same bin. The bins are 1k wide and it is feasible that some positions are highly mappable and other positions are not. This conjecture can be tested by collecting coverage on smaller bins and to check whether the bimodality weakens. If it does, I suggest filtering based on read position, similar to Genome STRiP, as opposed to filtering bins.**. Also, there is little sample-to-sample variation in coverage-mappability scatter plots (as opposed to, let's say, GC). **Therefore, there is no reason to consider mappability as a bias covariate**. The mappability coverage bias can be captured by a cohort-wide mean bias. Finally, let us study the NB overdispersion of different samples for different contigs:; ![image](https://user-images.githubusercontent.com/15305869/37785938-c47b4e38-2dd1-11e8-85f5-6e82764afbde.png). There's a clear structure here: some samples have higher overdispersion than the others. This could be due to degraded samples, less even GC curve, different chemistry, etc. In any event, we can regress the residual variance $psi_sj$ (for sample s, contig j) with a linear model:. psi_sj ~ N(a_s * psi_j + b_s, \beta). Here's how the regression looks like:; ![image](https://user-images.githubusercontent.com/15305869/37786020-fec1fccc-2dd1-11e8-9751-92e38979f120.png). Pretty much everything is explained by the linear model. This provides support for our choice of linear-NB model in gCNV. Finally, let us examine whether there is a correlation between $a_s$, $b_s$, and depth of coverage:. ![image](https://user-images.githubusercontent.com/15305869/37786095-26b08384-2dd2-11e8-9aed-8adff8995ac5.png); ![image](https://user-images.githubusercontent.com/15305869/37786101-2a6d4be2-2dd2-11e8-85c5-5c6754d42d52.png). There is absolutely no correlation, which is again expected.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4558
https://github.com/broadinstitute/gatk/issues/4558:3245,Testability,test,tested,3245,"/15305869/37785631-078c3fa8-2dd1-11e8-8bd1-3e8b1da36c8b.png); ![image](https://user-images.githubusercontent.com/15305869/37785638-0bc9f48e-2dd1-11e8-9aa1-47c77a5625aa.png); ![image](https://user-images.githubusercontent.com/15305869/37785656-19976c68-2dd1-11e8-802e-1893eb37cdd7.png); ![image](https://user-images.githubusercontent.com/15305869/37785664-1dcafa48-2dd1-11e8-832e-9d5fa0529653.png); ![image](https://user-images.githubusercontent.com/15305869/37785674-24d8d3dc-2dd1-11e8-8359-5dd266ca3947.png). Clearly, there is a strong correlation here. In particular, low mappability bins show two distinct modes: a mode at the coverage mode (~ 100 fragments/bin), and a mode that bifurcates to lower values. **I conjecture that the bimodality results from heterogeneity of mappability scores at different positions in the same bin. The bins are 1k wide and it is feasible that some positions are highly mappable and other positions are not. This conjecture can be tested by collecting coverage on smaller bins and to check whether the bimodality weakens. If it does, I suggest filtering based on read position, similar to Genome STRiP, as opposed to filtering bins.**. Also, there is little sample-to-sample variation in coverage-mappability scatter plots (as opposed to, let's say, GC). **Therefore, there is no reason to consider mappability as a bias covariate**. The mappability coverage bias can be captured by a cohort-wide mean bias. Finally, let us study the NB overdispersion of different samples for different contigs:; ![image](https://user-images.githubusercontent.com/15305869/37785938-c47b4e38-2dd1-11e8-85f5-6e82764afbde.png). There's a clear structure here: some samples have higher overdispersion than the others. This could be due to degraded samples, less even GC curve, different chemistry, etc. In any event, we can regress the residual variance $psi_sj$ (for sample s, contig j) with a linear model:. psi_sj ~ N(a_s * psi_j + b_s, \beta). Here's how the regression looks like",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4558
https://github.com/broadinstitute/gatk/issues/4558:2788,Usability,Clear,Clearly,2788,"-images.githubusercontent.com/15305869/37785506-b80a4790-2dd0-11e8-97e4-9bff775a93ce.png). As we expect, negative binomial is almost a _perfect_ fit! this is very reassuring, because gCNV model is based on negative binomial (based on purely theoretical reasoning). Now, let us look at unfiltered coverage and regress coverage with mappability. Here's how it looks like for 3 different contigs and 5 different samples:; ![image](https://user-images.githubusercontent.com/15305869/37785631-078c3fa8-2dd1-11e8-8bd1-3e8b1da36c8b.png); ![image](https://user-images.githubusercontent.com/15305869/37785638-0bc9f48e-2dd1-11e8-9aa1-47c77a5625aa.png); ![image](https://user-images.githubusercontent.com/15305869/37785656-19976c68-2dd1-11e8-802e-1893eb37cdd7.png); ![image](https://user-images.githubusercontent.com/15305869/37785664-1dcafa48-2dd1-11e8-832e-9d5fa0529653.png); ![image](https://user-images.githubusercontent.com/15305869/37785674-24d8d3dc-2dd1-11e8-8359-5dd266ca3947.png). Clearly, there is a strong correlation here. In particular, low mappability bins show two distinct modes: a mode at the coverage mode (~ 100 fragments/bin), and a mode that bifurcates to lower values. **I conjecture that the bimodality results from heterogeneity of mappability scores at different positions in the same bin. The bins are 1k wide and it is feasible that some positions are highly mappable and other positions are not. This conjecture can be tested by collecting coverage on smaller bins and to check whether the bimodality weakens. If it does, I suggest filtering based on read position, similar to Genome STRiP, as opposed to filtering bins.**. Also, there is little sample-to-sample variation in coverage-mappability scatter plots (as opposed to, let's say, GC). **Therefore, there is no reason to consider mappability as a bias covariate**. The mappability coverage bias can be captured by a cohort-wide mean bias. Finally, let us study the NB overdispersion of different samples for different contigs:;",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4558
https://github.com/broadinstitute/gatk/issues/4558:3933,Usability,clear,clear,3933," at the coverage mode (~ 100 fragments/bin), and a mode that bifurcates to lower values. **I conjecture that the bimodality results from heterogeneity of mappability scores at different positions in the same bin. The bins are 1k wide and it is feasible that some positions are highly mappable and other positions are not. This conjecture can be tested by collecting coverage on smaller bins and to check whether the bimodality weakens. If it does, I suggest filtering based on read position, similar to Genome STRiP, as opposed to filtering bins.**. Also, there is little sample-to-sample variation in coverage-mappability scatter plots (as opposed to, let's say, GC). **Therefore, there is no reason to consider mappability as a bias covariate**. The mappability coverage bias can be captured by a cohort-wide mean bias. Finally, let us study the NB overdispersion of different samples for different contigs:; ![image](https://user-images.githubusercontent.com/15305869/37785938-c47b4e38-2dd1-11e8-85f5-6e82764afbde.png). There's a clear structure here: some samples have higher overdispersion than the others. This could be due to degraded samples, less even GC curve, different chemistry, etc. In any event, we can regress the residual variance $psi_sj$ (for sample s, contig j) with a linear model:. psi_sj ~ N(a_s * psi_j + b_s, \beta). Here's how the regression looks like:; ![image](https://user-images.githubusercontent.com/15305869/37786020-fec1fccc-2dd1-11e8-9751-92e38979f120.png). Pretty much everything is explained by the linear model. This provides support for our choice of linear-NB model in gCNV. Finally, let us examine whether there is a correlation between $a_s$, $b_s$, and depth of coverage:. ![image](https://user-images.githubusercontent.com/15305869/37786095-26b08384-2dd2-11e8-9aed-8adff8995ac5.png); ![image](https://user-images.githubusercontent.com/15305869/37786101-2a6d4be2-2dd2-11e8-85c5-5c6754d42d52.png). There is absolutely no correlation, which is again expected.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4558
https://github.com/broadinstitute/gatk/issues/4559:31,Usability,learn,learning,31,"Hello,; I wish to try the deep learning search but what are we supposed to feed the program with the option ""a"" ?; `--architecture,-a:String Neural Net architecture and weights hd5 file Required`. By hd5 file, do you mean hdf5 file? . Thanks a lot. . Alessandro",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4559
https://github.com/broadinstitute/gatk/pull/4562:406,Testability,test,test,406,"- [x] Output bam instead of sam for assembly alignments; - [x] Instead of creating directory, new interpretation tool writes files (behavior consistent with current interpretation tool); - [x] Prefix with sample name for output files' names; - [x] Add `INSLEN` annotation when there's `INSSEQ`; - [x] Clarify the boundary between `AlignedContig` and `AssemblyContigWithFineTunedAlignments`; - [x] Increase test coverage for `AssemblyContigAlignmentsConfigPicker`. Up to date plans for more cleanups and improvements posted in #4111",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4562
https://github.com/broadinstitute/gatk/pull/4563:238,Deployability,upgrade,upgrade,238,"Added more logging functionality to better inform users. Fixed NullPointerException when onTraversalStart throws.; Now logs version number information for data sources.; Now checks minimum data sources version and requires that the user; upgrade if the version is too old. Fixes #4521 in a roundabout way - data sources need to be re-released for this version to run at all, but these new data sources contain a fix for the HG38 issue.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4563
https://github.com/broadinstitute/gatk/pull/4563:334,Deployability,release,released,334,"Added more logging functionality to better inform users. Fixed NullPointerException when onTraversalStart throws.; Now logs version number information for data sources.; Now checks minimum data sources version and requires that the user; upgrade if the version is too old. Fixes #4521 in a roundabout way - data sources need to be re-released for this version to run at all, but these new data sources contain a fix for the HG38 issue.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4563
https://github.com/broadinstitute/gatk/pull/4563:11,Testability,log,logging,11,"Added more logging functionality to better inform users. Fixed NullPointerException when onTraversalStart throws.; Now logs version number information for data sources.; Now checks minimum data sources version and requires that the user; upgrade if the version is too old. Fixes #4521 in a roundabout way - data sources need to be re-released for this version to run at all, but these new data sources contain a fix for the HG38 issue.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4563
https://github.com/broadinstitute/gatk/pull/4563:119,Testability,log,logs,119,"Added more logging functionality to better inform users. Fixed NullPointerException when onTraversalStart throws.; Now logs version number information for data sources.; Now checks minimum data sources version and requires that the user; upgrade if the version is too old. Fixes #4521 in a roundabout way - data sources need to be re-released for this version to run at all, but these new data sources contain a fix for the HG38 issue.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4563
https://github.com/broadinstitute/gatk/pull/4564:672,Deployability,update,updated,672,"Closes #4519. Note that we should still explore and evaluate other collection strategies in the future. However, I think we can go ahead and use this strategy as the default for now. @asmirnov239 @mbabadi Let's get this merged quick and run the evaluations for the AACR poster using this collection strategy. We should also run some subset of the evaluations with CollectFragmentCounts and check that results improve. @LeeTL1220 @MartonKN We should rerun the somatic evaluations as necessary to make sure they still look good. We may also want to show any interested parties in CGA the relevant results. @sooheelee Note that some tutorial material, slides, etc. should be updated at the appropriate time (and may need to be updated again as we continue to tweak the strategy).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4564
https://github.com/broadinstitute/gatk/pull/4564:724,Deployability,update,updated,724,"Closes #4519. Note that we should still explore and evaluate other collection strategies in the future. However, I think we can go ahead and use this strategy as the default for now. @asmirnov239 @mbabadi Let's get this merged quick and run the evaluations for the AACR poster using this collection strategy. We should also run some subset of the evaluations with CollectFragmentCounts and check that results improve. @LeeTL1220 @MartonKN We should rerun the somatic evaluations as necessary to make sure they still look good. We may also want to show any interested parties in CGA the relevant results. @sooheelee Note that some tutorial material, slides, etc. should be updated at the appropriate time (and may need to be updated again as we continue to tweak the strategy).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4564
https://github.com/broadinstitute/gatk/issues/4565:85,Deployability,update,update,85,"I have a project that depends on `4.alpha.2-183-ge1e71d7-SNAPSHOT` (I am planning to update it, that's why I look at it), and I discovered that the jfrog repository the folder is empty: https://broadinstitute.jfrog.io/broadinstitute/libs-snapshot/org/broadinstitute/gatk/4.alpha.2-183-ge1e71d7-SNAPSHOT/. When looking at the jfrog repository, it looks like no SNAPSHOT jar file is present. As an example, one after-release SNAPSHOT: https://broadinstitute.jfrog.io/broadinstitute/libs-snapshot/org/broadinstitute/gatk/4.0.0.0-1-g9423d25-SNAPSHOT/. Is this in purpose or should it be present? It looks like the current commit https://github.com/broadinstitute/gatk/commit/8463525cc9b523cd00daf2810bcf1c13b69ce0a1 does contain a proper artifact, but I am wondering how long the snapshots are going to be maintained. Thank you!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4565
https://github.com/broadinstitute/gatk/issues/4565:415,Deployability,release,release,415,"I have a project that depends on `4.alpha.2-183-ge1e71d7-SNAPSHOT` (I am planning to update it, that's why I look at it), and I discovered that the jfrog repository the folder is empty: https://broadinstitute.jfrog.io/broadinstitute/libs-snapshot/org/broadinstitute/gatk/4.alpha.2-183-ge1e71d7-SNAPSHOT/. When looking at the jfrog repository, it looks like no SNAPSHOT jar file is present. As an example, one after-release SNAPSHOT: https://broadinstitute.jfrog.io/broadinstitute/libs-snapshot/org/broadinstitute/gatk/4.0.0.0-1-g9423d25-SNAPSHOT/. Is this in purpose or should it be present? It looks like the current commit https://github.com/broadinstitute/gatk/commit/8463525cc9b523cd00daf2810bcf1c13b69ce0a1 does contain a proper artifact, but I am wondering how long the snapshots are going to be maintained. Thank you!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4565
https://github.com/broadinstitute/gatk/issues/4565:22,Integrability,depend,depends,22,"I have a project that depends on `4.alpha.2-183-ge1e71d7-SNAPSHOT` (I am planning to update it, that's why I look at it), and I discovered that the jfrog repository the folder is empty: https://broadinstitute.jfrog.io/broadinstitute/libs-snapshot/org/broadinstitute/gatk/4.alpha.2-183-ge1e71d7-SNAPSHOT/. When looking at the jfrog repository, it looks like no SNAPSHOT jar file is present. As an example, one after-release SNAPSHOT: https://broadinstitute.jfrog.io/broadinstitute/libs-snapshot/org/broadinstitute/gatk/4.0.0.0-1-g9423d25-SNAPSHOT/. Is this in purpose or should it be present? It looks like the current commit https://github.com/broadinstitute/gatk/commit/8463525cc9b523cd00daf2810bcf1c13b69ce0a1 does contain a proper artifact, but I am wondering how long the snapshots are going to be maintained. Thank you!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4565
https://github.com/broadinstitute/gatk/issues/4567:200,Energy Efficiency,reduce,reduce,200,"Currently when we publish an artifactory snapshot, we upload both the maven jars (~50 MB) plus the fully-packaged jars (~500 MB). We only need the maven jars! By eliminating the packaged jars, we can reduce our artifact size by an order of magnitude. Once this is done, we should talk to @davidbernick about increasing the artifact expiration time from the current 60 days to something longer.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4567
https://github.com/broadinstitute/gatk/issues/4570:38,Security,access,accessed,38,The name for the `variantContext` (as accessed by `getSource()`) is never populated properly. It needs to be populated in the codec. This field should be populated when the codec is created in `FeatureDataSource::getCodecForFeatureInput`.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4570
https://github.com/broadinstitute/gatk/issues/4572:136,Availability,error,error,136,Data is sensitive and bug is recapitulated in https://github.com/broadinstitute/dsde-docs/issues/3026. CombineGVCFs gives the following error message:; ```; java.lang.IllegalArgumentException: Invalid interval. Contig:HLA-DRB1*15:03:01:02 start:11569 end:11005; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); 	at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); 	at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); 	at org.broadinstitute.hellbender.tools.walkers.CombineGVCFs.onTraversalSuccess(CombineGVCFs.java:415); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:895); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:135); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:180); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:199); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:159); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:202); 	at org.broadinstitute.hellbender.Main.main(Main.java:288); ```; Here are the dictionary lines for two consecutive HLA-DRB1 contigs:; ```; @SQ SN:HLA-DRB1*15:03:01:02 LN:11569 M5:4e0d459b9bd15bff8645de84334e3d25 AS:38 UR:/seq/references/Homo_sapiens_assembly38/v0/Homo_sapiens_assembly38.fasta SP:Homo sapiens; @SQ SN:HLA-DRB1*16:02:01 LN:11005 M5:4a972df76bd3ee2857b87bd5be5ea00a AS:38 UR:/seq/references/Homo_sapiens_assembly38/v0/Homo_sapiens_assembly38.fasta SP:Homo sapiens; ```; Notice the `LN` lengths match up. It appears that our tool is mistaking contig information.; Note that `HLA-DRB1*16:02:01` is the very last contig in GRCh38.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4572
https://github.com/broadinstitute/gatk/issues/4572:142,Integrability,message,message,142,Data is sensitive and bug is recapitulated in https://github.com/broadinstitute/dsde-docs/issues/3026. CombineGVCFs gives the following error message:; ```; java.lang.IllegalArgumentException: Invalid interval. Contig:HLA-DRB1*15:03:01:02 start:11569 end:11005; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); 	at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); 	at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); 	at org.broadinstitute.hellbender.tools.walkers.CombineGVCFs.onTraversalSuccess(CombineGVCFs.java:415); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:895); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:135); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:180); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:199); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:159); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:202); 	at org.broadinstitute.hellbender.Main.main(Main.java:288); ```; Here are the dictionary lines for two consecutive HLA-DRB1 contigs:; ```; @SQ SN:HLA-DRB1*15:03:01:02 LN:11569 M5:4e0d459b9bd15bff8645de84334e3d25 AS:38 UR:/seq/references/Homo_sapiens_assembly38/v0/Homo_sapiens_assembly38.fasta SP:Homo sapiens; @SQ SN:HLA-DRB1*16:02:01 LN:11005 M5:4a972df76bd3ee2857b87bd5be5ea00a AS:38 UR:/seq/references/Homo_sapiens_assembly38/v0/Homo_sapiens_assembly38.fasta SP:Homo sapiens; ```; Notice the `LN` lengths match up. It appears that our tool is mistaking contig information.; Note that `HLA-DRB1*16:02:01` is the very last contig in GRCh38.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4572
https://github.com/broadinstitute/gatk/issues/4572:308,Security,validat,validateArg,308,Data is sensitive and bug is recapitulated in https://github.com/broadinstitute/dsde-docs/issues/3026. CombineGVCFs gives the following error message:; ```; java.lang.IllegalArgumentException: Invalid interval. Contig:HLA-DRB1*15:03:01:02 start:11569 end:11005; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); 	at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); 	at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); 	at org.broadinstitute.hellbender.tools.walkers.CombineGVCFs.onTraversalSuccess(CombineGVCFs.java:415); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:895); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:135); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:180); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:199); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:159); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:202); 	at org.broadinstitute.hellbender.Main.main(Main.java:288); ```; Here are the dictionary lines for two consecutive HLA-DRB1 contigs:; ```; @SQ SN:HLA-DRB1*15:03:01:02 LN:11569 M5:4e0d459b9bd15bff8645de84334e3d25 AS:38 UR:/seq/references/Homo_sapiens_assembly38/v0/Homo_sapiens_assembly38.fasta SP:Homo sapiens; @SQ SN:HLA-DRB1*16:02:01 LN:11005 M5:4a972df76bd3ee2857b87bd5be5ea00a AS:38 UR:/seq/references/Homo_sapiens_assembly38/v0/Homo_sapiens_assembly38.fasta SP:Homo sapiens; ```; Notice the `LN` lengths match up. It appears that our tool is mistaking contig information.; Note that `HLA-DRB1*16:02:01` is the very last contig in GRCh38.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4572
https://github.com/broadinstitute/gatk/issues/4572:392,Security,validat,validatePositions,392,Data is sensitive and bug is recapitulated in https://github.com/broadinstitute/dsde-docs/issues/3026. CombineGVCFs gives the following error message:; ```; java.lang.IllegalArgumentException: Invalid interval. Contig:HLA-DRB1*15:03:01:02 start:11569 end:11005; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); 	at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); 	at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); 	at org.broadinstitute.hellbender.tools.walkers.CombineGVCFs.onTraversalSuccess(CombineGVCFs.java:415); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:895); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:135); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:180); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:199); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:159); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:202); 	at org.broadinstitute.hellbender.Main.main(Main.java:288); ```; Here are the dictionary lines for two consecutive HLA-DRB1 contigs:; ```; @SQ SN:HLA-DRB1*15:03:01:02 LN:11569 M5:4e0d459b9bd15bff8645de84334e3d25 AS:38 UR:/seq/references/Homo_sapiens_assembly38/v0/Homo_sapiens_assembly38.fasta SP:Homo sapiens; @SQ SN:HLA-DRB1*16:02:01 LN:11005 M5:4a972df76bd3ee2857b87bd5be5ea00a AS:38 UR:/seq/references/Homo_sapiens_assembly38/v0/Homo_sapiens_assembly38.fasta SP:Homo sapiens; ```; Notice the `LN` lengths match up. It appears that our tool is mistaking contig information.; Note that `HLA-DRB1*16:02:01` is the very last contig in GRCh38.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4572
https://github.com/broadinstitute/gatk/issues/4572:377,Usability,Simpl,SimpleInterval,377,Data is sensitive and bug is recapitulated in https://github.com/broadinstitute/dsde-docs/issues/3026. CombineGVCFs gives the following error message:; ```; java.lang.IllegalArgumentException: Invalid interval. Contig:HLA-DRB1*15:03:01:02 start:11569 end:11005; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); 	at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); 	at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); 	at org.broadinstitute.hellbender.tools.walkers.CombineGVCFs.onTraversalSuccess(CombineGVCFs.java:415); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:895); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:135); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:180); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:199); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:159); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:202); 	at org.broadinstitute.hellbender.Main.main(Main.java:288); ```; Here are the dictionary lines for two consecutive HLA-DRB1 contigs:; ```; @SQ SN:HLA-DRB1*15:03:01:02 LN:11569 M5:4e0d459b9bd15bff8645de84334e3d25 AS:38 UR:/seq/references/Homo_sapiens_assembly38/v0/Homo_sapiens_assembly38.fasta SP:Homo sapiens; @SQ SN:HLA-DRB1*16:02:01 LN:11005 M5:4a972df76bd3ee2857b87bd5be5ea00a AS:38 UR:/seq/references/Homo_sapiens_assembly38/v0/Homo_sapiens_assembly38.fasta SP:Homo sapiens; ```; Notice the `LN` lengths match up. It appears that our tool is mistaking contig information.; Note that `HLA-DRB1*16:02:01` is the very last contig in GRCh38.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4572
https://github.com/broadinstitute/gatk/issues/4572:410,Usability,Simpl,SimpleInterval,410,Data is sensitive and bug is recapitulated in https://github.com/broadinstitute/dsde-docs/issues/3026. CombineGVCFs gives the following error message:; ```; java.lang.IllegalArgumentException: Invalid interval. Contig:HLA-DRB1*15:03:01:02 start:11569 end:11005; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); 	at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); 	at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); 	at org.broadinstitute.hellbender.tools.walkers.CombineGVCFs.onTraversalSuccess(CombineGVCFs.java:415); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:895); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:135); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:180); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:199); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:159); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:202); 	at org.broadinstitute.hellbender.Main.main(Main.java:288); ```; Here are the dictionary lines for two consecutive HLA-DRB1 contigs:; ```; @SQ SN:HLA-DRB1*15:03:01:02 LN:11569 M5:4e0d459b9bd15bff8645de84334e3d25 AS:38 UR:/seq/references/Homo_sapiens_assembly38/v0/Homo_sapiens_assembly38.fasta SP:Homo sapiens; @SQ SN:HLA-DRB1*16:02:01 LN:11005 M5:4a972df76bd3ee2857b87bd5be5ea00a AS:38 UR:/seq/references/Homo_sapiens_assembly38/v0/Homo_sapiens_assembly38.fasta SP:Homo sapiens; ```; Notice the `LN` lengths match up. It appears that our tool is mistaking contig information.; Note that `HLA-DRB1*16:02:01` is the very last contig in GRCh38.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4572
https://github.com/broadinstitute/gatk/issues/4572:475,Usability,Simpl,SimpleInterval,475,Data is sensitive and bug is recapitulated in https://github.com/broadinstitute/dsde-docs/issues/3026. CombineGVCFs gives the following error message:; ```; java.lang.IllegalArgumentException: Invalid interval. Contig:HLA-DRB1*15:03:01:02 start:11569 end:11005; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); 	at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); 	at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); 	at org.broadinstitute.hellbender.tools.walkers.CombineGVCFs.onTraversalSuccess(CombineGVCFs.java:415); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:895); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:135); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:180); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:199); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:159); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:202); 	at org.broadinstitute.hellbender.Main.main(Main.java:288); ```; Here are the dictionary lines for two consecutive HLA-DRB1 contigs:; ```; @SQ SN:HLA-DRB1*15:03:01:02 LN:11569 M5:4e0d459b9bd15bff8645de84334e3d25 AS:38 UR:/seq/references/Homo_sapiens_assembly38/v0/Homo_sapiens_assembly38.fasta SP:Homo sapiens; @SQ SN:HLA-DRB1*16:02:01 LN:11005 M5:4a972df76bd3ee2857b87bd5be5ea00a AS:38 UR:/seq/references/Homo_sapiens_assembly38/v0/Homo_sapiens_assembly38.fasta SP:Homo sapiens; ```; Notice the `LN` lengths match up. It appears that our tool is mistaking contig information.; Note that `HLA-DRB1*16:02:01` is the very last contig in GRCh38.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4572
https://github.com/broadinstitute/gatk/issues/4572:497,Usability,Simpl,SimpleInterval,497,Data is sensitive and bug is recapitulated in https://github.com/broadinstitute/dsde-docs/issues/3026. CombineGVCFs gives the following error message:; ```; java.lang.IllegalArgumentException: Invalid interval. Contig:HLA-DRB1*15:03:01:02 start:11569 end:11005; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); 	at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); 	at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); 	at org.broadinstitute.hellbender.tools.walkers.CombineGVCFs.onTraversalSuccess(CombineGVCFs.java:415); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:895); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:135); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:180); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:199); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:159); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:202); 	at org.broadinstitute.hellbender.Main.main(Main.java:288); ```; Here are the dictionary lines for two consecutive HLA-DRB1 contigs:; ```; @SQ SN:HLA-DRB1*15:03:01:02 LN:11569 M5:4e0d459b9bd15bff8645de84334e3d25 AS:38 UR:/seq/references/Homo_sapiens_assembly38/v0/Homo_sapiens_assembly38.fasta SP:Homo sapiens; @SQ SN:HLA-DRB1*16:02:01 LN:11005 M5:4a972df76bd3ee2857b87bd5be5ea00a AS:38 UR:/seq/references/Homo_sapiens_assembly38/v0/Homo_sapiens_assembly38.fasta SP:Homo sapiens; ```; Notice the `LN` lengths match up. It appears that our tool is mistaking contig information.; Note that `HLA-DRB1*16:02:01` is the very last contig in GRCh38.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4572
https://github.com/broadinstitute/gatk/issues/4578:993,Performance,load,loadNextAssemblyRegion,993,"undsException of Mutect2/GATK 4.0.2.1. ```; # java -jar /usr/hpc-bio/gatk/gatk-package-4.0.2.1-local.jar Mutect2 --verbosity WARNING -R /usr/bio-ref/GRCh38.p0.dnaref/dnaf.fa --germline-resource /usr/bio-ref/GRCh38.p0.dnaref/common.vcf --max-reads-per-alignment-start 100 -L X -I /biowrk/BaseSpace/bam.bwa/HiSe; qX-PCR-free-v2.5-NA12878/md.bam -tumor HiSeqX-PCR-free-v2.5-NA12878 -O mutect2.tumor-only.vcf; 23:17:38.084 WARN IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; [March 26, 2018 11:17:38 PM CST] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.08 minutes.; Runtime.totalMemory()=2384986112; java.lang.IndexOutOfBoundsException: Index: 0, Size: 0; at java.util.ArrayList.rangeCheck(ArrayList.java:657); at java.util.ArrayList.get(ArrayList.java:433); at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2Engine.isActive(Mutect2Engine.java:316); at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.loadNextAssemblyRegion(AssemblyRegionIterator.java:159); at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.next(AssemblyRegionIterator.java:135); at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.next(AssemblyRegionIterator.java:34); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:290); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:271); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:893); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:135); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:180); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:199); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:159); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:202); at org.broadinstit",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4578
https://github.com/broadinstitute/gatk/issues/4579:41,Testability,test,testing,41,"Though no use cases exist (yet) for NIO, testing should be added to make sure that the `AnnotatedIntervalCodec` will work using NIO. In theory, this should work out-of-the-box, but this must be confirmed.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4579
https://github.com/broadinstitute/gatk/issues/4580:154,Modifiability,inherit,inherit,154,"Currently, `AnnotatedIntervalCodec` has an attribute that is an instance of `XsvLocatableTableCodec`. There should be an abstract class that both classes inherit common code.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4580
https://github.com/broadinstitute/gatk/issues/4581:58,Deployability,configurat,configuration,58,Currently `Funcotator` uses many command-line options for configuration. This is inefficient and potentially confusing. `Funcotator` should be modified to take in a configuration file (using `Owner`) that can configure all of the CLI options for the tool.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4581
https://github.com/broadinstitute/gatk/issues/4581:165,Deployability,configurat,configuration,165,Currently `Funcotator` uses many command-line options for configuration. This is inefficient and potentially confusing. `Funcotator` should be modified to take in a configuration file (using `Owner`) that can configure all of the CLI options for the tool.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4581
https://github.com/broadinstitute/gatk/issues/4581:58,Modifiability,config,configuration,58,Currently `Funcotator` uses many command-line options for configuration. This is inefficient and potentially confusing. `Funcotator` should be modified to take in a configuration file (using `Owner`) that can configure all of the CLI options for the tool.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4581
https://github.com/broadinstitute/gatk/issues/4581:165,Modifiability,config,configuration,165,Currently `Funcotator` uses many command-line options for configuration. This is inefficient and potentially confusing. `Funcotator` should be modified to take in a configuration file (using `Owner`) that can configure all of the CLI options for the tool.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4581
https://github.com/broadinstitute/gatk/issues/4581:209,Modifiability,config,configure,209,Currently `Funcotator` uses many command-line options for configuration. This is inefficient and potentially confusing. `Funcotator` should be modified to take in a configuration file (using `Owner`) that can configure all of the CLI options for the tool.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4581
https://github.com/broadinstitute/gatk/issues/4582:35,Deployability,release,releases,35,Need to look at how to version the releases of data sources and individual data sources. This is exemplified with the dbSNP `common` vs dbSNP `All` problem.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4582
https://github.com/broadinstitute/gatk/issues/4583:64,Security,access,accessed,64,"The `MafOutputRenderer` changed how some funcotation fields are accessed. . Need to make VCF output consistent with the code in MAF output, and need to make sure it's consistent to the VCF format. Specifically, `VCFOutputRenderer` should use the funcotation map that is now created internally to go through and render the annotations. Specifically use `Funcotation::getFieldNames` and `Funcotation::getField`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4583
https://github.com/broadinstitute/gatk/issues/4591:41,Availability,error,error,41,"Hello,. when running the cnn I have this error : ; File ""<stdin>"", line 1, in <module>; AttributeError: 'module' object has no attribute 'get_metric_dict'. As I know you are working on another version, I am not totally sure if it is appropriate to report this issue for the current version. You can ignore it I can wait for the other version release. . Thanks a lot",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4591
https://github.com/broadinstitute/gatk/issues/4591:342,Deployability,release,release,342,"Hello,. when running the cnn I have this error : ; File ""<stdin>"", line 1, in <module>; AttributeError: 'module' object has no attribute 'get_metric_dict'. As I know you are working on another version, I am not totally sure if it is appropriate to report this issue for the current version. You can ignore it I can wait for the other version release. . Thanks a lot",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4591
https://github.com/broadinstitute/gatk/issues/4592:173,Availability,ERROR,ERROR,173,"I had a dumb permissions issue that had me spinning my wheels for a while because of some very terse logging. With the 5 reader threads we use in production I get:; `A USER ERROR has occurred: Couldn't read file. Error was: Failure while waiting for FeatureReader to initialize with exception: com.google.cloud.storage.StorageException: All 20 retries failed. Waited a total of 1918000 ms between attempts` ; (The error message has a placeholder for a path, but for some reason it's empty for me.). Finally I went to one thread, which called the serial FeatureReader creation method and gave me an error that could actually help solve my problem:; ```; com.google.cloud.storage.StorageException: All 20 retries failed. Waited a total of 1918000 ms between attempts; 	at com.google.cloud.storage.contrib.nio.CloudStorageRetryHandler.handleRetryForStorageException(CloudStorageRetryHandler.java:108); 	at com.google.cloud.storage.contrib.nio.CloudStorageRetryHandler.handleStorageException(CloudStorageRetryHandler.java:89); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSystemProvider.java:621); 	at java.nio.file.Files.exists(Files.java:2385); 	at htsjdk.tribble.util.ParsingUtils.resourceExists(ParsingUtils.java:419); 	at htsjdk.tribble.AbstractFeatureReader.isTabix(AbstractFeatureReader.java:243); 	at htsjdk.tribble.AbstractFeatureReader$ComponentMethods.isTabix(AbstractFeatureReader.java:249); 	at htsjdk.tribble.AbstractFeatureReader.getFeatureReader(AbstractFeatureReader.java:103); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getReaderFromPath(GenomicsDBImport.java:619); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getFeatureReadersSerially(GenomicsDBImport.java:602); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.traverse(GenomicsDBImport.java:490); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:893); 	at org.broadinstitute.hellbender.cmd",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4592
https://github.com/broadinstitute/gatk/issues/4592:213,Availability,Error,Error,213,"I had a dumb permissions issue that had me spinning my wheels for a while because of some very terse logging. With the 5 reader threads we use in production I get:; `A USER ERROR has occurred: Couldn't read file. Error was: Failure while waiting for FeatureReader to initialize with exception: com.google.cloud.storage.StorageException: All 20 retries failed. Waited a total of 1918000 ms between attempts` ; (The error message has a placeholder for a path, but for some reason it's empty for me.). Finally I went to one thread, which called the serial FeatureReader creation method and gave me an error that could actually help solve my problem:; ```; com.google.cloud.storage.StorageException: All 20 retries failed. Waited a total of 1918000 ms between attempts; 	at com.google.cloud.storage.contrib.nio.CloudStorageRetryHandler.handleRetryForStorageException(CloudStorageRetryHandler.java:108); 	at com.google.cloud.storage.contrib.nio.CloudStorageRetryHandler.handleStorageException(CloudStorageRetryHandler.java:89); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSystemProvider.java:621); 	at java.nio.file.Files.exists(Files.java:2385); 	at htsjdk.tribble.util.ParsingUtils.resourceExists(ParsingUtils.java:419); 	at htsjdk.tribble.AbstractFeatureReader.isTabix(AbstractFeatureReader.java:243); 	at htsjdk.tribble.AbstractFeatureReader$ComponentMethods.isTabix(AbstractFeatureReader.java:249); 	at htsjdk.tribble.AbstractFeatureReader.getFeatureReader(AbstractFeatureReader.java:103); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getReaderFromPath(GenomicsDBImport.java:619); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getFeatureReadersSerially(GenomicsDBImport.java:602); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.traverse(GenomicsDBImport.java:490); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:893); 	at org.broadinstitute.hellbender.cmd",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4592
https://github.com/broadinstitute/gatk/issues/4592:224,Availability,Failure,Failure,224,"I had a dumb permissions issue that had me spinning my wheels for a while because of some very terse logging. With the 5 reader threads we use in production I get:; `A USER ERROR has occurred: Couldn't read file. Error was: Failure while waiting for FeatureReader to initialize with exception: com.google.cloud.storage.StorageException: All 20 retries failed. Waited a total of 1918000 ms between attempts` ; (The error message has a placeholder for a path, but for some reason it's empty for me.). Finally I went to one thread, which called the serial FeatureReader creation method and gave me an error that could actually help solve my problem:; ```; com.google.cloud.storage.StorageException: All 20 retries failed. Waited a total of 1918000 ms between attempts; 	at com.google.cloud.storage.contrib.nio.CloudStorageRetryHandler.handleRetryForStorageException(CloudStorageRetryHandler.java:108); 	at com.google.cloud.storage.contrib.nio.CloudStorageRetryHandler.handleStorageException(CloudStorageRetryHandler.java:89); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSystemProvider.java:621); 	at java.nio.file.Files.exists(Files.java:2385); 	at htsjdk.tribble.util.ParsingUtils.resourceExists(ParsingUtils.java:419); 	at htsjdk.tribble.AbstractFeatureReader.isTabix(AbstractFeatureReader.java:243); 	at htsjdk.tribble.AbstractFeatureReader$ComponentMethods.isTabix(AbstractFeatureReader.java:249); 	at htsjdk.tribble.AbstractFeatureReader.getFeatureReader(AbstractFeatureReader.java:103); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getReaderFromPath(GenomicsDBImport.java:619); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getFeatureReadersSerially(GenomicsDBImport.java:602); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.traverse(GenomicsDBImport.java:490); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:893); 	at org.broadinstitute.hellbender.cmd",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4592
https://github.com/broadinstitute/gatk/issues/4592:414,Availability,error,error,414,"I had a dumb permissions issue that had me spinning my wheels for a while because of some very terse logging. With the 5 reader threads we use in production I get:; `A USER ERROR has occurred: Couldn't read file. Error was: Failure while waiting for FeatureReader to initialize with exception: com.google.cloud.storage.StorageException: All 20 retries failed. Waited a total of 1918000 ms between attempts` ; (The error message has a placeholder for a path, but for some reason it's empty for me.). Finally I went to one thread, which called the serial FeatureReader creation method and gave me an error that could actually help solve my problem:; ```; com.google.cloud.storage.StorageException: All 20 retries failed. Waited a total of 1918000 ms between attempts; 	at com.google.cloud.storage.contrib.nio.CloudStorageRetryHandler.handleRetryForStorageException(CloudStorageRetryHandler.java:108); 	at com.google.cloud.storage.contrib.nio.CloudStorageRetryHandler.handleStorageException(CloudStorageRetryHandler.java:89); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSystemProvider.java:621); 	at java.nio.file.Files.exists(Files.java:2385); 	at htsjdk.tribble.util.ParsingUtils.resourceExists(ParsingUtils.java:419); 	at htsjdk.tribble.AbstractFeatureReader.isTabix(AbstractFeatureReader.java:243); 	at htsjdk.tribble.AbstractFeatureReader$ComponentMethods.isTabix(AbstractFeatureReader.java:249); 	at htsjdk.tribble.AbstractFeatureReader.getFeatureReader(AbstractFeatureReader.java:103); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getReaderFromPath(GenomicsDBImport.java:619); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getFeatureReadersSerially(GenomicsDBImport.java:602); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.traverse(GenomicsDBImport.java:490); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:893); 	at org.broadinstitute.hellbender.cmd",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4592
https://github.com/broadinstitute/gatk/issues/4592:598,Availability,error,error,598,"I had a dumb permissions issue that had me spinning my wheels for a while because of some very terse logging. With the 5 reader threads we use in production I get:; `A USER ERROR has occurred: Couldn't read file. Error was: Failure while waiting for FeatureReader to initialize with exception: com.google.cloud.storage.StorageException: All 20 retries failed. Waited a total of 1918000 ms between attempts` ; (The error message has a placeholder for a path, but for some reason it's empty for me.). Finally I went to one thread, which called the serial FeatureReader creation method and gave me an error that could actually help solve my problem:; ```; com.google.cloud.storage.StorageException: All 20 retries failed. Waited a total of 1918000 ms between attempts; 	at com.google.cloud.storage.contrib.nio.CloudStorageRetryHandler.handleRetryForStorageException(CloudStorageRetryHandler.java:108); 	at com.google.cloud.storage.contrib.nio.CloudStorageRetryHandler.handleStorageException(CloudStorageRetryHandler.java:89); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSystemProvider.java:621); 	at java.nio.file.Files.exists(Files.java:2385); 	at htsjdk.tribble.util.ParsingUtils.resourceExists(ParsingUtils.java:419); 	at htsjdk.tribble.AbstractFeatureReader.isTabix(AbstractFeatureReader.java:243); 	at htsjdk.tribble.AbstractFeatureReader$ComponentMethods.isTabix(AbstractFeatureReader.java:249); 	at htsjdk.tribble.AbstractFeatureReader.getFeatureReader(AbstractFeatureReader.java:103); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getReaderFromPath(GenomicsDBImport.java:619); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getFeatureReadersSerially(GenomicsDBImport.java:602); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.traverse(GenomicsDBImport.java:490); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:893); 	at org.broadinstitute.hellbender.cmd",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4592
https://github.com/broadinstitute/gatk/issues/4592:3667,Availability,error,errors,3667,"44139c5-3371-4a67-a2c8-e054e46f814f/ReblockGVCF/601ea396-b6cf-4baa-95c7-83e88b92c194/call-GenotypeGVCF/09C97227.c9cd4496-b4ed-4686-babd-177c66168178.vcf.gz.tbi.; 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:189); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.get(HttpStorageRpc.java:335); 	at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:191); 	at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:188); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:94); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:54); 	at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:188); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSystemProvider.java:614); 	... 15 more; Caused by: shaded.cloud_nio.com.google.api.client.googleapis.json.GoogleJsonResponseException: 403 Forbidden; {; ""code"" : 403,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""443301511749-compute@developer.gserviceaccount.com does not have storage.objects.get access to fc-50c768b1-a285-4c95-8d8c-8ce209f1fda8/744139c5-3371-4a67-a2c8-e054e46f814f/ReblockGVCF/601ea396-b6cf-4baa-95c7-83e88b92c194/call-GenotypeGVCF/09C97227.c9cd4496-b4ed-4686-babd-177c66168178.vcf.gz.tbi."",; ""reason"" : ""forbidden""; } ],; ""message"" : ""443301511749-compute@developer.gserviceaccount.com does not have storage.objects.get access to fc-50c768b1-a285-4c95-8d8c-8ce209f1fda8/744139c5-3371-4a67-a2c8-e054e46f814f/ReblockGVCF/601ea396-b6cf-4baa-95c7-83e88b92c194/call-GenotypeGVCF/09C97227.c9cd4496-b4ed-4686-babd-177c66168178.vcf.gz.tbi.""; }; 	at shaded.cloud_nio.com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); 	at shaded.cloud_nio",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4592
https://github.com/broadinstitute/gatk/issues/4592:420,Integrability,message,message,420,"I had a dumb permissions issue that had me spinning my wheels for a while because of some very terse logging. With the 5 reader threads we use in production I get:; `A USER ERROR has occurred: Couldn't read file. Error was: Failure while waiting for FeatureReader to initialize with exception: com.google.cloud.storage.StorageException: All 20 retries failed. Waited a total of 1918000 ms between attempts` ; (The error message has a placeholder for a path, but for some reason it's empty for me.). Finally I went to one thread, which called the serial FeatureReader creation method and gave me an error that could actually help solve my problem:; ```; com.google.cloud.storage.StorageException: All 20 retries failed. Waited a total of 1918000 ms between attempts; 	at com.google.cloud.storage.contrib.nio.CloudStorageRetryHandler.handleRetryForStorageException(CloudStorageRetryHandler.java:108); 	at com.google.cloud.storage.contrib.nio.CloudStorageRetryHandler.handleStorageException(CloudStorageRetryHandler.java:89); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSystemProvider.java:621); 	at java.nio.file.Files.exists(Files.java:2385); 	at htsjdk.tribble.util.ParsingUtils.resourceExists(ParsingUtils.java:419); 	at htsjdk.tribble.AbstractFeatureReader.isTabix(AbstractFeatureReader.java:243); 	at htsjdk.tribble.AbstractFeatureReader$ComponentMethods.isTabix(AbstractFeatureReader.java:249); 	at htsjdk.tribble.AbstractFeatureReader.getFeatureReader(AbstractFeatureReader.java:103); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getReaderFromPath(GenomicsDBImport.java:619); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getFeatureReadersSerially(GenomicsDBImport.java:602); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.traverse(GenomicsDBImport.java:490); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:893); 	at org.broadinstitute.hellbender.cmd",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4592
https://github.com/broadinstitute/gatk/issues/4592:3705,Integrability,message,message,3705,"44139c5-3371-4a67-a2c8-e054e46f814f/ReblockGVCF/601ea396-b6cf-4baa-95c7-83e88b92c194/call-GenotypeGVCF/09C97227.c9cd4496-b4ed-4686-babd-177c66168178.vcf.gz.tbi.; 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:189); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.get(HttpStorageRpc.java:335); 	at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:191); 	at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:188); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:94); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:54); 	at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:188); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSystemProvider.java:614); 	... 15 more; Caused by: shaded.cloud_nio.com.google.api.client.googleapis.json.GoogleJsonResponseException: 403 Forbidden; {; ""code"" : 403,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""443301511749-compute@developer.gserviceaccount.com does not have storage.objects.get access to fc-50c768b1-a285-4c95-8d8c-8ce209f1fda8/744139c5-3371-4a67-a2c8-e054e46f814f/ReblockGVCF/601ea396-b6cf-4baa-95c7-83e88b92c194/call-GenotypeGVCF/09C97227.c9cd4496-b4ed-4686-babd-177c66168178.vcf.gz.tbi."",; ""reason"" : ""forbidden""; } ],; ""message"" : ""443301511749-compute@developer.gserviceaccount.com does not have storage.objects.get access to fc-50c768b1-a285-4c95-8d8c-8ce209f1fda8/744139c5-3371-4a67-a2c8-e054e46f814f/ReblockGVCF/601ea396-b6cf-4baa-95c7-83e88b92c194/call-GenotypeGVCF/09C97227.c9cd4496-b4ed-4686-babd-177c66168178.vcf.gz.tbi.""; }; 	at shaded.cloud_nio.com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); 	at shaded.cloud_nio",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4592
https://github.com/broadinstitute/gatk/issues/4592:4048,Integrability,message,message,4048,"l(StorageImpl.java:191); 	at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:188); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:94); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:54); 	at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:188); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSystemProvider.java:614); 	... 15 more; Caused by: shaded.cloud_nio.com.google.api.client.googleapis.json.GoogleJsonResponseException: 403 Forbidden; {; ""code"" : 403,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""443301511749-compute@developer.gserviceaccount.com does not have storage.objects.get access to fc-50c768b1-a285-4c95-8d8c-8ce209f1fda8/744139c5-3371-4a67-a2c8-e054e46f814f/ReblockGVCF/601ea396-b6cf-4baa-95c7-83e88b92c194/call-GenotypeGVCF/09C97227.c9cd4496-b4ed-4686-babd-177c66168178.vcf.gz.tbi."",; ""reason"" : ""forbidden""; } ],; ""message"" : ""443301511749-compute@developer.gserviceaccount.com does not have storage.objects.get access to fc-50c768b1-a285-4c95-8d8c-8ce209f1fda8/744139c5-3371-4a67-a2c8-e054e46f814f/ReblockGVCF/601ea396-b6cf-4baa-95c7-83e88b92c194/call-GenotypeGVCF/09C97227.c9cd4496-b4ed-4686-babd-177c66168178.vcf.gz.tbi.""; }; 	at shaded.cloud_nio.com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:321); 	at shaded.cloud_nio.com.google.api.client.http.HttpRequest.execute(HttpRequest.java",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4592
https://github.com/broadinstitute/gatk/issues/4592:5646,Integrability,message,message,5646,"te@developer.gserviceaccount.com does not have storage.objects.get access to fc-50c768b1-a285-4c95-8d8c-8ce209f1fda8/744139c5-3371-4a67-a2c8-e054e46f814f/ReblockGVCF/601ea396-b6cf-4baa-95c7-83e88b92c194/call-GenotypeGVCF/09C97227.c9cd4496-b4ed-4686-babd-177c66168178.vcf.gz.tbi."",; ""reason"" : ""forbidden""; } ],; ""message"" : ""443301511749-compute@developer.gserviceaccount.com does not have storage.objects.get access to fc-50c768b1-a285-4c95-8d8c-8ce209f1fda8/744139c5-3371-4a67-a2c8-e054e46f814f/ReblockGVCF/601ea396-b6cf-4baa-95c7-83e88b92c194/call-GenotypeGVCF/09C97227.c9cd4496-b4ed-4686-babd-177c66168178.vcf.gz.tbi.""; }; 	at shaded.cloud_nio.com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:321); 	at shaded.cloud_nio.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1056); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.get(HttpStorageRpc.java:333); 	... 21 more; ```; The latter is more verbose than I need, but having that message from the 403 was key (since I needed the service account name to give it access.)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4592
https://github.com/broadinstitute/gatk/issues/4592:2625,Security,access,access,2625,"micsdb.GenomicsDBImport.getFeatureReadersSerially(GenomicsDBImport.java:602); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.traverse(GenomicsDBImport.java:490); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:893); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:136); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:153); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:195); 	at org.broadinstitute.hellbender.Main.main(Main.java:277); Caused by: com.google.cloud.storage.StorageException: 443301511749-compute@developer.gserviceaccount.com does not have storage.objects.get access to fc-50c768b1-a285-4c95-8d8c-8ce209f1fda8/744139c5-3371-4a67-a2c8-e054e46f814f/ReblockGVCF/601ea396-b6cf-4baa-95c7-83e88b92c194/call-GenotypeGVCF/09C97227.c9cd4496-b4ed-4686-babd-177c66168178.vcf.gz.tbi.; 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:189); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.get(HttpStorageRpc.java:335); 	at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:191); 	at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:188); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:94); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:54); 	at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:188); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSystemProvider.java:614); 	... 15 more; Caused by: shaded.cloud_nio.com.google.api.client.googleapis.json.GoogleJsonResponseException: 403 Forbidden; {; ""code"" : 403,; ""errors"" : [ {; ""domain"" : ""global"",; """,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4592
https://github.com/broadinstitute/gatk/issues/4592:3802,Security,access,access,3802,"rageRpc.translate(HttpStorageRpc.java:189); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.get(HttpStorageRpc.java:335); 	at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:191); 	at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:188); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:94); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:54); 	at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:188); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSystemProvider.java:614); 	... 15 more; Caused by: shaded.cloud_nio.com.google.api.client.googleapis.json.GoogleJsonResponseException: 403 Forbidden; {; ""code"" : 403,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""443301511749-compute@developer.gserviceaccount.com does not have storage.objects.get access to fc-50c768b1-a285-4c95-8d8c-8ce209f1fda8/744139c5-3371-4a67-a2c8-e054e46f814f/ReblockGVCF/601ea396-b6cf-4baa-95c7-83e88b92c194/call-GenotypeGVCF/09C97227.c9cd4496-b4ed-4686-babd-177c66168178.vcf.gz.tbi."",; ""reason"" : ""forbidden""; } ],; ""message"" : ""443301511749-compute@developer.gserviceaccount.com does not have storage.objects.get access to fc-50c768b1-a285-4c95-8d8c-8ce209f1fda8/744139c5-3371-4a67-a2c8-e054e46f814f/ReblockGVCF/601ea396-b6cf-4baa-95c7-83e88b92c194/call-GenotypeGVCF/09C97227.c9cd4496-b4ed-4686-babd-177c66168178.vcf.gz.tbi.""; }; 	at shaded.cloud_nio.com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.A",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4592
https://github.com/broadinstitute/gatk/issues/4592:4145,Security,access,access,4145,"it(DirectRetryingExecutor.java:94); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:54); 	at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:188); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSystemProvider.java:614); 	... 15 more; Caused by: shaded.cloud_nio.com.google.api.client.googleapis.json.GoogleJsonResponseException: 403 Forbidden; {; ""code"" : 403,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""443301511749-compute@developer.gserviceaccount.com does not have storage.objects.get access to fc-50c768b1-a285-4c95-8d8c-8ce209f1fda8/744139c5-3371-4a67-a2c8-e054e46f814f/ReblockGVCF/601ea396-b6cf-4baa-95c7-83e88b92c194/call-GenotypeGVCF/09C97227.c9cd4496-b4ed-4686-babd-177c66168178.vcf.gz.tbi."",; ""reason"" : ""forbidden""; } ],; ""message"" : ""443301511749-compute@developer.gserviceaccount.com does not have storage.objects.get access to fc-50c768b1-a285-4c95-8d8c-8ce209f1fda8/744139c5-3371-4a67-a2c8-e054e46f814f/ReblockGVCF/601ea396-b6cf-4baa-95c7-83e88b92c194/call-GenotypeGVCF/09C97227.c9cd4496-b4ed-4686-babd-177c66168178.vcf.gz.tbi.""; }; 	at shaded.cloud_nio.com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:321); 	at shaded.cloud_nio.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1056); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419); 	at shaded.cloud_ni",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4592
https://github.com/broadinstitute/gatk/issues/4592:5727,Security,access,access,5727,"te@developer.gserviceaccount.com does not have storage.objects.get access to fc-50c768b1-a285-4c95-8d8c-8ce209f1fda8/744139c5-3371-4a67-a2c8-e054e46f814f/ReblockGVCF/601ea396-b6cf-4baa-95c7-83e88b92c194/call-GenotypeGVCF/09C97227.c9cd4496-b4ed-4686-babd-177c66168178.vcf.gz.tbi."",; ""reason"" : ""forbidden""; } ],; ""message"" : ""443301511749-compute@developer.gserviceaccount.com does not have storage.objects.get access to fc-50c768b1-a285-4c95-8d8c-8ce209f1fda8/744139c5-3371-4a67-a2c8-e054e46f814f/ReblockGVCF/601ea396-b6cf-4baa-95c7-83e88b92c194/call-GenotypeGVCF/09C97227.c9cd4496-b4ed-4686-babd-177c66168178.vcf.gz.tbi.""; }; 	at shaded.cloud_nio.com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:321); 	at shaded.cloud_nio.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1056); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.get(HttpStorageRpc.java:333); 	... 21 more; ```; The latter is more verbose than I need, but having that message from the 403 was key (since I needed the service account name to give it access.)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4592
https://github.com/broadinstitute/gatk/issues/4592:101,Testability,log,logging,101,"I had a dumb permissions issue that had me spinning my wheels for a while because of some very terse logging. With the 5 reader threads we use in production I get:; `A USER ERROR has occurred: Couldn't read file. Error was: Failure while waiting for FeatureReader to initialize with exception: com.google.cloud.storage.StorageException: All 20 retries failed. Waited a total of 1918000 ms between attempts` ; (The error message has a placeholder for a path, but for some reason it's empty for me.). Finally I went to one thread, which called the serial FeatureReader creation method and gave me an error that could actually help solve my problem:; ```; com.google.cloud.storage.StorageException: All 20 retries failed. Waited a total of 1918000 ms between attempts; 	at com.google.cloud.storage.contrib.nio.CloudStorageRetryHandler.handleRetryForStorageException(CloudStorageRetryHandler.java:108); 	at com.google.cloud.storage.contrib.nio.CloudStorageRetryHandler.handleStorageException(CloudStorageRetryHandler.java:89); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSystemProvider.java:621); 	at java.nio.file.Files.exists(Files.java:2385); 	at htsjdk.tribble.util.ParsingUtils.resourceExists(ParsingUtils.java:419); 	at htsjdk.tribble.AbstractFeatureReader.isTabix(AbstractFeatureReader.java:243); 	at htsjdk.tribble.AbstractFeatureReader$ComponentMethods.isTabix(AbstractFeatureReader.java:249); 	at htsjdk.tribble.AbstractFeatureReader.getFeatureReader(AbstractFeatureReader.java:103); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getReaderFromPath(GenomicsDBImport.java:619); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getFeatureReadersSerially(GenomicsDBImport.java:602); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.traverse(GenomicsDBImport.java:490); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:893); 	at org.broadinstitute.hellbender.cmd",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4592
https://github.com/broadinstitute/gatk/pull/4593:21,Deployability,release,release,21,Fixes #4357. Need to release new copy of data sources along with this.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4593
https://github.com/broadinstitute/gatk/issues/4597:77,Security,access,access,77,"SimpleInterval has a constructor that parses an interval String, but without access to a SequenceDictionary its not possible to correctly interpret intervals with contig names such as those used in hg38. It looks like the only non-test consumer of this method is TableCodec. For example:. - `HLA-A*01:01:01:01` is interpreted as `HLA-A*01:01:01:1-1`, but `HLA-A*01:01:01` doesn't exist; - `HLA-A*01:01:01:02N` its interpreted as position `02N` on contig `HLA-A*01:01:01`, which fails to parse, and the contig doesn't exist. GATK command line intervals resolve these by consulting the sequence dictionary. For hg38 at least, there can be no ambiguity and there is always only one correct interpretation. Its possible to construct a legal sequence dictionary that has ambiguities though.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4597
https://github.com/broadinstitute/gatk/issues/4597:231,Testability,test,test,231,"SimpleInterval has a constructor that parses an interval String, but without access to a SequenceDictionary its not possible to correctly interpret intervals with contig names such as those used in hg38. It looks like the only non-test consumer of this method is TableCodec. For example:. - `HLA-A*01:01:01:01` is interpreted as `HLA-A*01:01:01:1-1`, but `HLA-A*01:01:01` doesn't exist; - `HLA-A*01:01:01:02N` its interpreted as position `02N` on contig `HLA-A*01:01:01`, which fails to parse, and the contig doesn't exist. GATK command line intervals resolve these by consulting the sequence dictionary. For hg38 at least, there can be no ambiguity and there is always only one correct interpretation. Its possible to construct a legal sequence dictionary that has ambiguities though.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4597
https://github.com/broadinstitute/gatk/issues/4597:0,Usability,Simpl,SimpleInterval,0,"SimpleInterval has a constructor that parses an interval String, but without access to a SequenceDictionary its not possible to correctly interpret intervals with contig names such as those used in hg38. It looks like the only non-test consumer of this method is TableCodec. For example:. - `HLA-A*01:01:01:01` is interpreted as `HLA-A*01:01:01:1-1`, but `HLA-A*01:01:01` doesn't exist; - `HLA-A*01:01:01:02N` its interpreted as position `02N` on contig `HLA-A*01:01:01`, which fails to parse, and the contig doesn't exist. GATK command line intervals resolve these by consulting the sequence dictionary. For hg38 at least, there can be no ambiguity and there is always only one correct interpretation. Its possible to construct a legal sequence dictionary that has ambiguities though.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4597
https://github.com/broadinstitute/gatk/issues/4598:40,Energy Efficiency,efficient,efficiently,40,"Provide @eitanbanks with a WDL that can efficiently run `SortSamSpark` on a normal-size (~100GB genome) on a single large compute instance. It should make whatever resource requests are necessary, including multiple local SSDs stitched together into a RAID or virtual filesystem, sufficient memory, and use an appropriate number of cores to balance memory usage against performance. For @jamesemery and @lbergelson",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4598
https://github.com/broadinstitute/gatk/issues/4598:370,Performance,perform,performance,370,"Provide @eitanbanks with a WDL that can efficiently run `SortSamSpark` on a normal-size (~100GB genome) on a single large compute instance. It should make whatever resource requests are necessary, including multiple local SSDs stitched together into a RAID or virtual filesystem, sufficient memory, and use an appropriate number of cores to balance memory usage against performance. For @jamesemery and @lbergelson",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4598
https://github.com/broadinstitute/gatk/pull/4601:98,Security,validat,validation,98,@takutosato Can you review this PR?. This is a community request and a useful feature for our MC3 validation.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4601
https://github.com/broadinstitute/gatk/pull/4602:4855,Energy Efficiency,green,green,4855,"AAGAACACATAGATGCATTTGGAAGCCAGTGTGGACGCCATGTGATCTGTGCCCACATATCACATGGCCGCTTTGGGATAGGGCCTGTCTGCCCATACTGGCTTCCAAACGCCTCTGTGTGTTCCTGTATGTGGGTGTGCACGTACCTGTCACATGTGTATGCACAGACCACAGGATGTCCACACTGGCTTCCAAATGCGTCTCTGTGTTCCTGTCTGTGAGTTCCAAATGTGTGCACACCTACAGACAGGAACATGGAAACACATTTGGAAGCCAGTGTGGACACCCTGTGATCTGTGCGTACACATGTGACACGTGCATGCACACCCACAGACAGGAACACAGAGACACATTTGGAAGCCAGTGTGGACGCCCTGTGATCTGTGCCCACACACATCACACGTGCATACACACCCACAGACAGGAACACAGAGACACATTTGGAAGCCAGTGTGGATGCCCTGTGATCTGTGTGTACACGTGACACGTGCGTACACACCCACATACAGGAACACAGCCACATTTGGAAGCCAGTGCAGACGCCCTGTGATCTGTGTGTACACATGTGACACGTGCGTGCACACTCACAGACAGGAACACAGAGACGCATTTGGAAGCCAGTGTGGACATCCTGTGGTCTGCGCGTACACATGTGACAGGTACGTGCACGCCCACATACAGGAACACACAGAGGCCTTTGGAAGCCAGCATGGGCAGACAGGCCCTATCCCAAAGCGGCC;SVLEN=1454;SVTYPE=CPX;TOTAL_MAPPINGS=1; ```. So the strategy taken in this branch is; * for the first two cases, re-interpretation is easy and done in this ""post-processing"" tool, and bare-bone annotated simple variants are given , annotated with `EVENT` that links the simple variants back to the complex variant; * for the last case, ; * re-collect the contigs that induced the CPX call, preprocess its alignment, then send the contig to the current pair-iteration algorithm for re-interpretation, the returned simple variants will be checked for consistency with the CPX variant that was induced by the same contig, and dropped if it is inconsistent (the two types of variants `<DEL>` and `<INV>`, are main concerns as they could easily stem from mis-interpretations of small dispersed duplications); then, ; * the CPX variants who have rejected re-interpreted simple variants will be analyzed one last time, to extract `<DEL>` and `<INV>`; ; * these variants will also be annotated with `EVENT` to link back to the CPX variants. Based on manual review, this salvages ~600 variants that would be dropped by evaluation scripts that would simply ignore the CPX variants. ---; Tests will be added if this strategy is given the green light (so no merging yet).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4602
https://github.com/broadinstitute/gatk/pull/4602:551,Integrability,depend,depending,551,"A not-so-elegant way to tackle #4323, and part of #4111 . __UPDATE__: fixes #4323 . ---; Brief explanation:. The `<CPX>` variants we currently output has an annotation `SEGMENTS`, which could contain. * 0-entries (which will be simply omitted): this is when the head and tail alignments seamlessly stitch together on the reference, and middle alignments are all taken as inserted sequence; * 1-entry: this is when head/tail alignment overlap on reference over the region specified in the entry, hence we have a deletion or duplication of that region (depending on if the segment is present in another annotation `ALT_ARRANGEMENT`, or if present, whether it is inverted), and insertion of more sequences; * multiple entries: there are the truly complex ones. while the first two cases are easy to deal with, the last one is very difficult to parse into simple variants, and has the inherent evil of ambiguity in representations (to demonstrate, a not very complicated one could be like this); ```; chr6	166997615	CPX_chr6:166997615-166997944	ACCCACAGACAGAAACACAGAGACATGTTTGGAAGCCAGTGTGGATGCCCTGTGATCTGTGTGTACACATGACAAGTGCATACACACGCACATAAAGGAACCCAGAGACGTGTTTGGAAGCCAGTGTGGACACCCTGTGATCTGTGCGTACACATTTGACACCTGCGTACACACTCACAGACAGAAACACAGAGATGTGTTTGGAAGCCAGTGTGGACATCCTGTGGTCTGCGCGTACACATGTGACAGGTACGTGCACGCCCACATACAGGAACACACAGAGGCCTTTGGAAGCCAGCATGGGCAGACAGGCCCTATCCCAAAGCGGCC	<CPX>	.	.	ALIGN_LENGTHS=309;ALT_ARRANGEMENT=1,2,3,4,5,UINS-733,2,UINS-94,-6,-5,UINS-41,4,5,UINS-40,1,2,3,4,5,6;CTG_GOOD_NONCANONICAL_MAPPING=chrUn_JTFH01000473v1_decoy,1,-,51H1640M204H,60,0,1640;CTG_NAMES=asm011602:tig00001;END=166997944;HQ_MAPPINGS=1;MAPPING_QUALITIES=60;MAX_ALIGN_LENGTH=309;SEGMENTS=chr6:166997615-166997617,chr6:166997617-166997679,chr6:166997679-166997727,chr6:166997727-166997787,chr6:166997787-166997831,chr6:166997832-166997944;SEQ_ALT_HAPLOTYPE=ACCCACAGACAGAAACACAGAGACATGTTTGGAAGCCAGTGTGGATGCCCTGTGATCTGTGTGTACACATGACAAGTGCATACACACGCACATAAAGGAACCCAGAGACGTGTTTGGAAGCCAGTGTGGACACCCTGTGATCTGTGCGTACACATTTG",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4602
https://github.com/broadinstitute/gatk/pull/4602:4805,Testability,Test,Tests,4805,"AAGAACACATAGATGCATTTGGAAGCCAGTGTGGACGCCATGTGATCTGTGCCCACATATCACATGGCCGCTTTGGGATAGGGCCTGTCTGCCCATACTGGCTTCCAAACGCCTCTGTGTGTTCCTGTATGTGGGTGTGCACGTACCTGTCACATGTGTATGCACAGACCACAGGATGTCCACACTGGCTTCCAAATGCGTCTCTGTGTTCCTGTCTGTGAGTTCCAAATGTGTGCACACCTACAGACAGGAACATGGAAACACATTTGGAAGCCAGTGTGGACACCCTGTGATCTGTGCGTACACATGTGACACGTGCATGCACACCCACAGACAGGAACACAGAGACACATTTGGAAGCCAGTGTGGACGCCCTGTGATCTGTGCCCACACACATCACACGTGCATACACACCCACAGACAGGAACACAGAGACACATTTGGAAGCCAGTGTGGATGCCCTGTGATCTGTGTGTACACGTGACACGTGCGTACACACCCACATACAGGAACACAGCCACATTTGGAAGCCAGTGCAGACGCCCTGTGATCTGTGTGTACACATGTGACACGTGCGTGCACACTCACAGACAGGAACACAGAGACGCATTTGGAAGCCAGTGTGGACATCCTGTGGTCTGCGCGTACACATGTGACAGGTACGTGCACGCCCACATACAGGAACACACAGAGGCCTTTGGAAGCCAGCATGGGCAGACAGGCCCTATCCCAAAGCGGCC;SVLEN=1454;SVTYPE=CPX;TOTAL_MAPPINGS=1; ```. So the strategy taken in this branch is; * for the first two cases, re-interpretation is easy and done in this ""post-processing"" tool, and bare-bone annotated simple variants are given , annotated with `EVENT` that links the simple variants back to the complex variant; * for the last case, ; * re-collect the contigs that induced the CPX call, preprocess its alignment, then send the contig to the current pair-iteration algorithm for re-interpretation, the returned simple variants will be checked for consistency with the CPX variant that was induced by the same contig, and dropped if it is inconsistent (the two types of variants `<DEL>` and `<INV>`, are main concerns as they could easily stem from mis-interpretations of small dispersed duplications); then, ; * the CPX variants who have rejected re-interpreted simple variants will be analyzed one last time, to extract `<DEL>` and `<INV>`; ; * these variants will also be annotated with `EVENT` to link back to the CPX variants. Based on manual review, this salvages ~600 variants that would be dropped by evaluation scripts that would simply ignore the CPX variants. ---; Tests will be added if this strategy is given the green light (so no merging yet).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4602
https://github.com/broadinstitute/gatk/pull/4602:228,Usability,simpl,simply,228,"A not-so-elegant way to tackle #4323, and part of #4111 . __UPDATE__: fixes #4323 . ---; Brief explanation:. The `<CPX>` variants we currently output has an annotation `SEGMENTS`, which could contain. * 0-entries (which will be simply omitted): this is when the head and tail alignments seamlessly stitch together on the reference, and middle alignments are all taken as inserted sequence; * 1-entry: this is when head/tail alignment overlap on reference over the region specified in the entry, hence we have a deletion or duplication of that region (depending on if the segment is present in another annotation `ALT_ARRANGEMENT`, or if present, whether it is inverted), and insertion of more sequences; * multiple entries: there are the truly complex ones. while the first two cases are easy to deal with, the last one is very difficult to parse into simple variants, and has the inherent evil of ambiguity in representations (to demonstrate, a not very complicated one could be like this); ```; chr6	166997615	CPX_chr6:166997615-166997944	ACCCACAGACAGAAACACAGAGACATGTTTGGAAGCCAGTGTGGATGCCCTGTGATCTGTGTGTACACATGACAAGTGCATACACACGCACATAAAGGAACCCAGAGACGTGTTTGGAAGCCAGTGTGGACACCCTGTGATCTGTGCGTACACATTTGACACCTGCGTACACACTCACAGACAGAAACACAGAGATGTGTTTGGAAGCCAGTGTGGACATCCTGTGGTCTGCGCGTACACATGTGACAGGTACGTGCACGCCCACATACAGGAACACACAGAGGCCTTTGGAAGCCAGCATGGGCAGACAGGCCCTATCCCAAAGCGGCC	<CPX>	.	.	ALIGN_LENGTHS=309;ALT_ARRANGEMENT=1,2,3,4,5,UINS-733,2,UINS-94,-6,-5,UINS-41,4,5,UINS-40,1,2,3,4,5,6;CTG_GOOD_NONCANONICAL_MAPPING=chrUn_JTFH01000473v1_decoy,1,-,51H1640M204H,60,0,1640;CTG_NAMES=asm011602:tig00001;END=166997944;HQ_MAPPINGS=1;MAPPING_QUALITIES=60;MAX_ALIGN_LENGTH=309;SEGMENTS=chr6:166997615-166997617,chr6:166997617-166997679,chr6:166997679-166997727,chr6:166997727-166997787,chr6:166997787-166997831,chr6:166997832-166997944;SEQ_ALT_HAPLOTYPE=ACCCACAGACAGAAACACAGAGACATGTTTGGAAGCCAGTGTGGATGCCCTGTGATCTGTGTGTACACATGACAAGTGCATACACACGCACATAAAGGAACCCAGAGACGTGTTTGGAAGCCAGTGTGGACACCCTGTGATCTGTGCGTACACATTTG",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4602
https://github.com/broadinstitute/gatk/pull/4602:852,Usability,simpl,simple,852,": fixes #4323 . ---; Brief explanation:. The `<CPX>` variants we currently output has an annotation `SEGMENTS`, which could contain. * 0-entries (which will be simply omitted): this is when the head and tail alignments seamlessly stitch together on the reference, and middle alignments are all taken as inserted sequence; * 1-entry: this is when head/tail alignment overlap on reference over the region specified in the entry, hence we have a deletion or duplication of that region (depending on if the segment is present in another annotation `ALT_ARRANGEMENT`, or if present, whether it is inverted), and insertion of more sequences; * multiple entries: there are the truly complex ones. while the first two cases are easy to deal with, the last one is very difficult to parse into simple variants, and has the inherent evil of ambiguity in representations (to demonstrate, a not very complicated one could be like this); ```; chr6	166997615	CPX_chr6:166997615-166997944	ACCCACAGACAGAAACACAGAGACATGTTTGGAAGCCAGTGTGGATGCCCTGTGATCTGTGTGTACACATGACAAGTGCATACACACGCACATAAAGGAACCCAGAGACGTGTTTGGAAGCCAGTGTGGACACCCTGTGATCTGTGCGTACACATTTGACACCTGCGTACACACTCACAGACAGAAACACAGAGATGTGTTTGGAAGCCAGTGTGGACATCCTGTGGTCTGCGCGTACACATGTGACAGGTACGTGCACGCCCACATACAGGAACACACAGAGGCCTTTGGAAGCCAGCATGGGCAGACAGGCCCTATCCCAAAGCGGCC	<CPX>	.	.	ALIGN_LENGTHS=309;ALT_ARRANGEMENT=1,2,3,4,5,UINS-733,2,UINS-94,-6,-5,UINS-41,4,5,UINS-40,1,2,3,4,5,6;CTG_GOOD_NONCANONICAL_MAPPING=chrUn_JTFH01000473v1_decoy,1,-,51H1640M204H,60,0,1640;CTG_NAMES=asm011602:tig00001;END=166997944;HQ_MAPPINGS=1;MAPPING_QUALITIES=60;MAX_ALIGN_LENGTH=309;SEGMENTS=chr6:166997615-166997617,chr6:166997617-166997679,chr6:166997679-166997727,chr6:166997727-166997787,chr6:166997787-166997831,chr6:166997832-166997944;SEQ_ALT_HAPLOTYPE=ACCCACAGACAGAAACACAGAGACATGTTTGGAAGCCAGTGTGGATGCCCTGTGATCTGTGTGTACACATGACAAGTGCATACACACGCACATAAAGGAACCCAGAGACGTGTTTGGAAGCCAGTGTGGACACCCTGTGATCTGTGCGTACACATTTGACACCTGCGTACACACTCACAGACAGAAACACAGAGATGTGTTTGGAAGCCAGCGTGGATGCCCTGT",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4602
https://github.com/broadinstitute/gatk/pull/4602:3832,Usability,simpl,simple,3832,"AAGAACACATAGATGCATTTGGAAGCCAGTGTGGACGCCATGTGATCTGTGCCCACATATCACATGGCCGCTTTGGGATAGGGCCTGTCTGCCCATACTGGCTTCCAAACGCCTCTGTGTGTTCCTGTATGTGGGTGTGCACGTACCTGTCACATGTGTATGCACAGACCACAGGATGTCCACACTGGCTTCCAAATGCGTCTCTGTGTTCCTGTCTGTGAGTTCCAAATGTGTGCACACCTACAGACAGGAACATGGAAACACATTTGGAAGCCAGTGTGGACACCCTGTGATCTGTGCGTACACATGTGACACGTGCATGCACACCCACAGACAGGAACACAGAGACACATTTGGAAGCCAGTGTGGACGCCCTGTGATCTGTGCCCACACACATCACACGTGCATACACACCCACAGACAGGAACACAGAGACACATTTGGAAGCCAGTGTGGATGCCCTGTGATCTGTGTGTACACGTGACACGTGCGTACACACCCACATACAGGAACACAGCCACATTTGGAAGCCAGTGCAGACGCCCTGTGATCTGTGTGTACACATGTGACACGTGCGTGCACACTCACAGACAGGAACACAGAGACGCATTTGGAAGCCAGTGTGGACATCCTGTGGTCTGCGCGTACACATGTGACAGGTACGTGCACGCCCACATACAGGAACACACAGAGGCCTTTGGAAGCCAGCATGGGCAGACAGGCCCTATCCCAAAGCGGCC;SVLEN=1454;SVTYPE=CPX;TOTAL_MAPPINGS=1; ```. So the strategy taken in this branch is; * for the first two cases, re-interpretation is easy and done in this ""post-processing"" tool, and bare-bone annotated simple variants are given , annotated with `EVENT` that links the simple variants back to the complex variant; * for the last case, ; * re-collect the contigs that induced the CPX call, preprocess its alignment, then send the contig to the current pair-iteration algorithm for re-interpretation, the returned simple variants will be checked for consistency with the CPX variant that was induced by the same contig, and dropped if it is inconsistent (the two types of variants `<DEL>` and `<INV>`, are main concerns as they could easily stem from mis-interpretations of small dispersed duplications); then, ; * the CPX variants who have rejected re-interpreted simple variants will be analyzed one last time, to extract `<DEL>` and `<INV>`; ; * these variants will also be annotated with `EVENT` to link back to the CPX variants. Based on manual review, this salvages ~600 variants that would be dropped by evaluation scripts that would simply ignore the CPX variants. ---; Tests will be added if this strategy is given the green light (so no merging yet).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4602
https://github.com/broadinstitute/gatk/pull/4602:3898,Usability,simpl,simple,3898,"AAGAACACATAGATGCATTTGGAAGCCAGTGTGGACGCCATGTGATCTGTGCCCACATATCACATGGCCGCTTTGGGATAGGGCCTGTCTGCCCATACTGGCTTCCAAACGCCTCTGTGTGTTCCTGTATGTGGGTGTGCACGTACCTGTCACATGTGTATGCACAGACCACAGGATGTCCACACTGGCTTCCAAATGCGTCTCTGTGTTCCTGTCTGTGAGTTCCAAATGTGTGCACACCTACAGACAGGAACATGGAAACACATTTGGAAGCCAGTGTGGACACCCTGTGATCTGTGCGTACACATGTGACACGTGCATGCACACCCACAGACAGGAACACAGAGACACATTTGGAAGCCAGTGTGGACGCCCTGTGATCTGTGCCCACACACATCACACGTGCATACACACCCACAGACAGGAACACAGAGACACATTTGGAAGCCAGTGTGGATGCCCTGTGATCTGTGTGTACACGTGACACGTGCGTACACACCCACATACAGGAACACAGCCACATTTGGAAGCCAGTGCAGACGCCCTGTGATCTGTGTGTACACATGTGACACGTGCGTGCACACTCACAGACAGGAACACAGAGACGCATTTGGAAGCCAGTGTGGACATCCTGTGGTCTGCGCGTACACATGTGACAGGTACGTGCACGCCCACATACAGGAACACACAGAGGCCTTTGGAAGCCAGCATGGGCAGACAGGCCCTATCCCAAAGCGGCC;SVLEN=1454;SVTYPE=CPX;TOTAL_MAPPINGS=1; ```. So the strategy taken in this branch is; * for the first two cases, re-interpretation is easy and done in this ""post-processing"" tool, and bare-bone annotated simple variants are given , annotated with `EVENT` that links the simple variants back to the complex variant; * for the last case, ; * re-collect the contigs that induced the CPX call, preprocess its alignment, then send the contig to the current pair-iteration algorithm for re-interpretation, the returned simple variants will be checked for consistency with the CPX variant that was induced by the same contig, and dropped if it is inconsistent (the two types of variants `<DEL>` and `<INV>`, are main concerns as they could easily stem from mis-interpretations of small dispersed duplications); then, ; * the CPX variants who have rejected re-interpreted simple variants will be analyzed one last time, to extract `<DEL>` and `<INV>`; ; * these variants will also be annotated with `EVENT` to link back to the CPX variants. Based on manual review, this salvages ~600 variants that would be dropped by evaluation scripts that would simply ignore the CPX variants. ---; Tests will be added if this strategy is given the green light (so no merging yet).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4602
https://github.com/broadinstitute/gatk/pull/4602:4141,Usability,simpl,simple,4141,"AAGAACACATAGATGCATTTGGAAGCCAGTGTGGACGCCATGTGATCTGTGCCCACATATCACATGGCCGCTTTGGGATAGGGCCTGTCTGCCCATACTGGCTTCCAAACGCCTCTGTGTGTTCCTGTATGTGGGTGTGCACGTACCTGTCACATGTGTATGCACAGACCACAGGATGTCCACACTGGCTTCCAAATGCGTCTCTGTGTTCCTGTCTGTGAGTTCCAAATGTGTGCACACCTACAGACAGGAACATGGAAACACATTTGGAAGCCAGTGTGGACACCCTGTGATCTGTGCGTACACATGTGACACGTGCATGCACACCCACAGACAGGAACACAGAGACACATTTGGAAGCCAGTGTGGACGCCCTGTGATCTGTGCCCACACACATCACACGTGCATACACACCCACAGACAGGAACACAGAGACACATTTGGAAGCCAGTGTGGATGCCCTGTGATCTGTGTGTACACGTGACACGTGCGTACACACCCACATACAGGAACACAGCCACATTTGGAAGCCAGTGCAGACGCCCTGTGATCTGTGTGTACACATGTGACACGTGCGTGCACACTCACAGACAGGAACACAGAGACGCATTTGGAAGCCAGTGTGGACATCCTGTGGTCTGCGCGTACACATGTGACAGGTACGTGCACGCCCACATACAGGAACACACAGAGGCCTTTGGAAGCCAGCATGGGCAGACAGGCCCTATCCCAAAGCGGCC;SVLEN=1454;SVTYPE=CPX;TOTAL_MAPPINGS=1; ```. So the strategy taken in this branch is; * for the first two cases, re-interpretation is easy and done in this ""post-processing"" tool, and bare-bone annotated simple variants are given , annotated with `EVENT` that links the simple variants back to the complex variant; * for the last case, ; * re-collect the contigs that induced the CPX call, preprocess its alignment, then send the contig to the current pair-iteration algorithm for re-interpretation, the returned simple variants will be checked for consistency with the CPX variant that was induced by the same contig, and dropped if it is inconsistent (the two types of variants `<DEL>` and `<INV>`, are main concerns as they could easily stem from mis-interpretations of small dispersed duplications); then, ; * the CPX variants who have rejected re-interpreted simple variants will be analyzed one last time, to extract `<DEL>` and `<INV>`; ; * these variants will also be annotated with `EVENT` to link back to the CPX variants. Based on manual review, this salvages ~600 variants that would be dropped by evaluation scripts that would simply ignore the CPX variants. ---; Tests will be added if this strategy is given the green light (so no merging yet).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4602
https://github.com/broadinstitute/gatk/pull/4602:4492,Usability,simpl,simple,4492,"AAGAACACATAGATGCATTTGGAAGCCAGTGTGGACGCCATGTGATCTGTGCCCACATATCACATGGCCGCTTTGGGATAGGGCCTGTCTGCCCATACTGGCTTCCAAACGCCTCTGTGTGTTCCTGTATGTGGGTGTGCACGTACCTGTCACATGTGTATGCACAGACCACAGGATGTCCACACTGGCTTCCAAATGCGTCTCTGTGTTCCTGTCTGTGAGTTCCAAATGTGTGCACACCTACAGACAGGAACATGGAAACACATTTGGAAGCCAGTGTGGACACCCTGTGATCTGTGCGTACACATGTGACACGTGCATGCACACCCACAGACAGGAACACAGAGACACATTTGGAAGCCAGTGTGGACGCCCTGTGATCTGTGCCCACACACATCACACGTGCATACACACCCACAGACAGGAACACAGAGACACATTTGGAAGCCAGTGTGGATGCCCTGTGATCTGTGTGTACACGTGACACGTGCGTACACACCCACATACAGGAACACAGCCACATTTGGAAGCCAGTGCAGACGCCCTGTGATCTGTGTGTACACATGTGACACGTGCGTGCACACTCACAGACAGGAACACAGAGACGCATTTGGAAGCCAGTGTGGACATCCTGTGGTCTGCGCGTACACATGTGACAGGTACGTGCACGCCCACATACAGGAACACACAGAGGCCTTTGGAAGCCAGCATGGGCAGACAGGCCCTATCCCAAAGCGGCC;SVLEN=1454;SVTYPE=CPX;TOTAL_MAPPINGS=1; ```. So the strategy taken in this branch is; * for the first two cases, re-interpretation is easy and done in this ""post-processing"" tool, and bare-bone annotated simple variants are given , annotated with `EVENT` that links the simple variants back to the complex variant; * for the last case, ; * re-collect the contigs that induced the CPX call, preprocess its alignment, then send the contig to the current pair-iteration algorithm for re-interpretation, the returned simple variants will be checked for consistency with the CPX variant that was induced by the same contig, and dropped if it is inconsistent (the two types of variants `<DEL>` and `<INV>`, are main concerns as they could easily stem from mis-interpretations of small dispersed duplications); then, ; * the CPX variants who have rejected re-interpreted simple variants will be analyzed one last time, to extract `<DEL>` and `<INV>`; ; * these variants will also be annotated with `EVENT` to link back to the CPX variants. Based on manual review, this salvages ~600 variants that would be dropped by evaluation scripts that would simply ignore the CPX variants. ---; Tests will be added if this strategy is given the green light (so no merging yet).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4602
https://github.com/broadinstitute/gatk/pull/4602:4768,Usability,simpl,simply,4768,"AAGAACACATAGATGCATTTGGAAGCCAGTGTGGACGCCATGTGATCTGTGCCCACATATCACATGGCCGCTTTGGGATAGGGCCTGTCTGCCCATACTGGCTTCCAAACGCCTCTGTGTGTTCCTGTATGTGGGTGTGCACGTACCTGTCACATGTGTATGCACAGACCACAGGATGTCCACACTGGCTTCCAAATGCGTCTCTGTGTTCCTGTCTGTGAGTTCCAAATGTGTGCACACCTACAGACAGGAACATGGAAACACATTTGGAAGCCAGTGTGGACACCCTGTGATCTGTGCGTACACATGTGACACGTGCATGCACACCCACAGACAGGAACACAGAGACACATTTGGAAGCCAGTGTGGACGCCCTGTGATCTGTGCCCACACACATCACACGTGCATACACACCCACAGACAGGAACACAGAGACACATTTGGAAGCCAGTGTGGATGCCCTGTGATCTGTGTGTACACGTGACACGTGCGTACACACCCACATACAGGAACACAGCCACATTTGGAAGCCAGTGCAGACGCCCTGTGATCTGTGTGTACACATGTGACACGTGCGTGCACACTCACAGACAGGAACACAGAGACGCATTTGGAAGCCAGTGTGGACATCCTGTGGTCTGCGCGTACACATGTGACAGGTACGTGCACGCCCACATACAGGAACACACAGAGGCCTTTGGAAGCCAGCATGGGCAGACAGGCCCTATCCCAAAGCGGCC;SVLEN=1454;SVTYPE=CPX;TOTAL_MAPPINGS=1; ```. So the strategy taken in this branch is; * for the first two cases, re-interpretation is easy and done in this ""post-processing"" tool, and bare-bone annotated simple variants are given , annotated with `EVENT` that links the simple variants back to the complex variant; * for the last case, ; * re-collect the contigs that induced the CPX call, preprocess its alignment, then send the contig to the current pair-iteration algorithm for re-interpretation, the returned simple variants will be checked for consistency with the CPX variant that was induced by the same contig, and dropped if it is inconsistent (the two types of variants `<DEL>` and `<INV>`, are main concerns as they could easily stem from mis-interpretations of small dispersed duplications); then, ; * the CPX variants who have rejected re-interpreted simple variants will be analyzed one last time, to extract `<DEL>` and `<INV>`; ; * these variants will also be annotated with `EVENT` to link back to the CPX variants. Based on manual review, this salvages ~600 variants that would be dropped by evaluation scripts that would simply ignore the CPX variants. ---; Tests will be added if this strategy is given the green light (so no merging yet).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4602
https://github.com/broadinstitute/gatk/issues/4603:433,Deployability,release,released,433,"I would like to start a new project to extend the engine of GATK (mostly walker types, e.g. https://github.com/broadinstitute/gatk/issues/1198 and common utilities), and thus I require to have an idea of how the versioning scheme is related to the public API if at all. This will allow me to say that the extensions works with GATK between 4.0.0.0 and 4.1.0.0, for example, and to know when some work is required to move to the next released version. Thank you!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4603
https://github.com/broadinstitute/gatk/issues/4603:39,Modifiability,extend,extend,39,"I would like to start a new project to extend the engine of GATK (mostly walker types, e.g. https://github.com/broadinstitute/gatk/issues/1198 and common utilities), and thus I require to have an idea of how the versioning scheme is related to the public API if at all. This will allow me to say that the extensions works with GATK between 4.0.0.0 and 4.1.0.0, for example, and to know when some work is required to move to the next released version. Thank you!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4603
https://github.com/broadinstitute/gatk/issues/4604:184,Availability,ERROR,ERROR,184,"Passing a .ped file to the VariantAnnotator tool does not work:. > gatk4 VariantAnnotator **-ped** my.ped --annotation PossibleDeNovo --variant my.vcf --output out.vcf. Result: A USER ERROR has occurred: **p** is not a recognized option. > gatk4 VariantAnnotator **--ped** my.ped --annotation PossibleDeNovo --variant my.vcf --output out.vcf. Result: A USER ERROR has occurred: **ped** is not a recognized option. Above, ""gatk4"" is an alias of ""/Library/Java/JavaVirtualMachines/jdk1.8.0_162.jdk/Contents/Home/bin/java -jar ~/lib/gatk/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar""",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4604
https://github.com/broadinstitute/gatk/issues/4604:358,Availability,ERROR,ERROR,358,"Passing a .ped file to the VariantAnnotator tool does not work:. > gatk4 VariantAnnotator **-ped** my.ped --annotation PossibleDeNovo --variant my.vcf --output out.vcf. Result: A USER ERROR has occurred: **p** is not a recognized option. > gatk4 VariantAnnotator **--ped** my.ped --annotation PossibleDeNovo --variant my.vcf --output out.vcf. Result: A USER ERROR has occurred: **ped** is not a recognized option. Above, ""gatk4"" is an alias of ""/Library/Java/JavaVirtualMachines/jdk1.8.0_162.jdk/Contents/Home/bin/java -jar ~/lib/gatk/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar""",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4604
https://github.com/broadinstitute/gatk/issues/4610:66,Availability,avail,available,66,"For the various GATK docker images, the larger `4.x.y.z` tags are available in GCR but the smaller `gatkbase-a.b.c` tags are only available in [docker hub](https://hub.docker.com/r/broadinstitute/gatk/tags/). ```shell; $ gcloud container images list-tags us.gcr.io/broad-gatk/gatk; DIGEST TAGS TIMESTAMP; 1c870d8bea16 4.0.3.0,latest 2018-03-28T05:12:42; fd8e7a9e65e6 4.0.2.1 2018-03-03T03:52:42; 899b5d55c45b 4.0.2.0 2018-02-27T11:57:49; 14b4dd387cf6 4.0.1.2 2018-02-10T03:48:55; 98b2f223dce4 4.0.1.1 2018-02-02T05:45:05; 634e2aaa5565 4.0.1.0 2018-02-01T00:13:47; 277658fad225 4.0.0.0 2018-01-10T03:16:45; 460c6b71b019 4.beta.6 2017-10-17T05:14:55; 98ed56704964 4.beta.5 2017-09-07T02:47:16; 12b65b9fe0bf 4.beta.4 2017-08-26T11:03:39; fa4d2b05e376 4.beta.3 2017-07-26T23:55:37; $ ; ```. The smaller ""base"" images are 1/5th of the size of the larger images while still containing a number of useful utilities for GATK based workflows.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4610
https://github.com/broadinstitute/gatk/issues/4610:130,Availability,avail,available,130,"For the various GATK docker images, the larger `4.x.y.z` tags are available in GCR but the smaller `gatkbase-a.b.c` tags are only available in [docker hub](https://hub.docker.com/r/broadinstitute/gatk/tags/). ```shell; $ gcloud container images list-tags us.gcr.io/broad-gatk/gatk; DIGEST TAGS TIMESTAMP; 1c870d8bea16 4.0.3.0,latest 2018-03-28T05:12:42; fd8e7a9e65e6 4.0.2.1 2018-03-03T03:52:42; 899b5d55c45b 4.0.2.0 2018-02-27T11:57:49; 14b4dd387cf6 4.0.1.2 2018-02-10T03:48:55; 98b2f223dce4 4.0.1.1 2018-02-02T05:45:05; 634e2aaa5565 4.0.1.0 2018-02-01T00:13:47; 277658fad225 4.0.0.0 2018-01-10T03:16:45; 460c6b71b019 4.beta.6 2017-10-17T05:14:55; 98ed56704964 4.beta.5 2017-09-07T02:47:16; 12b65b9fe0bf 4.beta.4 2017-08-26T11:03:39; fa4d2b05e376 4.beta.3 2017-07-26T23:55:37; $ ; ```. The smaller ""base"" images are 1/5th of the size of the larger images while still containing a number of useful utilities for GATK based workflows.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4610
https://github.com/broadinstitute/gatk/pull/4611:13,Deployability,Configurat,Configuration,13,"Includes:. * Configuration for packages to search read filters; * Configuration for packages to search annotations. In addition, it changes the behavior of `VariantAnnotatorEngine` to use the annotation packages from the configuration, and mimic what the plugin is doing. This closes https://github.com/broadinstitute/gatk/issues/2155",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4611
https://github.com/broadinstitute/gatk/pull/4611:66,Deployability,Configurat,Configuration,66,"Includes:. * Configuration for packages to search read filters; * Configuration for packages to search annotations. In addition, it changes the behavior of `VariantAnnotatorEngine` to use the annotation packages from the configuration, and mimic what the plugin is doing. This closes https://github.com/broadinstitute/gatk/issues/2155",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4611
https://github.com/broadinstitute/gatk/pull/4611:221,Deployability,configurat,configuration,221,"Includes:. * Configuration for packages to search read filters; * Configuration for packages to search annotations. In addition, it changes the behavior of `VariantAnnotatorEngine` to use the annotation packages from the configuration, and mimic what the plugin is doing. This closes https://github.com/broadinstitute/gatk/issues/2155",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4611
https://github.com/broadinstitute/gatk/pull/4611:13,Modifiability,Config,Configuration,13,"Includes:. * Configuration for packages to search read filters; * Configuration for packages to search annotations. In addition, it changes the behavior of `VariantAnnotatorEngine` to use the annotation packages from the configuration, and mimic what the plugin is doing. This closes https://github.com/broadinstitute/gatk/issues/2155",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4611
https://github.com/broadinstitute/gatk/pull/4611:66,Modifiability,Config,Configuration,66,"Includes:. * Configuration for packages to search read filters; * Configuration for packages to search annotations. In addition, it changes the behavior of `VariantAnnotatorEngine` to use the annotation packages from the configuration, and mimic what the plugin is doing. This closes https://github.com/broadinstitute/gatk/issues/2155",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4611
https://github.com/broadinstitute/gatk/pull/4611:221,Modifiability,config,configuration,221,"Includes:. * Configuration for packages to search read filters; * Configuration for packages to search annotations. In addition, it changes the behavior of `VariantAnnotatorEngine` to use the annotation packages from the configuration, and mimic what the plugin is doing. This closes https://github.com/broadinstitute/gatk/issues/2155",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4611
https://github.com/broadinstitute/gatk/pull/4611:255,Modifiability,plugin,plugin,255,"Includes:. * Configuration for packages to search read filters; * Configuration for packages to search annotations. In addition, it changes the behavior of `VariantAnnotatorEngine` to use the annotation packages from the configuration, and mimic what the plugin is doing. This closes https://github.com/broadinstitute/gatk/issues/2155",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4611
https://github.com/broadinstitute/gatk/issues/4612:116,Deployability,pipeline,pipeline,116,"Hi, a small question (and maybe issue here). Does BwaSpark require the input bam to be queryname sorted ?. I have a pipeline doing `.fastq -> FastqToSam -> .bam -> BwaSpark -> Some other stuffs` and FastqToSam take time (40min) relative to the entire pipeline (2h).; I tried to speed-up it by creating a unsorted bam using -SO unsorted. It's faster but BwaSpark crashes. There is no indication in BwaSpark documentation and help it requires a name sorted bam. The only information about it is in the help message of `BwaAndMarkDuplicatesPipelineSpark`: ; > Takes name-sorted file and runs BWA and MarkDuplicates.; > Version:4.0.1.2. Is it a missing information ?. Additional question : could it be possible to do the conversion on Spark (FastqToSamSpark) ?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4612
https://github.com/broadinstitute/gatk/issues/4612:251,Deployability,pipeline,pipeline,251,"Hi, a small question (and maybe issue here). Does BwaSpark require the input bam to be queryname sorted ?. I have a pipeline doing `.fastq -> FastqToSam -> .bam -> BwaSpark -> Some other stuffs` and FastqToSam take time (40min) relative to the entire pipeline (2h).; I tried to speed-up it by creating a unsorted bam using -SO unsorted. It's faster but BwaSpark crashes. There is no indication in BwaSpark documentation and help it requires a name sorted bam. The only information about it is in the help message of `BwaAndMarkDuplicatesPipelineSpark`: ; > Takes name-sorted file and runs BWA and MarkDuplicates.; > Version:4.0.1.2. Is it a missing information ?. Additional question : could it be possible to do the conversion on Spark (FastqToSamSpark) ?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4612
https://github.com/broadinstitute/gatk/issues/4612:505,Integrability,message,message,505,"Hi, a small question (and maybe issue here). Does BwaSpark require the input bam to be queryname sorted ?. I have a pipeline doing `.fastq -> FastqToSam -> .bam -> BwaSpark -> Some other stuffs` and FastqToSam take time (40min) relative to the entire pipeline (2h).; I tried to speed-up it by creating a unsorted bam using -SO unsorted. It's faster but BwaSpark crashes. There is no indication in BwaSpark documentation and help it requires a name sorted bam. The only information about it is in the help message of `BwaAndMarkDuplicatesPipelineSpark`: ; > Takes name-sorted file and runs BWA and MarkDuplicates.; > Version:4.0.1.2. Is it a missing information ?. Additional question : could it be possible to do the conversion on Spark (FastqToSamSpark) ?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4612
https://github.com/broadinstitute/gatk/issues/4614:103,Energy Efficiency,efficient,efficient,103,People don't know to use new qual with GenotypeGVCFs so they're wasting a lot of time running the less efficient old qual. There are also people encountering bugs in old qual (see https://github.com/broadinstitute/gatk/issues/4544) We should consider making new qual the default and deprecating old qual. @ldgauthier @davidbenjamin Thoughts?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4614
https://github.com/broadinstitute/gatk/issues/4617:467,Availability,down,download,467,"Hi. The VCF output file of HaplotypeCaller is not same when different interval split. It seems that smaller region will product more var into vcf file.; split1:chr unit; 5,089,533 var in vcf; split2: gatk4.SplitIntervals.sh(500 unit); 5,142,322 var in vcf. fastq files; HiSeqX-PCR-free-v2.5-NA12878 70x from BaseSpace. gatk version:4.0.3.0. By the way, there are some many diff site just in QD value. ; such as QD=28.13 vs QD=25.36, and other attr are same. Reason:; download sample without a fixed seed for every site?; or https://github.com/broadinstitute/gatk/issues/4614?; or others?. Best Regards",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4617
https://github.com/broadinstitute/gatk/issues/4618:40,Performance,perform,performance,40,Our Jenkins tests are reporting a major performance regression in ApplyBQSR. We should figure out what's going on.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4618
https://github.com/broadinstitute/gatk/issues/4618:12,Testability,test,tests,12,Our Jenkins tests are reporting a major performance regression in ApplyBQSR. We should figure out what's going on.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4618
https://github.com/broadinstitute/gatk/issues/4619:129,Availability,error,error,129,"### Affected tool(s); IntervalListTools. ### Affected version(s); GATK 4.0.3.0. ### Description; Ive been getting the following error when executing IntervalListTools in gatk4 . `SECOND_LIST must be null when ACTION is CONCAT, found []. Please put all the inputs in INPUT for this ACTION.`. I can replicate the error when manually executing the command on gatk4 docker but the same command works fine on gitc using picard. COMMANDS; latest gatk4 docker; `/gatk/gatk IntervalListTools --SCATTER_COUNT 6 --SUBDIVISION_MODE BALANCING_WITHOUT_INTERVAL_SUBDIVISION_WITH_OVERFLOW --UNIQUE true --SORT true --INPUT /gatk/interval/Homo_sapiens_assembly19_1000genomes_decoy.whole_genome.interval_list`; gitc 2.3.3-1513176735; `java -Xms1g -jar /usr/gitc/picard.jar IntervalListTools SCATTER_COUNT=6 SUBDIVISION_MODE=BALANCING_WITHOUT_INTERVAL_SUBDIVISION_WITH_OVERFLOW UNIQUE=true SORT=true INPUT=/gatk/interval/Homo_sapiens_assembly19_1000genomes_decoy.whole_genome.interval_list`. INPUT:; gs://broad-references/Homo_sapiens_assembly19_1000genomes_decoy/Homo_sapiens_assembly19_1000genomes_decoy.whole_genome.interval_list",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4619
https://github.com/broadinstitute/gatk/issues/4619:312,Availability,error,error,312,"### Affected tool(s); IntervalListTools. ### Affected version(s); GATK 4.0.3.0. ### Description; Ive been getting the following error when executing IntervalListTools in gatk4 . `SECOND_LIST must be null when ACTION is CONCAT, found []. Please put all the inputs in INPUT for this ACTION.`. I can replicate the error when manually executing the command on gatk4 docker but the same command works fine on gitc using picard. COMMANDS; latest gatk4 docker; `/gatk/gatk IntervalListTools --SCATTER_COUNT 6 --SUBDIVISION_MODE BALANCING_WITHOUT_INTERVAL_SUBDIVISION_WITH_OVERFLOW --UNIQUE true --SORT true --INPUT /gatk/interval/Homo_sapiens_assembly19_1000genomes_decoy.whole_genome.interval_list`; gitc 2.3.3-1513176735; `java -Xms1g -jar /usr/gitc/picard.jar IntervalListTools SCATTER_COUNT=6 SUBDIVISION_MODE=BALANCING_WITHOUT_INTERVAL_SUBDIVISION_WITH_OVERFLOW UNIQUE=true SORT=true INPUT=/gatk/interval/Homo_sapiens_assembly19_1000genomes_decoy.whole_genome.interval_list`. INPUT:; gs://broad-references/Homo_sapiens_assembly19_1000genomes_decoy/Homo_sapiens_assembly19_1000genomes_decoy.whole_genome.interval_list",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4619
https://github.com/broadinstitute/gatk/pull/4621:80,Integrability,depend,depends,80,"Calling `hashCode` directly on an instance of a Java enum produces a value that depends on the object's memory location and is therefore not stable across machines in a distributed environment. This causes issues when using these objects (or objects that contain them) as keys in Spark RDDs, which by default use `HashPartitioner` to parition by hash code. See this blog post for a summary:. http://dev.bizo.com/2014/02/beware-enums-in-spark.html. This PR modifies all places within the sv tools where we call `hashCode` on an enum, instead computing the hash of the ordinal of the enum value. For @SHuang-Broad",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4621
https://github.com/broadinstitute/gatk/pull/4621:9,Security,hash,hashCode,9,"Calling `hashCode` directly on an instance of a Java enum produces a value that depends on the object's memory location and is therefore not stable across machines in a distributed environment. This causes issues when using these objects (or objects that contain them) as keys in Spark RDDs, which by default use `HashPartitioner` to parition by hash code. See this blog post for a summary:. http://dev.bizo.com/2014/02/beware-enums-in-spark.html. This PR modifies all places within the sv tools where we call `hashCode` on an enum, instead computing the hash of the ordinal of the enum value. For @SHuang-Broad",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4621
https://github.com/broadinstitute/gatk/pull/4621:314,Security,Hash,HashPartitioner,314,"Calling `hashCode` directly on an instance of a Java enum produces a value that depends on the object's memory location and is therefore not stable across machines in a distributed environment. This causes issues when using these objects (or objects that contain them) as keys in Spark RDDs, which by default use `HashPartitioner` to parition by hash code. See this blog post for a summary:. http://dev.bizo.com/2014/02/beware-enums-in-spark.html. This PR modifies all places within the sv tools where we call `hashCode` on an enum, instead computing the hash of the ordinal of the enum value. For @SHuang-Broad",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4621
https://github.com/broadinstitute/gatk/pull/4621:346,Security,hash,hash,346,"Calling `hashCode` directly on an instance of a Java enum produces a value that depends on the object's memory location and is therefore not stable across machines in a distributed environment. This causes issues when using these objects (or objects that contain them) as keys in Spark RDDs, which by default use `HashPartitioner` to parition by hash code. See this blog post for a summary:. http://dev.bizo.com/2014/02/beware-enums-in-spark.html. This PR modifies all places within the sv tools where we call `hashCode` on an enum, instead computing the hash of the ordinal of the enum value. For @SHuang-Broad",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4621
https://github.com/broadinstitute/gatk/pull/4621:511,Security,hash,hashCode,511,"Calling `hashCode` directly on an instance of a Java enum produces a value that depends on the object's memory location and is therefore not stable across machines in a distributed environment. This causes issues when using these objects (or objects that contain them) as keys in Spark RDDs, which by default use `HashPartitioner` to parition by hash code. See this blog post for a summary:. http://dev.bizo.com/2014/02/beware-enums-in-spark.html. This PR modifies all places within the sv tools where we call `hashCode` on an enum, instead computing the hash of the ordinal of the enum value. For @SHuang-Broad",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4621
https://github.com/broadinstitute/gatk/pull/4621:555,Security,hash,hash,555,"Calling `hashCode` directly on an instance of a Java enum produces a value that depends on the object's memory location and is therefore not stable across machines in a distributed environment. This causes issues when using these objects (or objects that contain them) as keys in Spark RDDs, which by default use `HashPartitioner` to parition by hash code. See this blog post for a summary:. http://dev.bizo.com/2014/02/beware-enums-in-spark.html. This PR modifies all places within the sv tools where we call `hashCode` on an enum, instead computing the hash of the ordinal of the enum value. For @SHuang-Broad",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4621
https://github.com/broadinstitute/gatk/issues/4626:624,Safety,avoid,avoid,624,Hi all;; I'm running into an issue when running GATK Spark based tools inside of Docker containers. Spark tries to look up the current username as part of initialization:. https://github.com/jaceklaskowski/mastering-apache-spark-book/blob/master/spark-sparkcontext-creating-instance-internals.adoc#-utilsgetcurrentusername. while fails in Docker container where the user ID is not present in `/etc/passwd`. This SO thread has a pretty good summary of the problem along with some hacky work arounds:. https://stackoverflow.com/questions/45198252/apache-spark-standalone-for-anonymous-uid-without-user-name. Is it possible to avoid needing Spark to login via username? Do you have any other tips/clues to work around this issue when running GATK Spark inside of container environments?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4626
https://github.com/broadinstitute/gatk/issues/4626:647,Testability,log,login,647,Hi all;; I'm running into an issue when running GATK Spark based tools inside of Docker containers. Spark tries to look up the current username as part of initialization:. https://github.com/jaceklaskowski/mastering-apache-spark-book/blob/master/spark-sparkcontext-creating-instance-internals.adoc#-utilsgetcurrentusername. while fails in Docker container where the user ID is not present in `/etc/passwd`. This SO thread has a pretty good summary of the problem along with some hacky work arounds:. https://stackoverflow.com/questions/45198252/apache-spark-standalone-for-anonymous-uid-without-user-name. Is it possible to avoid needing Spark to login via username? Do you have any other tips/clues to work around this issue when running GATK Spark inside of container environments?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4626
https://github.com/broadinstitute/gatk/pull/4627:80,Modifiability,variab,variables,80,"If this passes, we can probably remove encrypted_29f3b7c4d8c3_key from the repo variables.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4627
https://github.com/broadinstitute/gatk/issues/4629:126,Testability,log,logger,126,"... and this leads some users to believe that GATK has frozen (I've seen several such complaints on the forum). I use python `logger` to log INFO/WARN/DEBUG to `stdout` in `gcnvkernel`. At the moment, I do not pass GATK's log verbosity level to `gcnvkernel` (provisions for doing this is already in place -- it's just a one-liner fix to enable it). A simple strategy is to pipe python's stdout to JVM's, regardless of GATK's log level, and delegate the responsibility setting the log level in python scripts to python tool devs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4629
https://github.com/broadinstitute/gatk/issues/4629:137,Testability,log,log,137,"... and this leads some users to believe that GATK has frozen (I've seen several such complaints on the forum). I use python `logger` to log INFO/WARN/DEBUG to `stdout` in `gcnvkernel`. At the moment, I do not pass GATK's log verbosity level to `gcnvkernel` (provisions for doing this is already in place -- it's just a one-liner fix to enable it). A simple strategy is to pipe python's stdout to JVM's, regardless of GATK's log level, and delegate the responsibility setting the log level in python scripts to python tool devs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4629
https://github.com/broadinstitute/gatk/issues/4629:222,Testability,log,log,222,"... and this leads some users to believe that GATK has frozen (I've seen several such complaints on the forum). I use python `logger` to log INFO/WARN/DEBUG to `stdout` in `gcnvkernel`. At the moment, I do not pass GATK's log verbosity level to `gcnvkernel` (provisions for doing this is already in place -- it's just a one-liner fix to enable it). A simple strategy is to pipe python's stdout to JVM's, regardless of GATK's log level, and delegate the responsibility setting the log level in python scripts to python tool devs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4629
https://github.com/broadinstitute/gatk/issues/4629:425,Testability,log,log,425,"... and this leads some users to believe that GATK has frozen (I've seen several such complaints on the forum). I use python `logger` to log INFO/WARN/DEBUG to `stdout` in `gcnvkernel`. At the moment, I do not pass GATK's log verbosity level to `gcnvkernel` (provisions for doing this is already in place -- it's just a one-liner fix to enable it). A simple strategy is to pipe python's stdout to JVM's, regardless of GATK's log level, and delegate the responsibility setting the log level in python scripts to python tool devs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4629
https://github.com/broadinstitute/gatk/issues/4629:480,Testability,log,log,480,"... and this leads some users to believe that GATK has frozen (I've seen several such complaints on the forum). I use python `logger` to log INFO/WARN/DEBUG to `stdout` in `gcnvkernel`. At the moment, I do not pass GATK's log verbosity level to `gcnvkernel` (provisions for doing this is already in place -- it's just a one-liner fix to enable it). A simple strategy is to pipe python's stdout to JVM's, regardless of GATK's log level, and delegate the responsibility setting the log level in python scripts to python tool devs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4629
https://github.com/broadinstitute/gatk/issues/4629:351,Usability,simpl,simple,351,"... and this leads some users to believe that GATK has frozen (I've seen several such complaints on the forum). I use python `logger` to log INFO/WARN/DEBUG to `stdout` in `gcnvkernel`. At the moment, I do not pass GATK's log verbosity level to `gcnvkernel` (provisions for doing this is already in place -- it's just a one-liner fix to enable it). A simple strategy is to pipe python's stdout to JVM's, regardless of GATK's log level, and delegate the responsibility setting the log level in python scripts to python tool devs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4629
https://github.com/broadinstitute/gatk/issues/4630:120,Availability,robust,robust,120,"* Should be runnable on-demand using a convenient mechanism (eg., reviewer types a command on a github PR). * Should be robust enough to provide confidence that a substantial change to a stable variant-calling tool is safe to merge. * Should cover performance as well as correctness. * Output may be a report that a human has to read (do not need automated pass/fail). * Implement for `HaplotypeCaller` and CNV tools first (with help of @LeeTL1220), then work with other teams to get test coverage for their tools.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4630
https://github.com/broadinstitute/gatk/issues/4630:248,Performance,perform,performance,248,"* Should be runnable on-demand using a convenient mechanism (eg., reviewer types a command on a github PR). * Should be robust enough to provide confidence that a substantial change to a stable variant-calling tool is safe to merge. * Should cover performance as well as correctness. * Output may be a report that a human has to read (do not need automated pass/fail). * Implement for `HaplotypeCaller` and CNV tools first (with help of @LeeTL1220), then work with other teams to get test coverage for their tools.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4630
https://github.com/broadinstitute/gatk/issues/4630:218,Safety,safe,safe,218,"* Should be runnable on-demand using a convenient mechanism (eg., reviewer types a command on a github PR). * Should be robust enough to provide confidence that a substantial change to a stable variant-calling tool is safe to merge. * Should cover performance as well as correctness. * Output may be a report that a human has to read (do not need automated pass/fail). * Implement for `HaplotypeCaller` and CNV tools first (with help of @LeeTL1220), then work with other teams to get test coverage for their tools.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4630
https://github.com/broadinstitute/gatk/issues/4630:484,Testability,test,test,484,"* Should be runnable on-demand using a convenient mechanism (eg., reviewer types a command on a github PR). * Should be robust enough to provide confidence that a substantial change to a stable variant-calling tool is safe to merge. * Should cover performance as well as correctness. * Output may be a report that a human has to read (do not need automated pass/fail). * Implement for `HaplotypeCaller` and CNV tools first (with help of @LeeTL1220), then work with other teams to get test coverage for their tools.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4630
https://github.com/broadinstitute/gatk/issues/4632:1031,Availability,avail,available,1031,"Hello. I am Adam Yongxin Ye, a PhD candidate in Peking University, supervised by Prof Liping Wei. We have developed MosaicHunter, a bioinformatic tool that can identify postzygotic single-nucleotide mosaicisms (with allele fraction deviated from homozygous 0, 1 and heterozygous 0.5) in bulk sequencing data of a single sample without matched control. After I had the recent lectures on GATK4 tutorial in Beijing, I thought it might be great to merge MosaicHunter into GATK framework, especially for the local assembly function in HaplotypeCaller and Mutect2, to increase the sensitivity & specificity and even extend for mosaic indels, as well as to make MosaicHunter easy for more users to use. MosaicHunter utilized GATK preprocessing, distinguished mosaicisms from germline homozygous and heterozygous sites by a Bayesian genotyper, and applied several stringent hard filters. MosaicHunter has been published (https://academic.oup.com/nar/article/45/10/e76/2962179 and http://www.nature.com/articles/cr2014131) and is publicly available (http://mosaichunter.cbi.pku.edu.cn/ and https://github.com/zzhang526/MosaicHunter (with source code in java)). So I wonder if GATK team is interested in this suggestion. Could someone or may we contribute it into GATK?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4632
https://github.com/broadinstitute/gatk/issues/4632:611,Modifiability,extend,extend,611,"Hello. I am Adam Yongxin Ye, a PhD candidate in Peking University, supervised by Prof Liping Wei. We have developed MosaicHunter, a bioinformatic tool that can identify postzygotic single-nucleotide mosaicisms (with allele fraction deviated from homozygous 0, 1 and heterozygous 0.5) in bulk sequencing data of a single sample without matched control. After I had the recent lectures on GATK4 tutorial in Beijing, I thought it might be great to merge MosaicHunter into GATK framework, especially for the local assembly function in HaplotypeCaller and Mutect2, to increase the sensitivity & specificity and even extend for mosaic indels, as well as to make MosaicHunter easy for more users to use. MosaicHunter utilized GATK preprocessing, distinguished mosaicisms from germline homozygous and heterozygous sites by a Bayesian genotyper, and applied several stringent hard filters. MosaicHunter has been published (https://academic.oup.com/nar/article/45/10/e76/2962179 and http://www.nature.com/articles/cr2014131) and is publicly available (http://mosaichunter.cbi.pku.edu.cn/ and https://github.com/zzhang526/MosaicHunter (with source code in java)). So I wonder if GATK team is interested in this suggestion. Could someone or may we contribute it into GATK?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4632
https://github.com/broadinstitute/gatk/issues/4633:61,Availability,error,error,61,"When the gvcf was merged by ""bcftools concat"", the following error will be happen. more info: ; https://gatkforums.broadinstitute.org/gatk/discussion/10817/gatk-runtime-error-on-genotypegvcfs-java-lang-double-cannot-be-cast-to-java-lang-integer. **java.lang.ClassCastException: java.lang.Double cannot be cast to java.lang.Integer**; 	at java.lang.Integer.compareTo(Integer.java:52); 	at java.util.Comparators$NaturalOrderComparator.compare(Comparators.java:52); 	at java.util.Comparators$NaturalOrderComparator.compare(Comparators.java:47); 	at java.util.TimSort.countRunAndMakeAscending(TimSort.java:351); 	at java.util.TimSort.sort(TimSort.java:216); 	at java.util.Arrays.sort(Arrays.java:1507); 	at java.util.stream.SortedOps$SizedRefSortingSink.end(SortedOps.java:302); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:513); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:502); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.utils.Utils.getMedianValue(Utils.java:1137); 	at org.broadinstitute.hellbender.tools.walkers.ReferenceConfidenceVariantContextMerger.mergeAttributes(ReferenceConfidenceVariantContextMerger.java:277); 	at org.broadinstitute.hellbender.tools.walkers.ReferenceConfidenceVariantContextMerger.merge(ReferenceConfidenceVariantContextMerger.java:101); 	at org.broadinstitute.hellbender.tools.walkers.CombineGVCFs.endPreviousStates(CombineGVCFs.java:340); 	at org.broadinstitute.hellbender.tools.walkers.CombineGVCFs.createIntermediateVariants(CombineGVCFs.java:189); 	at org.broadinstitute.hellbender.tools.walkers.CombineGVCFs.apply(CombineGVCFs.java:134); 	at org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart.apply(MultiVariantWalkerGroupedOnStart.java:73); 	at org.broadinstitute.hellben",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4633
https://github.com/broadinstitute/gatk/issues/4633:169,Availability,error,error-on-genotypegvcfs-java-lang-double-cannot-be-cast-to-java-lang-integer,169,"When the gvcf was merged by ""bcftools concat"", the following error will be happen. more info: ; https://gatkforums.broadinstitute.org/gatk/discussion/10817/gatk-runtime-error-on-genotypegvcfs-java-lang-double-cannot-be-cast-to-java-lang-integer. **java.lang.ClassCastException: java.lang.Double cannot be cast to java.lang.Integer**; 	at java.lang.Integer.compareTo(Integer.java:52); 	at java.util.Comparators$NaturalOrderComparator.compare(Comparators.java:52); 	at java.util.Comparators$NaturalOrderComparator.compare(Comparators.java:47); 	at java.util.TimSort.countRunAndMakeAscending(TimSort.java:351); 	at java.util.TimSort.sort(TimSort.java:216); 	at java.util.Arrays.sort(Arrays.java:1507); 	at java.util.stream.SortedOps$SizedRefSortingSink.end(SortedOps.java:302); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:513); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:502); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.utils.Utils.getMedianValue(Utils.java:1137); 	at org.broadinstitute.hellbender.tools.walkers.ReferenceConfidenceVariantContextMerger.mergeAttributes(ReferenceConfidenceVariantContextMerger.java:277); 	at org.broadinstitute.hellbender.tools.walkers.ReferenceConfidenceVariantContextMerger.merge(ReferenceConfidenceVariantContextMerger.java:101); 	at org.broadinstitute.hellbender.tools.walkers.CombineGVCFs.endPreviousStates(CombineGVCFs.java:340); 	at org.broadinstitute.hellbender.tools.walkers.CombineGVCFs.createIntermediateVariants(CombineGVCFs.java:189); 	at org.broadinstitute.hellbender.tools.walkers.CombineGVCFs.apply(CombineGVCFs.java:134); 	at org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart.apply(MultiVariantWalkerGroupedOnStart.java:73); 	at org.broadinstitute.hellben",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4633
https://github.com/broadinstitute/gatk/issues/4633:953,Energy Efficiency,Reduce,ReduceOps,953,"When the gvcf was merged by ""bcftools concat"", the following error will be happen. more info: ; https://gatkforums.broadinstitute.org/gatk/discussion/10817/gatk-runtime-error-on-genotypegvcfs-java-lang-double-cannot-be-cast-to-java-lang-integer. **java.lang.ClassCastException: java.lang.Double cannot be cast to java.lang.Integer**; 	at java.lang.Integer.compareTo(Integer.java:52); 	at java.util.Comparators$NaturalOrderComparator.compare(Comparators.java:52); 	at java.util.Comparators$NaturalOrderComparator.compare(Comparators.java:47); 	at java.util.TimSort.countRunAndMakeAscending(TimSort.java:351); 	at java.util.TimSort.sort(TimSort.java:216); 	at java.util.Arrays.sort(Arrays.java:1507); 	at java.util.stream.SortedOps$SizedRefSortingSink.end(SortedOps.java:302); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:513); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:502); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.utils.Utils.getMedianValue(Utils.java:1137); 	at org.broadinstitute.hellbender.tools.walkers.ReferenceConfidenceVariantContextMerger.mergeAttributes(ReferenceConfidenceVariantContextMerger.java:277); 	at org.broadinstitute.hellbender.tools.walkers.ReferenceConfidenceVariantContextMerger.merge(ReferenceConfidenceVariantContextMerger.java:101); 	at org.broadinstitute.hellbender.tools.walkers.CombineGVCFs.endPreviousStates(CombineGVCFs.java:340); 	at org.broadinstitute.hellbender.tools.walkers.CombineGVCFs.createIntermediateVariants(CombineGVCFs.java:189); 	at org.broadinstitute.hellbender.tools.walkers.CombineGVCFs.apply(CombineGVCFs.java:134); 	at org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart.apply(MultiVariantWalkerGroupedOnStart.java:73); 	at org.broadinstitute.hellben",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4633
https://github.com/broadinstitute/gatk/issues/4633:963,Energy Efficiency,Reduce,ReduceOp,963,"When the gvcf was merged by ""bcftools concat"", the following error will be happen. more info: ; https://gatkforums.broadinstitute.org/gatk/discussion/10817/gatk-runtime-error-on-genotypegvcfs-java-lang-double-cannot-be-cast-to-java-lang-integer. **java.lang.ClassCastException: java.lang.Double cannot be cast to java.lang.Integer**; 	at java.lang.Integer.compareTo(Integer.java:52); 	at java.util.Comparators$NaturalOrderComparator.compare(Comparators.java:52); 	at java.util.Comparators$NaturalOrderComparator.compare(Comparators.java:47); 	at java.util.TimSort.countRunAndMakeAscending(TimSort.java:351); 	at java.util.TimSort.sort(TimSort.java:216); 	at java.util.Arrays.sort(Arrays.java:1507); 	at java.util.stream.SortedOps$SizedRefSortingSink.end(SortedOps.java:302); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:513); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:502); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.utils.Utils.getMedianValue(Utils.java:1137); 	at org.broadinstitute.hellbender.tools.walkers.ReferenceConfidenceVariantContextMerger.mergeAttributes(ReferenceConfidenceVariantContextMerger.java:277); 	at org.broadinstitute.hellbender.tools.walkers.ReferenceConfidenceVariantContextMerger.merge(ReferenceConfidenceVariantContextMerger.java:101); 	at org.broadinstitute.hellbender.tools.walkers.CombineGVCFs.endPreviousStates(CombineGVCFs.java:340); 	at org.broadinstitute.hellbender.tools.walkers.CombineGVCFs.createIntermediateVariants(CombineGVCFs.java:189); 	at org.broadinstitute.hellbender.tools.walkers.CombineGVCFs.apply(CombineGVCFs.java:134); 	at org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart.apply(MultiVariantWalkerGroupedOnStart.java:73); 	at org.broadinstitute.hellben",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4633
https://github.com/broadinstitute/gatk/issues/4633:991,Energy Efficiency,Reduce,ReduceOps,991,"When the gvcf was merged by ""bcftools concat"", the following error will be happen. more info: ; https://gatkforums.broadinstitute.org/gatk/discussion/10817/gatk-runtime-error-on-genotypegvcfs-java-lang-double-cannot-be-cast-to-java-lang-integer. **java.lang.ClassCastException: java.lang.Double cannot be cast to java.lang.Integer**; 	at java.lang.Integer.compareTo(Integer.java:52); 	at java.util.Comparators$NaturalOrderComparator.compare(Comparators.java:52); 	at java.util.Comparators$NaturalOrderComparator.compare(Comparators.java:47); 	at java.util.TimSort.countRunAndMakeAscending(TimSort.java:351); 	at java.util.TimSort.sort(TimSort.java:216); 	at java.util.Arrays.sort(Arrays.java:1507); 	at java.util.stream.SortedOps$SizedRefSortingSink.end(SortedOps.java:302); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:513); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:502); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.utils.Utils.getMedianValue(Utils.java:1137); 	at org.broadinstitute.hellbender.tools.walkers.ReferenceConfidenceVariantContextMerger.mergeAttributes(ReferenceConfidenceVariantContextMerger.java:277); 	at org.broadinstitute.hellbender.tools.walkers.ReferenceConfidenceVariantContextMerger.merge(ReferenceConfidenceVariantContextMerger.java:101); 	at org.broadinstitute.hellbender.tools.walkers.CombineGVCFs.endPreviousStates(CombineGVCFs.java:340); 	at org.broadinstitute.hellbender.tools.walkers.CombineGVCFs.createIntermediateVariants(CombineGVCFs.java:189); 	at org.broadinstitute.hellbender.tools.walkers.CombineGVCFs.apply(CombineGVCFs.java:134); 	at org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart.apply(MultiVariantWalkerGroupedOnStart.java:73); 	at org.broadinstitute.hellben",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4633
https://github.com/broadinstitute/gatk/issues/4633:888,Integrability,wrap,wrapAndCopyInto,888,"When the gvcf was merged by ""bcftools concat"", the following error will be happen. more info: ; https://gatkforums.broadinstitute.org/gatk/discussion/10817/gatk-runtime-error-on-genotypegvcfs-java-lang-double-cannot-be-cast-to-java-lang-integer. **java.lang.ClassCastException: java.lang.Double cannot be cast to java.lang.Integer**; 	at java.lang.Integer.compareTo(Integer.java:52); 	at java.util.Comparators$NaturalOrderComparator.compare(Comparators.java:52); 	at java.util.Comparators$NaturalOrderComparator.compare(Comparators.java:47); 	at java.util.TimSort.countRunAndMakeAscending(TimSort.java:351); 	at java.util.TimSort.sort(TimSort.java:216); 	at java.util.Arrays.sort(Arrays.java:1507); 	at java.util.stream.SortedOps$SizedRefSortingSink.end(SortedOps.java:302); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:513); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:502); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.utils.Utils.getMedianValue(Utils.java:1137); 	at org.broadinstitute.hellbender.tools.walkers.ReferenceConfidenceVariantContextMerger.mergeAttributes(ReferenceConfidenceVariantContextMerger.java:277); 	at org.broadinstitute.hellbender.tools.walkers.ReferenceConfidenceVariantContextMerger.merge(ReferenceConfidenceVariantContextMerger.java:101); 	at org.broadinstitute.hellbender.tools.walkers.CombineGVCFs.endPreviousStates(CombineGVCFs.java:340); 	at org.broadinstitute.hellbender.tools.walkers.CombineGVCFs.createIntermediateVariants(CombineGVCFs.java:189); 	at org.broadinstitute.hellbender.tools.walkers.CombineGVCFs.apply(CombineGVCFs.java:134); 	at org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart.apply(MultiVariantWalkerGroupedOnStart.java:73); 	at org.broadinstitute.hellben",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4633
https://github.com/broadinstitute/gatk/issues/4633:2599,Integrability,wrap,wrapAndCopyInto,2599,6); 	at java.util.Arrays.sort(Arrays.java:1507); 	at java.util.stream.SortedOps$SizedRefSortingSink.end(SortedOps.java:302); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:513); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:502); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.utils.Utils.getMedianValue(Utils.java:1137); 	at org.broadinstitute.hellbender.tools.walkers.ReferenceConfidenceVariantContextMerger.mergeAttributes(ReferenceConfidenceVariantContextMerger.java:277); 	at org.broadinstitute.hellbender.tools.walkers.ReferenceConfidenceVariantContextMerger.merge(ReferenceConfidenceVariantContextMerger.java:101); 	at org.broadinstitute.hellbender.tools.walkers.CombineGVCFs.endPreviousStates(CombineGVCFs.java:340); 	at org.broadinstitute.hellbender.tools.walkers.CombineGVCFs.createIntermediateVariants(CombineGVCFs.java:189); 	at org.broadinstitute.hellbender.tools.walkers.CombineGVCFs.apply(CombineGVCFs.java:134); 	at org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart.apply(MultiVariantWalkerGroupedOnStart.java:73); 	at org.broadinstitute.hellbender.engine.VariantWalkerBase.lambda$traverse$0(VariantWalkerBase.java:110); 	at org.broadinstitute.hellbender.engine.VariantWalkerBase$$Lambda$87/908435478.accept(Unknown Source); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183); 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:512); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:502); .......,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4633
https://github.com/broadinstitute/gatk/issues/4635:8,Deployability,pipeline,pipeline,8,"GATK SV pipeline typically run sucessfully in 1 hour 20 minutes for a 30x WGS crams. However, there will be some crams that get stuck at stage 7 with little progress. These never finish and need to be killed. Any suggestions for this? . Below is some stage info.... Stage 7	. collect at FindBreakpointEvidenceSpark.java:738 +details. org.apache.spark.api.java.AbstractJavaRDDLike.collect(JavaRDDLike.scala:45); org.broadinstitute.hellbender.tools.spark.sv.evidence.FindBreakpointEvidenceSpark.getKmerIntervals(FindBreakpointEvidenceSpark.java:738); org.broadinstitute.hellbender.tools.spark.sv.evidence.FindBreakpointEvidenceSpark.getKmerAndIntervalsSet(FindBreakpointEvidenceSpark.java:532); org.broadinstitute.hellbender.tools.spark.sv.evidence.FindBreakpointEvidenceSpark.addAssemblyQNames(FindBreakpointEvidenceSpark.java:489); org.broadinstitute.hellbender.tools.spark.sv.evidence.FindBreakpointEvidenceSpark.gatherEvidenceAndWriteContigSamFile(FindBreakpointEvidenceSpark.java:174); org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDiscoveryPipelineSpark.runTool(StructuralVariationDiscoveryPipelineSpark.java:147); org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:387); org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:135); org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:180); org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:199); org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:159); org.broadinstitute.hellbender.Main.mainEntry(Main.java:202); org.broadinstitute.hellbender.Main.main(Main.java:288); sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); sun.reflect.DelegatingMe",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4635
https://github.com/broadinstitute/gatk/issues/4635:2131,Deployability,deploy,deploy,2131,killed. Any suggestions for this? . Below is some stage info.... Stage 7	. collect at FindBreakpointEvidenceSpark.java:738 +details. org.apache.spark.api.java.AbstractJavaRDDLike.collect(JavaRDDLike.scala:45); org.broadinstitute.hellbender.tools.spark.sv.evidence.FindBreakpointEvidenceSpark.getKmerIntervals(FindBreakpointEvidenceSpark.java:738); org.broadinstitute.hellbender.tools.spark.sv.evidence.FindBreakpointEvidenceSpark.getKmerAndIntervalsSet(FindBreakpointEvidenceSpark.java:532); org.broadinstitute.hellbender.tools.spark.sv.evidence.FindBreakpointEvidenceSpark.addAssemblyQNames(FindBreakpointEvidenceSpark.java:489); org.broadinstitute.hellbender.tools.spark.sv.evidence.FindBreakpointEvidenceSpark.gatherEvidenceAndWriteContigSamFile(FindBreakpointEvidenceSpark.java:174); org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDiscoveryPipelineSpark.runTool(StructuralVariationDiscoveryPipelineSpark.java:147); org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:387); org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:135); org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:180); org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:199); org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:159); org.broadinstitute.hellbender.Main.mainEntry(Main.java:202); org.broadinstitute.hellbender.Main.main(Main.java:288); sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); java.lang.reflect.Method.invoke(Method.java:498); org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:637),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4635
https://github.com/broadinstitute/gatk/issues/4636:260,Testability,log,logical,260,Currently there is no way to specify where the bam parts get written to before being combined into a single sorted bam. Being able to specify this directory is important for supporting google compute engine LOCAL disks which currently can't be stitched into a logical volume.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4636
https://github.com/broadinstitute/gatk/issues/4637:88,Testability,log,logger,88,"User is running VariantFiltration and gets. `log4j:WARN No appenders could be found for logger (org.apache.commons.jexl2.JexlEngine).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info. Tagging Chris, as he already helped a bit. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/comment/47736#Comment_47736",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4637
https://github.com/broadinstitute/gatk/issues/4637:214,Testability,log,logging,214,"User is running VariantFiltration and gets. `log4j:WARN No appenders could be found for logger (org.apache.commons.jexl2.JexlEngine).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info. Tagging Chris, as he already helped a bit. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/comment/47736#Comment_47736",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4637
https://github.com/broadinstitute/gatk/pull/4638:14,Deployability,Release,Release,14,Fixes #4490 . Release notes:; http://owner.aeonbits.org/news/2018/03/01/owner-1-0-10-released,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4638
https://github.com/broadinstitute/gatk/pull/4638:85,Deployability,release,released,85,Fixes #4490 . Release notes:; http://owner.aeonbits.org/news/2018/03/01/owner-1-0-10-released,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4638
https://github.com/broadinstitute/gatk/pull/4639:93,Testability,test,tests,93,"Closes #4596. @TedBrookings Could you review this PR for an issue we saw earlier today, once tests pass?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4639
https://github.com/broadinstitute/gatk/issues/4641:307,Deployability,update,updated,307,"I have noticed that running print reads with a stringent filter which I expect to only return a handful of reads results in the progress meter never printing any progress. This makes it look like the gatk has hung despite the fact it is chugging away and filtering every read it passes over. This should be updated to include an indication of how many reads have been filtered. Additionally, it should be improved to use a second thread to make periodic updates based on execution time in case the tool really has hung in order to make it clearer to the user what is going on.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4641
https://github.com/broadinstitute/gatk/issues/4641:454,Deployability,update,updates,454,"I have noticed that running print reads with a stringent filter which I expect to only return a handful of reads results in the progress meter never printing any progress. This makes it look like the gatk has hung despite the fact it is chugging away and filtering every read it passes over. This should be updated to include an indication of how many reads have been filtered. Additionally, it should be improved to use a second thread to make periodic updates based on execution time in case the tool really has hung in order to make it clearer to the user what is going on.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4641
https://github.com/broadinstitute/gatk/issues/4641:137,Energy Efficiency,meter,meter,137,"I have noticed that running print reads with a stringent filter which I expect to only return a handful of reads results in the progress meter never printing any progress. This makes it look like the gatk has hung despite the fact it is chugging away and filtering every read it passes over. This should be updated to include an indication of how many reads have been filtered. Additionally, it should be improved to use a second thread to make periodic updates based on execution time in case the tool really has hung in order to make it clearer to the user what is going on.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4641
https://github.com/broadinstitute/gatk/issues/4641:539,Usability,clear,clearer,539,"I have noticed that running print reads with a stringent filter which I expect to only return a handful of reads results in the progress meter never printing any progress. This makes it look like the gatk has hung despite the fact it is chugging away and filtering every read it passes over. This should be updated to include an indication of how many reads have been filtered. Additionally, it should be improved to use a second thread to make periodic updates based on execution time in case the tool really has hung in order to make it clearer to the user what is going on.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4641
https://github.com/broadinstitute/gatk/issues/4642:189,Availability,error,errors,189,"Reposting from the Slack channel:. When I run ValidateVariants on an *invalid* VCF without providing a reference or any ""--validation-type-to-exclude"" arguments, I don't get any validation errors. However, if I add ""--validation-type-to-exclude REF"", then I get validation errors as expected. Even when I get validation errors in the second case, the error message seems to terminate abruptly: `A USER ERROR has occurred: Input output.vcf fails strict validation: the Allele Count (AC) tag is incorrect for the record at position 1:1262288, 2 vs. 1 of type:`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4642
https://github.com/broadinstitute/gatk/issues/4642:273,Availability,error,errors,273,"Reposting from the Slack channel:. When I run ValidateVariants on an *invalid* VCF without providing a reference or any ""--validation-type-to-exclude"" arguments, I don't get any validation errors. However, if I add ""--validation-type-to-exclude REF"", then I get validation errors as expected. Even when I get validation errors in the second case, the error message seems to terminate abruptly: `A USER ERROR has occurred: Input output.vcf fails strict validation: the Allele Count (AC) tag is incorrect for the record at position 1:1262288, 2 vs. 1 of type:`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4642
https://github.com/broadinstitute/gatk/issues/4642:320,Availability,error,errors,320,"Reposting from the Slack channel:. When I run ValidateVariants on an *invalid* VCF without providing a reference or any ""--validation-type-to-exclude"" arguments, I don't get any validation errors. However, if I add ""--validation-type-to-exclude REF"", then I get validation errors as expected. Even when I get validation errors in the second case, the error message seems to terminate abruptly: `A USER ERROR has occurred: Input output.vcf fails strict validation: the Allele Count (AC) tag is incorrect for the record at position 1:1262288, 2 vs. 1 of type:`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4642
https://github.com/broadinstitute/gatk/issues/4642:351,Availability,error,error,351,"Reposting from the Slack channel:. When I run ValidateVariants on an *invalid* VCF without providing a reference or any ""--validation-type-to-exclude"" arguments, I don't get any validation errors. However, if I add ""--validation-type-to-exclude REF"", then I get validation errors as expected. Even when I get validation errors in the second case, the error message seems to terminate abruptly: `A USER ERROR has occurred: Input output.vcf fails strict validation: the Allele Count (AC) tag is incorrect for the record at position 1:1262288, 2 vs. 1 of type:`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4642
https://github.com/broadinstitute/gatk/issues/4642:402,Availability,ERROR,ERROR,402,"Reposting from the Slack channel:. When I run ValidateVariants on an *invalid* VCF without providing a reference or any ""--validation-type-to-exclude"" arguments, I don't get any validation errors. However, if I add ""--validation-type-to-exclude REF"", then I get validation errors as expected. Even when I get validation errors in the second case, the error message seems to terminate abruptly: `A USER ERROR has occurred: Input output.vcf fails strict validation: the Allele Count (AC) tag is incorrect for the record at position 1:1262288, 2 vs. 1 of type:`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4642
https://github.com/broadinstitute/gatk/issues/4642:357,Integrability,message,message,357,"Reposting from the Slack channel:. When I run ValidateVariants on an *invalid* VCF without providing a reference or any ""--validation-type-to-exclude"" arguments, I don't get any validation errors. However, if I add ""--validation-type-to-exclude REF"", then I get validation errors as expected. Even when I get validation errors in the second case, the error message seems to terminate abruptly: `A USER ERROR has occurred: Input output.vcf fails strict validation: the Allele Count (AC) tag is incorrect for the record at position 1:1262288, 2 vs. 1 of type:`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4642
https://github.com/broadinstitute/gatk/issues/4642:46,Security,Validat,ValidateVariants,46,"Reposting from the Slack channel:. When I run ValidateVariants on an *invalid* VCF without providing a reference or any ""--validation-type-to-exclude"" arguments, I don't get any validation errors. However, if I add ""--validation-type-to-exclude REF"", then I get validation errors as expected. Even when I get validation errors in the second case, the error message seems to terminate abruptly: `A USER ERROR has occurred: Input output.vcf fails strict validation: the Allele Count (AC) tag is incorrect for the record at position 1:1262288, 2 vs. 1 of type:`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4642
https://github.com/broadinstitute/gatk/issues/4642:123,Security,validat,validation-type-to-exclude,123,"Reposting from the Slack channel:. When I run ValidateVariants on an *invalid* VCF without providing a reference or any ""--validation-type-to-exclude"" arguments, I don't get any validation errors. However, if I add ""--validation-type-to-exclude REF"", then I get validation errors as expected. Even when I get validation errors in the second case, the error message seems to terminate abruptly: `A USER ERROR has occurred: Input output.vcf fails strict validation: the Allele Count (AC) tag is incorrect for the record at position 1:1262288, 2 vs. 1 of type:`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4642
https://github.com/broadinstitute/gatk/issues/4642:178,Security,validat,validation,178,"Reposting from the Slack channel:. When I run ValidateVariants on an *invalid* VCF without providing a reference or any ""--validation-type-to-exclude"" arguments, I don't get any validation errors. However, if I add ""--validation-type-to-exclude REF"", then I get validation errors as expected. Even when I get validation errors in the second case, the error message seems to terminate abruptly: `A USER ERROR has occurred: Input output.vcf fails strict validation: the Allele Count (AC) tag is incorrect for the record at position 1:1262288, 2 vs. 1 of type:`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4642
https://github.com/broadinstitute/gatk/issues/4642:218,Security,validat,validation-type-to-exclude,218,"Reposting from the Slack channel:. When I run ValidateVariants on an *invalid* VCF without providing a reference or any ""--validation-type-to-exclude"" arguments, I don't get any validation errors. However, if I add ""--validation-type-to-exclude REF"", then I get validation errors as expected. Even when I get validation errors in the second case, the error message seems to terminate abruptly: `A USER ERROR has occurred: Input output.vcf fails strict validation: the Allele Count (AC) tag is incorrect for the record at position 1:1262288, 2 vs. 1 of type:`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4642
https://github.com/broadinstitute/gatk/issues/4642:262,Security,validat,validation,262,"Reposting from the Slack channel:. When I run ValidateVariants on an *invalid* VCF without providing a reference or any ""--validation-type-to-exclude"" arguments, I don't get any validation errors. However, if I add ""--validation-type-to-exclude REF"", then I get validation errors as expected. Even when I get validation errors in the second case, the error message seems to terminate abruptly: `A USER ERROR has occurred: Input output.vcf fails strict validation: the Allele Count (AC) tag is incorrect for the record at position 1:1262288, 2 vs. 1 of type:`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4642
https://github.com/broadinstitute/gatk/issues/4642:309,Security,validat,validation,309,"Reposting from the Slack channel:. When I run ValidateVariants on an *invalid* VCF without providing a reference or any ""--validation-type-to-exclude"" arguments, I don't get any validation errors. However, if I add ""--validation-type-to-exclude REF"", then I get validation errors as expected. Even when I get validation errors in the second case, the error message seems to terminate abruptly: `A USER ERROR has occurred: Input output.vcf fails strict validation: the Allele Count (AC) tag is incorrect for the record at position 1:1262288, 2 vs. 1 of type:`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4642
https://github.com/broadinstitute/gatk/issues/4642:452,Security,validat,validation,452,"Reposting from the Slack channel:. When I run ValidateVariants on an *invalid* VCF without providing a reference or any ""--validation-type-to-exclude"" arguments, I don't get any validation errors. However, if I add ""--validation-type-to-exclude REF"", then I get validation errors as expected. Even when I get validation errors in the second case, the error message seems to terminate abruptly: `A USER ERROR has occurred: Input output.vcf fails strict validation: the Allele Count (AC) tag is incorrect for the record at position 1:1262288, 2 vs. 1 of type:`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4642
https://github.com/broadinstitute/gatk/pull/4645:2650,Availability,error,error,2650," and reading the right arrays. For example, if you wish to read (""chr2"", [ 50, 50M] ), then only the second array is queried.; - In the previous version of the tool, the array name was a constant - _genomicsdb_array_. The new version will be backward compatible with respect to reads. Hence, if a directory named _genomicsdb_array_ is found in the workspace directory, it's passed as the array for the _GenomicsDBFeatureReader_ otherwise the array names are generated from the directory entry names.; - Parallel import based on chromosome intervals. The number of threads to use can be specified as an integer argument to the [executeImport call](https://github.com/francares/gatk/blob/fmc_GenomicsDB_parallel_import/src/main/java/org/broadinstitute/hellbender/tools/genomicsdb/GenomicsDBImport.java#L535). If no argument is specified, the number of threads is determined by Java's ForkJoinPool (typically equal to the \#cores in the system). ; - The max number of intervals to import in parallel can be controlled by the command line argument --max-num-intervals-to-import-in-parallel (default 1); - Note that increasing parallelism increases the number of FeatureReaders opened to feed data to the importer. So, if you are using _N_ threads and your batch size is _B_, you will have _N*B_ feature readers open.; - Protobuf based API for import and read; - [Import](https://github.com/francares/gatk/blob/fmc_GenomicsDB_parallel_import/src/main/java/org/broadinstitute/hellbender/tools/genomicsdb/GenomicsDBImport.java#L522); - [Read](https://github.com/francares/gatk/blob/fmc_GenomicsDB_parallel_import/src/main/java/org/broadinstitute/hellbender/engine/FeatureDataSource.java#L405); - #3688 ; - Option to produce GT field; - [Option to produce GT for spanning deletion based on min PL value](https://github.com/Intel-HLS/GenomicsDB/issues/161); - https://github.com/broadinstitute/gatk/issues/2687; - Doesn't support #4541 or #3689 yet - next version; - Bug fixes; - #4716 ; - More error messages",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4645
https://github.com/broadinstitute/gatk/pull/4645:499,Integrability,depend,depend,499,"This PR addresses required changes in order to use latest version of GenomicsDB which exposes new functionality such as:; - [Multi interval import and query support](https://github.com/broadinstitute/gatk/issues/3269):; - We create multiple arrays (directories) in a single workspace - one per interval. So, if you wish to import intervals (""chr1"", [ 1, 100M ]) and (""chr2"", [ 1, 100M ]), you end up with 2 directories/arrays in the workspace with names chr1$1$100M and chr2$1$100M. The array names depend on the partition bounds.; - During the read phase, the user only supplies the workspace. The array names are obtained by scanning the entries in the workspace and reading the right arrays. For example, if you wish to read (""chr2"", [ 50, 50M] ), then only the second array is queried.; - In the previous version of the tool, the array name was a constant - _genomicsdb_array_. The new version will be backward compatible with respect to reads. Hence, if a directory named _genomicsdb_array_ is found in the workspace directory, it's passed as the array for the _GenomicsDBFeatureReader_ otherwise the array names are generated from the directory entry names.; - Parallel import based on chromosome intervals. The number of threads to use can be specified as an integer argument to the [executeImport call](https://github.com/francares/gatk/blob/fmc_GenomicsDB_parallel_import/src/main/java/org/broadinstitute/hellbender/tools/genomicsdb/GenomicsDBImport.java#L535). If no argument is specified, the number of threads is determined by Java's ForkJoinPool (typically equal to the \#cores in the system). ; - The max number of intervals to import in parallel can be controlled by the command line argument --max-num-intervals-to-import-in-parallel (default 1); - Note that increasing parallelism increases the number of FeatureReaders opened to feed data to the importer. So, if you are using _N_ threads and your batch size is _B_, you will have _N*B_ feature readers open.; - Protobuf based API fo",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4645
https://github.com/broadinstitute/gatk/pull/4645:2656,Integrability,message,messages,2656," and reading the right arrays. For example, if you wish to read (""chr2"", [ 50, 50M] ), then only the second array is queried.; - In the previous version of the tool, the array name was a constant - _genomicsdb_array_. The new version will be backward compatible with respect to reads. Hence, if a directory named _genomicsdb_array_ is found in the workspace directory, it's passed as the array for the _GenomicsDBFeatureReader_ otherwise the array names are generated from the directory entry names.; - Parallel import based on chromosome intervals. The number of threads to use can be specified as an integer argument to the [executeImport call](https://github.com/francares/gatk/blob/fmc_GenomicsDB_parallel_import/src/main/java/org/broadinstitute/hellbender/tools/genomicsdb/GenomicsDBImport.java#L535). If no argument is specified, the number of threads is determined by Java's ForkJoinPool (typically equal to the \#cores in the system). ; - The max number of intervals to import in parallel can be controlled by the command line argument --max-num-intervals-to-import-in-parallel (default 1); - Note that increasing parallelism increases the number of FeatureReaders opened to feed data to the importer. So, if you are using _N_ threads and your batch size is _B_, you will have _N*B_ feature readers open.; - Protobuf based API for import and read; - [Import](https://github.com/francares/gatk/blob/fmc_GenomicsDB_parallel_import/src/main/java/org/broadinstitute/hellbender/tools/genomicsdb/GenomicsDBImport.java#L522); - [Read](https://github.com/francares/gatk/blob/fmc_GenomicsDB_parallel_import/src/main/java/org/broadinstitute/hellbender/engine/FeatureDataSource.java#L405); - #3688 ; - Option to produce GT field; - [Option to produce GT for spanning deletion based on min PL value](https://github.com/Intel-HLS/GenomicsDB/issues/161); - https://github.com/broadinstitute/gatk/issues/2687; - Doesn't support #4541 or #3689 yet - next version; - Bug fixes; - #4716 ; - More error messages",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4645
https://github.com/broadinstitute/gatk/pull/4645:86,Security,expose,exposes,86,"This PR addresses required changes in order to use latest version of GenomicsDB which exposes new functionality such as:; - [Multi interval import and query support](https://github.com/broadinstitute/gatk/issues/3269):; - We create multiple arrays (directories) in a single workspace - one per interval. So, if you wish to import intervals (""chr1"", [ 1, 100M ]) and (""chr2"", [ 1, 100M ]), you end up with 2 directories/arrays in the workspace with names chr1$1$100M and chr2$1$100M. The array names depend on the partition bounds.; - During the read phase, the user only supplies the workspace. The array names are obtained by scanning the entries in the workspace and reading the right arrays. For example, if you wish to read (""chr2"", [ 50, 50M] ), then only the second array is queried.; - In the previous version of the tool, the array name was a constant - _genomicsdb_array_. The new version will be backward compatible with respect to reads. Hence, if a directory named _genomicsdb_array_ is found in the workspace directory, it's passed as the array for the _GenomicsDBFeatureReader_ otherwise the array names are generated from the directory entry names.; - Parallel import based on chromosome intervals. The number of threads to use can be specified as an integer argument to the [executeImport call](https://github.com/francares/gatk/blob/fmc_GenomicsDB_parallel_import/src/main/java/org/broadinstitute/hellbender/tools/genomicsdb/GenomicsDBImport.java#L535). If no argument is specified, the number of threads is determined by Java's ForkJoinPool (typically equal to the \#cores in the system). ; - The max number of intervals to import in parallel can be controlled by the command line argument --max-num-intervals-to-import-in-parallel (default 1); - Note that increasing parallelism increases the number of FeatureReaders opened to feed data to the importer. So, if you are using _N_ threads and your batch size is _B_, you will have _N*B_ feature readers open.; - Protobuf based API fo",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4645
https://github.com/broadinstitute/gatk/pull/4646:37,Deployability,pipeline,pipeline,37,"This PR makes four changes to the SV pipeline management scripts: First it uses the new glob argument for copying data into HDFS to only copy the requested BAM file and its index, rather than everything in the GCS bucket directory (this is useful when the bucket directory contains multiple samples, as is the case with our HGSV trios). Second, the scripts now respect the project argument at each phase of the pipeline. Third, the output directory will include the name of the cluster so that if multiple samples are being processed by the same branch in parallel it's easier to figure out which results directory contains which sample and the results won't collide if two runs happen to start at the same timestamp. Finally, if the user requests not to copy the FASTQ files from the main management script, the script will not direct the SV discovery tool to write them to disk.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4646
https://github.com/broadinstitute/gatk/pull/4646:411,Deployability,pipeline,pipeline,411,"This PR makes four changes to the SV pipeline management scripts: First it uses the new glob argument for copying data into HDFS to only copy the requested BAM file and its index, rather than everything in the GCS bucket directory (this is useful when the bucket directory contains multiple samples, as is the case with our HGSV trios). Second, the scripts now respect the project argument at each phase of the pipeline. Third, the output directory will include the name of the cluster so that if multiple samples are being processed by the same branch in parallel it's easier to figure out which results directory contains which sample and the results won't collide if two runs happen to start at the same timestamp. Finally, if the user requests not to copy the FASTQ files from the main management script, the script will not direct the SV discovery tool to write them to disk.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4646
https://github.com/broadinstitute/gatk/issues/4648:219,Energy Efficiency,schedul,scheduler,219,"Several of our HGSV snapshot samples are failing with current master due to an exception in `CpxVariantInterpreter`. For example, sample HG00732 fails with this stacktrace:. ```; 18/04/11 14:30:28 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 2.0 in stage 42.0 (TID 60116, cwhelan-hg00732-cram-samtools-bam-feature-w-5.c.broad-dsde-methods.internal, executor 27): java.lang.IllegalArgumentException: Invalid interval. Contig:chr19 start:33757506 end:33757488; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.ContigAlignmentsModifier.computeNewRefSpanAndCigar(ContigAlignmentsModifier.java:159); at org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.ContigAlignmentsModifier.clipAlignmentInterval(ContigAlignmentsModifier.java:42); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.removeOverlap(CpxVariantInterpreter.java:179); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.deOverlapAlignments(CpxVariantInterpreter.java:122); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.furtherPreprocess(CpxVariantInterpreter.java:79); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.lambda$inferCpxVariant$bdd686a3$1(CpxVariantInterpreter.java:51); at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.jav",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4648
https://github.com/broadinstitute/gatk/issues/4648:2029,Energy Efficiency,schedul,scheduler,2029,.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.ContigAlignmentsModifier.computeNewRefSpanAndCigar(ContigAlignmentsModifier.java:159); at org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.ContigAlignmentsModifier.clipAlignmentInterval(ContigAlignmentsModifier.java:42); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.removeOverlap(CpxVariantInterpreter.java:179); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.deOverlapAlignments(CpxVariantInterpreter.java:122); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.furtherPreprocess(CpxVariantInterpreter.java:79); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.lambda$inferCpxVariant$bdd686a3$1(CpxVariantInterpreter.java:51); at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4648
https://github.com/broadinstitute/gatk/issues/4648:2108,Energy Efficiency,schedul,scheduler,2108,.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.ContigAlignmentsModifier.computeNewRefSpanAndCigar(ContigAlignmentsModifier.java:159); at org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.ContigAlignmentsModifier.clipAlignmentInterval(ContigAlignmentsModifier.java:42); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.removeOverlap(CpxVariantInterpreter.java:179); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.deOverlapAlignments(CpxVariantInterpreter.java:122); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.furtherPreprocess(CpxVariantInterpreter.java:79); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.lambda$inferCpxVariant$bdd686a3$1(CpxVariantInterpreter.java:51); at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4648
https://github.com/broadinstitute/gatk/issues/4648:2187,Energy Efficiency,schedul,scheduler,2187,.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.ContigAlignmentsModifier.computeNewRefSpanAndCigar(ContigAlignmentsModifier.java:159); at org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.ContigAlignmentsModifier.clipAlignmentInterval(ContigAlignmentsModifier.java:42); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.removeOverlap(CpxVariantInterpreter.java:179); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.deOverlapAlignments(CpxVariantInterpreter.java:122); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.furtherPreprocess(CpxVariantInterpreter.java:79); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.lambda$inferCpxVariant$bdd686a3$1(CpxVariantInterpreter.java:51); at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4648
https://github.com/broadinstitute/gatk/issues/4648:2310,Performance,concurren,concurrent,2310,.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.ContigAlignmentsModifier.computeNewRefSpanAndCigar(ContigAlignmentsModifier.java:159); at org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.ContigAlignmentsModifier.clipAlignmentInterval(ContigAlignmentsModifier.java:42); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.removeOverlap(CpxVariantInterpreter.java:179); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.deOverlapAlignments(CpxVariantInterpreter.java:122); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.furtherPreprocess(CpxVariantInterpreter.java:79); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.lambda$inferCpxVariant$bdd686a3$1(CpxVariantInterpreter.java:51); at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4648
https://github.com/broadinstitute/gatk/issues/4648:2394,Performance,concurren,concurrent,2394,.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.ContigAlignmentsModifier.computeNewRefSpanAndCigar(ContigAlignmentsModifier.java:159); at org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.ContigAlignmentsModifier.clipAlignmentInterval(ContigAlignmentsModifier.java:42); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.removeOverlap(CpxVariantInterpreter.java:179); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.deOverlapAlignments(CpxVariantInterpreter.java:122); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.furtherPreprocess(CpxVariantInterpreter.java:79); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.lambda$inferCpxVariant$bdd686a3$1(CpxVariantInterpreter.java:51); at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4648
https://github.com/broadinstitute/gatk/issues/4648:517,Security,validat,validateArg,517,"Several of our HGSV snapshot samples are failing with current master due to an exception in `CpxVariantInterpreter`. For example, sample HG00732 fails with this stacktrace:. ```; 18/04/11 14:30:28 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 2.0 in stage 42.0 (TID 60116, cwhelan-hg00732-cram-samtools-bam-feature-w-5.c.broad-dsde-methods.internal, executor 27): java.lang.IllegalArgumentException: Invalid interval. Contig:chr19 start:33757506 end:33757488; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.ContigAlignmentsModifier.computeNewRefSpanAndCigar(ContigAlignmentsModifier.java:159); at org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.ContigAlignmentsModifier.clipAlignmentInterval(ContigAlignmentsModifier.java:42); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.removeOverlap(CpxVariantInterpreter.java:179); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.deOverlapAlignments(CpxVariantInterpreter.java:122); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.furtherPreprocess(CpxVariantInterpreter.java:79); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.lambda$inferCpxVariant$bdd686a3$1(CpxVariantInterpreter.java:51); at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.jav",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4648
https://github.com/broadinstitute/gatk/issues/4648:600,Security,validat,validatePositions,600,"Several of our HGSV snapshot samples are failing with current master due to an exception in `CpxVariantInterpreter`. For example, sample HG00732 fails with this stacktrace:. ```; 18/04/11 14:30:28 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 2.0 in stage 42.0 (TID 60116, cwhelan-hg00732-cram-samtools-bam-feature-w-5.c.broad-dsde-methods.internal, executor 27): java.lang.IllegalArgumentException: Invalid interval. Contig:chr19 start:33757506 end:33757488; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.ContigAlignmentsModifier.computeNewRefSpanAndCigar(ContigAlignmentsModifier.java:159); at org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.ContigAlignmentsModifier.clipAlignmentInterval(ContigAlignmentsModifier.java:42); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.removeOverlap(CpxVariantInterpreter.java:179); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.deOverlapAlignments(CpxVariantInterpreter.java:122); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.furtherPreprocess(CpxVariantInterpreter.java:79); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.lambda$inferCpxVariant$bdd686a3$1(CpxVariantInterpreter.java:51); at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.jav",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4648
https://github.com/broadinstitute/gatk/issues/4648:585,Usability,Simpl,SimpleInterval,585,"Several of our HGSV snapshot samples are failing with current master due to an exception in `CpxVariantInterpreter`. For example, sample HG00732 fails with this stacktrace:. ```; 18/04/11 14:30:28 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 2.0 in stage 42.0 (TID 60116, cwhelan-hg00732-cram-samtools-bam-feature-w-5.c.broad-dsde-methods.internal, executor 27): java.lang.IllegalArgumentException: Invalid interval. Contig:chr19 start:33757506 end:33757488; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.ContigAlignmentsModifier.computeNewRefSpanAndCigar(ContigAlignmentsModifier.java:159); at org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.ContigAlignmentsModifier.clipAlignmentInterval(ContigAlignmentsModifier.java:42); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.removeOverlap(CpxVariantInterpreter.java:179); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.deOverlapAlignments(CpxVariantInterpreter.java:122); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.furtherPreprocess(CpxVariantInterpreter.java:79); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.lambda$inferCpxVariant$bdd686a3$1(CpxVariantInterpreter.java:51); at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.jav",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4648
https://github.com/broadinstitute/gatk/issues/4648:618,Usability,Simpl,SimpleInterval,618,"Several of our HGSV snapshot samples are failing with current master due to an exception in `CpxVariantInterpreter`. For example, sample HG00732 fails with this stacktrace:. ```; 18/04/11 14:30:28 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 2.0 in stage 42.0 (TID 60116, cwhelan-hg00732-cram-samtools-bam-feature-w-5.c.broad-dsde-methods.internal, executor 27): java.lang.IllegalArgumentException: Invalid interval. Contig:chr19 start:33757506 end:33757488; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.ContigAlignmentsModifier.computeNewRefSpanAndCigar(ContigAlignmentsModifier.java:159); at org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.ContigAlignmentsModifier.clipAlignmentInterval(ContigAlignmentsModifier.java:42); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.removeOverlap(CpxVariantInterpreter.java:179); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.deOverlapAlignments(CpxVariantInterpreter.java:122); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.furtherPreprocess(CpxVariantInterpreter.java:79); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.lambda$inferCpxVariant$bdd686a3$1(CpxVariantInterpreter.java:51); at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.jav",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4648
https://github.com/broadinstitute/gatk/issues/4648:682,Usability,Simpl,SimpleInterval,682,"Several of our HGSV snapshot samples are failing with current master due to an exception in `CpxVariantInterpreter`. For example, sample HG00732 fails with this stacktrace:. ```; 18/04/11 14:30:28 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 2.0 in stage 42.0 (TID 60116, cwhelan-hg00732-cram-samtools-bam-feature-w-5.c.broad-dsde-methods.internal, executor 27): java.lang.IllegalArgumentException: Invalid interval. Contig:chr19 start:33757506 end:33757488; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.ContigAlignmentsModifier.computeNewRefSpanAndCigar(ContigAlignmentsModifier.java:159); at org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.ContigAlignmentsModifier.clipAlignmentInterval(ContigAlignmentsModifier.java:42); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.removeOverlap(CpxVariantInterpreter.java:179); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.deOverlapAlignments(CpxVariantInterpreter.java:122); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.furtherPreprocess(CpxVariantInterpreter.java:79); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.lambda$inferCpxVariant$bdd686a3$1(CpxVariantInterpreter.java:51); at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.jav",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4648
https://github.com/broadinstitute/gatk/issues/4648:704,Usability,Simpl,SimpleInterval,704,"Several of our HGSV snapshot samples are failing with current master due to an exception in `CpxVariantInterpreter`. For example, sample HG00732 fails with this stacktrace:. ```; 18/04/11 14:30:28 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 2.0 in stage 42.0 (TID 60116, cwhelan-hg00732-cram-samtools-bam-feature-w-5.c.broad-dsde-methods.internal, executor 27): java.lang.IllegalArgumentException: Invalid interval. Contig:chr19 start:33757506 end:33757488; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.ContigAlignmentsModifier.computeNewRefSpanAndCigar(ContigAlignmentsModifier.java:159); at org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.ContigAlignmentsModifier.clipAlignmentInterval(ContigAlignmentsModifier.java:42); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.removeOverlap(CpxVariantInterpreter.java:179); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.deOverlapAlignments(CpxVariantInterpreter.java:122); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.furtherPreprocess(CpxVariantInterpreter.java:79); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.lambda$inferCpxVariant$bdd686a3$1(CpxVariantInterpreter.java:51); at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.jav",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4648
https://github.com/broadinstitute/gatk/issues/4649:116,Deployability,pipeline,pipeline,116,"When run with the current master build against our snapshot HG00514 sample, the experimental variant interpretation pipeline fails with the following exception:. ```; 18/04/11 20:27:50 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 32.0 in stage 42.0 (TID 56552, cwhelan-hg00514-1-cram-samtools-bam-feature-w-4.c.broad-dsde-methods.internal, executor 28): org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter$UnhandledCaseSeen: 1st segment is not overlapping with head alignment but it is not immediately before/after the head alignment either; AssemblyContigWithFineTunedAlignments{sourceTig=(asm022672:tig00004, [1_1430_chr9:130955309-130956738_-_1430M2216S_60_-1_-1_S, 1587_1763_chr9:130955156-130955308_-_1586S54M24I99M1883S_60_-1_-1_S, 1824_2015_chr9:130954964-130955155_-_1823S192M1631S_60_-1_-1_S, 2164_3646_chr9:130953867-130955307_-_2163H167M42I1274M_60_55_1318_O]), insertionMappings=[1963_2177_chr9:130955093-130955304_-_1962H179M3I33M1469H_19_14_138_O], hasEquallyGoodAlnConfigurations=false, saTAGForGoodMappingToNonCanonicalChromosome='NONE'}; at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantCanonicalRepresentation.extractAltHaplotypeSeq(CpxVariantCanonicalRepresentation.java:338); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantCanonicalRepresentation.<init>(CpxVariantCanonicalRepresentation.java:143); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.lambda$inferCpxVariant$b3be3b47$1(CpxVariantInterpreter.java:53); at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1043); at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1043); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149); at org.apache.spark.scheduler.ShuffleMapTask",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4649
https://github.com/broadinstitute/gatk/issues/4649:207,Energy Efficiency,schedul,scheduler,207,"When run with the current master build against our snapshot HG00514 sample, the experimental variant interpretation pipeline fails with the following exception:. ```; 18/04/11 20:27:50 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 32.0 in stage 42.0 (TID 56552, cwhelan-hg00514-1-cram-samtools-bam-feature-w-4.c.broad-dsde-methods.internal, executor 28): org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter$UnhandledCaseSeen: 1st segment is not overlapping with head alignment but it is not immediately before/after the head alignment either; AssemblyContigWithFineTunedAlignments{sourceTig=(asm022672:tig00004, [1_1430_chr9:130955309-130956738_-_1430M2216S_60_-1_-1_S, 1587_1763_chr9:130955156-130955308_-_1586S54M24I99M1883S_60_-1_-1_S, 1824_2015_chr9:130954964-130955155_-_1823S192M1631S_60_-1_-1_S, 2164_3646_chr9:130953867-130955307_-_2163H167M42I1274M_60_55_1318_O]), insertionMappings=[1963_2177_chr9:130955093-130955304_-_1962H179M3I33M1469H_19_14_138_O], hasEquallyGoodAlnConfigurations=false, saTAGForGoodMappingToNonCanonicalChromosome='NONE'}; at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantCanonicalRepresentation.extractAltHaplotypeSeq(CpxVariantCanonicalRepresentation.java:338); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantCanonicalRepresentation.<init>(CpxVariantCanonicalRepresentation.java:143); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.lambda$inferCpxVariant$b3be3b47$1(CpxVariantInterpreter.java:53); at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1043); at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1043); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149); at org.apache.spark.scheduler.ShuffleMapTask",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4649
https://github.com/broadinstitute/gatk/issues/4649:1977,Energy Efficiency,schedul,scheduler,1977,"ndledCaseSeen: 1st segment is not overlapping with head alignment but it is not immediately before/after the head alignment either; AssemblyContigWithFineTunedAlignments{sourceTig=(asm022672:tig00004, [1_1430_chr9:130955309-130956738_-_1430M2216S_60_-1_-1_S, 1587_1763_chr9:130955156-130955308_-_1586S54M24I99M1883S_60_-1_-1_S, 1824_2015_chr9:130954964-130955155_-_1823S192M1631S_60_-1_-1_S, 2164_3646_chr9:130953867-130955307_-_2163H167M42I1274M_60_55_1318_O]), insertionMappings=[1963_2177_chr9:130955093-130955304_-_1962H179M3I33M1469H_19_14_138_O], hasEquallyGoodAlnConfigurations=false, saTAGForGoodMappingToNonCanonicalChromosome='NONE'}; at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantCanonicalRepresentation.extractAltHaplotypeSeq(CpxVariantCanonicalRepresentation.java:338); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantCanonicalRepresentation.<init>(CpxVariantCanonicalRepresentation.java:143); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.lambda$inferCpxVariant$b3be3b47$1(CpxVariantInterpreter.java:53); at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1043); at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1043); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4649
https://github.com/broadinstitute/gatk/issues/4649:2056,Energy Efficiency,schedul,scheduler,2056,"ndledCaseSeen: 1st segment is not overlapping with head alignment but it is not immediately before/after the head alignment either; AssemblyContigWithFineTunedAlignments{sourceTig=(asm022672:tig00004, [1_1430_chr9:130955309-130956738_-_1430M2216S_60_-1_-1_S, 1587_1763_chr9:130955156-130955308_-_1586S54M24I99M1883S_60_-1_-1_S, 1824_2015_chr9:130954964-130955155_-_1823S192M1631S_60_-1_-1_S, 2164_3646_chr9:130953867-130955307_-_2163H167M42I1274M_60_55_1318_O]), insertionMappings=[1963_2177_chr9:130955093-130955304_-_1962H179M3I33M1469H_19_14_138_O], hasEquallyGoodAlnConfigurations=false, saTAGForGoodMappingToNonCanonicalChromosome='NONE'}; at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantCanonicalRepresentation.extractAltHaplotypeSeq(CpxVariantCanonicalRepresentation.java:338); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantCanonicalRepresentation.<init>(CpxVariantCanonicalRepresentation.java:143); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.lambda$inferCpxVariant$b3be3b47$1(CpxVariantInterpreter.java:53); at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1043); at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1043); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4649
https://github.com/broadinstitute/gatk/issues/4649:2135,Energy Efficiency,schedul,scheduler,2135,"ndledCaseSeen: 1st segment is not overlapping with head alignment but it is not immediately before/after the head alignment either; AssemblyContigWithFineTunedAlignments{sourceTig=(asm022672:tig00004, [1_1430_chr9:130955309-130956738_-_1430M2216S_60_-1_-1_S, 1587_1763_chr9:130955156-130955308_-_1586S54M24I99M1883S_60_-1_-1_S, 1824_2015_chr9:130954964-130955155_-_1823S192M1631S_60_-1_-1_S, 2164_3646_chr9:130953867-130955307_-_2163H167M42I1274M_60_55_1318_O]), insertionMappings=[1963_2177_chr9:130955093-130955304_-_1962H179M3I33M1469H_19_14_138_O], hasEquallyGoodAlnConfigurations=false, saTAGForGoodMappingToNonCanonicalChromosome='NONE'}; at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantCanonicalRepresentation.extractAltHaplotypeSeq(CpxVariantCanonicalRepresentation.java:338); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantCanonicalRepresentation.<init>(CpxVariantCanonicalRepresentation.java:143); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.lambda$inferCpxVariant$b3be3b47$1(CpxVariantInterpreter.java:53); at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1043); at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1043); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4649
https://github.com/broadinstitute/gatk/issues/4649:2258,Performance,concurren,concurrent,2258,"ndledCaseSeen: 1st segment is not overlapping with head alignment but it is not immediately before/after the head alignment either; AssemblyContigWithFineTunedAlignments{sourceTig=(asm022672:tig00004, [1_1430_chr9:130955309-130956738_-_1430M2216S_60_-1_-1_S, 1587_1763_chr9:130955156-130955308_-_1586S54M24I99M1883S_60_-1_-1_S, 1824_2015_chr9:130954964-130955155_-_1823S192M1631S_60_-1_-1_S, 2164_3646_chr9:130953867-130955307_-_2163H167M42I1274M_60_55_1318_O]), insertionMappings=[1963_2177_chr9:130955093-130955304_-_1962H179M3I33M1469H_19_14_138_O], hasEquallyGoodAlnConfigurations=false, saTAGForGoodMappingToNonCanonicalChromosome='NONE'}; at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantCanonicalRepresentation.extractAltHaplotypeSeq(CpxVariantCanonicalRepresentation.java:338); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantCanonicalRepresentation.<init>(CpxVariantCanonicalRepresentation.java:143); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.lambda$inferCpxVariant$b3be3b47$1(CpxVariantInterpreter.java:53); at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1043); at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1043); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4649
https://github.com/broadinstitute/gatk/issues/4649:2342,Performance,concurren,concurrent,2342,"ndledCaseSeen: 1st segment is not overlapping with head alignment but it is not immediately before/after the head alignment either; AssemblyContigWithFineTunedAlignments{sourceTig=(asm022672:tig00004, [1_1430_chr9:130955309-130956738_-_1430M2216S_60_-1_-1_S, 1587_1763_chr9:130955156-130955308_-_1586S54M24I99M1883S_60_-1_-1_S, 1824_2015_chr9:130954964-130955155_-_1823S192M1631S_60_-1_-1_S, 2164_3646_chr9:130953867-130955307_-_2163H167M42I1274M_60_55_1318_O]), insertionMappings=[1963_2177_chr9:130955093-130955304_-_1962H179M3I33M1469H_19_14_138_O], hasEquallyGoodAlnConfigurations=false, saTAGForGoodMappingToNonCanonicalChromosome='NONE'}; at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantCanonicalRepresentation.extractAltHaplotypeSeq(CpxVariantCanonicalRepresentation.java:338); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantCanonicalRepresentation.<init>(CpxVariantCanonicalRepresentation.java:143); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.lambda$inferCpxVariant$b3be3b47$1(CpxVariantInterpreter.java:53); at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1043); at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1043); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4649
https://github.com/broadinstitute/gatk/issues/4651:1001,Deployability,integrat,integrate,1001,"I found some edge cases when implemented a new walker based on GATKTool that should be considered in the base class to avoid some mistakes by developers. They should either being correctly handled or documented as not-applicable for some walkers:. - [ ] If a tool/walker overrides getPluginDescriptors to remove the `GATKReadFilterPluginDescriptor`, the method `makeReadFilter` will not return a filter with the `getDefaultReadFilters`. This is important for implementing tools where the user shouldn't be able to override the filters. I suggest to either handle the case where the plugin cannot be detected and return a filter with only defaults, or to specify in the documentation that `makeReadFilter` should be overriden in that case. - [ ] Transformer methods for reads (`makePreReadFilterTransformer`and `makePostReadFilterTransformer`) only have effect in `ReadWalker`(and extensions). I think that the `ReadsContext` should have a method to set pre/post transformers, and call this methods to integrate with every extension of `GATKTool`. Otherwise, it should be documented that it has no effect in most of the cases. @droazen - could you give me some way to proceed here? I think that the best way is to implement the proper behavior, but maybe the engine team has a different opinion...",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4651
https://github.com/broadinstitute/gatk/issues/4651:1001,Integrability,integrat,integrate,1001,"I found some edge cases when implemented a new walker based on GATKTool that should be considered in the base class to avoid some mistakes by developers. They should either being correctly handled or documented as not-applicable for some walkers:. - [ ] If a tool/walker overrides getPluginDescriptors to remove the `GATKReadFilterPluginDescriptor`, the method `makeReadFilter` will not return a filter with the `getDefaultReadFilters`. This is important for implementing tools where the user shouldn't be able to override the filters. I suggest to either handle the case where the plugin cannot be detected and return a filter with only defaults, or to specify in the documentation that `makeReadFilter` should be overriden in that case. - [ ] Transformer methods for reads (`makePreReadFilterTransformer`and `makePostReadFilterTransformer`) only have effect in `ReadWalker`(and extensions). I think that the `ReadsContext` should have a method to set pre/post transformers, and call this methods to integrate with every extension of `GATKTool`. Otherwise, it should be documented that it has no effect in most of the cases. @droazen - could you give me some way to proceed here? I think that the best way is to implement the proper behavior, but maybe the engine team has a different opinion...",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4651
https://github.com/broadinstitute/gatk/issues/4651:582,Modifiability,plugin,plugin,582,"I found some edge cases when implemented a new walker based on GATKTool that should be considered in the base class to avoid some mistakes by developers. They should either being correctly handled or documented as not-applicable for some walkers:. - [ ] If a tool/walker overrides getPluginDescriptors to remove the `GATKReadFilterPluginDescriptor`, the method `makeReadFilter` will not return a filter with the `getDefaultReadFilters`. This is important for implementing tools where the user shouldn't be able to override the filters. I suggest to either handle the case where the plugin cannot be detected and return a filter with only defaults, or to specify in the documentation that `makeReadFilter` should be overriden in that case. - [ ] Transformer methods for reads (`makePreReadFilterTransformer`and `makePostReadFilterTransformer`) only have effect in `ReadWalker`(and extensions). I think that the `ReadsContext` should have a method to set pre/post transformers, and call this methods to integrate with every extension of `GATKTool`. Otherwise, it should be documented that it has no effect in most of the cases. @droazen - could you give me some way to proceed here? I think that the best way is to implement the proper behavior, but maybe the engine team has a different opinion...",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4651
https://github.com/broadinstitute/gatk/issues/4651:119,Safety,avoid,avoid,119,"I found some edge cases when implemented a new walker based on GATKTool that should be considered in the base class to avoid some mistakes by developers. They should either being correctly handled or documented as not-applicable for some walkers:. - [ ] If a tool/walker overrides getPluginDescriptors to remove the `GATKReadFilterPluginDescriptor`, the method `makeReadFilter` will not return a filter with the `getDefaultReadFilters`. This is important for implementing tools where the user shouldn't be able to override the filters. I suggest to either handle the case where the plugin cannot be detected and return a filter with only defaults, or to specify in the documentation that `makeReadFilter` should be overriden in that case. - [ ] Transformer methods for reads (`makePreReadFilterTransformer`and `makePostReadFilterTransformer`) only have effect in `ReadWalker`(and extensions). I think that the `ReadsContext` should have a method to set pre/post transformers, and call this methods to integrate with every extension of `GATKTool`. Otherwise, it should be documented that it has no effect in most of the cases. @droazen - could you give me some way to proceed here? I think that the best way is to implement the proper behavior, but maybe the engine team has a different opinion...",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4651
https://github.com/broadinstitute/gatk/issues/4651:599,Safety,detect,detected,599,"I found some edge cases when implemented a new walker based on GATKTool that should be considered in the base class to avoid some mistakes by developers. They should either being correctly handled or documented as not-applicable for some walkers:. - [ ] If a tool/walker overrides getPluginDescriptors to remove the `GATKReadFilterPluginDescriptor`, the method `makeReadFilter` will not return a filter with the `getDefaultReadFilters`. This is important for implementing tools where the user shouldn't be able to override the filters. I suggest to either handle the case where the plugin cannot be detected and return a filter with only defaults, or to specify in the documentation that `makeReadFilter` should be overriden in that case. - [ ] Transformer methods for reads (`makePreReadFilterTransformer`and `makePostReadFilterTransformer`) only have effect in `ReadWalker`(and extensions). I think that the `ReadsContext` should have a method to set pre/post transformers, and call this methods to integrate with every extension of `GATKTool`. Otherwise, it should be documented that it has no effect in most of the cases. @droazen - could you give me some way to proceed here? I think that the best way is to implement the proper behavior, but maybe the engine team has a different opinion...",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4651
https://github.com/broadinstitute/gatk/issues/4654:41,Deployability,update,updated,41,Documentation for Funcotator needs to be updated to be consistent with current functionality.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4654
https://github.com/broadinstitute/gatk/pull/4656:305,Availability,checkpoint,checkpoint,305,This PR is the culmination of work from myself and @lbergelson to improve the runtime for MarkDuplicatesSpark on a single machine. This involved a rewrite of the tool as well as a number of improvements which should bring it into closer agreement with MarkDuplicates from picard. . Note: this is merely a checkpoint and there is still work that must be done to bring the work into agreement with recent MarkDuplicates development in picard. . Resolves #3706,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4656
https://github.com/broadinstitute/gatk/pull/4656:147,Modifiability,rewrite,rewrite,147,This PR is the culmination of work from myself and @lbergelson to improve the runtime for MarkDuplicatesSpark on a single machine. This involved a rewrite of the tool as well as a number of improvements which should bring it into closer agreement with MarkDuplicates from picard. . Note: this is merely a checkpoint and there is still work that must be done to bring the work into agreement with recent MarkDuplicates development in picard. . Resolves #3706,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4656
https://github.com/broadinstitute/gatk/issues/4657:363,Availability,error,errors,363,"I prepared a clean Bam file following GATK Best Practice and used GATK4 HaplotypeCaller to create a gvcf with ploidy1 option:. '''; gatk-4.0.2.1/gatk HaplotypeCaller --native-pair-hmm-threads 24 -I KU_filtered_sorted_mdup.bam -O HC.KU.raw.snps.indels.g.vcf -R ref.fasta -ploidy 1 --emit-ref-confidence GVCF; '''. When I validated the gvcf, ValidateVariants threw errors at the end:. '''; <br />11:27:55.681 INFO ProgressMeter - Traversal complete. Processed 124689522 total variants in 3.8 minutes.; 11:27:55.681 INFO ValidateVariants - Shutting down engine; [April 10, 2018 11:27:55 AM JST] org.broadinstitute.hellbender.tools.walkers.variantutils.ValidateVariants done. Elapsed time: 3.82 minutes.; Runtime.totalMemory()=4682940416; java.lang.IllegalArgumentException: Illegal character in path at index 15:HC.KU.raw.snps.indels.g.vcf; at java.net.URI.create(URI.java:852); at org.broadinstitute.hellbender.engine.FeatureInput.makeIntoAbsolutePath(FeatureInput.java:242); at org.broadinstitute.hellbender.engine.FeatureInput.toString(FeatureInput.java:314); at java.util.Formatter$FormatSpecifier.printString(Formatter.java:2886); at java.util.Formatter$FormatSpecifier.print(Formatter.java:2763); at java.util.Formatter.format(Formatter.java:2520); at java.util.Formatter.format(Formatter.java:2455); at java.lang.String.format(String.java:2940); at org.broadinstitute.hellbender.engine.FeatureDataSource.close(FeatureDataSource.java:589); at org.broadinstitute.hellbender.engine.FeatureManager.lambda$close$9(FeatureManager.java:505); at java.util.LinkedHashMap$LinkedValues.forEach(LinkedHashMap.java:608); at org.broadinstitute.hellbender.engine.FeatureManager.close(FeatureManager.java:505); at org.broadinstitute.hellbender.engine.GATKTool.onShutdown(GATKTool.java:857); at org.broadinstitute.hellbender.engine.VariantWalker.onShutdown(VariantWalker.java:95); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); at org.broadinstitute.hellbender.cm",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4657
https://github.com/broadinstitute/gatk/issues/4657:546,Availability,down,down,546,"I prepared a clean Bam file following GATK Best Practice and used GATK4 HaplotypeCaller to create a gvcf with ploidy1 option:. '''; gatk-4.0.2.1/gatk HaplotypeCaller --native-pair-hmm-threads 24 -I KU_filtered_sorted_mdup.bam -O HC.KU.raw.snps.indels.g.vcf -R ref.fasta -ploidy 1 --emit-ref-confidence GVCF; '''. When I validated the gvcf, ValidateVariants threw errors at the end:. '''; <br />11:27:55.681 INFO ProgressMeter - Traversal complete. Processed 124689522 total variants in 3.8 minutes.; 11:27:55.681 INFO ValidateVariants - Shutting down engine; [April 10, 2018 11:27:55 AM JST] org.broadinstitute.hellbender.tools.walkers.variantutils.ValidateVariants done. Elapsed time: 3.82 minutes.; Runtime.totalMemory()=4682940416; java.lang.IllegalArgumentException: Illegal character in path at index 15:HC.KU.raw.snps.indels.g.vcf; at java.net.URI.create(URI.java:852); at org.broadinstitute.hellbender.engine.FeatureInput.makeIntoAbsolutePath(FeatureInput.java:242); at org.broadinstitute.hellbender.engine.FeatureInput.toString(FeatureInput.java:314); at java.util.Formatter$FormatSpecifier.printString(Formatter.java:2886); at java.util.Formatter$FormatSpecifier.print(Formatter.java:2763); at java.util.Formatter.format(Formatter.java:2520); at java.util.Formatter.format(Formatter.java:2455); at java.lang.String.format(String.java:2940); at org.broadinstitute.hellbender.engine.FeatureDataSource.close(FeatureDataSource.java:589); at org.broadinstitute.hellbender.engine.FeatureManager.lambda$close$9(FeatureManager.java:505); at java.util.LinkedHashMap$LinkedValues.forEach(LinkedHashMap.java:608); at org.broadinstitute.hellbender.engine.FeatureManager.close(FeatureManager.java:505); at org.broadinstitute.hellbender.engine.GATKTool.onShutdown(GATKTool.java:857); at org.broadinstitute.hellbender.engine.VariantWalker.onShutdown(VariantWalker.java:95); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); at org.broadinstitute.hellbender.cm",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4657
https://github.com/broadinstitute/gatk/issues/4657:2948,Availability,error,errors,2948,"tter$FormatSpecifier.printString(Formatter.java:2886); at java.util.Formatter$FormatSpecifier.print(Formatter.java:2763); at java.util.Formatter.format(Formatter.java:2520); at java.util.Formatter.format(Formatter.java:2455); at java.lang.String.format(String.java:2940); at org.broadinstitute.hellbender.engine.FeatureDataSource.close(FeatureDataSource.java:589); at org.broadinstitute.hellbender.engine.FeatureManager.lambda$close$9(FeatureManager.java:505); at java.util.LinkedHashMap$LinkedValues.forEach(LinkedHashMap.java:608); at org.broadinstitute.hellbender.engine.FeatureManager.close(FeatureManager.java:505); at org.broadinstitute.hellbender.engine.GATKTool.onShutdown(GATKTool.java:857); at org.broadinstitute.hellbender.engine.VariantWalker.onShutdown(VariantWalker.java:95); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:180); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:199); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:159); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:202); at org.broadinstitute.hellbender.Main.main(Main.java:288); Caused by: java.net.URISyntaxException: Illegal character in path at index 15: /media/yoshi/My Book/Aet_v4.0_ChrSeqSplit/HC.KU-2103.raw.snps.indels.g.vcf; at java.net.URI$Parser.fail(URI.java:2848); at java.net.URI$Parser.checkChars(URI.java:3021); at java.net.URI$Parser.parseHierarchical(URI.java:3105); at java.net.URI$Parser.parse(URI.java:3063); at java.net.URI.<init>(URI.java:588); at java.net.URI.create(URI.java:850); ... 19 more; '''. When I ran GenotypeGVCFs using the gvcf, it ran to completion, but threw the same ""java.lang.IllegalArgumentException"" errors at the end. I have submitted a bug report to FTP server.; The name of the uploaded archive file is YoshiM_BugReport.tar.gz.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4657
https://github.com/broadinstitute/gatk/issues/4657:320,Security,validat,validated,320,"I prepared a clean Bam file following GATK Best Practice and used GATK4 HaplotypeCaller to create a gvcf with ploidy1 option:. '''; gatk-4.0.2.1/gatk HaplotypeCaller --native-pair-hmm-threads 24 -I KU_filtered_sorted_mdup.bam -O HC.KU.raw.snps.indels.g.vcf -R ref.fasta -ploidy 1 --emit-ref-confidence GVCF; '''. When I validated the gvcf, ValidateVariants threw errors at the end:. '''; <br />11:27:55.681 INFO ProgressMeter - Traversal complete. Processed 124689522 total variants in 3.8 minutes.; 11:27:55.681 INFO ValidateVariants - Shutting down engine; [April 10, 2018 11:27:55 AM JST] org.broadinstitute.hellbender.tools.walkers.variantutils.ValidateVariants done. Elapsed time: 3.82 minutes.; Runtime.totalMemory()=4682940416; java.lang.IllegalArgumentException: Illegal character in path at index 15:HC.KU.raw.snps.indels.g.vcf; at java.net.URI.create(URI.java:852); at org.broadinstitute.hellbender.engine.FeatureInput.makeIntoAbsolutePath(FeatureInput.java:242); at org.broadinstitute.hellbender.engine.FeatureInput.toString(FeatureInput.java:314); at java.util.Formatter$FormatSpecifier.printString(Formatter.java:2886); at java.util.Formatter$FormatSpecifier.print(Formatter.java:2763); at java.util.Formatter.format(Formatter.java:2520); at java.util.Formatter.format(Formatter.java:2455); at java.lang.String.format(String.java:2940); at org.broadinstitute.hellbender.engine.FeatureDataSource.close(FeatureDataSource.java:589); at org.broadinstitute.hellbender.engine.FeatureManager.lambda$close$9(FeatureManager.java:505); at java.util.LinkedHashMap$LinkedValues.forEach(LinkedHashMap.java:608); at org.broadinstitute.hellbender.engine.FeatureManager.close(FeatureManager.java:505); at org.broadinstitute.hellbender.engine.GATKTool.onShutdown(GATKTool.java:857); at org.broadinstitute.hellbender.engine.VariantWalker.onShutdown(VariantWalker.java:95); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); at org.broadinstitute.hellbender.cm",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4657
https://github.com/broadinstitute/gatk/issues/4657:340,Security,Validat,ValidateVariants,340,"I prepared a clean Bam file following GATK Best Practice and used GATK4 HaplotypeCaller to create a gvcf with ploidy1 option:. '''; gatk-4.0.2.1/gatk HaplotypeCaller --native-pair-hmm-threads 24 -I KU_filtered_sorted_mdup.bam -O HC.KU.raw.snps.indels.g.vcf -R ref.fasta -ploidy 1 --emit-ref-confidence GVCF; '''. When I validated the gvcf, ValidateVariants threw errors at the end:. '''; <br />11:27:55.681 INFO ProgressMeter - Traversal complete. Processed 124689522 total variants in 3.8 minutes.; 11:27:55.681 INFO ValidateVariants - Shutting down engine; [April 10, 2018 11:27:55 AM JST] org.broadinstitute.hellbender.tools.walkers.variantutils.ValidateVariants done. Elapsed time: 3.82 minutes.; Runtime.totalMemory()=4682940416; java.lang.IllegalArgumentException: Illegal character in path at index 15:HC.KU.raw.snps.indels.g.vcf; at java.net.URI.create(URI.java:852); at org.broadinstitute.hellbender.engine.FeatureInput.makeIntoAbsolutePath(FeatureInput.java:242); at org.broadinstitute.hellbender.engine.FeatureInput.toString(FeatureInput.java:314); at java.util.Formatter$FormatSpecifier.printString(Formatter.java:2886); at java.util.Formatter$FormatSpecifier.print(Formatter.java:2763); at java.util.Formatter.format(Formatter.java:2520); at java.util.Formatter.format(Formatter.java:2455); at java.lang.String.format(String.java:2940); at org.broadinstitute.hellbender.engine.FeatureDataSource.close(FeatureDataSource.java:589); at org.broadinstitute.hellbender.engine.FeatureManager.lambda$close$9(FeatureManager.java:505); at java.util.LinkedHashMap$LinkedValues.forEach(LinkedHashMap.java:608); at org.broadinstitute.hellbender.engine.FeatureManager.close(FeatureManager.java:505); at org.broadinstitute.hellbender.engine.GATKTool.onShutdown(GATKTool.java:857); at org.broadinstitute.hellbender.engine.VariantWalker.onShutdown(VariantWalker.java:95); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); at org.broadinstitute.hellbender.cm",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4657
https://github.com/broadinstitute/gatk/issues/4657:518,Security,Validat,ValidateVariants,518,"I prepared a clean Bam file following GATK Best Practice and used GATK4 HaplotypeCaller to create a gvcf with ploidy1 option:. '''; gatk-4.0.2.1/gatk HaplotypeCaller --native-pair-hmm-threads 24 -I KU_filtered_sorted_mdup.bam -O HC.KU.raw.snps.indels.g.vcf -R ref.fasta -ploidy 1 --emit-ref-confidence GVCF; '''. When I validated the gvcf, ValidateVariants threw errors at the end:. '''; <br />11:27:55.681 INFO ProgressMeter - Traversal complete. Processed 124689522 total variants in 3.8 minutes.; 11:27:55.681 INFO ValidateVariants - Shutting down engine; [April 10, 2018 11:27:55 AM JST] org.broadinstitute.hellbender.tools.walkers.variantutils.ValidateVariants done. Elapsed time: 3.82 minutes.; Runtime.totalMemory()=4682940416; java.lang.IllegalArgumentException: Illegal character in path at index 15:HC.KU.raw.snps.indels.g.vcf; at java.net.URI.create(URI.java:852); at org.broadinstitute.hellbender.engine.FeatureInput.makeIntoAbsolutePath(FeatureInput.java:242); at org.broadinstitute.hellbender.engine.FeatureInput.toString(FeatureInput.java:314); at java.util.Formatter$FormatSpecifier.printString(Formatter.java:2886); at java.util.Formatter$FormatSpecifier.print(Formatter.java:2763); at java.util.Formatter.format(Formatter.java:2520); at java.util.Formatter.format(Formatter.java:2455); at java.lang.String.format(String.java:2940); at org.broadinstitute.hellbender.engine.FeatureDataSource.close(FeatureDataSource.java:589); at org.broadinstitute.hellbender.engine.FeatureManager.lambda$close$9(FeatureManager.java:505); at java.util.LinkedHashMap$LinkedValues.forEach(LinkedHashMap.java:608); at org.broadinstitute.hellbender.engine.FeatureManager.close(FeatureManager.java:505); at org.broadinstitute.hellbender.engine.GATKTool.onShutdown(GATKTool.java:857); at org.broadinstitute.hellbender.engine.VariantWalker.onShutdown(VariantWalker.java:95); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); at org.broadinstitute.hellbender.cm",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4657
https://github.com/broadinstitute/gatk/issues/4657:649,Security,Validat,ValidateVariants,649,"I prepared a clean Bam file following GATK Best Practice and used GATK4 HaplotypeCaller to create a gvcf with ploidy1 option:. '''; gatk-4.0.2.1/gatk HaplotypeCaller --native-pair-hmm-threads 24 -I KU_filtered_sorted_mdup.bam -O HC.KU.raw.snps.indels.g.vcf -R ref.fasta -ploidy 1 --emit-ref-confidence GVCF; '''. When I validated the gvcf, ValidateVariants threw errors at the end:. '''; <br />11:27:55.681 INFO ProgressMeter - Traversal complete. Processed 124689522 total variants in 3.8 minutes.; 11:27:55.681 INFO ValidateVariants - Shutting down engine; [April 10, 2018 11:27:55 AM JST] org.broadinstitute.hellbender.tools.walkers.variantutils.ValidateVariants done. Elapsed time: 3.82 minutes.; Runtime.totalMemory()=4682940416; java.lang.IllegalArgumentException: Illegal character in path at index 15:HC.KU.raw.snps.indels.g.vcf; at java.net.URI.create(URI.java:852); at org.broadinstitute.hellbender.engine.FeatureInput.makeIntoAbsolutePath(FeatureInput.java:242); at org.broadinstitute.hellbender.engine.FeatureInput.toString(FeatureInput.java:314); at java.util.Formatter$FormatSpecifier.printString(Formatter.java:2886); at java.util.Formatter$FormatSpecifier.print(Formatter.java:2763); at java.util.Formatter.format(Formatter.java:2520); at java.util.Formatter.format(Formatter.java:2455); at java.lang.String.format(String.java:2940); at org.broadinstitute.hellbender.engine.FeatureDataSource.close(FeatureDataSource.java:589); at org.broadinstitute.hellbender.engine.FeatureManager.lambda$close$9(FeatureManager.java:505); at java.util.LinkedHashMap$LinkedValues.forEach(LinkedHashMap.java:608); at org.broadinstitute.hellbender.engine.FeatureManager.close(FeatureManager.java:505); at org.broadinstitute.hellbender.engine.GATKTool.onShutdown(GATKTool.java:857); at org.broadinstitute.hellbender.engine.VariantWalker.onShutdown(VariantWalker.java:95); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); at org.broadinstitute.hellbender.cm",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4657
https://github.com/broadinstitute/gatk/issues/4659:28,Deployability,configurat,configurations,28,"This can improve some build configurations for GATK (ony noted the ones in 4.6, but not previos ones):. * `failFast` property for test tasks. This would be useful for PRs; * Declare reasons for dependency resolution rules and constraint dependencies. This could be useful for explaining why some dependencies are not the latest (e.g., protobuf).; * Allow options in the command line. This could be nice for the doc generation.; * Default jacoco is 0.8.0, which improves the coverage report by filtering out some empty constructors",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4659
https://github.com/broadinstitute/gatk/issues/4659:194,Integrability,depend,dependency,194,"This can improve some build configurations for GATK (ony noted the ones in 4.6, but not previos ones):. * `failFast` property for test tasks. This would be useful for PRs; * Declare reasons for dependency resolution rules and constraint dependencies. This could be useful for explaining why some dependencies are not the latest (e.g., protobuf).; * Allow options in the command line. This could be nice for the doc generation.; * Default jacoco is 0.8.0, which improves the coverage report by filtering out some empty constructors",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4659
https://github.com/broadinstitute/gatk/issues/4659:237,Integrability,depend,dependencies,237,"This can improve some build configurations for GATK (ony noted the ones in 4.6, but not previos ones):. * `failFast` property for test tasks. This would be useful for PRs; * Declare reasons for dependency resolution rules and constraint dependencies. This could be useful for explaining why some dependencies are not the latest (e.g., protobuf).; * Allow options in the command line. This could be nice for the doc generation.; * Default jacoco is 0.8.0, which improves the coverage report by filtering out some empty constructors",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4659
https://github.com/broadinstitute/gatk/issues/4659:296,Integrability,depend,dependencies,296,"This can improve some build configurations for GATK (ony noted the ones in 4.6, but not previos ones):. * `failFast` property for test tasks. This would be useful for PRs; * Declare reasons for dependency resolution rules and constraint dependencies. This could be useful for explaining why some dependencies are not the latest (e.g., protobuf).; * Allow options in the command line. This could be nice for the doc generation.; * Default jacoco is 0.8.0, which improves the coverage report by filtering out some empty constructors",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4659
https://github.com/broadinstitute/gatk/issues/4659:28,Modifiability,config,configurations,28,"This can improve some build configurations for GATK (ony noted the ones in 4.6, but not previos ones):. * `failFast` property for test tasks. This would be useful for PRs; * Declare reasons for dependency resolution rules and constraint dependencies. This could be useful for explaining why some dependencies are not the latest (e.g., protobuf).; * Allow options in the command line. This could be nice for the doc generation.; * Default jacoco is 0.8.0, which improves the coverage report by filtering out some empty constructors",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4659
https://github.com/broadinstitute/gatk/issues/4659:130,Testability,test,test,130,"This can improve some build configurations for GATK (ony noted the ones in 4.6, but not previos ones):. * `failFast` property for test tasks. This would be useful for PRs; * Declare reasons for dependency resolution rules and constraint dependencies. This could be useful for explaining why some dependencies are not the latest (e.g., protobuf).; * Allow options in the command line. This could be nice for the doc generation.; * Default jacoco is 0.8.0, which improves the coverage report by filtering out some empty constructors",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4659
https://github.com/broadinstitute/gatk/issues/4660:4,Availability,down,downstream,4,"For downstream projects, there is a way to bundle single class files into the toolkit by adding them to the `Main.getClassList`; nevertheless, this only allows to include classes extending the `org.broadinstitute.hellbender.cmdline.CommandLineProgram`. For including a tool from picard, the only way is to add a whole package where `picard.cmdline.CommandLineProgram` extensions are located. Either a new method for include single classes from Picard should be added, or https://github.com/broadinstitute/barclay/issues/127 implemented (a proposal has been done in https://github.com/broadinstitute/barclay/pull/96) and move Picard/GATK to use the common interface.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4660
https://github.com/broadinstitute/gatk/issues/4660:655,Integrability,interface,interface,655,"For downstream projects, there is a way to bundle single class files into the toolkit by adding them to the `Main.getClassList`; nevertheless, this only allows to include classes extending the `org.broadinstitute.hellbender.cmdline.CommandLineProgram`. For including a tool from picard, the only way is to add a whole package where `picard.cmdline.CommandLineProgram` extensions are located. Either a new method for include single classes from Picard should be added, or https://github.com/broadinstitute/barclay/issues/127 implemented (a proposal has been done in https://github.com/broadinstitute/barclay/pull/96) and move Picard/GATK to use the common interface.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4660
https://github.com/broadinstitute/gatk/issues/4660:179,Modifiability,extend,extending,179,"For downstream projects, there is a way to bundle single class files into the toolkit by adding them to the `Main.getClassList`; nevertheless, this only allows to include classes extending the `org.broadinstitute.hellbender.cmdline.CommandLineProgram`. For including a tool from picard, the only way is to add a whole package where `picard.cmdline.CommandLineProgram` extensions are located. Either a new method for include single classes from Picard should be added, or https://github.com/broadinstitute/barclay/issues/127 implemented (a proposal has been done in https://github.com/broadinstitute/barclay/pull/96) and move Picard/GATK to use the common interface.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4660
https://github.com/broadinstitute/gatk/issues/4661:153,Availability,error,error,153,I'm running in intermittent issues when running HaplotypeCallerSpark with GATK 4.0.3.0 and was hoping to generate ideas to debug further. The underlying error is an index error when calculating likelihoods:; ```; java.lang.ArrayIndexOutOfBoundsException: 4; 	at org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypeLikelihoodCalculators.calculateGenotypeCountUsingTables(GenotypeLikelihoodCalculators.java:388); ```; I've been unable to generate a reproducible test case. Re-running on the same machine (Amazon m4.4xlarge instances with 16 cores and 64Gb of memory) works. I've seen the error on two different datasets but it happens infrequently as I've also run hundreds using the same setup without any exceptions. The only other thing I spot when looking through the traceback is block issues about the RDDs but I'm not sure if these are a symptom of the failure or a cause:; ```; 18/04/15 03:55:19 WARN BlockManager: Putting block rdd_18_12 failed due to an exception; 18/04/15 03:55:19 WARN BlockManager: Block rdd_18_12 could not be removed as it was not found on disk or in memory; ```; Here's the full traceback of the failure:; ```; [2018-04-15T03:55Z] ip-10-0-0-57: 18/04/15 03:55:19 WARN BlockManager: Putting block rdd_18_12 failed due to an exception; [2018-04-15T03:55Z] ip-10-0-0-57: 18/04/15 03:55:19 WARN BlockManager: Block rdd_18_12 could not be removed as it was not found on disk or in memory; [2018-04-15T03:55Z] ip-10-0-0-57: 18/04/15 03:55:19 ERROR Executor: Exception in task 12.0 in stage 7.0 (TID 828); [2018-04-15T03:55Z] ip-10-0-0-57: java.lang.ArrayIndexOutOfBoundsException: 4; [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypeLikelihoodCalculators.calculateGenotypeCountUsingTables(GenotypeLikelihoodCalculators.java:388); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypeLikelihoodCalculators.getInstance(GenotypeLikelihoodCalculators.java:263); [2018-04-1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4661
https://github.com/broadinstitute/gatk/issues/4661:171,Availability,error,error,171,I'm running in intermittent issues when running HaplotypeCallerSpark with GATK 4.0.3.0 and was hoping to generate ideas to debug further. The underlying error is an index error when calculating likelihoods:; ```; java.lang.ArrayIndexOutOfBoundsException: 4; 	at org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypeLikelihoodCalculators.calculateGenotypeCountUsingTables(GenotypeLikelihoodCalculators.java:388); ```; I've been unable to generate a reproducible test case. Re-running on the same machine (Amazon m4.4xlarge instances with 16 cores and 64Gb of memory) works. I've seen the error on two different datasets but it happens infrequently as I've also run hundreds using the same setup without any exceptions. The only other thing I spot when looking through the traceback is block issues about the RDDs but I'm not sure if these are a symptom of the failure or a cause:; ```; 18/04/15 03:55:19 WARN BlockManager: Putting block rdd_18_12 failed due to an exception; 18/04/15 03:55:19 WARN BlockManager: Block rdd_18_12 could not be removed as it was not found on disk or in memory; ```; Here's the full traceback of the failure:; ```; [2018-04-15T03:55Z] ip-10-0-0-57: 18/04/15 03:55:19 WARN BlockManager: Putting block rdd_18_12 failed due to an exception; [2018-04-15T03:55Z] ip-10-0-0-57: 18/04/15 03:55:19 WARN BlockManager: Block rdd_18_12 could not be removed as it was not found on disk or in memory; [2018-04-15T03:55Z] ip-10-0-0-57: 18/04/15 03:55:19 ERROR Executor: Exception in task 12.0 in stage 7.0 (TID 828); [2018-04-15T03:55Z] ip-10-0-0-57: java.lang.ArrayIndexOutOfBoundsException: 4; [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypeLikelihoodCalculators.calculateGenotypeCountUsingTables(GenotypeLikelihoodCalculators.java:388); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypeLikelihoodCalculators.getInstance(GenotypeLikelihoodCalculators.java:263); [2018-04-1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4661
https://github.com/broadinstitute/gatk/issues/4661:596,Availability,error,error,596,I'm running in intermittent issues when running HaplotypeCallerSpark with GATK 4.0.3.0 and was hoping to generate ideas to debug further. The underlying error is an index error when calculating likelihoods:; ```; java.lang.ArrayIndexOutOfBoundsException: 4; 	at org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypeLikelihoodCalculators.calculateGenotypeCountUsingTables(GenotypeLikelihoodCalculators.java:388); ```; I've been unable to generate a reproducible test case. Re-running on the same machine (Amazon m4.4xlarge instances with 16 cores and 64Gb of memory) works. I've seen the error on two different datasets but it happens infrequently as I've also run hundreds using the same setup without any exceptions. The only other thing I spot when looking through the traceback is block issues about the RDDs but I'm not sure if these are a symptom of the failure or a cause:; ```; 18/04/15 03:55:19 WARN BlockManager: Putting block rdd_18_12 failed due to an exception; 18/04/15 03:55:19 WARN BlockManager: Block rdd_18_12 could not be removed as it was not found on disk or in memory; ```; Here's the full traceback of the failure:; ```; [2018-04-15T03:55Z] ip-10-0-0-57: 18/04/15 03:55:19 WARN BlockManager: Putting block rdd_18_12 failed due to an exception; [2018-04-15T03:55Z] ip-10-0-0-57: 18/04/15 03:55:19 WARN BlockManager: Block rdd_18_12 could not be removed as it was not found on disk or in memory; [2018-04-15T03:55Z] ip-10-0-0-57: 18/04/15 03:55:19 ERROR Executor: Exception in task 12.0 in stage 7.0 (TID 828); [2018-04-15T03:55Z] ip-10-0-0-57: java.lang.ArrayIndexOutOfBoundsException: 4; [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypeLikelihoodCalculators.calculateGenotypeCountUsingTables(GenotypeLikelihoodCalculators.java:388); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypeLikelihoodCalculators.getInstance(GenotypeLikelihoodCalculators.java:263); [2018-04-1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4661
https://github.com/broadinstitute/gatk/issues/4661:868,Availability,failure,failure,868,to debug further. The underlying error is an index error when calculating likelihoods:; ```; java.lang.ArrayIndexOutOfBoundsException: 4; 	at org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypeLikelihoodCalculators.calculateGenotypeCountUsingTables(GenotypeLikelihoodCalculators.java:388); ```; I've been unable to generate a reproducible test case. Re-running on the same machine (Amazon m4.4xlarge instances with 16 cores and 64Gb of memory) works. I've seen the error on two different datasets but it happens infrequently as I've also run hundreds using the same setup without any exceptions. The only other thing I spot when looking through the traceback is block issues about the RDDs but I'm not sure if these are a symptom of the failure or a cause:; ```; 18/04/15 03:55:19 WARN BlockManager: Putting block rdd_18_12 failed due to an exception; 18/04/15 03:55:19 WARN BlockManager: Block rdd_18_12 could not be removed as it was not found on disk or in memory; ```; Here's the full traceback of the failure:; ```; [2018-04-15T03:55Z] ip-10-0-0-57: 18/04/15 03:55:19 WARN BlockManager: Putting block rdd_18_12 failed due to an exception; [2018-04-15T03:55Z] ip-10-0-0-57: 18/04/15 03:55:19 WARN BlockManager: Block rdd_18_12 could not be removed as it was not found on disk or in memory; [2018-04-15T03:55Z] ip-10-0-0-57: 18/04/15 03:55:19 ERROR Executor: Exception in task 12.0 in stage 7.0 (TID 828); [2018-04-15T03:55Z] ip-10-0-0-57: java.lang.ArrayIndexOutOfBoundsException: 4; [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypeLikelihoodCalculators.calculateGenotypeCountUsingTables(GenotypeLikelihoodCalculators.java:388); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypeLikelihoodCalculators.getInstance(GenotypeLikelihoodCalculators.java:263); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.makeGenotypeCall(GATKVa,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4661
https://github.com/broadinstitute/gatk/issues/4661:1137,Availability,failure,failure,1137,to debug further. The underlying error is an index error when calculating likelihoods:; ```; java.lang.ArrayIndexOutOfBoundsException: 4; 	at org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypeLikelihoodCalculators.calculateGenotypeCountUsingTables(GenotypeLikelihoodCalculators.java:388); ```; I've been unable to generate a reproducible test case. Re-running on the same machine (Amazon m4.4xlarge instances with 16 cores and 64Gb of memory) works. I've seen the error on two different datasets but it happens infrequently as I've also run hundreds using the same setup without any exceptions. The only other thing I spot when looking through the traceback is block issues about the RDDs but I'm not sure if these are a symptom of the failure or a cause:; ```; 18/04/15 03:55:19 WARN BlockManager: Putting block rdd_18_12 failed due to an exception; 18/04/15 03:55:19 WARN BlockManager: Block rdd_18_12 could not be removed as it was not found on disk or in memory; ```; Here's the full traceback of the failure:; ```; [2018-04-15T03:55Z] ip-10-0-0-57: 18/04/15 03:55:19 WARN BlockManager: Putting block rdd_18_12 failed due to an exception; [2018-04-15T03:55Z] ip-10-0-0-57: 18/04/15 03:55:19 WARN BlockManager: Block rdd_18_12 could not be removed as it was not found on disk or in memory; [2018-04-15T03:55Z] ip-10-0-0-57: 18/04/15 03:55:19 ERROR Executor: Exception in task 12.0 in stage 7.0 (TID 828); [2018-04-15T03:55Z] ip-10-0-0-57: java.lang.ArrayIndexOutOfBoundsException: 4; [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypeLikelihoodCalculators.calculateGenotypeCountUsingTables(GenotypeLikelihoodCalculators.java:388); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypeLikelihoodCalculators.getInstance(GenotypeLikelihoodCalculators.java:263); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.makeGenotypeCall(GATKVa,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4661
https://github.com/broadinstitute/gatk/issues/4661:1477,Availability,ERROR,ERROR,1477,to debug further. The underlying error is an index error when calculating likelihoods:; ```; java.lang.ArrayIndexOutOfBoundsException: 4; 	at org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypeLikelihoodCalculators.calculateGenotypeCountUsingTables(GenotypeLikelihoodCalculators.java:388); ```; I've been unable to generate a reproducible test case. Re-running on the same machine (Amazon m4.4xlarge instances with 16 cores and 64Gb of memory) works. I've seen the error on two different datasets but it happens infrequently as I've also run hundreds using the same setup without any exceptions. The only other thing I spot when looking through the traceback is block issues about the RDDs but I'm not sure if these are a symptom of the failure or a cause:; ```; 18/04/15 03:55:19 WARN BlockManager: Putting block rdd_18_12 failed due to an exception; 18/04/15 03:55:19 WARN BlockManager: Block rdd_18_12 could not be removed as it was not found on disk or in memory; ```; Here's the full traceback of the failure:; ```; [2018-04-15T03:55Z] ip-10-0-0-57: 18/04/15 03:55:19 WARN BlockManager: Putting block rdd_18_12 failed due to an exception; [2018-04-15T03:55Z] ip-10-0-0-57: 18/04/15 03:55:19 WARN BlockManager: Block rdd_18_12 could not be removed as it was not found on disk or in memory; [2018-04-15T03:55Z] ip-10-0-0-57: 18/04/15 03:55:19 ERROR Executor: Exception in task 12.0 in stage 7.0 (TID 828); [2018-04-15T03:55Z] ip-10-0-0-57: java.lang.ArrayIndexOutOfBoundsException: 4; [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypeLikelihoodCalculators.calculateGenotypeCountUsingTables(GenotypeLikelihoodCalculators.java:388); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypeLikelihoodCalculators.getInstance(GenotypeLikelihoodCalculators.java:263); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.makeGenotypeCall(GATKVa,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4661
https://github.com/broadinstitute/gatk/issues/4661:6041,Energy Efficiency,schedul,scheduler,6041,"d.RDD.iterator(RDD.scala:285); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.apache.spark.scheduler.Task.run(Task.scala:108); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); [2018-04-15T03:55Z] ip-10-0-0-57: 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); [2018-04-15T03:55Z] ip-10-0-0-57: 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); [2018-04-15T03:55Z] ip-10-0-0-57: 	at java.lang.Thread.run(Thread.java:745); ```; and in case I am missing anything in how I'm calling HaplotypeCallerSpark, here is the full command line we're using:; ```; gatk-launch --java-options '-Xms1000m -Xmx46965m -Djava.io.tmpdir=/mnt/work/cwl/bcbio_validation_workflows/giab-joint/bunny_work/main-giab-joint-2018-04-14-195952.723/root/variantcall/2/variantcall_batch_region/3/bcbiotx/tmpno7wyh' HaplotypeCallerSpark --reference /mnt/work/cwl/bcbio_validation_workflows/giab",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4661
https://github.com/broadinstitute/gatk/issues/4661:6147,Energy Efficiency,schedul,scheduler,6147,".compute(MapPartitionsRDD.scala:38); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.apache.spark.scheduler.Task.run(Task.scala:108); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); [2018-04-15T03:55Z] ip-10-0-0-57: 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); [2018-04-15T03:55Z] ip-10-0-0-57: 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); [2018-04-15T03:55Z] ip-10-0-0-57: 	at java.lang.Thread.run(Thread.java:745); ```; and in case I am missing anything in how I'm calling HaplotypeCallerSpark, here is the full command line we're using:; ```; gatk-launch --java-options '-Xms1000m -Xmx46965m -Djava.io.tmpdir=/mnt/work/cwl/bcbio_validation_workflows/giab-joint/bunny_work/main-giab-joint-2018-04-14-195952.723/root/variantcall/2/variantcall_batch_region/3/bcbiotx/tmpno7wyh' HaplotypeCallerSpark --reference /mnt/work/cwl/bcbio_validation_workflows/giab-joint/biodata/collections/hg38/ucsc/hg38.2bit --annotation MappingQualityRankSumTest --annotation Mapping",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4661
https://github.com/broadinstitute/gatk/issues/4661:3441,Integrability,Wrap,WrappingSpliterator,3441,typingEngine.java:296); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypingEngine.calculateGenotypes(GenotypingEngine.java:210); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerGenotypingEngine.assignGenotypeLikelihoods(HaplotypeCallerGenotypingEngine.java:150); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:565); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.lambda$regionToVariants$1(HaplotypeCallerSpark.java:271); [2018-04-15T03:55Z] ip-10-0-0-57: 	at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:267); [2018-04-15T03:55Z] ip-10-0-0-57: 	at java.util.Spliterators$IteratorSpliterator.tryAdvance(Spliterators.java:1812); [2018-04-15T03:55Z] ip-10-0-0-57: 	at java.util.stream.StreamSpliterators$WrappingSpliterator.lambda$initPartialTraversalState$0(StreamSpliterators.java:294); [2018-04-15T03:55Z] ip-10-0-0-57: 	at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.fillBuffer(StreamSpliterators.java:206); [2018-04-15T03:55Z] ip-10-0-0-57: 	at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.doAdvance(StreamSpliterators.java:169); [2018-04-15T03:55Z] ip-10-0-0-57: 	at java.util.stream.StreamSpliterators$WrappingSpliterator.tryAdvance(StreamSpliterators.java:300); [2018-04-15T03:55Z] ip-10-0-0-57: 	at java.util.Spliterators$1Adapter.hasNext(Spliterators.java:681); [2018-04-15T03:55Z] ip-10-0-0-57: 	at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:215); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1038); [2018-04-15T03:55Z,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4661
https://github.com/broadinstitute/gatk/issues/4661:3885,Integrability,Wrap,WrappingSpliterator,3885,er.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:565); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.lambda$regionToVariants$1(HaplotypeCallerSpark.java:271); [2018-04-15T03:55Z] ip-10-0-0-57: 	at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:267); [2018-04-15T03:55Z] ip-10-0-0-57: 	at java.util.Spliterators$IteratorSpliterator.tryAdvance(Spliterators.java:1812); [2018-04-15T03:55Z] ip-10-0-0-57: 	at java.util.stream.StreamSpliterators$WrappingSpliterator.lambda$initPartialTraversalState$0(StreamSpliterators.java:294); [2018-04-15T03:55Z] ip-10-0-0-57: 	at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.fillBuffer(StreamSpliterators.java:206); [2018-04-15T03:55Z] ip-10-0-0-57: 	at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.doAdvance(StreamSpliterators.java:169); [2018-04-15T03:55Z] ip-10-0-0-57: 	at java.util.stream.StreamSpliterators$WrappingSpliterator.tryAdvance(StreamSpliterators.java:300); [2018-04-15T03:55Z] ip-10-0-0-57: 	at java.util.Spliterators$1Adapter.hasNext(Spliterators.java:681); [2018-04-15T03:55Z] ip-10-0-0-57: 	at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:215); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1038); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1029); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:969); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1029); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.s,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4661
https://github.com/broadinstitute/gatk/issues/4661:4111,Integrability,Wrap,Wrappers,4111,k.java:271); [2018-04-15T03:55Z] ip-10-0-0-57: 	at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:267); [2018-04-15T03:55Z] ip-10-0-0-57: 	at java.util.Spliterators$IteratorSpliterator.tryAdvance(Spliterators.java:1812); [2018-04-15T03:55Z] ip-10-0-0-57: 	at java.util.stream.StreamSpliterators$WrappingSpliterator.lambda$initPartialTraversalState$0(StreamSpliterators.java:294); [2018-04-15T03:55Z] ip-10-0-0-57: 	at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.fillBuffer(StreamSpliterators.java:206); [2018-04-15T03:55Z] ip-10-0-0-57: 	at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.doAdvance(StreamSpliterators.java:169); [2018-04-15T03:55Z] ip-10-0-0-57: 	at java.util.stream.StreamSpliterators$WrappingSpliterator.tryAdvance(StreamSpliterators.java:300); [2018-04-15T03:55Z] ip-10-0-0-57: 	at java.util.Spliterators$1Adapter.hasNext(Spliterators.java:681); [2018-04-15T03:55Z] ip-10-0-0-57: 	at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:215); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1038); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1029); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:969); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1029); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:760); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.apach,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4661
https://github.com/broadinstitute/gatk/issues/4661:4145,Integrability,Wrap,Wrappers,4145,-15T03:55Z] ip-10-0-0-57: 	at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:267); [2018-04-15T03:55Z] ip-10-0-0-57: 	at java.util.Spliterators$IteratorSpliterator.tryAdvance(Spliterators.java:1812); [2018-04-15T03:55Z] ip-10-0-0-57: 	at java.util.stream.StreamSpliterators$WrappingSpliterator.lambda$initPartialTraversalState$0(StreamSpliterators.java:294); [2018-04-15T03:55Z] ip-10-0-0-57: 	at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.fillBuffer(StreamSpliterators.java:206); [2018-04-15T03:55Z] ip-10-0-0-57: 	at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.doAdvance(StreamSpliterators.java:169); [2018-04-15T03:55Z] ip-10-0-0-57: 	at java.util.stream.StreamSpliterators$WrappingSpliterator.tryAdvance(StreamSpliterators.java:300); [2018-04-15T03:55Z] ip-10-0-0-57: 	at java.util.Spliterators$1Adapter.hasNext(Spliterators.java:681); [2018-04-15T03:55Z] ip-10-0-0-57: 	at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:215); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1038); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1029); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:969); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1029); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:760); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.apache.spark.rdd.MapPartiti,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4661
https://github.com/broadinstitute/gatk/issues/4661:6340,Performance,concurren,concurrent,6340,"k.rdd.RDD.iterator(RDD.scala:287); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.apache.spark.scheduler.Task.run(Task.scala:108); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); [2018-04-15T03:55Z] ip-10-0-0-57: 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); [2018-04-15T03:55Z] ip-10-0-0-57: 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); [2018-04-15T03:55Z] ip-10-0-0-57: 	at java.lang.Thread.run(Thread.java:745); ```; and in case I am missing anything in how I'm calling HaplotypeCallerSpark, here is the full command line we're using:; ```; gatk-launch --java-options '-Xms1000m -Xmx46965m -Djava.io.tmpdir=/mnt/work/cwl/bcbio_validation_workflows/giab-joint/bunny_work/main-giab-joint-2018-04-14-195952.723/root/variantcall/2/variantcall_batch_region/3/bcbiotx/tmpno7wyh' HaplotypeCallerSpark --reference /mnt/work/cwl/bcbio_validation_workflows/giab-joint/biodata/collections/hg38/ucsc/hg38.2bit --annotation MappingQualityRankSumTest --annotation MappingQualityZero --annotation QualByDepth --annotation ReadPosRankSumTest --annotation RMSMappingQuality --annotation BaseQualityRankSumTest --annotation FisherStrand --annotation MappingQuality --an",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4661
https://github.com/broadinstitute/gatk/issues/4661:6459,Performance,concurren,concurrent,6459,"MapPartitionsRDD.scala:38); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.apache.spark.scheduler.Task.run(Task.scala:108); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); [2018-04-15T03:55Z] ip-10-0-0-57: 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); [2018-04-15T03:55Z] ip-10-0-0-57: 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); [2018-04-15T03:55Z] ip-10-0-0-57: 	at java.lang.Thread.run(Thread.java:745); ```; and in case I am missing anything in how I'm calling HaplotypeCallerSpark, here is the full command line we're using:; ```; gatk-launch --java-options '-Xms1000m -Xmx46965m -Djava.io.tmpdir=/mnt/work/cwl/bcbio_validation_workflows/giab-joint/bunny_work/main-giab-joint-2018-04-14-195952.723/root/variantcall/2/variantcall_batch_region/3/bcbiotx/tmpno7wyh' HaplotypeCallerSpark --reference /mnt/work/cwl/bcbio_validation_workflows/giab-joint/biodata/collections/hg38/ucsc/hg38.2bit --annotation MappingQualityRankSumTest --annotation MappingQualityZero --annotation QualByDepth --annotation ReadPosRankSumTest --annotation RMSMappingQuality --annotation BaseQualityRankSumTest --annotation FisherStrand --annotation MappingQuality --annotation DepthPerAlleleBySample --annotation Coverage -I /mnt/work/cwl/bcbio_validation_workflows/giab-joint/bunny_work",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4661
https://github.com/broadinstitute/gatk/issues/4661:8444,Safety,avoid,avoid,8444,"oncurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); [2018-04-15T03:55Z] ip-10-0-0-57: 	at java.lang.Thread.run(Thread.java:745); ```; and in case I am missing anything in how I'm calling HaplotypeCallerSpark, here is the full command line we're using:; ```; gatk-launch --java-options '-Xms1000m -Xmx46965m -Djava.io.tmpdir=/mnt/work/cwl/bcbio_validation_workflows/giab-joint/bunny_work/main-giab-joint-2018-04-14-195952.723/root/variantcall/2/variantcall_batch_region/3/bcbiotx/tmpno7wyh' HaplotypeCallerSpark --reference /mnt/work/cwl/bcbio_validation_workflows/giab-joint/biodata/collections/hg38/ucsc/hg38.2bit --annotation MappingQualityRankSumTest --annotation MappingQualityZero --annotation QualByDepth --annotation ReadPosRankSumTest --annotation RMSMappingQuality --annotation BaseQualityRankSumTest --annotation FisherStrand --annotation MappingQuality --annotation DepthPerAlleleBySample --annotation Coverage -I /mnt/work/cwl/bcbio_validation_workflows/giab-joint/bunny_work/main-giab-joint-2018-04-14-195952.723/root/alignment/1/process_alignment/align/NA24385/NA24385-sort.bam -L /mnt/work/cwl/bcbio_validation_workflows/giab-joint/bunny_work/main-giab-joint-2018-04-14-195952.723/root/variantcall/2/variantcall_batch_region/3/gatk-haplotype/chr15/NA24385-chr15_0_101977614-block-regions.bed --interval-set-rule INTERSECTION --spark-master local[16] --conf spark.local.dir=/mnt/work/cwl/bcbio_validation_workflows/giab-joint/bunny_work/main-giab-joint-2018-04-14-195952.723/root/variantcall/2/variantcall_batch_region/3/bcbiotx/tmpno7wyh --annotation ClippingRankSumTest --annotation DepthPerSampleHC --emit-ref-confidence GVCF -GQB 10 -GQB 20 -GQB 30 -GQB 40 -GQB 60 -GQB 80 --output /mnt/work/cwl/bcbio_validation_workflows/giab-joint/bunny_work/main-giab-joint-2018-04-14-195952.723/root/variantcall/2/variantcall_batch_region/3/bcbiotx/tmpno7wyh/NA24385-chr15_0_101977614-block.vcf; ```; Thanks so much for any clues about how to debug further or avoid the issue.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4661
https://github.com/broadinstitute/gatk/issues/4661:470,Testability,test,test,470,I'm running in intermittent issues when running HaplotypeCallerSpark with GATK 4.0.3.0 and was hoping to generate ideas to debug further. The underlying error is an index error when calculating likelihoods:; ```; java.lang.ArrayIndexOutOfBoundsException: 4; 	at org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypeLikelihoodCalculators.calculateGenotypeCountUsingTables(GenotypeLikelihoodCalculators.java:388); ```; I've been unable to generate a reproducible test case. Re-running on the same machine (Amazon m4.4xlarge instances with 16 cores and 64Gb of memory) works. I've seen the error on two different datasets but it happens infrequently as I've also run hundreds using the same setup without any exceptions. The only other thing I spot when looking through the traceback is block issues about the RDDs but I'm not sure if these are a symptom of the failure or a cause:; ```; 18/04/15 03:55:19 WARN BlockManager: Putting block rdd_18_12 failed due to an exception; 18/04/15 03:55:19 WARN BlockManager: Block rdd_18_12 could not be removed as it was not found on disk or in memory; ```; Here's the full traceback of the failure:; ```; [2018-04-15T03:55Z] ip-10-0-0-57: 18/04/15 03:55:19 WARN BlockManager: Putting block rdd_18_12 failed due to an exception; [2018-04-15T03:55Z] ip-10-0-0-57: 18/04/15 03:55:19 WARN BlockManager: Block rdd_18_12 could not be removed as it was not found on disk or in memory; [2018-04-15T03:55Z] ip-10-0-0-57: 18/04/15 03:55:19 ERROR Executor: Exception in task 12.0 in stage 7.0 (TID 828); [2018-04-15T03:55Z] ip-10-0-0-57: java.lang.ArrayIndexOutOfBoundsException: 4; [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypeLikelihoodCalculators.calculateGenotypeCountUsingTables(GenotypeLikelihoodCalculators.java:388); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypeLikelihoodCalculators.getInstance(GenotypeLikelihoodCalculators.java:263); [2018-04-1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4661
https://github.com/broadinstitute/gatk/pull/4663:1515,Availability,avail,available,1515,"awTypes` is moved to `AssemblyContigWithFineTunedAlignments.AlignmentSignatureBasicTypes` and reduced into fewer cases (`Suspicious`, `Simple` and `Complex`). - [x] static method `BreakpointsInference.inferFromSimpleChimera()` now moved to state query method `ChimericAlignment.inferType()`. - [x] `AssemblyContigWithFineTunedAlignments.hasIncompletePictureFromTwoAlignments()` merged with `ChimericAlignment.hasIncompletePicture()`. ### update how variants are represented. - [x] change `SVLEN` for `CPX` variants to the difference between _[alt haplotype sequence length]_ and _[affected reference region length]_, which is following the technical definition of `SVLEN` in VCF spec. - [x] change `RPL` output to one of these (note that test coverage is expected); - [x] ins/del, when del/ins bases are < 50 and annotate; when type is determined as ins, the POS will be 1 base before the micro-deleted range and END will be end of the micro-deleted range, where the REF allele will be the corresponding reference bases.; - [x] ins and del when both are >= 50, and link by `EVENT`. - [x] change `SVTYPE=DUP` to`SVYTPE=INS` when the duplicated region is shorter than 50 bp (tests). Note that this will lead to `INS` records with `DUP_REPEAT_UNIT_REF_SPAN` and `DUP_SEQ_CIGARS` (when available). In addition, we are currently treating duplication expansion as insertion. ; The VCF spec doesn't force `DUP` records as such.; If we decide to allow `POS` and `END` to designate the beginning and end of the duplicated reference region, we need to make at least the following change:. - [ ] shift the left breakpoint to the right by 1 base compared to the current implementation, and ; - [ ] `downstreamBreakpointRefPos = complication.getDupSeqRepeatUnitRefSpan().getEnd();`. ### bump test coverage. - [x] `SimpleNovelAdjacencyAndChimericAlignmentEvidence` serialization test. - [x] `NovelAdjacencyAndAltHaplotype.toSimpleOrBNDTypes()` (but only related to `SimpleSvType`'s, BND's will wait for a later PR.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4663
https://github.com/broadinstitute/gatk/pull/4663:1920,Availability,down,downstreamBreakpointRefPos,1920,"awTypes` is moved to `AssemblyContigWithFineTunedAlignments.AlignmentSignatureBasicTypes` and reduced into fewer cases (`Suspicious`, `Simple` and `Complex`). - [x] static method `BreakpointsInference.inferFromSimpleChimera()` now moved to state query method `ChimericAlignment.inferType()`. - [x] `AssemblyContigWithFineTunedAlignments.hasIncompletePictureFromTwoAlignments()` merged with `ChimericAlignment.hasIncompletePicture()`. ### update how variants are represented. - [x] change `SVLEN` for `CPX` variants to the difference between _[alt haplotype sequence length]_ and _[affected reference region length]_, which is following the technical definition of `SVLEN` in VCF spec. - [x] change `RPL` output to one of these (note that test coverage is expected); - [x] ins/del, when del/ins bases are < 50 and annotate; when type is determined as ins, the POS will be 1 base before the micro-deleted range and END will be end of the micro-deleted range, where the REF allele will be the corresponding reference bases.; - [x] ins and del when both are >= 50, and link by `EVENT`. - [x] change `SVTYPE=DUP` to`SVYTPE=INS` when the duplicated region is shorter than 50 bp (tests). Note that this will lead to `INS` records with `DUP_REPEAT_UNIT_REF_SPAN` and `DUP_SEQ_CIGARS` (when available). In addition, we are currently treating duplication expansion as insertion. ; The VCF spec doesn't force `DUP` records as such.; If we decide to allow `POS` and `END` to designate the beginning and end of the duplicated reference region, we need to make at least the following change:. - [ ] shift the left breakpoint to the right by 1 base compared to the current implementation, and ; - [ ] `downstreamBreakpointRefPos = complication.getDupSeqRepeatUnitRefSpan().getEnd();`. ### bump test coverage. - [x] `SimpleNovelAdjacencyAndChimericAlignmentEvidence` serialization test. - [x] `NovelAdjacencyAndAltHaplotype.toSimpleOrBNDTypes()` (but only related to `SimpleSvType`'s, BND's will wait for a later PR.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4663
https://github.com/broadinstitute/gatk/pull/4663:59,Deployability,update,update,59,"Part of road map laid out in #4111 . ## Consolidate logic, update variant representation (PR#4663) . ### consolidate logic in the following classes. - [x] `AssemblyContigAlignmentSignatureClassifier` now gone, its inner enum class `RawTypes` is moved to `AssemblyContigWithFineTunedAlignments.AlignmentSignatureBasicTypes` and reduced into fewer cases (`Suspicious`, `Simple` and `Complex`). - [x] static method `BreakpointsInference.inferFromSimpleChimera()` now moved to state query method `ChimericAlignment.inferType()`. - [x] `AssemblyContigWithFineTunedAlignments.hasIncompletePictureFromTwoAlignments()` merged with `ChimericAlignment.hasIncompletePicture()`. ### update how variants are represented. - [x] change `SVLEN` for `CPX` variants to the difference between _[alt haplotype sequence length]_ and _[affected reference region length]_, which is following the technical definition of `SVLEN` in VCF spec. - [x] change `RPL` output to one of these (note that test coverage is expected); - [x] ins/del, when del/ins bases are < 50 and annotate; when type is determined as ins, the POS will be 1 base before the micro-deleted range and END will be end of the micro-deleted range, where the REF allele will be the corresponding reference bases.; - [x] ins and del when both are >= 50, and link by `EVENT`. - [x] change `SVTYPE=DUP` to`SVYTPE=INS` when the duplicated region is shorter than 50 bp (tests). Note that this will lead to `INS` records with `DUP_REPEAT_UNIT_REF_SPAN` and `DUP_SEQ_CIGARS` (when available). In addition, we are currently treating duplication expansion as insertion. ; The VCF spec doesn't force `DUP` records as such.; If we decide to allow `POS` and `END` to designate the beginning and end of the duplicated reference region, we need to make at least the following change:. - [ ] shift the left breakpoint to the right by 1 base compared to the current implementation, and ; - [ ] `downstreamBreakpointRefPos = complication.getDupSeqRepeatUnitRefSpan().getEnd();`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4663
https://github.com/broadinstitute/gatk/pull/4663:671,Deployability,update,update,671,"Part of road map laid out in #4111 . ## Consolidate logic, update variant representation (PR#4663) . ### consolidate logic in the following classes. - [x] `AssemblyContigAlignmentSignatureClassifier` now gone, its inner enum class `RawTypes` is moved to `AssemblyContigWithFineTunedAlignments.AlignmentSignatureBasicTypes` and reduced into fewer cases (`Suspicious`, `Simple` and `Complex`). - [x] static method `BreakpointsInference.inferFromSimpleChimera()` now moved to state query method `ChimericAlignment.inferType()`. - [x] `AssemblyContigWithFineTunedAlignments.hasIncompletePictureFromTwoAlignments()` merged with `ChimericAlignment.hasIncompletePicture()`. ### update how variants are represented. - [x] change `SVLEN` for `CPX` variants to the difference between _[alt haplotype sequence length]_ and _[affected reference region length]_, which is following the technical definition of `SVLEN` in VCF spec. - [x] change `RPL` output to one of these (note that test coverage is expected); - [x] ins/del, when del/ins bases are < 50 and annotate; when type is determined as ins, the POS will be 1 base before the micro-deleted range and END will be end of the micro-deleted range, where the REF allele will be the corresponding reference bases.; - [x] ins and del when both are >= 50, and link by `EVENT`. - [x] change `SVTYPE=DUP` to`SVYTPE=INS` when the duplicated region is shorter than 50 bp (tests). Note that this will lead to `INS` records with `DUP_REPEAT_UNIT_REF_SPAN` and `DUP_SEQ_CIGARS` (when available). In addition, we are currently treating duplication expansion as insertion. ; The VCF spec doesn't force `DUP` records as such.; If we decide to allow `POS` and `END` to designate the beginning and end of the duplicated reference region, we need to make at least the following change:. - [ ] shift the left breakpoint to the right by 1 base compared to the current implementation, and ; - [ ] `downstreamBreakpointRefPos = complication.getDupSeqRepeatUnitRefSpan().getEnd();`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4663
https://github.com/broadinstitute/gatk/pull/4663:327,Energy Efficiency,reduce,reduced,327,"Part of road map laid out in #4111 . ## Consolidate logic, update variant representation (PR#4663) . ### consolidate logic in the following classes. - [x] `AssemblyContigAlignmentSignatureClassifier` now gone, its inner enum class `RawTypes` is moved to `AssemblyContigWithFineTunedAlignments.AlignmentSignatureBasicTypes` and reduced into fewer cases (`Suspicious`, `Simple` and `Complex`). - [x] static method `BreakpointsInference.inferFromSimpleChimera()` now moved to state query method `ChimericAlignment.inferType()`. - [x] `AssemblyContigWithFineTunedAlignments.hasIncompletePictureFromTwoAlignments()` merged with `ChimericAlignment.hasIncompletePicture()`. ### update how variants are represented. - [x] change `SVLEN` for `CPX` variants to the difference between _[alt haplotype sequence length]_ and _[affected reference region length]_, which is following the technical definition of `SVLEN` in VCF spec. - [x] change `RPL` output to one of these (note that test coverage is expected); - [x] ins/del, when del/ins bases are < 50 and annotate; when type is determined as ins, the POS will be 1 base before the micro-deleted range and END will be end of the micro-deleted range, where the REF allele will be the corresponding reference bases.; - [x] ins and del when both are >= 50, and link by `EVENT`. - [x] change `SVTYPE=DUP` to`SVYTPE=INS` when the duplicated region is shorter than 50 bp (tests). Note that this will lead to `INS` records with `DUP_REPEAT_UNIT_REF_SPAN` and `DUP_SEQ_CIGARS` (when available). In addition, we are currently treating duplication expansion as insertion. ; The VCF spec doesn't force `DUP` records as such.; If we decide to allow `POS` and `END` to designate the beginning and end of the duplicated reference region, we need to make at least the following change:. - [ ] shift the left breakpoint to the right by 1 base compared to the current implementation, and ; - [ ] `downstreamBreakpointRefPos = complication.getDupSeqRepeatUnitRefSpan().getEnd();`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4663
https://github.com/broadinstitute/gatk/pull/4663:52,Testability,log,logic,52,"Part of road map laid out in #4111 . ## Consolidate logic, update variant representation (PR#4663) . ### consolidate logic in the following classes. - [x] `AssemblyContigAlignmentSignatureClassifier` now gone, its inner enum class `RawTypes` is moved to `AssemblyContigWithFineTunedAlignments.AlignmentSignatureBasicTypes` and reduced into fewer cases (`Suspicious`, `Simple` and `Complex`). - [x] static method `BreakpointsInference.inferFromSimpleChimera()` now moved to state query method `ChimericAlignment.inferType()`. - [x] `AssemblyContigWithFineTunedAlignments.hasIncompletePictureFromTwoAlignments()` merged with `ChimericAlignment.hasIncompletePicture()`. ### update how variants are represented. - [x] change `SVLEN` for `CPX` variants to the difference between _[alt haplotype sequence length]_ and _[affected reference region length]_, which is following the technical definition of `SVLEN` in VCF spec. - [x] change `RPL` output to one of these (note that test coverage is expected); - [x] ins/del, when del/ins bases are < 50 and annotate; when type is determined as ins, the POS will be 1 base before the micro-deleted range and END will be end of the micro-deleted range, where the REF allele will be the corresponding reference bases.; - [x] ins and del when both are >= 50, and link by `EVENT`. - [x] change `SVTYPE=DUP` to`SVYTPE=INS` when the duplicated region is shorter than 50 bp (tests). Note that this will lead to `INS` records with `DUP_REPEAT_UNIT_REF_SPAN` and `DUP_SEQ_CIGARS` (when available). In addition, we are currently treating duplication expansion as insertion. ; The VCF spec doesn't force `DUP` records as such.; If we decide to allow `POS` and `END` to designate the beginning and end of the duplicated reference region, we need to make at least the following change:. - [ ] shift the left breakpoint to the right by 1 base compared to the current implementation, and ; - [ ] `downstreamBreakpointRefPos = complication.getDupSeqRepeatUnitRefSpan().getEnd();`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4663
https://github.com/broadinstitute/gatk/pull/4663:117,Testability,log,logic,117,"Part of road map laid out in #4111 . ## Consolidate logic, update variant representation (PR#4663) . ### consolidate logic in the following classes. - [x] `AssemblyContigAlignmentSignatureClassifier` now gone, its inner enum class `RawTypes` is moved to `AssemblyContigWithFineTunedAlignments.AlignmentSignatureBasicTypes` and reduced into fewer cases (`Suspicious`, `Simple` and `Complex`). - [x] static method `BreakpointsInference.inferFromSimpleChimera()` now moved to state query method `ChimericAlignment.inferType()`. - [x] `AssemblyContigWithFineTunedAlignments.hasIncompletePictureFromTwoAlignments()` merged with `ChimericAlignment.hasIncompletePicture()`. ### update how variants are represented. - [x] change `SVLEN` for `CPX` variants to the difference between _[alt haplotype sequence length]_ and _[affected reference region length]_, which is following the technical definition of `SVLEN` in VCF spec. - [x] change `RPL` output to one of these (note that test coverage is expected); - [x] ins/del, when del/ins bases are < 50 and annotate; when type is determined as ins, the POS will be 1 base before the micro-deleted range and END will be end of the micro-deleted range, where the REF allele will be the corresponding reference bases.; - [x] ins and del when both are >= 50, and link by `EVENT`. - [x] change `SVTYPE=DUP` to`SVYTPE=INS` when the duplicated region is shorter than 50 bp (tests). Note that this will lead to `INS` records with `DUP_REPEAT_UNIT_REF_SPAN` and `DUP_SEQ_CIGARS` (when available). In addition, we are currently treating duplication expansion as insertion. ; The VCF spec doesn't force `DUP` records as such.; If we decide to allow `POS` and `END` to designate the beginning and end of the duplicated reference region, we need to make at least the following change:. - [ ] shift the left breakpoint to the right by 1 base compared to the current implementation, and ; - [ ] `downstreamBreakpointRefPos = complication.getDupSeqRepeatUnitRefSpan().getEnd();`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4663
https://github.com/broadinstitute/gatk/pull/4663:971,Testability,test,test,971,"ion (PR#4663) . ### consolidate logic in the following classes. - [x] `AssemblyContigAlignmentSignatureClassifier` now gone, its inner enum class `RawTypes` is moved to `AssemblyContigWithFineTunedAlignments.AlignmentSignatureBasicTypes` and reduced into fewer cases (`Suspicious`, `Simple` and `Complex`). - [x] static method `BreakpointsInference.inferFromSimpleChimera()` now moved to state query method `ChimericAlignment.inferType()`. - [x] `AssemblyContigWithFineTunedAlignments.hasIncompletePictureFromTwoAlignments()` merged with `ChimericAlignment.hasIncompletePicture()`. ### update how variants are represented. - [x] change `SVLEN` for `CPX` variants to the difference between _[alt haplotype sequence length]_ and _[affected reference region length]_, which is following the technical definition of `SVLEN` in VCF spec. - [x] change `RPL` output to one of these (note that test coverage is expected); - [x] ins/del, when del/ins bases are < 50 and annotate; when type is determined as ins, the POS will be 1 base before the micro-deleted range and END will be end of the micro-deleted range, where the REF allele will be the corresponding reference bases.; - [x] ins and del when both are >= 50, and link by `EVENT`. - [x] change `SVTYPE=DUP` to`SVYTPE=INS` when the duplicated region is shorter than 50 bp (tests). Note that this will lead to `INS` records with `DUP_REPEAT_UNIT_REF_SPAN` and `DUP_SEQ_CIGARS` (when available). In addition, we are currently treating duplication expansion as insertion. ; The VCF spec doesn't force `DUP` records as such.; If we decide to allow `POS` and `END` to designate the beginning and end of the duplicated reference region, we need to make at least the following change:. - [ ] shift the left breakpoint to the right by 1 base compared to the current implementation, and ; - [ ] `downstreamBreakpointRefPos = complication.getDupSeqRepeatUnitRefSpan().getEnd();`. ### bump test coverage. - [x] `SimpleNovelAdjacencyAndChimericAlignmentEvidence` s",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4663
https://github.com/broadinstitute/gatk/pull/4663:1406,Testability,test,tests,1406,"awTypes` is moved to `AssemblyContigWithFineTunedAlignments.AlignmentSignatureBasicTypes` and reduced into fewer cases (`Suspicious`, `Simple` and `Complex`). - [x] static method `BreakpointsInference.inferFromSimpleChimera()` now moved to state query method `ChimericAlignment.inferType()`. - [x] `AssemblyContigWithFineTunedAlignments.hasIncompletePictureFromTwoAlignments()` merged with `ChimericAlignment.hasIncompletePicture()`. ### update how variants are represented. - [x] change `SVLEN` for `CPX` variants to the difference between _[alt haplotype sequence length]_ and _[affected reference region length]_, which is following the technical definition of `SVLEN` in VCF spec. - [x] change `RPL` output to one of these (note that test coverage is expected); - [x] ins/del, when del/ins bases are < 50 and annotate; when type is determined as ins, the POS will be 1 base before the micro-deleted range and END will be end of the micro-deleted range, where the REF allele will be the corresponding reference bases.; - [x] ins and del when both are >= 50, and link by `EVENT`. - [x] change `SVTYPE=DUP` to`SVYTPE=INS` when the duplicated region is shorter than 50 bp (tests). Note that this will lead to `INS` records with `DUP_REPEAT_UNIT_REF_SPAN` and `DUP_SEQ_CIGARS` (when available). In addition, we are currently treating duplication expansion as insertion. ; The VCF spec doesn't force `DUP` records as such.; If we decide to allow `POS` and `END` to designate the beginning and end of the duplicated reference region, we need to make at least the following change:. - [ ] shift the left breakpoint to the right by 1 base compared to the current implementation, and ; - [ ] `downstreamBreakpointRefPos = complication.getDupSeqRepeatUnitRefSpan().getEnd();`. ### bump test coverage. - [x] `SimpleNovelAdjacencyAndChimericAlignmentEvidence` serialization test. - [x] `NovelAdjacencyAndAltHaplotype.toSimpleOrBNDTypes()` (but only related to `SimpleSvType`'s, BND's will wait for a later PR.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4663
https://github.com/broadinstitute/gatk/pull/4663:2012,Testability,test,test,2012,"awTypes` is moved to `AssemblyContigWithFineTunedAlignments.AlignmentSignatureBasicTypes` and reduced into fewer cases (`Suspicious`, `Simple` and `Complex`). - [x] static method `BreakpointsInference.inferFromSimpleChimera()` now moved to state query method `ChimericAlignment.inferType()`. - [x] `AssemblyContigWithFineTunedAlignments.hasIncompletePictureFromTwoAlignments()` merged with `ChimericAlignment.hasIncompletePicture()`. ### update how variants are represented. - [x] change `SVLEN` for `CPX` variants to the difference between _[alt haplotype sequence length]_ and _[affected reference region length]_, which is following the technical definition of `SVLEN` in VCF spec. - [x] change `RPL` output to one of these (note that test coverage is expected); - [x] ins/del, when del/ins bases are < 50 and annotate; when type is determined as ins, the POS will be 1 base before the micro-deleted range and END will be end of the micro-deleted range, where the REF allele will be the corresponding reference bases.; - [x] ins and del when both are >= 50, and link by `EVENT`. - [x] change `SVTYPE=DUP` to`SVYTPE=INS` when the duplicated region is shorter than 50 bp (tests). Note that this will lead to `INS` records with `DUP_REPEAT_UNIT_REF_SPAN` and `DUP_SEQ_CIGARS` (when available). In addition, we are currently treating duplication expansion as insertion. ; The VCF spec doesn't force `DUP` records as such.; If we decide to allow `POS` and `END` to designate the beginning and end of the duplicated reference region, we need to make at least the following change:. - [ ] shift the left breakpoint to the right by 1 base compared to the current implementation, and ; - [ ] `downstreamBreakpointRefPos = complication.getDupSeqRepeatUnitRefSpan().getEnd();`. ### bump test coverage. - [x] `SimpleNovelAdjacencyAndChimericAlignmentEvidence` serialization test. - [x] `NovelAdjacencyAndAltHaplotype.toSimpleOrBNDTypes()` (but only related to `SimpleSvType`'s, BND's will wait for a later PR.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4663
https://github.com/broadinstitute/gatk/pull/4663:2098,Testability,test,test,2098,"awTypes` is moved to `AssemblyContigWithFineTunedAlignments.AlignmentSignatureBasicTypes` and reduced into fewer cases (`Suspicious`, `Simple` and `Complex`). - [x] static method `BreakpointsInference.inferFromSimpleChimera()` now moved to state query method `ChimericAlignment.inferType()`. - [x] `AssemblyContigWithFineTunedAlignments.hasIncompletePictureFromTwoAlignments()` merged with `ChimericAlignment.hasIncompletePicture()`. ### update how variants are represented. - [x] change `SVLEN` for `CPX` variants to the difference between _[alt haplotype sequence length]_ and _[affected reference region length]_, which is following the technical definition of `SVLEN` in VCF spec. - [x] change `RPL` output to one of these (note that test coverage is expected); - [x] ins/del, when del/ins bases are < 50 and annotate; when type is determined as ins, the POS will be 1 base before the micro-deleted range and END will be end of the micro-deleted range, where the REF allele will be the corresponding reference bases.; - [x] ins and del when both are >= 50, and link by `EVENT`. - [x] change `SVTYPE=DUP` to`SVYTPE=INS` when the duplicated region is shorter than 50 bp (tests). Note that this will lead to `INS` records with `DUP_REPEAT_UNIT_REF_SPAN` and `DUP_SEQ_CIGARS` (when available). In addition, we are currently treating duplication expansion as insertion. ; The VCF spec doesn't force `DUP` records as such.; If we decide to allow `POS` and `END` to designate the beginning and end of the duplicated reference region, we need to make at least the following change:. - [ ] shift the left breakpoint to the right by 1 base compared to the current implementation, and ; - [ ] `downstreamBreakpointRefPos = complication.getDupSeqRepeatUnitRefSpan().getEnd();`. ### bump test coverage. - [x] `SimpleNovelAdjacencyAndChimericAlignmentEvidence` serialization test. - [x] `NovelAdjacencyAndAltHaplotype.toSimpleOrBNDTypes()` (but only related to `SimpleSvType`'s, BND's will wait for a later PR.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4663
https://github.com/broadinstitute/gatk/pull/4663:368,Usability,Simpl,Simple,368,"Part of road map laid out in #4111 . ## Consolidate logic, update variant representation (PR#4663) . ### consolidate logic in the following classes. - [x] `AssemblyContigAlignmentSignatureClassifier` now gone, its inner enum class `RawTypes` is moved to `AssemblyContigWithFineTunedAlignments.AlignmentSignatureBasicTypes` and reduced into fewer cases (`Suspicious`, `Simple` and `Complex`). - [x] static method `BreakpointsInference.inferFromSimpleChimera()` now moved to state query method `ChimericAlignment.inferType()`. - [x] `AssemblyContigWithFineTunedAlignments.hasIncompletePictureFromTwoAlignments()` merged with `ChimericAlignment.hasIncompletePicture()`. ### update how variants are represented. - [x] change `SVLEN` for `CPX` variants to the difference between _[alt haplotype sequence length]_ and _[affected reference region length]_, which is following the technical definition of `SVLEN` in VCF spec. - [x] change `RPL` output to one of these (note that test coverage is expected); - [x] ins/del, when del/ins bases are < 50 and annotate; when type is determined as ins, the POS will be 1 base before the micro-deleted range and END will be end of the micro-deleted range, where the REF allele will be the corresponding reference bases.; - [x] ins and del when both are >= 50, and link by `EVENT`. - [x] change `SVTYPE=DUP` to`SVYTPE=INS` when the duplicated region is shorter than 50 bp (tests). Note that this will lead to `INS` records with `DUP_REPEAT_UNIT_REF_SPAN` and `DUP_SEQ_CIGARS` (when available). In addition, we are currently treating duplication expansion as insertion. ; The VCF spec doesn't force `DUP` records as such.; If we decide to allow `POS` and `END` to designate the beginning and end of the duplicated reference region, we need to make at least the following change:. - [ ] shift the left breakpoint to the right by 1 base compared to the current implementation, and ; - [ ] `downstreamBreakpointRefPos = complication.getDupSeqRepeatUnitRefSpan().getEnd();`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4663
https://github.com/broadinstitute/gatk/pull/4663:2034,Usability,Simpl,SimpleNovelAdjacencyAndChimericAlignmentEvidence,2034,"awTypes` is moved to `AssemblyContigWithFineTunedAlignments.AlignmentSignatureBasicTypes` and reduced into fewer cases (`Suspicious`, `Simple` and `Complex`). - [x] static method `BreakpointsInference.inferFromSimpleChimera()` now moved to state query method `ChimericAlignment.inferType()`. - [x] `AssemblyContigWithFineTunedAlignments.hasIncompletePictureFromTwoAlignments()` merged with `ChimericAlignment.hasIncompletePicture()`. ### update how variants are represented. - [x] change `SVLEN` for `CPX` variants to the difference between _[alt haplotype sequence length]_ and _[affected reference region length]_, which is following the technical definition of `SVLEN` in VCF spec. - [x] change `RPL` output to one of these (note that test coverage is expected); - [x] ins/del, when del/ins bases are < 50 and annotate; when type is determined as ins, the POS will be 1 base before the micro-deleted range and END will be end of the micro-deleted range, where the REF allele will be the corresponding reference bases.; - [x] ins and del when both are >= 50, and link by `EVENT`. - [x] change `SVTYPE=DUP` to`SVYTPE=INS` when the duplicated region is shorter than 50 bp (tests). Note that this will lead to `INS` records with `DUP_REPEAT_UNIT_REF_SPAN` and `DUP_SEQ_CIGARS` (when available). In addition, we are currently treating duplication expansion as insertion. ; The VCF spec doesn't force `DUP` records as such.; If we decide to allow `POS` and `END` to designate the beginning and end of the duplicated reference region, we need to make at least the following change:. - [ ] shift the left breakpoint to the right by 1 base compared to the current implementation, and ; - [ ] `downstreamBreakpointRefPos = complication.getDupSeqRepeatUnitRefSpan().getEnd();`. ### bump test coverage. - [x] `SimpleNovelAdjacencyAndChimericAlignmentEvidence` serialization test. - [x] `NovelAdjacencyAndAltHaplotype.toSimpleOrBNDTypes()` (but only related to `SimpleSvType`'s, BND's will wait for a later PR.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4663
https://github.com/broadinstitute/gatk/pull/4663:2185,Usability,Simpl,SimpleSvType,2185,"awTypes` is moved to `AssemblyContigWithFineTunedAlignments.AlignmentSignatureBasicTypes` and reduced into fewer cases (`Suspicious`, `Simple` and `Complex`). - [x] static method `BreakpointsInference.inferFromSimpleChimera()` now moved to state query method `ChimericAlignment.inferType()`. - [x] `AssemblyContigWithFineTunedAlignments.hasIncompletePictureFromTwoAlignments()` merged with `ChimericAlignment.hasIncompletePicture()`. ### update how variants are represented. - [x] change `SVLEN` for `CPX` variants to the difference between _[alt haplotype sequence length]_ and _[affected reference region length]_, which is following the technical definition of `SVLEN` in VCF spec. - [x] change `RPL` output to one of these (note that test coverage is expected); - [x] ins/del, when del/ins bases are < 50 and annotate; when type is determined as ins, the POS will be 1 base before the micro-deleted range and END will be end of the micro-deleted range, where the REF allele will be the corresponding reference bases.; - [x] ins and del when both are >= 50, and link by `EVENT`. - [x] change `SVTYPE=DUP` to`SVYTPE=INS` when the duplicated region is shorter than 50 bp (tests). Note that this will lead to `INS` records with `DUP_REPEAT_UNIT_REF_SPAN` and `DUP_SEQ_CIGARS` (when available). In addition, we are currently treating duplication expansion as insertion. ; The VCF spec doesn't force `DUP` records as such.; If we decide to allow `POS` and `END` to designate the beginning and end of the duplicated reference region, we need to make at least the following change:. - [ ] shift the left breakpoint to the right by 1 base compared to the current implementation, and ; - [ ] `downstreamBreakpointRefPos = complication.getDupSeqRepeatUnitRefSpan().getEnd();`. ### bump test coverage. - [x] `SimpleNovelAdjacencyAndChimericAlignmentEvidence` serialization test. - [x] `NovelAdjacencyAndAltHaplotype.toSimpleOrBNDTypes()` (but only related to `SimpleSvType`'s, BND's will wait for a later PR.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4663
https://github.com/broadinstitute/gatk/issues/4665:32,Availability,error,errors,32,"A couple of users have reported errors with --disable-tool-default-read-filters. Is this expected? This user says the tool runs without that flag. ----; User Report; ----. I just encountered this error in Mutect2: . Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx4g -jar /Users/loeblabm11/bioinformatics/programs/GATK/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar Mutect2 -R /Users/loeblabm11/bioinformatics/reference/human/hg19/hg19.fa -I 20171027_BN31_python.dcs.filt.no_overlap.bam -tumor BN31 -O 20171027_BN31_python.dcs.MuTect2.vcf -bamout 20171027_BN31_python.dcs.MuTect2.bam --max-reads-per-alignment-start 0 --max-population-af 1 --disable-tool-default-read-filters; 12:18:10.900 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/loeblabm11/bioinformatics/programs/GATK/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar!/com/intel/gkl/native/libgkl_compression.dylib; 12:18:11.387 INFO Mutect2 - ------------------------------------------------------------; 12:18:11.388 INFO Mutect2 - The Genome Analysis Toolkit (GATK) v4.0.3.0; 12:18:11.388 INFO Mutect2 - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:18:11.388 INFO Mutect2 - Executing as loeblabm11@LoeblabM11s-iMac.local on Mac OS X v10.12.6 x86_64; 12:18:11.388 INFO Mutect2 - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_151-b12; 12:18:11.388 INFO Mutect2 - Start Date/Time: April 11, 2018 12:18:10 PM PDT; 12:18:11.388 INFO Mutect2 - ------------------------------------------------------------; 12:18:11.388 INFO Mutect2 - ------------------------------------------------------------; 12:18:11.388 INFO Mutect2 - HTSJDK Version: 2.14.3; 12:18:11.388 INFO Mutect2 - Picard Version: 2.17.2; 12:18:11.388 INFO Mutect2 - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 12:18:11.388 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : fals",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4665
https://github.com/broadinstitute/gatk/issues/4665:196,Availability,error,error,196,"A couple of users have reported errors with --disable-tool-default-read-filters. Is this expected? This user says the tool runs without that flag. ----; User Report; ----. I just encountered this error in Mutect2: . Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx4g -jar /Users/loeblabm11/bioinformatics/programs/GATK/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar Mutect2 -R /Users/loeblabm11/bioinformatics/reference/human/hg19/hg19.fa -I 20171027_BN31_python.dcs.filt.no_overlap.bam -tumor BN31 -O 20171027_BN31_python.dcs.MuTect2.vcf -bamout 20171027_BN31_python.dcs.MuTect2.bam --max-reads-per-alignment-start 0 --max-population-af 1 --disable-tool-default-read-filters; 12:18:10.900 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/loeblabm11/bioinformatics/programs/GATK/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar!/com/intel/gkl/native/libgkl_compression.dylib; 12:18:11.387 INFO Mutect2 - ------------------------------------------------------------; 12:18:11.388 INFO Mutect2 - The Genome Analysis Toolkit (GATK) v4.0.3.0; 12:18:11.388 INFO Mutect2 - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:18:11.388 INFO Mutect2 - Executing as loeblabm11@LoeblabM11s-iMac.local on Mac OS X v10.12.6 x86_64; 12:18:11.388 INFO Mutect2 - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_151-b12; 12:18:11.388 INFO Mutect2 - Start Date/Time: April 11, 2018 12:18:10 PM PDT; 12:18:11.388 INFO Mutect2 - ------------------------------------------------------------; 12:18:11.388 INFO Mutect2 - ------------------------------------------------------------; 12:18:11.388 INFO Mutect2 - HTSJDK Version: 2.14.3; 12:18:11.388 INFO Mutect2 - Picard Version: 2.17.2; 12:18:11.388 INFO Mutect2 - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 12:18:11.388 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : fals",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4665
https://github.com/broadinstitute/gatk/issues/4665:4069,Availability,down,down,4069,"- Loading libgkl_pairhmm.dylib from jar:file:/Users/loeblabm11/bioinformatics/programs/GATK/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm.dylib; 12:18:12.368 WARN IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 12:18:12.368 WARN IntelPairHmm - Ignoring request for 4 threads; not using OpenMP implementation; 12:18:12.369 INFO PairHMM - Using the AVX-accelerated native PairHMM implementation; 12:18:12.403 INFO ProgressMeter - Starting traversal; 12:18:12.403 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 12:18:22.403 INFO ProgressMeter - chr1:75065650 0.2 250240 1501440.0; 12:18:29.713 INFO VectorLoglessPairHMM - Time spent in setup for JNI call : 0.009098343; 12:18:29.713 INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 0.121747383; 12:18:29.713 INFO SmithWatermanAligner - Total compute time in java Smith-Waterman : 0.42 sec; 12:18:29.763 INFO Mutect2 - Shutting down engine; [April 11, 2018 12:18:29 PM PDT] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.31 minutes.; Runtime.totalMemory()=1781006336; java.lang.NullPointerException; 	at org.broadinstitute.hellbender.utils.locusiterator.ReadStateManager.readStartsAtCurrentPosition(ReadStateManager.java:132); 	at org.broadinstitute.hellbender.utils.locusiterator.ReadStateManager.collectPendingReads(ReadStateManager.java:159); 	at org.broadinstitute.hellbender.utils.locusiterator.LocusIteratorByState.lazyLoadNextAlignmentContext(LocusIteratorByState.java:315); 	at org.broadinstitute.hellbender.utils.locusiterator.LocusIteratorByState.hasNext(LocusIteratorByState.java:252); 	at org.broadinstitute.hellbender.utils.locusiterator.IntervalAlignmentContextIterator.advanceAlignmentContext(IntervalAlignmentContextIterator.java:104); 	at org.broadinstitute.hellbender.utils.locusiterator.IntervalAlignmentContextIterator.advanceAlignmentContextToCurrentInterval(IntervalAlignmentContextI",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4665
https://github.com/broadinstitute/gatk/issues/4665:7579,Availability,error,error-in-,7579,"g.broadinstitute.hellbender.engine.AssemblyRegionIterator.next(AssemblyRegionIterator.java:34); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:290); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:271); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:893); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289). My command looks like this:; ~/bioinformatics/programs/GATK/gatk-4.0.3.0/gatk --java-options ""-Xmx4g"" Mutect2 -R /Users/loeblabm11/bioinformatics/reference/human/hg19/hg19.fa -I 20171027_BN31_python.dcs.filt.no_overlap.bam -tumor BN31 -O 20171027_BN31_python.dcs.MuTect2.vcf -bamout 20171027_BN31_python.dcs.MuTect2.bam --max-reads-per-alignment-start 0 --max-population-af 1 --disable-tool-default-read-filters. and I'm running on MacOSX Sierra (10.12.6). java -version returns ; java version ""1.8.0_151""; Java(TM) SE Runtime Environment (build 1.8.0_151-b12); Java HotSpot(TM) 64-Bit Server VM (build 25.151-b12, mixed mode). The input file is position-sorted output from Duplex Sequencing pipelines, and has been through IndelRealigner (from GATK 3.7), and ClipOverlappingReads from FgBio. It has been indexed. . I can put together a full bug report, complete with my input file, if necessary; I just want an idea of what might be happening. . Brendan. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/11790/error-in-mutect2/p1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4665
https://github.com/broadinstitute/gatk/issues/4665:2387,Deployability,patch,patch,2387," 12:18:11.388 INFO Mutect2 - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_151-b12; 12:18:11.388 INFO Mutect2 - Start Date/Time: April 11, 2018 12:18:10 PM PDT; 12:18:11.388 INFO Mutect2 - ------------------------------------------------------------; 12:18:11.388 INFO Mutect2 - ------------------------------------------------------------; 12:18:11.388 INFO Mutect2 - HTSJDK Version: 2.14.3; 12:18:11.388 INFO Mutect2 - Picard Version: 2.17.2; 12:18:11.388 INFO Mutect2 - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 12:18:11.388 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 12:18:11.388 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 12:18:11.388 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 12:18:11.388 INFO Mutect2 - Deflater: IntelDeflater; 12:18:11.388 INFO Mutect2 - Inflater: IntelInflater; 12:18:11.389 INFO Mutect2 - GCS max retries/reopens: 20; 12:18:11.389 INFO Mutect2 - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 12:18:11.389 INFO Mutect2 - Initializing engine; 12:18:11.724 INFO Mutect2 - Done initializing engine; 12:18:12.288 INFO NativeLibraryLoader - Loading libgkl_utils.dylib from jar:file:/Users/loeblabm11/bioinformatics/programs/GATK/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar!/com/intel/gkl/native/libgkl_utils.dylib; 12:18:12.290 WARN NativeLibraryLoader - Unable to find native library: native/libgkl_pairhmm_omp.dylib; 12:18:12.290 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported; 12:18:12.290 INFO NativeLibraryLoader - Loading libgkl_pairhmm.dylib from jar:file:/Users/loeblabm11/bioinformatics/programs/GATK/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm.dylib; 12:18:12.368 WARN IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 12:18:12.368 WARN IntelPairHmm - Ignoring request for",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4665
https://github.com/broadinstitute/gatk/issues/4665:7199,Deployability,pipeline,pipelines,7199,"g.broadinstitute.hellbender.engine.AssemblyRegionIterator.next(AssemblyRegionIterator.java:34); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:290); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:271); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:893); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289). My command looks like this:; ~/bioinformatics/programs/GATK/gatk-4.0.3.0/gatk --java-options ""-Xmx4g"" Mutect2 -R /Users/loeblabm11/bioinformatics/reference/human/hg19/hg19.fa -I 20171027_BN31_python.dcs.filt.no_overlap.bam -tumor BN31 -O 20171027_BN31_python.dcs.MuTect2.vcf -bamout 20171027_BN31_python.dcs.MuTect2.bam --max-reads-per-alignment-start 0 --max-population-af 1 --disable-tool-default-read-filters. and I'm running on MacOSX Sierra (10.12.6). java -version returns ; java version ""1.8.0_151""; Java(TM) SE Runtime Environment (build 1.8.0_151-b12); Java HotSpot(TM) 64-Bit Server VM (build 25.151-b12, mixed mode). The input file is position-sorted output from Duplex Sequencing pipelines, and has been through IndelRealigner (from GATK 3.7), and ClipOverlappingReads from FgBio. It has been indexed. . I can put together a full bug report, complete with my input file, if necessary; I just want an idea of what might be happening. . Brendan. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/11790/error-in-mutect2/p1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4665
https://github.com/broadinstitute/gatk/issues/4665:840,Performance,Load,Loading,840,"A couple of users have reported errors with --disable-tool-default-read-filters. Is this expected? This user says the tool runs without that flag. ----; User Report; ----. I just encountered this error in Mutect2: . Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx4g -jar /Users/loeblabm11/bioinformatics/programs/GATK/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar Mutect2 -R /Users/loeblabm11/bioinformatics/reference/human/hg19/hg19.fa -I 20171027_BN31_python.dcs.filt.no_overlap.bam -tumor BN31 -O 20171027_BN31_python.dcs.MuTect2.vcf -bamout 20171027_BN31_python.dcs.MuTect2.bam --max-reads-per-alignment-start 0 --max-population-af 1 --disable-tool-default-read-filters; 12:18:10.900 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/loeblabm11/bioinformatics/programs/GATK/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar!/com/intel/gkl/native/libgkl_compression.dylib; 12:18:11.387 INFO Mutect2 - ------------------------------------------------------------; 12:18:11.388 INFO Mutect2 - The Genome Analysis Toolkit (GATK) v4.0.3.0; 12:18:11.388 INFO Mutect2 - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:18:11.388 INFO Mutect2 - Executing as loeblabm11@LoeblabM11s-iMac.local on Mac OS X v10.12.6 x86_64; 12:18:11.388 INFO Mutect2 - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_151-b12; 12:18:11.388 INFO Mutect2 - Start Date/Time: April 11, 2018 12:18:10 PM PDT; 12:18:11.388 INFO Mutect2 - ------------------------------------------------------------; 12:18:11.388 INFO Mutect2 - ------------------------------------------------------------; 12:18:11.388 INFO Mutect2 - HTSJDK Version: 2.14.3; 12:18:11.388 INFO Mutect2 - Picard Version: 2.17.2; 12:18:11.388 INFO Mutect2 - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 12:18:11.388 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : fals",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4665
https://github.com/broadinstitute/gatk/issues/4665:2650,Performance,Load,Loading,2650,------------; 12:18:11.388 INFO Mutect2 - ------------------------------------------------------------; 12:18:11.388 INFO Mutect2 - HTSJDK Version: 2.14.3; 12:18:11.388 INFO Mutect2 - Picard Version: 2.17.2; 12:18:11.388 INFO Mutect2 - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 12:18:11.388 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 12:18:11.388 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 12:18:11.388 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 12:18:11.388 INFO Mutect2 - Deflater: IntelDeflater; 12:18:11.388 INFO Mutect2 - Inflater: IntelInflater; 12:18:11.389 INFO Mutect2 - GCS max retries/reopens: 20; 12:18:11.389 INFO Mutect2 - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 12:18:11.389 INFO Mutect2 - Initializing engine; 12:18:11.724 INFO Mutect2 - Done initializing engine; 12:18:12.288 INFO NativeLibraryLoader - Loading libgkl_utils.dylib from jar:file:/Users/loeblabm11/bioinformatics/programs/GATK/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar!/com/intel/gkl/native/libgkl_utils.dylib; 12:18:12.290 WARN NativeLibraryLoader - Unable to find native library: native/libgkl_pairhmm_omp.dylib; 12:18:12.290 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported; 12:18:12.290 INFO NativeLibraryLoader - Loading libgkl_pairhmm.dylib from jar:file:/Users/loeblabm11/bioinformatics/programs/GATK/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm.dylib; 12:18:12.368 WARN IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 12:18:12.368 WARN IntelPairHmm - Ignoring request for 4 threads; not using OpenMP implementation; 12:18:12.369 INFO PairHMM - Using the AVX-accelerated native PairHMM implementation; 12:18:12.403 INFO ProgressMeter - Starting traversal; 12:18:12.403 INFO ProgressMeter - Current Locus Elapsed Minute,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4665
https://github.com/broadinstitute/gatk/issues/4665:2963,Performance,multi-thread,multi-threaded,2963, : false; 12:18:11.388 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 12:18:11.388 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 12:18:11.388 INFO Mutect2 - Deflater: IntelDeflater; 12:18:11.388 INFO Mutect2 - Inflater: IntelInflater; 12:18:11.389 INFO Mutect2 - GCS max retries/reopens: 20; 12:18:11.389 INFO Mutect2 - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 12:18:11.389 INFO Mutect2 - Initializing engine; 12:18:11.724 INFO Mutect2 - Done initializing engine; 12:18:12.288 INFO NativeLibraryLoader - Loading libgkl_utils.dylib from jar:file:/Users/loeblabm11/bioinformatics/programs/GATK/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar!/com/intel/gkl/native/libgkl_utils.dylib; 12:18:12.290 WARN NativeLibraryLoader - Unable to find native library: native/libgkl_pairhmm_omp.dylib; 12:18:12.290 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported; 12:18:12.290 INFO NativeLibraryLoader - Loading libgkl_pairhmm.dylib from jar:file:/Users/loeblabm11/bioinformatics/programs/GATK/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm.dylib; 12:18:12.368 WARN IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 12:18:12.368 WARN IntelPairHmm - Ignoring request for 4 threads; not using OpenMP implementation; 12:18:12.369 INFO PairHMM - Using the AVX-accelerated native PairHMM implementation; 12:18:12.403 INFO ProgressMeter - Starting traversal; 12:18:12.403 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 12:18:22.403 INFO ProgressMeter - chr1:75065650 0.2 250240 1501440.0; 12:18:29.713 INFO VectorLoglessPairHMM - Time spent in setup for JNI call : 0.009098343; 12:18:29.713 INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 0.121747383; 12:18:29.713 INFO SmithWatermanAligner - Total compute ti,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4665
https://github.com/broadinstitute/gatk/issues/4665:3082,Performance,Load,Loading,3082,S : true; 12:18:11.388 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 12:18:11.388 INFO Mutect2 - Deflater: IntelDeflater; 12:18:11.388 INFO Mutect2 - Inflater: IntelInflater; 12:18:11.389 INFO Mutect2 - GCS max retries/reopens: 20; 12:18:11.389 INFO Mutect2 - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 12:18:11.389 INFO Mutect2 - Initializing engine; 12:18:11.724 INFO Mutect2 - Done initializing engine; 12:18:12.288 INFO NativeLibraryLoader - Loading libgkl_utils.dylib from jar:file:/Users/loeblabm11/bioinformatics/programs/GATK/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar!/com/intel/gkl/native/libgkl_utils.dylib; 12:18:12.290 WARN NativeLibraryLoader - Unable to find native library: native/libgkl_pairhmm_omp.dylib; 12:18:12.290 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported; 12:18:12.290 INFO NativeLibraryLoader - Loading libgkl_pairhmm.dylib from jar:file:/Users/loeblabm11/bioinformatics/programs/GATK/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm.dylib; 12:18:12.368 WARN IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 12:18:12.368 WARN IntelPairHmm - Ignoring request for 4 threads; not using OpenMP implementation; 12:18:12.369 INFO PairHMM - Using the AVX-accelerated native PairHMM implementation; 12:18:12.403 INFO ProgressMeter - Starting traversal; 12:18:12.403 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 12:18:22.403 INFO ProgressMeter - chr1:75065650 0.2 250240 1501440.0; 12:18:29.713 INFO VectorLoglessPairHMM - Time spent in setup for JNI call : 0.009098343; 12:18:29.713 INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 0.121747383; 12:18:29.713 INFO SmithWatermanAligner - Total compute time in java Smith-Waterman : 0.42 sec; 12:18:29.763 INFO Mutect2 - Shutting down engi,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4665
https://github.com/broadinstitute/gatk/issues/4665:5432,Performance,load,loadNextAssemblyRegion,5432,.ReadStateManager.collectPendingReads(ReadStateManager.java:159); 	at org.broadinstitute.hellbender.utils.locusiterator.LocusIteratorByState.lazyLoadNextAlignmentContext(LocusIteratorByState.java:315); 	at org.broadinstitute.hellbender.utils.locusiterator.LocusIteratorByState.hasNext(LocusIteratorByState.java:252); 	at org.broadinstitute.hellbender.utils.locusiterator.IntervalAlignmentContextIterator.advanceAlignmentContext(IntervalAlignmentContextIterator.java:104); 	at org.broadinstitute.hellbender.utils.locusiterator.IntervalAlignmentContextIterator.advanceAlignmentContextToCurrentInterval(IntervalAlignmentContextIterator.java:99); 	at org.broadinstitute.hellbender.utils.locusiterator.IntervalAlignmentContextIterator.next(IntervalAlignmentContextIterator.java:69); 	at org.broadinstitute.hellbender.utils.locusiterator.IntervalAlignmentContextIterator.next(IntervalAlignmentContextIterator.java:21); 	at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.loadNextAssemblyRegion(AssemblyRegionIterator.java:143); 	at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.next(AssemblyRegionIterator.java:135); 	at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.next(AssemblyRegionIterator.java:34); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:290); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:271); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:893); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4665
https://github.com/broadinstitute/gatk/issues/4667:695,Availability,error,error,695,"I think this user sums it up nicely:. ""Unless I am missing something, the current set up in GATK 4.0 is not ideal for routine sequencing. First, I need to combine all animals every time a new batch of data is added (rather than adding a batch to existing database). Second, if I decide to use combineGVCFs in GATK 4.0, I have to run it twice, first to combine the new cohort, then to combine the new cohort with older animals so that I have 1 file to feed to genotypegVCF. . As such I am now reverting back to GATK 3.6. It would be very nice if genomicDBimport allowed addition of new data to existing database, and/or genotypegVCF allowed multiple gVCFs."". ----; User Report; ----. This is the error message I am getting but it doesn't make much sense, given that I got it also on smaller datasets with large amounts of memory (larger than some of the specifications listed on this forum for genomicDBimport). Our IT people, after observing the job, seem to think this is a java-related bug as the process itself doesn't use anywhere near the memory specified. We have installed a new version of java and I will be re-running the analysis to see if this solved the issues. . I'm not sure if I should be creating a new thread for this, but I do have a general comment about genomicDBimport. The project I am involved with is in partnership with an industrial partner, who sequences a number of animals every few weeks. In the pipeline using GATK 3.6, the newly sequenced animals were combined using combinegVCF and multiple gVCFs were then fed into genotypeGVCFs. . Unless I am missing something, the current set up in GATK 4.0 is not ideal for routine sequencing. First, I need to combine all animals every time a new batch of data is added (rather than adding a batch to existing database). Second, if I decide to use combineGVCFs in GATK 4.0, I have to run it twice, first to combine the new cohort, then to combine the new cohort with older animals so that I have 1 file to feed to genotypegVCF. .",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4667
https://github.com/broadinstitute/gatk/issues/4667:1070,Deployability,install,installed,1070," routine sequencing. First, I need to combine all animals every time a new batch of data is added (rather than adding a batch to existing database). Second, if I decide to use combineGVCFs in GATK 4.0, I have to run it twice, first to combine the new cohort, then to combine the new cohort with older animals so that I have 1 file to feed to genotypegVCF. . As such I am now reverting back to GATK 3.6. It would be very nice if genomicDBimport allowed addition of new data to existing database, and/or genotypegVCF allowed multiple gVCFs."". ----; User Report; ----. This is the error message I am getting but it doesn't make much sense, given that I got it also on smaller datasets with large amounts of memory (larger than some of the specifications listed on this forum for genomicDBimport). Our IT people, after observing the job, seem to think this is a java-related bug as the process itself doesn't use anywhere near the memory specified. We have installed a new version of java and I will be re-running the analysis to see if this solved the issues. . I'm not sure if I should be creating a new thread for this, but I do have a general comment about genomicDBimport. The project I am involved with is in partnership with an industrial partner, who sequences a number of animals every few weeks. In the pipeline using GATK 3.6, the newly sequenced animals were combined using combinegVCF and multiple gVCFs were then fed into genotypeGVCFs. . Unless I am missing something, the current set up in GATK 4.0 is not ideal for routine sequencing. First, I need to combine all animals every time a new batch of data is added (rather than adding a batch to existing database). Second, if I decide to use combineGVCFs in GATK 4.0, I have to run it twice, first to combine the new cohort, then to combine the new cohort with older animals so that I have 1 file to feed to genotypegVCF. . As such I am now reverting back to GATK 3.6. It would be very nice if genomicDBimport allowed addition of new data ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4667
https://github.com/broadinstitute/gatk/issues/4667:1426,Deployability,pipeline,pipeline,1426," have to run it twice, first to combine the new cohort, then to combine the new cohort with older animals so that I have 1 file to feed to genotypegVCF. . As such I am now reverting back to GATK 3.6. It would be very nice if genomicDBimport allowed addition of new data to existing database, and/or genotypegVCF allowed multiple gVCFs."". ----; User Report; ----. This is the error message I am getting but it doesn't make much sense, given that I got it also on smaller datasets with large amounts of memory (larger than some of the specifications listed on this forum for genomicDBimport). Our IT people, after observing the job, seem to think this is a java-related bug as the process itself doesn't use anywhere near the memory specified. We have installed a new version of java and I will be re-running the analysis to see if this solved the issues. . I'm not sure if I should be creating a new thread for this, but I do have a general comment about genomicDBimport. The project I am involved with is in partnership with an industrial partner, who sequences a number of animals every few weeks. In the pipeline using GATK 3.6, the newly sequenced animals were combined using combinegVCF and multiple gVCFs were then fed into genotypeGVCFs. . Unless I am missing something, the current set up in GATK 4.0 is not ideal for routine sequencing. First, I need to combine all animals every time a new batch of data is added (rather than adding a batch to existing database). Second, if I decide to use combineGVCFs in GATK 4.0, I have to run it twice, first to combine the new cohort, then to combine the new cohort with older animals so that I have 1 file to feed to genotypegVCF. . As such I am now reverting back to GATK 3.6. It would be very nice if genomicDBimport allowed addition of new data to existing database, and/or genotypegVCF allowed multiple gVCFs. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/comment/47819#Comment_47819",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4667
https://github.com/broadinstitute/gatk/issues/4667:118,Integrability,rout,routine,118,"I think this user sums it up nicely:. ""Unless I am missing something, the current set up in GATK 4.0 is not ideal for routine sequencing. First, I need to combine all animals every time a new batch of data is added (rather than adding a batch to existing database). Second, if I decide to use combineGVCFs in GATK 4.0, I have to run it twice, first to combine the new cohort, then to combine the new cohort with older animals so that I have 1 file to feed to genotypegVCF. . As such I am now reverting back to GATK 3.6. It would be very nice if genomicDBimport allowed addition of new data to existing database, and/or genotypegVCF allowed multiple gVCFs."". ----; User Report; ----. This is the error message I am getting but it doesn't make much sense, given that I got it also on smaller datasets with large amounts of memory (larger than some of the specifications listed on this forum for genomicDBimport). Our IT people, after observing the job, seem to think this is a java-related bug as the process itself doesn't use anywhere near the memory specified. We have installed a new version of java and I will be re-running the analysis to see if this solved the issues. . I'm not sure if I should be creating a new thread for this, but I do have a general comment about genomicDBimport. The project I am involved with is in partnership with an industrial partner, who sequences a number of animals every few weeks. In the pipeline using GATK 3.6, the newly sequenced animals were combined using combinegVCF and multiple gVCFs were then fed into genotypeGVCFs. . Unless I am missing something, the current set up in GATK 4.0 is not ideal for routine sequencing. First, I need to combine all animals every time a new batch of data is added (rather than adding a batch to existing database). Second, if I decide to use combineGVCFs in GATK 4.0, I have to run it twice, first to combine the new cohort, then to combine the new cohort with older animals so that I have 1 file to feed to genotypegVCF. .",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4667
https://github.com/broadinstitute/gatk/issues/4667:701,Integrability,message,message,701,"I think this user sums it up nicely:. ""Unless I am missing something, the current set up in GATK 4.0 is not ideal for routine sequencing. First, I need to combine all animals every time a new batch of data is added (rather than adding a batch to existing database). Second, if I decide to use combineGVCFs in GATK 4.0, I have to run it twice, first to combine the new cohort, then to combine the new cohort with older animals so that I have 1 file to feed to genotypegVCF. . As such I am now reverting back to GATK 3.6. It would be very nice if genomicDBimport allowed addition of new data to existing database, and/or genotypegVCF allowed multiple gVCFs."". ----; User Report; ----. This is the error message I am getting but it doesn't make much sense, given that I got it also on smaller datasets with large amounts of memory (larger than some of the specifications listed on this forum for genomicDBimport). Our IT people, after observing the job, seem to think this is a java-related bug as the process itself doesn't use anywhere near the memory specified. We have installed a new version of java and I will be re-running the analysis to see if this solved the issues. . I'm not sure if I should be creating a new thread for this, but I do have a general comment about genomicDBimport. The project I am involved with is in partnership with an industrial partner, who sequences a number of animals every few weeks. In the pipeline using GATK 3.6, the newly sequenced animals were combined using combinegVCF and multiple gVCFs were then fed into genotypeGVCFs. . Unless I am missing something, the current set up in GATK 4.0 is not ideal for routine sequencing. First, I need to combine all animals every time a new batch of data is added (rather than adding a batch to existing database). Second, if I decide to use combineGVCFs in GATK 4.0, I have to run it twice, first to combine the new cohort, then to combine the new cohort with older animals so that I have 1 file to feed to genotypegVCF. .",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4667
https://github.com/broadinstitute/gatk/issues/4667:1645,Integrability,rout,routine,1645," have to run it twice, first to combine the new cohort, then to combine the new cohort with older animals so that I have 1 file to feed to genotypegVCF. . As such I am now reverting back to GATK 3.6. It would be very nice if genomicDBimport allowed addition of new data to existing database, and/or genotypegVCF allowed multiple gVCFs."". ----; User Report; ----. This is the error message I am getting but it doesn't make much sense, given that I got it also on smaller datasets with large amounts of memory (larger than some of the specifications listed on this forum for genomicDBimport). Our IT people, after observing the job, seem to think this is a java-related bug as the process itself doesn't use anywhere near the memory specified. We have installed a new version of java and I will be re-running the analysis to see if this solved the issues. . I'm not sure if I should be creating a new thread for this, but I do have a general comment about genomicDBimport. The project I am involved with is in partnership with an industrial partner, who sequences a number of animals every few weeks. In the pipeline using GATK 3.6, the newly sequenced animals were combined using combinegVCF and multiple gVCFs were then fed into genotypeGVCFs. . Unless I am missing something, the current set up in GATK 4.0 is not ideal for routine sequencing. First, I need to combine all animals every time a new batch of data is added (rather than adding a batch to existing database). Second, if I decide to use combineGVCFs in GATK 4.0, I have to run it twice, first to combine the new cohort, then to combine the new cohort with older animals so that I have 1 file to feed to genotypegVCF. . As such I am now reverting back to GATK 3.6. It would be very nice if genomicDBimport allowed addition of new data to existing database, and/or genotypegVCF allowed multiple gVCFs. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/comment/47819#Comment_47819",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4667
https://github.com/broadinstitute/gatk/issues/4669:24,Availability,Down,Downloading,24,"""""""""; gatk]# ./gradlew; Downloading https://services.gradle.org/distributions/gradle-3.1-bin.zip; ............................................; Download https://repo1.maven.org/maven2/commons-codec/commons-codec/1.6/commons-codec-1.6.jar; Executing: git lfs pull --include src/main/resources/large. FAILURE: Build failed with an exception. * Where:; Build file '/data/md1/zhouyajun/biotools/gatk/gatk/build.gradle' line: 102. * What went wrong:; A problem occurred evaluating root project 'gatk'.; > Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 1. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED; """"""; what should I do ?; How can I install GATK4 successful?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4669
https://github.com/broadinstitute/gatk/issues/4669:144,Availability,Down,Download,144,"""""""""; gatk]# ./gradlew; Downloading https://services.gradle.org/distributions/gradle-3.1-bin.zip; ............................................; Download https://repo1.maven.org/maven2/commons-codec/commons-codec/1.6/commons-codec-1.6.jar; Executing: git lfs pull --include src/main/resources/large. FAILURE: Build failed with an exception. * Where:; Build file '/data/md1/zhouyajun/biotools/gatk/gatk/build.gradle' line: 102. * What went wrong:; A problem occurred evaluating root project 'gatk'.; > Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 1. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED; """"""; what should I do ?; How can I install GATK4 successful?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4669
https://github.com/broadinstitute/gatk/issues/4669:299,Availability,FAILURE,FAILURE,299,"""""""""; gatk]# ./gradlew; Downloading https://services.gradle.org/distributions/gradle-3.1-bin.zip; ............................................; Download https://repo1.maven.org/maven2/commons-codec/commons-codec/1.6/commons-codec-1.6.jar; Executing: git lfs pull --include src/main/resources/large. FAILURE: Build failed with an exception. * Where:; Build file '/data/md1/zhouyajun/biotools/gatk/gatk/build.gradle' line: 102. * What went wrong:; A problem occurred evaluating root project 'gatk'.; > Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 1. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED; """"""; what should I do ?; How can I install GATK4 successful?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4669
https://github.com/broadinstitute/gatk/issues/4669:757,Deployability,install,install,757,"""""""""; gatk]# ./gradlew; Downloading https://services.gradle.org/distributions/gradle-3.1-bin.zip; ............................................; Download https://repo1.maven.org/maven2/commons-codec/commons-codec/1.6/commons-codec-1.6.jar; Executing: git lfs pull --include src/main/resources/large. FAILURE: Build failed with an exception. * Where:; Build file '/data/md1/zhouyajun/biotools/gatk/gatk/build.gradle' line: 102. * What went wrong:; A problem occurred evaluating root project 'gatk'.; > Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 1. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED; """"""; what should I do ?; How can I install GATK4 successful?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4669
https://github.com/broadinstitute/gatk/issues/4669:696,Testability,log,log,696,"""""""""; gatk]# ./gradlew; Downloading https://services.gradle.org/distributions/gradle-3.1-bin.zip; ............................................; Download https://repo1.maven.org/maven2/commons-codec/commons-codec/1.6/commons-codec-1.6.jar; Executing: git lfs pull --include src/main/resources/large. FAILURE: Build failed with an exception. * Where:; Build file '/data/md1/zhouyajun/biotools/gatk/gatk/build.gradle' line: 102. * What went wrong:; A problem occurred evaluating root project 'gatk'.; > Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 1. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED; """"""; what should I do ?; How can I install GATK4 successful?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4669
https://github.com/broadinstitute/gatk/pull/4670:229,Availability,error,errors,229,"fix #4648 . The problems are in clipping the alignments when de-overlapping the alignments, particularly computing the new ref and read spans. Also adds a check on arguments used for constructing alignments, which helped finding errors in test data (now corrected)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4670
https://github.com/broadinstitute/gatk/pull/4670:239,Testability,test,test,239,"fix #4648 . The problems are in clipping the alignments when de-overlapping the alignments, particularly computing the new ref and read spans. Also adds a check on arguments used for constructing alignments, which helped finding errors in test data (now corrected)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4670
https://github.com/broadinstitute/gatk/issues/4672:503,Security,validat,validateArg,503,"This happens whenever the start position of an interval for which intermediate bands must be created is less than the value `of break-bands-at-multiples-of`. For example, an input reference block record with a `start` position (say 1) that is less than the value of `of break-bands-at-multiples-of` (say 10000) would result in the invalid intermediate band interval:; ```; java.lang.IllegalArgumentException: Invalid interval. Contig:chr21 start:-1 end:-1. 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); 	at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); 	at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); 	at org.broadinstitute.hellbender.tools.walkers.CombineGVCFs.createIntermediateVariants(CombineGVCFs.java:191); 	at org.broadinstitute.hellbender.tools.walkers.CombineGVCFs.apply(CombineGVCFs.java:134). ```; This doesn't happen in GATK3.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4672
https://github.com/broadinstitute/gatk/issues/4672:587,Security,validat,validatePositions,587,"This happens whenever the start position of an interval for which intermediate bands must be created is less than the value `of break-bands-at-multiples-of`. For example, an input reference block record with a `start` position (say 1) that is less than the value of `of break-bands-at-multiples-of` (say 10000) would result in the invalid intermediate band interval:; ```; java.lang.IllegalArgumentException: Invalid interval. Contig:chr21 start:-1 end:-1. 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); 	at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); 	at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); 	at org.broadinstitute.hellbender.tools.walkers.CombineGVCFs.createIntermediateVariants(CombineGVCFs.java:191); 	at org.broadinstitute.hellbender.tools.walkers.CombineGVCFs.apply(CombineGVCFs.java:134). ```; This doesn't happen in GATK3.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4672
https://github.com/broadinstitute/gatk/issues/4672:572,Usability,Simpl,SimpleInterval,572,"This happens whenever the start position of an interval for which intermediate bands must be created is less than the value `of break-bands-at-multiples-of`. For example, an input reference block record with a `start` position (say 1) that is less than the value of `of break-bands-at-multiples-of` (say 10000) would result in the invalid intermediate band interval:; ```; java.lang.IllegalArgumentException: Invalid interval. Contig:chr21 start:-1 end:-1. 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); 	at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); 	at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); 	at org.broadinstitute.hellbender.tools.walkers.CombineGVCFs.createIntermediateVariants(CombineGVCFs.java:191); 	at org.broadinstitute.hellbender.tools.walkers.CombineGVCFs.apply(CombineGVCFs.java:134). ```; This doesn't happen in GATK3.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4672
https://github.com/broadinstitute/gatk/issues/4672:605,Usability,Simpl,SimpleInterval,605,"This happens whenever the start position of an interval for which intermediate bands must be created is less than the value `of break-bands-at-multiples-of`. For example, an input reference block record with a `start` position (say 1) that is less than the value of `of break-bands-at-multiples-of` (say 10000) would result in the invalid intermediate band interval:; ```; java.lang.IllegalArgumentException: Invalid interval. Contig:chr21 start:-1 end:-1. 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); 	at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); 	at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); 	at org.broadinstitute.hellbender.tools.walkers.CombineGVCFs.createIntermediateVariants(CombineGVCFs.java:191); 	at org.broadinstitute.hellbender.tools.walkers.CombineGVCFs.apply(CombineGVCFs.java:134). ```; This doesn't happen in GATK3.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4672
https://github.com/broadinstitute/gatk/issues/4672:670,Usability,Simpl,SimpleInterval,670,"This happens whenever the start position of an interval for which intermediate bands must be created is less than the value `of break-bands-at-multiples-of`. For example, an input reference block record with a `start` position (say 1) that is less than the value of `of break-bands-at-multiples-of` (say 10000) would result in the invalid intermediate band interval:; ```; java.lang.IllegalArgumentException: Invalid interval. Contig:chr21 start:-1 end:-1. 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); 	at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); 	at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); 	at org.broadinstitute.hellbender.tools.walkers.CombineGVCFs.createIntermediateVariants(CombineGVCFs.java:191); 	at org.broadinstitute.hellbender.tools.walkers.CombineGVCFs.apply(CombineGVCFs.java:134). ```; This doesn't happen in GATK3.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4672
https://github.com/broadinstitute/gatk/issues/4672:692,Usability,Simpl,SimpleInterval,692,"This happens whenever the start position of an interval for which intermediate bands must be created is less than the value `of break-bands-at-multiples-of`. For example, an input reference block record with a `start` position (say 1) that is less than the value of `of break-bands-at-multiples-of` (say 10000) would result in the invalid intermediate band interval:; ```; java.lang.IllegalArgumentException: Invalid interval. Contig:chr21 start:-1 end:-1. 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); 	at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); 	at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); 	at org.broadinstitute.hellbender.tools.walkers.CombineGVCFs.createIntermediateVariants(CombineGVCFs.java:191); 	at org.broadinstitute.hellbender.tools.walkers.CombineGVCFs.apply(CombineGVCFs.java:134). ```; This doesn't happen in GATK3.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4672
https://github.com/broadinstitute/gatk/pull/4674:267,Modifiability,plugin,plugin,267,@cmnbroad I left some of the AnnoationManager code in VariantAnnotatorEngine because there were tests for VariantAnnotatorEngine which required `ofSelectedMinusExcluded` and it seemed clunky to achieve the same thing through the creation and execution of an abstract plugin. I can change it if you would like. Fixes #3287,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4674
https://github.com/broadinstitute/gatk/pull/4674:96,Testability,test,tests,96,@cmnbroad I left some of the AnnoationManager code in VariantAnnotatorEngine because there were tests for VariantAnnotatorEngine which required `ofSelectedMinusExcluded` and it seemed clunky to achieve the same thing through the creation and execution of an abstract plugin. I can change it if you would like. Fixes #3287,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4674
https://github.com/broadinstitute/gatk/issues/4675:159,Availability,robust,robust,159,MarkDuplicates Spark output needs to tested against the version of picard they use in production to ensure that it produces identical output and is reasonably robust to pathological files. This requires that the following issues have been resolved:; #3705 ; #3706,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675
https://github.com/broadinstitute/gatk/issues/4675:37,Testability,test,tested,37,MarkDuplicates Spark output needs to tested against the version of picard they use in production to ensure that it produces identical output and is reasonably robust to pathological files. This requires that the following issues have been resolved:; #3705 ; #3706,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675
https://github.com/broadinstitute/gatk/pull/4676:83,Deployability,release,release,83,upgrading picard dependency from 2.18.1 -> 2.18.2. this way we'll be on the latest release when we start doing MarkDuplicates tieout,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4676
https://github.com/broadinstitute/gatk/pull/4676:17,Integrability,depend,dependency,17,upgrading picard dependency from 2.18.1 -> 2.18.2. this way we'll be on the latest release when we start doing MarkDuplicates tieout,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4676
https://github.com/broadinstitute/gatk/pull/4677:171,Deployability,update,updated,171,"fix #4649 . The cause of the exception is a new edge case that was not imagined when the reference region segmenting logic was initially written.; It is now covered, with updated tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4677
https://github.com/broadinstitute/gatk/pull/4677:117,Testability,log,logic,117,"fix #4649 . The cause of the exception is a new edge case that was not imagined when the reference region segmenting logic was initially written.; It is now covered, with updated tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4677
https://github.com/broadinstitute/gatk/pull/4677:179,Testability,test,tests,179,"fix #4649 . The cause of the exception is a new edge case that was not imagined when the reference region segmenting logic was initially written.; It is now covered, with updated tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4677
https://github.com/broadinstitute/gatk/pull/4678:7,Availability,error,error,7,Update error message based on https://github.com/broadinstitute/gatk/issues/4669.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4678
https://github.com/broadinstitute/gatk/pull/4678:0,Deployability,Update,Update,0,Update error message based on https://github.com/broadinstitute/gatk/issues/4669.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4678
https://github.com/broadinstitute/gatk/pull/4678:13,Integrability,message,message,13,Update error message based on https://github.com/broadinstitute/gatk/issues/4669.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4678
https://github.com/broadinstitute/gatk/issues/4679:145,Availability,error,error,145,"Dear Sir or Madame, . I am testing the germline CNV tools (GATK version 4.0.3.0). I know it is still in beta but maybe you can tell me what this error is about. . ```; 13:16:06.332 INFO gcnvkernel.tasks.inference_task_base - (sampling epoch 1): 0%| | 0/100 [00:00<?, ?it/s]; 13:16:08.494 INFO gcnvkernel.tasks.inference_task_base - (sampling epoch 1) relative error: 1.0000 +/- 0.0000: 1%|1 | 1/100 [00:02<03:33, 2.16s/it]; 13:16:10.652 INFO gcnvkernel.tasks.inference_task_base -; Stderr: Traceback (most recent call last):; File ""/tmp/die9s/cohort_determine_ploidy_and_depth.861556744637254264.py"", line 106, in <module>; gcnvkernel.io_ploidy.PloidyModelWriter(ploidy_config, ploidy_workspace,; AttributeError: module 'gcnvkernel.io.io_ploidy' has no attribute 'PloidyModelWriter'. at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75); at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCuratedArgs(ScriptExecutor.java:126); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeArgs(PythonScriptExecutor.java:170); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeScript(PythonScriptExecutor.java:151); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeScript(PythonScriptExecutor.java:121); at org.broadinstitute.hellbender.tools.copynumber.DetermineGermlineContigPloidy.executeDeterminePloidyAndDepthPythonScript(DetermineGermlineContigPloidy.java:365); at org.broadinstitute.hellbender.tools.copynumber.DetermineGermlineContigPloidy.doWork(DetermineGermlineContigPloidy.java:263); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLin",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4679
https://github.com/broadinstitute/gatk/issues/4679:360,Availability,error,error,360,"Dear Sir or Madame, . I am testing the germline CNV tools (GATK version 4.0.3.0). I know it is still in beta but maybe you can tell me what this error is about. . ```; 13:16:06.332 INFO gcnvkernel.tasks.inference_task_base - (sampling epoch 1): 0%| | 0/100 [00:00<?, ?it/s]; 13:16:08.494 INFO gcnvkernel.tasks.inference_task_base - (sampling epoch 1) relative error: 1.0000 +/- 0.0000: 1%|1 | 1/100 [00:02<03:33, 2.16s/it]; 13:16:10.652 INFO gcnvkernel.tasks.inference_task_base -; Stderr: Traceback (most recent call last):; File ""/tmp/die9s/cohort_determine_ploidy_and_depth.861556744637254264.py"", line 106, in <module>; gcnvkernel.io_ploidy.PloidyModelWriter(ploidy_config, ploidy_workspace,; AttributeError: module 'gcnvkernel.io.io_ploidy' has no attribute 'PloidyModelWriter'. at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75); at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCuratedArgs(ScriptExecutor.java:126); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeArgs(PythonScriptExecutor.java:170); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeScript(PythonScriptExecutor.java:151); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeScript(PythonScriptExecutor.java:121); at org.broadinstitute.hellbender.tools.copynumber.DetermineGermlineContigPloidy.executeDeterminePloidyAndDepthPythonScript(DetermineGermlineContigPloidy.java:365); at org.broadinstitute.hellbender.tools.copynumber.DetermineGermlineContigPloidy.doWork(DetermineGermlineContigPloidy.java:263); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLin",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4679
https://github.com/broadinstitute/gatk/issues/4679:2185,Availability,error,errors,2185,"; 13:16:10.652 INFO gcnvkernel.tasks.inference_task_base -; Stderr: Traceback (most recent call last):; File ""/tmp/die9s/cohort_determine_ploidy_and_depth.861556744637254264.py"", line 106, in <module>; gcnvkernel.io_ploidy.PloidyModelWriter(ploidy_config, ploidy_workspace,; AttributeError: module 'gcnvkernel.io.io_ploidy' has no attribute 'PloidyModelWriter'. at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75); at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCuratedArgs(ScriptExecutor.java:126); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeArgs(PythonScriptExecutor.java:170); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeScript(PythonScriptExecutor.java:151); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeScript(PythonScriptExecutor.java:121); at org.broadinstitute.hellbender.tools.copynumber.DetermineGermlineContigPloidy.executeDeterminePloidyAndDepthPythonScript(DetermineGermlineContigPloidy.java:365); at org.broadinstitute.hellbender.tools.copynumber.DetermineGermlineContigPloidy.doWork(DetermineGermlineContigPloidy.java:263); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. I also know that I should post errors in the GATK forum, but when I do this I get the following error message :; ``` { ""Code"": 403, ""Exception"": ""You need the Garden.Community.Manage permission to do that."", ""Class"": ""Gdn_UserException"" }```. Thanks in advance; Stefan",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4679
https://github.com/broadinstitute/gatk/issues/4679:2250,Availability,error,error,2250,"; 13:16:10.652 INFO gcnvkernel.tasks.inference_task_base -; Stderr: Traceback (most recent call last):; File ""/tmp/die9s/cohort_determine_ploidy_and_depth.861556744637254264.py"", line 106, in <module>; gcnvkernel.io_ploidy.PloidyModelWriter(ploidy_config, ploidy_workspace,; AttributeError: module 'gcnvkernel.io.io_ploidy' has no attribute 'PloidyModelWriter'. at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75); at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCuratedArgs(ScriptExecutor.java:126); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeArgs(PythonScriptExecutor.java:170); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeScript(PythonScriptExecutor.java:151); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeScript(PythonScriptExecutor.java:121); at org.broadinstitute.hellbender.tools.copynumber.DetermineGermlineContigPloidy.executeDeterminePloidyAndDepthPythonScript(DetermineGermlineContigPloidy.java:365); at org.broadinstitute.hellbender.tools.copynumber.DetermineGermlineContigPloidy.doWork(DetermineGermlineContigPloidy.java:263); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. I also know that I should post errors in the GATK forum, but when I do this I get the following error message :; ``` { ""Code"": 403, ""Exception"": ""You need the Garden.Community.Manage permission to do that."", ""Class"": ""Gdn_UserException"" }```. Thanks in advance; Stefan",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4679
https://github.com/broadinstitute/gatk/issues/4679:2256,Integrability,message,message,2256,"; 13:16:10.652 INFO gcnvkernel.tasks.inference_task_base -; Stderr: Traceback (most recent call last):; File ""/tmp/die9s/cohort_determine_ploidy_and_depth.861556744637254264.py"", line 106, in <module>; gcnvkernel.io_ploidy.PloidyModelWriter(ploidy_config, ploidy_workspace,; AttributeError: module 'gcnvkernel.io.io_ploidy' has no attribute 'PloidyModelWriter'. at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75); at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCuratedArgs(ScriptExecutor.java:126); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeArgs(PythonScriptExecutor.java:170); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeScript(PythonScriptExecutor.java:151); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeScript(PythonScriptExecutor.java:121); at org.broadinstitute.hellbender.tools.copynumber.DetermineGermlineContigPloidy.executeDeterminePloidyAndDepthPythonScript(DetermineGermlineContigPloidy.java:365); at org.broadinstitute.hellbender.tools.copynumber.DetermineGermlineContigPloidy.doWork(DetermineGermlineContigPloidy.java:263); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. I also know that I should post errors in the GATK forum, but when I do this I get the following error message :; ``` { ""Code"": 403, ""Exception"": ""You need the Garden.Community.Manage permission to do that."", ""Class"": ""Gdn_UserException"" }```. Thanks in advance; Stefan",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4679
https://github.com/broadinstitute/gatk/issues/4679:27,Testability,test,testing,27,"Dear Sir or Madame, . I am testing the germline CNV tools (GATK version 4.0.3.0). I know it is still in beta but maybe you can tell me what this error is about. . ```; 13:16:06.332 INFO gcnvkernel.tasks.inference_task_base - (sampling epoch 1): 0%| | 0/100 [00:00<?, ?it/s]; 13:16:08.494 INFO gcnvkernel.tasks.inference_task_base - (sampling epoch 1) relative error: 1.0000 +/- 0.0000: 1%|1 | 1/100 [00:02<03:33, 2.16s/it]; 13:16:10.652 INFO gcnvkernel.tasks.inference_task_base -; Stderr: Traceback (most recent call last):; File ""/tmp/die9s/cohort_determine_ploidy_and_depth.861556744637254264.py"", line 106, in <module>; gcnvkernel.io_ploidy.PloidyModelWriter(ploidy_config, ploidy_workspace,; AttributeError: module 'gcnvkernel.io.io_ploidy' has no attribute 'PloidyModelWriter'. at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75); at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCuratedArgs(ScriptExecutor.java:126); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeArgs(PythonScriptExecutor.java:170); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeScript(PythonScriptExecutor.java:151); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeScript(PythonScriptExecutor.java:121); at org.broadinstitute.hellbender.tools.copynumber.DetermineGermlineContigPloidy.executeDeterminePloidyAndDepthPythonScript(DetermineGermlineContigPloidy.java:365); at org.broadinstitute.hellbender.tools.copynumber.DetermineGermlineContigPloidy.doWork(DetermineGermlineContigPloidy.java:263); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLin",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4679
https://github.com/broadinstitute/gatk/pull/4680:161,Modifiability,rewrite,rewrite,161,Fixes for https://github.com/broadinstitute/gatk/issues/4525 and https://github.com/broadinstitute/gatk/issues/4633. Longer term we should do a more significant rewrite/refactoring.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4680
https://github.com/broadinstitute/gatk/pull/4680:169,Modifiability,refactor,refactoring,169,Fixes for https://github.com/broadinstitute/gatk/issues/4525 and https://github.com/broadinstitute/gatk/issues/4633. Longer term we should do a more significant rewrite/refactoring.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4680
https://github.com/broadinstitute/gatk/pull/4682:205,Integrability,interface,interface,205,Includes:. * `ShardingIterator` to avoid too many query calls; * `ReadSliderWalker` with documentation on how to implement (edited: walker name is `SlidingWindowReadWalker` instead); * Argument collection interface for sharding arguments; * Example walker and argument collection; * Tests for example walker. Closes https://github.com/broadinstitute/gatk/issues/1198,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4682
https://github.com/broadinstitute/gatk/pull/4682:35,Safety,avoid,avoid,35,Includes:. * `ShardingIterator` to avoid too many query calls; * `ReadSliderWalker` with documentation on how to implement (edited: walker name is `SlidingWindowReadWalker` instead); * Argument collection interface for sharding arguments; * Example walker and argument collection; * Tests for example walker. Closes https://github.com/broadinstitute/gatk/issues/1198,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4682
https://github.com/broadinstitute/gatk/pull/4682:283,Testability,Test,Tests,283,Includes:. * `ShardingIterator` to avoid too many query calls; * `ReadSliderWalker` with documentation on how to implement (edited: walker name is `SlidingWindowReadWalker` instead); * Argument collection interface for sharding arguments; * Example walker and argument collection; * Tests for example walker. Closes https://github.com/broadinstitute/gatk/issues/1198,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4682
https://github.com/broadinstitute/gatk/issues/4683:523,Availability,error,error,523,"Discovered in v4.0.1.1, recapitulates in v4.0.3.0. ModelSegments `--number-of-smoothing-iterations-per-fit` option default is 0 and accepts any integer I give it. ![screenshot 2018-04-19 09 54 13](https://user-images.githubusercontent.com/11543866/38995751-9c85f3d2-43b7-11e8-84be-4793da9c123f.png). However, the number of smoothing iterations changes only when the parameter is set to one. It does not change for 2, 5, 25, etc and if given these non-one numbers, behaves as if it is set to zero, without any exception nor error. Seems this option should be a boolian type, where there is either smoothing per fit (1) or not (0), in which case it should be renamed to represent its type. Let me know if this is just a matter of clarifying in the documentation that the parameter takes only 0 or 1 argument.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4683
https://github.com/broadinstitute/gatk/issues/4684:15,Modifiability,evolve,evolve,15,"As the scripts evolve to be more and more complicated, ; it is time to plan a transition from scripts to a Java tool in GATK. Following the structure that is set up by the scripts in PR #4406 ,; the 1st stage development could be:. 1. parse and check the call sets emitted by callers; basically this is to make sure the tool won't be ""surprised"" by the call sets' ""features"" (bash scripts do this); 2. some basic accounting and metrics, e.g. SINE, LINE peaks (bash scripts do accounting and plots); 3. simple overlap-based TP/FP/FN analysis (bash scripts rely on bedtools for such purpose); 4. Basic reporting on FN/FP rates (bash scripts print a slew of information to screen). Variant files from different callers have their own quirks, (the BND records don't help) having a general purpose tool that covers all major callers is going to take a hefty investment, so we could start from PacBio and GATK-SV call sets.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4684
https://github.com/broadinstitute/gatk/issues/4684:502,Usability,simpl,simple,502,"As the scripts evolve to be more and more complicated, ; it is time to plan a transition from scripts to a Java tool in GATK. Following the structure that is set up by the scripts in PR #4406 ,; the 1st stage development could be:. 1. parse and check the call sets emitted by callers; basically this is to make sure the tool won't be ""surprised"" by the call sets' ""features"" (bash scripts do this); 2. some basic accounting and metrics, e.g. SINE, LINE peaks (bash scripts do accounting and plots); 3. simple overlap-based TP/FP/FN analysis (bash scripts rely on bedtools for such purpose); 4. Basic reporting on FN/FP rates (bash scripts print a slew of information to screen). Variant files from different callers have their own quirks, (the BND records don't help) having a general purpose tool that covers all major callers is going to take a hefty investment, so we could start from PacBio and GATK-SV call sets.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4684
https://github.com/broadinstitute/gatk/issues/4685:160,Availability,down,downloads,160,"Hi,; first of all, I find it very awkward that after 3.8 release there is 3.8-1. Why the dash instead of a dot, as usual? It only complicates automated package downloads which in general work with numbers separated by dots. You just mix together two schemes. Is that really necessary?. Anyway, the pom.xml is broken:. ```; >>> Preparing source in /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1 ...; Equivalent maven command; mvn -Dmaven.repo.local=/scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/.m2/repository verify '-Ddisable.shadepackage'; [INFO] Scanning for projects...; [ERROR] [ERROR] Some problems were encountered while processing the POMs:; [FATAL] Non-parseable POM /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml, line 15, column 3; @ ; [ERROR] The build could not read 1 project -> [Help 1]; [ERROR] ; [ERROR] The project org.broadinstitute.gatk:gatk-aggregator:[unknown-version] (/scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/pom.xml) has 1 error; [ERROR] Non-parseable POM /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml, line 15, column 3 -> [Help 2]; [ERROR] ; [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.; [ERROR] Re-run Maven using the -X switch to enable full debug logging.; [ERROR] ; [ERROR] For more information about the errors and possible solutions, please read the following articles:; [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/ProjectBuildingException; [ERROR] [Help 2] http://cwiki.apache.org/confluence/display/MAVEN/ModelPar",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4685
https://github.com/broadinstitute/gatk/issues/4685:600,Availability,ERROR,ERROR,600,"Hi,; first of all, I find it very awkward that after 3.8 release there is 3.8-1. Why the dash instead of a dot, as usual? It only complicates automated package downloads which in general work with numbers separated by dots. You just mix together two schemes. Is that really necessary?. Anyway, the pom.xml is broken:. ```; >>> Preparing source in /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1 ...; Equivalent maven command; mvn -Dmaven.repo.local=/scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/.m2/repository verify '-Ddisable.shadepackage'; [INFO] Scanning for projects...; [ERROR] [ERROR] Some problems were encountered while processing the POMs:; [FATAL] Non-parseable POM /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml, line 15, column 3; @ ; [ERROR] The build could not read 1 project -> [Help 1]; [ERROR] ; [ERROR] The project org.broadinstitute.gatk:gatk-aggregator:[unknown-version] (/scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/pom.xml) has 1 error; [ERROR] Non-parseable POM /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml, line 15, column 3 -> [Help 2]; [ERROR] ; [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.; [ERROR] Re-run Maven using the -X switch to enable full debug logging.; [ERROR] ; [ERROR] For more information about the errors and possible solutions, please read the following articles:; [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/ProjectBuildingException; [ERROR] [Help 2] http://cwiki.apache.org/confluence/display/MAVEN/ModelPar",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4685
https://github.com/broadinstitute/gatk/issues/4685:608,Availability,ERROR,ERROR,608,"Hi,; first of all, I find it very awkward that after 3.8 release there is 3.8-1. Why the dash instead of a dot, as usual? It only complicates automated package downloads which in general work with numbers separated by dots. You just mix together two schemes. Is that really necessary?. Anyway, the pom.xml is broken:. ```; >>> Preparing source in /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1 ...; Equivalent maven command; mvn -Dmaven.repo.local=/scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/.m2/repository verify '-Ddisable.shadepackage'; [INFO] Scanning for projects...; [ERROR] [ERROR] Some problems were encountered while processing the POMs:; [FATAL] Non-parseable POM /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml, line 15, column 3; @ ; [ERROR] The build could not read 1 project -> [Help 1]; [ERROR] ; [ERROR] The project org.broadinstitute.gatk:gatk-aggregator:[unknown-version] (/scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/pom.xml) has 1 error; [ERROR] Non-parseable POM /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml, line 15, column 3 -> [Help 2]; [ERROR] ; [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.; [ERROR] Re-run Maven using the -X switch to enable full debug logging.; [ERROR] ; [ERROR] For more information about the errors and possible solutions, please read the following articles:; [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/ProjectBuildingException; [ERROR] [Help 2] http://cwiki.apache.org/confluence/display/MAVEN/ModelPar",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4685
https://github.com/broadinstitute/gatk/issues/4685:994,Availability,ERROR,ERROR,994,"d it very awkward that after 3.8 release there is 3.8-1. Why the dash instead of a dot, as usual? It only complicates automated package downloads which in general work with numbers separated by dots. You just mix together two schemes. Is that really necessary?. Anyway, the pom.xml is broken:. ```; >>> Preparing source in /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1 ...; Equivalent maven command; mvn -Dmaven.repo.local=/scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/.m2/repository verify '-Ddisable.shadepackage'; [INFO] Scanning for projects...; [ERROR] [ERROR] Some problems were encountered while processing the POMs:; [FATAL] Non-parseable POM /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml, line 15, column 3; @ ; [ERROR] The build could not read 1 project -> [Help 1]; [ERROR] ; [ERROR] The project org.broadinstitute.gatk:gatk-aggregator:[unknown-version] (/scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/pom.xml) has 1 error; [ERROR] Non-parseable POM /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml, line 15, column 3 -> [Help 2]; [ERROR] ; [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.; [ERROR] Re-run Maven using the -X switch to enable full debug logging.; [ERROR] ; [ERROR] For more information about the errors and possible solutions, please read the following articles:; [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/ProjectBuildingException; [ERROR] [Help 2] http://cwiki.apache.org/confluence/display/MAVEN/ModelParseException; ```. ```; ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4685
https://github.com/broadinstitute/gatk/issues/4685:1050,Availability,ERROR,ERROR,1050,"d it very awkward that after 3.8 release there is 3.8-1. Why the dash instead of a dot, as usual? It only complicates automated package downloads which in general work with numbers separated by dots. You just mix together two schemes. Is that really necessary?. Anyway, the pom.xml is broken:. ```; >>> Preparing source in /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1 ...; Equivalent maven command; mvn -Dmaven.repo.local=/scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/.m2/repository verify '-Ddisable.shadepackage'; [INFO] Scanning for projects...; [ERROR] [ERROR] Some problems were encountered while processing the POMs:; [FATAL] Non-parseable POM /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml, line 15, column 3; @ ; [ERROR] The build could not read 1 project -> [Help 1]; [ERROR] ; [ERROR] The project org.broadinstitute.gatk:gatk-aggregator:[unknown-version] (/scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/pom.xml) has 1 error; [ERROR] Non-parseable POM /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml, line 15, column 3 -> [Help 2]; [ERROR] ; [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.; [ERROR] Re-run Maven using the -X switch to enable full debug logging.; [ERROR] ; [ERROR] For more information about the errors and possible solutions, please read the following articles:; [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/ProjectBuildingException; [ERROR] [Help 2] http://cwiki.apache.org/confluence/display/MAVEN/ModelParseException; ```. ```; ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4685
https://github.com/broadinstitute/gatk/issues/4685:1060,Availability,ERROR,ERROR,1060,"d it very awkward that after 3.8 release there is 3.8-1. Why the dash instead of a dot, as usual? It only complicates automated package downloads which in general work with numbers separated by dots. You just mix together two schemes. Is that really necessary?. Anyway, the pom.xml is broken:. ```; >>> Preparing source in /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1 ...; Equivalent maven command; mvn -Dmaven.repo.local=/scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/.m2/repository verify '-Ddisable.shadepackage'; [INFO] Scanning for projects...; [ERROR] [ERROR] Some problems were encountered while processing the POMs:; [FATAL] Non-parseable POM /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml, line 15, column 3; @ ; [ERROR] The build could not read 1 project -> [Help 1]; [ERROR] ; [ERROR] The project org.broadinstitute.gatk:gatk-aggregator:[unknown-version] (/scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/pom.xml) has 1 error; [ERROR] Non-parseable POM /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml, line 15, column 3 -> [Help 2]; [ERROR] ; [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.; [ERROR] Re-run Maven using the -X switch to enable full debug logging.; [ERROR] ; [ERROR] For more information about the errors and possible solutions, please read the following articles:; [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/ProjectBuildingException; [ERROR] [Help 2] http://cwiki.apache.org/confluence/display/MAVEN/ModelParseException; ```. ```; ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4685
https://github.com/broadinstitute/gatk/issues/4685:1217,Availability,error,error,1217,"schemes. Is that really necessary?. Anyway, the pom.xml is broken:. ```; >>> Preparing source in /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1 ...; Equivalent maven command; mvn -Dmaven.repo.local=/scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/.m2/repository verify '-Ddisable.shadepackage'; [INFO] Scanning for projects...; [ERROR] [ERROR] Some problems were encountered while processing the POMs:; [FATAL] Non-parseable POM /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml, line 15, column 3; @ ; [ERROR] The build could not read 1 project -> [Help 1]; [ERROR] ; [ERROR] The project org.broadinstitute.gatk:gatk-aggregator:[unknown-version] (/scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/pom.xml) has 1 error; [ERROR] Non-parseable POM /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml, line 15, column 3 -> [Help 2]; [ERROR] ; [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.; [ERROR] Re-run Maven using the -X switch to enable full debug logging.; [ERROR] ; [ERROR] For more information about the errors and possible solutions, please read the following articles:; [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/ProjectBuildingException; [ERROR] [Help 2] http://cwiki.apache.org/confluence/display/MAVEN/ModelParseException; ```. ```; 1 <?xml version=""1.0"" encoding=""UTF-8""?>; 2 <project xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4685
https://github.com/broadinstitute/gatk/issues/4685:1225,Availability,ERROR,ERROR,1225,"schemes. Is that really necessary?. Anyway, the pom.xml is broken:. ```; >>> Preparing source in /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1 ...; Equivalent maven command; mvn -Dmaven.repo.local=/scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/.m2/repository verify '-Ddisable.shadepackage'; [INFO] Scanning for projects...; [ERROR] [ERROR] Some problems were encountered while processing the POMs:; [FATAL] Non-parseable POM /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml, line 15, column 3; @ ; [ERROR] The build could not read 1 project -> [Help 1]; [ERROR] ; [ERROR] The project org.broadinstitute.gatk:gatk-aggregator:[unknown-version] (/scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/pom.xml) has 1 error; [ERROR] Non-parseable POM /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml, line 15, column 3 -> [Help 2]; [ERROR] ; [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.; [ERROR] Re-run Maven using the -X switch to enable full debug logging.; [ERROR] ; [ERROR] For more information about the errors and possible solutions, please read the following articles:; [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/ProjectBuildingException; [ERROR] [Help 2] http://cwiki.apache.org/confluence/display/MAVEN/ModelParseException; ```. ```; 1 <?xml version=""1.0"" encoding=""UTF-8""?>; 2 <project xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4685
https://github.com/broadinstitute/gatk/issues/4685:1552,Availability,ERROR,ERROR,1552,"ng for projects...; [ERROR] [ERROR] Some problems were encountered while processing the POMs:; [FATAL] Non-parseable POM /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml, line 15, column 3; @ ; [ERROR] The build could not read 1 project -> [Help 1]; [ERROR] ; [ERROR] The project org.broadinstitute.gatk:gatk-aggregator:[unknown-version] (/scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/pom.xml) has 1 error; [ERROR] Non-parseable POM /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml, line 15, column 3 -> [Help 2]; [ERROR] ; [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.; [ERROR] Re-run Maven using the -X switch to enable full debug logging.; [ERROR] ; [ERROR] For more information about the errors and possible solutions, please read the following articles:; [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/ProjectBuildingException; [ERROR] [Help 2] http://cwiki.apache.org/confluence/display/MAVEN/ModelParseException; ```. ```; 1 <?xml version=""1.0"" encoding=""UTF-8""?>; 2 <project xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd"">; 3 <modelVersion>4.0.0</modelVersion>; 4 ; 5 <!--; 6 This pom is parent for all gatk poms; 7 See also:; 8 http://maven.apache.org/pom.html#Inheritance_v; 9 http://maven.apache.org/guides/introduction/introduction-to-the-pom.html#Project_Inheritance_vs_Project_Aggregation; 10 http://stackoverflow.com/questions",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4685
https://github.com/broadinstitute/gatk/issues/4685:1562,Availability,ERROR,ERROR,1562,"ng for projects...; [ERROR] [ERROR] Some problems were encountered while processing the POMs:; [FATAL] Non-parseable POM /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml, line 15, column 3; @ ; [ERROR] The build could not read 1 project -> [Help 1]; [ERROR] ; [ERROR] The project org.broadinstitute.gatk:gatk-aggregator:[unknown-version] (/scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/pom.xml) has 1 error; [ERROR] Non-parseable POM /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml, line 15, column 3 -> [Help 2]; [ERROR] ; [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.; [ERROR] Re-run Maven using the -X switch to enable full debug logging.; [ERROR] ; [ERROR] For more information about the errors and possible solutions, please read the following articles:; [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/ProjectBuildingException; [ERROR] [Help 2] http://cwiki.apache.org/confluence/display/MAVEN/ModelParseException; ```. ```; 1 <?xml version=""1.0"" encoding=""UTF-8""?>; 2 <project xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd"">; 3 <modelVersion>4.0.0</modelVersion>; 4 ; 5 <!--; 6 This pom is parent for all gatk poms; 7 See also:; 8 http://maven.apache.org/pom.html#Inheritance_v; 9 http://maven.apache.org/guides/introduction/introduction-to-the-pom.html#Project_Inheritance_vs_Project_Aggregation; 10 http://stackoverflow.com/questions",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4685
https://github.com/broadinstitute/gatk/issues/4685:1604,Availability,error,errors,1604,"ng for projects...; [ERROR] [ERROR] Some problems were encountered while processing the POMs:; [FATAL] Non-parseable POM /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml, line 15, column 3; @ ; [ERROR] The build could not read 1 project -> [Help 1]; [ERROR] ; [ERROR] The project org.broadinstitute.gatk:gatk-aggregator:[unknown-version] (/scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/pom.xml) has 1 error; [ERROR] Non-parseable POM /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml, line 15, column 3 -> [Help 2]; [ERROR] ; [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.; [ERROR] Re-run Maven using the -X switch to enable full debug logging.; [ERROR] ; [ERROR] For more information about the errors and possible solutions, please read the following articles:; [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/ProjectBuildingException; [ERROR] [Help 2] http://cwiki.apache.org/confluence/display/MAVEN/ModelParseException; ```. ```; 1 <?xml version=""1.0"" encoding=""UTF-8""?>; 2 <project xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd"">; 3 <modelVersion>4.0.0</modelVersion>; 4 ; 5 <!--; 6 This pom is parent for all gatk poms; 7 See also:; 8 http://maven.apache.org/pom.html#Inheritance_v; 9 http://maven.apache.org/guides/introduction/introduction-to-the-pom.html#Project_Inheritance_vs_Project_Aggregation; 10 http://stackoverflow.com/questions",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4685
https://github.com/broadinstitute/gatk/issues/4685:1647,Availability,ERROR,ERROR,1647,"] Non-parseable POM /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml, line 15, column 3; @ ; [ERROR] The build could not read 1 project -> [Help 1]; [ERROR] ; [ERROR] The project org.broadinstitute.gatk:gatk-aggregator:[unknown-version] (/scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/pom.xml) has 1 error; [ERROR] Non-parseable POM /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml, line 15, column 3 -> [Help 2]; [ERROR] ; [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.; [ERROR] Re-run Maven using the -X switch to enable full debug logging.; [ERROR] ; [ERROR] For more information about the errors and possible solutions, please read the following articles:; [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/ProjectBuildingException; [ERROR] [Help 2] http://cwiki.apache.org/confluence/display/MAVEN/ModelParseException; ```. ```; 1 <?xml version=""1.0"" encoding=""UTF-8""?>; 2 <project xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd"">; 3 <modelVersion>4.0.0</modelVersion>; 4 ; 5 <!--; 6 This pom is parent for all gatk poms; 7 See also:; 8 http://maven.apache.org/pom.html#Inheritance_v; 9 http://maven.apache.org/guides/introduction/introduction-to-the-pom.html#Project_Inheritance_vs_Project_Aggregation; 10 http://stackoverflow.com/questions/1992213/maven-parent-pom-vs-modules-pom; 11 -->; 12 ; 13 <groupId>org.broadinstitute.gatk</groupId>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4685
https://github.com/broadinstitute/gatk/issues/4685:1719,Availability,ERROR,ERROR,1719,"unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml, line 15, column 3; @ ; [ERROR] The build could not read 1 project -> [Help 1]; [ERROR] ; [ERROR] The project org.broadinstitute.gatk:gatk-aggregator:[unknown-version] (/scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/pom.xml) has 1 error; [ERROR] Non-parseable POM /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml, line 15, column 3 -> [Help 2]; [ERROR] ; [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.; [ERROR] Re-run Maven using the -X switch to enable full debug logging.; [ERROR] ; [ERROR] For more information about the errors and possible solutions, please read the following articles:; [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/ProjectBuildingException; [ERROR] [Help 2] http://cwiki.apache.org/confluence/display/MAVEN/ModelParseException; ```. ```; 1 <?xml version=""1.0"" encoding=""UTF-8""?>; 2 <project xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd"">; 3 <modelVersion>4.0.0</modelVersion>; 4 ; 5 <!--; 6 This pom is parent for all gatk poms; 7 See also:; 8 http://maven.apache.org/pom.html#Inheritance_v; 9 http://maven.apache.org/guides/introduction/introduction-to-the-pom.html#Project_Inheritance_vs_Project_Aggregation; 10 http://stackoverflow.com/questions/1992213/maven-parent-pom-vs-modules-pom; 11 -->; 12 ; 13 <groupId>org.broadinstitute.gatk</groupId>; 14 <artifactId>gatk-root</artifactId>; 15 <<<<<<< HEAD; 16 <version>3.8-1</version>; 17 =======; 18 <version>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4685
https://github.com/broadinstitute/gatk/issues/4685:1729,Availability,ERROR,ERROR,1729,"unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml, line 15, column 3; @ ; [ERROR] The build could not read 1 project -> [Help 1]; [ERROR] ; [ERROR] The project org.broadinstitute.gatk:gatk-aggregator:[unknown-version] (/scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/pom.xml) has 1 error; [ERROR] Non-parseable POM /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml, line 15, column 3 -> [Help 2]; [ERROR] ; [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.; [ERROR] Re-run Maven using the -X switch to enable full debug logging.; [ERROR] ; [ERROR] For more information about the errors and possible solutions, please read the following articles:; [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/ProjectBuildingException; [ERROR] [Help 2] http://cwiki.apache.org/confluence/display/MAVEN/ModelParseException; ```. ```; 1 <?xml version=""1.0"" encoding=""UTF-8""?>; 2 <project xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd"">; 3 <modelVersion>4.0.0</modelVersion>; 4 ; 5 <!--; 6 This pom is parent for all gatk poms; 7 See also:; 8 http://maven.apache.org/pom.html#Inheritance_v; 9 http://maven.apache.org/guides/introduction/introduction-to-the-pom.html#Project_Inheritance_vs_Project_Aggregation; 10 http://stackoverflow.com/questions/1992213/maven-parent-pom-vs-modules-pom; 11 -->; 12 ; 13 <groupId>org.broadinstitute.gatk</groupId>; 14 <artifactId>gatk-root</artifactId>; 15 <<<<<<< HEAD; 16 <version>3.8-1</version>; 17 =======; 18 <version>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4685
https://github.com/broadinstitute/gatk/issues/4685:1767,Availability,error,errors,1767,"unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml, line 15, column 3; @ ; [ERROR] The build could not read 1 project -> [Help 1]; [ERROR] ; [ERROR] The project org.broadinstitute.gatk:gatk-aggregator:[unknown-version] (/scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/pom.xml) has 1 error; [ERROR] Non-parseable POM /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml, line 15, column 3 -> [Help 2]; [ERROR] ; [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.; [ERROR] Re-run Maven using the -X switch to enable full debug logging.; [ERROR] ; [ERROR] For more information about the errors and possible solutions, please read the following articles:; [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/ProjectBuildingException; [ERROR] [Help 2] http://cwiki.apache.org/confluence/display/MAVEN/ModelParseException; ```. ```; 1 <?xml version=""1.0"" encoding=""UTF-8""?>; 2 <project xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd"">; 3 <modelVersion>4.0.0</modelVersion>; 4 ; 5 <!--; 6 This pom is parent for all gatk poms; 7 See also:; 8 http://maven.apache.org/pom.html#Inheritance_v; 9 http://maven.apache.org/guides/introduction/introduction-to-the-pom.html#Project_Inheritance_vs_Project_Aggregation; 10 http://stackoverflow.com/questions/1992213/maven-parent-pom-vs-modules-pom; 11 -->; 12 ; 13 <groupId>org.broadinstitute.gatk</groupId>; 14 <artifactId>gatk-root</artifactId>; 15 <<<<<<< HEAD; 16 <version>3.8-1</version>; 17 =======; 18 <version>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4685
https://github.com/broadinstitute/gatk/issues/4685:1836,Availability,ERROR,ERROR,1836,"unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml, line 15, column 3; @ ; [ERROR] The build could not read 1 project -> [Help 1]; [ERROR] ; [ERROR] The project org.broadinstitute.gatk:gatk-aggregator:[unknown-version] (/scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/pom.xml) has 1 error; [ERROR] Non-parseable POM /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml, line 15, column 3 -> [Help 2]; [ERROR] ; [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.; [ERROR] Re-run Maven using the -X switch to enable full debug logging.; [ERROR] ; [ERROR] For more information about the errors and possible solutions, please read the following articles:; [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/ProjectBuildingException; [ERROR] [Help 2] http://cwiki.apache.org/confluence/display/MAVEN/ModelParseException; ```. ```; 1 <?xml version=""1.0"" encoding=""UTF-8""?>; 2 <project xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd"">; 3 <modelVersion>4.0.0</modelVersion>; 4 ; 5 <!--; 6 This pom is parent for all gatk poms; 7 See also:; 8 http://maven.apache.org/pom.html#Inheritance_v; 9 http://maven.apache.org/guides/introduction/introduction-to-the-pom.html#Project_Inheritance_vs_Project_Aggregation; 10 http://stackoverflow.com/questions/1992213/maven-parent-pom-vs-modules-pom; 11 -->; 12 ; 13 <groupId>org.broadinstitute.gatk</groupId>; 14 <artifactId>gatk-root</artifactId>; 15 <<<<<<< HEAD; 16 <version>3.8-1</version>; 17 =======; 18 <version>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4685
https://github.com/broadinstitute/gatk/issues/4685:1928,Availability,ERROR,ERROR,1928,"gy/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml, line 15, column 3; @ ; [ERROR] The build could not read 1 project -> [Help 1]; [ERROR] ; [ERROR] The project org.broadinstitute.gatk:gatk-aggregator:[unknown-version] (/scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/pom.xml) has 1 error; [ERROR] Non-parseable POM /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml, line 15, column 3 -> [Help 2]; [ERROR] ; [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.; [ERROR] Re-run Maven using the -X switch to enable full debug logging.; [ERROR] ; [ERROR] For more information about the errors and possible solutions, please read the following articles:; [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/ProjectBuildingException; [ERROR] [Help 2] http://cwiki.apache.org/confluence/display/MAVEN/ModelParseException; ```. ```; 1 <?xml version=""1.0"" encoding=""UTF-8""?>; 2 <project xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd"">; 3 <modelVersion>4.0.0</modelVersion>; 4 ; 5 <!--; 6 This pom is parent for all gatk poms; 7 See also:; 8 http://maven.apache.org/pom.html#Inheritance_v; 9 http://maven.apache.org/guides/introduction/introduction-to-the-pom.html#Project_Inheritance_vs_Project_Aggregation; 10 http://stackoverflow.com/questions/1992213/maven-parent-pom-vs-modules-pom; 11 -->; 12 ; 13 <groupId>org.broadinstitute.gatk</groupId>; 14 <artifactId>gatk-root</artifactId>; 15 <<<<<<< HEAD; 16 <version>3.8-1</version>; 17 =======; 18 <version>3.8-2-SNAPSHOT</version>; 19 >>>>>>> 0450e2531ee021e28bd7c5e92b5ba736d530d9af; 20 <packaging>pom</packaging>; 21 <name>GATK ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4685
https://github.com/broadinstitute/gatk/issues/4685:57,Deployability,release,release,57,"Hi,; first of all, I find it very awkward that after 3.8 release there is 3.8-1. Why the dash instead of a dot, as usual? It only complicates automated package downloads which in general work with numbers separated by dots. You just mix together two schemes. Is that really necessary?. Anyway, the pom.xml is broken:. ```; >>> Preparing source in /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1 ...; Equivalent maven command; mvn -Dmaven.repo.local=/scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/.m2/repository verify '-Ddisable.shadepackage'; [INFO] Scanning for projects...; [ERROR] [ERROR] Some problems were encountered while processing the POMs:; [FATAL] Non-parseable POM /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml, line 15, column 3; @ ; [ERROR] The build could not read 1 project -> [Help 1]; [ERROR] ; [ERROR] The project org.broadinstitute.gatk:gatk-aggregator:[unknown-version] (/scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/pom.xml) has 1 error; [ERROR] Non-parseable POM /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml, line 15, column 3 -> [Help 2]; [ERROR] ; [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.; [ERROR] Re-run Maven using the -X switch to enable full debug logging.; [ERROR] ; [ERROR] For more information about the errors and possible solutions, please read the following articles:; [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/ProjectBuildingException; [ERROR] [Help 2] http://cwiki.apache.org/confluence/display/MAVEN/ModelPar",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4685
https://github.com/broadinstitute/gatk/issues/4685:3022,Deployability,release,release,3022,"/gatk-3.8-1/pom.xml) has 1 error; [ERROR] Non-parseable POM /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml, line 15, column 3 -> [Help 2]; [ERROR] ; [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.; [ERROR] Re-run Maven using the -X switch to enable full debug logging.; [ERROR] ; [ERROR] For more information about the errors and possible solutions, please read the following articles:; [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/ProjectBuildingException; [ERROR] [Help 2] http://cwiki.apache.org/confluence/display/MAVEN/ModelParseException; ```. ```; 1 <?xml version=""1.0"" encoding=""UTF-8""?>; 2 <project xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd"">; 3 <modelVersion>4.0.0</modelVersion>; 4 ; 5 <!--; 6 This pom is parent for all gatk poms; 7 See also:; 8 http://maven.apache.org/pom.html#Inheritance_v; 9 http://maven.apache.org/guides/introduction/introduction-to-the-pom.html#Project_Inheritance_vs_Project_Aggregation; 10 http://stackoverflow.com/questions/1992213/maven-parent-pom-vs-modules-pom; 11 -->; 12 ; 13 <groupId>org.broadinstitute.gatk</groupId>; 14 <artifactId>gatk-root</artifactId>; 15 <<<<<<< HEAD; 16 <version>3.8-1</version>; 17 =======; 18 <version>3.8-2-SNAPSHOT</version>; 19 >>>>>>> 0450e2531ee021e28bd7c5e92b5ba736d530d9af; 20 <packaging>pom</packaging>; 21 <name>GATK Root</name>; 22 ; 23 <prerequisites>; 24 <maven>3.0.4</maven>; 25 </prerequisites>; ```. Please make a new release, preferably 3.8.2 which unpacks as usual into `./gatk-3.8.2/`. Thank you. While making a bugfix release? because it is the last version supporting `old` syntax.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4685
https://github.com/broadinstitute/gatk/issues/4685:3126,Deployability,release,release,3126,"/gatk-3.8-1/pom.xml) has 1 error; [ERROR] Non-parseable POM /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml, line 15, column 3 -> [Help 2]; [ERROR] ; [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.; [ERROR] Re-run Maven using the -X switch to enable full debug logging.; [ERROR] ; [ERROR] For more information about the errors and possible solutions, please read the following articles:; [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/ProjectBuildingException; [ERROR] [Help 2] http://cwiki.apache.org/confluence/display/MAVEN/ModelParseException; ```. ```; 1 <?xml version=""1.0"" encoding=""UTF-8""?>; 2 <project xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd"">; 3 <modelVersion>4.0.0</modelVersion>; 4 ; 5 <!--; 6 This pom is parent for all gatk poms; 7 See also:; 8 http://maven.apache.org/pom.html#Inheritance_v; 9 http://maven.apache.org/guides/introduction/introduction-to-the-pom.html#Project_Inheritance_vs_Project_Aggregation; 10 http://stackoverflow.com/questions/1992213/maven-parent-pom-vs-modules-pom; 11 -->; 12 ; 13 <groupId>org.broadinstitute.gatk</groupId>; 14 <artifactId>gatk-root</artifactId>; 15 <<<<<<< HEAD; 16 <version>3.8-1</version>; 17 =======; 18 <version>3.8-2-SNAPSHOT</version>; 19 >>>>>>> 0450e2531ee021e28bd7c5e92b5ba736d530d9af; 20 <packaging>pom</packaging>; 21 <name>GATK Root</name>; 22 ; 23 <prerequisites>; 24 <maven>3.0.4</maven>; 25 </prerequisites>; ```. Please make a new release, preferably 3.8.2 which unpacks as usual into `./gatk-3.8.2/`. Thank you. While making a bugfix release? because it is the last version supporting `old` syntax.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4685
https://github.com/broadinstitute/gatk/issues/4685:1708,Testability,log,logging,1708,"] Non-parseable POM /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml, line 15, column 3; @ ; [ERROR] The build could not read 1 project -> [Help 1]; [ERROR] ; [ERROR] The project org.broadinstitute.gatk:gatk-aggregator:[unknown-version] (/scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/pom.xml) has 1 error; [ERROR] Non-parseable POM /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml, line 15, column 3 -> [Help 2]; [ERROR] ; [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.; [ERROR] Re-run Maven using the -X switch to enable full debug logging.; [ERROR] ; [ERROR] For more information about the errors and possible solutions, please read the following articles:; [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/ProjectBuildingException; [ERROR] [Help 2] http://cwiki.apache.org/confluence/display/MAVEN/ModelParseException; ```. ```; 1 <?xml version=""1.0"" encoding=""UTF-8""?>; 2 <project xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd"">; 3 <modelVersion>4.0.0</modelVersion>; 4 ; 5 <!--; 6 This pom is parent for all gatk poms; 7 See also:; 8 http://maven.apache.org/pom.html#Inheritance_v; 9 http://maven.apache.org/guides/introduction/introduction-to-the-pom.html#Project_Inheritance_vs_Project_Aggregation; 10 http://stackoverflow.com/questions/1992213/maven-parent-pom-vs-modules-pom; 11 -->; 12 ; 13 <groupId>org.broadinstitute.gatk</groupId>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4685
https://github.com/broadinstitute/gatk/issues/4685:2450,Usability,guid,guides,2450,"/gatk-3.8-1/pom.xml) has 1 error; [ERROR] Non-parseable POM /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml, line 15, column 3 -> [Help 2]; [ERROR] ; [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.; [ERROR] Re-run Maven using the -X switch to enable full debug logging.; [ERROR] ; [ERROR] For more information about the errors and possible solutions, please read the following articles:; [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/ProjectBuildingException; [ERROR] [Help 2] http://cwiki.apache.org/confluence/display/MAVEN/ModelParseException; ```. ```; 1 <?xml version=""1.0"" encoding=""UTF-8""?>; 2 <project xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd"">; 3 <modelVersion>4.0.0</modelVersion>; 4 ; 5 <!--; 6 This pom is parent for all gatk poms; 7 See also:; 8 http://maven.apache.org/pom.html#Inheritance_v; 9 http://maven.apache.org/guides/introduction/introduction-to-the-pom.html#Project_Inheritance_vs_Project_Aggregation; 10 http://stackoverflow.com/questions/1992213/maven-parent-pom-vs-modules-pom; 11 -->; 12 ; 13 <groupId>org.broadinstitute.gatk</groupId>; 14 <artifactId>gatk-root</artifactId>; 15 <<<<<<< HEAD; 16 <version>3.8-1</version>; 17 =======; 18 <version>3.8-2-SNAPSHOT</version>; 19 >>>>>>> 0450e2531ee021e28bd7c5e92b5ba736d530d9af; 20 <packaging>pom</packaging>; 21 <name>GATK Root</name>; 22 ; 23 <prerequisites>; 24 <maven>3.0.4</maven>; 25 </prerequisites>; ```. Please make a new release, preferably 3.8.2 which unpacks as usual into `./gatk-3.8.2/`. Thank you. While making a bugfix release? because it is the last version supporting `old` syntax.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4685
https://github.com/broadinstitute/gatk/issues/4686:411,Availability,down,downstream,411,"Hi,; during compilation of 3.8 sources I get. ```; [INFO] --- exec-maven-plugin:1.2.1:exec (delete-mavens-links) @ gatk-aggregator ---; rm: missing operand; Try 'rm --help' for more information.; rm: missing operand; Try 'rm --help' for more information.; [INFO] ; [INFO] --- maven-failsafe-plugin:2.16:integration-test (integration-tests) @ gatk-aggregator ---; ```. I have no idea whether it breaks something downstream but provided building fails for me later with. ```; [INFO] Reactor Summary:; [INFO] ; [INFO] GATK Root .......................................... SUCCESS [ 16.744 s]; [INFO] GATK Aggregator .................................... SUCCESS [ 4.647 s]; [INFO] GATK GSALib ........................................ SUCCESS [ 6.040 s]; [INFO] GATK Utils ......................................... SUCCESS [ 39.733 s]; [INFO] GATK Engine ........................................ SUCCESS [ 7.557 s]; [INFO] GATK Tools Public .................................. SUCCESS [ 7.689 s]; [INFO] External Example ................................... FAILURE [ 0.051 s]; [INFO] GATK Queue ......................................... SKIPPED; [INFO] GATK Queue Extensions Generator .................... SKIPPED; [INFO] GATK Queue Extensions Public ....................... SKIPPED; [INFO] GATK Aggregator Public ............................. SKIPPED; [INFO] GATK Tools Protected ............................... SKIPPED; [INFO] GATK Package Distribution .......................... SKIPPED; [INFO] GATK Queue Extensions Distribution ................. SKIPPED; [INFO] GATK Queue Package Distribution .................... SKIPPED; [INFO] GATK Aggregator Protected .......................... SKIPPED; [INFO] GATK Tools Private ................................. SKIPPED; [INFO] GATK Package Internal .............................. SKIPPED; [INFO] NA12878 KB Utilities ............................... SKIPPED; [INFO] GATK Queue Private ................................. SKIPPED; [INFO] GATK Queue Extensions Inter",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4686
https://github.com/broadinstitute/gatk/issues/4686:1050,Availability,FAILURE,FAILURE,1050,O] --- exec-maven-plugin:1.2.1:exec (delete-mavens-links) @ gatk-aggregator ---; rm: missing operand; Try 'rm --help' for more information.; rm: missing operand; Try 'rm --help' for more information.; [INFO] ; [INFO] --- maven-failsafe-plugin:2.16:integration-test (integration-tests) @ gatk-aggregator ---; ```. I have no idea whether it breaks something downstream but provided building fails for me later with. ```; [INFO] Reactor Summary:; [INFO] ; [INFO] GATK Root .......................................... SUCCESS [ 16.744 s]; [INFO] GATK Aggregator .................................... SUCCESS [ 4.647 s]; [INFO] GATK GSALib ........................................ SUCCESS [ 6.040 s]; [INFO] GATK Utils ......................................... SUCCESS [ 39.733 s]; [INFO] GATK Engine ........................................ SUCCESS [ 7.557 s]; [INFO] GATK Tools Public .................................. SUCCESS [ 7.689 s]; [INFO] External Example ................................... FAILURE [ 0.051 s]; [INFO] GATK Queue ......................................... SKIPPED; [INFO] GATK Queue Extensions Generator .................... SKIPPED; [INFO] GATK Queue Extensions Public ....................... SKIPPED; [INFO] GATK Aggregator Public ............................. SKIPPED; [INFO] GATK Tools Protected ............................... SKIPPED; [INFO] GATK Package Distribution .......................... SKIPPED; [INFO] GATK Queue Extensions Distribution ................. SKIPPED; [INFO] GATK Queue Package Distribution .................... SKIPPED; [INFO] GATK Aggregator Protected .......................... SKIPPED; [INFO] GATK Tools Private ................................. SKIPPED; [INFO] GATK Package Internal .............................. SKIPPED; [INFO] NA12878 KB Utilities ............................... SKIPPED; [INFO] GATK Queue Private ................................. SKIPPED; [INFO] GATK Queue Extensions Internal ..................... SKIPPED; [INFO] GATK Queue Pa,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4686
https://github.com/broadinstitute/gatk/issues/4686:2268,Availability,FAILURE,FAILURE,2268,"PPED; [INFO] GATK Package Distribution .......................... SKIPPED; [INFO] GATK Queue Extensions Distribution ................. SKIPPED; [INFO] GATK Queue Package Distribution .................... SKIPPED; [INFO] GATK Aggregator Protected .......................... SKIPPED; [INFO] GATK Tools Private ................................. SKIPPED; [INFO] GATK Package Internal .............................. SKIPPED; [INFO] NA12878 KB Utilities ............................... SKIPPED; [INFO] GATK Queue Private ................................. SKIPPED; [INFO] GATK Queue Extensions Internal ..................... SKIPPED; [INFO] GATK Queue Package Internal ........................ SKIPPED; [INFO] GATK Aggregator Private ............................ SKIPPED; [INFO] ------------------------------------------------------------------------; [INFO] BUILD FAILURE; [INFO] ------------------------------------------------------------------------; [INFO] Total time: 01:23 min; [INFO] Finished at: 2018-04-20T20:52:19+02:00; [INFO] Final Memory: 67M/922M; [INFO] ------------------------------------------------------------------------; [ERROR] Failed to execute goal on project external-example: Could not resolve dependencies for project org.mycompany.app:external-example:jar:1.0-SNAPSHOT: The following artifacts could not be resolved: org.broadinstitute.gatk:gatk-tools-public:jar:3.8-SNAPSHOT, org.broadinstitute.gatk:gatk-utils:jar:tests:3.8-SNAPSHOT, org.broadinstitute.gatk:gatk-engine:jar:tests:3.8-SNAPSHOT: Could not find artifact org.broadinstitute.gatk:gatk-tools-public:jar:3.8-SNAPSHOT in gatk.public.repo.local (file:../../public/repo) -> [Help 1]; [ERROR] ; [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.; [ERROR] Re-run Maven using the -X switch to enable full debug logging.; [ERROR] ; [ERROR] For more information about the errors and possible solutions, please read the following articles:; [ERROR] [Help 1] http://cwiki.apache.org/confluen",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4686
https://github.com/broadinstitute/gatk/issues/4686:2548,Availability,ERROR,ERROR,2548,"PPED; [INFO] GATK Package Distribution .......................... SKIPPED; [INFO] GATK Queue Extensions Distribution ................. SKIPPED; [INFO] GATK Queue Package Distribution .................... SKIPPED; [INFO] GATK Aggregator Protected .......................... SKIPPED; [INFO] GATK Tools Private ................................. SKIPPED; [INFO] GATK Package Internal .............................. SKIPPED; [INFO] NA12878 KB Utilities ............................... SKIPPED; [INFO] GATK Queue Private ................................. SKIPPED; [INFO] GATK Queue Extensions Internal ..................... SKIPPED; [INFO] GATK Queue Package Internal ........................ SKIPPED; [INFO] GATK Aggregator Private ............................ SKIPPED; [INFO] ------------------------------------------------------------------------; [INFO] BUILD FAILURE; [INFO] ------------------------------------------------------------------------; [INFO] Total time: 01:23 min; [INFO] Finished at: 2018-04-20T20:52:19+02:00; [INFO] Final Memory: 67M/922M; [INFO] ------------------------------------------------------------------------; [ERROR] Failed to execute goal on project external-example: Could not resolve dependencies for project org.mycompany.app:external-example:jar:1.0-SNAPSHOT: The following artifacts could not be resolved: org.broadinstitute.gatk:gatk-tools-public:jar:3.8-SNAPSHOT, org.broadinstitute.gatk:gatk-utils:jar:tests:3.8-SNAPSHOT, org.broadinstitute.gatk:gatk-engine:jar:tests:3.8-SNAPSHOT: Could not find artifact org.broadinstitute.gatk:gatk-tools-public:jar:3.8-SNAPSHOT in gatk.public.repo.local (file:../../public/repo) -> [Help 1]; [ERROR] ; [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.; [ERROR] Re-run Maven using the -X switch to enable full debug logging.; [ERROR] ; [ERROR] For more information about the errors and possible solutions, please read the following articles:; [ERROR] [Help 1] http://cwiki.apache.org/confluen",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4686
https://github.com/broadinstitute/gatk/issues/4686:3077,Availability,ERROR,ERROR,3077,"D; [INFO] GATK Aggregator Protected .......................... SKIPPED; [INFO] GATK Tools Private ................................. SKIPPED; [INFO] GATK Package Internal .............................. SKIPPED; [INFO] NA12878 KB Utilities ............................... SKIPPED; [INFO] GATK Queue Private ................................. SKIPPED; [INFO] GATK Queue Extensions Internal ..................... SKIPPED; [INFO] GATK Queue Package Internal ........................ SKIPPED; [INFO] GATK Aggregator Private ............................ SKIPPED; [INFO] ------------------------------------------------------------------------; [INFO] BUILD FAILURE; [INFO] ------------------------------------------------------------------------; [INFO] Total time: 01:23 min; [INFO] Finished at: 2018-04-20T20:52:19+02:00; [INFO] Final Memory: 67M/922M; [INFO] ------------------------------------------------------------------------; [ERROR] Failed to execute goal on project external-example: Could not resolve dependencies for project org.mycompany.app:external-example:jar:1.0-SNAPSHOT: The following artifacts could not be resolved: org.broadinstitute.gatk:gatk-tools-public:jar:3.8-SNAPSHOT, org.broadinstitute.gatk:gatk-utils:jar:tests:3.8-SNAPSHOT, org.broadinstitute.gatk:gatk-engine:jar:tests:3.8-SNAPSHOT: Could not find artifact org.broadinstitute.gatk:gatk-tools-public:jar:3.8-SNAPSHOT in gatk.public.repo.local (file:../../public/repo) -> [Help 1]; [ERROR] ; [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.; [ERROR] Re-run Maven using the -X switch to enable full debug logging.; [ERROR] ; [ERROR] For more information about the errors and possible solutions, please read the following articles:; [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionException; [ERROR] ; [ERROR] After correcting the problems, you can resume the build with the command; [ERROR] mvn <goals> -rf :external-example; ```. it could be the cause.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4686
https://github.com/broadinstitute/gatk/issues/4686:3087,Availability,ERROR,ERROR,3087,"D; [INFO] GATK Aggregator Protected .......................... SKIPPED; [INFO] GATK Tools Private ................................. SKIPPED; [INFO] GATK Package Internal .............................. SKIPPED; [INFO] NA12878 KB Utilities ............................... SKIPPED; [INFO] GATK Queue Private ................................. SKIPPED; [INFO] GATK Queue Extensions Internal ..................... SKIPPED; [INFO] GATK Queue Package Internal ........................ SKIPPED; [INFO] GATK Aggregator Private ............................ SKIPPED; [INFO] ------------------------------------------------------------------------; [INFO] BUILD FAILURE; [INFO] ------------------------------------------------------------------------; [INFO] Total time: 01:23 min; [INFO] Finished at: 2018-04-20T20:52:19+02:00; [INFO] Final Memory: 67M/922M; [INFO] ------------------------------------------------------------------------; [ERROR] Failed to execute goal on project external-example: Could not resolve dependencies for project org.mycompany.app:external-example:jar:1.0-SNAPSHOT: The following artifacts could not be resolved: org.broadinstitute.gatk:gatk-tools-public:jar:3.8-SNAPSHOT, org.broadinstitute.gatk:gatk-utils:jar:tests:3.8-SNAPSHOT, org.broadinstitute.gatk:gatk-engine:jar:tests:3.8-SNAPSHOT: Could not find artifact org.broadinstitute.gatk:gatk-tools-public:jar:3.8-SNAPSHOT in gatk.public.repo.local (file:../../public/repo) -> [Help 1]; [ERROR] ; [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.; [ERROR] Re-run Maven using the -X switch to enable full debug logging.; [ERROR] ; [ERROR] For more information about the errors and possible solutions, please read the following articles:; [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionException; [ERROR] ; [ERROR] After correcting the problems, you can resume the build with the command; [ERROR] mvn <goals> -rf :external-example; ```. it could be the cause.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4686
https://github.com/broadinstitute/gatk/issues/4686:3129,Availability,error,errors,3129,"D; [INFO] GATK Aggregator Protected .......................... SKIPPED; [INFO] GATK Tools Private ................................. SKIPPED; [INFO] GATK Package Internal .............................. SKIPPED; [INFO] NA12878 KB Utilities ............................... SKIPPED; [INFO] GATK Queue Private ................................. SKIPPED; [INFO] GATK Queue Extensions Internal ..................... SKIPPED; [INFO] GATK Queue Package Internal ........................ SKIPPED; [INFO] GATK Aggregator Private ............................ SKIPPED; [INFO] ------------------------------------------------------------------------; [INFO] BUILD FAILURE; [INFO] ------------------------------------------------------------------------; [INFO] Total time: 01:23 min; [INFO] Finished at: 2018-04-20T20:52:19+02:00; [INFO] Final Memory: 67M/922M; [INFO] ------------------------------------------------------------------------; [ERROR] Failed to execute goal on project external-example: Could not resolve dependencies for project org.mycompany.app:external-example:jar:1.0-SNAPSHOT: The following artifacts could not be resolved: org.broadinstitute.gatk:gatk-tools-public:jar:3.8-SNAPSHOT, org.broadinstitute.gatk:gatk-utils:jar:tests:3.8-SNAPSHOT, org.broadinstitute.gatk:gatk-engine:jar:tests:3.8-SNAPSHOT: Could not find artifact org.broadinstitute.gatk:gatk-tools-public:jar:3.8-SNAPSHOT in gatk.public.repo.local (file:../../public/repo) -> [Help 1]; [ERROR] ; [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.; [ERROR] Re-run Maven using the -X switch to enable full debug logging.; [ERROR] ; [ERROR] For more information about the errors and possible solutions, please read the following articles:; [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionException; [ERROR] ; [ERROR] After correcting the problems, you can resume the build with the command; [ERROR] mvn <goals> -rf :external-example; ```. it could be the cause.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4686
https://github.com/broadinstitute/gatk/issues/4686:3172,Availability,ERROR,ERROR,3172,"D; [INFO] GATK Aggregator Protected .......................... SKIPPED; [INFO] GATK Tools Private ................................. SKIPPED; [INFO] GATK Package Internal .............................. SKIPPED; [INFO] NA12878 KB Utilities ............................... SKIPPED; [INFO] GATK Queue Private ................................. SKIPPED; [INFO] GATK Queue Extensions Internal ..................... SKIPPED; [INFO] GATK Queue Package Internal ........................ SKIPPED; [INFO] GATK Aggregator Private ............................ SKIPPED; [INFO] ------------------------------------------------------------------------; [INFO] BUILD FAILURE; [INFO] ------------------------------------------------------------------------; [INFO] Total time: 01:23 min; [INFO] Finished at: 2018-04-20T20:52:19+02:00; [INFO] Final Memory: 67M/922M; [INFO] ------------------------------------------------------------------------; [ERROR] Failed to execute goal on project external-example: Could not resolve dependencies for project org.mycompany.app:external-example:jar:1.0-SNAPSHOT: The following artifacts could not be resolved: org.broadinstitute.gatk:gatk-tools-public:jar:3.8-SNAPSHOT, org.broadinstitute.gatk:gatk-utils:jar:tests:3.8-SNAPSHOT, org.broadinstitute.gatk:gatk-engine:jar:tests:3.8-SNAPSHOT: Could not find artifact org.broadinstitute.gatk:gatk-tools-public:jar:3.8-SNAPSHOT in gatk.public.repo.local (file:../../public/repo) -> [Help 1]; [ERROR] ; [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.; [ERROR] Re-run Maven using the -X switch to enable full debug logging.; [ERROR] ; [ERROR] For more information about the errors and possible solutions, please read the following articles:; [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionException; [ERROR] ; [ERROR] After correcting the problems, you can resume the build with the command; [ERROR] mvn <goals> -rf :external-example; ```. it could be the cause.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4686
https://github.com/broadinstitute/gatk/issues/4686:3244,Availability,ERROR,ERROR,3244,"D; [INFO] GATK Aggregator Protected .......................... SKIPPED; [INFO] GATK Tools Private ................................. SKIPPED; [INFO] GATK Package Internal .............................. SKIPPED; [INFO] NA12878 KB Utilities ............................... SKIPPED; [INFO] GATK Queue Private ................................. SKIPPED; [INFO] GATK Queue Extensions Internal ..................... SKIPPED; [INFO] GATK Queue Package Internal ........................ SKIPPED; [INFO] GATK Aggregator Private ............................ SKIPPED; [INFO] ------------------------------------------------------------------------; [INFO] BUILD FAILURE; [INFO] ------------------------------------------------------------------------; [INFO] Total time: 01:23 min; [INFO] Finished at: 2018-04-20T20:52:19+02:00; [INFO] Final Memory: 67M/922M; [INFO] ------------------------------------------------------------------------; [ERROR] Failed to execute goal on project external-example: Could not resolve dependencies for project org.mycompany.app:external-example:jar:1.0-SNAPSHOT: The following artifacts could not be resolved: org.broadinstitute.gatk:gatk-tools-public:jar:3.8-SNAPSHOT, org.broadinstitute.gatk:gatk-utils:jar:tests:3.8-SNAPSHOT, org.broadinstitute.gatk:gatk-engine:jar:tests:3.8-SNAPSHOT: Could not find artifact org.broadinstitute.gatk:gatk-tools-public:jar:3.8-SNAPSHOT in gatk.public.repo.local (file:../../public/repo) -> [Help 1]; [ERROR] ; [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.; [ERROR] Re-run Maven using the -X switch to enable full debug logging.; [ERROR] ; [ERROR] For more information about the errors and possible solutions, please read the following articles:; [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionException; [ERROR] ; [ERROR] After correcting the problems, you can resume the build with the command; [ERROR] mvn <goals> -rf :external-example; ```. it could be the cause.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4686
https://github.com/broadinstitute/gatk/issues/4686:3254,Availability,ERROR,ERROR,3254,"D; [INFO] GATK Aggregator Protected .......................... SKIPPED; [INFO] GATK Tools Private ................................. SKIPPED; [INFO] GATK Package Internal .............................. SKIPPED; [INFO] NA12878 KB Utilities ............................... SKIPPED; [INFO] GATK Queue Private ................................. SKIPPED; [INFO] GATK Queue Extensions Internal ..................... SKIPPED; [INFO] GATK Queue Package Internal ........................ SKIPPED; [INFO] GATK Aggregator Private ............................ SKIPPED; [INFO] ------------------------------------------------------------------------; [INFO] BUILD FAILURE; [INFO] ------------------------------------------------------------------------; [INFO] Total time: 01:23 min; [INFO] Finished at: 2018-04-20T20:52:19+02:00; [INFO] Final Memory: 67M/922M; [INFO] ------------------------------------------------------------------------; [ERROR] Failed to execute goal on project external-example: Could not resolve dependencies for project org.mycompany.app:external-example:jar:1.0-SNAPSHOT: The following artifacts could not be resolved: org.broadinstitute.gatk:gatk-tools-public:jar:3.8-SNAPSHOT, org.broadinstitute.gatk:gatk-utils:jar:tests:3.8-SNAPSHOT, org.broadinstitute.gatk:gatk-engine:jar:tests:3.8-SNAPSHOT: Could not find artifact org.broadinstitute.gatk:gatk-tools-public:jar:3.8-SNAPSHOT in gatk.public.repo.local (file:../../public/repo) -> [Help 1]; [ERROR] ; [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.; [ERROR] Re-run Maven using the -X switch to enable full debug logging.; [ERROR] ; [ERROR] For more information about the errors and possible solutions, please read the following articles:; [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionException; [ERROR] ; [ERROR] After correcting the problems, you can resume the build with the command; [ERROR] mvn <goals> -rf :external-example; ```. it could be the cause.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4686
https://github.com/broadinstitute/gatk/issues/4686:3292,Availability,error,errors,3292,"D; [INFO] GATK Aggregator Protected .......................... SKIPPED; [INFO] GATK Tools Private ................................. SKIPPED; [INFO] GATK Package Internal .............................. SKIPPED; [INFO] NA12878 KB Utilities ............................... SKIPPED; [INFO] GATK Queue Private ................................. SKIPPED; [INFO] GATK Queue Extensions Internal ..................... SKIPPED; [INFO] GATK Queue Package Internal ........................ SKIPPED; [INFO] GATK Aggregator Private ............................ SKIPPED; [INFO] ------------------------------------------------------------------------; [INFO] BUILD FAILURE; [INFO] ------------------------------------------------------------------------; [INFO] Total time: 01:23 min; [INFO] Finished at: 2018-04-20T20:52:19+02:00; [INFO] Final Memory: 67M/922M; [INFO] ------------------------------------------------------------------------; [ERROR] Failed to execute goal on project external-example: Could not resolve dependencies for project org.mycompany.app:external-example:jar:1.0-SNAPSHOT: The following artifacts could not be resolved: org.broadinstitute.gatk:gatk-tools-public:jar:3.8-SNAPSHOT, org.broadinstitute.gatk:gatk-utils:jar:tests:3.8-SNAPSHOT, org.broadinstitute.gatk:gatk-engine:jar:tests:3.8-SNAPSHOT: Could not find artifact org.broadinstitute.gatk:gatk-tools-public:jar:3.8-SNAPSHOT in gatk.public.repo.local (file:../../public/repo) -> [Help 1]; [ERROR] ; [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.; [ERROR] Re-run Maven using the -X switch to enable full debug logging.; [ERROR] ; [ERROR] For more information about the errors and possible solutions, please read the following articles:; [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionException; [ERROR] ; [ERROR] After correcting the problems, you can resume the build with the command; [ERROR] mvn <goals> -rf :external-example; ```. it could be the cause.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4686
https://github.com/broadinstitute/gatk/issues/4686:3361,Availability,ERROR,ERROR,3361,"D; [INFO] GATK Aggregator Protected .......................... SKIPPED; [INFO] GATK Tools Private ................................. SKIPPED; [INFO] GATK Package Internal .............................. SKIPPED; [INFO] NA12878 KB Utilities ............................... SKIPPED; [INFO] GATK Queue Private ................................. SKIPPED; [INFO] GATK Queue Extensions Internal ..................... SKIPPED; [INFO] GATK Queue Package Internal ........................ SKIPPED; [INFO] GATK Aggregator Private ............................ SKIPPED; [INFO] ------------------------------------------------------------------------; [INFO] BUILD FAILURE; [INFO] ------------------------------------------------------------------------; [INFO] Total time: 01:23 min; [INFO] Finished at: 2018-04-20T20:52:19+02:00; [INFO] Final Memory: 67M/922M; [INFO] ------------------------------------------------------------------------; [ERROR] Failed to execute goal on project external-example: Could not resolve dependencies for project org.mycompany.app:external-example:jar:1.0-SNAPSHOT: The following artifacts could not be resolved: org.broadinstitute.gatk:gatk-tools-public:jar:3.8-SNAPSHOT, org.broadinstitute.gatk:gatk-utils:jar:tests:3.8-SNAPSHOT, org.broadinstitute.gatk:gatk-engine:jar:tests:3.8-SNAPSHOT: Could not find artifact org.broadinstitute.gatk:gatk-tools-public:jar:3.8-SNAPSHOT in gatk.public.repo.local (file:../../public/repo) -> [Help 1]; [ERROR] ; [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.; [ERROR] Re-run Maven using the -X switch to enable full debug logging.; [ERROR] ; [ERROR] For more information about the errors and possible solutions, please read the following articles:; [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionException; [ERROR] ; [ERROR] After correcting the problems, you can resume the build with the command; [ERROR] mvn <goals> -rf :external-example; ```. it could be the cause.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4686
https://github.com/broadinstitute/gatk/issues/4686:3458,Availability,ERROR,ERROR,3458,"D; [INFO] GATK Aggregator Protected .......................... SKIPPED; [INFO] GATK Tools Private ................................. SKIPPED; [INFO] GATK Package Internal .............................. SKIPPED; [INFO] NA12878 KB Utilities ............................... SKIPPED; [INFO] GATK Queue Private ................................. SKIPPED; [INFO] GATK Queue Extensions Internal ..................... SKIPPED; [INFO] GATK Queue Package Internal ........................ SKIPPED; [INFO] GATK Aggregator Private ............................ SKIPPED; [INFO] ------------------------------------------------------------------------; [INFO] BUILD FAILURE; [INFO] ------------------------------------------------------------------------; [INFO] Total time: 01:23 min; [INFO] Finished at: 2018-04-20T20:52:19+02:00; [INFO] Final Memory: 67M/922M; [INFO] ------------------------------------------------------------------------; [ERROR] Failed to execute goal on project external-example: Could not resolve dependencies for project org.mycompany.app:external-example:jar:1.0-SNAPSHOT: The following artifacts could not be resolved: org.broadinstitute.gatk:gatk-tools-public:jar:3.8-SNAPSHOT, org.broadinstitute.gatk:gatk-utils:jar:tests:3.8-SNAPSHOT, org.broadinstitute.gatk:gatk-engine:jar:tests:3.8-SNAPSHOT: Could not find artifact org.broadinstitute.gatk:gatk-tools-public:jar:3.8-SNAPSHOT in gatk.public.repo.local (file:../../public/repo) -> [Help 1]; [ERROR] ; [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.; [ERROR] Re-run Maven using the -X switch to enable full debug logging.; [ERROR] ; [ERROR] For more information about the errors and possible solutions, please read the following articles:; [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionException; [ERROR] ; [ERROR] After correcting the problems, you can resume the build with the command; [ERROR] mvn <goals> -rf :external-example; ```. it could be the cause.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4686
https://github.com/broadinstitute/gatk/issues/4686:3468,Availability,ERROR,ERROR,3468,"D; [INFO] GATK Aggregator Protected .......................... SKIPPED; [INFO] GATK Tools Private ................................. SKIPPED; [INFO] GATK Package Internal .............................. SKIPPED; [INFO] NA12878 KB Utilities ............................... SKIPPED; [INFO] GATK Queue Private ................................. SKIPPED; [INFO] GATK Queue Extensions Internal ..................... SKIPPED; [INFO] GATK Queue Package Internal ........................ SKIPPED; [INFO] GATK Aggregator Private ............................ SKIPPED; [INFO] ------------------------------------------------------------------------; [INFO] BUILD FAILURE; [INFO] ------------------------------------------------------------------------; [INFO] Total time: 01:23 min; [INFO] Finished at: 2018-04-20T20:52:19+02:00; [INFO] Final Memory: 67M/922M; [INFO] ------------------------------------------------------------------------; [ERROR] Failed to execute goal on project external-example: Could not resolve dependencies for project org.mycompany.app:external-example:jar:1.0-SNAPSHOT: The following artifacts could not be resolved: org.broadinstitute.gatk:gatk-tools-public:jar:3.8-SNAPSHOT, org.broadinstitute.gatk:gatk-utils:jar:tests:3.8-SNAPSHOT, org.broadinstitute.gatk:gatk-engine:jar:tests:3.8-SNAPSHOT: Could not find artifact org.broadinstitute.gatk:gatk-tools-public:jar:3.8-SNAPSHOT in gatk.public.repo.local (file:../../public/repo) -> [Help 1]; [ERROR] ; [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.; [ERROR] Re-run Maven using the -X switch to enable full debug logging.; [ERROR] ; [ERROR] For more information about the errors and possible solutions, please read the following articles:; [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionException; [ERROR] ; [ERROR] After correcting the problems, you can resume the build with the command; [ERROR] mvn <goals> -rf :external-example; ```. it could be the cause.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4686
https://github.com/broadinstitute/gatk/issues/4686:3550,Availability,ERROR,ERROR,3550,"D; [INFO] GATK Aggregator Protected .......................... SKIPPED; [INFO] GATK Tools Private ................................. SKIPPED; [INFO] GATK Package Internal .............................. SKIPPED; [INFO] NA12878 KB Utilities ............................... SKIPPED; [INFO] GATK Queue Private ................................. SKIPPED; [INFO] GATK Queue Extensions Internal ..................... SKIPPED; [INFO] GATK Queue Package Internal ........................ SKIPPED; [INFO] GATK Aggregator Private ............................ SKIPPED; [INFO] ------------------------------------------------------------------------; [INFO] BUILD FAILURE; [INFO] ------------------------------------------------------------------------; [INFO] Total time: 01:23 min; [INFO] Finished at: 2018-04-20T20:52:19+02:00; [INFO] Final Memory: 67M/922M; [INFO] ------------------------------------------------------------------------; [ERROR] Failed to execute goal on project external-example: Could not resolve dependencies for project org.mycompany.app:external-example:jar:1.0-SNAPSHOT: The following artifacts could not be resolved: org.broadinstitute.gatk:gatk-tools-public:jar:3.8-SNAPSHOT, org.broadinstitute.gatk:gatk-utils:jar:tests:3.8-SNAPSHOT, org.broadinstitute.gatk:gatk-engine:jar:tests:3.8-SNAPSHOT: Could not find artifact org.broadinstitute.gatk:gatk-tools-public:jar:3.8-SNAPSHOT in gatk.public.repo.local (file:../../public/repo) -> [Help 1]; [ERROR] ; [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.; [ERROR] Re-run Maven using the -X switch to enable full debug logging.; [ERROR] ; [ERROR] For more information about the errors and possible solutions, please read the following articles:; [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionException; [ERROR] ; [ERROR] After correcting the problems, you can resume the build with the command; [ERROR] mvn <goals> -rf :external-example; ```. it could be the cause.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4686
https://github.com/broadinstitute/gatk/issues/4686:303,Deployability,integrat,integration-test,303,"Hi,; during compilation of 3.8 sources I get. ```; [INFO] --- exec-maven-plugin:1.2.1:exec (delete-mavens-links) @ gatk-aggregator ---; rm: missing operand; Try 'rm --help' for more information.; rm: missing operand; Try 'rm --help' for more information.; [INFO] ; [INFO] --- maven-failsafe-plugin:2.16:integration-test (integration-tests) @ gatk-aggregator ---; ```. I have no idea whether it breaks something downstream but provided building fails for me later with. ```; [INFO] Reactor Summary:; [INFO] ; [INFO] GATK Root .......................................... SUCCESS [ 16.744 s]; [INFO] GATK Aggregator .................................... SUCCESS [ 4.647 s]; [INFO] GATK GSALib ........................................ SUCCESS [ 6.040 s]; [INFO] GATK Utils ......................................... SUCCESS [ 39.733 s]; [INFO] GATK Engine ........................................ SUCCESS [ 7.557 s]; [INFO] GATK Tools Public .................................. SUCCESS [ 7.689 s]; [INFO] External Example ................................... FAILURE [ 0.051 s]; [INFO] GATK Queue ......................................... SKIPPED; [INFO] GATK Queue Extensions Generator .................... SKIPPED; [INFO] GATK Queue Extensions Public ....................... SKIPPED; [INFO] GATK Aggregator Public ............................. SKIPPED; [INFO] GATK Tools Protected ............................... SKIPPED; [INFO] GATK Package Distribution .......................... SKIPPED; [INFO] GATK Queue Extensions Distribution ................. SKIPPED; [INFO] GATK Queue Package Distribution .................... SKIPPED; [INFO] GATK Aggregator Protected .......................... SKIPPED; [INFO] GATK Tools Private ................................. SKIPPED; [INFO] GATK Package Internal .............................. SKIPPED; [INFO] NA12878 KB Utilities ............................... SKIPPED; [INFO] GATK Queue Private ................................. SKIPPED; [INFO] GATK Queue Extensions Inter",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4686
https://github.com/broadinstitute/gatk/issues/4686:321,Deployability,integrat,integration-tests,321,"Hi,; during compilation of 3.8 sources I get. ```; [INFO] --- exec-maven-plugin:1.2.1:exec (delete-mavens-links) @ gatk-aggregator ---; rm: missing operand; Try 'rm --help' for more information.; rm: missing operand; Try 'rm --help' for more information.; [INFO] ; [INFO] --- maven-failsafe-plugin:2.16:integration-test (integration-tests) @ gatk-aggregator ---; ```. I have no idea whether it breaks something downstream but provided building fails for me later with. ```; [INFO] Reactor Summary:; [INFO] ; [INFO] GATK Root .......................................... SUCCESS [ 16.744 s]; [INFO] GATK Aggregator .................................... SUCCESS [ 4.647 s]; [INFO] GATK GSALib ........................................ SUCCESS [ 6.040 s]; [INFO] GATK Utils ......................................... SUCCESS [ 39.733 s]; [INFO] GATK Engine ........................................ SUCCESS [ 7.557 s]; [INFO] GATK Tools Public .................................. SUCCESS [ 7.689 s]; [INFO] External Example ................................... FAILURE [ 0.051 s]; [INFO] GATK Queue ......................................... SKIPPED; [INFO] GATK Queue Extensions Generator .................... SKIPPED; [INFO] GATK Queue Extensions Public ....................... SKIPPED; [INFO] GATK Aggregator Public ............................. SKIPPED; [INFO] GATK Tools Protected ............................... SKIPPED; [INFO] GATK Package Distribution .......................... SKIPPED; [INFO] GATK Queue Extensions Distribution ................. SKIPPED; [INFO] GATK Queue Package Distribution .................... SKIPPED; [INFO] GATK Aggregator Protected .......................... SKIPPED; [INFO] GATK Tools Private ................................. SKIPPED; [INFO] GATK Package Internal .............................. SKIPPED; [INFO] NA12878 KB Utilities ............................... SKIPPED; [INFO] GATK Queue Private ................................. SKIPPED; [INFO] GATK Queue Extensions Inter",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4686
https://github.com/broadinstitute/gatk/issues/4686:303,Integrability,integrat,integration-test,303,"Hi,; during compilation of 3.8 sources I get. ```; [INFO] --- exec-maven-plugin:1.2.1:exec (delete-mavens-links) @ gatk-aggregator ---; rm: missing operand; Try 'rm --help' for more information.; rm: missing operand; Try 'rm --help' for more information.; [INFO] ; [INFO] --- maven-failsafe-plugin:2.16:integration-test (integration-tests) @ gatk-aggregator ---; ```. I have no idea whether it breaks something downstream but provided building fails for me later with. ```; [INFO] Reactor Summary:; [INFO] ; [INFO] GATK Root .......................................... SUCCESS [ 16.744 s]; [INFO] GATK Aggregator .................................... SUCCESS [ 4.647 s]; [INFO] GATK GSALib ........................................ SUCCESS [ 6.040 s]; [INFO] GATK Utils ......................................... SUCCESS [ 39.733 s]; [INFO] GATK Engine ........................................ SUCCESS [ 7.557 s]; [INFO] GATK Tools Public .................................. SUCCESS [ 7.689 s]; [INFO] External Example ................................... FAILURE [ 0.051 s]; [INFO] GATK Queue ......................................... SKIPPED; [INFO] GATK Queue Extensions Generator .................... SKIPPED; [INFO] GATK Queue Extensions Public ....................... SKIPPED; [INFO] GATK Aggregator Public ............................. SKIPPED; [INFO] GATK Tools Protected ............................... SKIPPED; [INFO] GATK Package Distribution .......................... SKIPPED; [INFO] GATK Queue Extensions Distribution ................. SKIPPED; [INFO] GATK Queue Package Distribution .................... SKIPPED; [INFO] GATK Aggregator Protected .......................... SKIPPED; [INFO] GATK Tools Private ................................. SKIPPED; [INFO] GATK Package Internal .............................. SKIPPED; [INFO] NA12878 KB Utilities ............................... SKIPPED; [INFO] GATK Queue Private ................................. SKIPPED; [INFO] GATK Queue Extensions Inter",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4686
https://github.com/broadinstitute/gatk/issues/4686:321,Integrability,integrat,integration-tests,321,"Hi,; during compilation of 3.8 sources I get. ```; [INFO] --- exec-maven-plugin:1.2.1:exec (delete-mavens-links) @ gatk-aggregator ---; rm: missing operand; Try 'rm --help' for more information.; rm: missing operand; Try 'rm --help' for more information.; [INFO] ; [INFO] --- maven-failsafe-plugin:2.16:integration-test (integration-tests) @ gatk-aggregator ---; ```. I have no idea whether it breaks something downstream but provided building fails for me later with. ```; [INFO] Reactor Summary:; [INFO] ; [INFO] GATK Root .......................................... SUCCESS [ 16.744 s]; [INFO] GATK Aggregator .................................... SUCCESS [ 4.647 s]; [INFO] GATK GSALib ........................................ SUCCESS [ 6.040 s]; [INFO] GATK Utils ......................................... SUCCESS [ 39.733 s]; [INFO] GATK Engine ........................................ SUCCESS [ 7.557 s]; [INFO] GATK Tools Public .................................. SUCCESS [ 7.689 s]; [INFO] External Example ................................... FAILURE [ 0.051 s]; [INFO] GATK Queue ......................................... SKIPPED; [INFO] GATK Queue Extensions Generator .................... SKIPPED; [INFO] GATK Queue Extensions Public ....................... SKIPPED; [INFO] GATK Aggregator Public ............................. SKIPPED; [INFO] GATK Tools Protected ............................... SKIPPED; [INFO] GATK Package Distribution .......................... SKIPPED; [INFO] GATK Queue Extensions Distribution ................. SKIPPED; [INFO] GATK Queue Package Distribution .................... SKIPPED; [INFO] GATK Aggregator Protected .......................... SKIPPED; [INFO] GATK Tools Private ................................. SKIPPED; [INFO] GATK Package Internal .............................. SKIPPED; [INFO] NA12878 KB Utilities ............................... SKIPPED; [INFO] GATK Queue Private ................................. SKIPPED; [INFO] GATK Queue Extensions Inter",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4686
https://github.com/broadinstitute/gatk/issues/4686:2625,Integrability,depend,dependencies,2625,"PPED; [INFO] GATK Package Distribution .......................... SKIPPED; [INFO] GATK Queue Extensions Distribution ................. SKIPPED; [INFO] GATK Queue Package Distribution .................... SKIPPED; [INFO] GATK Aggregator Protected .......................... SKIPPED; [INFO] GATK Tools Private ................................. SKIPPED; [INFO] GATK Package Internal .............................. SKIPPED; [INFO] NA12878 KB Utilities ............................... SKIPPED; [INFO] GATK Queue Private ................................. SKIPPED; [INFO] GATK Queue Extensions Internal ..................... SKIPPED; [INFO] GATK Queue Package Internal ........................ SKIPPED; [INFO] GATK Aggregator Private ............................ SKIPPED; [INFO] ------------------------------------------------------------------------; [INFO] BUILD FAILURE; [INFO] ------------------------------------------------------------------------; [INFO] Total time: 01:23 min; [INFO] Finished at: 2018-04-20T20:52:19+02:00; [INFO] Final Memory: 67M/922M; [INFO] ------------------------------------------------------------------------; [ERROR] Failed to execute goal on project external-example: Could not resolve dependencies for project org.mycompany.app:external-example:jar:1.0-SNAPSHOT: The following artifacts could not be resolved: org.broadinstitute.gatk:gatk-tools-public:jar:3.8-SNAPSHOT, org.broadinstitute.gatk:gatk-utils:jar:tests:3.8-SNAPSHOT, org.broadinstitute.gatk:gatk-engine:jar:tests:3.8-SNAPSHOT: Could not find artifact org.broadinstitute.gatk:gatk-tools-public:jar:3.8-SNAPSHOT in gatk.public.repo.local (file:../../public/repo) -> [Help 1]; [ERROR] ; [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.; [ERROR] Re-run Maven using the -X switch to enable full debug logging.; [ERROR] ; [ERROR] For more information about the errors and possible solutions, please read the following articles:; [ERROR] [Help 1] http://cwiki.apache.org/confluen",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4686
https://github.com/broadinstitute/gatk/issues/4686:3426,Integrability,Depend,DependencyResolutionException,3426,"D; [INFO] GATK Aggregator Protected .......................... SKIPPED; [INFO] GATK Tools Private ................................. SKIPPED; [INFO] GATK Package Internal .............................. SKIPPED; [INFO] NA12878 KB Utilities ............................... SKIPPED; [INFO] GATK Queue Private ................................. SKIPPED; [INFO] GATK Queue Extensions Internal ..................... SKIPPED; [INFO] GATK Queue Package Internal ........................ SKIPPED; [INFO] GATK Aggregator Private ............................ SKIPPED; [INFO] ------------------------------------------------------------------------; [INFO] BUILD FAILURE; [INFO] ------------------------------------------------------------------------; [INFO] Total time: 01:23 min; [INFO] Finished at: 2018-04-20T20:52:19+02:00; [INFO] Final Memory: 67M/922M; [INFO] ------------------------------------------------------------------------; [ERROR] Failed to execute goal on project external-example: Could not resolve dependencies for project org.mycompany.app:external-example:jar:1.0-SNAPSHOT: The following artifacts could not be resolved: org.broadinstitute.gatk:gatk-tools-public:jar:3.8-SNAPSHOT, org.broadinstitute.gatk:gatk-utils:jar:tests:3.8-SNAPSHOT, org.broadinstitute.gatk:gatk-engine:jar:tests:3.8-SNAPSHOT: Could not find artifact org.broadinstitute.gatk:gatk-tools-public:jar:3.8-SNAPSHOT in gatk.public.repo.local (file:../../public/repo) -> [Help 1]; [ERROR] ; [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.; [ERROR] Re-run Maven using the -X switch to enable full debug logging.; [ERROR] ; [ERROR] For more information about the errors and possible solutions, please read the following articles:; [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionException; [ERROR] ; [ERROR] After correcting the problems, you can resume the build with the command; [ERROR] mvn <goals> -rf :external-example; ```. it could be the cause.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4686
https://github.com/broadinstitute/gatk/issues/4686:73,Modifiability,plugin,plugin,73,"Hi,; during compilation of 3.8 sources I get. ```; [INFO] --- exec-maven-plugin:1.2.1:exec (delete-mavens-links) @ gatk-aggregator ---; rm: missing operand; Try 'rm --help' for more information.; rm: missing operand; Try 'rm --help' for more information.; [INFO] ; [INFO] --- maven-failsafe-plugin:2.16:integration-test (integration-tests) @ gatk-aggregator ---; ```. I have no idea whether it breaks something downstream but provided building fails for me later with. ```; [INFO] Reactor Summary:; [INFO] ; [INFO] GATK Root .......................................... SUCCESS [ 16.744 s]; [INFO] GATK Aggregator .................................... SUCCESS [ 4.647 s]; [INFO] GATK GSALib ........................................ SUCCESS [ 6.040 s]; [INFO] GATK Utils ......................................... SUCCESS [ 39.733 s]; [INFO] GATK Engine ........................................ SUCCESS [ 7.557 s]; [INFO] GATK Tools Public .................................. SUCCESS [ 7.689 s]; [INFO] External Example ................................... FAILURE [ 0.051 s]; [INFO] GATK Queue ......................................... SKIPPED; [INFO] GATK Queue Extensions Generator .................... SKIPPED; [INFO] GATK Queue Extensions Public ....................... SKIPPED; [INFO] GATK Aggregator Public ............................. SKIPPED; [INFO] GATK Tools Protected ............................... SKIPPED; [INFO] GATK Package Distribution .......................... SKIPPED; [INFO] GATK Queue Extensions Distribution ................. SKIPPED; [INFO] GATK Queue Package Distribution .................... SKIPPED; [INFO] GATK Aggregator Protected .......................... SKIPPED; [INFO] GATK Tools Private ................................. SKIPPED; [INFO] GATK Package Internal .............................. SKIPPED; [INFO] NA12878 KB Utilities ............................... SKIPPED; [INFO] GATK Queue Private ................................. SKIPPED; [INFO] GATK Queue Extensions Inter",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4686
https://github.com/broadinstitute/gatk/issues/4686:291,Modifiability,plugin,plugin,291,"Hi,; during compilation of 3.8 sources I get. ```; [INFO] --- exec-maven-plugin:1.2.1:exec (delete-mavens-links) @ gatk-aggregator ---; rm: missing operand; Try 'rm --help' for more information.; rm: missing operand; Try 'rm --help' for more information.; [INFO] ; [INFO] --- maven-failsafe-plugin:2.16:integration-test (integration-tests) @ gatk-aggregator ---; ```. I have no idea whether it breaks something downstream but provided building fails for me later with. ```; [INFO] Reactor Summary:; [INFO] ; [INFO] GATK Root .......................................... SUCCESS [ 16.744 s]; [INFO] GATK Aggregator .................................... SUCCESS [ 4.647 s]; [INFO] GATK GSALib ........................................ SUCCESS [ 6.040 s]; [INFO] GATK Utils ......................................... SUCCESS [ 39.733 s]; [INFO] GATK Engine ........................................ SUCCESS [ 7.557 s]; [INFO] GATK Tools Public .................................. SUCCESS [ 7.689 s]; [INFO] External Example ................................... FAILURE [ 0.051 s]; [INFO] GATK Queue ......................................... SKIPPED; [INFO] GATK Queue Extensions Generator .................... SKIPPED; [INFO] GATK Queue Extensions Public ....................... SKIPPED; [INFO] GATK Aggregator Public ............................. SKIPPED; [INFO] GATK Tools Protected ............................... SKIPPED; [INFO] GATK Package Distribution .......................... SKIPPED; [INFO] GATK Queue Extensions Distribution ................. SKIPPED; [INFO] GATK Queue Package Distribution .................... SKIPPED; [INFO] GATK Aggregator Protected .......................... SKIPPED; [INFO] GATK Tools Private ................................. SKIPPED; [INFO] GATK Package Internal .............................. SKIPPED; [INFO] NA12878 KB Utilities ............................... SKIPPED; [INFO] GATK Queue Private ................................. SKIPPED; [INFO] GATK Queue Extensions Inter",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4686
https://github.com/broadinstitute/gatk/issues/4686:1082,Performance,Queue,Queue,1082,ugin:1.2.1:exec (delete-mavens-links) @ gatk-aggregator ---; rm: missing operand; Try 'rm --help' for more information.; rm: missing operand; Try 'rm --help' for more information.; [INFO] ; [INFO] --- maven-failsafe-plugin:2.16:integration-test (integration-tests) @ gatk-aggregator ---; ```. I have no idea whether it breaks something downstream but provided building fails for me later with. ```; [INFO] Reactor Summary:; [INFO] ; [INFO] GATK Root .......................................... SUCCESS [ 16.744 s]; [INFO] GATK Aggregator .................................... SUCCESS [ 4.647 s]; [INFO] GATK GSALib ........................................ SUCCESS [ 6.040 s]; [INFO] GATK Utils ......................................... SUCCESS [ 39.733 s]; [INFO] GATK Engine ........................................ SUCCESS [ 7.557 s]; [INFO] GATK Tools Public .................................. SUCCESS [ 7.689 s]; [INFO] External Example ................................... FAILURE [ 0.051 s]; [INFO] GATK Queue ......................................... SKIPPED; [INFO] GATK Queue Extensions Generator .................... SKIPPED; [INFO] GATK Queue Extensions Public ....................... SKIPPED; [INFO] GATK Aggregator Public ............................. SKIPPED; [INFO] GATK Tools Protected ............................... SKIPPED; [INFO] GATK Package Distribution .......................... SKIPPED; [INFO] GATK Queue Extensions Distribution ................. SKIPPED; [INFO] GATK Queue Package Distribution .................... SKIPPED; [INFO] GATK Aggregator Protected .......................... SKIPPED; [INFO] GATK Tools Private ................................. SKIPPED; [INFO] GATK Package Internal .............................. SKIPPED; [INFO] NA12878 KB Utilities ............................... SKIPPED; [INFO] GATK Queue Private ................................. SKIPPED; [INFO] GATK Queue Extensions Internal ..................... SKIPPED; [INFO] GATK Queue Package Internal .....,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4686
https://github.com/broadinstitute/gatk/issues/4686:1151,Performance,Queue,Queue,1151,d; Try 'rm --help' for more information.; rm: missing operand; Try 'rm --help' for more information.; [INFO] ; [INFO] --- maven-failsafe-plugin:2.16:integration-test (integration-tests) @ gatk-aggregator ---; ```. I have no idea whether it breaks something downstream but provided building fails for me later with. ```; [INFO] Reactor Summary:; [INFO] ; [INFO] GATK Root .......................................... SUCCESS [ 16.744 s]; [INFO] GATK Aggregator .................................... SUCCESS [ 4.647 s]; [INFO] GATK GSALib ........................................ SUCCESS [ 6.040 s]; [INFO] GATK Utils ......................................... SUCCESS [ 39.733 s]; [INFO] GATK Engine ........................................ SUCCESS [ 7.557 s]; [INFO] GATK Tools Public .................................. SUCCESS [ 7.689 s]; [INFO] External Example ................................... FAILURE [ 0.051 s]; [INFO] GATK Queue ......................................... SKIPPED; [INFO] GATK Queue Extensions Generator .................... SKIPPED; [INFO] GATK Queue Extensions Public ....................... SKIPPED; [INFO] GATK Aggregator Public ............................. SKIPPED; [INFO] GATK Tools Protected ............................... SKIPPED; [INFO] GATK Package Distribution .......................... SKIPPED; [INFO] GATK Queue Extensions Distribution ................. SKIPPED; [INFO] GATK Queue Package Distribution .................... SKIPPED; [INFO] GATK Aggregator Protected .......................... SKIPPED; [INFO] GATK Tools Private ................................. SKIPPED; [INFO] GATK Package Internal .............................. SKIPPED; [INFO] NA12878 KB Utilities ............................... SKIPPED; [INFO] GATK Queue Private ................................. SKIPPED; [INFO] GATK Queue Extensions Internal ..................... SKIPPED; [INFO] GATK Queue Package Internal ........................ SKIPPED; [INFO] GATK Aggregator Private ..................,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4686
https://github.com/broadinstitute/gatk/issues/4686:1220,Performance,Queue,Queue,1220,'rm --help' for more information.; [INFO] ; [INFO] --- maven-failsafe-plugin:2.16:integration-test (integration-tests) @ gatk-aggregator ---; ```. I have no idea whether it breaks something downstream but provided building fails for me later with. ```; [INFO] Reactor Summary:; [INFO] ; [INFO] GATK Root .......................................... SUCCESS [ 16.744 s]; [INFO] GATK Aggregator .................................... SUCCESS [ 4.647 s]; [INFO] GATK GSALib ........................................ SUCCESS [ 6.040 s]; [INFO] GATK Utils ......................................... SUCCESS [ 39.733 s]; [INFO] GATK Engine ........................................ SUCCESS [ 7.557 s]; [INFO] GATK Tools Public .................................. SUCCESS [ 7.689 s]; [INFO] External Example ................................... FAILURE [ 0.051 s]; [INFO] GATK Queue ......................................... SKIPPED; [INFO] GATK Queue Extensions Generator .................... SKIPPED; [INFO] GATK Queue Extensions Public ....................... SKIPPED; [INFO] GATK Aggregator Public ............................. SKIPPED; [INFO] GATK Tools Protected ............................... SKIPPED; [INFO] GATK Package Distribution .......................... SKIPPED; [INFO] GATK Queue Extensions Distribution ................. SKIPPED; [INFO] GATK Queue Package Distribution .................... SKIPPED; [INFO] GATK Aggregator Protected .......................... SKIPPED; [INFO] GATK Tools Private ................................. SKIPPED; [INFO] GATK Package Internal .............................. SKIPPED; [INFO] NA12878 KB Utilities ............................... SKIPPED; [INFO] GATK Queue Private ................................. SKIPPED; [INFO] GATK Queue Extensions Internal ..................... SKIPPED; [INFO] GATK Queue Package Internal ........................ SKIPPED; [INFO] GATK Aggregator Private ............................ SKIPPED; [INFO] -----------------------------------------,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4686
https://github.com/broadinstitute/gatk/issues/4686:1496,Performance,Queue,Queue,1496,INFO] ; [INFO] GATK Root .......................................... SUCCESS [ 16.744 s]; [INFO] GATK Aggregator .................................... SUCCESS [ 4.647 s]; [INFO] GATK GSALib ........................................ SUCCESS [ 6.040 s]; [INFO] GATK Utils ......................................... SUCCESS [ 39.733 s]; [INFO] GATK Engine ........................................ SUCCESS [ 7.557 s]; [INFO] GATK Tools Public .................................. SUCCESS [ 7.689 s]; [INFO] External Example ................................... FAILURE [ 0.051 s]; [INFO] GATK Queue ......................................... SKIPPED; [INFO] GATK Queue Extensions Generator .................... SKIPPED; [INFO] GATK Queue Extensions Public ....................... SKIPPED; [INFO] GATK Aggregator Public ............................. SKIPPED; [INFO] GATK Tools Protected ............................... SKIPPED; [INFO] GATK Package Distribution .......................... SKIPPED; [INFO] GATK Queue Extensions Distribution ................. SKIPPED; [INFO] GATK Queue Package Distribution .................... SKIPPED; [INFO] GATK Aggregator Protected .......................... SKIPPED; [INFO] GATK Tools Private ................................. SKIPPED; [INFO] GATK Package Internal .............................. SKIPPED; [INFO] NA12878 KB Utilities ............................... SKIPPED; [INFO] GATK Queue Private ................................. SKIPPED; [INFO] GATK Queue Extensions Internal ..................... SKIPPED; [INFO] GATK Queue Package Internal ........................ SKIPPED; [INFO] GATK Aggregator Private ............................ SKIPPED; [INFO] ------------------------------------------------------------------------; [INFO] BUILD FAILURE; [INFO] ------------------------------------------------------------------------; [INFO] Total time: 01:23 min; [INFO] Finished at: 2018-04-20T20:52:19+02:00; [INFO] Final Memory: 67M/922M; [INFO] ----------------------------,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4686
https://github.com/broadinstitute/gatk/issues/4686:1565,Performance,Queue,Queue,1565,SUCCESS [ 16.744 s]; [INFO] GATK Aggregator .................................... SUCCESS [ 4.647 s]; [INFO] GATK GSALib ........................................ SUCCESS [ 6.040 s]; [INFO] GATK Utils ......................................... SUCCESS [ 39.733 s]; [INFO] GATK Engine ........................................ SUCCESS [ 7.557 s]; [INFO] GATK Tools Public .................................. SUCCESS [ 7.689 s]; [INFO] External Example ................................... FAILURE [ 0.051 s]; [INFO] GATK Queue ......................................... SKIPPED; [INFO] GATK Queue Extensions Generator .................... SKIPPED; [INFO] GATK Queue Extensions Public ....................... SKIPPED; [INFO] GATK Aggregator Public ............................. SKIPPED; [INFO] GATK Tools Protected ............................... SKIPPED; [INFO] GATK Package Distribution .......................... SKIPPED; [INFO] GATK Queue Extensions Distribution ................. SKIPPED; [INFO] GATK Queue Package Distribution .................... SKIPPED; [INFO] GATK Aggregator Protected .......................... SKIPPED; [INFO] GATK Tools Private ................................. SKIPPED; [INFO] GATK Package Internal .............................. SKIPPED; [INFO] NA12878 KB Utilities ............................... SKIPPED; [INFO] GATK Queue Private ................................. SKIPPED; [INFO] GATK Queue Extensions Internal ..................... SKIPPED; [INFO] GATK Queue Package Internal ........................ SKIPPED; [INFO] GATK Aggregator Private ............................ SKIPPED; [INFO] ------------------------------------------------------------------------; [INFO] BUILD FAILURE; [INFO] ------------------------------------------------------------------------; [INFO] Total time: 01:23 min; [INFO] Finished at: 2018-04-20T20:52:19+02:00; [INFO] Final Memory: 67M/922M; [INFO] ------------------------------------------------------------------------; [ERROR] Failed to exe,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4686
https://github.com/broadinstitute/gatk/issues/4686:1910,Performance,Queue,Queue,1910,"s]; [INFO] GATK Tools Public .................................. SUCCESS [ 7.689 s]; [INFO] External Example ................................... FAILURE [ 0.051 s]; [INFO] GATK Queue ......................................... SKIPPED; [INFO] GATK Queue Extensions Generator .................... SKIPPED; [INFO] GATK Queue Extensions Public ....................... SKIPPED; [INFO] GATK Aggregator Public ............................. SKIPPED; [INFO] GATK Tools Protected ............................... SKIPPED; [INFO] GATK Package Distribution .......................... SKIPPED; [INFO] GATK Queue Extensions Distribution ................. SKIPPED; [INFO] GATK Queue Package Distribution .................... SKIPPED; [INFO] GATK Aggregator Protected .......................... SKIPPED; [INFO] GATK Tools Private ................................. SKIPPED; [INFO] GATK Package Internal .............................. SKIPPED; [INFO] NA12878 KB Utilities ............................... SKIPPED; [INFO] GATK Queue Private ................................. SKIPPED; [INFO] GATK Queue Extensions Internal ..................... SKIPPED; [INFO] GATK Queue Package Internal ........................ SKIPPED; [INFO] GATK Aggregator Private ............................ SKIPPED; [INFO] ------------------------------------------------------------------------; [INFO] BUILD FAILURE; [INFO] ------------------------------------------------------------------------; [INFO] Total time: 01:23 min; [INFO] Finished at: 2018-04-20T20:52:19+02:00; [INFO] Final Memory: 67M/922M; [INFO] ------------------------------------------------------------------------; [ERROR] Failed to execute goal on project external-example: Could not resolve dependencies for project org.mycompany.app:external-example:jar:1.0-SNAPSHOT: The following artifacts could not be resolved: org.broadinstitute.gatk:gatk-tools-public:jar:3.8-SNAPSHOT, org.broadinstitute.gatk:gatk-utils:jar:tests:3.8-SNAPSHOT, org.broadinstitute.gatk:gatk-engine:ja",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4686
https://github.com/broadinstitute/gatk/issues/4686:1979,Performance,Queue,Queue,1979,".689 s]; [INFO] External Example ................................... FAILURE [ 0.051 s]; [INFO] GATK Queue ......................................... SKIPPED; [INFO] GATK Queue Extensions Generator .................... SKIPPED; [INFO] GATK Queue Extensions Public ....................... SKIPPED; [INFO] GATK Aggregator Public ............................. SKIPPED; [INFO] GATK Tools Protected ............................... SKIPPED; [INFO] GATK Package Distribution .......................... SKIPPED; [INFO] GATK Queue Extensions Distribution ................. SKIPPED; [INFO] GATK Queue Package Distribution .................... SKIPPED; [INFO] GATK Aggregator Protected .......................... SKIPPED; [INFO] GATK Tools Private ................................. SKIPPED; [INFO] GATK Package Internal .............................. SKIPPED; [INFO] NA12878 KB Utilities ............................... SKIPPED; [INFO] GATK Queue Private ................................. SKIPPED; [INFO] GATK Queue Extensions Internal ..................... SKIPPED; [INFO] GATK Queue Package Internal ........................ SKIPPED; [INFO] GATK Aggregator Private ............................ SKIPPED; [INFO] ------------------------------------------------------------------------; [INFO] BUILD FAILURE; [INFO] ------------------------------------------------------------------------; [INFO] Total time: 01:23 min; [INFO] Finished at: 2018-04-20T20:52:19+02:00; [INFO] Final Memory: 67M/922M; [INFO] ------------------------------------------------------------------------; [ERROR] Failed to execute goal on project external-example: Could not resolve dependencies for project org.mycompany.app:external-example:jar:1.0-SNAPSHOT: The following artifacts could not be resolved: org.broadinstitute.gatk:gatk-tools-public:jar:3.8-SNAPSHOT, org.broadinstitute.gatk:gatk-utils:jar:tests:3.8-SNAPSHOT, org.broadinstitute.gatk:gatk-engine:jar:tests:3.8-SNAPSHOT: Could not find artifact org.broadinstitute.gatk:gatk-",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4686
https://github.com/broadinstitute/gatk/issues/4686:2048,Performance,Queue,Queue,2048," FAILURE [ 0.051 s]; [INFO] GATK Queue ......................................... SKIPPED; [INFO] GATK Queue Extensions Generator .................... SKIPPED; [INFO] GATK Queue Extensions Public ....................... SKIPPED; [INFO] GATK Aggregator Public ............................. SKIPPED; [INFO] GATK Tools Protected ............................... SKIPPED; [INFO] GATK Package Distribution .......................... SKIPPED; [INFO] GATK Queue Extensions Distribution ................. SKIPPED; [INFO] GATK Queue Package Distribution .................... SKIPPED; [INFO] GATK Aggregator Protected .......................... SKIPPED; [INFO] GATK Tools Private ................................. SKIPPED; [INFO] GATK Package Internal .............................. SKIPPED; [INFO] NA12878 KB Utilities ............................... SKIPPED; [INFO] GATK Queue Private ................................. SKIPPED; [INFO] GATK Queue Extensions Internal ..................... SKIPPED; [INFO] GATK Queue Package Internal ........................ SKIPPED; [INFO] GATK Aggregator Private ............................ SKIPPED; [INFO] ------------------------------------------------------------------------; [INFO] BUILD FAILURE; [INFO] ------------------------------------------------------------------------; [INFO] Total time: 01:23 min; [INFO] Finished at: 2018-04-20T20:52:19+02:00; [INFO] Final Memory: 67M/922M; [INFO] ------------------------------------------------------------------------; [ERROR] Failed to execute goal on project external-example: Could not resolve dependencies for project org.mycompany.app:external-example:jar:1.0-SNAPSHOT: The following artifacts could not be resolved: org.broadinstitute.gatk:gatk-tools-public:jar:3.8-SNAPSHOT, org.broadinstitute.gatk:gatk-utils:jar:tests:3.8-SNAPSHOT, org.broadinstitute.gatk:gatk-engine:jar:tests:3.8-SNAPSHOT: Could not find artifact org.broadinstitute.gatk:gatk-tools-public:jar:3.8-SNAPSHOT in gatk.public.repo.local (file:../..",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4686
https://github.com/broadinstitute/gatk/issues/4686:315,Testability,test,test,315,"Hi,; during compilation of 3.8 sources I get. ```; [INFO] --- exec-maven-plugin:1.2.1:exec (delete-mavens-links) @ gatk-aggregator ---; rm: missing operand; Try 'rm --help' for more information.; rm: missing operand; Try 'rm --help' for more information.; [INFO] ; [INFO] --- maven-failsafe-plugin:2.16:integration-test (integration-tests) @ gatk-aggregator ---; ```. I have no idea whether it breaks something downstream but provided building fails for me later with. ```; [INFO] Reactor Summary:; [INFO] ; [INFO] GATK Root .......................................... SUCCESS [ 16.744 s]; [INFO] GATK Aggregator .................................... SUCCESS [ 4.647 s]; [INFO] GATK GSALib ........................................ SUCCESS [ 6.040 s]; [INFO] GATK Utils ......................................... SUCCESS [ 39.733 s]; [INFO] GATK Engine ........................................ SUCCESS [ 7.557 s]; [INFO] GATK Tools Public .................................. SUCCESS [ 7.689 s]; [INFO] External Example ................................... FAILURE [ 0.051 s]; [INFO] GATK Queue ......................................... SKIPPED; [INFO] GATK Queue Extensions Generator .................... SKIPPED; [INFO] GATK Queue Extensions Public ....................... SKIPPED; [INFO] GATK Aggregator Public ............................. SKIPPED; [INFO] GATK Tools Protected ............................... SKIPPED; [INFO] GATK Package Distribution .......................... SKIPPED; [INFO] GATK Queue Extensions Distribution ................. SKIPPED; [INFO] GATK Queue Package Distribution .................... SKIPPED; [INFO] GATK Aggregator Protected .......................... SKIPPED; [INFO] GATK Tools Private ................................. SKIPPED; [INFO] GATK Package Internal .............................. SKIPPED; [INFO] NA12878 KB Utilities ............................... SKIPPED; [INFO] GATK Queue Private ................................. SKIPPED; [INFO] GATK Queue Extensions Inter",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4686
https://github.com/broadinstitute/gatk/issues/4686:333,Testability,test,tests,333,"Hi,; during compilation of 3.8 sources I get. ```; [INFO] --- exec-maven-plugin:1.2.1:exec (delete-mavens-links) @ gatk-aggregator ---; rm: missing operand; Try 'rm --help' for more information.; rm: missing operand; Try 'rm --help' for more information.; [INFO] ; [INFO] --- maven-failsafe-plugin:2.16:integration-test (integration-tests) @ gatk-aggregator ---; ```. I have no idea whether it breaks something downstream but provided building fails for me later with. ```; [INFO] Reactor Summary:; [INFO] ; [INFO] GATK Root .......................................... SUCCESS [ 16.744 s]; [INFO] GATK Aggregator .................................... SUCCESS [ 4.647 s]; [INFO] GATK GSALib ........................................ SUCCESS [ 6.040 s]; [INFO] GATK Utils ......................................... SUCCESS [ 39.733 s]; [INFO] GATK Engine ........................................ SUCCESS [ 7.557 s]; [INFO] GATK Tools Public .................................. SUCCESS [ 7.689 s]; [INFO] External Example ................................... FAILURE [ 0.051 s]; [INFO] GATK Queue ......................................... SKIPPED; [INFO] GATK Queue Extensions Generator .................... SKIPPED; [INFO] GATK Queue Extensions Public ....................... SKIPPED; [INFO] GATK Aggregator Public ............................. SKIPPED; [INFO] GATK Tools Protected ............................... SKIPPED; [INFO] GATK Package Distribution .......................... SKIPPED; [INFO] GATK Queue Extensions Distribution ................. SKIPPED; [INFO] GATK Queue Package Distribution .................... SKIPPED; [INFO] GATK Aggregator Protected .......................... SKIPPED; [INFO] GATK Tools Private ................................. SKIPPED; [INFO] GATK Package Internal .............................. SKIPPED; [INFO] NA12878 KB Utilities ............................... SKIPPED; [INFO] GATK Queue Private ................................. SKIPPED; [INFO] GATK Queue Extensions Inter",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4686
https://github.com/broadinstitute/gatk/issues/4686:2849,Testability,test,tests,2849,"D; [INFO] GATK Aggregator Protected .......................... SKIPPED; [INFO] GATK Tools Private ................................. SKIPPED; [INFO] GATK Package Internal .............................. SKIPPED; [INFO] NA12878 KB Utilities ............................... SKIPPED; [INFO] GATK Queue Private ................................. SKIPPED; [INFO] GATK Queue Extensions Internal ..................... SKIPPED; [INFO] GATK Queue Package Internal ........................ SKIPPED; [INFO] GATK Aggregator Private ............................ SKIPPED; [INFO] ------------------------------------------------------------------------; [INFO] BUILD FAILURE; [INFO] ------------------------------------------------------------------------; [INFO] Total time: 01:23 min; [INFO] Finished at: 2018-04-20T20:52:19+02:00; [INFO] Final Memory: 67M/922M; [INFO] ------------------------------------------------------------------------; [ERROR] Failed to execute goal on project external-example: Could not resolve dependencies for project org.mycompany.app:external-example:jar:1.0-SNAPSHOT: The following artifacts could not be resolved: org.broadinstitute.gatk:gatk-tools-public:jar:3.8-SNAPSHOT, org.broadinstitute.gatk:gatk-utils:jar:tests:3.8-SNAPSHOT, org.broadinstitute.gatk:gatk-engine:jar:tests:3.8-SNAPSHOT: Could not find artifact org.broadinstitute.gatk:gatk-tools-public:jar:3.8-SNAPSHOT in gatk.public.repo.local (file:../../public/repo) -> [Help 1]; [ERROR] ; [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.; [ERROR] Re-run Maven using the -X switch to enable full debug logging.; [ERROR] ; [ERROR] For more information about the errors and possible solutions, please read the following articles:; [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionException; [ERROR] ; [ERROR] After correcting the problems, you can resume the build with the command; [ERROR] mvn <goals> -rf :external-example; ```. it could be the cause.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4686
https://github.com/broadinstitute/gatk/issues/4686:2909,Testability,test,tests,2909,"D; [INFO] GATK Aggregator Protected .......................... SKIPPED; [INFO] GATK Tools Private ................................. SKIPPED; [INFO] GATK Package Internal .............................. SKIPPED; [INFO] NA12878 KB Utilities ............................... SKIPPED; [INFO] GATK Queue Private ................................. SKIPPED; [INFO] GATK Queue Extensions Internal ..................... SKIPPED; [INFO] GATK Queue Package Internal ........................ SKIPPED; [INFO] GATK Aggregator Private ............................ SKIPPED; [INFO] ------------------------------------------------------------------------; [INFO] BUILD FAILURE; [INFO] ------------------------------------------------------------------------; [INFO] Total time: 01:23 min; [INFO] Finished at: 2018-04-20T20:52:19+02:00; [INFO] Final Memory: 67M/922M; [INFO] ------------------------------------------------------------------------; [ERROR] Failed to execute goal on project external-example: Could not resolve dependencies for project org.mycompany.app:external-example:jar:1.0-SNAPSHOT: The following artifacts could not be resolved: org.broadinstitute.gatk:gatk-tools-public:jar:3.8-SNAPSHOT, org.broadinstitute.gatk:gatk-utils:jar:tests:3.8-SNAPSHOT, org.broadinstitute.gatk:gatk-engine:jar:tests:3.8-SNAPSHOT: Could not find artifact org.broadinstitute.gatk:gatk-tools-public:jar:3.8-SNAPSHOT in gatk.public.repo.local (file:../../public/repo) -> [Help 1]; [ERROR] ; [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.; [ERROR] Re-run Maven using the -X switch to enable full debug logging.; [ERROR] ; [ERROR] For more information about the errors and possible solutions, please read the following articles:; [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionException; [ERROR] ; [ERROR] After correcting the problems, you can resume the build with the command; [ERROR] mvn <goals> -rf :external-example; ```. it could be the cause.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4686
https://github.com/broadinstitute/gatk/issues/4686:3233,Testability,log,logging,3233,"D; [INFO] GATK Aggregator Protected .......................... SKIPPED; [INFO] GATK Tools Private ................................. SKIPPED; [INFO] GATK Package Internal .............................. SKIPPED; [INFO] NA12878 KB Utilities ............................... SKIPPED; [INFO] GATK Queue Private ................................. SKIPPED; [INFO] GATK Queue Extensions Internal ..................... SKIPPED; [INFO] GATK Queue Package Internal ........................ SKIPPED; [INFO] GATK Aggregator Private ............................ SKIPPED; [INFO] ------------------------------------------------------------------------; [INFO] BUILD FAILURE; [INFO] ------------------------------------------------------------------------; [INFO] Total time: 01:23 min; [INFO] Finished at: 2018-04-20T20:52:19+02:00; [INFO] Final Memory: 67M/922M; [INFO] ------------------------------------------------------------------------; [ERROR] Failed to execute goal on project external-example: Could not resolve dependencies for project org.mycompany.app:external-example:jar:1.0-SNAPSHOT: The following artifacts could not be resolved: org.broadinstitute.gatk:gatk-tools-public:jar:3.8-SNAPSHOT, org.broadinstitute.gatk:gatk-utils:jar:tests:3.8-SNAPSHOT, org.broadinstitute.gatk:gatk-engine:jar:tests:3.8-SNAPSHOT: Could not find artifact org.broadinstitute.gatk:gatk-tools-public:jar:3.8-SNAPSHOT in gatk.public.repo.local (file:../../public/repo) -> [Help 1]; [ERROR] ; [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.; [ERROR] Re-run Maven using the -X switch to enable full debug logging.; [ERROR] ; [ERROR] For more information about the errors and possible solutions, please read the following articles:; [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionException; [ERROR] ; [ERROR] After correcting the problems, you can resume the build with the command; [ERROR] mvn <goals> -rf :external-example; ```. it could be the cause.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4686
https://github.com/broadinstitute/gatk/issues/4686:3514,Usability,resume,resume,3514,"D; [INFO] GATK Aggregator Protected .......................... SKIPPED; [INFO] GATK Tools Private ................................. SKIPPED; [INFO] GATK Package Internal .............................. SKIPPED; [INFO] NA12878 KB Utilities ............................... SKIPPED; [INFO] GATK Queue Private ................................. SKIPPED; [INFO] GATK Queue Extensions Internal ..................... SKIPPED; [INFO] GATK Queue Package Internal ........................ SKIPPED; [INFO] GATK Aggregator Private ............................ SKIPPED; [INFO] ------------------------------------------------------------------------; [INFO] BUILD FAILURE; [INFO] ------------------------------------------------------------------------; [INFO] Total time: 01:23 min; [INFO] Finished at: 2018-04-20T20:52:19+02:00; [INFO] Final Memory: 67M/922M; [INFO] ------------------------------------------------------------------------; [ERROR] Failed to execute goal on project external-example: Could not resolve dependencies for project org.mycompany.app:external-example:jar:1.0-SNAPSHOT: The following artifacts could not be resolved: org.broadinstitute.gatk:gatk-tools-public:jar:3.8-SNAPSHOT, org.broadinstitute.gatk:gatk-utils:jar:tests:3.8-SNAPSHOT, org.broadinstitute.gatk:gatk-engine:jar:tests:3.8-SNAPSHOT: Could not find artifact org.broadinstitute.gatk:gatk-tools-public:jar:3.8-SNAPSHOT in gatk.public.repo.local (file:../../public/repo) -> [Help 1]; [ERROR] ; [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.; [ERROR] Re-run Maven using the -X switch to enable full debug logging.; [ERROR] ; [ERROR] For more information about the errors and possible solutions, please read the following articles:; [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionException; [ERROR] ; [ERROR] After correcting the problems, you can resume the build with the command; [ERROR] mvn <goals> -rf :external-example; ```. it could be the cause.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4686
https://github.com/broadinstitute/gatk/issues/4687:70,Availability,error,error,70,"Hi,; I am trying to build from gatk-4 master sources. I received this error code `2` admitedly when I had no git-lfs installed. Now it is installed and in my PATH, but the error still occurs. Can't you capture the real error message?. ```; 22:05:55.883 [QUIET] [system.out] Executing: git lfs pull --include src/main/resources/large; 22:05:55.943 [DEBUG] [org.gradle.configuration.project.BuildScriptProcessor] Timing: Running the build script took 12.879 secs; 22:05:55.952 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.954 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] FAILURE: Build failed with an exception.; 22:05:55.955 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.956 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Where:; 22:05:55.956 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Build file '/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle' line: 102; 22:05:55.964 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.964 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * What went wrong:; 22:05:55.966 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] A problem occurred evaluating root project 'gatk'.; 22:05:55.966 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] > Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 2. git-lfs is required to build GATK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK.; 22:05:55.967 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.968 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Exception is:; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] org.gradle.api.GradleScriptException: A problem occurred evaluating root project 'gatk'.; 22:05:55.969 [ERROR] [o",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:172,Availability,error,error,172,"Hi,; I am trying to build from gatk-4 master sources. I received this error code `2` admitedly when I had no git-lfs installed. Now it is installed and in my PATH, but the error still occurs. Can't you capture the real error message?. ```; 22:05:55.883 [QUIET] [system.out] Executing: git lfs pull --include src/main/resources/large; 22:05:55.943 [DEBUG] [org.gradle.configuration.project.BuildScriptProcessor] Timing: Running the build script took 12.879 secs; 22:05:55.952 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.954 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] FAILURE: Build failed with an exception.; 22:05:55.955 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.956 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Where:; 22:05:55.956 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Build file '/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle' line: 102; 22:05:55.964 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.964 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * What went wrong:; 22:05:55.966 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] A problem occurred evaluating root project 'gatk'.; 22:05:55.966 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] > Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 2. git-lfs is required to build GATK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK.; 22:05:55.967 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.968 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Exception is:; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] org.gradle.api.GradleScriptException: A problem occurred evaluating root project 'gatk'.; 22:05:55.969 [ERROR] [o",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:219,Availability,error,error,219,"Hi,; I am trying to build from gatk-4 master sources. I received this error code `2` admitedly when I had no git-lfs installed. Now it is installed and in my PATH, but the error still occurs. Can't you capture the real error message?. ```; 22:05:55.883 [QUIET] [system.out] Executing: git lfs pull --include src/main/resources/large; 22:05:55.943 [DEBUG] [org.gradle.configuration.project.BuildScriptProcessor] Timing: Running the build script took 12.879 secs; 22:05:55.952 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.954 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] FAILURE: Build failed with an exception.; 22:05:55.955 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.956 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Where:; 22:05:55.956 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Build file '/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle' line: 102; 22:05:55.964 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.964 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * What went wrong:; 22:05:55.966 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] A problem occurred evaluating root project 'gatk'.; 22:05:55.966 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] > Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 2. git-lfs is required to build GATK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK.; 22:05:55.967 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.968 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Exception is:; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] org.gradle.api.GradleScriptException: A problem occurred evaluating root project 'gatk'.; 22:05:55.969 [ERROR] [o",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:476,Availability,ERROR,ERROR,476,"Hi,; I am trying to build from gatk-4 master sources. I received this error code `2` admitedly when I had no git-lfs installed. Now it is installed and in my PATH, but the error still occurs. Can't you capture the real error message?. ```; 22:05:55.883 [QUIET] [system.out] Executing: git lfs pull --include src/main/resources/large; 22:05:55.943 [DEBUG] [org.gradle.configuration.project.BuildScriptProcessor] Timing: Running the build script took 12.879 secs; 22:05:55.952 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.954 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] FAILURE: Build failed with an exception.; 22:05:55.955 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.956 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Where:; 22:05:55.956 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Build file '/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle' line: 102; 22:05:55.964 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.964 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * What went wrong:; 22:05:55.966 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] A problem occurred evaluating root project 'gatk'.; 22:05:55.966 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] > Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 2. git-lfs is required to build GATK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK.; 22:05:55.967 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.968 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Exception is:; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] org.gradle.api.GradleScriptException: A problem occurred evaluating root project 'gatk'.; 22:05:55.969 [ERROR] [o",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:556,Availability,ERROR,ERROR,556,"Hi,; I am trying to build from gatk-4 master sources. I received this error code `2` admitedly when I had no git-lfs installed. Now it is installed and in my PATH, but the error still occurs. Can't you capture the real error message?. ```; 22:05:55.883 [QUIET] [system.out] Executing: git lfs pull --include src/main/resources/large; 22:05:55.943 [DEBUG] [org.gradle.configuration.project.BuildScriptProcessor] Timing: Running the build script took 12.879 secs; 22:05:55.952 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.954 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] FAILURE: Build failed with an exception.; 22:05:55.955 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.956 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Where:; 22:05:55.956 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Build file '/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle' line: 102; 22:05:55.964 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.964 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * What went wrong:; 22:05:55.966 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] A problem occurred evaluating root project 'gatk'.; 22:05:55.966 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] > Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 2. git-lfs is required to build GATK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK.; 22:05:55.967 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.968 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Exception is:; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] org.gradle.api.GradleScriptException: A problem occurred evaluating root project 'gatk'.; 22:05:55.969 [ERROR] [o",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:620,Availability,FAILURE,FAILURE,620,"Hi,; I am trying to build from gatk-4 master sources. I received this error code `2` admitedly when I had no git-lfs installed. Now it is installed and in my PATH, but the error still occurs. Can't you capture the real error message?. ```; 22:05:55.883 [QUIET] [system.out] Executing: git lfs pull --include src/main/resources/large; 22:05:55.943 [DEBUG] [org.gradle.configuration.project.BuildScriptProcessor] Timing: Running the build script took 12.879 secs; 22:05:55.952 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.954 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] FAILURE: Build failed with an exception.; 22:05:55.955 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.956 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Where:; 22:05:55.956 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Build file '/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle' line: 102; 22:05:55.964 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.964 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * What went wrong:; 22:05:55.966 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] A problem occurred evaluating root project 'gatk'.; 22:05:55.966 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] > Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 2. git-lfs is required to build GATK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK.; 22:05:55.967 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.968 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Exception is:; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] org.gradle.api.GradleScriptException: A problem occurred evaluating root project 'gatk'.; 22:05:55.969 [ERROR] [o",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:676,Availability,ERROR,ERROR,676,"Hi,; I am trying to build from gatk-4 master sources. I received this error code `2` admitedly when I had no git-lfs installed. Now it is installed and in my PATH, but the error still occurs. Can't you capture the real error message?. ```; 22:05:55.883 [QUIET] [system.out] Executing: git lfs pull --include src/main/resources/large; 22:05:55.943 [DEBUG] [org.gradle.configuration.project.BuildScriptProcessor] Timing: Running the build script took 12.879 secs; 22:05:55.952 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.954 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] FAILURE: Build failed with an exception.; 22:05:55.955 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.956 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Where:; 22:05:55.956 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Build file '/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle' line: 102; 22:05:55.964 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.964 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * What went wrong:; 22:05:55.966 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] A problem occurred evaluating root project 'gatk'.; 22:05:55.966 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] > Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 2. git-lfs is required to build GATK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK.; 22:05:55.967 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.968 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Exception is:; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] org.gradle.api.GradleScriptException: A problem occurred evaluating root project 'gatk'.; 22:05:55.969 [ERROR] [o",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:756,Availability,ERROR,ERROR,756,"Hi,; I am trying to build from gatk-4 master sources. I received this error code `2` admitedly when I had no git-lfs installed. Now it is installed and in my PATH, but the error still occurs. Can't you capture the real error message?. ```; 22:05:55.883 [QUIET] [system.out] Executing: git lfs pull --include src/main/resources/large; 22:05:55.943 [DEBUG] [org.gradle.configuration.project.BuildScriptProcessor] Timing: Running the build script took 12.879 secs; 22:05:55.952 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.954 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] FAILURE: Build failed with an exception.; 22:05:55.955 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.956 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Where:; 22:05:55.956 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Build file '/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle' line: 102; 22:05:55.964 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.964 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * What went wrong:; 22:05:55.966 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] A problem occurred evaluating root project 'gatk'.; 22:05:55.966 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] > Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 2. git-lfs is required to build GATK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK.; 22:05:55.967 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.968 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Exception is:; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] org.gradle.api.GradleScriptException: A problem occurred evaluating root project 'gatk'.; 22:05:55.969 [ERROR] [o",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:844,Availability,ERROR,ERROR,844,"Hi,; I am trying to build from gatk-4 master sources. I received this error code `2` admitedly when I had no git-lfs installed. Now it is installed and in my PATH, but the error still occurs. Can't you capture the real error message?. ```; 22:05:55.883 [QUIET] [system.out] Executing: git lfs pull --include src/main/resources/large; 22:05:55.943 [DEBUG] [org.gradle.configuration.project.BuildScriptProcessor] Timing: Running the build script took 12.879 secs; 22:05:55.952 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.954 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] FAILURE: Build failed with an exception.; 22:05:55.955 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.956 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Where:; 22:05:55.956 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Build file '/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle' line: 102; 22:05:55.964 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.964 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * What went wrong:; 22:05:55.966 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] A problem occurred evaluating root project 'gatk'.; 22:05:55.966 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] > Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 2. git-lfs is required to build GATK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK.; 22:05:55.967 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.968 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Exception is:; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] org.gradle.api.GradleScriptException: A problem occurred evaluating root project 'gatk'.; 22:05:55.969 [ERROR] [o",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:1021,Availability,ERROR,ERROR,1021,"d from gatk-4 master sources. I received this error code `2` admitedly when I had no git-lfs installed. Now it is installed and in my PATH, but the error still occurs. Can't you capture the real error message?. ```; 22:05:55.883 [QUIET] [system.out] Executing: git lfs pull --include src/main/resources/large; 22:05:55.943 [DEBUG] [org.gradle.configuration.project.BuildScriptProcessor] Timing: Running the build script took 12.879 secs; 22:05:55.952 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.954 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] FAILURE: Build failed with an exception.; 22:05:55.955 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.956 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Where:; 22:05:55.956 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Build file '/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle' line: 102; 22:05:55.964 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.964 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * What went wrong:; 22:05:55.966 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] A problem occurred evaluating root project 'gatk'.; 22:05:55.966 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] > Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 2. git-lfs is required to build GATK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK.; 22:05:55.967 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.968 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Exception is:; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] org.gradle.api.GradleScriptException: A problem occurred evaluating root project 'gatk'.; 22:05:55.969 [ERROR] [org.gradle.internal.build",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:1101,Availability,ERROR,ERROR,1101,"d no git-lfs installed. Now it is installed and in my PATH, but the error still occurs. Can't you capture the real error message?. ```; 22:05:55.883 [QUIET] [system.out] Executing: git lfs pull --include src/main/resources/large; 22:05:55.943 [DEBUG] [org.gradle.configuration.project.BuildScriptProcessor] Timing: Running the build script took 12.879 secs; 22:05:55.952 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.954 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] FAILURE: Build failed with an exception.; 22:05:55.955 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.956 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Where:; 22:05:55.956 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Build file '/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle' line: 102; 22:05:55.964 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.964 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * What went wrong:; 22:05:55.966 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] A problem occurred evaluating root project 'gatk'.; 22:05:55.966 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] > Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 2. git-lfs is required to build GATK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK.; 22:05:55.967 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.968 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Exception is:; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] org.gradle.api.GradleScriptException: A problem occurred evaluating root project 'gatk'.; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.groovy.scripts.internal.DefaultScr",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:1199,Availability,ERROR,ERROR,1199,"capture the real error message?. ```; 22:05:55.883 [QUIET] [system.out] Executing: git lfs pull --include src/main/resources/large; 22:05:55.943 [DEBUG] [org.gradle.configuration.project.BuildScriptProcessor] Timing: Running the build script took 12.879 secs; 22:05:55.952 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.954 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] FAILURE: Build failed with an exception.; 22:05:55.955 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.956 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Where:; 22:05:55.956 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Build file '/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle' line: 102; 22:05:55.964 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.964 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * What went wrong:; 22:05:55.966 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] A problem occurred evaluating root project 'gatk'.; 22:05:55.966 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] > Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 2. git-lfs is required to build GATK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK.; 22:05:55.967 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.968 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Exception is:; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] org.gradle.api.GradleScriptException: A problem occurred evaluating root project 'gatk'.; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.groovy.scripts.internal.DefaultScriptRunnerFactory$ScriptRunnerImpl.run(DefaultScriptRunnerFactory.java:92); 22:05:55.969 [ERROR] [o",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:1329,Availability,ERROR,ERROR,1329,"; 22:05:55.943 [DEBUG] [org.gradle.configuration.project.BuildScriptProcessor] Timing: Running the build script took 12.879 secs; 22:05:55.952 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.954 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] FAILURE: Build failed with an exception.; 22:05:55.955 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.956 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Where:; 22:05:55.956 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Build file '/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle' line: 102; 22:05:55.964 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.964 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * What went wrong:; 22:05:55.966 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] A problem occurred evaluating root project 'gatk'.; 22:05:55.966 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] > Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 2. git-lfs is required to build GATK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK.; 22:05:55.967 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.968 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Exception is:; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] org.gradle.api.GradleScriptException: A problem occurred evaluating root project 'gatk'.; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.groovy.scripts.internal.DefaultScriptRunnerFactory$ScriptRunnerImpl.run(DefaultScriptRunnerFactory.java:92); 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl$2.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:1649,Availability,ERROR,ERROR,1649,"ception.; 22:05:55.955 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.956 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Where:; 22:05:55.956 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Build file '/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle' line: 102; 22:05:55.964 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.964 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * What went wrong:; 22:05:55.966 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] A problem occurred evaluating root project 'gatk'.; 22:05:55.966 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] > Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 2. git-lfs is required to build GATK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK.; 22:05:55.967 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.968 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Exception is:; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] org.gradle.api.GradleScriptException: A problem occurred evaluating root project 'gatk'.; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.groovy.scripts.internal.DefaultScriptRunnerFactory$ScriptRunnerImpl.run(DefaultScriptRunnerFactory.java:92); 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl$2.run(DefaultScriptPluginFactory.java:176); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.ProjectScriptTarget.addConfiguration(ProjectScriptTarget.java:77); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configurati",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:1729,Availability,ERROR,ERROR,1729,"porter] ; 22:05:55.956 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Where:; 22:05:55.956 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Build file '/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle' line: 102; 22:05:55.964 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.964 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * What went wrong:; 22:05:55.966 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] A problem occurred evaluating root project 'gatk'.; 22:05:55.966 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] > Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 2. git-lfs is required to build GATK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK.; 22:05:55.967 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.968 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Exception is:; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] org.gradle.api.GradleScriptException: A problem occurred evaluating root project 'gatk'.; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.groovy.scripts.internal.DefaultScriptRunnerFactory$ScriptRunnerImpl.run(DefaultScriptRunnerFactory.java:92); 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl$2.run(DefaultScriptPluginFactory.java:176); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.ProjectScriptTarget.addConfiguration(ProjectScriptTarget.java:77); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl.apply(DefaultScriptPluginFactory.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:1824,Availability,ERROR,ERROR,1824,":; 22:05:55.956 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Build file '/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle' line: 102; 22:05:55.964 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.964 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * What went wrong:; 22:05:55.966 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] A problem occurred evaluating root project 'gatk'.; 22:05:55.966 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] > Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 2. git-lfs is required to build GATK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK.; 22:05:55.967 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.968 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Exception is:; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] org.gradle.api.GradleScriptException: A problem occurred evaluating root project 'gatk'.; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.groovy.scripts.internal.DefaultScriptRunnerFactory$ScriptRunnerImpl.run(DefaultScriptRunnerFactory.java:92); 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl$2.run(DefaultScriptPluginFactory.java:176); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.ProjectScriptTarget.addConfiguration(ProjectScriptTarget.java:77); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl.apply(DefaultScriptPluginFactory.java:181); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at or",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:1992,Availability,ERROR,ERROR,1992," line: 102; 22:05:55.964 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.964 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * What went wrong:; 22:05:55.966 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] A problem occurred evaluating root project 'gatk'.; 22:05:55.966 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] > Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 2. git-lfs is required to build GATK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK.; 22:05:55.967 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.968 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Exception is:; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] org.gradle.api.GradleScriptException: A problem occurred evaluating root project 'gatk'.; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.groovy.scripts.internal.DefaultScriptRunnerFactory$ScriptRunnerImpl.run(DefaultScriptRunnerFactory.java:92); 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl$2.run(DefaultScriptPluginFactory.java:176); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.ProjectScriptTarget.addConfiguration(ProjectScriptTarget.java:77); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl.apply(DefaultScriptPluginFactory.java:181); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:38); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter]",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:2194,Availability,ERROR,ERROR,2194," [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] A problem occurred evaluating root project 'gatk'.; 22:05:55.966 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] > Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 2. git-lfs is required to build GATK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK.; 22:05:55.967 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.968 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Exception is:; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] org.gradle.api.GradleScriptException: A problem occurred evaluating root project 'gatk'.; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.groovy.scripts.internal.DefaultScriptRunnerFactory$ScriptRunnerImpl.run(DefaultScriptRunnerFactory.java:92); 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl$2.run(DefaultScriptPluginFactory.java:176); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.ProjectScriptTarget.addConfiguration(ProjectScriptTarget.java:77); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl.apply(DefaultScriptPluginFactory.java:181); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:38); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:25); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configurati",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:2389,Availability,ERROR,ERROR,2389," > Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 2. git-lfs is required to build GATK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK.; 22:05:55.967 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.968 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Exception is:; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] org.gradle.api.GradleScriptException: A problem occurred evaluating root project 'gatk'.; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.groovy.scripts.internal.DefaultScriptRunnerFactory$ScriptRunnerImpl.run(DefaultScriptRunnerFactory.java:92); 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl$2.run(DefaultScriptPluginFactory.java:176); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.ProjectScriptTarget.addConfiguration(ProjectScriptTarget.java:77); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl.apply(DefaultScriptPluginFactory.java:181); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:38); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:25); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.ConfigureActionsProjectEvaluator.evaluate(ConfigureActionsProjectEvaluator.java:34); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.config",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:2563,Availability,ERROR,ERROR,2563,/broadinstitute/gatk#building for information on how to build GATK.; 22:05:55.967 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.968 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Exception is:; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] org.gradle.api.GradleScriptException: A problem occurred evaluating root project 'gatk'.; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.groovy.scripts.internal.DefaultScriptRunnerFactory$ScriptRunnerImpl.run(DefaultScriptRunnerFactory.java:92); 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl$2.run(DefaultScriptPluginFactory.java:176); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.ProjectScriptTarget.addConfiguration(ProjectScriptTarget.java:77); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl.apply(DefaultScriptPluginFactory.java:181); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:38); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:25); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.ConfigureActionsProjectEvaluator.evaluate(ConfigureActionsProjectEvaluator.java:34); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.LifecycleProjectEvaluator.evaluate(LifecycleProjectEvaluator.java:55); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.g,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:2758,Availability,ERROR,ERROR,2758,devents.BuildExceptionReporter] * Exception is:; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] org.gradle.api.GradleScriptException: A problem occurred evaluating root project 'gatk'.; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.groovy.scripts.internal.DefaultScriptRunnerFactory$ScriptRunnerImpl.run(DefaultScriptRunnerFactory.java:92); 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl$2.run(DefaultScriptPluginFactory.java:176); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.ProjectScriptTarget.addConfiguration(ProjectScriptTarget.java:77); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl.apply(DefaultScriptPluginFactory.java:181); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:38); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:25); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.ConfigureActionsProjectEvaluator.evaluate(ConfigureActionsProjectEvaluator.java:34); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.LifecycleProjectEvaluator.evaluate(LifecycleProjectEvaluator.java:55); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:573); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.Defa,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:2933,Availability,ERROR,ERROR,2933,occurred evaluating root project 'gatk'.; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.groovy.scripts.internal.DefaultScriptRunnerFactory$ScriptRunnerImpl.run(DefaultScriptRunnerFactory.java:92); 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl$2.run(DefaultScriptPluginFactory.java:176); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.ProjectScriptTarget.addConfiguration(ProjectScriptTarget.java:77); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl.apply(DefaultScriptPluginFactory.java:181); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:38); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:25); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.ConfigureActionsProjectEvaluator.evaluate(ConfigureActionsProjectEvaluator.java:34); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.LifecycleProjectEvaluator.evaluate(LifecycleProjectEvaluator.java:55); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:573); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:125); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.TaskPathProjectEvaluator.c,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:3108,Availability,ERROR,ERROR,3108,nerFactory$ScriptRunnerImpl.run(DefaultScriptRunnerFactory.java:92); 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl$2.run(DefaultScriptPluginFactory.java:176); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.ProjectScriptTarget.addConfiguration(ProjectScriptTarget.java:77); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl.apply(DefaultScriptPluginFactory.java:181); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:38); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:25); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.ConfigureActionsProjectEvaluator.evaluate(ConfigureActionsProjectEvaluator.java:34); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.LifecycleProjectEvaluator.evaluate(LifecycleProjectEvaluator.java:55); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:573); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:125); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.TaskPathProjectEvaluator.configureHierarchy(TaskPathProjectEvaluator.java:42); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultBuildCon,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:3308,Availability,ERROR,ERROR,3308,ry$ScriptPluginImpl$2.run(DefaultScriptPluginFactory.java:176); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.ProjectScriptTarget.addConfiguration(ProjectScriptTarget.java:77); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl.apply(DefaultScriptPluginFactory.java:181); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:38); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:25); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.ConfigureActionsProjectEvaluator.evaluate(ConfigureActionsProjectEvaluator.java:34); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.LifecycleProjectEvaluator.evaluate(LifecycleProjectEvaluator.java:55); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:573); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:125); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.TaskPathProjectEvaluator.configureHierarchy(TaskPathProjectEvaluator.java:42); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultBuildConfigurer.configure(DefaultBuildConfigurer.java:38); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$2.run(DefaultGradle,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:3494,Availability,ERROR,ERROR,3494,rget.addConfiguration(ProjectScriptTarget.java:77); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl.apply(DefaultScriptPluginFactory.java:181); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:38); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:25); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.ConfigureActionsProjectEvaluator.evaluate(ConfigureActionsProjectEvaluator.java:34); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.LifecycleProjectEvaluator.evaluate(LifecycleProjectEvaluator.java:55); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:573); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:125); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.TaskPathProjectEvaluator.configureHierarchy(TaskPathProjectEvaluator.java:42); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultBuildConfigurer.configure(DefaultBuildConfigurer.java:38); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$2.run(DefaultGradleLauncher.java:151); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.Factories$1.create(Factories.java:22); 22:05:55.971 [ERROR] [org,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:3658,Availability,ERROR,ERROR,3658,ltScriptPluginFactory$ScriptPluginImpl.apply(DefaultScriptPluginFactory.java:181); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:38); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:25); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.ConfigureActionsProjectEvaluator.evaluate(ConfigureActionsProjectEvaluator.java:34); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.LifecycleProjectEvaluator.evaluate(LifecycleProjectEvaluator.java:55); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:573); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:125); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.TaskPathProjectEvaluator.configureHierarchy(TaskPathProjectEvaluator.java:42); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultBuildConfigurer.configure(DefaultBuildConfigurer.java:38); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$2.run(DefaultGradleLauncher.java:151); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.Factories$1.create(Factories.java:22); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:91); 22:0,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:3822,Availability,ERROR,ERROR,3822, org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:38); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:25); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.ConfigureActionsProjectEvaluator.evaluate(ConfigureActionsProjectEvaluator.java:34); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.LifecycleProjectEvaluator.evaluate(LifecycleProjectEvaluator.java:55); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:573); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:125); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.TaskPathProjectEvaluator.configureHierarchy(TaskPathProjectEvaluator.java:42); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultBuildConfigurer.configure(DefaultBuildConfigurer.java:38); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$2.run(DefaultGradleLauncher.java:151); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.Factories$1.create(Factories.java:22); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:91); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationEx,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:4004,Availability,ERROR,ERROR,4004,adle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:25); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.ConfigureActionsProjectEvaluator.evaluate(ConfigureActionsProjectEvaluator.java:34); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.LifecycleProjectEvaluator.evaluate(LifecycleProjectEvaluator.java:55); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:573); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:125); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.TaskPathProjectEvaluator.configureHierarchy(TaskPathProjectEvaluator.java:42); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultBuildConfigurer.configure(DefaultBuildConfigurer.java:38); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$2.run(DefaultGradleLauncher.java:151); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.Factories$1.create(Factories.java:22); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:91); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:53); 22:05:55.972 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuildStages(DefaultGradleLauncher,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:4177,Availability,ERROR,ERROR,4177,gradle.configuration.project.ConfigureActionsProjectEvaluator.evaluate(ConfigureActionsProjectEvaluator.java:34); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.LifecycleProjectEvaluator.evaluate(LifecycleProjectEvaluator.java:55); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:573); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:125); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.TaskPathProjectEvaluator.configureHierarchy(TaskPathProjectEvaluator.java:42); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultBuildConfigurer.configure(DefaultBuildConfigurer.java:38); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$2.run(DefaultGradleLauncher.java:151); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.Factories$1.create(Factories.java:22); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:91); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:53); 22:05:55.972 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuildStages(DefaultGradleLauncher.java:148); 22:05:55.972 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.access$200(DefaultGradleLaunche,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:4346,Availability,ERROR,ERROR,4346,uildExceptionReporter] 	at org.gradle.configuration.project.LifecycleProjectEvaluator.evaluate(LifecycleProjectEvaluator.java:55); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:573); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:125); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.TaskPathProjectEvaluator.configureHierarchy(TaskPathProjectEvaluator.java:42); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultBuildConfigurer.configure(DefaultBuildConfigurer.java:38); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$2.run(DefaultGradleLauncher.java:151); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.Factories$1.create(Factories.java:22); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:91); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:53); 22:05:55.972 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuildStages(DefaultGradleLauncher.java:148); 22:05:55.972 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.access$200(DefaultGradleLauncher.java:33); 22:05:55.973 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$1.create(DefaultGradleLaunc,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:4487,Availability,ERROR,ERROR,4487,70 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:573); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:125); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.TaskPathProjectEvaluator.configureHierarchy(TaskPathProjectEvaluator.java:42); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultBuildConfigurer.configure(DefaultBuildConfigurer.java:38); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$2.run(DefaultGradleLauncher.java:151); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.Factories$1.create(Factories.java:22); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:91); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:53); 22:05:55.972 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuildStages(DefaultGradleLauncher.java:148); 22:05:55.972 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.access$200(DefaultGradleLauncher.java:33); 22:05:55.973 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$1.create(DefaultGradleLauncher.java:112); 22:05:55.973 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLaunc,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:4672,Availability,ERROR,ERROR,4672,e.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:125); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.TaskPathProjectEvaluator.configureHierarchy(TaskPathProjectEvaluator.java:42); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultBuildConfigurer.configure(DefaultBuildConfigurer.java:38); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$2.run(DefaultGradleLauncher.java:151); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.Factories$1.create(Factories.java:22); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:91); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:53); 22:05:55.972 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuildStages(DefaultGradleLauncher.java:148); 22:05:55.972 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.access$200(DefaultGradleLauncher.java:33); 22:05:55.973 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$1.create(DefaultGradleLauncher.java:112); 22:05:55.973 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$1.create(DefaultGradleLauncher.java:106); 22:05:55.973 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecuto,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:4857,Availability,ERROR,ERROR,4857,s.BuildExceptionReporter] 	at org.gradle.execution.TaskPathProjectEvaluator.configureHierarchy(TaskPathProjectEvaluator.java:42); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultBuildConfigurer.configure(DefaultBuildConfigurer.java:38); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$2.run(DefaultGradleLauncher.java:151); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.Factories$1.create(Factories.java:22); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:91); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:53); 22:05:55.972 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuildStages(DefaultGradleLauncher.java:148); 22:05:55.972 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.access$200(DefaultGradleLauncher.java:33); 22:05:55.973 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$1.create(DefaultGradleLauncher.java:112); 22:05:55.973 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$1.create(DefaultGradleLauncher.java:106); 22:05:55.973 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:91); 22:05:55.976 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecuto,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:5034,Availability,ERROR,ERROR,5034,events.BuildExceptionReporter] 	at org.gradle.configuration.DefaultBuildConfigurer.configure(DefaultBuildConfigurer.java:38); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$2.run(DefaultGradleLauncher.java:151); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.Factories$1.create(Factories.java:22); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:91); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:53); 22:05:55.972 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuildStages(DefaultGradleLauncher.java:148); 22:05:55.972 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.access$200(DefaultGradleLauncher.java:33); 22:05:55.973 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$1.create(DefaultGradleLauncher.java:112); 22:05:55.973 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$1.create(DefaultGradleLauncher.java:106); 22:05:55.973 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:91); 22:05:55.976 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:63); 22:05:55.976 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.d,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:5207,Availability,ERROR,ERROR,5207,events.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$2.run(DefaultGradleLauncher.java:151); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.Factories$1.create(Factories.java:22); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:91); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:53); 22:05:55.972 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuildStages(DefaultGradleLauncher.java:148); 22:05:55.972 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.access$200(DefaultGradleLauncher.java:33); 22:05:55.973 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$1.create(DefaultGradleLauncher.java:112); 22:05:55.973 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$1.create(DefaultGradleLauncher.java:106); 22:05:55.973 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:91); 22:05:55.976 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:63); 22:05:55.976 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuild(DefaultGradleLauncher.java:106); 22:05:55.976 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.run,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:5379,Availability,ERROR,ERROR,5379,nts.BuildExceptionReporter] 	at org.gradle.internal.Factories$1.create(Factories.java:22); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:91); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:53); 22:05:55.972 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuildStages(DefaultGradleLauncher.java:148); 22:05:55.972 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.access$200(DefaultGradleLauncher.java:33); 22:05:55.973 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$1.create(DefaultGradleLauncher.java:112); 22:05:55.973 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$1.create(DefaultGradleLauncher.java:106); 22:05:55.973 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:91); 22:05:55.976 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:63); 22:05:55.976 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuild(DefaultGradleLauncher.java:106); 22:05:55.976 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.run(DefaultGradleLauncher.java:92); 22:05:55.976 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.GradleBuildController.run(Gradle,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:5551,Availability,ERROR,ERROR,5551, org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:91); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:53); 22:05:55.972 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuildStages(DefaultGradleLauncher.java:148); 22:05:55.972 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.access$200(DefaultGradleLauncher.java:33); 22:05:55.973 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$1.create(DefaultGradleLauncher.java:112); 22:05:55.973 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$1.create(DefaultGradleLauncher.java:106); 22:05:55.973 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:91); 22:05:55.976 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:63); 22:05:55.976 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuild(DefaultGradleLauncher.java:106); 22:05:55.976 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.run(DefaultGradleLauncher.java:92); 22:05:55.976 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.GradleBuildController.run(GradleBuildController.java:66); 22:05:55.977 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ExecuteBuildActionRunner.ru,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:5736,Availability,ERROR,ERROR,5736, org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:53); 22:05:55.972 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuildStages(DefaultGradleLauncher.java:148); 22:05:55.972 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.access$200(DefaultGradleLauncher.java:33); 22:05:55.973 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$1.create(DefaultGradleLauncher.java:112); 22:05:55.973 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$1.create(DefaultGradleLauncher.java:106); 22:05:55.973 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:91); 22:05:55.976 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:63); 22:05:55.976 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuild(DefaultGradleLauncher.java:106); 22:05:55.976 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.run(DefaultGradleLauncher.java:92); 22:05:55.976 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.GradleBuildController.run(GradleBuildController.java:66); 22:05:55.977 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ExecuteBuildActionRunner.run(ExecuteBuildActionRunner.java:28); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.ChainingBuildActionRunner.run(ChainingBui,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:5921,Availability,ERROR,ERROR,5921, org.gradle.initialization.DefaultGradleLauncher.doBuildStages(DefaultGradleLauncher.java:148); 22:05:55.972 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.access$200(DefaultGradleLauncher.java:33); 22:05:55.973 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$1.create(DefaultGradleLauncher.java:112); 22:05:55.973 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$1.create(DefaultGradleLauncher.java:106); 22:05:55.973 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:91); 22:05:55.976 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:63); 22:05:55.976 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuild(DefaultGradleLauncher.java:106); 22:05:55.976 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.run(DefaultGradleLauncher.java:92); 22:05:55.976 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.GradleBuildController.run(GradleBuildController.java:66); 22:05:55.977 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ExecuteBuildActionRunner.run(ExecuteBuildActionRunner.java:28); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.ChainingBuildActionRunner.run(ChainingBuildActionRunner.java:35); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildAc,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:6092,Availability,ERROR,ERROR,6092,r] 	at org.gradle.initialization.DefaultGradleLauncher.access$200(DefaultGradleLauncher.java:33); 22:05:55.973 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$1.create(DefaultGradleLauncher.java:112); 22:05:55.973 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$1.create(DefaultGradleLauncher.java:106); 22:05:55.973 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:91); 22:05:55.976 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:63); 22:05:55.976 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuild(DefaultGradleLauncher.java:106); 22:05:55.976 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.run(DefaultGradleLauncher.java:92); 22:05:55.976 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.GradleBuildController.run(GradleBuildController.java:66); 22:05:55.977 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ExecuteBuildActionRunner.run(ExecuteBuildActionRunner.java:28); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.ChainingBuildActionRunner.run(ChainingBuildActionRunner.java:35); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:41); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InPr,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:6258,Availability,ERROR,ERROR,6258,Reporter] 	at org.gradle.initialization.DefaultGradleLauncher$1.create(DefaultGradleLauncher.java:112); 22:05:55.973 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$1.create(DefaultGradleLauncher.java:106); 22:05:55.973 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:91); 22:05:55.976 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:63); 22:05:55.976 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuild(DefaultGradleLauncher.java:106); 22:05:55.976 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.run(DefaultGradleLauncher.java:92); 22:05:55.976 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.GradleBuildController.run(GradleBuildController.java:66); 22:05:55.977 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ExecuteBuildActionRunner.run(ExecuteBuildActionRunner.java:28); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.ChainingBuildActionRunner.run(ChainingBuildActionRunner.java:35); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:41); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:26); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBu,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:6423,Availability,ERROR,ERROR,6423,ceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$1.create(DefaultGradleLauncher.java:106); 22:05:55.973 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:91); 22:05:55.976 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:63); 22:05:55.976 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuild(DefaultGradleLauncher.java:106); 22:05:55.976 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.run(DefaultGradleLauncher.java:92); 22:05:55.976 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.GradleBuildController.run(GradleBuildController.java:66); 22:05:55.977 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ExecuteBuildActionRunner.run(ExecuteBuildActionRunner.java:28); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.ChainingBuildActionRunner.run(ChainingBuildActionRunner.java:35); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:41); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:26); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:79); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.toolin,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:6606,Availability,ERROR,ERROR,6606,rter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:91); 22:05:55.976 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:63); 22:05:55.976 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuild(DefaultGradleLauncher.java:106); 22:05:55.976 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.run(DefaultGradleLauncher.java:92); 22:05:55.976 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.GradleBuildController.run(GradleBuildController.java:66); 22:05:55.977 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ExecuteBuildActionRunner.run(ExecuteBuildActionRunner.java:28); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.ChainingBuildActionRunner.run(ChainingBuildActionRunner.java:35); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:41); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:26); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:79); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:51); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:6779,Availability,ERROR,ERROR,6779,xceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:63); 22:05:55.976 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuild(DefaultGradleLauncher.java:106); 22:05:55.976 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.run(DefaultGradleLauncher.java:92); 22:05:55.976 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.GradleBuildController.run(GradleBuildController.java:66); 22:05:55.977 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ExecuteBuildActionRunner.run(ExecuteBuildActionRunner.java:28); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.ChainingBuildActionRunner.run(ChainingBuildActionRunner.java:35); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:41); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:26); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:79); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:51); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ExecuteBuild.doBuild(ExecuteBuild.java:59); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:6962,Availability,ERROR,ERROR,6962,dExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuild(DefaultGradleLauncher.java:106); 22:05:55.976 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.run(DefaultGradleLauncher.java:92); 22:05:55.976 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.GradleBuildController.run(GradleBuildController.java:66); 22:05:55.977 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ExecuteBuildActionRunner.run(ExecuteBuildActionRunner.java:28); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.ChainingBuildActionRunner.run(ChainingBuildActionRunner.java:35); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:41); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:26); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:79); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:51); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ExecuteBuild.doBuild(ExecuteBuild.java:59); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.d,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:7145,Availability,ERROR,ERROR,7145,porter] 	at org.gradle.initialization.DefaultGradleLauncher.run(DefaultGradleLauncher.java:92); 22:05:55.976 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.GradleBuildController.run(GradleBuildController.java:66); 22:05:55.977 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ExecuteBuildActionRunner.run(ExecuteBuildActionRunner.java:28); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.ChainingBuildActionRunner.run(ChainingBuildActionRunner.java:35); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:41); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:26); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:79); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:51); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ExecuteBuild.doBuild(ExecuteBuild.java:59); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:7342,Availability,ERROR,ERROR,7342,.exec.GradleBuildController.run(GradleBuildController.java:66); 22:05:55.977 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ExecuteBuildActionRunner.run(ExecuteBuildActionRunner.java:28); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.ChainingBuildActionRunner.run(ChainingBuildActionRunner.java:35); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:41); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:26); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:79); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:51); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ExecuteBuild.doBuild(ExecuteBuild.java:59); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.WatchForDisconnection.execute(WatchForDisconnection.java:47); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:7539,Availability,ERROR,ERROR,7539,tionRunner.run(ExecuteBuildActionRunner.java:28); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.ChainingBuildActionRunner.run(ChainingBuildActionRunner.java:35); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:41); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:26); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:79); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:51); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ExecuteBuild.doBuild(ExecuteBuild.java:59); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.WatchForDisconnection.execute(WatchForDisconnection.java:47); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ResetD,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:7704,Availability,ERROR,ERROR,7704,BuildActionRunner.run(ChainingBuildActionRunner.java:35); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:41); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:26); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:79); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:51); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ExecuteBuild.doBuild(ExecuteBuild.java:59); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.WatchForDisconnection.execute(WatchForDisconnection.java:47); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ResetDeprecationLogger.execute(ResetDeprecationLogger.java:26); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemo,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:7877,Availability,ERROR,ERROR,7877,sBuildActionExecuter.execute(InProcessBuildActionExecuter.java:41); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:26); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:79); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:51); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ExecuteBuild.doBuild(ExecuteBuild.java:59); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.WatchForDisconnection.execute(WatchForDisconnection.java:47); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ResetDeprecationLogger.execute(ResetDeprecationLogger.java:26); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.la,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:8062,Availability,ERROR,ERROR,8062,uildActionExecuter.execute(InProcessBuildActionExecuter.java:26); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:79); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:51); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ExecuteBuild.doBuild(ExecuteBuild.java:59); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.WatchForDisconnection.execute(WatchForDisconnection.java:47); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ResetDeprecationLogger.execute(ResetDeprecationLogger.java:26); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.RequestStopIfSingleUsedDaemon.execute(RequestStopIfSingleUsedDaemon.java:34); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:8245,Availability,ERROR,ERROR,8245,r.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:79); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:51); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ExecuteBuild.doBuild(ExecuteBuild.java:59); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.WatchForDisconnection.execute(WatchForDisconnection.java:47); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ResetDeprecationLogger.execute(ResetDeprecationLogger.java:26); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.RequestStopIfSingleUsedDaemon.execute(RequestStopIfSingleUsedDaemon.java:34); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:8430,Availability,ERROR,ERROR,8430,rnal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:51); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ExecuteBuild.doBuild(ExecuteBuild.java:59); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.WatchForDisconnection.execute(WatchForDisconnection.java:47); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ResetDeprecationLogger.execute(ResetDeprecationLogger.java:26); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.RequestStopIfSingleUsedDaemon.execute(RequestStopIfSingleUsedDaemon.java:34); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:74); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gr,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:8615,Availability,ERROR,ERROR,8615,launcher.daemon.server.exec.ExecuteBuild.doBuild(ExecuteBuild.java:59); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.WatchForDisconnection.execute(WatchForDisconnection.java:47); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ResetDeprecationLogger.execute(ResetDeprecationLogger.java:26); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.RequestStopIfSingleUsedDaemon.execute(RequestStopIfSingleUsedDaemon.java:34); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:74); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:72); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.util,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:8800,Availability,ERROR,ERROR,8800,er.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.WatchForDisconnection.execute(WatchForDisconnection.java:47); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ResetDeprecationLogger.execute(ResetDeprecationLogger.java:26); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.RequestStopIfSingleUsedDaemon.execute(RequestStopIfSingleUsedDaemon.java:34); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:74); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:72); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.util.Swapper.swap(Swapper.java:38); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput.execute(Forwa,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:8999,Availability,ERROR,ERROR,8999,ion.proceed(DaemonCommandExecution.java:120); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.WatchForDisconnection.execute(WatchForDisconnection.java:47); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ResetDeprecationLogger.execute(ResetDeprecationLogger.java:26); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.RequestStopIfSingleUsedDaemon.execute(RequestStopIfSingleUsedDaemon.java:34); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:74); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:72); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.util.Swapper.swap(Swapper.java:38); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput.execute(ForwardClientInput.java:72); 22:05:55.981 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.j,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:9184,Availability,ERROR,ERROR,9184,ion.execute(WatchForDisconnection.java:47); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ResetDeprecationLogger.execute(ResetDeprecationLogger.java:26); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.RequestStopIfSingleUsedDaemon.execute(RequestStopIfSingleUsedDaemon.java:34); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:74); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:72); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.util.Swapper.swap(Swapper.java:38); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput.execute(ForwardClientInput.java:72); 22:05:55.981 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogAndCheckHealth.execute(LogAndCheckHealth.java:55); ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:9360,Availability,ERROR,ERROR,9360,dExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ResetDeprecationLogger.execute(ResetDeprecationLogger.java:26); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.RequestStopIfSingleUsedDaemon.execute(RequestStopIfSingleUsedDaemon.java:34); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:74); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:72); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.util.Swapper.swap(Swapper.java:38); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput.execute(ForwardClientInput.java:72); 22:05:55.981 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogAndCheckHealth.execute(LogAndCheckHealth.java:55); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.ja,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:9536,Availability,ERROR,ERROR,9536,setDeprecationLogger.execute(ResetDeprecationLogger.java:26); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.RequestStopIfSingleUsedDaemon.execute(RequestStopIfSingleUsedDaemon.java:34); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:74); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:72); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.util.Swapper.swap(Swapper.java:38); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput.execute(ForwardClientInput.java:72); 22:05:55.981 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogAndCheckHealth.execute(LogAndCheckHealth.java:55); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogToClient.doBuild(LogToClient.java:60); 22:0,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:9665,Availability,ERROR,ERROR,9665,nReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.RequestStopIfSingleUsedDaemon.execute(RequestStopIfSingleUsedDaemon.java:34); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:74); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:72); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.util.Swapper.swap(Swapper.java:38); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput.execute(ForwardClientInput.java:72); 22:05:55.981 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogAndCheckHealth.execute(LogAndCheckHealth.java:55); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogToClient.doBuild(LogToClient.java:60); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommand,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:9842,Availability,ERROR,ERROR,9842,ExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.RequestStopIfSingleUsedDaemon.execute(RequestStopIfSingleUsedDaemon.java:34); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:74); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:72); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.util.Swapper.swap(Swapper.java:38); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput.execute(ForwardClientInput.java:72); 22:05:55.981 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogAndCheckHealth.execute(LogAndCheckHealth.java:55); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogToClient.doBuild(LogToClient.java:60); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExec,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:10027,Availability,ERROR,ERROR,10027,ldevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:74); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:72); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.util.Swapper.swap(Swapper.java:38); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput.execute(ForwardClientInput.java:72); 22:05:55.981 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogAndCheckHealth.execute(LogAndCheckHealth.java:55); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogToClient.doBuild(LogToClient.java:60); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.EstablishBuildEn,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:10202,Availability,ERROR,ERROR,10202,ternal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:74); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:72); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.util.Swapper.swap(Swapper.java:38); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput.execute(ForwardClientInput.java:72); 22:05:55.981 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogAndCheckHealth.execute(LogAndCheckHealth.java:55); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogToClient.doBuild(LogToClient.java:60); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.EstablishBuildEnvironment.doBuild(EstablishBuildEnvironment.java:72); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:10387,Availability,ERROR,ERROR,10387,ildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:72); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.util.Swapper.swap(Swapper.java:38); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput.execute(ForwardClientInput.java:72); 22:05:55.981 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogAndCheckHealth.execute(LogAndCheckHealth.java:55); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogToClient.doBuild(LogToClient.java:60); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.EstablishBuildEnvironment.doBuild(EstablishBuildEnvironment.java:72); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommand,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:10550,Availability,ERROR,ERROR,10550,e.internal.buildevents.BuildExceptionReporter] 	at org.gradle.util.Swapper.swap(Swapper.java:38); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput.execute(ForwardClientInput.java:72); 22:05:55.981 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogAndCheckHealth.execute(LogAndCheckHealth.java:55); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogToClient.doBuild(LogToClient.java:60); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.EstablishBuildEnvironment.doBuild(EstablishBuildEnvironment.java:72); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.se,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:10723,Availability,ERROR,ERROR,10723,r] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput.execute(ForwardClientInput.java:72); 22:05:55.981 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogAndCheckHealth.execute(LogAndCheckHealth.java:55); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogToClient.doBuild(LogToClient.java:60); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.EstablishBuildEnvironment.doBuild(EstablishBuildEnvironment.java:72); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.HintGCAfterBuild.execute(HintGCAfterBuild.java:44); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.se,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:10908,Availability,ERROR,ERROR,10908,rg.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogAndCheckHealth.execute(LogAndCheckHealth.java:55); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogToClient.doBuild(LogToClient.java:60); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.EstablishBuildEnvironment.doBuild(EstablishBuildEnvironment.java:72); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.HintGCAfterBuild.execute(HintGCAfterBuild.java:44); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.se,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:11099,Availability,ERROR,ERROR,11099,dle.launcher.daemon.server.exec.LogAndCheckHealth.execute(LogAndCheckHealth.java:55); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogToClient.doBuild(LogToClient.java:60); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.EstablishBuildEnvironment.doBuild(EstablishBuildEnvironment.java:72); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.HintGCAfterBuild.execute(HintGCAfterBuild.java:44); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.StartBuildOrRespondWithBusy$1.run(StartBuildOrRespondWithBusy.java:50); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:11272,Availability,ERROR,ERROR,11272,radle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogToClient.doBuild(LogToClient.java:60); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.EstablishBuildEnvironment.doBuild(EstablishBuildEnvironment.java:72); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.HintGCAfterBuild.execute(HintGCAfterBuild.java:44); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.StartBuildOrRespondWithBusy$1.run(StartBuildOrRespondWithBusy.java:50); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.DaemonStateCoordinator$1.run(DaemonStateCoordinator.java:293); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.c,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:11457,Availability,ERROR,ERROR,11457,radle.launcher.daemon.server.exec.LogToClient.doBuild(LogToClient.java:60); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.EstablishBuildEnvironment.doBuild(EstablishBuildEnvironment.java:72); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.HintGCAfterBuild.execute(HintGCAfterBuild.java:44); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.StartBuildOrRespondWithBusy$1.run(StartBuildOrRespondWithBusy.java:50); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.DaemonStateCoordinator$1.run(DaemonStateCoordinator.java:293); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:54); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:11630,Availability,ERROR,ERROR,11630,cher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.EstablishBuildEnvironment.doBuild(EstablishBuildEnvironment.java:72); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.HintGCAfterBuild.execute(HintGCAfterBuild.java:44); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.StartBuildOrRespondWithBusy$1.run(StartBuildOrRespondWithBusy.java:50); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.DaemonStateCoordinator$1.run(DaemonStateCoordinator.java:293); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:54); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:40); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Caused by: org.gradle.ap,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:11815,Availability,ERROR,ERROR,11815,"server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.EstablishBuildEnvironment.doBuild(EstablishBuildEnvironment.java:72); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.HintGCAfterBuild.execute(HintGCAfterBuild.java:44); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.StartBuildOrRespondWithBusy$1.run(StartBuildOrRespondWithBusy.java:50); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.DaemonStateCoordinator$1.run(DaemonStateCoordinator.java:293); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:54); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:40); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Caused by: org.gradle.api.GradleException: Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 2. git-lfs is required to build GATK but may not be installed. See https://githu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:12008,Availability,ERROR,ERROR,12008,"xec.EstablishBuildEnvironment.doBuild(EstablishBuildEnvironment.java:72); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.HintGCAfterBuild.execute(HintGCAfterBuild.java:44); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.StartBuildOrRespondWithBusy$1.run(StartBuildOrRespondWithBusy.java:50); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.DaemonStateCoordinator$1.run(DaemonStateCoordinator.java:293); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:54); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:40); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Caused by: org.gradle.api.GradleException: Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 2. git-lfs is required to build GATK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK.; 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17$_reso",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:12187,Availability,ERROR,ERROR,12187,"mon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.HintGCAfterBuild.execute(HintGCAfterBuild.java:44); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.StartBuildOrRespondWithBusy$1.run(StartBuildOrRespondWithBusy.java:50); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.DaemonStateCoordinator$1.run(DaemonStateCoordinator.java:293); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:54); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:40); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Caused by: org.gradle.api.GradleException: Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 2. git-lfs is required to build GATK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK.; 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17$_resolveLargeResourceStubFiles_closure36.doCall(/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle:102); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.B",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:12373,Availability,ERROR,ERROR,12373,"i.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.HintGCAfterBuild.execute(HintGCAfterBuild.java:44); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.StartBuildOrRespondWithBusy$1.run(StartBuildOrRespondWithBusy.java:50); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.DaemonStateCoordinator$1.run(DaemonStateCoordinator.java:293); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:54); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:40); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Caused by: org.gradle.api.GradleException: Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 2. git-lfs is required to build GATK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK.; 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17$_resolveLargeResourceStubFiles_closure36.doCall(/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle:102); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17.resolveLargeResourceStubFiles(/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle:116); 22:05:55.985 [ER",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:12546,Availability,ERROR,ERROR,12546,"on.server.exec.HintGCAfterBuild.execute(HintGCAfterBuild.java:44); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.StartBuildOrRespondWithBusy$1.run(StartBuildOrRespondWithBusy.java:50); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.DaemonStateCoordinator$1.run(DaemonStateCoordinator.java:293); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:54); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:40); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Caused by: org.gradle.api.GradleException: Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 2. git-lfs is required to build GATK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK.; 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17$_resolveLargeResourceStubFiles_closure36.doCall(/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle:102); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17.resolveLargeResourceStubFiles(/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle:116); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17$resolveLargeResourceStubFiles$0.callCurrent(Unknown Source); 22:05:55.985 [",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:12907,Availability,ERROR,ERROR,12907,"server.exec.StartBuildOrRespondWithBusy$1.run(StartBuildOrRespondWithBusy.java:50); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.DaemonStateCoordinator$1.run(DaemonStateCoordinator.java:293); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:54); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:40); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Caused by: org.gradle.api.GradleException: Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 2. git-lfs is required to build GATK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK.; 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17$_resolveLargeResourceStubFiles_closure36.doCall(/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle:102); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17.resolveLargeResourceStubFiles(/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle:116); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17$resolveLargeResourceStubFiles$0.callCurrent(Unknown Source); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17.ensureBuildPrerequisites(/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle:140); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17$ensureBuildPrerequisites.callCurrent(Unkn",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:13150,Availability,ERROR,ERROR,13150,"rdinator.java:293); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:54); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:40); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Caused by: org.gradle.api.GradleException: Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 2. git-lfs is required to build GATK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK.; 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17$_resolveLargeResourceStubFiles_closure36.doCall(/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle:102); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17.resolveLargeResourceStubFiles(/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle:116); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17$resolveLargeResourceStubFiles$0.callCurrent(Unknown Source); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17.ensureBuildPrerequisites(/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle:140); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17$ensureBuildPrerequisites.callCurrent(Unknown Source); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17.run(/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle:143); 22:05:55.985 [ERROR] [org.gradl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:13375,Availability,ERROR,ERROR,13375,"] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:40); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Caused by: org.gradle.api.GradleException: Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 2. git-lfs is required to build GATK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK.; 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17$_resolveLargeResourceStubFiles_closure36.doCall(/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle:102); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17.resolveLargeResourceStubFiles(/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle:116); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17$resolveLargeResourceStubFiles$0.callCurrent(Unknown Source); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17.ensureBuildPrerequisites(/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle:140); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17$ensureBuildPrerequisites.callCurrent(Unknown Source); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17.run(/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle:143); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.groovy.scripts.internal.DefaultScriptRunnerFactory$ScriptRunnerImpl.run(DefaultScriptRunnerFactory.java:90); 22:05:55.986 [ERROR] [org.gradle.internal.buildevents.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:13550,Availability,ERROR,ERROR,13550,"[org.gradle.internal.buildevents.BuildExceptionReporter] Caused by: org.gradle.api.GradleException: Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 2. git-lfs is required to build GATK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK.; 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17$_resolveLargeResourceStubFiles_closure36.doCall(/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle:102); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17.resolveLargeResourceStubFiles(/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle:116); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17$resolveLargeResourceStubFiles$0.callCurrent(Unknown Source); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17.ensureBuildPrerequisites(/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle:140); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17$ensureBuildPrerequisites.callCurrent(Unknown Source); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17.run(/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle:143); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.groovy.scripts.internal.DefaultScriptRunnerFactory$ScriptRunnerImpl.run(DefaultScriptRunnerFactory.java:90); 22:05:55.986 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	... 58 more; 22:05:55.986 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.986 [LIFECYCLE] [org.gradle.internal.buildevents",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:13770,Availability,ERROR,ERROR,13770,TK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK.; 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17$_resolveLargeResourceStubFiles_closure36.doCall(/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle:102); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17.resolveLargeResourceStubFiles(/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle:116); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17$resolveLargeResourceStubFiles$0.callCurrent(Unknown Source); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17.ensureBuildPrerequisites(/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle:140); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17$ensureBuildPrerequisites.callCurrent(Unknown Source); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17.run(/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle:143); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.groovy.scripts.internal.DefaultScriptRunnerFactory$ScriptRunnerImpl.run(DefaultScriptRunnerFactory.java:90); 22:05:55.986 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	... 58 more; 22:05:55.986 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.986 [LIFECYCLE] [org.gradle.internal.buildevents.BuildResultLogger] ; 22:05:55.986 [LIFECYCLE] [org.gradle.internal.buildevents.BuildResultLogger] BUILD FAILED; 22:05:55.986 [LIFECYCLE] [org.gradle.internal.buildevents.BuildResultLogger] ; 22:05:55.987 [LIFECYCLE] [or,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:13938,Availability,ERROR,ERROR,13938,vents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17$_resolveLargeResourceStubFiles_closure36.doCall(/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle:102); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17.resolveLargeResourceStubFiles(/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle:116); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17$resolveLargeResourceStubFiles$0.callCurrent(Unknown Source); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17.ensureBuildPrerequisites(/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle:140); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17$ensureBuildPrerequisites.callCurrent(Unknown Source); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17.run(/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle:143); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.groovy.scripts.internal.DefaultScriptRunnerFactory$ScriptRunnerImpl.run(DefaultScriptRunnerFactory.java:90); 22:05:55.986 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	... 58 more; 22:05:55.986 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.986 [LIFECYCLE] [org.gradle.internal.buildevents.BuildResultLogger] ; 22:05:55.986 [LIFECYCLE] [org.gradle.internal.buildevents.BuildResultLogger] BUILD FAILED; 22:05:55.986 [LIFECYCLE] [org.gradle.internal.buildevents.BuildResultLogger] ; 22:05:55.987 [LIFECYCLE] [org.gradle.internal.buildevents.BuildResultLogger] Total time: 29.153 secs; ```. ```; root# su - portage; portage$ cd /scratch/var/tmp/portage/sci-biology/gatk-9999/work/,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:14137,Availability,ERROR,ERROR,14137,5:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17.resolveLargeResourceStubFiles(/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle:116); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17$resolveLargeResourceStubFiles$0.callCurrent(Unknown Source); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17.ensureBuildPrerequisites(/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle:140); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17$ensureBuildPrerequisites.callCurrent(Unknown Source); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17.run(/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle:143); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.groovy.scripts.internal.DefaultScriptRunnerFactory$ScriptRunnerImpl.run(DefaultScriptRunnerFactory.java:90); 22:05:55.986 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	... 58 more; 22:05:55.986 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.986 [LIFECYCLE] [org.gradle.internal.buildevents.BuildResultLogger] ; 22:05:55.986 [LIFECYCLE] [org.gradle.internal.buildevents.BuildResultLogger] BUILD FAILED; 22:05:55.986 [LIFECYCLE] [org.gradle.internal.buildevents.BuildResultLogger] ; 22:05:55.987 [LIFECYCLE] [org.gradle.internal.buildevents.BuildResultLogger] Total time: 29.153 secs; ```. ```; root# su - portage; portage$ cd /scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/; portage$ git lfs pull --include src/main/resources/large; No default remote. Errors logged to /scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git/lfs/objects/logs/20180420T,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:14339,Availability,ERROR,ERROR,14339,build.gradle:116); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17$resolveLargeResourceStubFiles$0.callCurrent(Unknown Source); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17.ensureBuildPrerequisites(/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle:140); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17$ensureBuildPrerequisites.callCurrent(Unknown Source); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17.run(/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle:143); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.groovy.scripts.internal.DefaultScriptRunnerFactory$ScriptRunnerImpl.run(DefaultScriptRunnerFactory.java:90); 22:05:55.986 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	... 58 more; 22:05:55.986 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.986 [LIFECYCLE] [org.gradle.internal.buildevents.BuildResultLogger] ; 22:05:55.986 [LIFECYCLE] [org.gradle.internal.buildevents.BuildResultLogger] BUILD FAILED; 22:05:55.986 [LIFECYCLE] [org.gradle.internal.buildevents.BuildResultLogger] ; 22:05:55.987 [LIFECYCLE] [org.gradle.internal.buildevents.BuildResultLogger] Total time: 29.153 secs; ```. ```; root# su - portage; portage$ cd /scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/; portage$ git lfs pull --include src/main/resources/large; No default remote. Errors logged to /scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git/lfs/objects/logs/20180420T221032.955218097.log; Use `git lfs logs last` to view the log.; portage$ cat /scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git/lfs/objects/logs/20180420T221032.955218097.log; git-lfs/2.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:14431,Availability,ERROR,ERROR,14431,ter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17$resolveLargeResourceStubFiles$0.callCurrent(Unknown Source); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17.ensureBuildPrerequisites(/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle:140); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17$ensureBuildPrerequisites.callCurrent(Unknown Source); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17.run(/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle:143); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.groovy.scripts.internal.DefaultScriptRunnerFactory$ScriptRunnerImpl.run(DefaultScriptRunnerFactory.java:90); 22:05:55.986 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	... 58 more; 22:05:55.986 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.986 [LIFECYCLE] [org.gradle.internal.buildevents.BuildResultLogger] ; 22:05:55.986 [LIFECYCLE] [org.gradle.internal.buildevents.BuildResultLogger] BUILD FAILED; 22:05:55.986 [LIFECYCLE] [org.gradle.internal.buildevents.BuildResultLogger] ; 22:05:55.987 [LIFECYCLE] [org.gradle.internal.buildevents.BuildResultLogger] Total time: 29.153 secs; ```. ```; root# su - portage; portage$ cd /scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/; portage$ git lfs pull --include src/main/resources/large; No default remote. Errors logged to /scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git/lfs/objects/logs/20180420T221032.955218097.log; Use `git lfs logs last` to view the log.; portage$ cat /scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git/lfs/objects/logs/20180420T221032.955218097.log; git-lfs/2.3.4 (GitHub; linux amd64; go 1.10); git version 2.16.3. $ git-lfs pull --include src/main/re,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:15031,Availability,Error,Errors,15031,-biology/gatk-9999/work/gatk-9999/build.gradle:143); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.groovy.scripts.internal.DefaultScriptRunnerFactory$ScriptRunnerImpl.run(DefaultScriptRunnerFactory.java:90); 22:05:55.986 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	... 58 more; 22:05:55.986 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.986 [LIFECYCLE] [org.gradle.internal.buildevents.BuildResultLogger] ; 22:05:55.986 [LIFECYCLE] [org.gradle.internal.buildevents.BuildResultLogger] BUILD FAILED; 22:05:55.986 [LIFECYCLE] [org.gradle.internal.buildevents.BuildResultLogger] ; 22:05:55.987 [LIFECYCLE] [org.gradle.internal.buildevents.BuildResultLogger] Total time: 29.153 secs; ```. ```; root# su - portage; portage$ cd /scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/; portage$ git lfs pull --include src/main/resources/large; No default remote. Errors logged to /scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git/lfs/objects/logs/20180420T221032.955218097.log; Use `git lfs logs last` to view the log.; portage$ cat /scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git/lfs/objects/logs/20180420T221032.955218097.log; git-lfs/2.3.4 (GitHub; linux amd64; go 1.10); git version 2.16.3. $ git-lfs pull --include src/main/resources/large; No default remote. No remotes defined. Current time in UTC: ; 2018-04-20 20:10:32. ENV:; LocalWorkingDir=/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999; LocalGitDir=/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git; LocalGitStorageDir=/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git; LocalMediaDir=/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git/lfs/objects; LocalReferenceDir=; TempDir=/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git/lfs/tmp; ConcurrentTransfers=3; TusTransfers=false; BasicTransfersOnly=false; SkipDownloadErrors=,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:16388,Availability,Down,DownloadTransfers,16388,ion 2.16.3. $ git-lfs pull --include src/main/resources/large; No default remote. No remotes defined. Current time in UTC: ; 2018-04-20 20:10:32. ENV:; LocalWorkingDir=/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999; LocalGitDir=/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git; LocalGitStorageDir=/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git; LocalMediaDir=/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git/lfs/objects; LocalReferenceDir=; TempDir=/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git/lfs/tmp; ConcurrentTransfers=3; TusTransfers=false; BasicTransfersOnly=false; SkipDownloadErrors=false; FetchRecentAlways=false; FetchRecentRefsDays=7; FetchRecentCommitsDays=0; FetchRecentRefsIncludeRemotes=true; PruneOffsetDays=3; PruneVerifyRemoteAlways=false; PruneRemoteName=origin; LfsStorageDir=/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git/lfs; AccessDownload=none; AccessUpload=none; DownloadTransfers=basic; UploadTransfers=basic. Client IP addresses:; xx.xx.xx.xx; xx.xx.xx.xx; xx.xx.xx.xx; portage$ ls -latr; total 188; -rw-r--r-- 1 portage portage 428 Apr 20 22:05 codecov.yml; -rwxr-xr-x 1 portage portage 5741 Apr 20 22:05 build_docker.sh; -rw-r--r-- 1 portage portage 32161 Apr 20 22:05 build.gradle; -rw-r--r-- 1 portage portage 37502 Apr 20 22:05 README.md; -rw-r--r-- 1 portage portage 1502 Apr 20 22:05 LICENSE.TXT; -rw-r--r-- 1 portage portage 1555 Apr 20 22:05 Dockerfile; -rw-r--r-- 1 portage portage 1128 Apr 20 22:05 AUTHORS; -rw-r--r-- 1 portage portage 8237 Apr 20 22:05 .travis.yml; -rw-r--r-- 1 portage portage 395 Apr 20 22:05 .gitignore; -rw-r--r-- 1 portage portage 128 Apr 20 22:05 .gitattributes; -rw-r--r-- 1 portage portage 142 Apr 20 22:05 .dockerignore; drwxr-xr-x 2 portage portage 4096 Apr 20 22:05 resources_for_CI; drwxr-xr-x 2 portage portage 4096 Apr 20 22:05 hooks; -rwxr-xr-x 1 portage portage 5242 Apr 20 22:05 gradlew; drwxr-xr-x 3 portage port,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:117,Deployability,install,installed,117,"Hi,; I am trying to build from gatk-4 master sources. I received this error code `2` admitedly when I had no git-lfs installed. Now it is installed and in my PATH, but the error still occurs. Can't you capture the real error message?. ```; 22:05:55.883 [QUIET] [system.out] Executing: git lfs pull --include src/main/resources/large; 22:05:55.943 [DEBUG] [org.gradle.configuration.project.BuildScriptProcessor] Timing: Running the build script took 12.879 secs; 22:05:55.952 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.954 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] FAILURE: Build failed with an exception.; 22:05:55.955 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.956 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Where:; 22:05:55.956 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Build file '/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle' line: 102; 22:05:55.964 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.964 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * What went wrong:; 22:05:55.966 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] A problem occurred evaluating root project 'gatk'.; 22:05:55.966 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] > Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 2. git-lfs is required to build GATK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK.; 22:05:55.967 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.968 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Exception is:; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] org.gradle.api.GradleScriptException: A problem occurred evaluating root project 'gatk'.; 22:05:55.969 [ERROR] [o",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:138,Deployability,install,installed,138,"Hi,; I am trying to build from gatk-4 master sources. I received this error code `2` admitedly when I had no git-lfs installed. Now it is installed and in my PATH, but the error still occurs. Can't you capture the real error message?. ```; 22:05:55.883 [QUIET] [system.out] Executing: git lfs pull --include src/main/resources/large; 22:05:55.943 [DEBUG] [org.gradle.configuration.project.BuildScriptProcessor] Timing: Running the build script took 12.879 secs; 22:05:55.952 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.954 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] FAILURE: Build failed with an exception.; 22:05:55.955 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.956 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Where:; 22:05:55.956 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Build file '/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle' line: 102; 22:05:55.964 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.964 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * What went wrong:; 22:05:55.966 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] A problem occurred evaluating root project 'gatk'.; 22:05:55.966 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] > Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 2. git-lfs is required to build GATK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK.; 22:05:55.967 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.968 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Exception is:; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] org.gradle.api.GradleScriptException: A problem occurred evaluating root project 'gatk'.; 22:05:55.969 [ERROR] [o",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:367,Deployability,configurat,configuration,367,"Hi,; I am trying to build from gatk-4 master sources. I received this error code `2` admitedly when I had no git-lfs installed. Now it is installed and in my PATH, but the error still occurs. Can't you capture the real error message?. ```; 22:05:55.883 [QUIET] [system.out] Executing: git lfs pull --include src/main/resources/large; 22:05:55.943 [DEBUG] [org.gradle.configuration.project.BuildScriptProcessor] Timing: Running the build script took 12.879 secs; 22:05:55.952 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.954 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] FAILURE: Build failed with an exception.; 22:05:55.955 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.956 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Where:; 22:05:55.956 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Build file '/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle' line: 102; 22:05:55.964 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.964 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * What went wrong:; 22:05:55.966 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] A problem occurred evaluating root project 'gatk'.; 22:05:55.966 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] > Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 2. git-lfs is required to build GATK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK.; 22:05:55.967 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.968 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Exception is:; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] org.gradle.api.GradleScriptException: A problem occurred evaluating root project 'gatk'.; 22:05:55.969 [ERROR] [o",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:1533,Deployability,install,installed,1533,"ts.BuildExceptionReporter] ; 22:05:55.954 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] FAILURE: Build failed with an exception.; 22:05:55.955 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.956 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Where:; 22:05:55.956 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Build file '/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle' line: 102; 22:05:55.964 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.964 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * What went wrong:; 22:05:55.966 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] A problem occurred evaluating root project 'gatk'.; 22:05:55.966 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] > Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 2. git-lfs is required to build GATK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK.; 22:05:55.967 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.968 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Exception is:; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] org.gradle.api.GradleScriptException: A problem occurred evaluating root project 'gatk'.; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.groovy.scripts.internal.DefaultScriptRunnerFactory$ScriptRunnerImpl.run(DefaultScriptRunnerFactory.java:92); 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl$2.run(DefaultScriptPluginFactory.java:176); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.ProjectScriptTarget.addConfigur",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:2273,Deployability,configurat,configuration,2273,"d evaluating root project 'gatk'.; 22:05:55.966 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] > Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 2. git-lfs is required to build GATK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK.; 22:05:55.967 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.968 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Exception is:; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] org.gradle.api.GradleScriptException: A problem occurred evaluating root project 'gatk'.; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.groovy.scripts.internal.DefaultScriptRunnerFactory$ScriptRunnerImpl.run(DefaultScriptRunnerFactory.java:92); 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl$2.run(DefaultScriptPluginFactory.java:176); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.ProjectScriptTarget.addConfiguration(ProjectScriptTarget.java:77); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl.apply(DefaultScriptPluginFactory.java:181); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:38); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:25); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.ConfigureActionsProjectEvaluator.evaluate(ConfigureActionsProjectEvalua",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:2468,Deployability,configurat,configuration,2468,code: 2. git-lfs is required to build GATK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK.; 22:05:55.967 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.968 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Exception is:; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] org.gradle.api.GradleScriptException: A problem occurred evaluating root project 'gatk'.; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.groovy.scripts.internal.DefaultScriptRunnerFactory$ScriptRunnerImpl.run(DefaultScriptRunnerFactory.java:92); 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl$2.run(DefaultScriptPluginFactory.java:176); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.ProjectScriptTarget.addConfiguration(ProjectScriptTarget.java:77); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl.apply(DefaultScriptPluginFactory.java:181); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:38); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:25); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.ConfigureActionsProjectEvaluator.evaluate(ConfigureActionsProjectEvaluator.java:34); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.LifecycleProjectEvaluator.evaluate(LifecycleProjectEvaluator.java:,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:2642,Deployability,configurat,configuration,2642,ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.968 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Exception is:; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] org.gradle.api.GradleScriptException: A problem occurred evaluating root project 'gatk'.; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.groovy.scripts.internal.DefaultScriptRunnerFactory$ScriptRunnerImpl.run(DefaultScriptRunnerFactory.java:92); 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl$2.run(DefaultScriptPluginFactory.java:176); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.ProjectScriptTarget.addConfiguration(ProjectScriptTarget.java:77); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl.apply(DefaultScriptPluginFactory.java:181); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:38); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:25); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.ConfigureActionsProjectEvaluator.evaluate(ConfigureActionsProjectEvaluator.java:34); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.LifecycleProjectEvaluator.evaluate(LifecycleProjectEvaluator.java:55); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:573); 22:05,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:2837,Deployability,configurat,configuration,2837,nternal.buildevents.BuildExceptionReporter] org.gradle.api.GradleScriptException: A problem occurred evaluating root project 'gatk'.; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.groovy.scripts.internal.DefaultScriptRunnerFactory$ScriptRunnerImpl.run(DefaultScriptRunnerFactory.java:92); 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl$2.run(DefaultScriptPluginFactory.java:176); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.ProjectScriptTarget.addConfiguration(ProjectScriptTarget.java:77); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl.apply(DefaultScriptPluginFactory.java:181); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:38); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:25); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.ConfigureActionsProjectEvaluator.evaluate(ConfigureActionsProjectEvaluator.java:34); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.LifecycleProjectEvaluator.evaluate(LifecycleProjectEvaluator.java:55); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:573); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:125); 22:05:55.971 [ERROR] [org.gradle.int,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:3012,Deployability,configurat,configuration,3012,.buildevents.BuildExceptionReporter] 	at org.gradle.groovy.scripts.internal.DefaultScriptRunnerFactory$ScriptRunnerImpl.run(DefaultScriptRunnerFactory.java:92); 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl$2.run(DefaultScriptPluginFactory.java:176); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.ProjectScriptTarget.addConfiguration(ProjectScriptTarget.java:77); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl.apply(DefaultScriptPluginFactory.java:181); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:38); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:25); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.ConfigureActionsProjectEvaluator.evaluate(ConfigureActionsProjectEvaluator.java:34); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.LifecycleProjectEvaluator.evaluate(LifecycleProjectEvaluator.java:55); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:573); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:125); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.TaskPathProjectEvaluator.configureHierarchy(TaskPathProjectEvaluator.java:42); 22:05:55.971 [ERROR] [org.gra,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:3187,Deployability,configurat,configuration,3187,ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl$2.run(DefaultScriptPluginFactory.java:176); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.ProjectScriptTarget.addConfiguration(ProjectScriptTarget.java:77); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl.apply(DefaultScriptPluginFactory.java:181); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:38); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:25); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.ConfigureActionsProjectEvaluator.evaluate(ConfigureActionsProjectEvaluator.java:34); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.LifecycleProjectEvaluator.evaluate(LifecycleProjectEvaluator.java:55); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:573); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:125); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.TaskPathProjectEvaluator.configureHierarchy(TaskPathProjectEvaluator.java:42); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultBuildConfigurer.configure(DefaultBuildConfigurer.java:38); 22:05:55.971 [ERROR] [org.gradl,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:3387,Deployability,configurat,configuration,3387,] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.ProjectScriptTarget.addConfiguration(ProjectScriptTarget.java:77); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl.apply(DefaultScriptPluginFactory.java:181); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:38); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:25); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.ConfigureActionsProjectEvaluator.evaluate(ConfigureActionsProjectEvaluator.java:34); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.LifecycleProjectEvaluator.evaluate(LifecycleProjectEvaluator.java:55); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:573); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:125); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.TaskPathProjectEvaluator.configureHierarchy(TaskPathProjectEvaluator.java:42); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultBuildConfigurer.configure(DefaultBuildConfigurer.java:38); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$2.run(DefaultGradleLauncher.java:151); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExc,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:4083,Deployability,configurat,configuration,4083,5); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.ConfigureActionsProjectEvaluator.evaluate(ConfigureActionsProjectEvaluator.java:34); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.LifecycleProjectEvaluator.evaluate(LifecycleProjectEvaluator.java:55); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:573); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:125); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.TaskPathProjectEvaluator.configureHierarchy(TaskPathProjectEvaluator.java:42); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultBuildConfigurer.configure(DefaultBuildConfigurer.java:38); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$2.run(DefaultGradleLauncher.java:151); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.Factories$1.create(Factories.java:22); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:91); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:53); 22:05:55.972 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuildStages(DefaultGradleLauncher.java:148); 22:05:55.972 [ERROR] [org.gradle.internal.buildevents.BuildExceptionRe,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:7250,Deployability,Continuous,ContinuousBuildActionExecuter,7250,[org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.GradleBuildController.run(GradleBuildController.java:66); 22:05:55.977 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ExecuteBuildActionRunner.run(ExecuteBuildActionRunner.java:28); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.ChainingBuildActionRunner.run(ChainingBuildActionRunner.java:35); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:41); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:26); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:79); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:51); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ExecuteBuild.doBuild(ExecuteBuild.java:59); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.WatchForDisconnection.execute(WatchForDisconnection.java:47); 22:05:55.979 [ERROR] [org.gradle.i,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:7288,Deployability,Continuous,ContinuousBuildActionExecuter,7288,uildExceptionReporter] 	at org.gradle.launcher.exec.GradleBuildController.run(GradleBuildController.java:66); 22:05:55.977 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ExecuteBuildActionRunner.run(ExecuteBuildActionRunner.java:28); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.ChainingBuildActionRunner.run(ChainingBuildActionRunner.java:35); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:41); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:26); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:79); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:51); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ExecuteBuild.doBuild(ExecuteBuild.java:59); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.WatchForDisconnection.execute(WatchForDisconnection.java:47); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildException,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:7447,Deployability,Continuous,ContinuousBuildActionExecuter,7447,.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ExecuteBuildActionRunner.run(ExecuteBuildActionRunner.java:28); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.ChainingBuildActionRunner.run(ChainingBuildActionRunner.java:35); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:41); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:26); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:79); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:51); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ExecuteBuild.doBuild(ExecuteBuild.java:59); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.WatchForDisconnection.execute(WatchForDisconnection.java:47); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.979 [ERROR] [org.gradle.internal.buil,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:7485,Deployability,Continuous,ContinuousBuildActionExecuter,7485,radle.tooling.internal.provider.ExecuteBuildActionRunner.run(ExecuteBuildActionRunner.java:28); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.ChainingBuildActionRunner.run(ChainingBuildActionRunner.java:35); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:41); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:26); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:79); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:51); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ExecuteBuild.doBuild(ExecuteBuild.java:59); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.WatchForDisconnection.execute(WatchForDisconnection.java:47); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	a,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:12791,Deployability,install,installed,12791,"onCommandExecution.java:120); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.StartBuildOrRespondWithBusy$1.run(StartBuildOrRespondWithBusy.java:50); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.DaemonStateCoordinator$1.run(DaemonStateCoordinator.java:293); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:54); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:40); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Caused by: org.gradle.api.GradleException: Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 2. git-lfs is required to build GATK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK.; 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17$_resolveLargeResourceStubFiles_closure36.doCall(/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle:102); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17.resolveLargeResourceStubFiles(/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle:116); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17$resolveLargeResourceStubFiles$0.callCurrent(Unknown Source); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17.ensureBuildPrerequisites(/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle:140); 22:05:55.985 [E",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:225,Integrability,message,message,225,"Hi,; I am trying to build from gatk-4 master sources. I received this error code `2` admitedly when I had no git-lfs installed. Now it is installed and in my PATH, but the error still occurs. Can't you capture the real error message?. ```; 22:05:55.883 [QUIET] [system.out] Executing: git lfs pull --include src/main/resources/large; 22:05:55.943 [DEBUG] [org.gradle.configuration.project.BuildScriptProcessor] Timing: Running the build script took 12.879 secs; 22:05:55.952 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.954 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] FAILURE: Build failed with an exception.; 22:05:55.955 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.956 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Where:; 22:05:55.956 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Build file '/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle' line: 102; 22:05:55.964 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.964 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * What went wrong:; 22:05:55.966 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] A problem occurred evaluating root project 'gatk'.; 22:05:55.966 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] > Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 2. git-lfs is required to build GATK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK.; 22:05:55.967 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.968 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Exception is:; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] org.gradle.api.GradleScriptException: A problem occurred evaluating root project 'gatk'.; 22:05:55.969 [ERROR] [o",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:367,Modifiability,config,configuration,367,"Hi,; I am trying to build from gatk-4 master sources. I received this error code `2` admitedly when I had no git-lfs installed. Now it is installed and in my PATH, but the error still occurs. Can't you capture the real error message?. ```; 22:05:55.883 [QUIET] [system.out] Executing: git lfs pull --include src/main/resources/large; 22:05:55.943 [DEBUG] [org.gradle.configuration.project.BuildScriptProcessor] Timing: Running the build script took 12.879 secs; 22:05:55.952 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.954 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] FAILURE: Build failed with an exception.; 22:05:55.955 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.956 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Where:; 22:05:55.956 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Build file '/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle' line: 102; 22:05:55.964 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.964 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * What went wrong:; 22:05:55.966 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] A problem occurred evaluating root project 'gatk'.; 22:05:55.966 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] > Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 2. git-lfs is required to build GATK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK.; 22:05:55.967 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.968 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Exception is:; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] org.gradle.api.GradleScriptException: A problem occurred evaluating root project 'gatk'.; 22:05:55.969 [ERROR] [o",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:2273,Modifiability,config,configuration,2273,"d evaluating root project 'gatk'.; 22:05:55.966 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] > Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 2. git-lfs is required to build GATK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK.; 22:05:55.967 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.968 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Exception is:; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] org.gradle.api.GradleScriptException: A problem occurred evaluating root project 'gatk'.; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.groovy.scripts.internal.DefaultScriptRunnerFactory$ScriptRunnerImpl.run(DefaultScriptRunnerFactory.java:92); 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl$2.run(DefaultScriptPluginFactory.java:176); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.ProjectScriptTarget.addConfiguration(ProjectScriptTarget.java:77); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl.apply(DefaultScriptPluginFactory.java:181); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:38); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:25); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.ConfigureActionsProjectEvaluator.evaluate(ConfigureActionsProjectEvalua",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:2468,Modifiability,config,configuration,2468,code: 2. git-lfs is required to build GATK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK.; 22:05:55.967 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.968 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Exception is:; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] org.gradle.api.GradleScriptException: A problem occurred evaluating root project 'gatk'.; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.groovy.scripts.internal.DefaultScriptRunnerFactory$ScriptRunnerImpl.run(DefaultScriptRunnerFactory.java:92); 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl$2.run(DefaultScriptPluginFactory.java:176); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.ProjectScriptTarget.addConfiguration(ProjectScriptTarget.java:77); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl.apply(DefaultScriptPluginFactory.java:181); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:38); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:25); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.ConfigureActionsProjectEvaluator.evaluate(ConfigureActionsProjectEvaluator.java:34); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.LifecycleProjectEvaluator.evaluate(LifecycleProjectEvaluator.java:,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:2642,Modifiability,config,configuration,2642,ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.968 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Exception is:; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] org.gradle.api.GradleScriptException: A problem occurred evaluating root project 'gatk'.; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.groovy.scripts.internal.DefaultScriptRunnerFactory$ScriptRunnerImpl.run(DefaultScriptRunnerFactory.java:92); 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl$2.run(DefaultScriptPluginFactory.java:176); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.ProjectScriptTarget.addConfiguration(ProjectScriptTarget.java:77); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl.apply(DefaultScriptPluginFactory.java:181); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:38); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:25); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.ConfigureActionsProjectEvaluator.evaluate(ConfigureActionsProjectEvaluator.java:34); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.LifecycleProjectEvaluator.evaluate(LifecycleProjectEvaluator.java:55); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:573); 22:05,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:2837,Modifiability,config,configuration,2837,nternal.buildevents.BuildExceptionReporter] org.gradle.api.GradleScriptException: A problem occurred evaluating root project 'gatk'.; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.groovy.scripts.internal.DefaultScriptRunnerFactory$ScriptRunnerImpl.run(DefaultScriptRunnerFactory.java:92); 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl$2.run(DefaultScriptPluginFactory.java:176); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.ProjectScriptTarget.addConfiguration(ProjectScriptTarget.java:77); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl.apply(DefaultScriptPluginFactory.java:181); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:38); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:25); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.ConfigureActionsProjectEvaluator.evaluate(ConfigureActionsProjectEvaluator.java:34); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.LifecycleProjectEvaluator.evaluate(LifecycleProjectEvaluator.java:55); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:573); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:125); 22:05:55.971 [ERROR] [org.gradle.int,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:3012,Modifiability,config,configuration,3012,.buildevents.BuildExceptionReporter] 	at org.gradle.groovy.scripts.internal.DefaultScriptRunnerFactory$ScriptRunnerImpl.run(DefaultScriptRunnerFactory.java:92); 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl$2.run(DefaultScriptPluginFactory.java:176); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.ProjectScriptTarget.addConfiguration(ProjectScriptTarget.java:77); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl.apply(DefaultScriptPluginFactory.java:181); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:38); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:25); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.ConfigureActionsProjectEvaluator.evaluate(ConfigureActionsProjectEvaluator.java:34); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.LifecycleProjectEvaluator.evaluate(LifecycleProjectEvaluator.java:55); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:573); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:125); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.TaskPathProjectEvaluator.configureHierarchy(TaskPathProjectEvaluator.java:42); 22:05:55.971 [ERROR] [org.gra,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:3187,Modifiability,config,configuration,3187,ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl$2.run(DefaultScriptPluginFactory.java:176); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.ProjectScriptTarget.addConfiguration(ProjectScriptTarget.java:77); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl.apply(DefaultScriptPluginFactory.java:181); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:38); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:25); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.ConfigureActionsProjectEvaluator.evaluate(ConfigureActionsProjectEvaluator.java:34); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.LifecycleProjectEvaluator.evaluate(LifecycleProjectEvaluator.java:55); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:573); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:125); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.TaskPathProjectEvaluator.configureHierarchy(TaskPathProjectEvaluator.java:42); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultBuildConfigurer.configure(DefaultBuildConfigurer.java:38); 22:05:55.971 [ERROR] [org.gradl,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:3209,Modifiability,Config,ConfigureActionsProjectEvaluator,3209,ldevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl$2.run(DefaultScriptPluginFactory.java:176); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.ProjectScriptTarget.addConfiguration(ProjectScriptTarget.java:77); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl.apply(DefaultScriptPluginFactory.java:181); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:38); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:25); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.ConfigureActionsProjectEvaluator.evaluate(ConfigureActionsProjectEvaluator.java:34); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.LifecycleProjectEvaluator.evaluate(LifecycleProjectEvaluator.java:55); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:573); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:125); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.TaskPathProjectEvaluator.configureHierarchy(TaskPathProjectEvaluator.java:42); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultBuildConfigurer.configure(DefaultBuildConfigurer.java:38); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExce,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:3251,Modifiability,Config,ConfigureActionsProjectEvaluator,3251,rg.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl$2.run(DefaultScriptPluginFactory.java:176); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.ProjectScriptTarget.addConfiguration(ProjectScriptTarget.java:77); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl.apply(DefaultScriptPluginFactory.java:181); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:38); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:25); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.ConfigureActionsProjectEvaluator.evaluate(ConfigureActionsProjectEvaluator.java:34); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.LifecycleProjectEvaluator.evaluate(LifecycleProjectEvaluator.java:55); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:573); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:125); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.TaskPathProjectEvaluator.configureHierarchy(TaskPathProjectEvaluator.java:42); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultBuildConfigurer.configure(DefaultBuildConfigurer.java:38); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initial,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:3387,Modifiability,config,configuration,3387,] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.ProjectScriptTarget.addConfiguration(ProjectScriptTarget.java:77); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl.apply(DefaultScriptPluginFactory.java:181); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:38); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:25); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.ConfigureActionsProjectEvaluator.evaluate(ConfigureActionsProjectEvaluator.java:34); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.LifecycleProjectEvaluator.evaluate(LifecycleProjectEvaluator.java:55); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:573); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:125); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.TaskPathProjectEvaluator.configureHierarchy(TaskPathProjectEvaluator.java:42); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultBuildConfigurer.configure(DefaultBuildConfigurer.java:38); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$2.run(DefaultGradleLauncher.java:151); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExc,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:3936,Modifiability,config,configureHierarchy,3936,al.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:25); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.ConfigureActionsProjectEvaluator.evaluate(ConfigureActionsProjectEvaluator.java:34); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.LifecycleProjectEvaluator.evaluate(LifecycleProjectEvaluator.java:55); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:573); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:125); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.TaskPathProjectEvaluator.configureHierarchy(TaskPathProjectEvaluator.java:42); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultBuildConfigurer.configure(DefaultBuildConfigurer.java:38); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$2.run(DefaultGradleLauncher.java:151); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.Factories$1.create(Factories.java:22); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:91); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:53); 22:05:55.972 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.Default,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:4083,Modifiability,config,configuration,4083,5); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.ConfigureActionsProjectEvaluator.evaluate(ConfigureActionsProjectEvaluator.java:34); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.LifecycleProjectEvaluator.evaluate(LifecycleProjectEvaluator.java:55); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:573); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:125); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.TaskPathProjectEvaluator.configureHierarchy(TaskPathProjectEvaluator.java:42); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultBuildConfigurer.configure(DefaultBuildConfigurer.java:38); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$2.run(DefaultGradleLauncher.java:151); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.Factories$1.create(Factories.java:22); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:91); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:53); 22:05:55.972 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuildStages(DefaultGradleLauncher.java:148); 22:05:55.972 [ERROR] [org.gradle.internal.buildevents.BuildExceptionRe,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:4120,Modifiability,config,configure,4120,buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.ConfigureActionsProjectEvaluator.evaluate(ConfigureActionsProjectEvaluator.java:34); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.LifecycleProjectEvaluator.evaluate(LifecycleProjectEvaluator.java:55); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:573); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:125); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.TaskPathProjectEvaluator.configureHierarchy(TaskPathProjectEvaluator.java:42); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultBuildConfigurer.configure(DefaultBuildConfigurer.java:38); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$2.run(DefaultGradleLauncher.java:151); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.Factories$1.create(Factories.java:22); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:91); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:53); 22:05:55.972 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuildStages(DefaultGradleLauncher.java:148); 22:05:55.972 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGr,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:12275,Performance,concurren,concurrent,12275,"org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.HintGCAfterBuild.execute(HintGCAfterBuild.java:44); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.StartBuildOrRespondWithBusy$1.run(StartBuildOrRespondWithBusy.java:50); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.DaemonStateCoordinator$1.run(DaemonStateCoordinator.java:293); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:54); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:40); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Caused by: org.gradle.api.GradleException: Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 2. git-lfs is required to build GATK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK.; 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17$_resolveLargeResourceStubFiles_closure36.doCall(/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle:102); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17.resolveLargeResourceStubFiles(/",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:12461,Performance,concurren,concurrent,12461,"rg.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.HintGCAfterBuild.execute(HintGCAfterBuild.java:44); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.StartBuildOrRespondWithBusy$1.run(StartBuildOrRespondWithBusy.java:50); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.DaemonStateCoordinator$1.run(DaemonStateCoordinator.java:293); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:54); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:40); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Caused by: org.gradle.api.GradleException: Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 2. git-lfs is required to build GATK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK.; 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17$_resolveLargeResourceStubFiles_closure36.doCall(/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle:102); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17.resolveLargeResourceStubFiles(/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle:116); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5h",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:15983,Performance,Concurren,ConcurrentTransfers,15983,097.log; Use `git lfs logs last` to view the log.; portage$ cat /scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git/lfs/objects/logs/20180420T221032.955218097.log; git-lfs/2.3.4 (GitHub; linux amd64; go 1.10); git version 2.16.3. $ git-lfs pull --include src/main/resources/large; No default remote. No remotes defined. Current time in UTC: ; 2018-04-20 20:10:32. ENV:; LocalWorkingDir=/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999; LocalGitDir=/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git; LocalGitStorageDir=/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git; LocalMediaDir=/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git/lfs/objects; LocalReferenceDir=; TempDir=/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git/lfs/tmp; ConcurrentTransfers=3; TusTransfers=false; BasicTransfersOnly=false; SkipDownloadErrors=false; FetchRecentAlways=false; FetchRecentRefsDays=7; FetchRecentCommitsDays=0; FetchRecentRefsIncludeRemotes=true; PruneOffsetDays=3; PruneVerifyRemoteAlways=false; PruneRemoteName=origin; LfsStorageDir=/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git/lfs; AccessDownload=none; AccessUpload=none; DownloadTransfers=basic; UploadTransfers=basic. Client IP addresses:; xx.xx.xx.xx; xx.xx.xx.xx; xx.xx.xx.xx; portage$ ls -latr; total 188; -rw-r--r-- 1 portage portage 428 Apr 20 22:05 codecov.yml; -rwxr-xr-x 1 portage portage 5741 Apr 20 22:05 build_docker.sh; -rw-r--r-- 1 portage portage 32161 Apr 20 22:05 build.gradle; -rw-r--r-- 1 portage portage 37502 Apr 20 22:05 README.md; -rw-r--r-- 1 portage portage 1502 Apr 20 22:05 LICENSE.TXT; -rw-r--r-- 1 portage portage 1555 Apr 20 22:05 Dockerfile; -rw-r--r-- 1 portage portage 1128 Apr 20 22:05 AUTHORS; -rw-r--r-- 1 portage portage 8237 Apr 20 22:05 .travis.yml; -rw-r--r-- 1 portage portage 395 Apr 20 22:05 .gitignore; -rw-r--r-- 1 portage portage 128 Apr 20 22:05 .gitattributes; -rw-r--r-- 1 portage portage ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:5150,Security,access,access,5150,05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$2.run(DefaultGradleLauncher.java:151); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.Factories$1.create(Factories.java:22); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:91); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:53); 22:05:55.972 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuildStages(DefaultGradleLauncher.java:148); 22:05:55.972 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.access$200(DefaultGradleLauncher.java:33); 22:05:55.973 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$1.create(DefaultGradleLauncher.java:112); 22:05:55.973 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$1.create(DefaultGradleLauncher.java:106); 22:05:55.973 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:91); 22:05:55.976 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:63); 22:05:55.976 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuild(DefaultGradleLauncher.java:106); 22:05:55.976 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gra,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:16348,Security,Access,AccessDownload,16348,ion 2.16.3. $ git-lfs pull --include src/main/resources/large; No default remote. No remotes defined. Current time in UTC: ; 2018-04-20 20:10:32. ENV:; LocalWorkingDir=/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999; LocalGitDir=/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git; LocalGitStorageDir=/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git; LocalMediaDir=/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git/lfs/objects; LocalReferenceDir=; TempDir=/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git/lfs/tmp; ConcurrentTransfers=3; TusTransfers=false; BasicTransfersOnly=false; SkipDownloadErrors=false; FetchRecentAlways=false; FetchRecentRefsDays=7; FetchRecentCommitsDays=0; FetchRecentRefsIncludeRemotes=true; PruneOffsetDays=3; PruneVerifyRemoteAlways=false; PruneRemoteName=origin; LfsStorageDir=/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git/lfs; AccessDownload=none; AccessUpload=none; DownloadTransfers=basic; UploadTransfers=basic. Client IP addresses:; xx.xx.xx.xx; xx.xx.xx.xx; xx.xx.xx.xx; portage$ ls -latr; total 188; -rw-r--r-- 1 portage portage 428 Apr 20 22:05 codecov.yml; -rwxr-xr-x 1 portage portage 5741 Apr 20 22:05 build_docker.sh; -rw-r--r-- 1 portage portage 32161 Apr 20 22:05 build.gradle; -rw-r--r-- 1 portage portage 37502 Apr 20 22:05 README.md; -rw-r--r-- 1 portage portage 1502 Apr 20 22:05 LICENSE.TXT; -rw-r--r-- 1 portage portage 1555 Apr 20 22:05 Dockerfile; -rw-r--r-- 1 portage portage 1128 Apr 20 22:05 AUTHORS; -rw-r--r-- 1 portage portage 8237 Apr 20 22:05 .travis.yml; -rw-r--r-- 1 portage portage 395 Apr 20 22:05 .gitignore; -rw-r--r-- 1 portage portage 128 Apr 20 22:05 .gitattributes; -rw-r--r-- 1 portage portage 142 Apr 20 22:05 .dockerignore; drwxr-xr-x 2 portage portage 4096 Apr 20 22:05 resources_for_CI; drwxr-xr-x 2 portage portage 4096 Apr 20 22:05 hooks; -rwxr-xr-x 1 portage portage 5242 Apr 20 22:05 gradlew; drwxr-xr-x 3 portage port,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:16369,Security,Access,AccessUpload,16369,ion 2.16.3. $ git-lfs pull --include src/main/resources/large; No default remote. No remotes defined. Current time in UTC: ; 2018-04-20 20:10:32. ENV:; LocalWorkingDir=/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999; LocalGitDir=/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git; LocalGitStorageDir=/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git; LocalMediaDir=/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git/lfs/objects; LocalReferenceDir=; TempDir=/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git/lfs/tmp; ConcurrentTransfers=3; TusTransfers=false; BasicTransfersOnly=false; SkipDownloadErrors=false; FetchRecentAlways=false; FetchRecentRefsDays=7; FetchRecentCommitsDays=0; FetchRecentRefsIncludeRemotes=true; PruneOffsetDays=3; PruneVerifyRemoteAlways=false; PruneRemoteName=origin; LfsStorageDir=/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git/lfs; AccessDownload=none; AccessUpload=none; DownloadTransfers=basic; UploadTransfers=basic. Client IP addresses:; xx.xx.xx.xx; xx.xx.xx.xx; xx.xx.xx.xx; portage$ ls -latr; total 188; -rw-r--r-- 1 portage portage 428 Apr 20 22:05 codecov.yml; -rwxr-xr-x 1 portage portage 5741 Apr 20 22:05 build_docker.sh; -rw-r--r-- 1 portage portage 32161 Apr 20 22:05 build.gradle; -rw-r--r-- 1 portage portage 37502 Apr 20 22:05 README.md; -rw-r--r-- 1 portage portage 1502 Apr 20 22:05 LICENSE.TXT; -rw-r--r-- 1 portage portage 1555 Apr 20 22:05 Dockerfile; -rw-r--r-- 1 portage portage 1128 Apr 20 22:05 AUTHORS; -rw-r--r-- 1 portage portage 8237 Apr 20 22:05 .travis.yml; -rw-r--r-- 1 portage portage 395 Apr 20 22:05 .gitignore; -rw-r--r-- 1 portage portage 128 Apr 20 22:05 .gitattributes; -rw-r--r-- 1 portage portage 142 Apr 20 22:05 .dockerignore; drwxr-xr-x 2 portage portage 4096 Apr 20 22:05 resources_for_CI; drwxr-xr-x 2 portage portage 4096 Apr 20 22:05 hooks; -rwxr-xr-x 1 portage portage 5242 Apr 20 22:05 gradlew; drwxr-xr-x 3 portage port,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:10134,Testability,Log,LogAndCheckHealth,10134,ommandExecution.java:120); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:74); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:72); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.util.Swapper.swap(Swapper.java:38); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput.execute(ForwardClientInput.java:72); 22:05:55.981 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogAndCheckHealth.execute(LogAndCheckHealth.java:55); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogToClient.doBuild(LogToClient.java:60); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.EstablishBuildEnvironment.doBuild(EstablishBuildEnvironment.java:72); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.Buil,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:10160,Testability,Log,LogAndCheckHealth,10160,20); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:74); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:72); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.util.Swapper.swap(Swapper.java:38); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput.execute(ForwardClientInput.java:72); 22:05:55.981 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogAndCheckHealth.execute(LogAndCheckHealth.java:55); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogToClient.doBuild(LogToClient.java:60); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.EstablishBuildEnvironment.doBuild(EstablishBuildEnvironment.java:72); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	a,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:10494,Testability,Log,LogToClient,10494,ClientInput.java:72); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.util.Swapper.swap(Swapper.java:38); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput.execute(ForwardClientInput.java:72); 22:05:55.981 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogAndCheckHealth.execute(LogAndCheckHealth.java:55); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogToClient.doBuild(LogToClient.java:60); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.EstablishBuildEnvironment.doBuild(EstablishBuildEnvironment.java:72); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.Bui,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:10514,Testability,Log,LogToClient,10514,:72); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.util.Swapper.swap(Swapper.java:38); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput.execute(ForwardClientInput.java:72); 22:05:55.981 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogAndCheckHealth.execute(LogAndCheckHealth.java:55); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogToClient.doBuild(LogToClient.java:60); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.EstablishBuildEnvironment.doBuild(EstablishBuildEnvironment.java:72); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionRepor,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:15038,Testability,log,logged,15038,-biology/gatk-9999/work/gatk-9999/build.gradle:143); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.groovy.scripts.internal.DefaultScriptRunnerFactory$ScriptRunnerImpl.run(DefaultScriptRunnerFactory.java:90); 22:05:55.986 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	... 58 more; 22:05:55.986 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.986 [LIFECYCLE] [org.gradle.internal.buildevents.BuildResultLogger] ; 22:05:55.986 [LIFECYCLE] [org.gradle.internal.buildevents.BuildResultLogger] BUILD FAILED; 22:05:55.986 [LIFECYCLE] [org.gradle.internal.buildevents.BuildResultLogger] ; 22:05:55.987 [LIFECYCLE] [org.gradle.internal.buildevents.BuildResultLogger] Total time: 29.153 secs; ```. ```; root# su - portage; portage$ cd /scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/; portage$ git lfs pull --include src/main/resources/large; No default remote. Errors logged to /scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git/lfs/objects/logs/20180420T221032.955218097.log; Use `git lfs logs last` to view the log.; portage$ cat /scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git/lfs/objects/logs/20180420T221032.955218097.log; git-lfs/2.3.4 (GitHub; linux amd64; go 1.10); git version 2.16.3. $ git-lfs pull --include src/main/resources/large; No default remote. No remotes defined. Current time in UTC: ; 2018-04-20 20:10:32. ENV:; LocalWorkingDir=/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999; LocalGitDir=/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git; LocalGitStorageDir=/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git; LocalMediaDir=/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git/lfs/objects; LocalReferenceDir=; TempDir=/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git/lfs/tmp; ConcurrentTransfers=3; TusTransfers=false; BasicTransfersOnly=false; SkipDownloadErrors=,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:15127,Testability,log,logs,15127,55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.groovy.scripts.internal.DefaultScriptRunnerFactory$ScriptRunnerImpl.run(DefaultScriptRunnerFactory.java:90); 22:05:55.986 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	... 58 more; 22:05:55.986 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.986 [LIFECYCLE] [org.gradle.internal.buildevents.BuildResultLogger] ; 22:05:55.986 [LIFECYCLE] [org.gradle.internal.buildevents.BuildResultLogger] BUILD FAILED; 22:05:55.986 [LIFECYCLE] [org.gradle.internal.buildevents.BuildResultLogger] ; 22:05:55.987 [LIFECYCLE] [org.gradle.internal.buildevents.BuildResultLogger] Total time: 29.153 secs; ```. ```; root# su - portage; portage$ cd /scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/; portage$ git lfs pull --include src/main/resources/large; No default remote. Errors logged to /scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git/lfs/objects/logs/20180420T221032.955218097.log; Use `git lfs logs last` to view the log.; portage$ cat /scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git/lfs/objects/logs/20180420T221032.955218097.log; git-lfs/2.3.4 (GitHub; linux amd64; go 1.10); git version 2.16.3. $ git-lfs pull --include src/main/resources/large; No default remote. No remotes defined. Current time in UTC: ; 2018-04-20 20:10:32. ENV:; LocalWorkingDir=/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999; LocalGitDir=/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git; LocalGitStorageDir=/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git; LocalMediaDir=/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git/lfs/objects; LocalReferenceDir=; TempDir=/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git/lfs/tmp; ConcurrentTransfers=3; TusTransfers=false; BasicTransfersOnly=false; SkipDownloadErrors=false; FetchRecentAlways=false; FetchRecentRefsDays=7; Fetc,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:15158,Testability,log,log,15158,ldExceptionReporter] 	at org.gradle.groovy.scripts.internal.DefaultScriptRunnerFactory$ScriptRunnerImpl.run(DefaultScriptRunnerFactory.java:90); 22:05:55.986 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	... 58 more; 22:05:55.986 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.986 [LIFECYCLE] [org.gradle.internal.buildevents.BuildResultLogger] ; 22:05:55.986 [LIFECYCLE] [org.gradle.internal.buildevents.BuildResultLogger] BUILD FAILED; 22:05:55.986 [LIFECYCLE] [org.gradle.internal.buildevents.BuildResultLogger] ; 22:05:55.987 [LIFECYCLE] [org.gradle.internal.buildevents.BuildResultLogger] Total time: 29.153 secs; ```. ```; root# su - portage; portage$ cd /scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/; portage$ git lfs pull --include src/main/resources/large; No default remote. Errors logged to /scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git/lfs/objects/logs/20180420T221032.955218097.log; Use `git lfs logs last` to view the log.; portage$ cat /scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git/lfs/objects/logs/20180420T221032.955218097.log; git-lfs/2.3.4 (GitHub; linux amd64; go 1.10); git version 2.16.3. $ git-lfs pull --include src/main/resources/large; No default remote. No remotes defined. Current time in UTC: ; 2018-04-20 20:10:32. ENV:; LocalWorkingDir=/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999; LocalGitDir=/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git; LocalGitStorageDir=/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git; LocalMediaDir=/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git/lfs/objects; LocalReferenceDir=; TempDir=/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git/lfs/tmp; ConcurrentTransfers=3; TusTransfers=false; BasicTransfersOnly=false; SkipDownloadErrors=false; FetchRecentAlways=false; FetchRecentRefsDays=7; FetchRecentCommitsDays=0; FetchRecentRefsIncludeRemotes,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:15176,Testability,log,logs,15176,ldExceptionReporter] 	at org.gradle.groovy.scripts.internal.DefaultScriptRunnerFactory$ScriptRunnerImpl.run(DefaultScriptRunnerFactory.java:90); 22:05:55.986 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	... 58 more; 22:05:55.986 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.986 [LIFECYCLE] [org.gradle.internal.buildevents.BuildResultLogger] ; 22:05:55.986 [LIFECYCLE] [org.gradle.internal.buildevents.BuildResultLogger] BUILD FAILED; 22:05:55.986 [LIFECYCLE] [org.gradle.internal.buildevents.BuildResultLogger] ; 22:05:55.987 [LIFECYCLE] [org.gradle.internal.buildevents.BuildResultLogger] Total time: 29.153 secs; ```. ```; root# su - portage; portage$ cd /scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/; portage$ git lfs pull --include src/main/resources/large; No default remote. Errors logged to /scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git/lfs/objects/logs/20180420T221032.955218097.log; Use `git lfs logs last` to view the log.; portage$ cat /scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git/lfs/objects/logs/20180420T221032.955218097.log; git-lfs/2.3.4 (GitHub; linux amd64; go 1.10); git version 2.16.3. $ git-lfs pull --include src/main/resources/large; No default remote. No remotes defined. Current time in UTC: ; 2018-04-20 20:10:32. ENV:; LocalWorkingDir=/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999; LocalGitDir=/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git; LocalGitStorageDir=/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git; LocalMediaDir=/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git/lfs/objects; LocalReferenceDir=; TempDir=/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git/lfs/tmp; ConcurrentTransfers=3; TusTransfers=false; BasicTransfersOnly=false; SkipDownloadErrors=false; FetchRecentAlways=false; FetchRecentRefsDays=7; FetchRecentCommitsDays=0; FetchRecentRefsIncludeRemotes,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:15199,Testability,log,log,15199,ldExceptionReporter] 	at org.gradle.groovy.scripts.internal.DefaultScriptRunnerFactory$ScriptRunnerImpl.run(DefaultScriptRunnerFactory.java:90); 22:05:55.986 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	... 58 more; 22:05:55.986 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.986 [LIFECYCLE] [org.gradle.internal.buildevents.BuildResultLogger] ; 22:05:55.986 [LIFECYCLE] [org.gradle.internal.buildevents.BuildResultLogger] BUILD FAILED; 22:05:55.986 [LIFECYCLE] [org.gradle.internal.buildevents.BuildResultLogger] ; 22:05:55.987 [LIFECYCLE] [org.gradle.internal.buildevents.BuildResultLogger] Total time: 29.153 secs; ```. ```; root# su - portage; portage$ cd /scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/; portage$ git lfs pull --include src/main/resources/large; No default remote. Errors logged to /scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git/lfs/objects/logs/20180420T221032.955218097.log; Use `git lfs logs last` to view the log.; portage$ cat /scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git/lfs/objects/logs/20180420T221032.955218097.log; git-lfs/2.3.4 (GitHub; linux amd64; go 1.10); git version 2.16.3. $ git-lfs pull --include src/main/resources/large; No default remote. No remotes defined. Current time in UTC: ; 2018-04-20 20:10:32. ENV:; LocalWorkingDir=/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999; LocalGitDir=/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git; LocalGitStorageDir=/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git; LocalMediaDir=/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git/lfs/objects; LocalReferenceDir=; TempDir=/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git/lfs/tmp; ConcurrentTransfers=3; TusTransfers=false; BasicTransfersOnly=false; SkipDownloadErrors=false; FetchRecentAlways=false; FetchRecentRefsDays=7; FetchRecentCommitsDays=0; FetchRecentRefsIncludeRemotes,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:15297,Testability,log,logs,15297,ptRunnerFactory.java:90); 22:05:55.986 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	... 58 more; 22:05:55.986 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.986 [LIFECYCLE] [org.gradle.internal.buildevents.BuildResultLogger] ; 22:05:55.986 [LIFECYCLE] [org.gradle.internal.buildevents.BuildResultLogger] BUILD FAILED; 22:05:55.986 [LIFECYCLE] [org.gradle.internal.buildevents.BuildResultLogger] ; 22:05:55.987 [LIFECYCLE] [org.gradle.internal.buildevents.BuildResultLogger] Total time: 29.153 secs; ```. ```; root# su - portage; portage$ cd /scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/; portage$ git lfs pull --include src/main/resources/large; No default remote. Errors logged to /scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git/lfs/objects/logs/20180420T221032.955218097.log; Use `git lfs logs last` to view the log.; portage$ cat /scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git/lfs/objects/logs/20180420T221032.955218097.log; git-lfs/2.3.4 (GitHub; linux amd64; go 1.10); git version 2.16.3. $ git-lfs pull --include src/main/resources/large; No default remote. No remotes defined. Current time in UTC: ; 2018-04-20 20:10:32. ENV:; LocalWorkingDir=/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999; LocalGitDir=/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git; LocalGitStorageDir=/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git; LocalMediaDir=/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git/lfs/objects; LocalReferenceDir=; TempDir=/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git/lfs/tmp; ConcurrentTransfers=3; TusTransfers=false; BasicTransfersOnly=false; SkipDownloadErrors=false; FetchRecentAlways=false; FetchRecentRefsDays=7; FetchRecentCommitsDays=0; FetchRecentRefsIncludeRemotes=true; PruneOffsetDays=3; PruneVerifyRemoteAlways=false; PruneRemoteName=origin; LfsStorageDir=/scratch/var/tmp/portage,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/issues/4687:15328,Testability,log,log,15328,86 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	... 58 more; 22:05:55.986 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.986 [LIFECYCLE] [org.gradle.internal.buildevents.BuildResultLogger] ; 22:05:55.986 [LIFECYCLE] [org.gradle.internal.buildevents.BuildResultLogger] BUILD FAILED; 22:05:55.986 [LIFECYCLE] [org.gradle.internal.buildevents.BuildResultLogger] ; 22:05:55.987 [LIFECYCLE] [org.gradle.internal.buildevents.BuildResultLogger] Total time: 29.153 secs; ```. ```; root# su - portage; portage$ cd /scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/; portage$ git lfs pull --include src/main/resources/large; No default remote. Errors logged to /scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git/lfs/objects/logs/20180420T221032.955218097.log; Use `git lfs logs last` to view the log.; portage$ cat /scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git/lfs/objects/logs/20180420T221032.955218097.log; git-lfs/2.3.4 (GitHub; linux amd64; go 1.10); git version 2.16.3. $ git-lfs pull --include src/main/resources/large; No default remote. No remotes defined. Current time in UTC: ; 2018-04-20 20:10:32. ENV:; LocalWorkingDir=/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999; LocalGitDir=/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git; LocalGitStorageDir=/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git; LocalMediaDir=/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git/lfs/objects; LocalReferenceDir=; TempDir=/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git/lfs/tmp; ConcurrentTransfers=3; TusTransfers=false; BasicTransfersOnly=false; SkipDownloadErrors=false; FetchRecentAlways=false; FetchRecentRefsDays=7; FetchRecentCommitsDays=0; FetchRecentRefsIncludeRemotes=true; PruneOffsetDays=3; PruneVerifyRemoteAlways=false; PruneRemoteName=origin; LfsStorageDir=/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-999,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687
https://github.com/broadinstitute/gatk/pull/4688:68,Testability,test,test,68,Added a quick-and-dirty temporary port of RepeatLengthCovariate; to test this functionality. Resolves #4568,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4688
https://github.com/broadinstitute/gatk/issues/4689:23,Security,validat,validation,23,"From our normal-normal validation. Some irrelevant annotations removed. ```; 3	124464215	.	ATTT	A,ATTTT	.	PASS	N_ART_LOD=-1.573e+00,7.21;RPA=15,12,16;RU=T;STR;TLOD=5.65,4.81	GT:AD:AF 0/1/2:91,12,6:0.183,0.174 0/0:100,13,8:0.166,0.179; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4689
https://github.com/broadinstitute/gatk/pull/4690:112,Integrability,contract,contraction,112,@takutosato The first commit gives tumor-normal the old default AF. The second commit is a more intelligent STR contraction filter that improves both sensitivity and precision.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4690
https://github.com/broadinstitute/gatk/issues/4694:243,Availability,error,error,243,I have been trying to run the tutorial ( https://gatkforums.broadinstitute.org/gatk/discussion/10913/how-to-run-the-pathseq-pipeline ). When running without the --spark-master the turoail runs smoothly. Bu twhen I try my spark master I get an error. I downloaded SPARK 2.2.0 with hadoop 2.7.3; Java is 1.8.0_131; I set the java classpath (I think correctly); I am aware of this thread: https://github.com/broadinstitute/gatk/issues/3050. But noentheless I cannot get the error to solve. I tried to copy the jar files:; hbase-client-1.4.3.jar; hbase-common-1.4.3.jar; hbase-hadoop2-compat-1.4.3.jar; hbase-protocol-1.4.3.jar; hbase-server-1.4.3.jar; To my spark jar folder. Shall I do smething else? I am also a SPARK newbie. Thank you very much!. ***************** Here is the error log:. ../../../gatk PathSeqPipelineSpark --spark-master spark://xx.xx.xx.xx:7077 --input test_sample.bam --filter-bwa-image hg19mini.fasta.img --kmer-file hg19mini.hss --min-clipped-read-length 70 --microbe-fasta e_coli_k12.fasta --microbe-bwa-image e_coli_k12.fasta.img --conf [jars=~/bin/spark-2.2.0-bin-hadoop2.7/jars/hbase-client-1.4.3.jar] --taxonomy-file e_coli_k12.db --output output.pathseq.bam --verbosity DEBUG --scores-output output.pathseq.txt; Using GATK jar /scratch/home/int/eva/zorzan/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /scratch/home/int/eva/zorzan/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar PathSeqPipelineSpark --spark-master spark://xx.xx.xx.xx:7077 --input test_sample.bam --filter-bwa-image hg19mini.fasta.img --kmer-file hg19mini.hss --min-clipped-read-length 70 --microbe-fasta e_coli_k12.fasta --microbe-bwa-image e_coli_k12.fasta.img --conf [jars=~/bin/spark-2.2.0-bin-hadoop2.7/jars/hbase-client-1.4.3.jar] --taxonomy-file e_coli_k12.db --output output.pathseq.bam --verbosity DEBUG --scores-output,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694
https://github.com/broadinstitute/gatk/issues/4694:252,Availability,down,downloaded,252,I have been trying to run the tutorial ( https://gatkforums.broadinstitute.org/gatk/discussion/10913/how-to-run-the-pathseq-pipeline ). When running without the --spark-master the turoail runs smoothly. Bu twhen I try my spark master I get an error. I downloaded SPARK 2.2.0 with hadoop 2.7.3; Java is 1.8.0_131; I set the java classpath (I think correctly); I am aware of this thread: https://github.com/broadinstitute/gatk/issues/3050. But noentheless I cannot get the error to solve. I tried to copy the jar files:; hbase-client-1.4.3.jar; hbase-common-1.4.3.jar; hbase-hadoop2-compat-1.4.3.jar; hbase-protocol-1.4.3.jar; hbase-server-1.4.3.jar; To my spark jar folder. Shall I do smething else? I am also a SPARK newbie. Thank you very much!. ***************** Here is the error log:. ../../../gatk PathSeqPipelineSpark --spark-master spark://xx.xx.xx.xx:7077 --input test_sample.bam --filter-bwa-image hg19mini.fasta.img --kmer-file hg19mini.hss --min-clipped-read-length 70 --microbe-fasta e_coli_k12.fasta --microbe-bwa-image e_coli_k12.fasta.img --conf [jars=~/bin/spark-2.2.0-bin-hadoop2.7/jars/hbase-client-1.4.3.jar] --taxonomy-file e_coli_k12.db --output output.pathseq.bam --verbosity DEBUG --scores-output output.pathseq.txt; Using GATK jar /scratch/home/int/eva/zorzan/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /scratch/home/int/eva/zorzan/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar PathSeqPipelineSpark --spark-master spark://xx.xx.xx.xx:7077 --input test_sample.bam --filter-bwa-image hg19mini.fasta.img --kmer-file hg19mini.hss --min-clipped-read-length 70 --microbe-fasta e_coli_k12.fasta --microbe-bwa-image e_coli_k12.fasta.img --conf [jars=~/bin/spark-2.2.0-bin-hadoop2.7/jars/hbase-client-1.4.3.jar] --taxonomy-file e_coli_k12.db --output output.pathseq.bam --verbosity DEBUG --scores-output,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694
https://github.com/broadinstitute/gatk/issues/4694:471,Availability,error,error,471,I have been trying to run the tutorial ( https://gatkforums.broadinstitute.org/gatk/discussion/10913/how-to-run-the-pathseq-pipeline ). When running without the --spark-master the turoail runs smoothly. Bu twhen I try my spark master I get an error. I downloaded SPARK 2.2.0 with hadoop 2.7.3; Java is 1.8.0_131; I set the java classpath (I think correctly); I am aware of this thread: https://github.com/broadinstitute/gatk/issues/3050. But noentheless I cannot get the error to solve. I tried to copy the jar files:; hbase-client-1.4.3.jar; hbase-common-1.4.3.jar; hbase-hadoop2-compat-1.4.3.jar; hbase-protocol-1.4.3.jar; hbase-server-1.4.3.jar; To my spark jar folder. Shall I do smething else? I am also a SPARK newbie. Thank you very much!. ***************** Here is the error log:. ../../../gatk PathSeqPipelineSpark --spark-master spark://xx.xx.xx.xx:7077 --input test_sample.bam --filter-bwa-image hg19mini.fasta.img --kmer-file hg19mini.hss --min-clipped-read-length 70 --microbe-fasta e_coli_k12.fasta --microbe-bwa-image e_coli_k12.fasta.img --conf [jars=~/bin/spark-2.2.0-bin-hadoop2.7/jars/hbase-client-1.4.3.jar] --taxonomy-file e_coli_k12.db --output output.pathseq.bam --verbosity DEBUG --scores-output output.pathseq.txt; Using GATK jar /scratch/home/int/eva/zorzan/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /scratch/home/int/eva/zorzan/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar PathSeqPipelineSpark --spark-master spark://xx.xx.xx.xx:7077 --input test_sample.bam --filter-bwa-image hg19mini.fasta.img --kmer-file hg19mini.hss --min-clipped-read-length 70 --microbe-fasta e_coli_k12.fasta --microbe-bwa-image e_coli_k12.fasta.img --conf [jars=~/bin/spark-2.2.0-bin-hadoop2.7/jars/hbase-client-1.4.3.jar] --taxonomy-file e_coli_k12.db --output output.pathseq.bam --verbosity DEBUG --scores-output,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694
https://github.com/broadinstitute/gatk/issues/4694:777,Availability,error,error,777,I have been trying to run the tutorial ( https://gatkforums.broadinstitute.org/gatk/discussion/10913/how-to-run-the-pathseq-pipeline ). When running without the --spark-master the turoail runs smoothly. Bu twhen I try my spark master I get an error. I downloaded SPARK 2.2.0 with hadoop 2.7.3; Java is 1.8.0_131; I set the java classpath (I think correctly); I am aware of this thread: https://github.com/broadinstitute/gatk/issues/3050. But noentheless I cannot get the error to solve. I tried to copy the jar files:; hbase-client-1.4.3.jar; hbase-common-1.4.3.jar; hbase-hadoop2-compat-1.4.3.jar; hbase-protocol-1.4.3.jar; hbase-server-1.4.3.jar; To my spark jar folder. Shall I do smething else? I am also a SPARK newbie. Thank you very much!. ***************** Here is the error log:. ../../../gatk PathSeqPipelineSpark --spark-master spark://xx.xx.xx.xx:7077 --input test_sample.bam --filter-bwa-image hg19mini.fasta.img --kmer-file hg19mini.hss --min-clipped-read-length 70 --microbe-fasta e_coli_k12.fasta --microbe-bwa-image e_coli_k12.fasta.img --conf [jars=~/bin/spark-2.2.0-bin-hadoop2.7/jars/hbase-client-1.4.3.jar] --taxonomy-file e_coli_k12.db --output output.pathseq.bam --verbosity DEBUG --scores-output output.pathseq.txt; Using GATK jar /scratch/home/int/eva/zorzan/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /scratch/home/int/eva/zorzan/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar PathSeqPipelineSpark --spark-master spark://xx.xx.xx.xx:7077 --input test_sample.bam --filter-bwa-image hg19mini.fasta.img --kmer-file hg19mini.hss --min-clipped-read-length 70 --microbe-fasta e_coli_k12.fasta --microbe-bwa-image e_coli_k12.fasta.img --conf [jars=~/bin/spark-2.2.0-bin-hadoop2.7/jars/hbase-client-1.4.3.jar] --taxonomy-file e_coli_k12.db --output output.pathseq.bam --verbosity DEBUG --scores-output,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694
https://github.com/broadinstitute/gatk/issues/4694:14877,Availability,ERROR,ERROR,14877,"xecutor.java:617); at java.lang.Thread.run(Thread.java:748). 18/04/23 20:42:02 INFO TaskSetManager: Starting task 0.1 in stage 0.0 (TID 1, xx.xx.xx.xx, executor 0, partition 0, PROCESS_LOCAL, 4956 bytes); 18/04/23 20:42:02 INFO TaskSetManager: Lost task 0.1 in stage 0.0 (TID 1) on xx.xx.xx.xx, executor 0: java.lang.IllegalStateException (unread block data) [duplicate 1]; 18/04/23 20:42:02 INFO TaskSetManager: Starting task 0.2 in stage 0.0 (TID 2, xx.xx.xx.xx, executor 0, partition 0, PROCESS_LOCAL, 4956 bytes); 18/04/23 20:42:02 INFO TaskSetManager: Lost task 0.2 in stage 0.0 (TID 2) on xx.xx.xx.xx, executor 0: java.lang.IllegalStateException (unread block data) [duplicate 2]; 18/04/23 20:42:02 INFO TaskSetManager: Starting task 0.3 in stage 0.0 (TID 3, xx.xx.xx.xx, executor 0, partition 0, PROCESS_LOCAL, 4956 bytes); 18/04/23 20:42:02 INFO TaskSetManager: Lost task 0.3 in stage 0.0 (TID 3) on xx.xx.xx.xx, executor 0: java.lang.IllegalStateException (unread block data) [duplicate 3]; 18/04/23 20:42:02 ERROR TaskSetManager: Task 0 in stage 0.0 failed 4 times; aborting job; 18/04/23 20:42:02 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool; 18/04/23 20:42:02 INFO TaskSchedulerImpl: Cancelling stage 0; 18/04/23 20:42:02 INFO DAGScheduler: ResultStage 0 (first at ReadsSparkSource.java:221) failed in 11.519 s due to Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3, xx.xx.xx.xx, executor 0): java.lang.IllegalStateException: unread block data; at java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(ObjectInputStream.java:2740); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1567); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027); at java.io.ObjectInputStream",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694
https://github.com/broadinstitute/gatk/issues/4694:15256,Availability,failure,failure,15256," INFO TaskSetManager: Starting task 0.2 in stage 0.0 (TID 2, xx.xx.xx.xx, executor 0, partition 0, PROCESS_LOCAL, 4956 bytes); 18/04/23 20:42:02 INFO TaskSetManager: Lost task 0.2 in stage 0.0 (TID 2) on xx.xx.xx.xx, executor 0: java.lang.IllegalStateException (unread block data) [duplicate 2]; 18/04/23 20:42:02 INFO TaskSetManager: Starting task 0.3 in stage 0.0 (TID 3, xx.xx.xx.xx, executor 0, partition 0, PROCESS_LOCAL, 4956 bytes); 18/04/23 20:42:02 INFO TaskSetManager: Lost task 0.3 in stage 0.0 (TID 3) on xx.xx.xx.xx, executor 0: java.lang.IllegalStateException (unread block data) [duplicate 3]; 18/04/23 20:42:02 ERROR TaskSetManager: Task 0 in stage 0.0 failed 4 times; aborting job; 18/04/23 20:42:02 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool; 18/04/23 20:42:02 INFO TaskSchedulerImpl: Cancelling stage 0; 18/04/23 20:42:02 INFO DAGScheduler: ResultStage 0 (first at ReadsSparkSource.java:221) failed in 11.519 s due to Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3, xx.xx.xx.xx, executor 0): java.lang.IllegalStateException: unread block data; at java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(ObjectInputStream.java:2740); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1567); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422); at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:309); at java.util.concu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694
https://github.com/broadinstitute/gatk/issues/4694:15313,Availability,failure,failure,15313,"2, xx.xx.xx.xx, executor 0, partition 0, PROCESS_LOCAL, 4956 bytes); 18/04/23 20:42:02 INFO TaskSetManager: Lost task 0.2 in stage 0.0 (TID 2) on xx.xx.xx.xx, executor 0: java.lang.IllegalStateException (unread block data) [duplicate 2]; 18/04/23 20:42:02 INFO TaskSetManager: Starting task 0.3 in stage 0.0 (TID 3, xx.xx.xx.xx, executor 0, partition 0, PROCESS_LOCAL, 4956 bytes); 18/04/23 20:42:02 INFO TaskSetManager: Lost task 0.3 in stage 0.0 (TID 3) on xx.xx.xx.xx, executor 0: java.lang.IllegalStateException (unread block data) [duplicate 3]; 18/04/23 20:42:02 ERROR TaskSetManager: Task 0 in stage 0.0 failed 4 times; aborting job; 18/04/23 20:42:02 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool; 18/04/23 20:42:02 INFO TaskSchedulerImpl: Cancelling stage 0; 18/04/23 20:42:02 INFO DAGScheduler: ResultStage 0 (first at ReadsSparkSource.java:221) failed in 11.519 s due to Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3, xx.xx.xx.xx, executor 0): java.lang.IllegalStateException: unread block data; at java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(ObjectInputStream.java:2740); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1567); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422); at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:309); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694
https://github.com/broadinstitute/gatk/issues/4694:16709,Availability,down,down,16709,"org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:309); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 18/04/23 20:42:02 INFO DAGScheduler: Job 0 failed: first at ReadsSparkSource.java:221, took 11.814317 s; 18/04/23 20:42:02 INFO SparkUI: Stopped Spark web UI at http://xx.xx.xx.xx:4040; 18/04/23 20:42:02 INFO StandaloneSchedulerBackend: Shutting down all executors; 18/04/23 20:42:02 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down; 18/04/23 20:42:03 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 18/04/23 20:42:03 INFO MemoryStore: MemoryStore cleared; 18/04/23 20:42:03 INFO BlockManager: BlockManager stopped; 18/04/23 20:42:03 INFO BlockManagerMaster: BlockManagerMaster stopped; 18/04/23 20:42:03 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/04/23 20:42:03 INFO SparkContext: Successfully stopped SparkContext; 20:42:03.045 INFO PathSeqPipelineSpark - Shutting down engine; [April 23, 2018 8:42:03 PM CEST] org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark done. Elapsed time: 0.43 minutes.; Runtime.totalMemory()=793247744; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3, xx.xx.xx.xx, executor 0): java.lang.IllegalStateException: unread block data; at java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(ObjectInputStream.java:2740); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1567); at java.io.ObjectInputStream.defau",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694
https://github.com/broadinstitute/gatk/issues/4694:16827,Availability,down,down,16827,"org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:309); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 18/04/23 20:42:02 INFO DAGScheduler: Job 0 failed: first at ReadsSparkSource.java:221, took 11.814317 s; 18/04/23 20:42:02 INFO SparkUI: Stopped Spark web UI at http://xx.xx.xx.xx:4040; 18/04/23 20:42:02 INFO StandaloneSchedulerBackend: Shutting down all executors; 18/04/23 20:42:02 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down; 18/04/23 20:42:03 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 18/04/23 20:42:03 INFO MemoryStore: MemoryStore cleared; 18/04/23 20:42:03 INFO BlockManager: BlockManager stopped; 18/04/23 20:42:03 INFO BlockManagerMaster: BlockManagerMaster stopped; 18/04/23 20:42:03 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/04/23 20:42:03 INFO SparkContext: Successfully stopped SparkContext; 20:42:03.045 INFO PathSeqPipelineSpark - Shutting down engine; [April 23, 2018 8:42:03 PM CEST] org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark done. Elapsed time: 0.43 minutes.; Runtime.totalMemory()=793247744; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3, xx.xx.xx.xx, executor 0): java.lang.IllegalStateException: unread block data; at java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(ObjectInputStream.java:2740); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1567); at java.io.ObjectInputStream.defau",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694
https://github.com/broadinstitute/gatk/issues/4694:17352,Availability,down,down,17352,"tor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 18/04/23 20:42:02 INFO DAGScheduler: Job 0 failed: first at ReadsSparkSource.java:221, took 11.814317 s; 18/04/23 20:42:02 INFO SparkUI: Stopped Spark web UI at http://xx.xx.xx.xx:4040; 18/04/23 20:42:02 INFO StandaloneSchedulerBackend: Shutting down all executors; 18/04/23 20:42:02 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down; 18/04/23 20:42:03 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 18/04/23 20:42:03 INFO MemoryStore: MemoryStore cleared; 18/04/23 20:42:03 INFO BlockManager: BlockManager stopped; 18/04/23 20:42:03 INFO BlockManagerMaster: BlockManagerMaster stopped; 18/04/23 20:42:03 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/04/23 20:42:03 INFO SparkContext: Successfully stopped SparkContext; 20:42:03.045 INFO PathSeqPipelineSpark - Shutting down engine; [April 23, 2018 8:42:03 PM CEST] org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark done. Elapsed time: 0.43 minutes.; Runtime.totalMemory()=793247744; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3, xx.xx.xx.xx, executor 0): java.lang.IllegalStateException: unread block data; at java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(ObjectInputStream.java:2740); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1567); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422); at org.apache.spark.serializer.JavaDeserializa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694
https://github.com/broadinstitute/gatk/issues/4694:17595,Availability,failure,failure,17595,"FO SparkUI: Stopped Spark web UI at http://xx.xx.xx.xx:4040; 18/04/23 20:42:02 INFO StandaloneSchedulerBackend: Shutting down all executors; 18/04/23 20:42:02 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down; 18/04/23 20:42:03 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 18/04/23 20:42:03 INFO MemoryStore: MemoryStore cleared; 18/04/23 20:42:03 INFO BlockManager: BlockManager stopped; 18/04/23 20:42:03 INFO BlockManagerMaster: BlockManagerMaster stopped; 18/04/23 20:42:03 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/04/23 20:42:03 INFO SparkContext: Successfully stopped SparkContext; 20:42:03.045 INFO PathSeqPipelineSpark - Shutting down engine; [April 23, 2018 8:42:03 PM CEST] org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark done. Elapsed time: 0.43 minutes.; Runtime.totalMemory()=793247744; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3, xx.xx.xx.xx, executor 0): java.lang.IllegalStateException: unread block data; at java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(ObjectInputStream.java:2740); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1567); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422); at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:309); at java.util.con",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694
https://github.com/broadinstitute/gatk/issues/4694:17652,Availability,failure,failure,17652,"; 18/04/23 20:42:02 INFO StandaloneSchedulerBackend: Shutting down all executors; 18/04/23 20:42:02 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down; 18/04/23 20:42:03 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 18/04/23 20:42:03 INFO MemoryStore: MemoryStore cleared; 18/04/23 20:42:03 INFO BlockManager: BlockManager stopped; 18/04/23 20:42:03 INFO BlockManagerMaster: BlockManagerMaster stopped; 18/04/23 20:42:03 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/04/23 20:42:03 INFO SparkContext: Successfully stopped SparkContext; 20:42:03.045 INFO PathSeqPipelineSpark - Shutting down engine; [April 23, 2018 8:42:03 PM CEST] org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark done. Elapsed time: 0.43 minutes.; Runtime.totalMemory()=793247744; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3, xx.xx.xx.xx, executor 0): java.lang.IllegalStateException: unread block data; at java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(ObjectInputStream.java:2740); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1567); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422); at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:309); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694
https://github.com/broadinstitute/gatk/issues/4694:124,Deployability,pipeline,pipeline,124,I have been trying to run the tutorial ( https://gatkforums.broadinstitute.org/gatk/discussion/10913/how-to-run-the-pathseq-pipeline ). When running without the --spark-master the turoail runs smoothly. Bu twhen I try my spark master I get an error. I downloaded SPARK 2.2.0 with hadoop 2.7.3; Java is 1.8.0_131; I set the java classpath (I think correctly); I am aware of this thread: https://github.com/broadinstitute/gatk/issues/3050. But noentheless I cannot get the error to solve. I tried to copy the jar files:; hbase-client-1.4.3.jar; hbase-common-1.4.3.jar; hbase-hadoop2-compat-1.4.3.jar; hbase-protocol-1.4.3.jar; hbase-server-1.4.3.jar; To my spark jar folder. Shall I do smething else? I am also a SPARK newbie. Thank you very much!. ***************** Here is the error log:. ../../../gatk PathSeqPipelineSpark --spark-master spark://xx.xx.xx.xx:7077 --input test_sample.bam --filter-bwa-image hg19mini.fasta.img --kmer-file hg19mini.hss --min-clipped-read-length 70 --microbe-fasta e_coli_k12.fasta --microbe-bwa-image e_coli_k12.fasta.img --conf [jars=~/bin/spark-2.2.0-bin-hadoop2.7/jars/hbase-client-1.4.3.jar] --taxonomy-file e_coli_k12.db --output output.pathseq.bam --verbosity DEBUG --scores-output output.pathseq.txt; Using GATK jar /scratch/home/int/eva/zorzan/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /scratch/home/int/eva/zorzan/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar PathSeqPipelineSpark --spark-master spark://xx.xx.xx.xx:7077 --input test_sample.bam --filter-bwa-image hg19mini.fasta.img --kmer-file hg19mini.hss --min-clipped-read-length 70 --microbe-fasta e_coli_k12.fasta --microbe-bwa-image e_coli_k12.fasta.img --conf [jars=~/bin/spark-2.2.0-bin-hadoop2.7/jars/hbase-client-1.4.3.jar] --taxonomy-file e_coli_k12.db --output output.pathseq.bam --verbosity DEBUG --scores-output,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694
https://github.com/broadinstitute/gatk/issues/4694:4801,Deployability,Configurat,Configuration,4801,REATE_MD5 : false; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.CUSTOM_READER_FACTORY :; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.DISABLE_SNAPPY_COMPRESSOR : false; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.REFERENCE_FASTA : null; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 20:41:37.620 DEBUG ConfigFactory - Configuration file values:; 20:41:37.626 DEBUG ConfigFactory - gcsMaxRetries = 20; 20:41:37.626 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.compression_level = 2; 20:41:37.626 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 20:41:37.626 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 20:41:37.626 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 20:41:37.626 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 20:41:37.626 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 20:41:37.626 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - codec_pack,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694
https://github.com/broadinstitute/gatk/issues/4694:6350,Deployability,patch,patch,6350,"ConfigFactory - spark.driver.maxResultSize = 0; 20:41:37.626 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 20:41:37.626 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 20:41:37.626 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 20:41:37.626 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 20:41:37.627 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 20:41:37.627 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 20:41:37.627 DEBUG ConfigFactory - createOutputBamIndex = true; 20:41:37.627 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 20:41:37.627 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 20:41:37.627 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 20:41:37.627 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 20:41:37.627 INFO PathSeqPipelineSpark - Initializing engine; 20:41:37.627 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 18/04/23 20:41:38 INFO SparkContext: Running Spark version 2.2.0; 18/04/23 20:41:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18/04/23 20:41:38 INFO SparkContext: Submitted application: PathSeqPipelineSpark; 18/04/23 20:41:39 INFO SecurityManager: Changing view acls to: zorzan; 18/04/23 20:41:39 INFO SecurityManager: Changing modify acls to: zorzan; 18/04/23 20:41:39 INFO SecurityManager: Changing view acls groups to:; 18/04/23 20:41:39 INFO SecurityManager: Changing modify acls groups to:; 18/04/23 20:41:39 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694
https://github.com/broadinstitute/gatk/issues/4694:10393,Deployability,update,updated,10393,"y; 18/04/23 20:41:43 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, xx.xx.xx.xx, 36833, None); 18/04/23 20:41:43 INFO BlockManagerMasterEndpoint: Registering block manager xx.xx.xx.xx:36833 with 4.0 GB RAM, BlockManagerId(driver, xx.xx.xx.xx, 36833, None); 18/04/23 20:41:43 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, xx.xx.xx.xx, 36833, None); 18/04/23 20:41:43 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, xx.xx.xx.xx, 36833, None); 18/04/23 20:41:43 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0; 18/04/23 20:41:43 INFO GoogleHadoopFileSystemBase: GHFS version: 1.6.3-hadoop2; 18/04/23 20:41:46 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 329.7 KB, free 4.0 GB); 18/04/23 20:41:46 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180423204143-0003/0 is now RUNNING; 00:09 DEBUG: [kryo] Write: SerializableConfiguration; 18/04/23 20:41:46 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KB, free 4.0 GB); 18/04/23 20:41:47 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on xx.xx.xx.xx:36833 (size: 27.5 KB, free: 4.0 GB); 18/04/23 20:41:47 INFO SparkContext: Created broadcast 0 from newAPIHadoopFile at ReadsSparkSource.java:113; 18/04/23 20:41:47 INFO FileInputFormat: Total input files to process : 1; 18/04/23 20:41:51 INFO SparkContext: Starting job: first at ReadsSparkSource.java:221; 18/04/23 20:41:51 INFO DAGScheduler: Got job 0 (first at ReadsSparkSource.java:221) with 1 output partitions; 18/04/23 20:41:51 INFO DAGScheduler: Final stage: ResultStage 0 (first at ReadsSparkSource.java:221); 18/04/23 20:41:51 INFO DAGScheduler: Parents of final stage: List(); 18/04/23 20:41:51 INFO DAGScheduler: Missing parents: List(); 18/04/23 20:41:51 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at filter at ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694
https://github.com/broadinstitute/gatk/issues/4694:10025,Energy Efficiency,Schedul,SchedulerBackend,10025,"0423204143-0003/0 on hostPort xx.xx.xx.xx:59994 with 16 cores, 1024.0 MB RAM; 18/04/23 20:41:43 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36833.; 18/04/23 20:41:43 INFO NettyBlockTransferService: Server created on xx.xx.xx.xx:36833; 18/04/23 20:41:43 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy; 18/04/23 20:41:43 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, xx.xx.xx.xx, 36833, None); 18/04/23 20:41:43 INFO BlockManagerMasterEndpoint: Registering block manager xx.xx.xx.xx:36833 with 4.0 GB RAM, BlockManagerId(driver, xx.xx.xx.xx, 36833, None); 18/04/23 20:41:43 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, xx.xx.xx.xx, 36833, None); 18/04/23 20:41:43 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, xx.xx.xx.xx, 36833, None); 18/04/23 20:41:43 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0; 18/04/23 20:41:43 INFO GoogleHadoopFileSystemBase: GHFS version: 1.6.3-hadoop2; 18/04/23 20:41:46 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 329.7 KB, free 4.0 GB); 18/04/23 20:41:46 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180423204143-0003/0 is now RUNNING; 00:09 DEBUG: [kryo] Write: SerializableConfiguration; 18/04/23 20:41:46 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KB, free 4.0 GB); 18/04/23 20:41:47 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on xx.xx.xx.xx:36833 (size: 27.5 KB, free: 4.0 GB); 18/04/23 20:41:47 INFO SparkContext: Created broadcast 0 from newAPIHadoopFile at ReadsSparkSource.java:113; 18/04/23 20:41:47 INFO FileInputFormat: Total input files to process : 1; 18/04/23 20:41:51 INFO SparkContext: Starting job: first at ReadsSparkSource.java:221; 18/04/23 20:41:51",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694
https://github.com/broadinstitute/gatk/issues/4694:10055,Energy Efficiency,schedul,scheduling,10055,"0423204143-0003/0 on hostPort xx.xx.xx.xx:59994 with 16 cores, 1024.0 MB RAM; 18/04/23 20:41:43 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36833.; 18/04/23 20:41:43 INFO NettyBlockTransferService: Server created on xx.xx.xx.xx:36833; 18/04/23 20:41:43 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy; 18/04/23 20:41:43 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, xx.xx.xx.xx, 36833, None); 18/04/23 20:41:43 INFO BlockManagerMasterEndpoint: Registering block manager xx.xx.xx.xx:36833 with 4.0 GB RAM, BlockManagerId(driver, xx.xx.xx.xx, 36833, None); 18/04/23 20:41:43 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, xx.xx.xx.xx, 36833, None); 18/04/23 20:41:43 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, xx.xx.xx.xx, 36833, None); 18/04/23 20:41:43 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0; 18/04/23 20:41:43 INFO GoogleHadoopFileSystemBase: GHFS version: 1.6.3-hadoop2; 18/04/23 20:41:46 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 329.7 KB, free 4.0 GB); 18/04/23 20:41:46 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180423204143-0003/0 is now RUNNING; 00:09 DEBUG: [kryo] Write: SerializableConfiguration; 18/04/23 20:41:46 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KB, free 4.0 GB); 18/04/23 20:41:47 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on xx.xx.xx.xx:36833 (size: 27.5 KB, free: 4.0 GB); 18/04/23 20:41:47 INFO SparkContext: Created broadcast 0 from newAPIHadoopFile at ReadsSparkSource.java:113; 18/04/23 20:41:47 INFO FileInputFormat: Total input files to process : 1; 18/04/23 20:41:51 INFO SparkContext: Starting job: first at ReadsSparkSource.java:221; 18/04/23 20:41:51",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694
https://github.com/broadinstitute/gatk/issues/4694:18822,Energy Efficiency,schedul,scheduler,18822,BlockDataMode(ObjectInputStream.java:2740); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1567); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422); at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:309); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.s,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694
https://github.com/broadinstitute/gatk/issues/4694:18862,Energy Efficiency,schedul,scheduler,18862,ctInputStream.readObject0(ObjectInputStream.java:1567); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422); at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:309); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSchedule,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694
https://github.com/broadinstitute/gatk/issues/4694:18960,Energy Efficiency,schedul,scheduler,18960,tream.defaultReadFields(ObjectInputStream.java:2245); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422); at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:309); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onRec,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694
https://github.com/broadinstitute/gatk/issues/4694:19057,Energy Efficiency,schedul,scheduler,19057,(ObjectInputStream.java:2169); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422); at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:309); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694
https://github.com/broadinstitute/gatk/issues/4694:19308,Energy Efficiency,schedul,scheduler,19308, org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:309); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); at org.apache.spark.SparkContext.runJob(Sp,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694
https://github.com/broadinstitute/gatk/issues/4694:19388,Energy Efficiency,schedul,scheduler,19388,.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:309); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); at org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.sc,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694
https://github.com/broadinstitute/gatk/issues/4694:19493,Energy Efficiency,schedul,scheduler,19493,at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:309); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); at org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1354); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apac,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694
https://github.com/broadinstitute/gatk/issues/4694:19641,Energy Efficiency,schedul,scheduler,19641,va:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); at org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1354); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rd,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694
https://github.com/broadinstitute/gatk/issues/4694:19729,Energy Efficiency,schedul,scheduler,19729,617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); at org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1354); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.take(RDD.scala:1327); at org.apache.spark.rdd.RDD$$anonfun$first$1.apply(RDD.scala,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694
https://github.com/broadinstitute/gatk/issues/4694:19826,Energy Efficiency,schedul,scheduler,19826,.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); at org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1354); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.take(RDD.scala:1327); at org.apache.spark.rdd.RDD$$anonfun$first$1.apply(RDD.scala:1368); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694
https://github.com/broadinstitute/gatk/issues/4694:19921,Energy Efficiency,schedul,scheduler,19921,.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); at org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1354); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.take(RDD.scala:1327); at org.apache.spark.rdd.RDD$$anonfun$first$1.apply(RDD.scala:1368); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spar,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694
https://github.com/broadinstitute/gatk/issues/4694:20084,Energy Efficiency,schedul,scheduler,20084,abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); at org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1354); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.take(RDD.scala:1327); at org.apache.spark.rdd.RDD$$anonfun$first$1.apply(RDD.scala:1368); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.first(RDD.scala:1367); at org.apache.spark.api.java.JavaRDDLike$class.first(JavaRDDLike.scala:538);,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694
https://github.com/broadinstitute/gatk/issues/4694:605,Integrability,protocol,protocol-,605,I have been trying to run the tutorial ( https://gatkforums.broadinstitute.org/gatk/discussion/10913/how-to-run-the-pathseq-pipeline ). When running without the --spark-master the turoail runs smoothly. Bu twhen I try my spark master I get an error. I downloaded SPARK 2.2.0 with hadoop 2.7.3; Java is 1.8.0_131; I set the java classpath (I think correctly); I am aware of this thread: https://github.com/broadinstitute/gatk/issues/3050. But noentheless I cannot get the error to solve. I tried to copy the jar files:; hbase-client-1.4.3.jar; hbase-common-1.4.3.jar; hbase-hadoop2-compat-1.4.3.jar; hbase-protocol-1.4.3.jar; hbase-server-1.4.3.jar; To my spark jar folder. Shall I do smething else? I am also a SPARK newbie. Thank you very much!. ***************** Here is the error log:. ../../../gatk PathSeqPipelineSpark --spark-master spark://xx.xx.xx.xx:7077 --input test_sample.bam --filter-bwa-image hg19mini.fasta.img --kmer-file hg19mini.hss --min-clipped-read-length 70 --microbe-fasta e_coli_k12.fasta --microbe-bwa-image e_coli_k12.fasta.img --conf [jars=~/bin/spark-2.2.0-bin-hadoop2.7/jars/hbase-client-1.4.3.jar] --taxonomy-file e_coli_k12.db --output output.pathseq.bam --verbosity DEBUG --scores-output output.pathseq.txt; Using GATK jar /scratch/home/int/eva/zorzan/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /scratch/home/int/eva/zorzan/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar PathSeqPipelineSpark --spark-master spark://xx.xx.xx.xx:7077 --input test_sample.bam --filter-bwa-image hg19mini.fasta.img --kmer-file hg19mini.hss --min-clipped-read-length 70 --microbe-fasta e_coli_k12.fasta --microbe-bwa-image e_coli_k12.fasta.img --conf [jars=~/bin/spark-2.2.0-bin-hadoop2.7/jars/hbase-client-1.4.3.jar] --taxonomy-file e_coli_k12.db --output output.pathseq.bam --verbosity DEBUG --scores-output,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694
https://github.com/broadinstitute/gatk/issues/4694:2074,Modifiability,variab,variables,2074,taxonomy-file e_coli_k12.db --output output.pathseq.bam --verbosity DEBUG --scores-output output.pathseq.txt; Using GATK jar /scratch/home/int/eva/zorzan/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /scratch/home/int/eva/zorzan/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar PathSeqPipelineSpark --spark-master spark://xx.xx.xx.xx:7077 --input test_sample.bam --filter-bwa-image hg19mini.fasta.img --kmer-file hg19mini.hss --min-clipped-read-length 70 --microbe-fasta e_coli_k12.fasta --microbe-bwa-image e_coli_k12.fasta.img --conf [jars=~/bin/spark-2.2.0-bin-hadoop2.7/jars/hbase-client-1.4.3.jar] --taxonomy-file e_coli_k12.db --output output.pathseq.bam --verbosity DEBUG --scores-output output.pathseq.txt; 20:41:36.853 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 20:41:37.246 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/scratch/home/int/eva/zorzan/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 20:41:37.277 DEBUG NativeLibraryLoader - Extracting libgkl_compression.so to /tmp/zorzan/libgkl_compression6179723182683465083.so; 20:41:37.613 INFO PathSeqPipelineSpark - ------------------------------------------------------------; 20:41:37.613 INFO PathSeqPipelineSpark - The Genome Analysis Toolkit (GATK) v4.0.3.0; 20:41:37.613 INFO PathSeqPipelineSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 20:41:37.614 INFO PathSeqPipelineSpark - Executing as zorzan@node016 on Linux v2.6.32-220.4.1.el6.x86_64 amd64; 20:41:37.614 INFO PathSeqPipelineSpark - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_131-b11; 20:41:37.614 INFO PathSeqPipelineSpark - Start ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694
https://github.com/broadinstitute/gatk/issues/4694:2200,Modifiability,config,configured,2200,taxonomy-file e_coli_k12.db --output output.pathseq.bam --verbosity DEBUG --scores-output output.pathseq.txt; Using GATK jar /scratch/home/int/eva/zorzan/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /scratch/home/int/eva/zorzan/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar PathSeqPipelineSpark --spark-master spark://xx.xx.xx.xx:7077 --input test_sample.bam --filter-bwa-image hg19mini.fasta.img --kmer-file hg19mini.hss --min-clipped-read-length 70 --microbe-fasta e_coli_k12.fasta --microbe-bwa-image e_coli_k12.fasta.img --conf [jars=~/bin/spark-2.2.0-bin-hadoop2.7/jars/hbase-client-1.4.3.jar] --taxonomy-file e_coli_k12.db --output output.pathseq.bam --verbosity DEBUG --scores-output output.pathseq.txt; 20:41:36.853 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 20:41:37.246 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/scratch/home/int/eva/zorzan/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 20:41:37.277 DEBUG NativeLibraryLoader - Extracting libgkl_compression.so to /tmp/zorzan/libgkl_compression6179723182683465083.so; 20:41:37.613 INFO PathSeqPipelineSpark - ------------------------------------------------------------; 20:41:37.613 INFO PathSeqPipelineSpark - The Genome Analysis Toolkit (GATK) v4.0.3.0; 20:41:37.613 INFO PathSeqPipelineSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 20:41:37.614 INFO PathSeqPipelineSpark - Executing as zorzan@node016 on Linux v2.6.32-220.4.1.el6.x86_64 amd64; 20:41:37.614 INFO PathSeqPipelineSpark - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_131-b11; 20:41:37.614 INFO PathSeqPipelineSpark - Start ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694
https://github.com/broadinstitute/gatk/issues/4694:4785,Modifiability,Config,ConfigFactory,4785,REATE_MD5 : false; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.CUSTOM_READER_FACTORY :; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.DISABLE_SNAPPY_COMPRESSOR : false; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.REFERENCE_FASTA : null; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 20:41:37.620 DEBUG ConfigFactory - Configuration file values:; 20:41:37.626 DEBUG ConfigFactory - gcsMaxRetries = 20; 20:41:37.626 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.compression_level = 2; 20:41:37.626 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 20:41:37.626 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 20:41:37.626 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 20:41:37.626 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 20:41:37.626 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 20:41:37.626 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - codec_pack,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694
https://github.com/broadinstitute/gatk/issues/4694:4801,Modifiability,Config,Configuration,4801,REATE_MD5 : false; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.CUSTOM_READER_FACTORY :; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.DISABLE_SNAPPY_COMPRESSOR : false; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.REFERENCE_FASTA : null; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 20:41:37.620 DEBUG ConfigFactory - Configuration file values:; 20:41:37.626 DEBUG ConfigFactory - gcsMaxRetries = 20; 20:41:37.626 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.compression_level = 2; 20:41:37.626 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 20:41:37.626 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 20:41:37.626 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 20:41:37.626 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 20:41:37.626 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 20:41:37.626 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - codec_pack,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694
https://github.com/broadinstitute/gatk/issues/4694:4848,Modifiability,Config,ConfigFactory,4848," HTSJDK Defaults.CUSTOM_READER_FACTORY :; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.DISABLE_SNAPPY_COMPRESSOR : false; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.REFERENCE_FASTA : null; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 20:41:37.620 DEBUG ConfigFactory - Configuration file values:; 20:41:37.626 DEBUG ConfigFactory - gcsMaxRetries = 20; 20:41:37.626 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.compression_level = 2; 20:41:37.626 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 20:41:37.626 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 20:41:37.626 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 20:41:37.626 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 20:41:37.626 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 20:41:37.626 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694
https://github.com/broadinstitute/gatk/issues/4694:4903,Modifiability,Config,ConfigFactory,4903,"PipelineSpark - HTSJDK Defaults.DISABLE_SNAPPY_COMPRESSOR : false; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.REFERENCE_FASTA : null; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 20:41:37.620 DEBUG ConfigFactory - Configuration file values:; 20:41:37.626 DEBUG ConfigFactory - gcsMaxRetries = 20; 20:41:37.626 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.compression_level = 2; 20:41:37.626 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 20:41:37.626 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 20:41:37.626 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 20:41:37.626 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 20:41:37.626 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 20:41:37.626 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 20:41:37.627 DEBUG ConfigFactory - cloud",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694
https://github.com/broadinstitute/gatk/issues/4694:4981,Modifiability,Config,ConfigFactory,4981,"OR : false; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.REFERENCE_FASTA : null; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 20:41:37.620 DEBUG ConfigFactory - Configuration file values:; 20:41:37.626 DEBUG ConfigFactory - gcsMaxRetries = 20; 20:41:37.626 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.compression_level = 2; 20:41:37.626 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 20:41:37.626 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 20:41:37.626 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 20:41:37.626 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 20:41:37.626 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 20:41:37.626 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 20:41:37.627 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 20:41:37.627 DEBUG ConfigFactory - ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694
https://github.com/broadinstitute/gatk/issues/4694:5059,Modifiability,Config,ConfigFactory,5059,"ENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.REFERENCE_FASTA : null; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 20:41:37.620 DEBUG ConfigFactory - Configuration file values:; 20:41:37.626 DEBUG ConfigFactory - gcsMaxRetries = 20; 20:41:37.626 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.compression_level = 2; 20:41:37.626 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 20:41:37.626 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 20:41:37.626 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 20:41:37.626 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 20:41:37.626 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 20:41:37.626 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 20:41:37.627 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 20:41:37.627 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 20:41:37.627 DEBUG ConfigFactory - createOutput",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694
https://github.com/broadinstitute/gatk/issues/4694:5137,Modifiability,Config,ConfigFactory,5137,"FO PathSeqPipelineSpark - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.REFERENCE_FASTA : null; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 20:41:37.620 DEBUG ConfigFactory - Configuration file values:; 20:41:37.626 DEBUG ConfigFactory - gcsMaxRetries = 20; 20:41:37.626 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.compression_level = 2; 20:41:37.626 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 20:41:37.626 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 20:41:37.626 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 20:41:37.626 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 20:41:37.626 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 20:41:37.626 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 20:41:37.627 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 20:41:37.627 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 20:41:37.627 DEBUG ConfigFactory - createOutputBamIndex = true; 20:41:37.627 INFO PathSeqPipelineSpark - Deflater: IntelDefla",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694
https://github.com/broadinstitute/gatk/issues/4694:5215,Modifiability,Config,ConfigFactory,5215,":37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.REFERENCE_FASTA : null; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 20:41:37.620 DEBUG ConfigFactory - Configuration file values:; 20:41:37.626 DEBUG ConfigFactory - gcsMaxRetries = 20; 20:41:37.626 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.compression_level = 2; 20:41:37.626 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 20:41:37.626 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 20:41:37.626 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 20:41:37.626 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 20:41:37.626 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 20:41:37.626 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 20:41:37.627 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 20:41:37.627 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 20:41:37.627 DEBUG ConfigFactory - createOutputBamIndex = true; 20:41:37.627 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 20:41:37.627 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 20:41:3",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694
https://github.com/broadinstitute/gatk/issues/4694:5280,Modifiability,Config,ConfigFactory,5280,"TA : null; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 20:41:37.620 DEBUG ConfigFactory - Configuration file values:; 20:41:37.626 DEBUG ConfigFactory - gcsMaxRetries = 20; 20:41:37.626 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.compression_level = 2; 20:41:37.626 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 20:41:37.626 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 20:41:37.626 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 20:41:37.626 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 20:41:37.626 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 20:41:37.626 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 20:41:37.627 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 20:41:37.627 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 20:41:37.627 DEBUG ConfigFactory - createOutputBamIndex = true; 20:41:37.627 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 20:41:37.627 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 20:41:37.627 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 2",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694
https://github.com/broadinstitute/gatk/issues/4694:5355,Modifiability,Config,ConfigFactory,5355,"G_FIELD_FORMAT : DECIMAL; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 20:41:37.620 DEBUG ConfigFactory - Configuration file values:; 20:41:37.626 DEBUG ConfigFactory - gcsMaxRetries = 20; 20:41:37.626 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.compression_level = 2; 20:41:37.626 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 20:41:37.626 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 20:41:37.626 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 20:41:37.626 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 20:41:37.626 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 20:41:37.626 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 20:41:37.627 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 20:41:37.627 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 20:41:37.627 DEBUG ConfigFactory - createOutputBamIndex = true; 20:41:37.627 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 20:41:37.627 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 20:41:37.627 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 20:41:37.627 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11b",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694
https://github.com/broadinstitute/gatk/issues/4694:5422,Modifiability,Config,ConfigFactory,5422,"HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 20:41:37.620 DEBUG ConfigFactory - Configuration file values:; 20:41:37.626 DEBUG ConfigFactory - gcsMaxRetries = 20; 20:41:37.626 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.compression_level = 2; 20:41:37.626 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 20:41:37.626 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 20:41:37.626 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 20:41:37.626 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 20:41:37.626 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 20:41:37.626 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 20:41:37.627 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 20:41:37.627 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 20:41:37.627 DEBUG ConfigFactory - createOutputBamIndex = true; 20:41:37.627 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 20:41:37.627 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 20:41:37.627 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 20:41:37.627 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694
https://github.com/broadinstitute/gatk/issues/4694:5497,Modifiability,Config,ConfigFactory,5497,"athSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 20:41:37.620 DEBUG ConfigFactory - Configuration file values:; 20:41:37.626 DEBUG ConfigFactory - gcsMaxRetries = 20; 20:41:37.626 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.compression_level = 2; 20:41:37.626 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 20:41:37.626 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 20:41:37.626 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 20:41:37.626 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 20:41:37.626 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 20:41:37.626 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 20:41:37.627 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 20:41:37.627 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 20:41:37.627 DEBUG ConfigFactory - createOutputBamIndex = true; 20:41:37.627 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 20:41:37.627 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 20:41:37.627 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 20:41:37.627 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 20:41:37.627 INFO PathSeqPipeline",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694
https://github.com/broadinstitute/gatk/issues/4694:5566,Modifiability,Config,ConfigFactory,5566," : true; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 20:41:37.620 DEBUG ConfigFactory - Configuration file values:; 20:41:37.626 DEBUG ConfigFactory - gcsMaxRetries = 20; 20:41:37.626 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.compression_level = 2; 20:41:37.626 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 20:41:37.626 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 20:41:37.626 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 20:41:37.626 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 20:41:37.626 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 20:41:37.626 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 20:41:37.627 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 20:41:37.627 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 20:41:37.627 DEBUG ConfigFactory - createOutputBamIndex = true; 20:41:37.627 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 20:41:37.627 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 20:41:37.627 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 20:41:37.627 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 20:41:37.627 INFO PathSeqPipelineSpark - Initializing engine; 20:41:37.627 INFO PathSeqPipelineSpark -",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694
https://github.com/broadinstitute/gatk/issues/4694:5643,Modifiability,Config,ConfigFactory,5643,"O_WRITE_FOR_TRIBBLE : false; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 20:41:37.620 DEBUG ConfigFactory - Configuration file values:; 20:41:37.626 DEBUG ConfigFactory - gcsMaxRetries = 20; 20:41:37.626 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.compression_level = 2; 20:41:37.626 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 20:41:37.626 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 20:41:37.626 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 20:41:37.626 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 20:41:37.626 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 20:41:37.626 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 20:41:37.627 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 20:41:37.627 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 20:41:37.627 DEBUG ConfigFactory - createOutputBamIndex = true; 20:41:37.627 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 20:41:37.627 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 20:41:37.627 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 20:41:37.627 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 20:41:37.627 INFO PathSeqPipelineSpark - Initializing engine; 20:41:37.627 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/sp",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694
https://github.com/broadinstitute/gatk/issues/4694:5711,Modifiability,Config,ConfigFactory,5711,"- HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 20:41:37.620 DEBUG ConfigFactory - Configuration file values:; 20:41:37.626 DEBUG ConfigFactory - gcsMaxRetries = 20; 20:41:37.626 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.compression_level = 2; 20:41:37.626 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 20:41:37.626 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 20:41:37.626 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 20:41:37.626 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 20:41:37.626 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 20:41:37.626 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 20:41:37.627 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 20:41:37.627 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 20:41:37.627 DEBUG ConfigFactory - createOutputBamIndex = true; 20:41:37.627 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 20:41:37.627 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 20:41:37.627 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 20:41:37.627 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 20:41:37.627 INFO PathSeqPipelineSpark - Initializing engine; 20:41:37.627 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 18/04/23 20:41:38 INFO SparkContext: ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694
https://github.com/broadinstitute/gatk/issues/4694:5781,Modifiability,Config,ConfigFactory,5781,"ry - Configuration file values:; 20:41:37.626 DEBUG ConfigFactory - gcsMaxRetries = 20; 20:41:37.626 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.compression_level = 2; 20:41:37.626 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 20:41:37.626 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 20:41:37.626 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 20:41:37.626 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 20:41:37.626 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 20:41:37.626 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 20:41:37.627 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 20:41:37.627 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 20:41:37.627 DEBUG ConfigFactory - createOutputBamIndex = true; 20:41:37.627 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 20:41:37.627 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 20:41:37.627 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 20:41:37.627 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 20:41:37.627 INFO PathSeqPipelineSpark - Initializing engine; 20:41:37.627 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 18/04/23 20:41:38 INFO SparkContext: Running Spark version 2.2.0; 18/04/23 20:41:38 WARN NativeCodeLoader: Unable to ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694
https://github.com/broadinstitute/gatk/issues/4694:5911,Modifiability,Config,ConfigFactory,5911,"ace_on_user_exception = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.compression_level = 2; 20:41:37.626 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 20:41:37.626 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 20:41:37.626 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 20:41:37.626 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 20:41:37.626 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 20:41:37.626 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 20:41:37.627 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 20:41:37.627 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 20:41:37.627 DEBUG ConfigFactory - createOutputBamIndex = true; 20:41:37.627 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 20:41:37.627 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 20:41:37.627 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 20:41:37.627 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 20:41:37.627 INFO PathSeqPipelineSpark - Initializing engine; 20:41:37.627 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 18/04/23 20:41:38 INFO SparkContext: Running Spark version 2.2.0; 18/04/23 20:41:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18/04/23 20:41:38 INFO SparkContext: Submi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694
https://github.com/broadinstitute/gatk/issues/4694:5972,Modifiability,Config,ConfigFactory,5972,"- samjdk.use_async_io_read_samtools = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.compression_level = 2; 20:41:37.626 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 20:41:37.626 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 20:41:37.626 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 20:41:37.626 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 20:41:37.626 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 20:41:37.626 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 20:41:37.627 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 20:41:37.627 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 20:41:37.627 DEBUG ConfigFactory - createOutputBamIndex = true; 20:41:37.627 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 20:41:37.627 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 20:41:37.627 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 20:41:37.627 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 20:41:37.627 INFO PathSeqPipelineSpark - Initializing engine; 20:41:37.627 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 18/04/23 20:41:38 INFO SparkContext: Running Spark version 2.2.0; 18/04/23 20:41:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18/04/23 20:41:38 INFO SparkContext: Submitted application: PathSeqPipelineSpark; 18/04/23 20:41:39 INFO ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694
https://github.com/broadinstitute/gatk/issues/4694:6038,Modifiability,Config,ConfigFactory,6038,"onfigFactory - samjdk.use_async_io_write_samtools = true; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.compression_level = 2; 20:41:37.626 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 20:41:37.626 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 20:41:37.626 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 20:41:37.626 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 20:41:37.626 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 20:41:37.626 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 20:41:37.627 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 20:41:37.627 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 20:41:37.627 DEBUG ConfigFactory - createOutputBamIndex = true; 20:41:37.627 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 20:41:37.627 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 20:41:37.627 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 20:41:37.627 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 20:41:37.627 INFO PathSeqPipelineSpark - Initializing engine; 20:41:37.627 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 18/04/23 20:41:38 INFO SparkContext: Running Spark version 2.2.0; 18/04/23 20:41:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18/04/23 20:41:38 INFO SparkContext: Submitted application: PathSeqPipelineSpark; 18/04/23 20:41:39 INFO SecurityManager: Changing view acls to: zorzan; 18/04/23 20:41:39",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694
https://github.com/broadinstitute/gatk/issues/4694:2261,Performance,Load,Loading,2261,"atch/home/int/eva/zorzan/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /scratch/home/int/eva/zorzan/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar PathSeqPipelineSpark --spark-master spark://xx.xx.xx.xx:7077 --input test_sample.bam --filter-bwa-image hg19mini.fasta.img --kmer-file hg19mini.hss --min-clipped-read-length 70 --microbe-fasta e_coli_k12.fasta --microbe-bwa-image e_coli_k12.fasta.img --conf [jars=~/bin/spark-2.2.0-bin-hadoop2.7/jars/hbase-client-1.4.3.jar] --taxonomy-file e_coli_k12.db --output output.pathseq.bam --verbosity DEBUG --scores-output output.pathseq.txt; 20:41:36.853 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 20:41:37.246 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/scratch/home/int/eva/zorzan/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 20:41:37.277 DEBUG NativeLibraryLoader - Extracting libgkl_compression.so to /tmp/zorzan/libgkl_compression6179723182683465083.so; 20:41:37.613 INFO PathSeqPipelineSpark - ------------------------------------------------------------; 20:41:37.613 INFO PathSeqPipelineSpark - The Genome Analysis Toolkit (GATK) v4.0.3.0; 20:41:37.613 INFO PathSeqPipelineSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 20:41:37.614 INFO PathSeqPipelineSpark - Executing as zorzan@node016 on Linux v2.6.32-220.4.1.el6.x86_64 amd64; 20:41:37.614 INFO PathSeqPipelineSpark - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_131-b11; 20:41:37.614 INFO PathSeqPipelineSpark - Start Date/Time: April 23, 2018 8:41:37 PM CEST; 20:41:37.614 INFO PathSeqPipelineSpark - --------------------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694
https://github.com/broadinstitute/gatk/issues/4694:6797,Performance,load,load,6797,"ry - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 20:41:37.627 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 20:41:37.627 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 20:41:37.627 DEBUG ConfigFactory - createOutputBamIndex = true; 20:41:37.627 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 20:41:37.627 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 20:41:37.627 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 20:41:37.627 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 20:41:37.627 INFO PathSeqPipelineSpark - Initializing engine; 20:41:37.627 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 18/04/23 20:41:38 INFO SparkContext: Running Spark version 2.2.0; 18/04/23 20:41:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18/04/23 20:41:38 INFO SparkContext: Submitted application: PathSeqPipelineSpark; 18/04/23 20:41:39 INFO SecurityManager: Changing view acls to: zorzan; 18/04/23 20:41:39 INFO SecurityManager: Changing modify acls to: zorzan; 18/04/23 20:41:39 INFO SecurityManager: Changing view acls groups to:; 18/04/23 20:41:39 INFO SecurityManager: Changing modify acls groups to:; 18/04/23 20:41:39 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(zorzan); groups with view permissions: Set(); users with modify permissions: Set(zorzan); groups with modify permissions: Set(); 18/04/23 20:41:41 INFO Utils: Successfully started service 'sparkDriver' on port 36273.; 18/04/23 20:41:41 INFO SparkEnv: Registering MapOutputTracker; 18/04/23 20:41:41 INFO SparkEnv: Registering BlockManagerMaster; 18/04/23 20:41:41 INFO BlockManagerMasterEndpoint: Usi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694
https://github.com/broadinstitute/gatk/issues/4694:13723,Performance,concurren,concurrent,13723,"d(0, xx.xx.xx.xx, 60154, None); 18/04/23 20:42:02 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, xx.xx.xx.xx, executor 0): java.lang.IllegalStateException: unread block data; at java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(ObjectInputStream.java:2740); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1567); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422); at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:309); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). 18/04/23 20:42:02 INFO TaskSetManager: Starting task 0.1 in stage 0.0 (TID 1, xx.xx.xx.xx, executor 0, partition 0, PROCESS_LOCAL, 4956 bytes); 18/04/23 20:42:02 INFO TaskSetManager: Lost task 0.1 in stage 0.0 (TID 1) on xx.xx.xx.xx, executor 0: java.lang.IllegalStateException (unread block data) [duplicate 1]; 18/04/23 20:42:02 INFO TaskSetManager: Starting task 0.2 in stage 0.0 (TID 2, xx.xx.xx.xx, executor 0, partition 0, PROCESS_LOCAL, 4956 bytes); 18/04/23 20:42:02 INFO TaskSetManager: Lost task 0.2 in stage 0.0 (TID 2) on xx.xx.xx.xx, executor 0: java.lang.IllegalStateException (unread block data) [duplicate 2]; 18/04/23 20:42:02 INFO TaskSetManager: Starting task 0.3 in stage 0.0 (TID 3, xx.xx.xx.xx, executor 0, partition 0, PROCESS_LOCAL, 4956 bytes); 18/04/23 20:42:02 INFO TaskSetManager: ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694
https://github.com/broadinstitute/gatk/issues/4694:13807,Performance,concurren,concurrent,13807," in stage 0.0 (TID 0, xx.xx.xx.xx, executor 0): java.lang.IllegalStateException: unread block data; at java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(ObjectInputStream.java:2740); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1567); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422); at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:309); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). 18/04/23 20:42:02 INFO TaskSetManager: Starting task 0.1 in stage 0.0 (TID 1, xx.xx.xx.xx, executor 0, partition 0, PROCESS_LOCAL, 4956 bytes); 18/04/23 20:42:02 INFO TaskSetManager: Lost task 0.1 in stage 0.0 (TID 1) on xx.xx.xx.xx, executor 0: java.lang.IllegalStateException (unread block data) [duplicate 1]; 18/04/23 20:42:02 INFO TaskSetManager: Starting task 0.2 in stage 0.0 (TID 2, xx.xx.xx.xx, executor 0, partition 0, PROCESS_LOCAL, 4956 bytes); 18/04/23 20:42:02 INFO TaskSetManager: Lost task 0.2 in stage 0.0 (TID 2) on xx.xx.xx.xx, executor 0: java.lang.IllegalStateException (unread block data) [duplicate 2]; 18/04/23 20:42:02 INFO TaskSetManager: Starting task 0.3 in stage 0.0 (TID 3, xx.xx.xx.xx, executor 0, partition 0, PROCESS_LOCAL, 4956 bytes); 18/04/23 20:42:02 INFO TaskSetManager: Lost task 0.3 in stage 0.0 (TID 3) on xx.xx.xx.xx, executor 0: java.lang.IllegalStat",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694
https://github.com/broadinstitute/gatk/issues/4694:16246,Performance,concurren,concurrent,16246,"tage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3, xx.xx.xx.xx, executor 0): java.lang.IllegalStateException: unread block data; at java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(ObjectInputStream.java:2740); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1567); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422); at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:309); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 18/04/23 20:42:02 INFO DAGScheduler: Job 0 failed: first at ReadsSparkSource.java:221, took 11.814317 s; 18/04/23 20:42:02 INFO SparkUI: Stopped Spark web UI at http://xx.xx.xx.xx:4040; 18/04/23 20:42:02 INFO StandaloneSchedulerBackend: Shutting down all executors; 18/04/23 20:42:02 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down; 18/04/23 20:42:03 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 18/04/23 20:42:03 INFO MemoryStore: MemoryStore cleared; 18/04/23 20:42:03 INFO BlockManager: BlockManager stopped; 18/04/23 20:42:03 INFO BlockManagerMaster: BlockManagerMaster stopped; 18/04/23 20:42:03 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/04/23 20:42:03 INFO",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694
https://github.com/broadinstitute/gatk/issues/4694:16330,Performance,concurren,concurrent,16330," in stage 0.0 (TID 3, xx.xx.xx.xx, executor 0): java.lang.IllegalStateException: unread block data; at java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(ObjectInputStream.java:2740); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1567); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422); at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:309); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 18/04/23 20:42:02 INFO DAGScheduler: Job 0 failed: first at ReadsSparkSource.java:221, took 11.814317 s; 18/04/23 20:42:02 INFO SparkUI: Stopped Spark web UI at http://xx.xx.xx.xx:4040; 18/04/23 20:42:02 INFO StandaloneSchedulerBackend: Shutting down all executors; 18/04/23 20:42:02 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down; 18/04/23 20:42:03 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 18/04/23 20:42:03 INFO MemoryStore: MemoryStore cleared; 18/04/23 20:42:03 INFO BlockManager: BlockManager stopped; 18/04/23 20:42:03 INFO BlockManagerMaster: BlockManagerMaster stopped; 18/04/23 20:42:03 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/04/23 20:42:03 INFO SparkContext: Successfully stopped SparkContext; 20:42:03.045 INFO PathSeqPipelineS",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694
https://github.com/broadinstitute/gatk/issues/4694:18585,Performance,concurren,concurrent,18585,"tage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3, xx.xx.xx.xx, executor 0): java.lang.IllegalStateException: unread block data; at java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(ObjectInputStream.java:2740); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1567); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422); at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:309); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Opti",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694
https://github.com/broadinstitute/gatk/issues/4694:18669,Performance,concurren,concurrent,18669," in stage 0.0 (TID 3, xx.xx.xx.xx, executor 0): java.lang.IllegalStateException: unread block data; at java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(ObjectInputStream.java:2740); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1567); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422); at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:309); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskS",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694
https://github.com/broadinstitute/gatk/issues/4694:23325,Performance,concurren,concurrent,23325,elineSpark.java:238); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:387); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: java.lang.IllegalStateException: unread block data; at java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(ObjectInputStream.java:2740); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1567); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422); at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:309); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); 18/04/23 20:42:03 INFO ShutdownHookManager: Shutdown hook called; 18/04/23 20:42:03 INFO ShutdownHookManager: Deleting directory /tmp/zorzan/spark-63a4d9a6-222a-4dca-8810-0482f6692b22,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694
https://github.com/broadinstitute/gatk/issues/4694:23409,Performance,concurren,concurrent,23409,elineSpark.java:238); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:387); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: java.lang.IllegalStateException: unread block data; at java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(ObjectInputStream.java:2740); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1567); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422); at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:309); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); 18/04/23 20:42:03 INFO ShutdownHookManager: Shutdown hook called; 18/04/23 20:42:03 INFO ShutdownHookManager: Deleting directory /tmp/zorzan/spark-63a4d9a6-222a-4dca-8810-0482f6692b22,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694
https://github.com/broadinstitute/gatk/issues/4694:14935,Safety,abort,aborting,14935,"ting task 0.1 in stage 0.0 (TID 1, xx.xx.xx.xx, executor 0, partition 0, PROCESS_LOCAL, 4956 bytes); 18/04/23 20:42:02 INFO TaskSetManager: Lost task 0.1 in stage 0.0 (TID 1) on xx.xx.xx.xx, executor 0: java.lang.IllegalStateException (unread block data) [duplicate 1]; 18/04/23 20:42:02 INFO TaskSetManager: Starting task 0.2 in stage 0.0 (TID 2, xx.xx.xx.xx, executor 0, partition 0, PROCESS_LOCAL, 4956 bytes); 18/04/23 20:42:02 INFO TaskSetManager: Lost task 0.2 in stage 0.0 (TID 2) on xx.xx.xx.xx, executor 0: java.lang.IllegalStateException (unread block data) [duplicate 2]; 18/04/23 20:42:02 INFO TaskSetManager: Starting task 0.3 in stage 0.0 (TID 3, xx.xx.xx.xx, executor 0, partition 0, PROCESS_LOCAL, 4956 bytes); 18/04/23 20:42:02 INFO TaskSetManager: Lost task 0.3 in stage 0.0 (TID 3) on xx.xx.xx.xx, executor 0: java.lang.IllegalStateException (unread block data) [duplicate 3]; 18/04/23 20:42:02 ERROR TaskSetManager: Task 0 in stage 0.0 failed 4 times; aborting job; 18/04/23 20:42:02 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool; 18/04/23 20:42:02 INFO TaskSchedulerImpl: Cancelling stage 0; 18/04/23 20:42:02 INFO DAGScheduler: ResultStage 0 (first at ReadsSparkSource.java:221) failed in 11.519 s due to Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3, xx.xx.xx.xx, executor 0): java.lang.IllegalStateException: unread block data; at java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(ObjectInputStream.java:2740); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1567); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); at java.io.ObjectInputStream.readObject(ObjectInputStream.ja",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694
https://github.com/broadinstitute/gatk/issues/4694:15235,Safety,abort,aborted,15235," INFO TaskSetManager: Starting task 0.2 in stage 0.0 (TID 2, xx.xx.xx.xx, executor 0, partition 0, PROCESS_LOCAL, 4956 bytes); 18/04/23 20:42:02 INFO TaskSetManager: Lost task 0.2 in stage 0.0 (TID 2) on xx.xx.xx.xx, executor 0: java.lang.IllegalStateException (unread block data) [duplicate 2]; 18/04/23 20:42:02 INFO TaskSetManager: Starting task 0.3 in stage 0.0 (TID 3, xx.xx.xx.xx, executor 0, partition 0, PROCESS_LOCAL, 4956 bytes); 18/04/23 20:42:02 INFO TaskSetManager: Lost task 0.3 in stage 0.0 (TID 3) on xx.xx.xx.xx, executor 0: java.lang.IllegalStateException (unread block data) [duplicate 3]; 18/04/23 20:42:02 ERROR TaskSetManager: Task 0 in stage 0.0 failed 4 times; aborting job; 18/04/23 20:42:02 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool; 18/04/23 20:42:02 INFO TaskSchedulerImpl: Cancelling stage 0; 18/04/23 20:42:02 INFO DAGScheduler: ResultStage 0 (first at ReadsSparkSource.java:221) failed in 11.519 s due to Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3, xx.xx.xx.xx, executor 0): java.lang.IllegalStateException: unread block data; at java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(ObjectInputStream.java:2740); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1567); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422); at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:309); at java.util.concu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694
https://github.com/broadinstitute/gatk/issues/4694:17574,Safety,abort,aborted,17574,"FO SparkUI: Stopped Spark web UI at http://xx.xx.xx.xx:4040; 18/04/23 20:42:02 INFO StandaloneSchedulerBackend: Shutting down all executors; 18/04/23 20:42:02 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down; 18/04/23 20:42:03 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 18/04/23 20:42:03 INFO MemoryStore: MemoryStore cleared; 18/04/23 20:42:03 INFO BlockManager: BlockManager stopped; 18/04/23 20:42:03 INFO BlockManagerMaster: BlockManagerMaster stopped; 18/04/23 20:42:03 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/04/23 20:42:03 INFO SparkContext: Successfully stopped SparkContext; 20:42:03.045 INFO PathSeqPipelineSpark - Shutting down engine; [April 23, 2018 8:42:03 PM CEST] org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark done. Elapsed time: 0.43 minutes.; Runtime.totalMemory()=793247744; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3, xx.xx.xx.xx, executor 0): java.lang.IllegalStateException: unread block data; at java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(ObjectInputStream.java:2740); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1567); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422); at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:309); at java.util.con",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694
https://github.com/broadinstitute/gatk/issues/4694:18992,Safety,abort,abortStage,18992,s(ObjectInputStream.java:2245); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422); at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:309); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694
https://github.com/broadinstitute/gatk/issues/4694:19089,Safety,abort,abortStage,19089,a:2169); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422); at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:309); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.s,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694
https://github.com/broadinstitute/gatk/issues/4694:19331,Safety,abort,abortStage,19331,avaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:309); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); at org,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694
https://github.com/broadinstitute/gatk/issues/4694:6995,Security,Secur,SecurityManager,6995,7 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 20:41:37.627 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 20:41:37.627 INFO PathSeqPipelineSpark - Initializing engine; 20:41:37.627 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 18/04/23 20:41:38 INFO SparkContext: Running Spark version 2.2.0; 18/04/23 20:41:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18/04/23 20:41:38 INFO SparkContext: Submitted application: PathSeqPipelineSpark; 18/04/23 20:41:39 INFO SecurityManager: Changing view acls to: zorzan; 18/04/23 20:41:39 INFO SecurityManager: Changing modify acls to: zorzan; 18/04/23 20:41:39 INFO SecurityManager: Changing view acls groups to:; 18/04/23 20:41:39 INFO SecurityManager: Changing modify acls groups to:; 18/04/23 20:41:39 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(zorzan); groups with view permissions: Set(); users with modify permissions: Set(zorzan); groups with modify permissions: Set(); 18/04/23 20:41:41 INFO Utils: Successfully started service 'sparkDriver' on port 36273.; 18/04/23 20:41:41 INFO SparkEnv: Registering MapOutputTracker; 18/04/23 20:41:41 INFO SparkEnv: Registering BlockManagerMaster; 18/04/23 20:41:41 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 18/04/23 20:41:41 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up; 18/04/23 20:41:41 INFO DiskBlockManager: Created local directory at /tmp/zorzan/blockmgr-994d8501-06ce-4315-84ff-3c29de358ae1; 18/04/23 20:41:41 INFO MemoryStore: MemoryStore started with capacity 4.0 GB; 18/04/23 20:41:41 INFO SparkEnv: Registering OutputCommitCoordi,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694
https://github.com/broadinstitute/gatk/issues/4694:7066,Security,Secur,SecurityManager,7066,7 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 20:41:37.627 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 20:41:37.627 INFO PathSeqPipelineSpark - Initializing engine; 20:41:37.627 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 18/04/23 20:41:38 INFO SparkContext: Running Spark version 2.2.0; 18/04/23 20:41:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18/04/23 20:41:38 INFO SparkContext: Submitted application: PathSeqPipelineSpark; 18/04/23 20:41:39 INFO SecurityManager: Changing view acls to: zorzan; 18/04/23 20:41:39 INFO SecurityManager: Changing modify acls to: zorzan; 18/04/23 20:41:39 INFO SecurityManager: Changing view acls groups to:; 18/04/23 20:41:39 INFO SecurityManager: Changing modify acls groups to:; 18/04/23 20:41:39 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(zorzan); groups with view permissions: Set(); users with modify permissions: Set(zorzan); groups with modify permissions: Set(); 18/04/23 20:41:41 INFO Utils: Successfully started service 'sparkDriver' on port 36273.; 18/04/23 20:41:41 INFO SparkEnv: Registering MapOutputTracker; 18/04/23 20:41:41 INFO SparkEnv: Registering BlockManagerMaster; 18/04/23 20:41:41 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 18/04/23 20:41:41 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up; 18/04/23 20:41:41 INFO DiskBlockManager: Created local directory at /tmp/zorzan/blockmgr-994d8501-06ce-4315-84ff-3c29de358ae1; 18/04/23 20:41:41 INFO MemoryStore: MemoryStore started with capacity 4.0 GB; 18/04/23 20:41:41 INFO SparkEnv: Registering OutputCommitCoordi,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694
https://github.com/broadinstitute/gatk/issues/4694:7139,Security,Secur,SecurityManager,7139,7 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 20:41:37.627 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 20:41:37.627 INFO PathSeqPipelineSpark - Initializing engine; 20:41:37.627 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 18/04/23 20:41:38 INFO SparkContext: Running Spark version 2.2.0; 18/04/23 20:41:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18/04/23 20:41:38 INFO SparkContext: Submitted application: PathSeqPipelineSpark; 18/04/23 20:41:39 INFO SecurityManager: Changing view acls to: zorzan; 18/04/23 20:41:39 INFO SecurityManager: Changing modify acls to: zorzan; 18/04/23 20:41:39 INFO SecurityManager: Changing view acls groups to:; 18/04/23 20:41:39 INFO SecurityManager: Changing modify acls groups to:; 18/04/23 20:41:39 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(zorzan); groups with view permissions: Set(); users with modify permissions: Set(zorzan); groups with modify permissions: Set(); 18/04/23 20:41:41 INFO Utils: Successfully started service 'sparkDriver' on port 36273.; 18/04/23 20:41:41 INFO SparkEnv: Registering MapOutputTracker; 18/04/23 20:41:41 INFO SparkEnv: Registering BlockManagerMaster; 18/04/23 20:41:41 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 18/04/23 20:41:41 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up; 18/04/23 20:41:41 INFO DiskBlockManager: Created local directory at /tmp/zorzan/blockmgr-994d8501-06ce-4315-84ff-3c29de358ae1; 18/04/23 20:41:41 INFO MemoryStore: MemoryStore started with capacity 4.0 GB; 18/04/23 20:41:41 INFO SparkEnv: Registering OutputCommitCoordi,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694
https://github.com/broadinstitute/gatk/issues/4694:7210,Security,Secur,SecurityManager,7210,7 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 20:41:37.627 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 20:41:37.627 INFO PathSeqPipelineSpark - Initializing engine; 20:41:37.627 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 18/04/23 20:41:38 INFO SparkContext: Running Spark version 2.2.0; 18/04/23 20:41:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18/04/23 20:41:38 INFO SparkContext: Submitted application: PathSeqPipelineSpark; 18/04/23 20:41:39 INFO SecurityManager: Changing view acls to: zorzan; 18/04/23 20:41:39 INFO SecurityManager: Changing modify acls to: zorzan; 18/04/23 20:41:39 INFO SecurityManager: Changing view acls groups to:; 18/04/23 20:41:39 INFO SecurityManager: Changing modify acls groups to:; 18/04/23 20:41:39 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(zorzan); groups with view permissions: Set(); users with modify permissions: Set(zorzan); groups with modify permissions: Set(); 18/04/23 20:41:41 INFO Utils: Successfully started service 'sparkDriver' on port 36273.; 18/04/23 20:41:41 INFO SparkEnv: Registering MapOutputTracker; 18/04/23 20:41:41 INFO SparkEnv: Registering BlockManagerMaster; 18/04/23 20:41:41 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 18/04/23 20:41:41 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up; 18/04/23 20:41:41 INFO DiskBlockManager: Created local directory at /tmp/zorzan/blockmgr-994d8501-06ce-4315-84ff-3c29de358ae1; 18/04/23 20:41:41 INFO MemoryStore: MemoryStore started with capacity 4.0 GB; 18/04/23 20:41:41 INFO SparkEnv: Registering OutputCommitCoordi,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694
https://github.com/broadinstitute/gatk/issues/4694:7283,Security,Secur,SecurityManager,7283,7 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 20:41:37.627 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 20:41:37.627 INFO PathSeqPipelineSpark - Initializing engine; 20:41:37.627 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 18/04/23 20:41:38 INFO SparkContext: Running Spark version 2.2.0; 18/04/23 20:41:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18/04/23 20:41:38 INFO SparkContext: Submitted application: PathSeqPipelineSpark; 18/04/23 20:41:39 INFO SecurityManager: Changing view acls to: zorzan; 18/04/23 20:41:39 INFO SecurityManager: Changing modify acls to: zorzan; 18/04/23 20:41:39 INFO SecurityManager: Changing view acls groups to:; 18/04/23 20:41:39 INFO SecurityManager: Changing modify acls groups to:; 18/04/23 20:41:39 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(zorzan); groups with view permissions: Set(); users with modify permissions: Set(zorzan); groups with modify permissions: Set(); 18/04/23 20:41:41 INFO Utils: Successfully started service 'sparkDriver' on port 36273.; 18/04/23 20:41:41 INFO SparkEnv: Registering MapOutputTracker; 18/04/23 20:41:41 INFO SparkEnv: Registering BlockManagerMaster; 18/04/23 20:41:41 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 18/04/23 20:41:41 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up; 18/04/23 20:41:41 INFO DiskBlockManager: Created local directory at /tmp/zorzan/blockmgr-994d8501-06ce-4315-84ff-3c29de358ae1; 18/04/23 20:41:41 INFO MemoryStore: MemoryStore started with capacity 4.0 GB; 18/04/23 20:41:41 INFO SparkEnv: Registering OutputCommitCoordi,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694
https://github.com/broadinstitute/gatk/issues/4694:7300,Security,Secur,SecurityManager,7300,7 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 20:41:37.627 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 20:41:37.627 INFO PathSeqPipelineSpark - Initializing engine; 20:41:37.627 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 18/04/23 20:41:38 INFO SparkContext: Running Spark version 2.2.0; 18/04/23 20:41:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18/04/23 20:41:38 INFO SparkContext: Submitted application: PathSeqPipelineSpark; 18/04/23 20:41:39 INFO SecurityManager: Changing view acls to: zorzan; 18/04/23 20:41:39 INFO SecurityManager: Changing modify acls to: zorzan; 18/04/23 20:41:39 INFO SecurityManager: Changing view acls groups to:; 18/04/23 20:41:39 INFO SecurityManager: Changing modify acls groups to:; 18/04/23 20:41:39 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(zorzan); groups with view permissions: Set(); users with modify permissions: Set(zorzan); groups with modify permissions: Set(); 18/04/23 20:41:41 INFO Utils: Successfully started service 'sparkDriver' on port 36273.; 18/04/23 20:41:41 INFO SparkEnv: Registering MapOutputTracker; 18/04/23 20:41:41 INFO SparkEnv: Registering BlockManagerMaster; 18/04/23 20:41:41 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 18/04/23 20:41:41 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up; 18/04/23 20:41:41 INFO DiskBlockManager: Created local directory at /tmp/zorzan/blockmgr-994d8501-06ce-4315-84ff-3c29de358ae1; 18/04/23 20:41:41 INFO MemoryStore: MemoryStore started with capacity 4.0 GB; 18/04/23 20:41:41 INFO SparkEnv: Registering OutputCommitCoordi,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694
https://github.com/broadinstitute/gatk/issues/4694:7317,Security,authenticat,authentication,7317,7 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 20:41:37.627 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 20:41:37.627 INFO PathSeqPipelineSpark - Initializing engine; 20:41:37.627 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 18/04/23 20:41:38 INFO SparkContext: Running Spark version 2.2.0; 18/04/23 20:41:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18/04/23 20:41:38 INFO SparkContext: Submitted application: PathSeqPipelineSpark; 18/04/23 20:41:39 INFO SecurityManager: Changing view acls to: zorzan; 18/04/23 20:41:39 INFO SecurityManager: Changing modify acls to: zorzan; 18/04/23 20:41:39 INFO SecurityManager: Changing view acls groups to:; 18/04/23 20:41:39 INFO SecurityManager: Changing modify acls groups to:; 18/04/23 20:41:39 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(zorzan); groups with view permissions: Set(); users with modify permissions: Set(zorzan); groups with modify permissions: Set(); 18/04/23 20:41:41 INFO Utils: Successfully started service 'sparkDriver' on port 36273.; 18/04/23 20:41:41 INFO SparkEnv: Registering MapOutputTracker; 18/04/23 20:41:41 INFO SparkEnv: Registering BlockManagerMaster; 18/04/23 20:41:41 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 18/04/23 20:41:41 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up; 18/04/23 20:41:41 INFO DiskBlockManager: Created local directory at /tmp/zorzan/blockmgr-994d8501-06ce-4315-84ff-3c29de358ae1; 18/04/23 20:41:41 INFO MemoryStore: MemoryStore started with capacity 4.0 GB; 18/04/23 20:41:41 INFO SparkEnv: Registering OutputCommitCoordi,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694
https://github.com/broadinstitute/gatk/issues/4694:783,Testability,log,log,783,I have been trying to run the tutorial ( https://gatkforums.broadinstitute.org/gatk/discussion/10913/how-to-run-the-pathseq-pipeline ). When running without the --spark-master the turoail runs smoothly. Bu twhen I try my spark master I get an error. I downloaded SPARK 2.2.0 with hadoop 2.7.3; Java is 1.8.0_131; I set the java classpath (I think correctly); I am aware of this thread: https://github.com/broadinstitute/gatk/issues/3050. But noentheless I cannot get the error to solve. I tried to copy the jar files:; hbase-client-1.4.3.jar; hbase-common-1.4.3.jar; hbase-hadoop2-compat-1.4.3.jar; hbase-protocol-1.4.3.jar; hbase-server-1.4.3.jar; To my spark jar folder. Shall I do smething else? I am also a SPARK newbie. Thank you very much!. ***************** Here is the error log:. ../../../gatk PathSeqPipelineSpark --spark-master spark://xx.xx.xx.xx:7077 --input test_sample.bam --filter-bwa-image hg19mini.fasta.img --kmer-file hg19mini.hss --min-clipped-read-length 70 --microbe-fasta e_coli_k12.fasta --microbe-bwa-image e_coli_k12.fasta.img --conf [jars=~/bin/spark-2.2.0-bin-hadoop2.7/jars/hbase-client-1.4.3.jar] --taxonomy-file e_coli_k12.db --output output.pathseq.bam --verbosity DEBUG --scores-output output.pathseq.txt; Using GATK jar /scratch/home/int/eva/zorzan/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /scratch/home/int/eva/zorzan/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar PathSeqPipelineSpark --spark-master spark://xx.xx.xx.xx:7077 --input test_sample.bam --filter-bwa-image hg19mini.fasta.img --kmer-file hg19mini.hss --min-clipped-read-length 70 --microbe-fasta e_coli_k12.fasta --microbe-bwa-image e_coli_k12.fasta.img --conf [jars=~/bin/spark-2.2.0-bin-hadoop2.7/jars/hbase-client-1.4.3.jar] --taxonomy-file e_coli_k12.db --output output.pathseq.bam --verbosity DEBUG --scores-output,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694
https://github.com/broadinstitute/gatk/issues/4694:16977,Usability,clear,cleared,16977,"org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:309); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 18/04/23 20:42:02 INFO DAGScheduler: Job 0 failed: first at ReadsSparkSource.java:221, took 11.814317 s; 18/04/23 20:42:02 INFO SparkUI: Stopped Spark web UI at http://xx.xx.xx.xx:4040; 18/04/23 20:42:02 INFO StandaloneSchedulerBackend: Shutting down all executors; 18/04/23 20:42:02 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down; 18/04/23 20:42:03 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 18/04/23 20:42:03 INFO MemoryStore: MemoryStore cleared; 18/04/23 20:42:03 INFO BlockManager: BlockManager stopped; 18/04/23 20:42:03 INFO BlockManagerMaster: BlockManagerMaster stopped; 18/04/23 20:42:03 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/04/23 20:42:03 INFO SparkContext: Successfully stopped SparkContext; 20:42:03.045 INFO PathSeqPipelineSpark - Shutting down engine; [April 23, 2018 8:42:03 PM CEST] org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark done. Elapsed time: 0.43 minutes.; Runtime.totalMemory()=793247744; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3, xx.xx.xx.xx, executor 0): java.lang.IllegalStateException: unread block data; at java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(ObjectInputStream.java:2740); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1567); at java.io.ObjectInputStream.defau",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694
https://github.com/broadinstitute/gatk/issues/4696:1073,Availability,down,down,1073,"a conda environment. $ conda env create -n gatk_gpu -f gatkcondaenv.yml. Activate this environment and execute GATK CNNScoreVariants(1D model). $ gatk-4.0.3.0/gatk --java-options '-Xmx8G -DGATK_STACKTRACE_ON_USER_EXCEPTION=true' CNNScoreVariants -V INPUT_VCF -R REF -O OUTPUT_VCF. Then, GATK outputs the following:. ```; 18:30:57.468 INFO CNNScoreVariants - Initializing engine; 18:30:57.985 INFO FeatureManager - Using codec VCFCodec to read file ...; 18:30:58.183 INFO IntervalArgumentCollection - Processing 48129895 bp from intervals; 18:30:58.188 INFO CNNScoreVariants - Done initializing engine; 18:31:00.188 INFO CNNScoreVariants - Using key:CNN_1D for CNN architecture:.../1d_cnn_mix_train_full_bn.2560984151625233621.json and weights:.../1d_cnn_mix_train_full_bn.2397909300265264152.hd5; 18:31:19.873 INFO ProgressMeter - Starting traversal; 18:31:19.874 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 18:31:50.095 INFO CNNScoreVariants - Shutting down engine; [2018/04/24 18:31:50 JST] org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants done. Elapsed time: 0.88 minutes.; Runtime.totalMemory()=2141716480; ***********************************************************************; A USER ERROR has occurred: A timeout ocurred waiting for output from the remote Python command.; ***********************************************************************; org.broadinstitute.hellbender.exceptions.UserException: A timeout ocurred waiting for output from the remote Python command.; at org.broadinstitute.hellbender.utils.python.StreamingPythonScriptExecutor.getAccumulatedOutput(StreamingPythonScriptExecutor.java:219); at org.broadinstitute.hellbender.utils.python.StreamingPythonScriptExecutor.sendSynchronousCommand(StreamingPythonScriptExecutor.java:153); at org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants.onTraversalStart(CNNScoreVariants.java:260); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.jav",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4696
https://github.com/broadinstitute/gatk/issues/4696:1327,Availability,ERROR,ERROR,1327,"nts -V INPUT_VCF -R REF -O OUTPUT_VCF. Then, GATK outputs the following:. ```; 18:30:57.468 INFO CNNScoreVariants - Initializing engine; 18:30:57.985 INFO FeatureManager - Using codec VCFCodec to read file ...; 18:30:58.183 INFO IntervalArgumentCollection - Processing 48129895 bp from intervals; 18:30:58.188 INFO CNNScoreVariants - Done initializing engine; 18:31:00.188 INFO CNNScoreVariants - Using key:CNN_1D for CNN architecture:.../1d_cnn_mix_train_full_bn.2560984151625233621.json and weights:.../1d_cnn_mix_train_full_bn.2397909300265264152.hd5; 18:31:19.873 INFO ProgressMeter - Starting traversal; 18:31:19.874 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 18:31:50.095 INFO CNNScoreVariants - Shutting down engine; [2018/04/24 18:31:50 JST] org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants done. Elapsed time: 0.88 minutes.; Runtime.totalMemory()=2141716480; ***********************************************************************; A USER ERROR has occurred: A timeout ocurred waiting for output from the remote Python command.; ***********************************************************************; org.broadinstitute.hellbender.exceptions.UserException: A timeout ocurred waiting for output from the remote Python command.; at org.broadinstitute.hellbender.utils.python.StreamingPythonScriptExecutor.getAccumulatedOutput(StreamingPythonScriptExecutor.java:219); at org.broadinstitute.hellbender.utils.python.StreamingPythonScriptExecutor.sendSynchronousCommand(StreamingPythonScriptExecutor.java:153); at org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants.onTraversalStart(CNNScoreVariants.java:260); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:891); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitut",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4696
https://github.com/broadinstitute/gatk/issues/4696:2621,Performance,concurren,concurrent,2621,"adinstitute.hellbender.utils.python.StreamingPythonScriptExecutor.getAccumulatedOutput(StreamingPythonScriptExecutor.java:219); at org.broadinstitute.hellbender.utils.python.StreamingPythonScriptExecutor.sendSynchronousCommand(StreamingPythonScriptExecutor.java:153); at org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants.onTraversalStart(CNNScoreVariants.java:260); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:891); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: java.util.concurrent.TimeoutException; at java.util.concurrent.FutureTask.get(FutureTask.java:205); at org.broadinstitute.hellbender.utils.runtime.StreamingProcessController.getProcessOutput(StreamingProcessController.java:278); at org.broadinstitute.hellbender.utils.runtime.StreamingProcessController.getOutputSynchronizedBy(StreamingProcessController.java:192); at org.broadinstitute.hellbender.utils.runtime.StreamingProcessController.getProcessOutputByPrompt(StreamingProcessController.java:163); at org.broadinstitute.hellbender.utils.python.StreamingPythonScriptExecutor.getAccumulatedOutput(StreamingPythonScriptExecutor.java:209); ... 9 more; ```. While this execution, continue watching the processes on GPU. $ watch -n 0.5 sh -c 'nvidia-smi | tail'; ```; +-----------------------------------------------------------------------------+; | Processes: GPU Memory |; | GPU PID Type Process name Usage |; |=============================================================================|; | 0 63485 C python 15422",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4696
https://github.com/broadinstitute/gatk/issues/4696:2663,Performance,concurren,concurrent,2663,"ingPythonScriptExecutor.getAccumulatedOutput(StreamingPythonScriptExecutor.java:219); at org.broadinstitute.hellbender.utils.python.StreamingPythonScriptExecutor.sendSynchronousCommand(StreamingPythonScriptExecutor.java:153); at org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants.onTraversalStart(CNNScoreVariants.java:260); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:891); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: java.util.concurrent.TimeoutException; at java.util.concurrent.FutureTask.get(FutureTask.java:205); at org.broadinstitute.hellbender.utils.runtime.StreamingProcessController.getProcessOutput(StreamingProcessController.java:278); at org.broadinstitute.hellbender.utils.runtime.StreamingProcessController.getOutputSynchronizedBy(StreamingProcessController.java:192); at org.broadinstitute.hellbender.utils.runtime.StreamingProcessController.getProcessOutputByPrompt(StreamingProcessController.java:163); at org.broadinstitute.hellbender.utils.python.StreamingPythonScriptExecutor.getAccumulatedOutput(StreamingPythonScriptExecutor.java:209); ... 9 more; ```. While this execution, continue watching the processes on GPU. $ watch -n 0.5 sh -c 'nvidia-smi | tail'; ```; +-----------------------------------------------------------------------------+; | Processes: GPU Memory |; | GPU PID Type Process name Usage |; |=============================================================================|; | 0 63485 C python 15422MiB |; | 1 63485 C python 15358MiB |; | 2 ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4696
https://github.com/broadinstitute/gatk/issues/4696:1349,Safety,timeout,timeout,1349,"nts -V INPUT_VCF -R REF -O OUTPUT_VCF. Then, GATK outputs the following:. ```; 18:30:57.468 INFO CNNScoreVariants - Initializing engine; 18:30:57.985 INFO FeatureManager - Using codec VCFCodec to read file ...; 18:30:58.183 INFO IntervalArgumentCollection - Processing 48129895 bp from intervals; 18:30:58.188 INFO CNNScoreVariants - Done initializing engine; 18:31:00.188 INFO CNNScoreVariants - Using key:CNN_1D for CNN architecture:.../1d_cnn_mix_train_full_bn.2560984151625233621.json and weights:.../1d_cnn_mix_train_full_bn.2397909300265264152.hd5; 18:31:19.873 INFO ProgressMeter - Starting traversal; 18:31:19.874 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 18:31:50.095 INFO CNNScoreVariants - Shutting down engine; [2018/04/24 18:31:50 JST] org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants done. Elapsed time: 0.88 minutes.; Runtime.totalMemory()=2141716480; ***********************************************************************; A USER ERROR has occurred: A timeout ocurred waiting for output from the remote Python command.; ***********************************************************************; org.broadinstitute.hellbender.exceptions.UserException: A timeout ocurred waiting for output from the remote Python command.; at org.broadinstitute.hellbender.utils.python.StreamingPythonScriptExecutor.getAccumulatedOutput(StreamingPythonScriptExecutor.java:219); at org.broadinstitute.hellbender.utils.python.StreamingPythonScriptExecutor.sendSynchronousCommand(StreamingPythonScriptExecutor.java:153); at org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants.onTraversalStart(CNNScoreVariants.java:260); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:891); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitut",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4696
https://github.com/broadinstitute/gatk/issues/4696:1548,Safety,timeout,timeout,1548,n - Processing 48129895 bp from intervals; 18:30:58.188 INFO CNNScoreVariants - Done initializing engine; 18:31:00.188 INFO CNNScoreVariants - Using key:CNN_1D for CNN architecture:.../1d_cnn_mix_train_full_bn.2560984151625233621.json and weights:.../1d_cnn_mix_train_full_bn.2397909300265264152.hd5; 18:31:19.873 INFO ProgressMeter - Starting traversal; 18:31:19.874 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 18:31:50.095 INFO CNNScoreVariants - Shutting down engine; [2018/04/24 18:31:50 JST] org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants done. Elapsed time: 0.88 minutes.; Runtime.totalMemory()=2141716480; ***********************************************************************; A USER ERROR has occurred: A timeout ocurred waiting for output from the remote Python command.; ***********************************************************************; org.broadinstitute.hellbender.exceptions.UserException: A timeout ocurred waiting for output from the remote Python command.; at org.broadinstitute.hellbender.utils.python.StreamingPythonScriptExecutor.getAccumulatedOutput(StreamingPythonScriptExecutor.java:219); at org.broadinstitute.hellbender.utils.python.StreamingPythonScriptExecutor.sendSynchronousCommand(StreamingPythonScriptExecutor.java:153); at org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants.onTraversalStart(CNNScoreVariants.java:260); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:891); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4696
https://github.com/broadinstitute/gatk/issues/4696:2632,Safety,Timeout,TimeoutException,2632,"der.utils.python.StreamingPythonScriptExecutor.getAccumulatedOutput(StreamingPythonScriptExecutor.java:219); at org.broadinstitute.hellbender.utils.python.StreamingPythonScriptExecutor.sendSynchronousCommand(StreamingPythonScriptExecutor.java:153); at org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants.onTraversalStart(CNNScoreVariants.java:260); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:891); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: java.util.concurrent.TimeoutException; at java.util.concurrent.FutureTask.get(FutureTask.java:205); at org.broadinstitute.hellbender.utils.runtime.StreamingProcessController.getProcessOutput(StreamingProcessController.java:278); at org.broadinstitute.hellbender.utils.runtime.StreamingProcessController.getOutputSynchronizedBy(StreamingProcessController.java:192); at org.broadinstitute.hellbender.utils.runtime.StreamingProcessController.getProcessOutputByPrompt(StreamingProcessController.java:163); at org.broadinstitute.hellbender.utils.python.StreamingPythonScriptExecutor.getAccumulatedOutput(StreamingPythonScriptExecutor.java:209); ... 9 more; ```. While this execution, continue watching the processes on GPU. $ watch -n 0.5 sh -c 'nvidia-smi | tail'; ```; +-----------------------------------------------------------------------------+; | Processes: GPU Memory |; | GPU PID Type Process name Usage |; |=============================================================================|; | 0 63485 C python 15422MiB |; | 1 63485 C",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4696
https://github.com/broadinstitute/gatk/pull/4697:0,Usability,Simpl,Simple,0,Simple one liner to replace an errant println.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4697
https://github.com/broadinstitute/gatk/issues/4699:88,Availability,down,downloaded,88,"Hi,; I run the PathSeqPipelineSpark on a SPARK HPC with a master and several workers. I downloaded SPARK 2.2.0 with hadoop 2.7.3; Java is 1.8.0_131; I set the java classpath (I think correctly). The command runs well without the --spark-master option, so the files are at the right place, but when I run the following command line:; `gatk PathSeqPipelineSpark --spark-master spark://XX.XX.XX.XX:7077 --input test_sample.bam --filter-bwa-image hg19mini.fasta.img --kmer-file hg19mini.hss --min-clipped-read-length 70 --microbe-fasta e_coli_k12.fasta --microbe-bwa-image e_coli_k12.fasta.img --taxonomy-file e_coli_k12.db --output output.pathseq.bam --verbosity DEBUG --scores-output output.pathseq.txt -- --spark-runner SPARK`. I get the following error:; ```; 18/04/24 17:55:54 WARN TaskSetManager: Lost task 1.0 in stage 2.0 (TID 4, xx.xx.xx.16, executor 3): **org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112)**; at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:747,Availability,error,error,747,"Hi,; I run the PathSeqPipelineSpark on a SPARK HPC with a master and several workers. I downloaded SPARK 2.2.0 with hadoop 2.7.3; Java is 1.8.0_131; I set the java classpath (I think correctly). The command runs well without the --spark-master option, so the files are at the right place, but when I run the following command line:; `gatk PathSeqPipelineSpark --spark-master spark://XX.XX.XX.XX:7077 --input test_sample.bam --filter-bwa-image hg19mini.fasta.img --kmer-file hg19mini.hss --min-clipped-read-length 70 --microbe-fasta e_coli_k12.fasta --microbe-bwa-image e_coli_k12.fasta.img --taxonomy-file e_coli_k12.db --output output.pathseq.bam --verbosity DEBUG --scores-output output.pathseq.txt -- --spark-runner SPARK`. I get the following error:; ```; 18/04/24 17:55:54 WARN TaskSetManager: Lost task 1.0 in stage 2.0 (TID 4, xx.xx.xx.16, executor 3): **org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112)**; at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:960,Availability,Error,Error,960,"Hi,; I run the PathSeqPipelineSpark on a SPARK HPC with a master and several workers. I downloaded SPARK 2.2.0 with hadoop 2.7.3; Java is 1.8.0_131; I set the java classpath (I think correctly). The command runs well without the --spark-master option, so the files are at the right place, but when I run the following command line:; `gatk PathSeqPipelineSpark --spark-master spark://XX.XX.XX.XX:7077 --input test_sample.bam --filter-bwa-image hg19mini.fasta.img --kmer-file hg19mini.hss --min-clipped-read-length 70 --microbe-fasta e_coli_k12.fasta --microbe-bwa-image e_coli_k12.fasta.img --taxonomy-file e_coli_k12.db --output output.pathseq.bam --verbosity DEBUG --scores-output output.pathseq.txt -- --spark-runner SPARK`. I get the following error:; ```; 18/04/24 17:55:54 WARN TaskSetManager: Lost task 1.0 in stage 2.0 (TID 4, xx.xx.xx.16, executor 3): **org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112)**; at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:24977,Availability,Error,Error,24977,"SchedulerImpl: Adding task set 2.0 with 2 tasks; 00:59 DEBUG: [kryo] Write: WrappedArray([NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED, NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED]); 18/04/24 17:55:54 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 3, xx.xx.xx.25, executor 2, partition 0, PROCESS_LOCAL, 6010 bytes); 00:59 DEBUG: [kryo] Write: WrappedArray(null); 18/04/24 17:55:54 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 4, xx.xx.xx.16, executor 3, partition 1, PROCESS_LOCAL, 5371 bytes); 18/04/24 17:55:54 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on xx.xx.xx.16:39037 (size: 6.4 KB, free: 366.3 MB); 18/04/24 17:55:54 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on xx.xx.xx.25:41354 (size: 6.4 KB, free: 366.3 MB); **18/04/24 17:55:54 WARN TaskSetManager: Lost task 1.0 in stage 2.0 (TID 4, xx.xx.xx.16, executor 3): org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeS",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:27319,Availability,Error,Error,27319,"hreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more**. 00:59 DEBUG: [kryo] Write: WrappedArray(null); 18/04/24 17:55:54 INFO TaskSetManager: Starting task 1.1 in stage 2.0 (TID 5, xx.xx.xx.24, executor 1, partition 1, PROCESS_LOCAL, 5371 bytes); 18/04/24 17:55:54 INFO TaskSetManager: Lost task 0.0 in stage 2.0 (TID 3) on xx.xx.xx.25, executor 2: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile (Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory)) [duplicate 1]; 01:00 DEBUG: [kryo] Write: WrappedArray([NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED, NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED]); 18/04/24 17:55:54 INFO TaskSetManager: Starting task 0.1 in stage 2.0 (TID 6, xx.xx.xx.16, executor 3, partition 0, PROCESS_LOCAL, 6010 bytes); 18/04/24 17:55:55 INFO TaskSetManager: Lost task 0.1 in stage 2.0 (TID 6) on xx.xx.xx.16, executor 3: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile (Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory)) [duplicate 2]; 01:00 DEBUG: [kryo] Write: WrappedArray([NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED, NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED]); 18/04/24 17:55:55 INFO TaskSetManager: Starting task 0.2 in stage 2.0 (TID 7, xx.xx.xx.23, executor 5, partition 0, PROCESS_LOCAL, 6010 bytes); 18/04/24 17:55:55 INFO BlockMa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:27909,Availability,Error,Error,27909,"6 more**. 00:59 DEBUG: [kryo] Write: WrappedArray(null); 18/04/24 17:55:54 INFO TaskSetManager: Starting task 1.1 in stage 2.0 (TID 5, xx.xx.xx.24, executor 1, partition 1, PROCESS_LOCAL, 5371 bytes); 18/04/24 17:55:54 INFO TaskSetManager: Lost task 0.0 in stage 2.0 (TID 3) on xx.xx.xx.25, executor 2: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile (Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory)) [duplicate 1]; 01:00 DEBUG: [kryo] Write: WrappedArray([NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED, NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED]); 18/04/24 17:55:54 INFO TaskSetManager: Starting task 0.1 in stage 2.0 (TID 6, xx.xx.xx.16, executor 3, partition 0, PROCESS_LOCAL, 6010 bytes); 18/04/24 17:55:55 INFO TaskSetManager: Lost task 0.1 in stage 2.0 (TID 6) on xx.xx.xx.16, executor 3: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile (Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory)) [duplicate 2]; 01:00 DEBUG: [kryo] Write: WrappedArray([NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED, NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED]); 18/04/24 17:55:55 INFO TaskSetManager: Starting task 0.2 in stage 2.0 (TID 7, xx.xx.xx.23, executor 5, partition 0, PROCESS_LOCAL, 6010 bytes); 18/04/24 17:55:55 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on xx.xx.xx.23:42535 (size: 6.4 KB, free: 366.3 MB); 18/04/24 17:55:55 INFO TaskSetManager: Lost task 0.2 in stage 2.0 (TID 7) on xx.xx.xx.23, executor 5: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile (Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory)) [duplicate 3]; 01:00 DEBUG: [kryo] Write: WrappedArray([NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED, NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED]); 18/04/24 17:55:55 INFO TaskSetManager: Starti",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:28628,Availability,Error,Error,28628,"ID 6, xx.xx.xx.16, executor 3, partition 0, PROCESS_LOCAL, 6010 bytes); 18/04/24 17:55:55 INFO TaskSetManager: Lost task 0.1 in stage 2.0 (TID 6) on xx.xx.xx.16, executor 3: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile (Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory)) [duplicate 2]; 01:00 DEBUG: [kryo] Write: WrappedArray([NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED, NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED]); 18/04/24 17:55:55 INFO TaskSetManager: Starting task 0.2 in stage 2.0 (TID 7, xx.xx.xx.23, executor 5, partition 0, PROCESS_LOCAL, 6010 bytes); 18/04/24 17:55:55 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on xx.xx.xx.23:42535 (size: 6.4 KB, free: 366.3 MB); 18/04/24 17:55:55 INFO TaskSetManager: Lost task 0.2 in stage 2.0 (TID 7) on xx.xx.xx.23, executor 5: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile (Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory)) [duplicate 3]; 01:00 DEBUG: [kryo] Write: WrappedArray([NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED, NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED]); 18/04/24 17:55:55 INFO TaskSetManager: Starting task 0.3 in stage 2.0 (TID 8, xx.xx.xx.24, executor 4, partition 0, PROCESS_LOCAL, 6010 bytes); 18/04/24 17:56:00 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on xx.xx.xx.24:49966 (size: 6.4 KB, free: 366.3 MB); 18/04/24 17:56:04 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on xx.xx.xx.24:49966 (size: 23.1 KB, free: 366.3 MB); 18/04/24 17:56:07 WARN TaskSetManager: Lost task 1.1 in stage 2.0 (TID 5, xx.xx.xx.24, executor 1): org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:29475,Availability,Error,Error,29475,"stage 2.0 (TID 7) on xx.xx.xx.23, executor 5: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile (Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory)) [duplicate 3]; 01:00 DEBUG: [kryo] Write: WrappedArray([NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED, NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED]); 18/04/24 17:55:55 INFO TaskSetManager: Starting task 0.3 in stage 2.0 (TID 8, xx.xx.xx.24, executor 4, partition 0, PROCESS_LOCAL, 6010 bytes); 18/04/24 17:56:00 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on xx.xx.xx.24:49966 (size: 6.4 KB, free: 366.3 MB); 18/04/24 17:56:04 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on xx.xx.xx.24:49966 (size: 23.1 KB, free: 366.3 MB); 18/04/24 17:56:07 WARN TaskSetManager: Lost task 1.1 in stage 2.0 (TID 5, xx.xx.xx.24, executor 1): org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeS",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:32072,Availability,Error,Error,32072,"r directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. 01:12 DEBUG: [kryo] Write: WrappedArray(null); 18/04/24 17:56:07 INFO TaskSetManager: Starting task 1.2 in stage 2.0 (TID 9, xx.xx.xx.27, executor 0, partition 1, PROCESS_LOCAL, 5371 bytes); 18/04/24 17:56:37 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on xx.xx.xx.27:46181 (size: 6.4 KB, free: 366.3 MB); 18/04/24 17:56:38 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on xx.xx.xx.27:46181 (size: 23.1 KB, free: 366.3 MB); 18/04/24 17:56:39 WARN TaskSetManager: Lost task 1.2 in stage 2.0 (TID 9, xx.xx.xx.27, executor 0): org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeS",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:34543,Availability,Error,Error,34543,"or.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. 01:44 DEBUG: [kryo] Write: WrappedArray(null); 18/04/24 17:56:39 INFO TaskSetManager: Starting task 1.3 in stage 2.0 (TID 10, xx.xx.xx.16, executor 3, partition 1, PROCESS_LOCAL, 5371 bytes); 18/04/24 17:56:39 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on xx.xx.xx.24:35903 (size: 6.4 KB, free: 366.3 MB); 18/04/24 17:56:39 INFO TaskSetManager: Lost task 1.3 in stage 2.0 (TID 10) on xx.xx.xx.16, executor 3: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile (Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory)) [duplicate 1]; 18/04/24 17:56:39 ERROR TaskSetManager: Task 1 in stage 2.0 failed 4 times; aborting job; 18/04/24 17:56:39 INFO TaskSchedulerImpl: Cancelling stage 2; 18/04/24 17:56:39 INFO TaskSchedulerImpl: Stage 2 was cancelled; 18/04/24 17:56:39 INFO DAGScheduler: ShuffleMapStage 2 (mapToPair at PSFilter.java:125) failed in 45.219 s due to Job aborted due to stage failure: Task 1 in stage 2.0 failed 4 times, most recent failure: Lost task 1.3 in stage 2.0 (TID 10, xx.xx.xx.16, executor 3): org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.Contai",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:34658,Availability,ERROR,ERROR,34658,"ion: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. 01:44 DEBUG: [kryo] Write: WrappedArray(null); 18/04/24 17:56:39 INFO TaskSetManager: Starting task 1.3 in stage 2.0 (TID 10, xx.xx.xx.16, executor 3, partition 1, PROCESS_LOCAL, 5371 bytes); 18/04/24 17:56:39 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on xx.xx.xx.24:35903 (size: 6.4 KB, free: 366.3 MB); 18/04/24 17:56:39 INFO TaskSetManager: Lost task 1.3 in stage 2.0 (TID 10) on xx.xx.xx.16, executor 3: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile (Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory)) [duplicate 1]; 18/04/24 17:56:39 ERROR TaskSetManager: Task 1 in stage 2.0 failed 4 times; aborting job; 18/04/24 17:56:39 INFO TaskSchedulerImpl: Cancelling stage 2; 18/04/24 17:56:39 INFO TaskSchedulerImpl: Stage 2 was cancelled; 18/04/24 17:56:39 INFO DAGScheduler: ShuffleMapStage 2 (mapToPair at PSFilter.java:125) failed in 45.219 s due to Job aborted due to stage failure: Task 1 in stage 2.0 failed 4 times, most recent failure: Lost task 1.3 in stage 2.0 (TID 10, xx.xx.xx.16, executor 3): org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.to",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:34996,Availability,failure,failure,34996,"tUtils.java:103); ... 16 more. 01:44 DEBUG: [kryo] Write: WrappedArray(null); 18/04/24 17:56:39 INFO TaskSetManager: Starting task 1.3 in stage 2.0 (TID 10, xx.xx.xx.16, executor 3, partition 1, PROCESS_LOCAL, 5371 bytes); 18/04/24 17:56:39 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on xx.xx.xx.24:35903 (size: 6.4 KB, free: 366.3 MB); 18/04/24 17:56:39 INFO TaskSetManager: Lost task 1.3 in stage 2.0 (TID 10) on xx.xx.xx.16, executor 3: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile (Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory)) [duplicate 1]; 18/04/24 17:56:39 ERROR TaskSetManager: Task 1 in stage 2.0 failed 4 times; aborting job; 18/04/24 17:56:39 INFO TaskSchedulerImpl: Cancelling stage 2; 18/04/24 17:56:39 INFO TaskSchedulerImpl: Stage 2 was cancelled; 18/04/24 17:56:39 INFO DAGScheduler: ShuffleMapStage 2 (mapToPair at PSFilter.java:125) failed in 45.219 s due to Job aborted due to stage failure: Task 1 in stage 2.0 failed 4 times, most recent failure: Lost task 1.3 in stage 2.0 (TID 10, xx.xx.xx.16, executor 3): org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:35053,Availability,failure,failure,35053,"WrappedArray(null); 18/04/24 17:56:39 INFO TaskSetManager: Starting task 1.3 in stage 2.0 (TID 10, xx.xx.xx.16, executor 3, partition 1, PROCESS_LOCAL, 5371 bytes); 18/04/24 17:56:39 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on xx.xx.xx.24:35903 (size: 6.4 KB, free: 366.3 MB); 18/04/24 17:56:39 INFO TaskSetManager: Lost task 1.3 in stage 2.0 (TID 10) on xx.xx.xx.16, executor 3: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile (Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory)) [duplicate 1]; 18/04/24 17:56:39 ERROR TaskSetManager: Task 1 in stage 2.0 failed 4 times; aborting job; 18/04/24 17:56:39 INFO TaskSchedulerImpl: Cancelling stage 2; 18/04/24 17:56:39 INFO TaskSchedulerImpl: Stage 2 was cancelled; 18/04/24 17:56:39 INFO DAGScheduler: ShuffleMapStage 2 (mapToPair at PSFilter.java:125) failed in 45.219 s due to Job aborted due to stage failure: Task 1 in stage 2.0 failed 4 times, most recent failure: Lost task 1.3 in stage 2.0 (TID 10, xx.xx.xx.16, executor 3): org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:35222,Availability,Error,Error,35222,"INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on xx.xx.xx.24:35903 (size: 6.4 KB, free: 366.3 MB); 18/04/24 17:56:39 INFO TaskSetManager: Lost task 1.3 in stage 2.0 (TID 10) on xx.xx.xx.16, executor 3: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile (Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory)) [duplicate 1]; 18/04/24 17:56:39 ERROR TaskSetManager: Task 1 in stage 2.0 failed 4 times; aborting job; 18/04/24 17:56:39 INFO TaskSchedulerImpl: Cancelling stage 2; 18/04/24 17:56:39 INFO TaskSchedulerImpl: Stage 2 was cancelled; 18/04/24 17:56:39 INFO DAGScheduler: ShuffleMapStage 2 (mapToPair at PSFilter.java:125) failed in 45.219 s due to Job aborted due to stage failure: Task 1 in stage 2.0 failed 4 times, most recent failure: Lost task 1.3 in stage 2.0 (TID 10, xx.xx.xx.16, executor 3): org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeS",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:37441,Availability,down,down,37441,"va:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. Driver stacktrace:; 18/04/24 17:56:39 INFO DAGScheduler: Job 2 failed: count at PathSeqPipelineSpark.java:245, took 45.308012 s; 18/04/24 17:56:39 INFO SparkUI: Stopped Spark web UI at http://xx.xx.xx.16:4040; 18/04/24 17:56:39 INFO StandaloneSchedulerBackend: Shutting down all executors; 18/04/24 17:56:39 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down; 18/04/24 17:56:39 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 18/04/24 17:56:39 INFO MemoryStore: MemoryStore cleared; 18/04/24 17:56:39 INFO BlockManager: BlockManager stopped; 18/04/24 17:56:39 INFO BlockManagerMaster: BlockManagerMaster stopped; 18/04/24 17:56:39 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/04/24 17:56:39 INFO SparkContext: Successfully stopped SparkContext; 17:56:39.758 INFO PathSeqPipelineSpark - Shutting down engine; [April 24, 2018 5:56:39 PM CEST] org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark done. Elapsed time: 1.75 minutes.; Runtime.totalMemory()=821559296; org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 2.0 failed 4 times, most recent failure: Lost task 1.3 in stage 2.0 (TID 10, xx.xx.xx.16, executor 3): org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUti",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:37559,Availability,down,down,37559,"va:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. Driver stacktrace:; 18/04/24 17:56:39 INFO DAGScheduler: Job 2 failed: count at PathSeqPipelineSpark.java:245, took 45.308012 s; 18/04/24 17:56:39 INFO SparkUI: Stopped Spark web UI at http://xx.xx.xx.16:4040; 18/04/24 17:56:39 INFO StandaloneSchedulerBackend: Shutting down all executors; 18/04/24 17:56:39 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down; 18/04/24 17:56:39 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 18/04/24 17:56:39 INFO MemoryStore: MemoryStore cleared; 18/04/24 17:56:39 INFO BlockManager: BlockManager stopped; 18/04/24 17:56:39 INFO BlockManagerMaster: BlockManagerMaster stopped; 18/04/24 17:56:39 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/04/24 17:56:39 INFO SparkContext: Successfully stopped SparkContext; 17:56:39.758 INFO PathSeqPipelineSpark - Shutting down engine; [April 24, 2018 5:56:39 PM CEST] org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark done. Elapsed time: 1.75 minutes.; Runtime.totalMemory()=821559296; org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 2.0 failed 4 times, most recent failure: Lost task 1.3 in stage 2.0 (TID 10, xx.xx.xx.16, executor 3): org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUti",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:38084,Availability,down,down,38084,"tute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. Driver stacktrace:; 18/04/24 17:56:39 INFO DAGScheduler: Job 2 failed: count at PathSeqPipelineSpark.java:245, took 45.308012 s; 18/04/24 17:56:39 INFO SparkUI: Stopped Spark web UI at http://xx.xx.xx.16:4040; 18/04/24 17:56:39 INFO StandaloneSchedulerBackend: Shutting down all executors; 18/04/24 17:56:39 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down; 18/04/24 17:56:39 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 18/04/24 17:56:39 INFO MemoryStore: MemoryStore cleared; 18/04/24 17:56:39 INFO BlockManager: BlockManager stopped; 18/04/24 17:56:39 INFO BlockManagerMaster: BlockManagerMaster stopped; 18/04/24 17:56:39 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/04/24 17:56:39 INFO SparkContext: Successfully stopped SparkContext; 17:56:39.758 INFO PathSeqPipelineSpark - Shutting down engine; [April 24, 2018 5:56:39 PM CEST] org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark done. Elapsed time: 1.75 minutes.; Runtime.totalMemory()=821559296; org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 2.0 failed 4 times, most recent failure: Lost task 1.3 in stage 2.0 (TID 10, xx.xx.xx.16, executor 3): org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.he",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:38327,Availability,failure,failure,38327,"FO SparkUI: Stopped Spark web UI at http://xx.xx.xx.16:4040; 18/04/24 17:56:39 INFO StandaloneSchedulerBackend: Shutting down all executors; 18/04/24 17:56:39 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down; 18/04/24 17:56:39 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 18/04/24 17:56:39 INFO MemoryStore: MemoryStore cleared; 18/04/24 17:56:39 INFO BlockManager: BlockManager stopped; 18/04/24 17:56:39 INFO BlockManagerMaster: BlockManagerMaster stopped; 18/04/24 17:56:39 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/04/24 17:56:39 INFO SparkContext: Successfully stopped SparkContext; 17:56:39.758 INFO PathSeqPipelineSpark - Shutting down engine; [April 24, 2018 5:56:39 PM CEST] org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark done. Elapsed time: 1.75 minutes.; Runtime.totalMemory()=821559296; org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 2.0 failed 4 times, most recent failure: Lost task 1.3 in stage 2.0 (TID 10, xx.xx.xx.16, executor 3): org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filte",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:38384,Availability,failure,failure,38384,"; 18/04/24 17:56:39 INFO StandaloneSchedulerBackend: Shutting down all executors; 18/04/24 17:56:39 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down; 18/04/24 17:56:39 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 18/04/24 17:56:39 INFO MemoryStore: MemoryStore cleared; 18/04/24 17:56:39 INFO BlockManager: BlockManager stopped; 18/04/24 17:56:39 INFO BlockManagerMaster: BlockManagerMaster stopped; 18/04/24 17:56:39 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/04/24 17:56:39 INFO SparkContext: Successfully stopped SparkContext; 17:56:39.758 INFO PathSeqPipelineSpark - Shutting down engine; [April 24, 2018 5:56:39 PM CEST] org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark done. Elapsed time: 1.75 minutes.; Runtime.totalMemory()=821559296; org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 2.0 failed 4 times, most recent failure: Lost task 1.3 in stage 2.0 (TID 10, xx.xx.xx.16, executor 3): org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:38553,Availability,Error,Error,38553,"n; 18/04/24 17:56:39 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 18/04/24 17:56:39 INFO MemoryStore: MemoryStore cleared; 18/04/24 17:56:39 INFO BlockManager: BlockManager stopped; 18/04/24 17:56:39 INFO BlockManagerMaster: BlockManagerMaster stopped; 18/04/24 17:56:39 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/04/24 17:56:39 INFO SparkContext: Successfully stopped SparkContext; 17:56:39.758 INFO PathSeqPipelineSpark - Shutting down engine; [April 24, 2018 5:56:39 PM CEST] org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark done. Elapsed time: 1.75 minutes.; Runtime.totalMemory()=821559296; org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 2.0 failed 4 times, most recent failure: Lost task 1.3 in stage 2.0 (TID 10, xx.xx.xx.16, executor 3): org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeS",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:43954,Availability,Error,Error,43954,98); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:755); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeS,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:5721,Deployability,Configurat,Configuration,5721,"EATE_MD5 : false; 17:54:55.301 INFO PathSeqPipelineSpark - HTSJDK Defaults.CUSTOM_READER_FACTORY :; 17:54:55.301 INFO PathSeqPipelineSpark - HTSJDK Defaults.DISABLE_SNAPPY_COMPRESSOR : false; 17:54:55.301 INFO PathSeqPipelineSpark - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 17:54:55.301 INFO PathSeqPipelineSpark - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 17:54:55.301 INFO PathSeqPipelineSpark - HTSJDK Defaults.REFERENCE_FASTA : null; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:54:55.302 DEBUG ConfigFactory - Configuration file values:; 17:54:55.320 DEBUG ConfigFactory - gcsMaxRetries = 20; 17:54:55.320 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:54:55.320 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:54:55.320 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:54:55.320 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:54:55.320 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:54:55.320 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:54:55.320 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:54:55.321 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:54:55.321 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:54:55.321 DEBUG ConfigFactory - createOutputBamIndex = true; 17:54:55.321 DEBUG ConfigFactory - g",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:7270,Deployability,patch,patch,7270,"iver.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:54:55.321 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:54:55.321 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:54:55.321 DEBUG ConfigFactory - createOutputBamIndex = true; 17:54:55.321 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 17:54:55.321 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 17:54:55.321 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 17:54:55.321 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 17:54:55.321 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 17:54:55.321 INFO PathSeqPipelineSpark - Initializing engine; 17:54:55.321 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 18/04/24 17:54:55 INFO SparkContext: Running Spark version 2.2.0; 18/04/24 17:54:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18/04/24 17:54:56 INFO SparkContext: Submitted application: PathSeqPipelineSpark; 18/04/24 17:54:56 INFO SecurityManager: Changing view acls to: userx; 18/04/24 17:54:56 INFO SecurityManager: Changing modify acls to: userx; 18/04/24 17:54:56 INFO SecurityManager: Changing view acls groups to:; 18/04/24 17:54:56 INFO SecurityManager: Changing modify acls groups to:; 18/04/24 17:54:56 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:12806,Deployability,update,updated,12806,"chedulerBackend: Granted executor ID app-20180424175501-0004/5 on hostPort xx.xx.xx.23:49023 with 16 cores, 1024.0 MB RAM; 18/04/24 17:55:01 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20180424175501-0004/6 on worker-20180424173107-xx.xx.xx.25-33478 (xx.xx.xx.25:33478) with 16 cores; 18/04/24 17:55:01 INFO NettyBlockTransferService: Server created on xx.xx.xx.16:49734; 18/04/24 17:55:01 INFO StandaloneSchedulerBackend: Granted executor ID app-20180424175501-0004/6 on hostPort xx.xx.xx.25:33478 with 16 cores, 1024.0 MB RAM; 18/04/24 17:55:01 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy; 18/04/24 17:55:01 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, xx.xx.xx.16, 49734, None); 18/04/24 17:55:01 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424175501-0004/1 is now RUNNING; 18/04/24 17:55:01 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424175501-0004/2 is now RUNNING; 18/04/24 17:55:01 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424175501-0004/3 is now RUNNING; 18/04/24 17:55:01 INFO BlockManagerMasterEndpoint: Registering block manager xx.xx.xx.16:49734 with 366.3 MB RAM, BlockManagerId(driver, xx.xx.xx.16, 49734, None); 18/04/24 17:55:01 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, xx.xx.xx.16, 49734, None); 18/04/24 17:55:01 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, xx.xx.xx.16, 49734, None); 18/04/24 17:55:01 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424175501-0004/4 is now RUNNING; 18/04/24 17:55:03 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0; 18/04/24 17:55:03 INFO GoogleHadoopFileSystemBase: GHFS version: 1.6.3-hadoop2; 18/04/24 17:55:04 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424175501-0004/5 is now ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:12925,Deployability,update,updated,12925,"chedulerBackend: Granted executor ID app-20180424175501-0004/5 on hostPort xx.xx.xx.23:49023 with 16 cores, 1024.0 MB RAM; 18/04/24 17:55:01 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20180424175501-0004/6 on worker-20180424173107-xx.xx.xx.25-33478 (xx.xx.xx.25:33478) with 16 cores; 18/04/24 17:55:01 INFO NettyBlockTransferService: Server created on xx.xx.xx.16:49734; 18/04/24 17:55:01 INFO StandaloneSchedulerBackend: Granted executor ID app-20180424175501-0004/6 on hostPort xx.xx.xx.25:33478 with 16 cores, 1024.0 MB RAM; 18/04/24 17:55:01 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy; 18/04/24 17:55:01 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, xx.xx.xx.16, 49734, None); 18/04/24 17:55:01 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424175501-0004/1 is now RUNNING; 18/04/24 17:55:01 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424175501-0004/2 is now RUNNING; 18/04/24 17:55:01 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424175501-0004/3 is now RUNNING; 18/04/24 17:55:01 INFO BlockManagerMasterEndpoint: Registering block manager xx.xx.xx.16:49734 with 366.3 MB RAM, BlockManagerId(driver, xx.xx.xx.16, 49734, None); 18/04/24 17:55:01 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, xx.xx.xx.16, 49734, None); 18/04/24 17:55:01 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, xx.xx.xx.16, 49734, None); 18/04/24 17:55:01 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424175501-0004/4 is now RUNNING; 18/04/24 17:55:03 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0; 18/04/24 17:55:03 INFO GoogleHadoopFileSystemBase: GHFS version: 1.6.3-hadoop2; 18/04/24 17:55:04 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424175501-0004/5 is now ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:13044,Deployability,update,updated,13044,"chedulerBackend: Granted executor ID app-20180424175501-0004/5 on hostPort xx.xx.xx.23:49023 with 16 cores, 1024.0 MB RAM; 18/04/24 17:55:01 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20180424175501-0004/6 on worker-20180424173107-xx.xx.xx.25-33478 (xx.xx.xx.25:33478) with 16 cores; 18/04/24 17:55:01 INFO NettyBlockTransferService: Server created on xx.xx.xx.16:49734; 18/04/24 17:55:01 INFO StandaloneSchedulerBackend: Granted executor ID app-20180424175501-0004/6 on hostPort xx.xx.xx.25:33478 with 16 cores, 1024.0 MB RAM; 18/04/24 17:55:01 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy; 18/04/24 17:55:01 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, xx.xx.xx.16, 49734, None); 18/04/24 17:55:01 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424175501-0004/1 is now RUNNING; 18/04/24 17:55:01 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424175501-0004/2 is now RUNNING; 18/04/24 17:55:01 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424175501-0004/3 is now RUNNING; 18/04/24 17:55:01 INFO BlockManagerMasterEndpoint: Registering block manager xx.xx.xx.16:49734 with 366.3 MB RAM, BlockManagerId(driver, xx.xx.xx.16, 49734, None); 18/04/24 17:55:01 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, xx.xx.xx.16, 49734, None); 18/04/24 17:55:01 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, xx.xx.xx.16, 49734, None); 18/04/24 17:55:01 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424175501-0004/4 is now RUNNING; 18/04/24 17:55:03 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0; 18/04/24 17:55:03 INFO GoogleHadoopFileSystemBase: GHFS version: 1.6.3-hadoop2; 18/04/24 17:55:04 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424175501-0004/5 is now ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:13557,Deployability,update,updated,13557,"olicy; 18/04/24 17:55:01 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, xx.xx.xx.16, 49734, None); 18/04/24 17:55:01 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424175501-0004/1 is now RUNNING; 18/04/24 17:55:01 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424175501-0004/2 is now RUNNING; 18/04/24 17:55:01 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424175501-0004/3 is now RUNNING; 18/04/24 17:55:01 INFO BlockManagerMasterEndpoint: Registering block manager xx.xx.xx.16:49734 with 366.3 MB RAM, BlockManagerId(driver, xx.xx.xx.16, 49734, None); 18/04/24 17:55:01 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, xx.xx.xx.16, 49734, None); 18/04/24 17:55:01 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, xx.xx.xx.16, 49734, None); 18/04/24 17:55:01 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424175501-0004/4 is now RUNNING; 18/04/24 17:55:03 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0; 18/04/24 17:55:03 INFO GoogleHadoopFileSystemBase: GHFS version: 1.6.3-hadoop2; 18/04/24 17:55:04 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424175501-0004/5 is now RUNNING; 18/04/24 17:55:05 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 276.0 KB, free 366.0 MB); 00:10 DEBUG: [kryo] Write: SerializableConfiguration; 18/04/24 17:55:05 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.1 KB, free 366.0 MB); 18/04/24 17:55:05 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on xx.xx.xx.16:49734 (size: 23.1 KB, free: 366.3 MB); 18/04/24 17:55:05 INFO SparkContext: Created broadcast 0 from newAPIHadoopFile at ReadsSparkSource.java:113; 18/04/24 17:55:06 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424175501-0004/0 is now RU",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:13906,Deployability,update,updated,13906,"0424175501-0004/2 is now RUNNING; 18/04/24 17:55:01 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424175501-0004/3 is now RUNNING; 18/04/24 17:55:01 INFO BlockManagerMasterEndpoint: Registering block manager xx.xx.xx.16:49734 with 366.3 MB RAM, BlockManagerId(driver, xx.xx.xx.16, 49734, None); 18/04/24 17:55:01 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, xx.xx.xx.16, 49734, None); 18/04/24 17:55:01 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, xx.xx.xx.16, 49734, None); 18/04/24 17:55:01 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424175501-0004/4 is now RUNNING; 18/04/24 17:55:03 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0; 18/04/24 17:55:03 INFO GoogleHadoopFileSystemBase: GHFS version: 1.6.3-hadoop2; 18/04/24 17:55:04 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424175501-0004/5 is now RUNNING; 18/04/24 17:55:05 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 276.0 KB, free 366.0 MB); 00:10 DEBUG: [kryo] Write: SerializableConfiguration; 18/04/24 17:55:05 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.1 KB, free 366.0 MB); 18/04/24 17:55:05 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on xx.xx.xx.16:49734 (size: 23.1 KB, free: 366.3 MB); 18/04/24 17:55:05 INFO SparkContext: Created broadcast 0 from newAPIHadoopFile at ReadsSparkSource.java:113; 18/04/24 17:55:06 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424175501-0004/0 is now RUNNING; 18/04/24 17:55:06 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424175501-0004/6 is now RUNNING; 18/04/24 17:55:07 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (xx.xx.xx.25:54754) with ID 2; 18/04/24 17:55:07 INFO BlockManagerMast",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:14569,Deployability,update,updated,14569," for scheduling beginning after reached minRegisteredResourcesRatio: 0.0; 18/04/24 17:55:03 INFO GoogleHadoopFileSystemBase: GHFS version: 1.6.3-hadoop2; 18/04/24 17:55:04 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424175501-0004/5 is now RUNNING; 18/04/24 17:55:05 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 276.0 KB, free 366.0 MB); 00:10 DEBUG: [kryo] Write: SerializableConfiguration; 18/04/24 17:55:05 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.1 KB, free 366.0 MB); 18/04/24 17:55:05 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on xx.xx.xx.16:49734 (size: 23.1 KB, free: 366.3 MB); 18/04/24 17:55:05 INFO SparkContext: Created broadcast 0 from newAPIHadoopFile at ReadsSparkSource.java:113; 18/04/24 17:55:06 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424175501-0004/0 is now RUNNING; 18/04/24 17:55:06 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424175501-0004/6 is now RUNNING; 18/04/24 17:55:07 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (xx.xx.xx.25:54754) with ID 2; 18/04/24 17:55:07 INFO BlockManagerMasterEndpoint: Registering block manager xx.xx.xx.25:41354 with 366.3 MB RAM, BlockManagerId(2, xx.xx.xx.25, 41354, None); 18/04/24 17:55:07 INFO FileInputFormat: Total input paths to process : 1; 18/04/24 17:55:07 INFO SparkContext: Starting job: first at ReadsSparkSource.java:221; 18/04/24 17:55:07 INFO DAGScheduler: Got job 0 (first at ReadsSparkSource.java:221) with 1 output partitions; 18/04/24 17:55:07 INFO DAGScheduler: Final stage: ResultStage 0 (first at ReadsSparkSource.java:221); 18/04/24 17:55:07 INFO DAGScheduler: Parents of final stage: List(); 18/04/24 17:55:07 INFO DAGScheduler: Missing parents: List(); 18/04/24 17:55:07 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at filter at ReadsSparkSource.java",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:14688,Deployability,update,updated,14688," for scheduling beginning after reached minRegisteredResourcesRatio: 0.0; 18/04/24 17:55:03 INFO GoogleHadoopFileSystemBase: GHFS version: 1.6.3-hadoop2; 18/04/24 17:55:04 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424175501-0004/5 is now RUNNING; 18/04/24 17:55:05 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 276.0 KB, free 366.0 MB); 00:10 DEBUG: [kryo] Write: SerializableConfiguration; 18/04/24 17:55:05 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.1 KB, free 366.0 MB); 18/04/24 17:55:05 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on xx.xx.xx.16:49734 (size: 23.1 KB, free: 366.3 MB); 18/04/24 17:55:05 INFO SparkContext: Created broadcast 0 from newAPIHadoopFile at ReadsSparkSource.java:113; 18/04/24 17:55:06 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424175501-0004/0 is now RUNNING; 18/04/24 17:55:06 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424175501-0004/6 is now RUNNING; 18/04/24 17:55:07 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (xx.xx.xx.25:54754) with ID 2; 18/04/24 17:55:07 INFO BlockManagerMasterEndpoint: Registering block manager xx.xx.xx.25:41354 with 366.3 MB RAM, BlockManagerId(2, xx.xx.xx.25, 41354, None); 18/04/24 17:55:07 INFO FileInputFormat: Total input paths to process : 1; 18/04/24 17:55:07 INFO SparkContext: Starting job: first at ReadsSparkSource.java:221; 18/04/24 17:55:07 INFO DAGScheduler: Got job 0 (first at ReadsSparkSource.java:221) with 1 output partitions; 18/04/24 17:55:07 INFO DAGScheduler: Final stage: ResultStage 0 (first at ReadsSparkSource.java:221); 18/04/24 17:55:07 INFO DAGScheduler: Parents of final stage: List(); 18/04/24 17:55:07 INFO DAGScheduler: Missing parents: List(); 18/04/24 17:55:07 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at filter at ReadsSparkSource.java",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:43476,Deployability,deploy,deploy,43476,ine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:387); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:755); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadin,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:43513,Deployability,deploy,deploy,43513,Tool.java:387); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:755); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.Contai,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:43585,Deployability,deploy,deploy,43585,arkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:755); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15);,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:43661,Deployability,deploy,deploy,43661,nstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:755); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:43732,Deployability,deploy,deploy,43732,am.java:134); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:755); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaR,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:43801,Deployability,deploy,deploy,43801,gram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:755); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:2020,Energy Efficiency,schedul,scheduler,2020,e or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112)**; at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more; ```. Thank you. Full log:; ````; 17:54:54.447 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PRO,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:2099,Energy Efficiency,schedul,scheduler,2099,e(BucketUtils.java:112)**; at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more; ```. Thank you. Full log:; ````; 17:54:54.447 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop conn,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:2178,Energy Efficiency,schedul,scheduler,2178,.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more; ```. Thank you. Full log:; ````; 17:54:54.447 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 17:54:54.891 INFO NativeLibraryLoader - ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:13659,Energy Efficiency,Schedul,SchedulerBackend,13659,"olicy; 18/04/24 17:55:01 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, xx.xx.xx.16, 49734, None); 18/04/24 17:55:01 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424175501-0004/1 is now RUNNING; 18/04/24 17:55:01 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424175501-0004/2 is now RUNNING; 18/04/24 17:55:01 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424175501-0004/3 is now RUNNING; 18/04/24 17:55:01 INFO BlockManagerMasterEndpoint: Registering block manager xx.xx.xx.16:49734 with 366.3 MB RAM, BlockManagerId(driver, xx.xx.xx.16, 49734, None); 18/04/24 17:55:01 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, xx.xx.xx.16, 49734, None); 18/04/24 17:55:01 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, xx.xx.xx.16, 49734, None); 18/04/24 17:55:01 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424175501-0004/4 is now RUNNING; 18/04/24 17:55:03 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0; 18/04/24 17:55:03 INFO GoogleHadoopFileSystemBase: GHFS version: 1.6.3-hadoop2; 18/04/24 17:55:04 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424175501-0004/5 is now RUNNING; 18/04/24 17:55:05 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 276.0 KB, free 366.0 MB); 00:10 DEBUG: [kryo] Write: SerializableConfiguration; 18/04/24 17:55:05 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.1 KB, free 366.0 MB); 18/04/24 17:55:05 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on xx.xx.xx.16:49734 (size: 23.1 KB, free: 366.3 MB); 18/04/24 17:55:05 INFO SparkContext: Created broadcast 0 from newAPIHadoopFile at ReadsSparkSource.java:113; 18/04/24 17:55:06 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424175501-0004/0 is now RU",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:13689,Energy Efficiency,schedul,scheduling,13689,"olicy; 18/04/24 17:55:01 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, xx.xx.xx.16, 49734, None); 18/04/24 17:55:01 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424175501-0004/1 is now RUNNING; 18/04/24 17:55:01 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424175501-0004/2 is now RUNNING; 18/04/24 17:55:01 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424175501-0004/3 is now RUNNING; 18/04/24 17:55:01 INFO BlockManagerMasterEndpoint: Registering block manager xx.xx.xx.16:49734 with 366.3 MB RAM, BlockManagerId(driver, xx.xx.xx.16, 49734, None); 18/04/24 17:55:01 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, xx.xx.xx.16, 49734, None); 18/04/24 17:55:01 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, xx.xx.xx.16, 49734, None); 18/04/24 17:55:01 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424175501-0004/4 is now RUNNING; 18/04/24 17:55:03 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0; 18/04/24 17:55:03 INFO GoogleHadoopFileSystemBase: GHFS version: 1.6.3-hadoop2; 18/04/24 17:55:04 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424175501-0004/5 is now RUNNING; 18/04/24 17:55:05 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 276.0 KB, free 366.0 MB); 00:10 DEBUG: [kryo] Write: SerializableConfiguration; 18/04/24 17:55:05 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.1 KB, free 366.0 MB); 18/04/24 17:55:05 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on xx.xx.xx.16:49734 (size: 23.1 KB, free: 366.3 MB); 18/04/24 17:55:05 INFO SparkContext: Created broadcast 0 from newAPIHadoopFile at ReadsSparkSource.java:113; 18/04/24 17:55:06 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424175501-0004/0 is now RU",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:26035,Energy Efficiency,schedul,scheduler,26035,ile or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more**. 00:59 DEBUG: [kryo] Write: WrappedArray(null); 18/04/24 17:55:54 INFO TaskSetManager: Starting task 1.1 in stage,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:26114,Energy Efficiency,schedul,scheduler,26114,"ile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more**. 00:59 DEBUG: [kryo] Write: WrappedArray(null); 18/04/24 17:55:54 INFO TaskSetManager: Starting task 1.1 in stage 2.0 (TID 5, xx.xx.xx.24, executor 1, partition 1, PROCESS_LOCAL, 5371 bytes); ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:26193,Energy Efficiency,schedul,scheduler,26193,".PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more**. 00:59 DEBUG: [kryo] Write: WrappedArray(null); 18/04/24 17:55:54 INFO TaskSetManager: Starting task 1.1 in stage 2.0 (TID 5, xx.xx.xx.24, executor 1, partition 1, PROCESS_LOCAL, 5371 bytes); 18/04/24 17:55:54 INFO TaskSetManager: Lost task 0.0 in stage 2.0 (TID 3) on xx",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:30533,Energy Efficiency,schedul,scheduler,30533,ile or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. 01:12 DEBUG: [kryo] Write: WrappedArray(null); 18/04/24 17:56:07 INFO TaskSetManager: Starting task 1.2 in stage 2,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:30612,Energy Efficiency,schedul,scheduler,30612,"ile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. 01:12 DEBUG: [kryo] Write: WrappedArray(null); 18/04/24 17:56:07 INFO TaskSetManager: Starting task 1.2 in stage 2.0 (TID 9, xx.xx.xx.27, executor 0, partition 1, PROCESS_LOCAL, 5371 bytes); 18",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:30691,Energy Efficiency,schedul,scheduler,30691,".PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. 01:12 DEBUG: [kryo] Write: WrappedArray(null); 18/04/24 17:56:07 INFO TaskSetManager: Starting task 1.2 in stage 2.0 (TID 9, xx.xx.xx.27, executor 0, partition 1, PROCESS_LOCAL, 5371 bytes); 18/04/24 17:56:37 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on xx",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:33130,Energy Efficiency,schedul,scheduler,33130,ile or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. 01:44 DEBUG: [kryo] Write: WrappedArray(null); 18/04/24 17:56:39 INFO TaskSetManager: Starting task 1.3 in stage 2,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:33209,Energy Efficiency,schedul,scheduler,33209,"ile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. 01:44 DEBUG: [kryo] Write: WrappedArray(null); 18/04/24 17:56:39 INFO TaskSetManager: Starting task 1.3 in stage 2.0 (TID 10, xx.xx.xx.16, executor 3, partition 1, PROCESS_LOCAL, 5371 bytes); 1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:33288,Energy Efficiency,schedul,scheduler,33288,".PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. 01:44 DEBUG: [kryo] Write: WrappedArray(null); 18/04/24 17:56:39 INFO TaskSetManager: Starting task 1.3 in stage 2.0 (TID 10, xx.xx.xx.16, executor 3, partition 1, PROCESS_LOCAL, 5371 bytes); 18/04/24 17:56:39 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on x",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:36280,Energy Efficiency,schedul,scheduler,36280,"ile or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. Driver stacktrace:; 18/04/24 17:56:39 INFO DAGScheduler: Job 2 failed: count at PathSeqPipelineSpark.java:245, too",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:36359,Energy Efficiency,schedul,scheduler,36359,"ile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. Driver stacktrace:; 18/04/24 17:56:39 INFO DAGScheduler: Job 2 failed: count at PathSeqPipelineSpark.java:245, took 45.308012 s; 18/04/24 17:56:39 INFO SparkUI: Stopped Spark web UI at http://x",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:36438,Energy Efficiency,schedul,scheduler,36438,".PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. Driver stacktrace:; 18/04/24 17:56:39 INFO DAGScheduler: Job 2 failed: count at PathSeqPipelineSpark.java:245, took 45.308012 s; 18/04/24 17:56:39 INFO SparkUI: Stopped Spark web UI at http://xx.xx.xx.16:4040; 18/04/24 17:56:39 INFO StandaloneSchedulerBackend: Shutting do",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:39611,Energy Efficiency,schedul,scheduler,39611,ile or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAnd,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:39690,Energy Efficiency,schedul,scheduler,39690,ile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGSc,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:39769,Energy Efficiency,schedul,scheduler,39769,.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spa,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:40542,Energy Efficiency,schedul,scheduler,40542,ite(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.s,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:40582,Energy Efficiency,schedul,scheduler,40582,park.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSchedule,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:40680,Energy Efficiency,schedul,scheduler,40680,park.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onRec,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:40777,Energy Efficiency,schedul,scheduler,40777,k.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:41028,Energy Efficiency,schedul,scheduler,41028,or.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); at org.apache.spark.SparkContext.runJob(Sp,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:41108,Energy Efficiency,schedul,scheduler,41108,otFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); at org.apache.spark.SparkContext.runJob(SparkContext.sca,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:41213,Energy Efficiency,schedul,scheduler,41213,d); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2087); at org.apache.spark.rdd.RDD.count(RDD.scala:1158); at org.apache.spark.api.java.JavaRDDLike$cla,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:41361,Energy Efficiency,schedul,scheduler,41361,am.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2087); at org.apache.spark.rdd.RDD.count(RDD.scala:1158); at org.apache.spark.api.java.JavaRDDLike$class.count(JavaRDDLike.scala:455); at org.apache.spark.api.java.AbstractJavaRDDLike.count(JavaRDDLike.scala:45); at org.broadinstitute.hellbender.tool,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:41449,Energy Efficiency,schedul,scheduler,41449,ls.openFile(BucketUtils.java:103); ... 16 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2087); at org.apache.spark.rdd.RDD.count(RDD.scala:1158); at org.apache.spark.api.java.JavaRDDLike$class.count(JavaRDDLike.scala:455); at org.apache.spark.api.java.AbstractJavaRDDLike.count(JavaRDDLike.scala:45); at org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark.runTool(PathSeqPipelineSpark.java:245); at org.broa,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:41546,Energy Efficiency,schedul,scheduler,41546,.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2087); at org.apache.spark.rdd.RDD.count(RDD.scala:1158); at org.apache.spark.api.java.JavaRDDLike$class.count(JavaRDDLike.scala:455); at org.apache.spark.api.java.AbstractJavaRDDLike.count(JavaRDDLike.scala:45); at org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark.runTool(PathSeqPipelineSpark.java:245); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:387); at org.broa,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:41641,Energy Efficiency,schedul,scheduler,41641,.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2087); at org.apache.spark.rdd.RDD.count(RDD.scala:1158); at org.apache.spark.api.java.JavaRDDLike$class.count(JavaRDDLike.scala:455); at org.apache.spark.api.java.AbstractJavaRDDLike.count(JavaRDDLike.scala:45); at org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark.runTool(PathSeqPipelineSpark.java:245); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:387); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:41804,Energy Efficiency,schedul,scheduler,41804,abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2087); at org.apache.spark.rdd.RDD.count(RDD.scala:1158); at org.apache.spark.api.java.JavaRDDLike$class.count(JavaRDDLike.scala:455); at org.apache.spark.api.java.AbstractJavaRDDLike.count(JavaRDDLike.scala:45); at org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark.runTool(PathSeqPipelineSpark.java:245); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:387); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:45012,Energy Efficiency,schedul,scheduler,45012,.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ```; `,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:45091,Energy Efficiency,schedul,scheduler,45091,.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ```; `,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:45170,Energy Efficiency,schedul,scheduler,45170,.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ```; `,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:24062,Integrability,Wrap,WrappedArray,24062,"er: Missing parents: List(ShuffleMapStage 6); 18/04/24 17:55:54 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[19] at mapToPair at PSFilter.java:125), which has no missing parents; 18/04/24 17:55:54 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 14.2 KB, free 366.0 MB); 00:59 DEBUG: [kryo] Write: byte[]; 18/04/24 17:55:54 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.4 KB, free 366.0 MB); 18/04/24 17:55:54 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on xx.xx.xx.16:49734 (size: 6.4 KB, free: 366.3 MB); 18/04/24 17:55:54 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1006; 18/04/24 17:55:54 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[19] at mapToPair at PSFilter.java:125) (first 15 tasks are for partitions Vector(0, 1)); 18/04/24 17:55:54 INFO TaskSchedulerImpl: Adding task set 2.0 with 2 tasks; 00:59 DEBUG: [kryo] Write: WrappedArray([NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED, NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED]); 18/04/24 17:55:54 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 3, xx.xx.xx.25, executor 2, partition 0, PROCESS_LOCAL, 6010 bytes); 00:59 DEBUG: [kryo] Write: WrappedArray(null); 18/04/24 17:55:54 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 4, xx.xx.xx.16, executor 3, partition 1, PROCESS_LOCAL, 5371 bytes); 18/04/24 17:55:54 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on xx.xx.xx.16:39037 (size: 6.4 KB, free: 366.3 MB); 18/04/24 17:55:54 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on xx.xx.xx.25:41354 (size: 6.4 KB, free: 366.3 MB); **18/04/24 17:55:54 WARN TaskSetManager: Lost task 1.0 in stage 2.0 (TID 4, xx.xx.xx.16, executor 3): org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or direct",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:24355,Integrability,Wrap,WrappedArray,24355,"4.2 KB, free 366.0 MB); 00:59 DEBUG: [kryo] Write: byte[]; 18/04/24 17:55:54 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.4 KB, free 366.0 MB); 18/04/24 17:55:54 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on xx.xx.xx.16:49734 (size: 6.4 KB, free: 366.3 MB); 18/04/24 17:55:54 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1006; 18/04/24 17:55:54 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[19] at mapToPair at PSFilter.java:125) (first 15 tasks are for partitions Vector(0, 1)); 18/04/24 17:55:54 INFO TaskSchedulerImpl: Adding task set 2.0 with 2 tasks; 00:59 DEBUG: [kryo] Write: WrappedArray([NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED, NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED]); 18/04/24 17:55:54 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 3, xx.xx.xx.25, executor 2, partition 0, PROCESS_LOCAL, 6010 bytes); 00:59 DEBUG: [kryo] Write: WrappedArray(null); 18/04/24 17:55:54 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 4, xx.xx.xx.16, executor 3, partition 1, PROCESS_LOCAL, 5371 bytes); 18/04/24 17:55:54 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on xx.xx.xx.16:39037 (size: 6.4 KB, free: 366.3 MB); 18/04/24 17:55:54 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on xx.xx.xx.25:41354 (size: 6.4 KB, free: 366.3 MB); **18/04/24 17:55:54 WARN TaskSetManager: Lost task 1.0 in stage 2.0 (TID 4, xx.xx.xx.16, executor 3): org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFil",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:26955,Integrability,Wrap,WrappedArray,26955,"assMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more**. 00:59 DEBUG: [kryo] Write: WrappedArray(null); 18/04/24 17:55:54 INFO TaskSetManager: Starting task 1.1 in stage 2.0 (TID 5, xx.xx.xx.24, executor 1, partition 1, PROCESS_LOCAL, 5371 bytes); 18/04/24 17:55:54 INFO TaskSetManager: Lost task 0.0 in stage 2.0 (TID 3) on xx.xx.xx.25, executor 2: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile (Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory)) [duplicate 1]; 01:00 DEBUG: [kryo] Write: WrappedArray([NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED, NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED]); 18/04/24 17:55:54 INFO TaskSetManager: Starting task 0.1 in stage 2.0 (TID 6, xx.xx.xx.16, executor 3, partition 0, PROCESS_LOCAL, 6010 bytes); 18/04/24 17:55:55 INFO TaskSetManager: Lost task 0.1 in stage 2.0 (TID 6) on xx.xx.xx.16, executor 3: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile (Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:27443,Integrability,Wrap,WrappedArray,27443,"PoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more**. 00:59 DEBUG: [kryo] Write: WrappedArray(null); 18/04/24 17:55:54 INFO TaskSetManager: Starting task 1.1 in stage 2.0 (TID 5, xx.xx.xx.24, executor 1, partition 1, PROCESS_LOCAL, 5371 bytes); 18/04/24 17:55:54 INFO TaskSetManager: Lost task 0.0 in stage 2.0 (TID 3) on xx.xx.xx.25, executor 2: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile (Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory)) [duplicate 1]; 01:00 DEBUG: [kryo] Write: WrappedArray([NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED, NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED]); 18/04/24 17:55:54 INFO TaskSetManager: Starting task 0.1 in stage 2.0 (TID 6, xx.xx.xx.16, executor 3, partition 0, PROCESS_LOCAL, 6010 bytes); 18/04/24 17:55:55 INFO TaskSetManager: Lost task 0.1 in stage 2.0 (TID 6) on xx.xx.xx.16, executor 3: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile (Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory)) [duplicate 2]; 01:00 DEBUG: [kryo] Write: WrappedArray([NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED, NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED]); 18/04/24 17:55:55 INFO TaskSetManager: Starting task 0.2 in stage 2.0 (TID 7, xx.xx.xx.23, executor 5, partition 0, PROCESS_LOCAL, 6010 bytes); 18/04/24 17:55:55 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on xx.xx.xx.23:42535 (size: 6.4 KB, free: 3",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:28033,Integrability,Wrap,WrappedArray,28033,"nager: Starting task 1.1 in stage 2.0 (TID 5, xx.xx.xx.24, executor 1, partition 1, PROCESS_LOCAL, 5371 bytes); 18/04/24 17:55:54 INFO TaskSetManager: Lost task 0.0 in stage 2.0 (TID 3) on xx.xx.xx.25, executor 2: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile (Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory)) [duplicate 1]; 01:00 DEBUG: [kryo] Write: WrappedArray([NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED, NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED]); 18/04/24 17:55:54 INFO TaskSetManager: Starting task 0.1 in stage 2.0 (TID 6, xx.xx.xx.16, executor 3, partition 0, PROCESS_LOCAL, 6010 bytes); 18/04/24 17:55:55 INFO TaskSetManager: Lost task 0.1 in stage 2.0 (TID 6) on xx.xx.xx.16, executor 3: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile (Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory)) [duplicate 2]; 01:00 DEBUG: [kryo] Write: WrappedArray([NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED, NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED]); 18/04/24 17:55:55 INFO TaskSetManager: Starting task 0.2 in stage 2.0 (TID 7, xx.xx.xx.23, executor 5, partition 0, PROCESS_LOCAL, 6010 bytes); 18/04/24 17:55:55 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on xx.xx.xx.23:42535 (size: 6.4 KB, free: 366.3 MB); 18/04/24 17:55:55 INFO TaskSetManager: Lost task 0.2 in stage 2.0 (TID 7) on xx.xx.xx.23, executor 5: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile (Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory)) [duplicate 3]; 01:00 DEBUG: [kryo] Write: WrappedArray([NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED, NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED]); 18/04/24 17:55:55 INFO TaskSetManager: Starting task 0.3 in stage 2.0 (TID 8, xx.xx.xx.24, executor 4, partition 0, PROCESS_LOCAL, 601",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:28752,Integrability,Wrap,WrappedArray,28752," INFO TaskSetManager: Lost task 0.1 in stage 2.0 (TID 6) on xx.xx.xx.16, executor 3: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile (Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory)) [duplicate 2]; 01:00 DEBUG: [kryo] Write: WrappedArray([NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED, NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED]); 18/04/24 17:55:55 INFO TaskSetManager: Starting task 0.2 in stage 2.0 (TID 7, xx.xx.xx.23, executor 5, partition 0, PROCESS_LOCAL, 6010 bytes); 18/04/24 17:55:55 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on xx.xx.xx.23:42535 (size: 6.4 KB, free: 366.3 MB); 18/04/24 17:55:55 INFO TaskSetManager: Lost task 0.2 in stage 2.0 (TID 7) on xx.xx.xx.23, executor 5: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile (Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory)) [duplicate 3]; 01:00 DEBUG: [kryo] Write: WrappedArray([NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED, NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED]); 18/04/24 17:55:55 INFO TaskSetManager: Starting task 0.3 in stage 2.0 (TID 8, xx.xx.xx.24, executor 4, partition 0, PROCESS_LOCAL, 6010 bytes); 18/04/24 17:56:00 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on xx.xx.xx.24:49966 (size: 6.4 KB, free: 366.3 MB); 18/04/24 17:56:04 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on xx.xx.xx.24:49966 (size: 23.1 KB, free: 366.3 MB); 18/04/24 17:56:07 WARN TaskSetManager: Lost task 1.1 in stage 2.0 (TID 5, xx.xx.xx.24, executor 1): org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSK",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:31451,Integrability,Wrap,WrappedArray,31451,"ypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. 01:12 DEBUG: [kryo] Write: WrappedArray(null); 18/04/24 17:56:07 INFO TaskSetManager: Starting task 1.2 in stage 2.0 (TID 9, xx.xx.xx.27, executor 0, partition 1, PROCESS_LOCAL, 5371 bytes); 18/04/24 17:56:37 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on xx.xx.xx.27:46181 (size: 6.4 KB, free: 366.3 MB); 18/04/24 17:56:38 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on xx.xx.xx.27:46181 (size: 23.1 KB, free: 366.3 MB); 18/04/24 17:56:39 WARN TaskSetManager: Lost task 1.2 in stage 2.0 (TID 9, xx.xx.xx.27, executor 0): org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadins",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:34048,Integrability,Wrap,WrappedArray,34048,"ypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. 01:44 DEBUG: [kryo] Write: WrappedArray(null); 18/04/24 17:56:39 INFO TaskSetManager: Starting task 1.3 in stage 2.0 (TID 10, xx.xx.xx.16, executor 3, partition 1, PROCESS_LOCAL, 5371 bytes); 18/04/24 17:56:39 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on xx.xx.xx.24:35903 (size: 6.4 KB, free: 366.3 MB); 18/04/24 17:56:39 INFO TaskSetManager: Lost task 1.3 in stage 2.0 (TID 10) on xx.xx.xx.16, executor 3: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile (Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory)) [duplicate 1]; 18/04/24 17:56:39 ERROR TaskSetManager: Task 1 in stage 2.0 failed 4 times; aborting job; 18/04/24 17:56:39 INFO TaskSchedulerImpl: Cancelling stage 2; 18/04/24 17:56:39 INFO TaskSchedulerImpl: Stage 2 was cancelled; 18/04/24 17:56:39 INFO DAGScheduler: ShuffleMapStage 2 (mapToPair at PSFilter.java:125) failed in 45.219 s due to Job aborted due to stage failure: Task 1 in stage 2.0 failed 4 times, most recent failure: Lost task ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:2996,Modifiability,variab,variables,2996,(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more; ```. Thank you. Full log:; ````; 17:54:54.447 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 17:54:54.891 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/scratch/home/int/eva/userx/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 17:54:54.924 DEBUG NativeLibraryLoader - Extracting libgkl_compression.so to /tmp/userx/libgkl_compression2910983555987484852.so; 17:54:55.293 INFO PathSeqPipelineSpark - ------------------------------------------------------------; 17:54:55.294 INFO PathSeqPipelineSpark - The Genome Analysis Toolkit (GATK) v4.0.3.0; 17:54:55.294 INFO PathSeqPipelineSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 17:54:55.295 INFO PathSeqPipelineSpark - Executing as userx@node016 on Linux v2.6.32-220.4.1.el6.x86_64 amd64; 17:54:55.295 INFO PathSeqPipelineSpark - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_131-b11; 17:54:55.295 INFO PathSeqPipelineSpark - Start Dat,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:3122,Modifiability,config,configured,3122,(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more; ```. Thank you. Full log:; ````; 17:54:54.447 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 17:54:54.891 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/scratch/home/int/eva/userx/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 17:54:54.924 DEBUG NativeLibraryLoader - Extracting libgkl_compression.so to /tmp/userx/libgkl_compression2910983555987484852.so; 17:54:55.293 INFO PathSeqPipelineSpark - ------------------------------------------------------------; 17:54:55.294 INFO PathSeqPipelineSpark - The Genome Analysis Toolkit (GATK) v4.0.3.0; 17:54:55.294 INFO PathSeqPipelineSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 17:54:55.295 INFO PathSeqPipelineSpark - Executing as userx@node016 on Linux v2.6.32-220.4.1.el6.x86_64 amd64; 17:54:55.295 INFO PathSeqPipelineSpark - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_131-b11; 17:54:55.295 INFO PathSeqPipelineSpark - Start Dat,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:5705,Modifiability,Config,ConfigFactory,5705,"EATE_MD5 : false; 17:54:55.301 INFO PathSeqPipelineSpark - HTSJDK Defaults.CUSTOM_READER_FACTORY :; 17:54:55.301 INFO PathSeqPipelineSpark - HTSJDK Defaults.DISABLE_SNAPPY_COMPRESSOR : false; 17:54:55.301 INFO PathSeqPipelineSpark - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 17:54:55.301 INFO PathSeqPipelineSpark - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 17:54:55.301 INFO PathSeqPipelineSpark - HTSJDK Defaults.REFERENCE_FASTA : null; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:54:55.302 DEBUG ConfigFactory - Configuration file values:; 17:54:55.320 DEBUG ConfigFactory - gcsMaxRetries = 20; 17:54:55.320 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:54:55.320 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:54:55.320 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:54:55.320 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:54:55.320 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:54:55.320 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:54:55.320 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:54:55.321 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:54:55.321 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:54:55.321 DEBUG ConfigFactory - createOutputBamIndex = true; 17:54:55.321 DEBUG ConfigFactory - g",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:5721,Modifiability,Config,Configuration,5721,"EATE_MD5 : false; 17:54:55.301 INFO PathSeqPipelineSpark - HTSJDK Defaults.CUSTOM_READER_FACTORY :; 17:54:55.301 INFO PathSeqPipelineSpark - HTSJDK Defaults.DISABLE_SNAPPY_COMPRESSOR : false; 17:54:55.301 INFO PathSeqPipelineSpark - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 17:54:55.301 INFO PathSeqPipelineSpark - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 17:54:55.301 INFO PathSeqPipelineSpark - HTSJDK Defaults.REFERENCE_FASTA : null; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:54:55.302 DEBUG ConfigFactory - Configuration file values:; 17:54:55.320 DEBUG ConfigFactory - gcsMaxRetries = 20; 17:54:55.320 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:54:55.320 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:54:55.320 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:54:55.320 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:54:55.320 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:54:55.320 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:54:55.320 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:54:55.321 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:54:55.321 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:54:55.321 DEBUG ConfigFactory - createOutputBamIndex = true; 17:54:55.321 DEBUG ConfigFactory - g",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:5768,Modifiability,Config,ConfigFactory,5768,"HTSJDK Defaults.CUSTOM_READER_FACTORY :; 17:54:55.301 INFO PathSeqPipelineSpark - HTSJDK Defaults.DISABLE_SNAPPY_COMPRESSOR : false; 17:54:55.301 INFO PathSeqPipelineSpark - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 17:54:55.301 INFO PathSeqPipelineSpark - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 17:54:55.301 INFO PathSeqPipelineSpark - HTSJDK Defaults.REFERENCE_FASTA : null; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:54:55.302 DEBUG ConfigFactory - Configuration file values:; 17:54:55.320 DEBUG ConfigFactory - gcsMaxRetries = 20; 17:54:55.320 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:54:55.320 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:54:55.320 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:54:55.320 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:54:55.320 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:54:55.320 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:54:55.320 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:54:55.321 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:54:55.321 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:54:55.321 DEBUG ConfigFactory - createOutputBamIndex = true; 17:54:55.321 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:54:55.321 DEBU",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:5823,Modifiability,Config,ConfigFactory,5823,"54:55.301 INFO PathSeqPipelineSpark - HTSJDK Defaults.DISABLE_SNAPPY_COMPRESSOR : false; 17:54:55.301 INFO PathSeqPipelineSpark - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 17:54:55.301 INFO PathSeqPipelineSpark - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 17:54:55.301 INFO PathSeqPipelineSpark - HTSJDK Defaults.REFERENCE_FASTA : null; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:54:55.302 DEBUG ConfigFactory - Configuration file values:; 17:54:55.320 DEBUG ConfigFactory - gcsMaxRetries = 20; 17:54:55.320 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:54:55.320 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:54:55.320 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:54:55.320 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:54:55.320 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:54:55.320 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:54:55.320 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:54:55.321 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:54:55.321 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:54:55.321 DEBUG ConfigFactory - createOutputBamIndex = true; 17:54:55.321 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_read_s",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:5888,Modifiability,Config,ConfigFactory,5888,"PPY_COMPRESSOR : false; 17:54:55.301 INFO PathSeqPipelineSpark - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 17:54:55.301 INFO PathSeqPipelineSpark - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 17:54:55.301 INFO PathSeqPipelineSpark - HTSJDK Defaults.REFERENCE_FASTA : null; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:54:55.302 DEBUG ConfigFactory - Configuration file values:; 17:54:55.320 DEBUG ConfigFactory - gcsMaxRetries = 20; 17:54:55.320 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:54:55.320 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:54:55.320 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:54:55.320 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:54:55.320 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:54:55.320 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:54:55.320 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:54:55.321 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:54:55.321 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:54:55.321 DEBUG ConfigFactory - createOutputBamIndex = true; 17:54:55.321 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:5963,Modifiability,Config,ConfigFactory,5963,"aults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 17:54:55.301 INFO PathSeqPipelineSpark - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 17:54:55.301 INFO PathSeqPipelineSpark - HTSJDK Defaults.REFERENCE_FASTA : null; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:54:55.302 DEBUG ConfigFactory - Configuration file values:; 17:54:55.320 DEBUG ConfigFactory - gcsMaxRetries = 20; 17:54:55.320 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:54:55.320 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:54:55.320 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:54:55.320 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:54:55.320 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:54:55.320 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:54:55.320 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:54:55.321 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:54:55.321 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:54:55.321 DEBUG ConfigFactory - createOutputBamIndex = true; 17:54:55.321 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 17:54:55.321 DEBUG ConfigFactory - samjdk.us",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:6030,Modifiability,Config,ConfigFactory,6030,"am/md5/%s; 17:54:55.301 INFO PathSeqPipelineSpark - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 17:54:55.301 INFO PathSeqPipelineSpark - HTSJDK Defaults.REFERENCE_FASTA : null; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:54:55.302 DEBUG ConfigFactory - Configuration file values:; 17:54:55.320 DEBUG ConfigFactory - gcsMaxRetries = 20; 17:54:55.320 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:54:55.320 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:54:55.320 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:54:55.320 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:54:55.320 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:54:55.320 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:54:55.320 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:54:55.321 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:54:55.321 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:54:55.321 DEBUG ConfigFactory - createOutputBamIndex = true; 17:54:55.321 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 17:54:55.321 INFO PathSeqPipeline",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:6105,Modifiability,Config,ConfigFactory,6105,"O_BUFFER_SIZE : 131072; 17:54:55.301 INFO PathSeqPipelineSpark - HTSJDK Defaults.REFERENCE_FASTA : null; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:54:55.302 DEBUG ConfigFactory - Configuration file values:; 17:54:55.320 DEBUG ConfigFactory - gcsMaxRetries = 20; 17:54:55.320 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:54:55.320 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:54:55.320 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:54:55.320 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:54:55.320 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:54:55.320 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:54:55.320 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:54:55.321 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:54:55.321 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:54:55.321 DEBUG ConfigFactory - createOutputBamIndex = true; 17:54:55.321 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 17:54:55.321 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 17:54:55.321 INFO PathSeqPipelineSpark - I",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:6174,Modifiability,Config,ConfigFactory,6174,"DK Defaults.REFERENCE_FASTA : null; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:54:55.302 DEBUG ConfigFactory - Configuration file values:; 17:54:55.320 DEBUG ConfigFactory - gcsMaxRetries = 20; 17:54:55.320 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:54:55.320 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:54:55.320 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:54:55.320 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:54:55.320 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:54:55.320 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:54:55.320 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:54:55.321 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:54:55.321 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:54:55.321 DEBUG ConfigFactory - createOutputBamIndex = true; 17:54:55.321 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 17:54:55.321 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 17:54:55.321 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 17:54:55.321 INFO PathSeqPipelineSpark - GCS ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:6251,Modifiability,Config,ConfigFactory,6251,"HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:54:55.302 DEBUG ConfigFactory - Configuration file values:; 17:54:55.320 DEBUG ConfigFactory - gcsMaxRetries = 20; 17:54:55.320 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:54:55.320 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:54:55.320 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:54:55.320 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:54:55.320 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:54:55.320 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:54:55.320 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:54:55.321 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:54:55.321 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:54:55.321 DEBUG ConfigFactory - createOutputBamIndex = true; 17:54:55.321 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 17:54:55.321 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 17:54:55.321 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 17:54:55.321 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 17:54:55.321 INFO PathSeqPipelineSpark - Using googl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:6319,Modifiability,Config,ConfigFactory,6319,"athSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:54:55.302 DEBUG ConfigFactory - Configuration file values:; 17:54:55.320 DEBUG ConfigFactory - gcsMaxRetries = 20; 17:54:55.320 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:54:55.320 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:54:55.320 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:54:55.320 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:54:55.320 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:54:55.320 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:54:55.320 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:54:55.321 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:54:55.321 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:54:55.321 DEBUG ConfigFactory - createOutputBamIndex = true; 17:54:55.321 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 17:54:55.321 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 17:54:55.321 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 17:54:55.321 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 17:54:55.321 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from htt",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:6389,Modifiability,Config,ConfigFactory,6389,"7:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:54:55.302 DEBUG ConfigFactory - Configuration file values:; 17:54:55.320 DEBUG ConfigFactory - gcsMaxRetries = 20; 17:54:55.320 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:54:55.320 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:54:55.320 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:54:55.320 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:54:55.320 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:54:55.320 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:54:55.320 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:54:55.321 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:54:55.321 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:54:55.321 DEBUG ConfigFactory - createOutputBamIndex = true; 17:54:55.321 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 17:54:55.321 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 17:54:55.321 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 17:54:55.321 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 17:54:55.321 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 17:54:55.321 IN",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:6519,Modifiability,Config,ConfigFactory,6519," - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:54:55.302 DEBUG ConfigFactory - Configuration file values:; 17:54:55.320 DEBUG ConfigFactory - gcsMaxRetries = 20; 17:54:55.320 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:54:55.320 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:54:55.320 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:54:55.320 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:54:55.320 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:54:55.320 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:54:55.320 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:54:55.321 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:54:55.321 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:54:55.321 DEBUG ConfigFactory - createOutputBamIndex = true; 17:54:55.321 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 17:54:55.321 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 17:54:55.321 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 17:54:55.321 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 17:54:55.321 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 17:54:55.321 INFO PathSeqPipelineSpark - Initializing engine; 17:54:55.321 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:6580,Modifiability,Config,ConfigFactory,6580,":55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:54:55.302 DEBUG ConfigFactory - Configuration file values:; 17:54:55.320 DEBUG ConfigFactory - gcsMaxRetries = 20; 17:54:55.320 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:54:55.320 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:54:55.320 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:54:55.320 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:54:55.320 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:54:55.320 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:54:55.320 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:54:55.321 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:54:55.321 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:54:55.321 DEBUG ConfigFactory - createOutputBamIndex = true; 17:54:55.321 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 17:54:55.321 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 17:54:55.321 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 17:54:55.321 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 17:54:55.321 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 17:54:55.321 INFO PathSeqPipelineSpark - Initializing engine; 17:54:55.321 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 18/",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:6646,Modifiability,Config,ConfigFactory,6646,"DOWNLOAD : false; 17:54:55.302 DEBUG ConfigFactory - Configuration file values:; 17:54:55.320 DEBUG ConfigFactory - gcsMaxRetries = 20; 17:54:55.320 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:54:55.320 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:54:55.320 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:54:55.320 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:54:55.320 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:54:55.320 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:54:55.320 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:54:55.321 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:54:55.321 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:54:55.321 DEBUG ConfigFactory - createOutputBamIndex = true; 17:54:55.321 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 17:54:55.321 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 17:54:55.321 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 17:54:55.321 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 17:54:55.321 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 17:54:55.321 INFO PathSeqPipelineSpark - Initializing engine; 17:54:55.321 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 18/04/24 17:54:55 INFO SparkContext: Running Spark version 2.2.0; 18",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:6710,Modifiability,Config,ConfigFactory,6710," values:; 17:54:55.320 DEBUG ConfigFactory - gcsMaxRetries = 20; 17:54:55.320 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:54:55.320 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:54:55.320 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:54:55.320 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:54:55.320 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:54:55.320 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:54:55.320 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:54:55.321 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:54:55.321 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:54:55.321 DEBUG ConfigFactory - createOutputBamIndex = true; 17:54:55.321 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 17:54:55.321 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 17:54:55.321 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 17:54:55.321 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 17:54:55.321 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 17:54:55.321 INFO PathSeqPipelineSpark - Initializing engine; 17:54:55.321 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 18/04/24 17:54:55 INFO SparkContext: Running Spark version 2.2.0; 18/04/24 17:54:55 WARN NativeCodeLoader: Unable to load native-hadoop lib",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:6788,Modifiability,Config,ConfigFactory,6788,"ies = 20; 17:54:55.320 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:54:55.320 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:54:55.320 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:54:55.320 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:54:55.320 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:54:55.320 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:54:55.320 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:54:55.321 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:54:55.321 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:54:55.321 DEBUG ConfigFactory - createOutputBamIndex = true; 17:54:55.321 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 17:54:55.321 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 17:54:55.321 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 17:54:55.321 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 17:54:55.321 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 17:54:55.321 INFO PathSeqPipelineSpark - Initializing engine; 17:54:55.321 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 18/04/24 17:54:55 INFO SparkContext: Running Spark version 2.2.0; 18/04/24 17:54:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes whe",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:6866,Modifiability,Config,ConfigFactory,6866,"54:55.320 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:54:55.320 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:54:55.320 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:54:55.320 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:54:55.320 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:54:55.320 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:54:55.321 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:54:55.321 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:54:55.321 DEBUG ConfigFactory - createOutputBamIndex = true; 17:54:55.321 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 17:54:55.321 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 17:54:55.321 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 17:54:55.321 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 17:54:55.321 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 17:54:55.321 INFO PathSeqPipelineSpark - Initializing engine; 17:54:55.321 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 18/04/24 17:54:55 INFO SparkContext: Running Spark version 2.2.0; 18/04/24 17:54:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18/04/24 17:54:56 INFO SparkContext: Submitted application: Pat",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:6944,Modifiability,Config,ConfigFactory,6944,"55.320 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:54:55.320 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:54:55.320 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:54:55.320 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:54:55.320 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:54:55.321 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:54:55.321 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:54:55.321 DEBUG ConfigFactory - createOutputBamIndex = true; 17:54:55.321 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 17:54:55.321 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 17:54:55.321 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 17:54:55.321 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 17:54:55.321 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 17:54:55.321 INFO PathSeqPipelineSpark - Initializing engine; 17:54:55.321 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 18/04/24 17:54:55 INFO SparkContext: Running Spark version 2.2.0; 18/04/24 17:54:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18/04/24 17:54:56 INFO SparkContext: Submitted application: PathSeqPipelineSpark; 18/04/24 17:54:56 INFO SecurityManager: Changing view acls ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:2301,Performance,concurren,concurrent,2301,ter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more; ```. Thank you. Full log:; ````; 17:54:54.447 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 17:54:54.891 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/scratch/home/int/eva/userx/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-spark.jar!/com,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:2385,Performance,concurren,concurrent,2385,park.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more; ```. Thank you. Full log:; ````; 17:54:54.447 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 17:54:54.891 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/scratch/home/int/eva/userx/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 17:54:54.924 DEBUG NativeLibraryLoader - Ex,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:3183,Performance,Load,Loading,3183,"eduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more; ```. Thank you. Full log:; ````; 17:54:54.447 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 17:54:54.891 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/scratch/home/int/eva/userx/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 17:54:54.924 DEBUG NativeLibraryLoader - Extracting libgkl_compression.so to /tmp/userx/libgkl_compression2910983555987484852.so; 17:54:55.293 INFO PathSeqPipelineSpark - ------------------------------------------------------------; 17:54:55.294 INFO PathSeqPipelineSpark - The Genome Analysis Toolkit (GATK) v4.0.3.0; 17:54:55.294 INFO PathSeqPipelineSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 17:54:55.295 INFO PathSeqPipelineSpark - Executing as userx@node016 on Linux v2.6.32-220.4.1.el6.x86_64 amd64; 17:54:55.295 INFO PathSeqPipelineSpark - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_131-b11; 17:54:55.295 INFO PathSeqPipelineSpark - Start Date/Time: April 24, 2018 5:54:54 PM CEST; 17:54:55.295 INFO PathSeqPipelineSpark - -----------------------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:7717,Performance,load,load,7717,nfigFactory - gatk_stacktrace_on_user_exception = false; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 17:54:55.321 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 17:54:55.321 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 17:54:55.321 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 17:54:55.321 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 17:54:55.321 INFO PathSeqPipelineSpark - Initializing engine; 17:54:55.321 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 18/04/24 17:54:55 INFO SparkContext: Running Spark version 2.2.0; 18/04/24 17:54:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18/04/24 17:54:56 INFO SparkContext: Submitted application: PathSeqPipelineSpark; 18/04/24 17:54:56 INFO SecurityManager: Changing view acls to: userx; 18/04/24 17:54:56 INFO SecurityManager: Changing modify acls to: userx; 18/04/24 17:54:56 INFO SecurityManager: Changing view acls groups to:; 18/04/24 17:54:56 INFO SecurityManager: Changing modify acls groups to:; 18/04/24 17:54:56 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(userx); groups with view permissions: Set(); users with modify permissions: Set(userx); groups with modify permissions: Set(); 18/04/24 17:54:57 INFO Utils: Successfully started service 'sparkDriver' on port 59501.; 18/04/24 17:54:57 INFO SparkEnv: Registering MapOutputTracker; 18/04/24 17:54:57 INFO SparkEnv: Registering BlockManagerMaster; 18/04/24 17:54:57 INFO BlockManagerMasterEndpoint: Using o,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:26316,Performance,concurren,concurrent,26316,"ter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more**. 00:59 DEBUG: [kryo] Write: WrappedArray(null); 18/04/24 17:55:54 INFO TaskSetManager: Starting task 1.1 in stage 2.0 (TID 5, xx.xx.xx.24, executor 1, partition 1, PROCESS_LOCAL, 5371 bytes); 18/04/24 17:55:54 INFO TaskSetManager: Lost task 0.0 in stage 2.0 (TID 3) on xx.xx.xx.25, executor 2: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile (Couldn't read file. Err",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:26400,Performance,concurren,concurrent,26400,"park.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more**. 00:59 DEBUG: [kryo] Write: WrappedArray(null); 18/04/24 17:55:54 INFO TaskSetManager: Starting task 1.1 in stage 2.0 (TID 5, xx.xx.xx.24, executor 1, partition 1, PROCESS_LOCAL, 5371 bytes); 18/04/24 17:55:54 INFO TaskSetManager: Lost task 0.0 in stage 2.0 (TID 3) on xx.xx.xx.25, executor 2: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile (Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory)) [dupl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:30814,Performance,concurren,concurrent,30814,"ter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. 01:12 DEBUG: [kryo] Write: WrappedArray(null); 18/04/24 17:56:07 INFO TaskSetManager: Starting task 1.2 in stage 2.0 (TID 9, xx.xx.xx.27, executor 0, partition 1, PROCESS_LOCAL, 5371 bytes); 18/04/24 17:56:37 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on xx.xx.xx.27:46181 (size: 6.4 KB, free: 366.3 MB); 18/04/24 17:56:38 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:30898,Performance,concurren,concurrent,30898,"park.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. 01:12 DEBUG: [kryo] Write: WrappedArray(null); 18/04/24 17:56:07 INFO TaskSetManager: Starting task 1.2 in stage 2.0 (TID 9, xx.xx.xx.27, executor 0, partition 1, PROCESS_LOCAL, 5371 bytes); 18/04/24 17:56:37 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on xx.xx.xx.27:46181 (size: 6.4 KB, free: 366.3 MB); 18/04/24 17:56:38 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on xx.xx.xx.27:46181 (size: 23.1 KB, free: 366.3 MB); 18/04/24 17:56:39 WARN TaskSet",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:33411,Performance,concurren,concurrent,33411,"ter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. 01:44 DEBUG: [kryo] Write: WrappedArray(null); 18/04/24 17:56:39 INFO TaskSetManager: Starting task 1.3 in stage 2.0 (TID 10, xx.xx.xx.16, executor 3, partition 1, PROCESS_LOCAL, 5371 bytes); 18/04/24 17:56:39 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on xx.xx.xx.24:35903 (size: 6.4 KB, free: 366.3 MB); 18/04/24 17:56:39 INFO TaskSetManager: Lost task 1.3 in stage 2.0 (TID 10) ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:33495,Performance,concurren,concurrent,33495,"park.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. 01:44 DEBUG: [kryo] Write: WrappedArray(null); 18/04/24 17:56:39 INFO TaskSetManager: Starting task 1.3 in stage 2.0 (TID 10, xx.xx.xx.16, executor 3, partition 1, PROCESS_LOCAL, 5371 bytes); 18/04/24 17:56:39 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on xx.xx.xx.24:35903 (size: 6.4 KB, free: 366.3 MB); 18/04/24 17:56:39 INFO TaskSetManager: Lost task 1.3 in stage 2.0 (TID 10) on xx.xx.xx.16, executor 3: org.broadinstitute.hellbender.exceptions.UserException$C",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:36561,Performance,concurren,concurrent,36561,"ter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. Driver stacktrace:; 18/04/24 17:56:39 INFO DAGScheduler: Job 2 failed: count at PathSeqPipelineSpark.java:245, took 45.308012 s; 18/04/24 17:56:39 INFO SparkUI: Stopped Spark web UI at http://xx.xx.xx.16:4040; 18/04/24 17:56:39 INFO StandaloneSchedulerBackend: Shutting down all executors; 18/04/24 17:56:39 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down; 18",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:36645,Performance,concurren,concurrent,36645,"park.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. Driver stacktrace:; 18/04/24 17:56:39 INFO DAGScheduler: Job 2 failed: count at PathSeqPipelineSpark.java:245, took 45.308012 s; 18/04/24 17:56:39 INFO SparkUI: Stopped Spark web UI at http://xx.xx.xx.16:4040; 18/04/24 17:56:39 INFO StandaloneSchedulerBackend: Shutting down all executors; 18/04/24 17:56:39 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down; 18/04/24 17:56:39 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:39892,Performance,concurren,concurrent,39892,ter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$c,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:39976,Performance,concurren,concurrent,39976,park.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.forea,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:45293,Performance,concurren,concurrent,45293,.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ```; `,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:45377,Performance,concurren,concurrent,45377,.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ```; `,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:34716,Safety,abort,aborting,34716,"nputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. 01:44 DEBUG: [kryo] Write: WrappedArray(null); 18/04/24 17:56:39 INFO TaskSetManager: Starting task 1.3 in stage 2.0 (TID 10, xx.xx.xx.16, executor 3, partition 1, PROCESS_LOCAL, 5371 bytes); 18/04/24 17:56:39 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on xx.xx.xx.24:35903 (size: 6.4 KB, free: 366.3 MB); 18/04/24 17:56:39 INFO TaskSetManager: Lost task 1.3 in stage 2.0 (TID 10) on xx.xx.xx.16, executor 3: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile (Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory)) [duplicate 1]; 18/04/24 17:56:39 ERROR TaskSetManager: Task 1 in stage 2.0 failed 4 times; aborting job; 18/04/24 17:56:39 INFO TaskSchedulerImpl: Cancelling stage 2; 18/04/24 17:56:39 INFO TaskSchedulerImpl: Stage 2 was cancelled; 18/04/24 17:56:39 INFO DAGScheduler: ShuffleMapStage 2 (mapToPair at PSFilter.java:125) failed in 45.219 s due to Job aborted due to stage failure: Task 1 in stage 2.0 failed 4 times, most recent failure: Lost task 1.3 in stage 2.0 (TID 10, xx.xx.xx.16, executor 3): org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.ca",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:34975,Safety,abort,aborted,34975,"tUtils.java:103); ... 16 more. 01:44 DEBUG: [kryo] Write: WrappedArray(null); 18/04/24 17:56:39 INFO TaskSetManager: Starting task 1.3 in stage 2.0 (TID 10, xx.xx.xx.16, executor 3, partition 1, PROCESS_LOCAL, 5371 bytes); 18/04/24 17:56:39 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on xx.xx.xx.24:35903 (size: 6.4 KB, free: 366.3 MB); 18/04/24 17:56:39 INFO TaskSetManager: Lost task 1.3 in stage 2.0 (TID 10) on xx.xx.xx.16, executor 3: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile (Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory)) [duplicate 1]; 18/04/24 17:56:39 ERROR TaskSetManager: Task 1 in stage 2.0 failed 4 times; aborting job; 18/04/24 17:56:39 INFO TaskSchedulerImpl: Cancelling stage 2; 18/04/24 17:56:39 INFO TaskSchedulerImpl: Stage 2 was cancelled; 18/04/24 17:56:39 INFO DAGScheduler: ShuffleMapStage 2 (mapToPair at PSFilter.java:125) failed in 45.219 s due to Job aborted due to stage failure: Task 1 in stage 2.0 failed 4 times, most recent failure: Lost task 1.3 in stage 2.0 (TID 10, xx.xx.xx.16, executor 3): org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:38306,Safety,abort,aborted,38306,"FO SparkUI: Stopped Spark web UI at http://xx.xx.xx.16:4040; 18/04/24 17:56:39 INFO StandaloneSchedulerBackend: Shutting down all executors; 18/04/24 17:56:39 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down; 18/04/24 17:56:39 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 18/04/24 17:56:39 INFO MemoryStore: MemoryStore cleared; 18/04/24 17:56:39 INFO BlockManager: BlockManager stopped; 18/04/24 17:56:39 INFO BlockManagerMaster: BlockManagerMaster stopped; 18/04/24 17:56:39 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/04/24 17:56:39 INFO SparkContext: Successfully stopped SparkContext; 17:56:39.758 INFO PathSeqPipelineSpark - Shutting down engine; [April 24, 2018 5:56:39 PM CEST] org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark done. Elapsed time: 1.75 minutes.; Runtime.totalMemory()=821559296; org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 2.0 failed 4 times, most recent failure: Lost task 1.3 in stage 2.0 (TID 10, xx.xx.xx.16, executor 3): org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filte",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:40712,Safety,abort,abortStage,40712,MapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:40809,Safety,abort,abortStage,40809, at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.s,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:41051,Safety,abort,abortStage,41051,ead.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); at org,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:7915,Security,Secur,SecurityManager,7915,321 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 17:54:55.321 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 17:54:55.321 INFO PathSeqPipelineSpark - Initializing engine; 17:54:55.321 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 18/04/24 17:54:55 INFO SparkContext: Running Spark version 2.2.0; 18/04/24 17:54:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18/04/24 17:54:56 INFO SparkContext: Submitted application: PathSeqPipelineSpark; 18/04/24 17:54:56 INFO SecurityManager: Changing view acls to: userx; 18/04/24 17:54:56 INFO SecurityManager: Changing modify acls to: userx; 18/04/24 17:54:56 INFO SecurityManager: Changing view acls groups to:; 18/04/24 17:54:56 INFO SecurityManager: Changing modify acls groups to:; 18/04/24 17:54:56 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(userx); groups with view permissions: Set(); users with modify permissions: Set(userx); groups with modify permissions: Set(); 18/04/24 17:54:57 INFO Utils: Successfully started service 'sparkDriver' on port 59501.; 18/04/24 17:54:57 INFO SparkEnv: Registering MapOutputTracker; 18/04/24 17:54:57 INFO SparkEnv: Registering BlockManagerMaster; 18/04/24 17:54:57 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 18/04/24 17:54:57 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up; 18/04/24 17:54:57 INFO DiskBlockManager: Created local directory at /tmp/userx/blockmgr-213553f6-dd2d-455d-85ef-3ed03ae12f7f; 18/04/24 17:54:57 INFO MemoryStore: MemoryStore started with capacity 366.3 MB; 18/04/24 17:54:57 INFO SparkEnv: Registering OutputCommitCoordin,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:7985,Security,Secur,SecurityManager,7985,321 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 17:54:55.321 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 17:54:55.321 INFO PathSeqPipelineSpark - Initializing engine; 17:54:55.321 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 18/04/24 17:54:55 INFO SparkContext: Running Spark version 2.2.0; 18/04/24 17:54:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18/04/24 17:54:56 INFO SparkContext: Submitted application: PathSeqPipelineSpark; 18/04/24 17:54:56 INFO SecurityManager: Changing view acls to: userx; 18/04/24 17:54:56 INFO SecurityManager: Changing modify acls to: userx; 18/04/24 17:54:56 INFO SecurityManager: Changing view acls groups to:; 18/04/24 17:54:56 INFO SecurityManager: Changing modify acls groups to:; 18/04/24 17:54:56 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(userx); groups with view permissions: Set(); users with modify permissions: Set(userx); groups with modify permissions: Set(); 18/04/24 17:54:57 INFO Utils: Successfully started service 'sparkDriver' on port 59501.; 18/04/24 17:54:57 INFO SparkEnv: Registering MapOutputTracker; 18/04/24 17:54:57 INFO SparkEnv: Registering BlockManagerMaster; 18/04/24 17:54:57 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 18/04/24 17:54:57 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up; 18/04/24 17:54:57 INFO DiskBlockManager: Created local directory at /tmp/userx/blockmgr-213553f6-dd2d-455d-85ef-3ed03ae12f7f; 18/04/24 17:54:57 INFO MemoryStore: MemoryStore started with capacity 366.3 MB; 18/04/24 17:54:57 INFO SparkEnv: Registering OutputCommitCoordin,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:8057,Security,Secur,SecurityManager,8057,321 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 17:54:55.321 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 17:54:55.321 INFO PathSeqPipelineSpark - Initializing engine; 17:54:55.321 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 18/04/24 17:54:55 INFO SparkContext: Running Spark version 2.2.0; 18/04/24 17:54:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18/04/24 17:54:56 INFO SparkContext: Submitted application: PathSeqPipelineSpark; 18/04/24 17:54:56 INFO SecurityManager: Changing view acls to: userx; 18/04/24 17:54:56 INFO SecurityManager: Changing modify acls to: userx; 18/04/24 17:54:56 INFO SecurityManager: Changing view acls groups to:; 18/04/24 17:54:56 INFO SecurityManager: Changing modify acls groups to:; 18/04/24 17:54:56 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(userx); groups with view permissions: Set(); users with modify permissions: Set(userx); groups with modify permissions: Set(); 18/04/24 17:54:57 INFO Utils: Successfully started service 'sparkDriver' on port 59501.; 18/04/24 17:54:57 INFO SparkEnv: Registering MapOutputTracker; 18/04/24 17:54:57 INFO SparkEnv: Registering BlockManagerMaster; 18/04/24 17:54:57 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 18/04/24 17:54:57 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up; 18/04/24 17:54:57 INFO DiskBlockManager: Created local directory at /tmp/userx/blockmgr-213553f6-dd2d-455d-85ef-3ed03ae12f7f; 18/04/24 17:54:57 INFO MemoryStore: MemoryStore started with capacity 366.3 MB; 18/04/24 17:54:57 INFO SparkEnv: Registering OutputCommitCoordin,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:8128,Security,Secur,SecurityManager,8128,321 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 17:54:55.321 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 17:54:55.321 INFO PathSeqPipelineSpark - Initializing engine; 17:54:55.321 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 18/04/24 17:54:55 INFO SparkContext: Running Spark version 2.2.0; 18/04/24 17:54:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18/04/24 17:54:56 INFO SparkContext: Submitted application: PathSeqPipelineSpark; 18/04/24 17:54:56 INFO SecurityManager: Changing view acls to: userx; 18/04/24 17:54:56 INFO SecurityManager: Changing modify acls to: userx; 18/04/24 17:54:56 INFO SecurityManager: Changing view acls groups to:; 18/04/24 17:54:56 INFO SecurityManager: Changing modify acls groups to:; 18/04/24 17:54:56 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(userx); groups with view permissions: Set(); users with modify permissions: Set(userx); groups with modify permissions: Set(); 18/04/24 17:54:57 INFO Utils: Successfully started service 'sparkDriver' on port 59501.; 18/04/24 17:54:57 INFO SparkEnv: Registering MapOutputTracker; 18/04/24 17:54:57 INFO SparkEnv: Registering BlockManagerMaster; 18/04/24 17:54:57 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 18/04/24 17:54:57 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up; 18/04/24 17:54:57 INFO DiskBlockManager: Created local directory at /tmp/userx/blockmgr-213553f6-dd2d-455d-85ef-3ed03ae12f7f; 18/04/24 17:54:57 INFO MemoryStore: MemoryStore started with capacity 366.3 MB; 18/04/24 17:54:57 INFO SparkEnv: Registering OutputCommitCoordin,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:8201,Security,Secur,SecurityManager,8201,321 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 17:54:55.321 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 17:54:55.321 INFO PathSeqPipelineSpark - Initializing engine; 17:54:55.321 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 18/04/24 17:54:55 INFO SparkContext: Running Spark version 2.2.0; 18/04/24 17:54:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18/04/24 17:54:56 INFO SparkContext: Submitted application: PathSeqPipelineSpark; 18/04/24 17:54:56 INFO SecurityManager: Changing view acls to: userx; 18/04/24 17:54:56 INFO SecurityManager: Changing modify acls to: userx; 18/04/24 17:54:56 INFO SecurityManager: Changing view acls groups to:; 18/04/24 17:54:56 INFO SecurityManager: Changing modify acls groups to:; 18/04/24 17:54:56 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(userx); groups with view permissions: Set(); users with modify permissions: Set(userx); groups with modify permissions: Set(); 18/04/24 17:54:57 INFO Utils: Successfully started service 'sparkDriver' on port 59501.; 18/04/24 17:54:57 INFO SparkEnv: Registering MapOutputTracker; 18/04/24 17:54:57 INFO SparkEnv: Registering BlockManagerMaster; 18/04/24 17:54:57 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 18/04/24 17:54:57 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up; 18/04/24 17:54:57 INFO DiskBlockManager: Created local directory at /tmp/userx/blockmgr-213553f6-dd2d-455d-85ef-3ed03ae12f7f; 18/04/24 17:54:57 INFO MemoryStore: MemoryStore started with capacity 366.3 MB; 18/04/24 17:54:57 INFO SparkEnv: Registering OutputCommitCoordin,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:8218,Security,Secur,SecurityManager,8218,321 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 17:54:55.321 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 17:54:55.321 INFO PathSeqPipelineSpark - Initializing engine; 17:54:55.321 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 18/04/24 17:54:55 INFO SparkContext: Running Spark version 2.2.0; 18/04/24 17:54:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18/04/24 17:54:56 INFO SparkContext: Submitted application: PathSeqPipelineSpark; 18/04/24 17:54:56 INFO SecurityManager: Changing view acls to: userx; 18/04/24 17:54:56 INFO SecurityManager: Changing modify acls to: userx; 18/04/24 17:54:56 INFO SecurityManager: Changing view acls groups to:; 18/04/24 17:54:56 INFO SecurityManager: Changing modify acls groups to:; 18/04/24 17:54:56 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(userx); groups with view permissions: Set(); users with modify permissions: Set(userx); groups with modify permissions: Set(); 18/04/24 17:54:57 INFO Utils: Successfully started service 'sparkDriver' on port 59501.; 18/04/24 17:54:57 INFO SparkEnv: Registering MapOutputTracker; 18/04/24 17:54:57 INFO SparkEnv: Registering BlockManagerMaster; 18/04/24 17:54:57 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 18/04/24 17:54:57 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up; 18/04/24 17:54:57 INFO DiskBlockManager: Created local directory at /tmp/userx/blockmgr-213553f6-dd2d-455d-85ef-3ed03ae12f7f; 18/04/24 17:54:57 INFO MemoryStore: MemoryStore started with capacity 366.3 MB; 18/04/24 17:54:57 INFO SparkEnv: Registering OutputCommitCoordin,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:8235,Security,authenticat,authentication,8235,321 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 17:54:55.321 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 17:54:55.321 INFO PathSeqPipelineSpark - Initializing engine; 17:54:55.321 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 18/04/24 17:54:55 INFO SparkContext: Running Spark version 2.2.0; 18/04/24 17:54:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18/04/24 17:54:56 INFO SparkContext: Submitted application: PathSeqPipelineSpark; 18/04/24 17:54:56 INFO SecurityManager: Changing view acls to: userx; 18/04/24 17:54:56 INFO SecurityManager: Changing modify acls to: userx; 18/04/24 17:54:56 INFO SecurityManager: Changing view acls groups to:; 18/04/24 17:54:56 INFO SecurityManager: Changing modify acls groups to:; 18/04/24 17:54:56 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(userx); groups with view permissions: Set(); users with modify permissions: Set(userx); groups with modify permissions: Set(); 18/04/24 17:54:57 INFO Utils: Successfully started service 'sparkDriver' on port 59501.; 18/04/24 17:54:57 INFO SparkEnv: Registering MapOutputTracker; 18/04/24 17:54:57 INFO SparkEnv: Registering BlockManagerMaster; 18/04/24 17:54:57 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 18/04/24 17:54:57 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up; 18/04/24 17:54:57 INFO DiskBlockManager: Created local directory at /tmp/userx/blockmgr-213553f6-dd2d-455d-85ef-3ed03ae12f7f; 18/04/24 17:54:57 INFO MemoryStore: MemoryStore started with capacity 366.3 MB; 18/04/24 17:54:57 INFO SparkEnv: Registering OutputCommitCoordin,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:2932,Testability,log,log,2932,tShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more; ```. Thank you. Full log:; ````; 17:54:54.447 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 17:54:54.891 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/scratch/home/int/eva/userx/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 17:54:54.924 DEBUG NativeLibraryLoader - Extracting libgkl_compression.so to /tmp/userx/libgkl_compression2910983555987484852.so; 17:54:55.293 INFO PathSeqPipelineSpark - ------------------------------------------------------------; 17:54:55.294 INFO PathSeqPipelineSpark - The Genome Analysis Toolkit (GATK) v4.0.3.0; 17:54:55.294 INFO PathSeqPipelineSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 17:54:55.295 INFO PathSeqPipelineSpark - Executing as userx@node016 on Linux v2.6.32-220.4.1.el6.x86_64 amd64; 17:54:55.295 INFO PathSeqPipelineSpark - J,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4699:37709,Usability,clear,cleared,37709,"va:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. Driver stacktrace:; 18/04/24 17:56:39 INFO DAGScheduler: Job 2 failed: count at PathSeqPipelineSpark.java:245, took 45.308012 s; 18/04/24 17:56:39 INFO SparkUI: Stopped Spark web UI at http://xx.xx.xx.16:4040; 18/04/24 17:56:39 INFO StandaloneSchedulerBackend: Shutting down all executors; 18/04/24 17:56:39 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down; 18/04/24 17:56:39 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 18/04/24 17:56:39 INFO MemoryStore: MemoryStore cleared; 18/04/24 17:56:39 INFO BlockManager: BlockManager stopped; 18/04/24 17:56:39 INFO BlockManagerMaster: BlockManagerMaster stopped; 18/04/24 17:56:39 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/04/24 17:56:39 INFO SparkContext: Successfully stopped SparkContext; 17:56:39.758 INFO PathSeqPipelineSpark - Shutting down engine; [April 24, 2018 5:56:39 PM CEST] org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark done. Elapsed time: 1.75 minutes.; Runtime.totalMemory()=821559296; org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 2.0 failed 4 times, most recent failure: Lost task 1.3 in stage 2.0 (TID 10, xx.xx.xx.16, executor 3): org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUti",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699
https://github.com/broadinstitute/gatk/issues/4700:46,Modifiability,refactor,refactored,46,"Picard had its Optical duplicate finding code refactored recently, additionally it has been noticed as part of #4656 that we are currently not properly accounting for the read groups when we stratify reads in MarkDuplicatesSpark which will likely cause problems for bams with more than one read group. Additionally better test coverage for multiple read groups should be added to ensure we are handling them sanely.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4700
https://github.com/broadinstitute/gatk/issues/4700:322,Testability,test,test,322,"Picard had its Optical duplicate finding code refactored recently, additionally it has been noticed as part of #4656 that we are currently not properly accounting for the read groups when we stratify reads in MarkDuplicatesSpark which will likely cause problems for bams with more than one read group. Additionally better test coverage for multiple read groups should be added to ensure we are handling them sanely.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4700
https://github.com/broadinstitute/gatk/issues/4701:444,Testability,test,test,444,"Currently there is a bug in MarkDuplicatesSpark where for non queryname sorted bams the index is incorrectly assumed to be sufficient for reads groups, which wis guaranteed to be adequate for queryname sorted bams. This means that under some circumstances reads can be inconsistently marked as duplicates when its mate is a non-duplicate if they were spread across multiple partitions. . There is a disabled `MarkDuplicatesSparkUtilsUnitTests` test in #4656 that should hopefully demonstrate this issue.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4701
https://github.com/broadinstitute/gatk/issues/4702:257,Availability,error,error,257,"If you have a barclay `@Argument` field of type `List`, barclay will fail to set the field value properly when the argument is specified if the `List` is initialized using an immutable Collection, such as that returned by `Collections.emptyList()`. Example error:. ```; java.lang.UnsupportedOperationException; 	at java.util.AbstractList.add(AbstractList.java:148); 	at java.util.AbstractList.add(AbstractList.java:108); 	at org.broadinstitute.barclay.argparser.CommandLineArgumentParser.setArgument(CommandLineArgumentParser.java:706); 	at org.broadinstitute.barclay.argparser.CommandLineArgumentParser.parseArguments(CommandLineArgumentParser.java:427); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.parseArgs(CommandLineProgram.java:220); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:194); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. Ideally, barclay should detect immutable collections and replace them with mutable ones when necessary.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4702
https://github.com/broadinstitute/gatk/issues/4702:1092,Safety,detect,detect,1092,"If you have a barclay `@Argument` field of type `List`, barclay will fail to set the field value properly when the argument is specified if the `List` is initialized using an immutable Collection, such as that returned by `Collections.emptyList()`. Example error:. ```; java.lang.UnsupportedOperationException; 	at java.util.AbstractList.add(AbstractList.java:148); 	at java.util.AbstractList.add(AbstractList.java:108); 	at org.broadinstitute.barclay.argparser.CommandLineArgumentParser.setArgument(CommandLineArgumentParser.java:706); 	at org.broadinstitute.barclay.argparser.CommandLineArgumentParser.parseArguments(CommandLineArgumentParser.java:427); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.parseArgs(CommandLineProgram.java:220); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:194); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. Ideally, barclay should detect immutable collections and replace them with mutable ones when necessary.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4702
https://github.com/broadinstitute/gatk/pull/4703:135,Testability,Test,Tests,135,"Barclay can't currently handle immutable collections in `@Argument` values due to; https://github.com/broadinstitute/gatk/issues/4702. Tests for these arguments are coming in a separate, larger branch, but I; wanted to get the fixes in first since it's such a simple fix.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4703
https://github.com/broadinstitute/gatk/pull/4703:260,Usability,simpl,simple,260,"Barclay can't currently handle immutable collections in `@Argument` values due to; https://github.com/broadinstitute/gatk/issues/4702. Tests for these arguments are coming in a separate, larger branch, but I; wanted to get the fixes in first since it's such a simple fix.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4703
https://github.com/broadinstitute/gatk/issues/4705:229,Availability,error,error,229,"Hi,; I'm running a command:; ```; java -jar gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar SelectVariants \; -R ref/human_g1k_b37_20.fasta \; -V dupa.raw.indels.snps.vcf \; -selectType INDEL \; -O dupa_raw.INDEL.vcf. ```; and I get error:; ```; ***********************************************************************. A USER ERROR has occurred: s is not a recognized option. ***********************************************************************; ```; Tested on various input files. ```; java -version; openjdk version ""1.8.0_152""; OpenJDK Runtime Environment (Zulu 8.25.0.1-linux64) (build 1.8.0_152-b16); OpenJDK 64-Bit Server VM (Zulu 8.25.0.1-linux64) (build 25.152-b16, mixed mode); ```. Looks like it's a bug?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4705
https://github.com/broadinstitute/gatk/issues/4705:322,Availability,ERROR,ERROR,322,"Hi,; I'm running a command:; ```; java -jar gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar SelectVariants \; -R ref/human_g1k_b37_20.fasta \; -V dupa.raw.indels.snps.vcf \; -selectType INDEL \; -O dupa_raw.INDEL.vcf. ```; and I get error:; ```; ***********************************************************************. A USER ERROR has occurred: s is not a recognized option. ***********************************************************************; ```; Tested on various input files. ```; java -version; openjdk version ""1.8.0_152""; OpenJDK Runtime Environment (Zulu 8.25.0.1-linux64) (build 1.8.0_152-b16); OpenJDK 64-Bit Server VM (Zulu 8.25.0.1-linux64) (build 25.152-b16, mixed mode); ```. Looks like it's a bug?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4705
https://github.com/broadinstitute/gatk/issues/4705:450,Testability,Test,Tested,450,"Hi,; I'm running a command:; ```; java -jar gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar SelectVariants \; -R ref/human_g1k_b37_20.fasta \; -V dupa.raw.indels.snps.vcf \; -selectType INDEL \; -O dupa_raw.INDEL.vcf. ```; and I get error:; ```; ***********************************************************************. A USER ERROR has occurred: s is not a recognized option. ***********************************************************************; ```; Tested on various input files. ```; java -version; openjdk version ""1.8.0_152""; OpenJDK Runtime Environment (Zulu 8.25.0.1-linux64) (build 1.8.0_152-b16); OpenJDK 64-Bit Server VM (Zulu 8.25.0.1-linux64) (build 25.152-b16, mixed mode); ```. Looks like it's a bug?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4705
https://github.com/broadinstitute/gatk/issues/4707:193,Performance,perform,performing,193,"There is some confusing code in MarkDuplicatesSpark for comparison, some of which will cause inherent differences with picard Mark Duplicates. Some time should be set aside to ensure it is all performing correctly and that the differences are justified during validation.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4707
https://github.com/broadinstitute/gatk/issues/4707:260,Security,validat,validation,260,"There is some confusing code in MarkDuplicatesSpark for comparison, some of which will cause inherent differences with picard Mark Duplicates. Some time should be set aside to ensure it is all performing correctly and that the differences are justified during validation.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4707
https://github.com/broadinstitute/gatk/issues/4709:34,Availability,error,error,34,"`UserException.BadTmpDir` has the error message: ; `""Failure working with the tmp directory %s. Override with -Djava.io.tmpdir=X on the command line to a bigger/better file system.""`. This should be changed refer to either the `--java-options` command or `--TMP_DIR`. Also, it doesn't attach the casual exception.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4709
https://github.com/broadinstitute/gatk/issues/4709:53,Availability,Failure,Failure,53,"`UserException.BadTmpDir` has the error message: ; `""Failure working with the tmp directory %s. Override with -Djava.io.tmpdir=X on the command line to a bigger/better file system.""`. This should be changed refer to either the `--java-options` command or `--TMP_DIR`. Also, it doesn't attach the casual exception.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4709
https://github.com/broadinstitute/gatk/issues/4709:40,Integrability,message,message,40,"`UserException.BadTmpDir` has the error message: ; `""Failure working with the tmp directory %s. Override with -Djava.io.tmpdir=X on the command line to a bigger/better file system.""`. This should be changed refer to either the `--java-options` command or `--TMP_DIR`. Also, it doesn't attach the casual exception.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4709
https://github.com/broadinstitute/gatk/pull/4710:36,Performance,concurren,concurrently,36,@LeeTL1220 Ready for review. I will concurrently test in Firecloud.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4710
https://github.com/broadinstitute/gatk/pull/4710:49,Testability,test,test,49,@LeeTL1220 Ready for review. I will concurrently test in Firecloud.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4710
https://github.com/broadinstitute/gatk/pull/4711:226,Energy Efficiency,schedul,scheduled,226,Two commits here:. - The first is to fix a no longer accurate message in `UserException.BadTmpDir`; - The second is a few improvements to IOUtils. ; 1. Rename and simplify `tmpDir` -> `createTempDir` and make it automatically scheduled for deletion; 2. Add documentation to the confusing `absolute` method so that I stop wondering what it's for,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4711
https://github.com/broadinstitute/gatk/pull/4711:62,Integrability,message,message,62,Two commits here:. - The first is to fix a no longer accurate message in `UserException.BadTmpDir`; - The second is a few improvements to IOUtils. ; 1. Rename and simplify `tmpDir` -> `createTempDir` and make it automatically scheduled for deletion; 2. Add documentation to the confusing `absolute` method so that I stop wondering what it's for,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4711
https://github.com/broadinstitute/gatk/pull/4711:163,Usability,simpl,simplify,163,Two commits here:. - The first is to fix a no longer accurate message in `UserException.BadTmpDir`; - The second is a few improvements to IOUtils. ; 1. Rename and simplify `tmpDir` -> `createTempDir` and make it automatically scheduled for deletion; 2. Add documentation to the confusing `absolute` method so that I stop wondering what it's for,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4711
https://github.com/broadinstitute/gatk/issues/4712:14,Availability,error,error,14,"I've got this error. java.lang.IllegalArgumentException: Invalid interval. Contig:ENST00000342066.7|ENSG00000187634.11|OTTHUMG00000040719.10|OTTHUMT00000276866.2|SAMD11-202|SAMD11|2551|protein_coding| start:0 end:0. I'm working on hg38. The VCF input file was produced from Mutect2. First, I've the error ""java.lang.NullPointerException"". So I put only ""gencode"" folder in the data-source folder. Then, I've got the error message above.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712
https://github.com/broadinstitute/gatk/issues/4712:299,Availability,error,error,299,"I've got this error. java.lang.IllegalArgumentException: Invalid interval. Contig:ENST00000342066.7|ENSG00000187634.11|OTTHUMG00000040719.10|OTTHUMT00000276866.2|SAMD11-202|SAMD11|2551|protein_coding| start:0 end:0. I'm working on hg38. The VCF input file was produced from Mutect2. First, I've the error ""java.lang.NullPointerException"". So I put only ""gencode"" folder in the data-source folder. Then, I've got the error message above.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712
https://github.com/broadinstitute/gatk/issues/4712:416,Availability,error,error,416,"I've got this error. java.lang.IllegalArgumentException: Invalid interval. Contig:ENST00000342066.7|ENSG00000187634.11|OTTHUMG00000040719.10|OTTHUMT00000276866.2|SAMD11-202|SAMD11|2551|protein_coding| start:0 end:0. I'm working on hg38. The VCF input file was produced from Mutect2. First, I've the error ""java.lang.NullPointerException"". So I put only ""gencode"" folder in the data-source folder. Then, I've got the error message above.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712
https://github.com/broadinstitute/gatk/issues/4712:422,Integrability,message,message,422,"I've got this error. java.lang.IllegalArgumentException: Invalid interval. Contig:ENST00000342066.7|ENSG00000187634.11|OTTHUMG00000040719.10|OTTHUMT00000276866.2|SAMD11-202|SAMD11|2551|protein_coding| start:0 end:0. I'm working on hg38. The VCF input file was produced from Mutect2. First, I've the error ""java.lang.NullPointerException"". So I put only ""gencode"" folder in the data-source folder. Then, I've got the error message above.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712
https://github.com/broadinstitute/gatk/issues/4716:246,Availability,error,error,246,"Reading from GenomicsDB fails when a some records containing spanning deletion alleles are imported into a workspace. Not all records seem to cause this to fail; I haven't been able to figure out what specific properties of the records cause the error. Here's the contents (minus header) of a VCF file that causes the error:. ```; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	NA12878; 20	10097436	.	CTTTTCTTTCTTTCTTTCTTTCTTTCTTTCTTTCTTT	C,<NON_REF>	1054.73	.	BaseQRankSum=1.820;ClippingRankSum=0.000;DP=89;ExcessHet=3.0103;MLEAC=1,0;MLEAF=0.500,0.00;MQRankSum=-6.464;RAW_MQ=262143.00;ReadPosRankSum=-3.231	GT:AD:DP:GQ:PL:SB	0/1:57,32,0:89:99:1092,0,2241,1263,2338,3601:23,34,11,21; 20	10097437	.	TTTTC	*,T,<NON_REF>	2089.73	.	DP=76;ExcessHet=3.0103;MLEAC=1,1,0;MLEAF=0.500,0.500,0.00;RAW_MQ=217330.00	GT:AD:DP:GQ:PL:SB	1/2:0,32,23,0:55:99:2127,940,1799,1195,0,1125,2201,1453,1262,2642:0,0,16,39; ```. Steps to reproduce:. ```; ./gatk GenomicsDBImport -R src/test/resources/large/human_g1k_v37.20.21.fasta -L 20 -V test_gdb_import.vcf.gz -genomicsdb-workspace-path spanDelWorkspace; ./gatk SelectVariants -V gendb://spanDelWorkspace -R src/test/resources/large/human_g1k_v37.20.21.fasta -O test.vcf -L 20; ```. Error:. ```; java.lang.IllegalArgumentException: Duplicate allele added to VariantContext: T; at htsjdk.variant.variantcontext.VariantContext.makeAlleles(VariantContext.java:1490); at htsjdk.variant.variantcontext.VariantContext.<init>(VariantContext.java:380); at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:494); at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:488); at htsjdk.variant.bcf2.BCF2Codec.decode(BCF2Codec.java:132); at htsjdk.variant.bcf2.BCF2Codec.decode(BCF2Codec.java:58); at com.intel.genomicsdb.GenomicsDBFeatureReader$GenomicsDBFeatureIterator.next(GenomicsDBFeatureReader.java:357); at com.intel.genomicsdb.GenomicsDBFeatureReader$GenomicsDBFeatureIterator.next(GenomicsDBFeatureReader.java",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4716
https://github.com/broadinstitute/gatk/issues/4716:318,Availability,error,error,318,"Reading from GenomicsDB fails when a some records containing spanning deletion alleles are imported into a workspace. Not all records seem to cause this to fail; I haven't been able to figure out what specific properties of the records cause the error. Here's the contents (minus header) of a VCF file that causes the error:. ```; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	NA12878; 20	10097436	.	CTTTTCTTTCTTTCTTTCTTTCTTTCTTTCTTTCTTT	C,<NON_REF>	1054.73	.	BaseQRankSum=1.820;ClippingRankSum=0.000;DP=89;ExcessHet=3.0103;MLEAC=1,0;MLEAF=0.500,0.00;MQRankSum=-6.464;RAW_MQ=262143.00;ReadPosRankSum=-3.231	GT:AD:DP:GQ:PL:SB	0/1:57,32,0:89:99:1092,0,2241,1263,2338,3601:23,34,11,21; 20	10097437	.	TTTTC	*,T,<NON_REF>	2089.73	.	DP=76;ExcessHet=3.0103;MLEAC=1,1,0;MLEAF=0.500,0.500,0.00;RAW_MQ=217330.00	GT:AD:DP:GQ:PL:SB	1/2:0,32,23,0:55:99:2127,940,1799,1195,0,1125,2201,1453,1262,2642:0,0,16,39; ```. Steps to reproduce:. ```; ./gatk GenomicsDBImport -R src/test/resources/large/human_g1k_v37.20.21.fasta -L 20 -V test_gdb_import.vcf.gz -genomicsdb-workspace-path spanDelWorkspace; ./gatk SelectVariants -V gendb://spanDelWorkspace -R src/test/resources/large/human_g1k_v37.20.21.fasta -O test.vcf -L 20; ```. Error:. ```; java.lang.IllegalArgumentException: Duplicate allele added to VariantContext: T; at htsjdk.variant.variantcontext.VariantContext.makeAlleles(VariantContext.java:1490); at htsjdk.variant.variantcontext.VariantContext.<init>(VariantContext.java:380); at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:494); at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:488); at htsjdk.variant.bcf2.BCF2Codec.decode(BCF2Codec.java:132); at htsjdk.variant.bcf2.BCF2Codec.decode(BCF2Codec.java:58); at com.intel.genomicsdb.GenomicsDBFeatureReader$GenomicsDBFeatureIterator.next(GenomicsDBFeatureReader.java:357); at com.intel.genomicsdb.GenomicsDBFeatureReader$GenomicsDBFeatureIterator.next(GenomicsDBFeatureReader.java",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4716
https://github.com/broadinstitute/gatk/issues/4716:1211,Availability,Error,Error,1211,"erties of the records cause the error. Here's the contents (minus header) of a VCF file that causes the error:. ```; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	NA12878; 20	10097436	.	CTTTTCTTTCTTTCTTTCTTTCTTTCTTTCTTTCTTT	C,<NON_REF>	1054.73	.	BaseQRankSum=1.820;ClippingRankSum=0.000;DP=89;ExcessHet=3.0103;MLEAC=1,0;MLEAF=0.500,0.00;MQRankSum=-6.464;RAW_MQ=262143.00;ReadPosRankSum=-3.231	GT:AD:DP:GQ:PL:SB	0/1:57,32,0:89:99:1092,0,2241,1263,2338,3601:23,34,11,21; 20	10097437	.	TTTTC	*,T,<NON_REF>	2089.73	.	DP=76;ExcessHet=3.0103;MLEAC=1,1,0;MLEAF=0.500,0.500,0.00;RAW_MQ=217330.00	GT:AD:DP:GQ:PL:SB	1/2:0,32,23,0:55:99:2127,940,1799,1195,0,1125,2201,1453,1262,2642:0,0,16,39; ```. Steps to reproduce:. ```; ./gatk GenomicsDBImport -R src/test/resources/large/human_g1k_v37.20.21.fasta -L 20 -V test_gdb_import.vcf.gz -genomicsdb-workspace-path spanDelWorkspace; ./gatk SelectVariants -V gendb://spanDelWorkspace -R src/test/resources/large/human_g1k_v37.20.21.fasta -O test.vcf -L 20; ```. Error:. ```; java.lang.IllegalArgumentException: Duplicate allele added to VariantContext: T; at htsjdk.variant.variantcontext.VariantContext.makeAlleles(VariantContext.java:1490); at htsjdk.variant.variantcontext.VariantContext.<init>(VariantContext.java:380); at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:494); at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:488); at htsjdk.variant.bcf2.BCF2Codec.decode(BCF2Codec.java:132); at htsjdk.variant.bcf2.BCF2Codec.decode(BCF2Codec.java:58); at com.intel.genomicsdb.GenomicsDBFeatureReader$GenomicsDBFeatureIterator.next(GenomicsDBFeatureReader.java:357); at com.intel.genomicsdb.GenomicsDBFeatureReader$GenomicsDBFeatureIterator.next(GenomicsDBFeatureReader.java:291); ```. This issue was discovered while trying to add spanning deletion genotyping support to HaplotypeCaller for https://github.com/broadinstitute/gatk/issues/2960 and resolution seems to be necessary to supp",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4716
https://github.com/broadinstitute/gatk/issues/4716:959,Testability,test,test,959,"Reading from GenomicsDB fails when a some records containing spanning deletion alleles are imported into a workspace. Not all records seem to cause this to fail; I haven't been able to figure out what specific properties of the records cause the error. Here's the contents (minus header) of a VCF file that causes the error:. ```; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	NA12878; 20	10097436	.	CTTTTCTTTCTTTCTTTCTTTCTTTCTTTCTTTCTTT	C,<NON_REF>	1054.73	.	BaseQRankSum=1.820;ClippingRankSum=0.000;DP=89;ExcessHet=3.0103;MLEAC=1,0;MLEAF=0.500,0.00;MQRankSum=-6.464;RAW_MQ=262143.00;ReadPosRankSum=-3.231	GT:AD:DP:GQ:PL:SB	0/1:57,32,0:89:99:1092,0,2241,1263,2338,3601:23,34,11,21; 20	10097437	.	TTTTC	*,T,<NON_REF>	2089.73	.	DP=76;ExcessHet=3.0103;MLEAC=1,1,0;MLEAF=0.500,0.500,0.00;RAW_MQ=217330.00	GT:AD:DP:GQ:PL:SB	1/2:0,32,23,0:55:99:2127,940,1799,1195,0,1125,2201,1453,1262,2642:0,0,16,39; ```. Steps to reproduce:. ```; ./gatk GenomicsDBImport -R src/test/resources/large/human_g1k_v37.20.21.fasta -L 20 -V test_gdb_import.vcf.gz -genomicsdb-workspace-path spanDelWorkspace; ./gatk SelectVariants -V gendb://spanDelWorkspace -R src/test/resources/large/human_g1k_v37.20.21.fasta -O test.vcf -L 20; ```. Error:. ```; java.lang.IllegalArgumentException: Duplicate allele added to VariantContext: T; at htsjdk.variant.variantcontext.VariantContext.makeAlleles(VariantContext.java:1490); at htsjdk.variant.variantcontext.VariantContext.<init>(VariantContext.java:380); at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:494); at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:488); at htsjdk.variant.bcf2.BCF2Codec.decode(BCF2Codec.java:132); at htsjdk.variant.bcf2.BCF2Codec.decode(BCF2Codec.java:58); at com.intel.genomicsdb.GenomicsDBFeatureReader$GenomicsDBFeatureIterator.next(GenomicsDBFeatureReader.java:357); at com.intel.genomicsdb.GenomicsDBFeatureReader$GenomicsDBFeatureIterator.next(GenomicsDBFeatureReader.java",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4716
https://github.com/broadinstitute/gatk/issues/4716:1140,Testability,test,test,1140,"ords seem to cause this to fail; I haven't been able to figure out what specific properties of the records cause the error. Here's the contents (minus header) of a VCF file that causes the error:. ```; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	NA12878; 20	10097436	.	CTTTTCTTTCTTTCTTTCTTTCTTTCTTTCTTTCTTT	C,<NON_REF>	1054.73	.	BaseQRankSum=1.820;ClippingRankSum=0.000;DP=89;ExcessHet=3.0103;MLEAC=1,0;MLEAF=0.500,0.00;MQRankSum=-6.464;RAW_MQ=262143.00;ReadPosRankSum=-3.231	GT:AD:DP:GQ:PL:SB	0/1:57,32,0:89:99:1092,0,2241,1263,2338,3601:23,34,11,21; 20	10097437	.	TTTTC	*,T,<NON_REF>	2089.73	.	DP=76;ExcessHet=3.0103;MLEAC=1,1,0;MLEAF=0.500,0.500,0.00;RAW_MQ=217330.00	GT:AD:DP:GQ:PL:SB	1/2:0,32,23,0:55:99:2127,940,1799,1195,0,1125,2201,1453,1262,2642:0,0,16,39; ```. Steps to reproduce:. ```; ./gatk GenomicsDBImport -R src/test/resources/large/human_g1k_v37.20.21.fasta -L 20 -V test_gdb_import.vcf.gz -genomicsdb-workspace-path spanDelWorkspace; ./gatk SelectVariants -V gendb://spanDelWorkspace -R src/test/resources/large/human_g1k_v37.20.21.fasta -O test.vcf -L 20; ```. Error:. ```; java.lang.IllegalArgumentException: Duplicate allele added to VariantContext: T; at htsjdk.variant.variantcontext.VariantContext.makeAlleles(VariantContext.java:1490); at htsjdk.variant.variantcontext.VariantContext.<init>(VariantContext.java:380); at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:494); at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:488); at htsjdk.variant.bcf2.BCF2Codec.decode(BCF2Codec.java:132); at htsjdk.variant.bcf2.BCF2Codec.decode(BCF2Codec.java:58); at com.intel.genomicsdb.GenomicsDBFeatureReader$GenomicsDBFeatureIterator.next(GenomicsDBFeatureReader.java:357); at com.intel.genomicsdb.GenomicsDBFeatureReader$GenomicsDBFeatureIterator.next(GenomicsDBFeatureReader.java:291); ```. This issue was discovered while trying to add spanning deletion genotyping support to HaplotypeCaller for https://git",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4716
https://github.com/broadinstitute/gatk/issues/4716:1190,Testability,test,test,1190,"ure out what specific properties of the records cause the error. Here's the contents (minus header) of a VCF file that causes the error:. ```; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	NA12878; 20	10097436	.	CTTTTCTTTCTTTCTTTCTTTCTTTCTTTCTTTCTTT	C,<NON_REF>	1054.73	.	BaseQRankSum=1.820;ClippingRankSum=0.000;DP=89;ExcessHet=3.0103;MLEAC=1,0;MLEAF=0.500,0.00;MQRankSum=-6.464;RAW_MQ=262143.00;ReadPosRankSum=-3.231	GT:AD:DP:GQ:PL:SB	0/1:57,32,0:89:99:1092,0,2241,1263,2338,3601:23,34,11,21; 20	10097437	.	TTTTC	*,T,<NON_REF>	2089.73	.	DP=76;ExcessHet=3.0103;MLEAC=1,1,0;MLEAF=0.500,0.500,0.00;RAW_MQ=217330.00	GT:AD:DP:GQ:PL:SB	1/2:0,32,23,0:55:99:2127,940,1799,1195,0,1125,2201,1453,1262,2642:0,0,16,39; ```. Steps to reproduce:. ```; ./gatk GenomicsDBImport -R src/test/resources/large/human_g1k_v37.20.21.fasta -L 20 -V test_gdb_import.vcf.gz -genomicsdb-workspace-path spanDelWorkspace; ./gatk SelectVariants -V gendb://spanDelWorkspace -R src/test/resources/large/human_g1k_v37.20.21.fasta -O test.vcf -L 20; ```. Error:. ```; java.lang.IllegalArgumentException: Duplicate allele added to VariantContext: T; at htsjdk.variant.variantcontext.VariantContext.makeAlleles(VariantContext.java:1490); at htsjdk.variant.variantcontext.VariantContext.<init>(VariantContext.java:380); at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:494); at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:488); at htsjdk.variant.bcf2.BCF2Codec.decode(BCF2Codec.java:132); at htsjdk.variant.bcf2.BCF2Codec.decode(BCF2Codec.java:58); at com.intel.genomicsdb.GenomicsDBFeatureReader$GenomicsDBFeatureIterator.next(GenomicsDBFeatureReader.java:357); at com.intel.genomicsdb.GenomicsDBFeatureReader$GenomicsDBFeatureIterator.next(GenomicsDBFeatureReader.java:291); ```. This issue was discovered while trying to add spanning deletion genotyping support to HaplotypeCaller for https://github.com/broadinstitute/gatk/issues/2960 and resolution see",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4716
https://github.com/broadinstitute/gatk/issues/4717:290,Availability,down,downstream,290,"This ticket opens a discussion between @samuelklee, @davidbenjamin and anyone else who is interested towards deciding if this is useful. . I think it would be useful for (i) the outputs of CollectAllelicCounts and GetPileupSummaries and other similar tools to be in VCF format and for (ii) downstream tools that take these results to then also take in VCF format. I think it's good to adhere to standard formats, for results that already nearly resemble variant calls, for versatility. At the least, it would be great if the _format_ that is output by the two tools and accepted by downstream tools is unified. There is also discussion on whether these two particular tools should be merged. There may be other tickets on this particular discussion.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4717
https://github.com/broadinstitute/gatk/issues/4717:582,Availability,down,downstream,582,"This ticket opens a discussion between @samuelklee, @davidbenjamin and anyone else who is interested towards deciding if this is useful. . I think it would be useful for (i) the outputs of CollectAllelicCounts and GetPileupSummaries and other similar tools to be in VCF format and for (ii) downstream tools that take these results to then also take in VCF format. I think it's good to adhere to standard formats, for results that already nearly resemble variant calls, for versatility. At the least, it would be great if the _format_ that is output by the two tools and accepted by downstream tools is unified. There is also discussion on whether these two particular tools should be merged. There may be other tickets on this particular discussion.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4717
https://github.com/broadinstitute/gatk/issues/4718:205,Safety,safe,safe,205,"@droazen Two .vcf files used for testing prevented cloning the GATK on my home laptop. Supposedly ecryptfs is okay up to 143 characters, but I think that might be _typical_ behavior rather than guaranteed safe. It seems like knocking 10-20 characters off the names should make them totally safe. I don't know how important it is for the files to have metadata in their names. **length 142:**; src/test/resources/org/broadinstitute/hellbender/tools/haplotypecaller/expected.CEUTrio.HiSeq.WGS.b37.NA12878.CONTAMINATED.WITH.HCC1143.NORMALS.AT.15PERCENT.calls.20.10100000-10150000.gatk3.8-1-1-gdde23f56a6.g.vcf; **length 140:**; src/test/resources/org/broadinstitute/hellbender/tools/haplotypecaller/expected.CEUTrio.HiSeq.WGS.b37.NA12878.CONTAMINATED.WITH.HCC1143.NORMALS.AT.15PERCENT.calls.20.10100000-10150000.gatk3.8-1-1-gdde23f56a6.vcf",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4718
https://github.com/broadinstitute/gatk/issues/4718:290,Safety,safe,safe,290,"@droazen Two .vcf files used for testing prevented cloning the GATK on my home laptop. Supposedly ecryptfs is okay up to 143 characters, but I think that might be _typical_ behavior rather than guaranteed safe. It seems like knocking 10-20 characters off the names should make them totally safe. I don't know how important it is for the files to have metadata in their names. **length 142:**; src/test/resources/org/broadinstitute/hellbender/tools/haplotypecaller/expected.CEUTrio.HiSeq.WGS.b37.NA12878.CONTAMINATED.WITH.HCC1143.NORMALS.AT.15PERCENT.calls.20.10100000-10150000.gatk3.8-1-1-gdde23f56a6.g.vcf; **length 140:**; src/test/resources/org/broadinstitute/hellbender/tools/haplotypecaller/expected.CEUTrio.HiSeq.WGS.b37.NA12878.CONTAMINATED.WITH.HCC1143.NORMALS.AT.15PERCENT.calls.20.10100000-10150000.gatk3.8-1-1-gdde23f56a6.vcf",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4718
https://github.com/broadinstitute/gatk/issues/4718:33,Testability,test,testing,33,"@droazen Two .vcf files used for testing prevented cloning the GATK on my home laptop. Supposedly ecryptfs is okay up to 143 characters, but I think that might be _typical_ behavior rather than guaranteed safe. It seems like knocking 10-20 characters off the names should make them totally safe. I don't know how important it is for the files to have metadata in their names. **length 142:**; src/test/resources/org/broadinstitute/hellbender/tools/haplotypecaller/expected.CEUTrio.HiSeq.WGS.b37.NA12878.CONTAMINATED.WITH.HCC1143.NORMALS.AT.15PERCENT.calls.20.10100000-10150000.gatk3.8-1-1-gdde23f56a6.g.vcf; **length 140:**; src/test/resources/org/broadinstitute/hellbender/tools/haplotypecaller/expected.CEUTrio.HiSeq.WGS.b37.NA12878.CONTAMINATED.WITH.HCC1143.NORMALS.AT.15PERCENT.calls.20.10100000-10150000.gatk3.8-1-1-gdde23f56a6.vcf",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4718
https://github.com/broadinstitute/gatk/issues/4718:397,Testability,test,test,397,"@droazen Two .vcf files used for testing prevented cloning the GATK on my home laptop. Supposedly ecryptfs is okay up to 143 characters, but I think that might be _typical_ behavior rather than guaranteed safe. It seems like knocking 10-20 characters off the names should make them totally safe. I don't know how important it is for the files to have metadata in their names. **length 142:**; src/test/resources/org/broadinstitute/hellbender/tools/haplotypecaller/expected.CEUTrio.HiSeq.WGS.b37.NA12878.CONTAMINATED.WITH.HCC1143.NORMALS.AT.15PERCENT.calls.20.10100000-10150000.gatk3.8-1-1-gdde23f56a6.g.vcf; **length 140:**; src/test/resources/org/broadinstitute/hellbender/tools/haplotypecaller/expected.CEUTrio.HiSeq.WGS.b37.NA12878.CONTAMINATED.WITH.HCC1143.NORMALS.AT.15PERCENT.calls.20.10100000-10150000.gatk3.8-1-1-gdde23f56a6.vcf",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4718
https://github.com/broadinstitute/gatk/issues/4718:629,Testability,test,test,629,"@droazen Two .vcf files used for testing prevented cloning the GATK on my home laptop. Supposedly ecryptfs is okay up to 143 characters, but I think that might be _typical_ behavior rather than guaranteed safe. It seems like knocking 10-20 characters off the names should make them totally safe. I don't know how important it is for the files to have metadata in their names. **length 142:**; src/test/resources/org/broadinstitute/hellbender/tools/haplotypecaller/expected.CEUTrio.HiSeq.WGS.b37.NA12878.CONTAMINATED.WITH.HCC1143.NORMALS.AT.15PERCENT.calls.20.10100000-10150000.gatk3.8-1-1-gdde23f56a6.g.vcf; **length 140:**; src/test/resources/org/broadinstitute/hellbender/tools/haplotypecaller/expected.CEUTrio.HiSeq.WGS.b37.NA12878.CONTAMINATED.WITH.HCC1143.NORMALS.AT.15PERCENT.calls.20.10100000-10150000.gatk3.8-1-1-gdde23f56a6.vcf",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4718
https://github.com/broadinstitute/gatk/issues/4719:383,Testability,log,log-mean-bias,383,"One issue with running gCNV properly and successfully (and perhaps other GATK tools that work with different data types) is that different data types requires running GATK using wildly different choice of several tool arguments. For example -- running gCNV on WES and WGS requires very different choice of the following parameters:. interval-psi-scale (WES: 0.002, WGS: 0.0005); std-log-mean-bias (WES: 10.0, WGS: 1.0); cnv-coherence-length (WES: 100000, WGS: 5000); ... There are two conceivable solutions:. 1) Declaring all such arguments as _required_, removing default values, and effectively forcing the user to do research (documentation, forum, tutorials) and to set the arguments appropriately according to the use case. This solution is not ideal since it delegates the burden to the user. If we have good default values for WES and WGS, it makes sense to somehow include both in the tool. 2) Declaring all such arguments as optional, though, providing a mechanism to allow the user to _start_ with good default values, with the possibility of overriding them. For example, we can add one required argument to set the baseline:; ```; gatk GermlineCNVCaller \; --set-defaults-for-data-type WES; ...; ```; Under the hood, `--set-defaults-for-data-type WES` would set several optional parameters. Also, by running:. ```; gatk GermlineCNVCaller \; --set-defaults-for-data-type WGS; --std-log-mean-bias 1.0 \; --interval-psi-scale 0.0001 \; ...; ```; the user is asking to override `std-log-mean-bias` and `interval-psi-scale` from shipped default values for `WGS` data type. We can definitely provide such mechanisms in a tool-by-tool basis as needed, though, if many tools can benefit from such a mechanism, it makes sense to provide it in Barclay.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4719
https://github.com/broadinstitute/gatk/issues/4719:1393,Testability,log,log-mean-bias,1393,"One issue with running gCNV properly and successfully (and perhaps other GATK tools that work with different data types) is that different data types requires running GATK using wildly different choice of several tool arguments. For example -- running gCNV on WES and WGS requires very different choice of the following parameters:. interval-psi-scale (WES: 0.002, WGS: 0.0005); std-log-mean-bias (WES: 10.0, WGS: 1.0); cnv-coherence-length (WES: 100000, WGS: 5000); ... There are two conceivable solutions:. 1) Declaring all such arguments as _required_, removing default values, and effectively forcing the user to do research (documentation, forum, tutorials) and to set the arguments appropriately according to the use case. This solution is not ideal since it delegates the burden to the user. If we have good default values for WES and WGS, it makes sense to somehow include both in the tool. 2) Declaring all such arguments as optional, though, providing a mechanism to allow the user to _start_ with good default values, with the possibility of overriding them. For example, we can add one required argument to set the baseline:; ```; gatk GermlineCNVCaller \; --set-defaults-for-data-type WES; ...; ```; Under the hood, `--set-defaults-for-data-type WES` would set several optional parameters. Also, by running:. ```; gatk GermlineCNVCaller \; --set-defaults-for-data-type WGS; --std-log-mean-bias 1.0 \; --interval-psi-scale 0.0001 \; ...; ```; the user is asking to override `std-log-mean-bias` and `interval-psi-scale` from shipped default values for `WGS` data type. We can definitely provide such mechanisms in a tool-by-tool basis as needed, though, if many tools can benefit from such a mechanism, it makes sense to provide it in Barclay.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4719
https://github.com/broadinstitute/gatk/issues/4719:1491,Testability,log,log-mean-bias,1491,"One issue with running gCNV properly and successfully (and perhaps other GATK tools that work with different data types) is that different data types requires running GATK using wildly different choice of several tool arguments. For example -- running gCNV on WES and WGS requires very different choice of the following parameters:. interval-psi-scale (WES: 0.002, WGS: 0.0005); std-log-mean-bias (WES: 10.0, WGS: 1.0); cnv-coherence-length (WES: 100000, WGS: 5000); ... There are two conceivable solutions:. 1) Declaring all such arguments as _required_, removing default values, and effectively forcing the user to do research (documentation, forum, tutorials) and to set the arguments appropriately according to the use case. This solution is not ideal since it delegates the burden to the user. If we have good default values for WES and WGS, it makes sense to somehow include both in the tool. 2) Declaring all such arguments as optional, though, providing a mechanism to allow the user to _start_ with good default values, with the possibility of overriding them. For example, we can add one required argument to set the baseline:; ```; gatk GermlineCNVCaller \; --set-defaults-for-data-type WES; ...; ```; Under the hood, `--set-defaults-for-data-type WES` would set several optional parameters. Also, by running:. ```; gatk GermlineCNVCaller \; --set-defaults-for-data-type WGS; --std-log-mean-bias 1.0 \; --interval-psi-scale 0.0001 \; ...; ```; the user is asking to override `std-log-mean-bias` and `interval-psi-scale` from shipped default values for `WGS` data type. We can definitely provide such mechanisms in a tool-by-tool basis as needed, though, if many tools can benefit from such a mechanism, it makes sense to provide it in Barclay.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4719
https://github.com/broadinstitute/gatk/pull/4720:55,Availability,error,error,55,"Summary of changes:. - Fixed a minor issue in sampling error estimation that could lead to NaN (as a result of division by zero). - Introduced separate _internal_ and _external_ admixing rates. The _internal_ admixing rate is to be used internally by discrete RV posterior update routines (""callers"") as a safety measure to stabilize self-consistency loops. For example, consider the mean-field treatment of two coupled Markov chains: the mean-field decoupling of the two chains yields two independent Markov chains with effective emission, transition, and prior probabilities, all of which must be self-consistency determined. The internal admixing rate would be used to admix the old and new self-consistent fields across the two chains in order to dampen oscillations and improve convergence properties. Once internal convergence is achieved, the converged posteriors must be saved to a workspace in order to be consumed by the continuous sub-model. The new internally converged posteriors will be admixed with the old internally converged posteriors from the previous epoch with the _external_ admixing rate. - Introduced two-stage inference for cohort denoising and calling. In the first (""warm-up"") stage, discrete variables are marginalized out, yielding an effective continuous-only model. The warm-up stage calculates continuous posteriors based on the marginalized model. Once convergence is achieved, continuous and discrete variables are decoupled for the second (""main"") stage. The second stage starts with a discrete calling step (crucial), using continuous posteriors from the warm-up stage as the starting point. The motivation behind the two-stage inference strategy is to avoid getting trapped in spurious local minima that are potentially introduced by mean-field decoupling of discrete and continuous RVs. Note that mean-field decoupling has a tendency to stabilize local minima, most of which will disappear or turn into saddle points once correlations are taken into account. Whi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4720
https://github.com/broadinstitute/gatk/pull/4720:273,Deployability,update,update,273,"Summary of changes:. - Fixed a minor issue in sampling error estimation that could lead to NaN (as a result of division by zero). - Introduced separate _internal_ and _external_ admixing rates. The _internal_ admixing rate is to be used internally by discrete RV posterior update routines (""callers"") as a safety measure to stabilize self-consistency loops. For example, consider the mean-field treatment of two coupled Markov chains: the mean-field decoupling of the two chains yields two independent Markov chains with effective emission, transition, and prior probabilities, all of which must be self-consistency determined. The internal admixing rate would be used to admix the old and new self-consistent fields across the two chains in order to dampen oscillations and improve convergence properties. Once internal convergence is achieved, the converged posteriors must be saved to a workspace in order to be consumed by the continuous sub-model. The new internally converged posteriors will be admixed with the old internally converged posteriors from the previous epoch with the _external_ admixing rate. - Introduced two-stage inference for cohort denoising and calling. In the first (""warm-up"") stage, discrete variables are marginalized out, yielding an effective continuous-only model. The warm-up stage calculates continuous posteriors based on the marginalized model. Once convergence is achieved, continuous and discrete variables are decoupled for the second (""main"") stage. The second stage starts with a discrete calling step (crucial), using continuous posteriors from the warm-up stage as the starting point. The motivation behind the two-stage inference strategy is to avoid getting trapped in spurious local minima that are potentially introduced by mean-field decoupling of discrete and continuous RVs. Note that mean-field decoupling has a tendency to stabilize local minima, most of which will disappear or turn into saddle points once correlations are taken into account. Whi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4720
https://github.com/broadinstitute/gatk/pull/4720:931,Deployability,continuous,continuous,931,"Summary of changes:. - Fixed a minor issue in sampling error estimation that could lead to NaN (as a result of division by zero). - Introduced separate _internal_ and _external_ admixing rates. The _internal_ admixing rate is to be used internally by discrete RV posterior update routines (""callers"") as a safety measure to stabilize self-consistency loops. For example, consider the mean-field treatment of two coupled Markov chains: the mean-field decoupling of the two chains yields two independent Markov chains with effective emission, transition, and prior probabilities, all of which must be self-consistency determined. The internal admixing rate would be used to admix the old and new self-consistent fields across the two chains in order to dampen oscillations and improve convergence properties. Once internal convergence is achieved, the converged posteriors must be saved to a workspace in order to be consumed by the continuous sub-model. The new internally converged posteriors will be admixed with the old internally converged posteriors from the previous epoch with the _external_ admixing rate. - Introduced two-stage inference for cohort denoising and calling. In the first (""warm-up"") stage, discrete variables are marginalized out, yielding an effective continuous-only model. The warm-up stage calculates continuous posteriors based on the marginalized model. Once convergence is achieved, continuous and discrete variables are decoupled for the second (""main"") stage. The second stage starts with a discrete calling step (crucial), using continuous posteriors from the warm-up stage as the starting point. The motivation behind the two-stage inference strategy is to avoid getting trapped in spurious local minima that are potentially introduced by mean-field decoupling of discrete and continuous RVs. Note that mean-field decoupling has a tendency to stabilize local minima, most of which will disappear or turn into saddle points once correlations are taken into account. Whi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4720
https://github.com/broadinstitute/gatk/pull/4720:1275,Deployability,continuous,continuous-only,1275,"nternally by discrete RV posterior update routines (""callers"") as a safety measure to stabilize self-consistency loops. For example, consider the mean-field treatment of two coupled Markov chains: the mean-field decoupling of the two chains yields two independent Markov chains with effective emission, transition, and prior probabilities, all of which must be self-consistency determined. The internal admixing rate would be used to admix the old and new self-consistent fields across the two chains in order to dampen oscillations and improve convergence properties. Once internal convergence is achieved, the converged posteriors must be saved to a workspace in order to be consumed by the continuous sub-model. The new internally converged posteriors will be admixed with the old internally converged posteriors from the previous epoch with the _external_ admixing rate. - Introduced two-stage inference for cohort denoising and calling. In the first (""warm-up"") stage, discrete variables are marginalized out, yielding an effective continuous-only model. The warm-up stage calculates continuous posteriors based on the marginalized model. Once convergence is achieved, continuous and discrete variables are decoupled for the second (""main"") stage. The second stage starts with a discrete calling step (crucial), using continuous posteriors from the warm-up stage as the starting point. The motivation behind the two-stage inference strategy is to avoid getting trapped in spurious local minima that are potentially introduced by mean-field decoupling of discrete and continuous RVs. Note that mean-field decoupling has a tendency to stabilize local minima, most of which will disappear or turn into saddle points once correlations are taken into account. While the marginalized model is free of such spurious local minima, it does not yield discrete posteriors in a tractable way; hence, the necessity of ultimately decoupling in the ""main"" stage. - Capped phred-scaled qualities to maximum valu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4720
https://github.com/broadinstitute/gatk/pull/4720:1327,Deployability,continuous,continuous,1327,"consistency loops. For example, consider the mean-field treatment of two coupled Markov chains: the mean-field decoupling of the two chains yields two independent Markov chains with effective emission, transition, and prior probabilities, all of which must be self-consistency determined. The internal admixing rate would be used to admix the old and new self-consistent fields across the two chains in order to dampen oscillations and improve convergence properties. Once internal convergence is achieved, the converged posteriors must be saved to a workspace in order to be consumed by the continuous sub-model. The new internally converged posteriors will be admixed with the old internally converged posteriors from the previous epoch with the _external_ admixing rate. - Introduced two-stage inference for cohort denoising and calling. In the first (""warm-up"") stage, discrete variables are marginalized out, yielding an effective continuous-only model. The warm-up stage calculates continuous posteriors based on the marginalized model. Once convergence is achieved, continuous and discrete variables are decoupled for the second (""main"") stage. The second stage starts with a discrete calling step (crucial), using continuous posteriors from the warm-up stage as the starting point. The motivation behind the two-stage inference strategy is to avoid getting trapped in spurious local minima that are potentially introduced by mean-field decoupling of discrete and continuous RVs. Note that mean-field decoupling has a tendency to stabilize local minima, most of which will disappear or turn into saddle points once correlations are taken into account. While the marginalized model is free of such spurious local minima, it does not yield discrete posteriors in a tractable way; hence, the necessity of ultimately decoupling in the ""main"" stage. - Capped phred-scaled qualities to maximum values permitted by machine precision in order to avoid NaNs and overflows. - Took a first step toward tr",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4720
https://github.com/broadinstitute/gatk/pull/4720:1412,Deployability,continuous,continuous,1412,"the mean-field decoupling of the two chains yields two independent Markov chains with effective emission, transition, and prior probabilities, all of which must be self-consistency determined. The internal admixing rate would be used to admix the old and new self-consistent fields across the two chains in order to dampen oscillations and improve convergence properties. Once internal convergence is achieved, the converged posteriors must be saved to a workspace in order to be consumed by the continuous sub-model. The new internally converged posteriors will be admixed with the old internally converged posteriors from the previous epoch with the _external_ admixing rate. - Introduced two-stage inference for cohort denoising and calling. In the first (""warm-up"") stage, discrete variables are marginalized out, yielding an effective continuous-only model. The warm-up stage calculates continuous posteriors based on the marginalized model. Once convergence is achieved, continuous and discrete variables are decoupled for the second (""main"") stage. The second stage starts with a discrete calling step (crucial), using continuous posteriors from the warm-up stage as the starting point. The motivation behind the two-stage inference strategy is to avoid getting trapped in spurious local minima that are potentially introduced by mean-field decoupling of discrete and continuous RVs. Note that mean-field decoupling has a tendency to stabilize local minima, most of which will disappear or turn into saddle points once correlations are taken into account. While the marginalized model is free of such spurious local minima, it does not yield discrete posteriors in a tractable way; hence, the necessity of ultimately decoupling in the ""main"" stage. - Capped phred-scaled qualities to maximum values permitted by machine precision in order to avoid NaNs and overflows. - Took a first step toward tracking and logging parameters during inference, starting with the ELBO history. In the future, it",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4720
https://github.com/broadinstitute/gatk/pull/4720:1561,Deployability,continuous,continuous,1561,"ior probabilities, all of which must be self-consistency determined. The internal admixing rate would be used to admix the old and new self-consistent fields across the two chains in order to dampen oscillations and improve convergence properties. Once internal convergence is achieved, the converged posteriors must be saved to a workspace in order to be consumed by the continuous sub-model. The new internally converged posteriors will be admixed with the old internally converged posteriors from the previous epoch with the _external_ admixing rate. - Introduced two-stage inference for cohort denoising and calling. In the first (""warm-up"") stage, discrete variables are marginalized out, yielding an effective continuous-only model. The warm-up stage calculates continuous posteriors based on the marginalized model. Once convergence is achieved, continuous and discrete variables are decoupled for the second (""main"") stage. The second stage starts with a discrete calling step (crucial), using continuous posteriors from the warm-up stage as the starting point. The motivation behind the two-stage inference strategy is to avoid getting trapped in spurious local minima that are potentially introduced by mean-field decoupling of discrete and continuous RVs. Note that mean-field decoupling has a tendency to stabilize local minima, most of which will disappear or turn into saddle points once correlations are taken into account. While the marginalized model is free of such spurious local minima, it does not yield discrete posteriors in a tractable way; hence, the necessity of ultimately decoupling in the ""main"" stage. - Capped phred-scaled qualities to maximum values permitted by machine precision in order to avoid NaNs and overflows. - Took a first step toward tracking and logging parameters during inference, starting with the ELBO history. In the future, it may be desirable to allow tracking of arbitrary RVs and deterministics via command line args (for debugging and explorator",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4720
https://github.com/broadinstitute/gatk/pull/4720:1810,Deployability,continuous,continuous,1810,"e two chains in order to dampen oscillations and improve convergence properties. Once internal convergence is achieved, the converged posteriors must be saved to a workspace in order to be consumed by the continuous sub-model. The new internally converged posteriors will be admixed with the old internally converged posteriors from the previous epoch with the _external_ admixing rate. - Introduced two-stage inference for cohort denoising and calling. In the first (""warm-up"") stage, discrete variables are marginalized out, yielding an effective continuous-only model. The warm-up stage calculates continuous posteriors based on the marginalized model. Once convergence is achieved, continuous and discrete variables are decoupled for the second (""main"") stage. The second stage starts with a discrete calling step (crucial), using continuous posteriors from the warm-up stage as the starting point. The motivation behind the two-stage inference strategy is to avoid getting trapped in spurious local minima that are potentially introduced by mean-field decoupling of discrete and continuous RVs. Note that mean-field decoupling has a tendency to stabilize local minima, most of which will disappear or turn into saddle points once correlations are taken into account. While the marginalized model is free of such spurious local minima, it does not yield discrete posteriors in a tractable way; hence, the necessity of ultimately decoupling in the ""main"" stage. - Capped phred-scaled qualities to maximum values permitted by machine precision in order to avoid NaNs and overflows. - Took a first step toward tracking and logging parameters during inference, starting with the ELBO history. In the future, it may be desirable to allow tracking of arbitrary RVs and deterministics via command line args (for debugging and exploratory work). Notes:. - We still need to decide about `GermlineCNVCaller` default arguments. See issue #4719.; - The case denoising and calling is unlikely to benefit from t",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4720
https://github.com/broadinstitute/gatk/pull/4720:280,Integrability,rout,routines,280,"Summary of changes:. - Fixed a minor issue in sampling error estimation that could lead to NaN (as a result of division by zero). - Introduced separate _internal_ and _external_ admixing rates. The _internal_ admixing rate is to be used internally by discrete RV posterior update routines (""callers"") as a safety measure to stabilize self-consistency loops. For example, consider the mean-field treatment of two coupled Markov chains: the mean-field decoupling of the two chains yields two independent Markov chains with effective emission, transition, and prior probabilities, all of which must be self-consistency determined. The internal admixing rate would be used to admix the old and new self-consistent fields across the two chains in order to dampen oscillations and improve convergence properties. Once internal convergence is achieved, the converged posteriors must be saved to a workspace in order to be consumed by the continuous sub-model. The new internally converged posteriors will be admixed with the old internally converged posteriors from the previous epoch with the _external_ admixing rate. - Introduced two-stage inference for cohort denoising and calling. In the first (""warm-up"") stage, discrete variables are marginalized out, yielding an effective continuous-only model. The warm-up stage calculates continuous posteriors based on the marginalized model. Once convergence is achieved, continuous and discrete variables are decoupled for the second (""main"") stage. The second stage starts with a discrete calling step (crucial), using continuous posteriors from the warm-up stage as the starting point. The motivation behind the two-stage inference strategy is to avoid getting trapped in spurious local minima that are potentially introduced by mean-field decoupling of discrete and continuous RVs. Note that mean-field decoupling has a tendency to stabilize local minima, most of which will disappear or turn into saddle points once correlations are taken into account. Whi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4720
https://github.com/broadinstitute/gatk/pull/4720:1221,Modifiability,variab,variables,1221,"nternally by discrete RV posterior update routines (""callers"") as a safety measure to stabilize self-consistency loops. For example, consider the mean-field treatment of two coupled Markov chains: the mean-field decoupling of the two chains yields two independent Markov chains with effective emission, transition, and prior probabilities, all of which must be self-consistency determined. The internal admixing rate would be used to admix the old and new self-consistent fields across the two chains in order to dampen oscillations and improve convergence properties. Once internal convergence is achieved, the converged posteriors must be saved to a workspace in order to be consumed by the continuous sub-model. The new internally converged posteriors will be admixed with the old internally converged posteriors from the previous epoch with the _external_ admixing rate. - Introduced two-stage inference for cohort denoising and calling. In the first (""warm-up"") stage, discrete variables are marginalized out, yielding an effective continuous-only model. The warm-up stage calculates continuous posteriors based on the marginalized model. Once convergence is achieved, continuous and discrete variables are decoupled for the second (""main"") stage. The second stage starts with a discrete calling step (crucial), using continuous posteriors from the warm-up stage as the starting point. The motivation behind the two-stage inference strategy is to avoid getting trapped in spurious local minima that are potentially introduced by mean-field decoupling of discrete and continuous RVs. Note that mean-field decoupling has a tendency to stabilize local minima, most of which will disappear or turn into saddle points once correlations are taken into account. While the marginalized model is free of such spurious local minima, it does not yield discrete posteriors in a tractable way; hence, the necessity of ultimately decoupling in the ""main"" stage. - Capped phred-scaled qualities to maximum valu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4720
https://github.com/broadinstitute/gatk/pull/4720:1436,Modifiability,variab,variables,1436,"the mean-field decoupling of the two chains yields two independent Markov chains with effective emission, transition, and prior probabilities, all of which must be self-consistency determined. The internal admixing rate would be used to admix the old and new self-consistent fields across the two chains in order to dampen oscillations and improve convergence properties. Once internal convergence is achieved, the converged posteriors must be saved to a workspace in order to be consumed by the continuous sub-model. The new internally converged posteriors will be admixed with the old internally converged posteriors from the previous epoch with the _external_ admixing rate. - Introduced two-stage inference for cohort denoising and calling. In the first (""warm-up"") stage, discrete variables are marginalized out, yielding an effective continuous-only model. The warm-up stage calculates continuous posteriors based on the marginalized model. Once convergence is achieved, continuous and discrete variables are decoupled for the second (""main"") stage. The second stage starts with a discrete calling step (crucial), using continuous posteriors from the warm-up stage as the starting point. The motivation behind the two-stage inference strategy is to avoid getting trapped in spurious local minima that are potentially introduced by mean-field decoupling of discrete and continuous RVs. Note that mean-field decoupling has a tendency to stabilize local minima, most of which will disappear or turn into saddle points once correlations are taken into account. While the marginalized model is free of such spurious local minima, it does not yield discrete posteriors in a tractable way; hence, the necessity of ultimately decoupling in the ""main"" stage. - Capped phred-scaled qualities to maximum values permitted by machine precision in order to avoid NaNs and overflows. - Took a first step toward tracking and logging parameters during inference, starting with the ELBO history. In the future, it",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4720
https://github.com/broadinstitute/gatk/pull/4720:306,Safety,safe,safety,306,"Summary of changes:. - Fixed a minor issue in sampling error estimation that could lead to NaN (as a result of division by zero). - Introduced separate _internal_ and _external_ admixing rates. The _internal_ admixing rate is to be used internally by discrete RV posterior update routines (""callers"") as a safety measure to stabilize self-consistency loops. For example, consider the mean-field treatment of two coupled Markov chains: the mean-field decoupling of the two chains yields two independent Markov chains with effective emission, transition, and prior probabilities, all of which must be self-consistency determined. The internal admixing rate would be used to admix the old and new self-consistent fields across the two chains in order to dampen oscillations and improve convergence properties. Once internal convergence is achieved, the converged posteriors must be saved to a workspace in order to be consumed by the continuous sub-model. The new internally converged posteriors will be admixed with the old internally converged posteriors from the previous epoch with the _external_ admixing rate. - Introduced two-stage inference for cohort denoising and calling. In the first (""warm-up"") stage, discrete variables are marginalized out, yielding an effective continuous-only model. The warm-up stage calculates continuous posteriors based on the marginalized model. Once convergence is achieved, continuous and discrete variables are decoupled for the second (""main"") stage. The second stage starts with a discrete calling step (crucial), using continuous posteriors from the warm-up stage as the starting point. The motivation behind the two-stage inference strategy is to avoid getting trapped in spurious local minima that are potentially introduced by mean-field decoupling of discrete and continuous RVs. Note that mean-field decoupling has a tendency to stabilize local minima, most of which will disappear or turn into saddle points once correlations are taken into account. Whi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4720
https://github.com/broadinstitute/gatk/pull/4720:1690,Safety,avoid,avoid,1690,"e two chains in order to dampen oscillations and improve convergence properties. Once internal convergence is achieved, the converged posteriors must be saved to a workspace in order to be consumed by the continuous sub-model. The new internally converged posteriors will be admixed with the old internally converged posteriors from the previous epoch with the _external_ admixing rate. - Introduced two-stage inference for cohort denoising and calling. In the first (""warm-up"") stage, discrete variables are marginalized out, yielding an effective continuous-only model. The warm-up stage calculates continuous posteriors based on the marginalized model. Once convergence is achieved, continuous and discrete variables are decoupled for the second (""main"") stage. The second stage starts with a discrete calling step (crucial), using continuous posteriors from the warm-up stage as the starting point. The motivation behind the two-stage inference strategy is to avoid getting trapped in spurious local minima that are potentially introduced by mean-field decoupling of discrete and continuous RVs. Note that mean-field decoupling has a tendency to stabilize local minima, most of which will disappear or turn into saddle points once correlations are taken into account. While the marginalized model is free of such spurious local minima, it does not yield discrete posteriors in a tractable way; hence, the necessity of ultimately decoupling in the ""main"" stage. - Capped phred-scaled qualities to maximum values permitted by machine precision in order to avoid NaNs and overflows. - Took a first step toward tracking and logging parameters during inference, starting with the ELBO history. In the future, it may be desirable to allow tracking of arbitrary RVs and deterministics via command line args (for debugging and exploratory work). Notes:. - We still need to decide about `GermlineCNVCaller` default arguments. See issue #4719.; - The case denoising and calling is unlikely to benefit from t",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4720
https://github.com/broadinstitute/gatk/pull/4720:2284,Safety,avoid,avoid,2284,"ties. Once internal convergence is achieved, the converged posteriors must be saved to a workspace in order to be consumed by the continuous sub-model. The new internally converged posteriors will be admixed with the old internally converged posteriors from the previous epoch with the _external_ admixing rate. - Introduced two-stage inference for cohort denoising and calling. In the first (""warm-up"") stage, discrete variables are marginalized out, yielding an effective continuous-only model. The warm-up stage calculates continuous posteriors based on the marginalized model. Once convergence is achieved, continuous and discrete variables are decoupled for the second (""main"") stage. The second stage starts with a discrete calling step (crucial), using continuous posteriors from the warm-up stage as the starting point. The motivation behind the two-stage inference strategy is to avoid getting trapped in spurious local minima that are potentially introduced by mean-field decoupling of discrete and continuous RVs. Note that mean-field decoupling has a tendency to stabilize local minima, most of which will disappear or turn into saddle points once correlations are taken into account. While the marginalized model is free of such spurious local minima, it does not yield discrete posteriors in a tractable way; hence, the necessity of ultimately decoupling in the ""main"" stage. - Capped phred-scaled qualities to maximum values permitted by machine precision in order to avoid NaNs and overflows. - Took a first step toward tracking and logging parameters during inference, starting with the ELBO history. In the future, it may be desirable to allow tracking of arbitrary RVs and deterministics via command line args (for debugging and exploratory work). Notes:. - We still need to decide about `GermlineCNVCaller` default arguments. See issue #4719.; - The case denoising and calling is unlikely to benefit from the marginalized warmup strategy. Therefore, it is not included in this PR.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4720
https://github.com/broadinstitute/gatk/pull/4720:2350,Testability,log,logging,2350,"ties. Once internal convergence is achieved, the converged posteriors must be saved to a workspace in order to be consumed by the continuous sub-model. The new internally converged posteriors will be admixed with the old internally converged posteriors from the previous epoch with the _external_ admixing rate. - Introduced two-stage inference for cohort denoising and calling. In the first (""warm-up"") stage, discrete variables are marginalized out, yielding an effective continuous-only model. The warm-up stage calculates continuous posteriors based on the marginalized model. Once convergence is achieved, continuous and discrete variables are decoupled for the second (""main"") stage. The second stage starts with a discrete calling step (crucial), using continuous posteriors from the warm-up stage as the starting point. The motivation behind the two-stage inference strategy is to avoid getting trapped in spurious local minima that are potentially introduced by mean-field decoupling of discrete and continuous RVs. Note that mean-field decoupling has a tendency to stabilize local minima, most of which will disappear or turn into saddle points once correlations are taken into account. While the marginalized model is free of such spurious local minima, it does not yield discrete posteriors in a tractable way; hence, the necessity of ultimately decoupling in the ""main"" stage. - Capped phred-scaled qualities to maximum values permitted by machine precision in order to avoid NaNs and overflows. - Took a first step toward tracking and logging parameters during inference, starting with the ELBO history. In the future, it may be desirable to allow tracking of arbitrary RVs and deterministics via command line args (for debugging and exploratory work). Notes:. - We still need to decide about `GermlineCNVCaller` default arguments. See issue #4719.; - The case denoising and calling is unlikely to benefit from the marginalized warmup strategy. Therefore, it is not included in this PR.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4720
https://github.com/broadinstitute/gatk/issues/4722:27,Availability,down,download,27,"I did not use the official download of the Funcotator datasources (1.2), but I did find some extraneous files that should be removed. Complete list is below:. ```; drwxrwx--- 1 root vboxsf 0 Apr 30 15:44 achilles/; drwxrwx--- 1 root vboxsf 0 Apr 30 15:44 cancer_gene_census/; drwxrwx--- 1 root vboxsf 0 Apr 30 15:44 clinvar/; drwxrwx--- 1 root vboxsf 0 Apr 30 15:44 cosmic/; drwxrwx--- 1 root vboxsf 0 Apr 30 15:44 cosmic_fusion/; drwxrwx--- 1 root vboxsf 0 Apr 30 15:44 cosmic_tissue/; drwxrwx--- 1 root vboxsf 0 Apr 30 15:39 dbsnp/; drwxrwx--- 1 root vboxsf 0 Apr 30 15:39 dna_repair_genes/; -rwxrwx--- 1 root vboxsf 6148 Apr 30 15:38 .DS_Store*; drwxrwx--- 1 root vboxsf 0 Apr 30 15:39 familial/; drwxrwx--- 1 root vboxsf 0 Apr 30 15:39 gencode/; drwxrwx--- 1 root vboxsf 0 Apr 30 15:39 gencode_xhgnc/; drwxrwx--- 1 root vboxsf 0 Apr 30 15:39 gencode_xrefseq/; drwxrwx--- 1 root vboxsf 0 Apr 30 15:39 hgnc/; drwxrwx--- 1 root vboxsf 0 Apr 30 15:44 .idea/; drwxrwx--- 1 root vboxsf 0 Apr 30 15:38 oreganno/; -rwxrwx--- 1 root vboxsf 5274 Apr 30 15:38 README.txt*; drwxrwx--- 1 root vboxsf 0 Apr 30 15:38 simple_uniprot/; -rwxrwx--- 1 root vboxsf 1557 Apr 30 15:38 template.config*. ```; `.DS_Store` and `.idea` should not be in the official download.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4722
https://github.com/broadinstitute/gatk/issues/4722:1243,Availability,down,download,1243,"I did not use the official download of the Funcotator datasources (1.2), but I did find some extraneous files that should be removed. Complete list is below:. ```; drwxrwx--- 1 root vboxsf 0 Apr 30 15:44 achilles/; drwxrwx--- 1 root vboxsf 0 Apr 30 15:44 cancer_gene_census/; drwxrwx--- 1 root vboxsf 0 Apr 30 15:44 clinvar/; drwxrwx--- 1 root vboxsf 0 Apr 30 15:44 cosmic/; drwxrwx--- 1 root vboxsf 0 Apr 30 15:44 cosmic_fusion/; drwxrwx--- 1 root vboxsf 0 Apr 30 15:44 cosmic_tissue/; drwxrwx--- 1 root vboxsf 0 Apr 30 15:39 dbsnp/; drwxrwx--- 1 root vboxsf 0 Apr 30 15:39 dna_repair_genes/; -rwxrwx--- 1 root vboxsf 6148 Apr 30 15:38 .DS_Store*; drwxrwx--- 1 root vboxsf 0 Apr 30 15:39 familial/; drwxrwx--- 1 root vboxsf 0 Apr 30 15:39 gencode/; drwxrwx--- 1 root vboxsf 0 Apr 30 15:39 gencode_xhgnc/; drwxrwx--- 1 root vboxsf 0 Apr 30 15:39 gencode_xrefseq/; drwxrwx--- 1 root vboxsf 0 Apr 30 15:39 hgnc/; drwxrwx--- 1 root vboxsf 0 Apr 30 15:44 .idea/; drwxrwx--- 1 root vboxsf 0 Apr 30 15:38 oreganno/; -rwxrwx--- 1 root vboxsf 5274 Apr 30 15:38 README.txt*; drwxrwx--- 1 root vboxsf 0 Apr 30 15:38 simple_uniprot/; -rwxrwx--- 1 root vboxsf 1557 Apr 30 15:38 template.config*. ```; `.DS_Store` and `.idea` should not be in the official download.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4722
https://github.com/broadinstitute/gatk/issues/4722:1175,Modifiability,config,config,1175,"I did not use the official download of the Funcotator datasources (1.2), but I did find some extraneous files that should be removed. Complete list is below:. ```; drwxrwx--- 1 root vboxsf 0 Apr 30 15:44 achilles/; drwxrwx--- 1 root vboxsf 0 Apr 30 15:44 cancer_gene_census/; drwxrwx--- 1 root vboxsf 0 Apr 30 15:44 clinvar/; drwxrwx--- 1 root vboxsf 0 Apr 30 15:44 cosmic/; drwxrwx--- 1 root vboxsf 0 Apr 30 15:44 cosmic_fusion/; drwxrwx--- 1 root vboxsf 0 Apr 30 15:44 cosmic_tissue/; drwxrwx--- 1 root vboxsf 0 Apr 30 15:39 dbsnp/; drwxrwx--- 1 root vboxsf 0 Apr 30 15:39 dna_repair_genes/; -rwxrwx--- 1 root vboxsf 6148 Apr 30 15:38 .DS_Store*; drwxrwx--- 1 root vboxsf 0 Apr 30 15:39 familial/; drwxrwx--- 1 root vboxsf 0 Apr 30 15:39 gencode/; drwxrwx--- 1 root vboxsf 0 Apr 30 15:39 gencode_xhgnc/; drwxrwx--- 1 root vboxsf 0 Apr 30 15:39 gencode_xrefseq/; drwxrwx--- 1 root vboxsf 0 Apr 30 15:39 hgnc/; drwxrwx--- 1 root vboxsf 0 Apr 30 15:44 .idea/; drwxrwx--- 1 root vboxsf 0 Apr 30 15:38 oreganno/; -rwxrwx--- 1 root vboxsf 5274 Apr 30 15:38 README.txt*; drwxrwx--- 1 root vboxsf 0 Apr 30 15:38 simple_uniprot/; -rwxrwx--- 1 root vboxsf 1557 Apr 30 15:38 template.config*. ```; `.DS_Store` and `.idea` should not be in the official download.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4722
https://github.com/broadinstitute/gatk/pull/4723:59,Availability,error,error,59,ceContentsAsFile(....). This gets rid of Hierarchical URI error message.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4723
https://github.com/broadinstitute/gatk/pull/4723:65,Integrability,message,message,65,ceContentsAsFile(....). This gets rid of Hierarchical URI error message.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4723
https://github.com/broadinstitute/gatk/issues/4724:286,Availability,error,error,286,"Hi,. I am trying to call germline CNVs for a set of samples. After running DetermineGermlineContigPloidy and GermlineCNVCaller, I am using PostprocessGermlineCNVCalls to generate the VCF files with CNV calls. The ""interval"" VCF files are generated successfully. But I got the following error message when segmenting contigs:. org.broadinstitute.hellbender.utils.python.PythonScriptExecutorException:; python exited with 1; Command Line: python /tmp/shulik7/segment_gcnv_calls.2338024416841754264.py --ploidy_calls_path /scratch/users/shulik7/test_GATK_CNV/Postprocess/../DetermineGermlineContigPloidy/model/test_run-calls/ --model_shards /scratch/shulik7/test_GATK_CNV/Postprocess/../GermlineCNVCaller/cnvs/test_run-model --calls_shards /scratch/shulik7/test_GATK_CNV/Postprocess/../GermlineCNVCaller/cnvs/test_run-calls --output_path /tmp/shulik7/gcnv-segmented-calls28280883609685538 --sample_index 0; Stdout: 11:32:16.728 INFO segment_gcnv_calls - Loading ploidy calls...; 11:32:16.729 INFO gcnvkernel.io.io_metadata - Loading germline contig ploidy and global read depth metadata...; 11:32:16.730 INFO segment_gcnv_calls - Instantiating the Viterbi segmentation engine...; 11:32:18.585 INFO gcnvkernel.postprocess.viterbi_segmentation - Assembling interval list and copy-number class posterior from model shards...; 11:32:25.158 INFO gcnvkernel.structs.metadata - Generating intervals metadata...; 11:32:27.543 INFO gcnvkernel.postprocess.viterbi_segmentation - Compiling theano forward-backward function...; 11:32:34.406 INFO gcnvkernel.postprocess.viterbi_segmentation - Compiling theano Viterbi function...; 11:32:40.598 INFO gcnvkernel.postprocess.viterbi_segmentation - Compiling theano variational HHMM...; 11:32:42.862 INFO gcnvkernel.postprocess.viterbi_segmentation - Processing sample index: 0, sample name: test_sample_0...; 11:32:43.631 INFO gcnvkernel.postprocess.viterbi_segmentation - Segmenting contig (1/24) (contig name: 1)... Stderr: Traceback (most recent call last):; File ""/t",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4724
https://github.com/broadinstitute/gatk/issues/4724:292,Integrability,message,message,292,"Hi,. I am trying to call germline CNVs for a set of samples. After running DetermineGermlineContigPloidy and GermlineCNVCaller, I am using PostprocessGermlineCNVCalls to generate the VCF files with CNV calls. The ""interval"" VCF files are generated successfully. But I got the following error message when segmenting contigs:. org.broadinstitute.hellbender.utils.python.PythonScriptExecutorException:; python exited with 1; Command Line: python /tmp/shulik7/segment_gcnv_calls.2338024416841754264.py --ploidy_calls_path /scratch/users/shulik7/test_GATK_CNV/Postprocess/../DetermineGermlineContigPloidy/model/test_run-calls/ --model_shards /scratch/shulik7/test_GATK_CNV/Postprocess/../GermlineCNVCaller/cnvs/test_run-model --calls_shards /scratch/shulik7/test_GATK_CNV/Postprocess/../GermlineCNVCaller/cnvs/test_run-calls --output_path /tmp/shulik7/gcnv-segmented-calls28280883609685538 --sample_index 0; Stdout: 11:32:16.728 INFO segment_gcnv_calls - Loading ploidy calls...; 11:32:16.729 INFO gcnvkernel.io.io_metadata - Loading germline contig ploidy and global read depth metadata...; 11:32:16.730 INFO segment_gcnv_calls - Instantiating the Viterbi segmentation engine...; 11:32:18.585 INFO gcnvkernel.postprocess.viterbi_segmentation - Assembling interval list and copy-number class posterior from model shards...; 11:32:25.158 INFO gcnvkernel.structs.metadata - Generating intervals metadata...; 11:32:27.543 INFO gcnvkernel.postprocess.viterbi_segmentation - Compiling theano forward-backward function...; 11:32:34.406 INFO gcnvkernel.postprocess.viterbi_segmentation - Compiling theano Viterbi function...; 11:32:40.598 INFO gcnvkernel.postprocess.viterbi_segmentation - Compiling theano variational HHMM...; 11:32:42.862 INFO gcnvkernel.postprocess.viterbi_segmentation - Processing sample index: 0, sample name: test_sample_0...; 11:32:43.631 INFO gcnvkernel.postprocess.viterbi_segmentation - Segmenting contig (1/24) (contig name: 1)... Stderr: Traceback (most recent call last):; File ""/t",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4724
https://github.com/broadinstitute/gatk/issues/4724:951,Performance,Load,Loading,951,"Hi,. I am trying to call germline CNVs for a set of samples. After running DetermineGermlineContigPloidy and GermlineCNVCaller, I am using PostprocessGermlineCNVCalls to generate the VCF files with CNV calls. The ""interval"" VCF files are generated successfully. But I got the following error message when segmenting contigs:. org.broadinstitute.hellbender.utils.python.PythonScriptExecutorException:; python exited with 1; Command Line: python /tmp/shulik7/segment_gcnv_calls.2338024416841754264.py --ploidy_calls_path /scratch/users/shulik7/test_GATK_CNV/Postprocess/../DetermineGermlineContigPloidy/model/test_run-calls/ --model_shards /scratch/shulik7/test_GATK_CNV/Postprocess/../GermlineCNVCaller/cnvs/test_run-model --calls_shards /scratch/shulik7/test_GATK_CNV/Postprocess/../GermlineCNVCaller/cnvs/test_run-calls --output_path /tmp/shulik7/gcnv-segmented-calls28280883609685538 --sample_index 0; Stdout: 11:32:16.728 INFO segment_gcnv_calls - Loading ploidy calls...; 11:32:16.729 INFO gcnvkernel.io.io_metadata - Loading germline contig ploidy and global read depth metadata...; 11:32:16.730 INFO segment_gcnv_calls - Instantiating the Viterbi segmentation engine...; 11:32:18.585 INFO gcnvkernel.postprocess.viterbi_segmentation - Assembling interval list and copy-number class posterior from model shards...; 11:32:25.158 INFO gcnvkernel.structs.metadata - Generating intervals metadata...; 11:32:27.543 INFO gcnvkernel.postprocess.viterbi_segmentation - Compiling theano forward-backward function...; 11:32:34.406 INFO gcnvkernel.postprocess.viterbi_segmentation - Compiling theano Viterbi function...; 11:32:40.598 INFO gcnvkernel.postprocess.viterbi_segmentation - Compiling theano variational HHMM...; 11:32:42.862 INFO gcnvkernel.postprocess.viterbi_segmentation - Processing sample index: 0, sample name: test_sample_0...; 11:32:43.631 INFO gcnvkernel.postprocess.viterbi_segmentation - Segmenting contig (1/24) (contig name: 1)... Stderr: Traceback (most recent call last):; File ""/t",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4724
https://github.com/broadinstitute/gatk/issues/4724:1022,Performance,Load,Loading,1022,"et of samples. After running DetermineGermlineContigPloidy and GermlineCNVCaller, I am using PostprocessGermlineCNVCalls to generate the VCF files with CNV calls. The ""interval"" VCF files are generated successfully. But I got the following error message when segmenting contigs:. org.broadinstitute.hellbender.utils.python.PythonScriptExecutorException:; python exited with 1; Command Line: python /tmp/shulik7/segment_gcnv_calls.2338024416841754264.py --ploidy_calls_path /scratch/users/shulik7/test_GATK_CNV/Postprocess/../DetermineGermlineContigPloidy/model/test_run-calls/ --model_shards /scratch/shulik7/test_GATK_CNV/Postprocess/../GermlineCNVCaller/cnvs/test_run-model --calls_shards /scratch/shulik7/test_GATK_CNV/Postprocess/../GermlineCNVCaller/cnvs/test_run-calls --output_path /tmp/shulik7/gcnv-segmented-calls28280883609685538 --sample_index 0; Stdout: 11:32:16.728 INFO segment_gcnv_calls - Loading ploidy calls...; 11:32:16.729 INFO gcnvkernel.io.io_metadata - Loading germline contig ploidy and global read depth metadata...; 11:32:16.730 INFO segment_gcnv_calls - Instantiating the Viterbi segmentation engine...; 11:32:18.585 INFO gcnvkernel.postprocess.viterbi_segmentation - Assembling interval list and copy-number class posterior from model shards...; 11:32:25.158 INFO gcnvkernel.structs.metadata - Generating intervals metadata...; 11:32:27.543 INFO gcnvkernel.postprocess.viterbi_segmentation - Compiling theano forward-backward function...; 11:32:34.406 INFO gcnvkernel.postprocess.viterbi_segmentation - Compiling theano Viterbi function...; 11:32:40.598 INFO gcnvkernel.postprocess.viterbi_segmentation - Compiling theano variational HHMM...; 11:32:42.862 INFO gcnvkernel.postprocess.viterbi_segmentation - Processing sample index: 0, sample name: test_sample_0...; 11:32:43.631 INFO gcnvkernel.postprocess.viterbi_segmentation - Segmenting contig (1/24) (contig name: 1)... Stderr: Traceback (most recent call last):; File ""/tmp/shulik7/segment_gcnv_calls.233802441684175",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4724
https://github.com/broadinstitute/gatk/issues/4725:90,Availability,error,error,90,"PathSeq is failing on the input files I'm using and its difficult for me to interpret the error message. What I'm seeing is,. 1. ERROR LiveListenerBus: SparkListenerBus has already stopped!; 2. Job aborted due to stage failure: Task 20 in stage 1.0 failed 1 times; 3. WARN ShutdownHookManager: ShutdownHook '$anon$2' timeout; 4. WARN ShutdownHookManager: ShutdownHook 'ClientFinalizer' timeout; 5. ERROR ShutdownHookManager: ShutdownHookManger shutdown forcefully.; 6. /var/spool/slurmd/job1619084/slurm_script: line 126: syntax error: unexpected end of file. In that order. I'm running this script in parallel on a SLURM scheduler (four cpus with 8Gb mem/cpu). Here is a sample of the last few lines of STDERR, but I'm also attaching the full error output.; [pathseq_TCGA.slurm.1619078_1.err.txt](https://github.com/broadinstitute/gatk/files/1965063/pathseq_TCGA.slurm.1619078_1.err.txt). Thanks so much for any help you can provide!. `; 18/05/01 14:20:59 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(driver, 10.12.137.46, 39719, None),broadcast_1_piece0,StorageLevel(memory, 1 replicas),127561,0)); 18/05/01 14:21:18 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 18/05/01 14:23:29 INFO MemoryStore: MemoryStore cleared; 18/05/01 14:23:29 INFO BlockManager: BlockManager stopped; 18/05/01 14:23:29 INFO BlockManagerMaster: BlockManagerMaster stopped; 18/05/01 14:24:38 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/05/01 14:25:36 INFO SparkContext: Successfully stopped SparkContext; 14:25:37.027 INFO PathSeqPipelineSpark - Shutting down engine; [May 1, 2018 2:25:37 PM EDT] org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark done. Elapsed time: 37.98 minutes.; Runtime.totalMemory()=23999283200; org.apache.spark.SparkException: Job aborted due to stage failure: Task 20 in stage 1.0 failed 1 times, most",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4725
https://github.com/broadinstitute/gatk/issues/4725:129,Availability,ERROR,ERROR,129,"PathSeq is failing on the input files I'm using and its difficult for me to interpret the error message. What I'm seeing is,. 1. ERROR LiveListenerBus: SparkListenerBus has already stopped!; 2. Job aborted due to stage failure: Task 20 in stage 1.0 failed 1 times; 3. WARN ShutdownHookManager: ShutdownHook '$anon$2' timeout; 4. WARN ShutdownHookManager: ShutdownHook 'ClientFinalizer' timeout; 5. ERROR ShutdownHookManager: ShutdownHookManger shutdown forcefully.; 6. /var/spool/slurmd/job1619084/slurm_script: line 126: syntax error: unexpected end of file. In that order. I'm running this script in parallel on a SLURM scheduler (four cpus with 8Gb mem/cpu). Here is a sample of the last few lines of STDERR, but I'm also attaching the full error output.; [pathseq_TCGA.slurm.1619078_1.err.txt](https://github.com/broadinstitute/gatk/files/1965063/pathseq_TCGA.slurm.1619078_1.err.txt). Thanks so much for any help you can provide!. `; 18/05/01 14:20:59 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(driver, 10.12.137.46, 39719, None),broadcast_1_piece0,StorageLevel(memory, 1 replicas),127561,0)); 18/05/01 14:21:18 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 18/05/01 14:23:29 INFO MemoryStore: MemoryStore cleared; 18/05/01 14:23:29 INFO BlockManager: BlockManager stopped; 18/05/01 14:23:29 INFO BlockManagerMaster: BlockManagerMaster stopped; 18/05/01 14:24:38 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/05/01 14:25:36 INFO SparkContext: Successfully stopped SparkContext; 14:25:37.027 INFO PathSeqPipelineSpark - Shutting down engine; [May 1, 2018 2:25:37 PM EDT] org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark done. Elapsed time: 37.98 minutes.; Runtime.totalMemory()=23999283200; org.apache.spark.SparkException: Job aborted due to stage failure: Task 20 in stage 1.0 failed 1 times, most",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4725
https://github.com/broadinstitute/gatk/issues/4725:219,Availability,failure,failure,219,"PathSeq is failing on the input files I'm using and its difficult for me to interpret the error message. What I'm seeing is,. 1. ERROR LiveListenerBus: SparkListenerBus has already stopped!; 2. Job aborted due to stage failure: Task 20 in stage 1.0 failed 1 times; 3. WARN ShutdownHookManager: ShutdownHook '$anon$2' timeout; 4. WARN ShutdownHookManager: ShutdownHook 'ClientFinalizer' timeout; 5. ERROR ShutdownHookManager: ShutdownHookManger shutdown forcefully.; 6. /var/spool/slurmd/job1619084/slurm_script: line 126: syntax error: unexpected end of file. In that order. I'm running this script in parallel on a SLURM scheduler (four cpus with 8Gb mem/cpu). Here is a sample of the last few lines of STDERR, but I'm also attaching the full error output.; [pathseq_TCGA.slurm.1619078_1.err.txt](https://github.com/broadinstitute/gatk/files/1965063/pathseq_TCGA.slurm.1619078_1.err.txt). Thanks so much for any help you can provide!. `; 18/05/01 14:20:59 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(driver, 10.12.137.46, 39719, None),broadcast_1_piece0,StorageLevel(memory, 1 replicas),127561,0)); 18/05/01 14:21:18 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 18/05/01 14:23:29 INFO MemoryStore: MemoryStore cleared; 18/05/01 14:23:29 INFO BlockManager: BlockManager stopped; 18/05/01 14:23:29 INFO BlockManagerMaster: BlockManagerMaster stopped; 18/05/01 14:24:38 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/05/01 14:25:36 INFO SparkContext: Successfully stopped SparkContext; 14:25:37.027 INFO PathSeqPipelineSpark - Shutting down engine; [May 1, 2018 2:25:37 PM EDT] org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark done. Elapsed time: 37.98 minutes.; Runtime.totalMemory()=23999283200; org.apache.spark.SparkException: Job aborted due to stage failure: Task 20 in stage 1.0 failed 1 times, most",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4725
https://github.com/broadinstitute/gatk/issues/4725:398,Availability,ERROR,ERROR,398,"PathSeq is failing on the input files I'm using and its difficult for me to interpret the error message. What I'm seeing is,. 1. ERROR LiveListenerBus: SparkListenerBus has already stopped!; 2. Job aborted due to stage failure: Task 20 in stage 1.0 failed 1 times; 3. WARN ShutdownHookManager: ShutdownHook '$anon$2' timeout; 4. WARN ShutdownHookManager: ShutdownHook 'ClientFinalizer' timeout; 5. ERROR ShutdownHookManager: ShutdownHookManger shutdown forcefully.; 6. /var/spool/slurmd/job1619084/slurm_script: line 126: syntax error: unexpected end of file. In that order. I'm running this script in parallel on a SLURM scheduler (four cpus with 8Gb mem/cpu). Here is a sample of the last few lines of STDERR, but I'm also attaching the full error output.; [pathseq_TCGA.slurm.1619078_1.err.txt](https://github.com/broadinstitute/gatk/files/1965063/pathseq_TCGA.slurm.1619078_1.err.txt). Thanks so much for any help you can provide!. `; 18/05/01 14:20:59 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(driver, 10.12.137.46, 39719, None),broadcast_1_piece0,StorageLevel(memory, 1 replicas),127561,0)); 18/05/01 14:21:18 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 18/05/01 14:23:29 INFO MemoryStore: MemoryStore cleared; 18/05/01 14:23:29 INFO BlockManager: BlockManager stopped; 18/05/01 14:23:29 INFO BlockManagerMaster: BlockManagerMaster stopped; 18/05/01 14:24:38 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/05/01 14:25:36 INFO SparkContext: Successfully stopped SparkContext; 14:25:37.027 INFO PathSeqPipelineSpark - Shutting down engine; [May 1, 2018 2:25:37 PM EDT] org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark done. Elapsed time: 37.98 minutes.; Runtime.totalMemory()=23999283200; org.apache.spark.SparkException: Job aborted due to stage failure: Task 20 in stage 1.0 failed 1 times, most",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4725
https://github.com/broadinstitute/gatk/issues/4725:529,Availability,error,error,529,"PathSeq is failing on the input files I'm using and its difficult for me to interpret the error message. What I'm seeing is,. 1. ERROR LiveListenerBus: SparkListenerBus has already stopped!; 2. Job aborted due to stage failure: Task 20 in stage 1.0 failed 1 times; 3. WARN ShutdownHookManager: ShutdownHook '$anon$2' timeout; 4. WARN ShutdownHookManager: ShutdownHook 'ClientFinalizer' timeout; 5. ERROR ShutdownHookManager: ShutdownHookManger shutdown forcefully.; 6. /var/spool/slurmd/job1619084/slurm_script: line 126: syntax error: unexpected end of file. In that order. I'm running this script in parallel on a SLURM scheduler (four cpus with 8Gb mem/cpu). Here is a sample of the last few lines of STDERR, but I'm also attaching the full error output.; [pathseq_TCGA.slurm.1619078_1.err.txt](https://github.com/broadinstitute/gatk/files/1965063/pathseq_TCGA.slurm.1619078_1.err.txt). Thanks so much for any help you can provide!. `; 18/05/01 14:20:59 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(driver, 10.12.137.46, 39719, None),broadcast_1_piece0,StorageLevel(memory, 1 replicas),127561,0)); 18/05/01 14:21:18 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 18/05/01 14:23:29 INFO MemoryStore: MemoryStore cleared; 18/05/01 14:23:29 INFO BlockManager: BlockManager stopped; 18/05/01 14:23:29 INFO BlockManagerMaster: BlockManagerMaster stopped; 18/05/01 14:24:38 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/05/01 14:25:36 INFO SparkContext: Successfully stopped SparkContext; 14:25:37.027 INFO PathSeqPipelineSpark - Shutting down engine; [May 1, 2018 2:25:37 PM EDT] org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark done. Elapsed time: 37.98 minutes.; Runtime.totalMemory()=23999283200; org.apache.spark.SparkException: Job aborted due to stage failure: Task 20 in stage 1.0 failed 1 times, most",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4725
https://github.com/broadinstitute/gatk/issues/4725:744,Availability,error,error,744,"PathSeq is failing on the input files I'm using and its difficult for me to interpret the error message. What I'm seeing is,. 1. ERROR LiveListenerBus: SparkListenerBus has already stopped!; 2. Job aborted due to stage failure: Task 20 in stage 1.0 failed 1 times; 3. WARN ShutdownHookManager: ShutdownHook '$anon$2' timeout; 4. WARN ShutdownHookManager: ShutdownHook 'ClientFinalizer' timeout; 5. ERROR ShutdownHookManager: ShutdownHookManger shutdown forcefully.; 6. /var/spool/slurmd/job1619084/slurm_script: line 126: syntax error: unexpected end of file. In that order. I'm running this script in parallel on a SLURM scheduler (four cpus with 8Gb mem/cpu). Here is a sample of the last few lines of STDERR, but I'm also attaching the full error output.; [pathseq_TCGA.slurm.1619078_1.err.txt](https://github.com/broadinstitute/gatk/files/1965063/pathseq_TCGA.slurm.1619078_1.err.txt). Thanks so much for any help you can provide!. `; 18/05/01 14:20:59 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(driver, 10.12.137.46, 39719, None),broadcast_1_piece0,StorageLevel(memory, 1 replicas),127561,0)); 18/05/01 14:21:18 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 18/05/01 14:23:29 INFO MemoryStore: MemoryStore cleared; 18/05/01 14:23:29 INFO BlockManager: BlockManager stopped; 18/05/01 14:23:29 INFO BlockManagerMaster: BlockManagerMaster stopped; 18/05/01 14:24:38 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/05/01 14:25:36 INFO SparkContext: Successfully stopped SparkContext; 14:25:37.027 INFO PathSeqPipelineSpark - Shutting down engine; [May 1, 2018 2:25:37 PM EDT] org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark done. Elapsed time: 37.98 minutes.; Runtime.totalMemory()=23999283200; org.apache.spark.SparkException: Job aborted due to stage failure: Task 20 in stage 1.0 failed 1 times, most",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4725
https://github.com/broadinstitute/gatk/issues/4725:957,Availability,ERROR,ERROR,957," on the input files I'm using and its difficult for me to interpret the error message. What I'm seeing is,. 1. ERROR LiveListenerBus: SparkListenerBus has already stopped!; 2. Job aborted due to stage failure: Task 20 in stage 1.0 failed 1 times; 3. WARN ShutdownHookManager: ShutdownHook '$anon$2' timeout; 4. WARN ShutdownHookManager: ShutdownHook 'ClientFinalizer' timeout; 5. ERROR ShutdownHookManager: ShutdownHookManger shutdown forcefully.; 6. /var/spool/slurmd/job1619084/slurm_script: line 126: syntax error: unexpected end of file. In that order. I'm running this script in parallel on a SLURM scheduler (four cpus with 8Gb mem/cpu). Here is a sample of the last few lines of STDERR, but I'm also attaching the full error output.; [pathseq_TCGA.slurm.1619078_1.err.txt](https://github.com/broadinstitute/gatk/files/1965063/pathseq_TCGA.slurm.1619078_1.err.txt). Thanks so much for any help you can provide!. `; 18/05/01 14:20:59 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(driver, 10.12.137.46, 39719, None),broadcast_1_piece0,StorageLevel(memory, 1 replicas),127561,0)); 18/05/01 14:21:18 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 18/05/01 14:23:29 INFO MemoryStore: MemoryStore cleared; 18/05/01 14:23:29 INFO BlockManager: BlockManager stopped; 18/05/01 14:23:29 INFO BlockManagerMaster: BlockManagerMaster stopped; 18/05/01 14:24:38 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/05/01 14:25:36 INFO SparkContext: Successfully stopped SparkContext; 14:25:37.027 INFO PathSeqPipelineSpark - Shutting down engine; [May 1, 2018 2:25:37 PM EDT] org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark done. Elapsed time: 37.98 minutes.; Runtime.totalMemory()=23999283200; org.apache.spark.SparkException: Job aborted due to stage failure: Task 20 in stage 1.0 failed 1 times, most recent failure: L",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4725
https://github.com/broadinstitute/gatk/issues/4725:1709,Availability,down,down,1709," but I'm also attaching the full error output.; [pathseq_TCGA.slurm.1619078_1.err.txt](https://github.com/broadinstitute/gatk/files/1965063/pathseq_TCGA.slurm.1619078_1.err.txt). Thanks so much for any help you can provide!. `; 18/05/01 14:20:59 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(driver, 10.12.137.46, 39719, None),broadcast_1_piece0,StorageLevel(memory, 1 replicas),127561,0)); 18/05/01 14:21:18 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 18/05/01 14:23:29 INFO MemoryStore: MemoryStore cleared; 18/05/01 14:23:29 INFO BlockManager: BlockManager stopped; 18/05/01 14:23:29 INFO BlockManagerMaster: BlockManagerMaster stopped; 18/05/01 14:24:38 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/05/01 14:25:36 INFO SparkContext: Successfully stopped SparkContext; 14:25:37.027 INFO PathSeqPipelineSpark - Shutting down engine; [May 1, 2018 2:25:37 PM EDT] org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark done. Elapsed time: 37.98 minutes.; Runtime.totalMemory()=23999283200; org.apache.spark.SparkException: Job aborted due to stage failure: Task 20 in stage 1.0 failed 1 times, most recent failure: Lost task 20.0 in stage 1.0 (TID 891, localhost, executor driver): ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 131031 ms; Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(Array",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4725
https://github.com/broadinstitute/gatk/issues/4725:1951,Availability,failure,failure,1951,"/01 14:20:59 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(driver, 10.12.137.46, 39719, None),broadcast_1_piece0,StorageLevel(memory, 1 replicas),127561,0)); 18/05/01 14:21:18 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 18/05/01 14:23:29 INFO MemoryStore: MemoryStore cleared; 18/05/01 14:23:29 INFO BlockManager: BlockManager stopped; 18/05/01 14:23:29 INFO BlockManagerMaster: BlockManagerMaster stopped; 18/05/01 14:24:38 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/05/01 14:25:36 INFO SparkContext: Successfully stopped SparkContext; 14:25:37.027 INFO PathSeqPipelineSpark - Shutting down engine; [May 1, 2018 2:25:37 PM EDT] org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark done. Elapsed time: 37.98 minutes.; Runtime.totalMemory()=23999283200; org.apache.spark.SparkException: Job aborted due to stage failure: Task 20 in stage 1.0 failed 1 times, most recent failure: Lost task 20.0 in stage 1.0 (TID 891, localhost, executor driver): ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 131031 ms; Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.schedul",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4725
https://github.com/broadinstitute/gatk/issues/4725:2009,Availability,failure,failure,2009,"ady stopped! Dropping event SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(driver, 10.12.137.46, 39719, None),broadcast_1_piece0,StorageLevel(memory, 1 replicas),127561,0)); 18/05/01 14:21:18 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 18/05/01 14:23:29 INFO MemoryStore: MemoryStore cleared; 18/05/01 14:23:29 INFO BlockManager: BlockManager stopped; 18/05/01 14:23:29 INFO BlockManagerMaster: BlockManagerMaster stopped; 18/05/01 14:24:38 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/05/01 14:25:36 INFO SparkContext: Successfully stopped SparkContext; 14:25:37.027 INFO PathSeqPipelineSpark - Shutting down engine; [May 1, 2018 2:25:37 PM EDT] org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark done. Elapsed time: 37.98 minutes.; Runtime.totalMemory()=23999283200; org.apache.spark.SparkException: Job aborted due to stage failure: Task 20 in stage 1.0 failed 1 times, most recent failure: Lost task 20.0 in stage 1.0 (TID 891, localhost, executor driver): ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 131031 ms; Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGSche",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4725
https://github.com/broadinstitute/gatk/issues/4725:2182,Availability,heartbeat,heartbeat,2182,"StorageLevel(memory, 1 replicas),127561,0)); 18/05/01 14:21:18 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 18/05/01 14:23:29 INFO MemoryStore: MemoryStore cleared; 18/05/01 14:23:29 INFO BlockManager: BlockManager stopped; 18/05/01 14:23:29 INFO BlockManagerMaster: BlockManagerMaster stopped; 18/05/01 14:24:38 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/05/01 14:25:36 INFO SparkContext: Successfully stopped SparkContext; 14:25:37.027 INFO PathSeqPipelineSpark - Shutting down engine; [May 1, 2018 2:25:37 PM EDT] org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark done. Elapsed time: 37.98 minutes.; Runtime.totalMemory()=23999283200; org.apache.spark.SparkException: Job aborted due to stage failure: Task 20 in stage 1.0 failed 1 times, most recent failure: Lost task 20.0 in stage 1.0 (TID 891, localhost, executor driver): ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 131031 ms; Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.sc",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4725
https://github.com/broadinstitute/gatk/issues/4725:5723,Availability,ERROR,ERROR,5723,".CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); 18/05/01 14:30:35 WARN ShutdownHookManager: ShutdownHook '$anon$2' timeout, java.util.concurrent.TimeoutException; java.util.concurrent.TimeoutException; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:67); 18/05/01 14:31:09 WARN ShutdownHookManager: ShutdownHook 'ClientFinalizer' timeout, java.util.concurrent.TimeoutException; java.util.concurrent.TimeoutException; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:67); 18/05/01 14:31:10 INFO ShutdownHookManager: Shutdown hook called; 18/05/01 14:31:15 INFO ShutdownHookManager: Deleting directory /tmp/abd30/spark-3f28d2e3-59d7-40f9-bba3-42d61eff6c6a; 18/05/01 14:31:20 ERROR ShutdownHookManager: ShutdownHookManger shutdown forcefully.; Using GATK jar /gpfs/fs0/home/abd30/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gpfs/fs0/home/abd30/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar PathSeqPipelineSpark --input /data/shenlab/abd/TCGA_microbiome/tmp_WXS_colorectal_all/TCGA-AH-6643-11A-01D-1826-10_hg19_Illumina_gdc_realn.bam --kmer-file /data/shenlab/abd/TCGA_microbiome/pathseq_bundle/host_ref/pathseq_host.bfi --filter-bwa-image /data/shenlab/abd/TCGA_microbiome/pathseq_bundle/host_ref/pathseq_host.fa.img --microbe-bwa-image /data/shenlab/abd/TCGA_microbiome/pathseq_bundle/pathogen_ref/pathseq_microbe.fa.img --microbe-fasta /data/shenlab/abd/TCGA_microbiome/pathseq_bundle/pathogen_ref/pathseq_microbe.fa --taxonom",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4725
https://github.com/broadinstitute/gatk/issues/4725:7103,Availability,error,error,7103,"org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:67); 18/05/01 14:31:09 WARN ShutdownHookManager: ShutdownHook 'ClientFinalizer' timeout, java.util.concurrent.TimeoutException; java.util.concurrent.TimeoutException; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:67); 18/05/01 14:31:10 INFO ShutdownHookManager: Shutdown hook called; 18/05/01 14:31:15 INFO ShutdownHookManager: Deleting directory /tmp/abd30/spark-3f28d2e3-59d7-40f9-bba3-42d61eff6c6a; 18/05/01 14:31:20 ERROR ShutdownHookManager: ShutdownHookManger shutdown forcefully.; Using GATK jar /gpfs/fs0/home/abd30/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gpfs/fs0/home/abd30/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar PathSeqPipelineSpark --input /data/shenlab/abd/TCGA_microbiome/tmp_WXS_colorectal_all/TCGA-AH-6643-11A-01D-1826-10_hg19_Illumina_gdc_realn.bam --kmer-file /data/shenlab/abd/TCGA_microbiome/pathseq_bundle/host_ref/pathseq_host.bfi --filter-bwa-image /data/shenlab/abd/TCGA_microbiome/pathseq_bundle/host_ref/pathseq_host.fa.img --microbe-bwa-image /data/shenlab/abd/TCGA_microbiome/pathseq_bundle/pathogen_ref/pathseq_microbe.fa.img --microbe-fasta /data/shenlab/abd/TCGA_microbiome/pathseq_bundle/pathogen_ref/pathseq_microbe.fa --taxonomy-file /data/shenlab/abd/TCGA_microbiome/pathseq_bundle/pathogen_ref/pathseq_taxonomy.db --min-clipped-read-length 50 --scores-output /data/shenlab/abd/TCGA_microbiome/WXS_colorectal_all/out/TCGA-AH-6643-11A-01D-1826-10_hg19_Illumina_gdc_realn.50.scores.txt --output /data/shenlab/abd/TCGA_microbiome/WXS_colorectal_all/out/TCGA-AH-6643-11A-01D-1826-10_hg19_Illumina_gdc_realn.50.pathseq.bam; /var/spool/slurmd/job1619084/slurm_script: line 126: syntax error: unexpected end of file; `",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4725
https://github.com/broadinstitute/gatk/issues/4725:622,Energy Efficiency,schedul,scheduler,622,"PathSeq is failing on the input files I'm using and its difficult for me to interpret the error message. What I'm seeing is,. 1. ERROR LiveListenerBus: SparkListenerBus has already stopped!; 2. Job aborted due to stage failure: Task 20 in stage 1.0 failed 1 times; 3. WARN ShutdownHookManager: ShutdownHook '$anon$2' timeout; 4. WARN ShutdownHookManager: ShutdownHook 'ClientFinalizer' timeout; 5. ERROR ShutdownHookManager: ShutdownHookManger shutdown forcefully.; 6. /var/spool/slurmd/job1619084/slurm_script: line 126: syntax error: unexpected end of file. In that order. I'm running this script in parallel on a SLURM scheduler (four cpus with 8Gb mem/cpu). Here is a sample of the last few lines of STDERR, but I'm also attaching the full error output.; [pathseq_TCGA.slurm.1619078_1.err.txt](https://github.com/broadinstitute/gatk/files/1965063/pathseq_TCGA.slurm.1619078_1.err.txt). Thanks so much for any help you can provide!. `; 18/05/01 14:20:59 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(driver, 10.12.137.46, 39719, None),broadcast_1_piece0,StorageLevel(memory, 1 replicas),127561,0)); 18/05/01 14:21:18 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 18/05/01 14:23:29 INFO MemoryStore: MemoryStore cleared; 18/05/01 14:23:29 INFO BlockManager: BlockManager stopped; 18/05/01 14:23:29 INFO BlockManagerMaster: BlockManagerMaster stopped; 18/05/01 14:24:38 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/05/01 14:25:36 INFO SparkContext: Successfully stopped SparkContext; 14:25:37.027 INFO PathSeqPipelineSpark - Shutting down engine; [May 1, 2018 2:25:37 PM EDT] org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark done. Elapsed time: 37.98 minutes.; Runtime.totalMemory()=23999283200; org.apache.spark.SparkException: Job aborted due to stage failure: Task 20 in stage 1.0 failed 1 times, most",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4725
https://github.com/broadinstitute/gatk/issues/4725:2260,Energy Efficiency,schedul,scheduler,2260,"erEndpoint stopped!; 18/05/01 14:23:29 INFO MemoryStore: MemoryStore cleared; 18/05/01 14:23:29 INFO BlockManager: BlockManager stopped; 18/05/01 14:23:29 INFO BlockManagerMaster: BlockManagerMaster stopped; 18/05/01 14:24:38 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/05/01 14:25:36 INFO SparkContext: Successfully stopped SparkContext; 14:25:37.027 INFO PathSeqPipelineSpark - Shutting down engine; [May 1, 2018 2:25:37 PM EDT] org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark done. Elapsed time: 37.98 minutes.; Runtime.totalMemory()=23999283200; org.apache.spark.SparkException: Job aborted due to stage failure: Task 20 in stage 1.0 failed 1 times, most recent failure: Lost task 20.0 in stage 1.0 (TID 891, localhost, executor driver): ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 131031 ms; Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4725
https://github.com/broadinstitute/gatk/issues/4725:2300,Energy Efficiency,schedul,scheduler,2300,"moryStore cleared; 18/05/01 14:23:29 INFO BlockManager: BlockManager stopped; 18/05/01 14:23:29 INFO BlockManagerMaster: BlockManagerMaster stopped; 18/05/01 14:24:38 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/05/01 14:25:36 INFO SparkContext: Successfully stopped SparkContext; 14:25:37.027 INFO PathSeqPipelineSpark - Shutting down engine; [May 1, 2018 2:25:37 PM EDT] org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark done. Elapsed time: 37.98 minutes.; Runtime.totalMemory()=23999283200; org.apache.spark.SparkException: Job aborted due to stage failure: Task 20 in stage 1.0 failed 1 times, most recent failure: Lost task 20.0 in stage 1.0 (TID 891, localhost, executor driver): ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 131031 ms; Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4725
https://github.com/broadinstitute/gatk/issues/4725:2399,Energy Efficiency,schedul,scheduler,2399,"/05/01 14:23:29 INFO BlockManagerMaster: BlockManagerMaster stopped; 18/05/01 14:24:38 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/05/01 14:25:36 INFO SparkContext: Successfully stopped SparkContext; 14:25:37.027 INFO PathSeqPipelineSpark - Shutting down engine; [May 1, 2018 2:25:37 PM EDT] org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark done. Elapsed time: 37.98 minutes.; Runtime.totalMemory()=23999283200; org.apache.spark.SparkException: Job aborted due to stage failure: Task 20 in stage 1.0 failed 1 times, most recent failure: Lost task 20.0 in stage 1.0 (TID 891, localhost, executor driver): ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 131031 ms; Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProces",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4725
https://github.com/broadinstitute/gatk/issues/4725:2497,Energy Efficiency,schedul,scheduler,2497,"CommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/05/01 14:25:36 INFO SparkContext: Successfully stopped SparkContext; 14:25:37.027 INFO PathSeqPipelineSpark - Shutting down engine; [May 1, 2018 2:25:37 PM EDT] org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark done. Elapsed time: 37.98 minutes.; Runtime.totalMemory()=23999283200; org.apache.spark.SparkException: Job aborted due to stage failure: Task 20 in stage 1.0 failed 1 times, most recent failure: Lost task 20.0 in stage 1.0 (TID 891, localhost, executor driver): ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 131031 ms; Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoo",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4725
https://github.com/broadinstitute/gatk/issues/4725:2751,Energy Efficiency,schedul,scheduler,2751,"roadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark done. Elapsed time: 37.98 minutes.; Runtime.totalMemory()=23999283200; org.apache.spark.SparkException: Job aborted due to stage failure: Task 20 in stage 1.0 failed 1 times, most recent failure: Lost task 20.0 in stage 1.0 (TID 891, localhost, executor driver): ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 131031 ms; Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); 	at org.apache.spark.SparkConte",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4725
https://github.com/broadinstitute/gatk/issues/4725:2832,Energy Efficiency,schedul,scheduler,2832,"ime: 37.98 minutes.; Runtime.totalMemory()=23999283200; org.apache.spark.SparkException: Job aborted due to stage failure: Task 20 in stage 1.0 failed 1 times, most recent failure: Lost task 20.0 in stage 1.0 (TID 891, localhost, executor driver): ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 131031 ms; Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); 	at org.apache.spark.SparkContext.runJob(Spar",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4725
https://github.com/broadinstitute/gatk/issues/4725:2938,Energy Efficiency,schedul,scheduler,2938,"o stage failure: Task 20 in stage 1.0 failed 1 times, most recent failure: Lost task 20.0 in stage 1.0 (TID 891, localhost, executor driver): ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 131031 ms; Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2087); 	at org.apache.spark.rdd.RDD.count(RDD.scala:1158); 	at org.apache.spark.api.java.Ja",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4725
https://github.com/broadinstitute/gatk/issues/4725:3088,Energy Efficiency,schedul,scheduler,3088,LostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 131031 ms; Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2087); 	at org.apache.spark.rdd.RDD.count(RDD.scala:1158); 	at org.apache.spark.api.java.JavaRDDLike$class.count(JavaRDDLike.scala:455); 	at org.apache.spark.api.java.AbstractJavaRDDLike.count(JavaRDDLike.scala:45); 	at org.broadinstitute.he,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4725
https://github.com/broadinstitute/gatk/issues/4725:3177,Energy Efficiency,schedul,scheduler,3177,heartbeat timed out after 131031 ms; Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2087); 	at org.apache.spark.rdd.RDD.count(RDD.scala:1158); 	at org.apache.spark.api.java.JavaRDDLike$class.count(JavaRDDLike.scala:455); 	at org.apache.spark.api.java.AbstractJavaRDDLike.count(JavaRDDLike.scala:45); 	at org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark.runTool(PathSeqPipelineSpark.java:245);,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4725
https://github.com/broadinstitute/gatk/issues/4725:3275,Energy Efficiency,schedul,scheduler,3275,er.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2087); 	at org.apache.spark.rdd.RDD.count(RDD.scala:1158); 	at org.apache.spark.api.java.JavaRDDLike$class.count(JavaRDDLike.scala:455); 	at org.apache.spark.api.java.AbstractJavaRDDLike.count(JavaRDDLike.scala:45); 	at org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark.runTool(PathSeqPipelineSpark.java:245); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:387);,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4725
https://github.com/broadinstitute/gatk/issues/4725:3371,Energy Efficiency,schedul,scheduler,3371,; 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2087); 	at org.apache.spark.rdd.RDD.count(RDD.scala:1158); 	at org.apache.spark.api.java.JavaRDDLike$class.count(JavaRDDLike.scala:455); 	at org.apache.spark.api.java.AbstractJavaRDDLike.count(JavaRDDLike.scala:45); 	at org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark.runTool(PathSeqPipelineSpark.java:245); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:387); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineP,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4725
https://github.com/broadinstitute/gatk/issues/4725:3536,Energy Efficiency,schedul,scheduler,3536,.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2087); 	at org.apache.spark.rdd.RDD.count(RDD.scala:1158); 	at org.apache.spark.api.java.JavaRDDLike$class.count(JavaRDDLike.scala:455); 	at org.apache.spark.api.java.AbstractJavaRDDLike.count(JavaRDDLike.scala:45); 	at org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark.runTool(PathSeqPipelineSpark.java:245); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:387); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); 	at org.broadinstitute.hellbender.cmdline.Command,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4725
https://github.com/broadinstitute/gatk/issues/4725:96,Integrability,message,message,96,"PathSeq is failing on the input files I'm using and its difficult for me to interpret the error message. What I'm seeing is,. 1. ERROR LiveListenerBus: SparkListenerBus has already stopped!; 2. Job aborted due to stage failure: Task 20 in stage 1.0 failed 1 times; 3. WARN ShutdownHookManager: ShutdownHook '$anon$2' timeout; 4. WARN ShutdownHookManager: ShutdownHook 'ClientFinalizer' timeout; 5. ERROR ShutdownHookManager: ShutdownHookManger shutdown forcefully.; 6. /var/spool/slurmd/job1619084/slurm_script: line 126: syntax error: unexpected end of file. In that order. I'm running this script in parallel on a SLURM scheduler (four cpus with 8Gb mem/cpu). Here is a sample of the last few lines of STDERR, but I'm also attaching the full error output.; [pathseq_TCGA.slurm.1619078_1.err.txt](https://github.com/broadinstitute/gatk/files/1965063/pathseq_TCGA.slurm.1619078_1.err.txt). Thanks so much for any help you can provide!. `; 18/05/01 14:20:59 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(driver, 10.12.137.46, 39719, None),broadcast_1_piece0,StorageLevel(memory, 1 replicas),127561,0)); 18/05/01 14:21:18 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 18/05/01 14:23:29 INFO MemoryStore: MemoryStore cleared; 18/05/01 14:23:29 INFO BlockManager: BlockManager stopped; 18/05/01 14:23:29 INFO BlockManagerMaster: BlockManagerMaster stopped; 18/05/01 14:24:38 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/05/01 14:25:36 INFO SparkContext: Successfully stopped SparkContext; 14:25:37.027 INFO PathSeqPipelineSpark - Shutting down engine; [May 1, 2018 2:25:37 PM EDT] org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark done. Elapsed time: 37.98 minutes.; Runtime.totalMemory()=23999283200; org.apache.spark.SparkException: Job aborted due to stage failure: Task 20 in stage 1.0 failed 1 times, most",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4725
https://github.com/broadinstitute/gatk/issues/4725:5001,Performance,concurren,concurrent,5001,"ark.api.java.AbstractJavaRDDLike.count(JavaRDDLike.scala:45); 	at org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark.runTool(PathSeqPipelineSpark.java:245); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:387); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); 18/05/01 14:30:35 WARN ShutdownHookManager: ShutdownHook '$anon$2' timeout, java.util.concurrent.TimeoutException; java.util.concurrent.TimeoutException; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:67); 18/05/01 14:31:09 WARN ShutdownHookManager: ShutdownHook 'ClientFinalizer' timeout, java.util.concurrent.TimeoutException; java.util.concurrent.TimeoutException; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:67); 18/05/01 14:31:10 INFO ShutdownHookManager: Shutdown hook called; 18/05/01 14:31:15 INFO ShutdownHookManager: Deleting directory /tmp/abd30/spark-3f28d2e3-59d7-40f9-bba3-42d61eff6c6a; 18/05/01 14:31:20 ERROR ShutdownHookManager: ShutdownHookManger shutdown forcefully.; Using GATK jar /gpfs/fs0/home/abd30/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4725
https://github.com/broadinstitute/gatk/issues/4725:5040,Performance,concurren,concurrent,5040,"JavaRDDLike.scala:45); 	at org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark.runTool(PathSeqPipelineSpark.java:245); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:387); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); 18/05/01 14:30:35 WARN ShutdownHookManager: ShutdownHook '$anon$2' timeout, java.util.concurrent.TimeoutException; java.util.concurrent.TimeoutException; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:67); 18/05/01 14:31:09 WARN ShutdownHookManager: ShutdownHook 'ClientFinalizer' timeout, java.util.concurrent.TimeoutException; java.util.concurrent.TimeoutException; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:67); 18/05/01 14:31:10 INFO ShutdownHookManager: Shutdown hook called; 18/05/01 14:31:15 INFO ShutdownHookManager: Deleting directory /tmp/abd30/spark-3f28d2e3-59d7-40f9-bba3-42d61eff6c6a; 18/05/01 14:31:20 ERROR ShutdownHookManager: ShutdownHookManger shutdown forcefully.; Using GATK jar /gpfs/fs0/home/abd30/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4725
https://github.com/broadinstitute/gatk/issues/4725:5083,Performance,concurren,concurrent,5083,"te.hellbender.tools.spark.pathseq.PathSeqPipelineSpark.runTool(PathSeqPipelineSpark.java:245); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:387); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); 18/05/01 14:30:35 WARN ShutdownHookManager: ShutdownHook '$anon$2' timeout, java.util.concurrent.TimeoutException; java.util.concurrent.TimeoutException; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:67); 18/05/01 14:31:09 WARN ShutdownHookManager: ShutdownHook 'ClientFinalizer' timeout, java.util.concurrent.TimeoutException; java.util.concurrent.TimeoutException; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:67); 18/05/01 14:31:10 INFO ShutdownHookManager: Shutdown hook called; 18/05/01 14:31:15 INFO ShutdownHookManager: Deleting directory /tmp/abd30/spark-3f28d2e3-59d7-40f9-bba3-42d61eff6c6a; 18/05/01 14:31:20 ERROR ShutdownHookManager: ShutdownHookManger shutdown forcefully.; Using GATK jar /gpfs/fs0/home/abd30/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gpfs/fs0/home/abd30/gatk-4.0.3.0/gatk-pac",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4725
https://github.com/broadinstitute/gatk/issues/4725:5308,Performance,concurren,concurrent,5308,"r.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); 18/05/01 14:30:35 WARN ShutdownHookManager: ShutdownHook '$anon$2' timeout, java.util.concurrent.TimeoutException; java.util.concurrent.TimeoutException; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:67); 18/05/01 14:31:09 WARN ShutdownHookManager: ShutdownHook 'ClientFinalizer' timeout, java.util.concurrent.TimeoutException; java.util.concurrent.TimeoutException; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:67); 18/05/01 14:31:10 INFO ShutdownHookManager: Shutdown hook called; 18/05/01 14:31:15 INFO ShutdownHookManager: Deleting directory /tmp/abd30/spark-3f28d2e3-59d7-40f9-bba3-42d61eff6c6a; 18/05/01 14:31:20 ERROR ShutdownHookManager: ShutdownHookManger shutdown forcefully.; Using GATK jar /gpfs/fs0/home/abd30/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gpfs/fs0/home/abd30/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar PathSeqPipelineSpark --input /data/shenlab/abd/TCGA_microbiome/tmp_WXS_colorectal_all/TCGA-AH-6643-11A-01D-1826-10_hg19_Illumina_gdc_realn.bam --kmer-file /data/shenlab/abd/TCGA_microbiome/pathseq_bundl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4725
https://github.com/broadinstitute/gatk/issues/4725:5347,Performance,concurren,concurrent,5347,"doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); 18/05/01 14:30:35 WARN ShutdownHookManager: ShutdownHook '$anon$2' timeout, java.util.concurrent.TimeoutException; java.util.concurrent.TimeoutException; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:67); 18/05/01 14:31:09 WARN ShutdownHookManager: ShutdownHook 'ClientFinalizer' timeout, java.util.concurrent.TimeoutException; java.util.concurrent.TimeoutException; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:67); 18/05/01 14:31:10 INFO ShutdownHookManager: Shutdown hook called; 18/05/01 14:31:15 INFO ShutdownHookManager: Deleting directory /tmp/abd30/spark-3f28d2e3-59d7-40f9-bba3-42d61eff6c6a; 18/05/01 14:31:20 ERROR ShutdownHookManager: ShutdownHookManger shutdown forcefully.; Using GATK jar /gpfs/fs0/home/abd30/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gpfs/fs0/home/abd30/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar PathSeqPipelineSpark --input /data/shenlab/abd/TCGA_microbiome/tmp_WXS_colorectal_all/TCGA-AH-6643-11A-01D-1826-10_hg19_Illumina_gdc_realn.bam --kmer-file /data/shenlab/abd/TCGA_microbiome/pathseq_bundle/host_ref/pathseq_host.bfi --filter-bw",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4725
https://github.com/broadinstitute/gatk/issues/4725:5390,Performance,concurren,concurrent,5390,"t org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); 18/05/01 14:30:35 WARN ShutdownHookManager: ShutdownHook '$anon$2' timeout, java.util.concurrent.TimeoutException; java.util.concurrent.TimeoutException; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:67); 18/05/01 14:31:09 WARN ShutdownHookManager: ShutdownHook 'ClientFinalizer' timeout, java.util.concurrent.TimeoutException; java.util.concurrent.TimeoutException; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:67); 18/05/01 14:31:10 INFO ShutdownHookManager: Shutdown hook called; 18/05/01 14:31:15 INFO ShutdownHookManager: Deleting directory /tmp/abd30/spark-3f28d2e3-59d7-40f9-bba3-42d61eff6c6a; 18/05/01 14:31:20 ERROR ShutdownHookManager: ShutdownHookManger shutdown forcefully.; Using GATK jar /gpfs/fs0/home/abd30/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gpfs/fs0/home/abd30/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar PathSeqPipelineSpark --input /data/shenlab/abd/TCGA_microbiome/tmp_WXS_colorectal_all/TCGA-AH-6643-11A-01D-1826-10_hg19_Illumina_gdc_realn.bam --kmer-file /data/shenlab/abd/TCGA_microbiome/pathseq_bundle/host_ref/pathseq_host.bfi --filter-bwa-image /data/shenlab/abd/TCGA_microbiome/p",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4725
https://github.com/broadinstitute/gatk/issues/4725:198,Safety,abort,aborted,198,"PathSeq is failing on the input files I'm using and its difficult for me to interpret the error message. What I'm seeing is,. 1. ERROR LiveListenerBus: SparkListenerBus has already stopped!; 2. Job aborted due to stage failure: Task 20 in stage 1.0 failed 1 times; 3. WARN ShutdownHookManager: ShutdownHook '$anon$2' timeout; 4. WARN ShutdownHookManager: ShutdownHook 'ClientFinalizer' timeout; 5. ERROR ShutdownHookManager: ShutdownHookManger shutdown forcefully.; 6. /var/spool/slurmd/job1619084/slurm_script: line 126: syntax error: unexpected end of file. In that order. I'm running this script in parallel on a SLURM scheduler (four cpus with 8Gb mem/cpu). Here is a sample of the last few lines of STDERR, but I'm also attaching the full error output.; [pathseq_TCGA.slurm.1619078_1.err.txt](https://github.com/broadinstitute/gatk/files/1965063/pathseq_TCGA.slurm.1619078_1.err.txt). Thanks so much for any help you can provide!. `; 18/05/01 14:20:59 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(driver, 10.12.137.46, 39719, None),broadcast_1_piece0,StorageLevel(memory, 1 replicas),127561,0)); 18/05/01 14:21:18 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 18/05/01 14:23:29 INFO MemoryStore: MemoryStore cleared; 18/05/01 14:23:29 INFO BlockManager: BlockManager stopped; 18/05/01 14:23:29 INFO BlockManagerMaster: BlockManagerMaster stopped; 18/05/01 14:24:38 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/05/01 14:25:36 INFO SparkContext: Successfully stopped SparkContext; 14:25:37.027 INFO PathSeqPipelineSpark - Shutting down engine; [May 1, 2018 2:25:37 PM EDT] org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark done. Elapsed time: 37.98 minutes.; Runtime.totalMemory()=23999283200; org.apache.spark.SparkException: Job aborted due to stage failure: Task 20 in stage 1.0 failed 1 times, most",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4725
https://github.com/broadinstitute/gatk/issues/4725:317,Safety,timeout,timeout,317,"PathSeq is failing on the input files I'm using and its difficult for me to interpret the error message. What I'm seeing is,. 1. ERROR LiveListenerBus: SparkListenerBus has already stopped!; 2. Job aborted due to stage failure: Task 20 in stage 1.0 failed 1 times; 3. WARN ShutdownHookManager: ShutdownHook '$anon$2' timeout; 4. WARN ShutdownHookManager: ShutdownHook 'ClientFinalizer' timeout; 5. ERROR ShutdownHookManager: ShutdownHookManger shutdown forcefully.; 6. /var/spool/slurmd/job1619084/slurm_script: line 126: syntax error: unexpected end of file. In that order. I'm running this script in parallel on a SLURM scheduler (four cpus with 8Gb mem/cpu). Here is a sample of the last few lines of STDERR, but I'm also attaching the full error output.; [pathseq_TCGA.slurm.1619078_1.err.txt](https://github.com/broadinstitute/gatk/files/1965063/pathseq_TCGA.slurm.1619078_1.err.txt). Thanks so much for any help you can provide!. `; 18/05/01 14:20:59 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(driver, 10.12.137.46, 39719, None),broadcast_1_piece0,StorageLevel(memory, 1 replicas),127561,0)); 18/05/01 14:21:18 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 18/05/01 14:23:29 INFO MemoryStore: MemoryStore cleared; 18/05/01 14:23:29 INFO BlockManager: BlockManager stopped; 18/05/01 14:23:29 INFO BlockManagerMaster: BlockManagerMaster stopped; 18/05/01 14:24:38 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/05/01 14:25:36 INFO SparkContext: Successfully stopped SparkContext; 14:25:37.027 INFO PathSeqPipelineSpark - Shutting down engine; [May 1, 2018 2:25:37 PM EDT] org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark done. Elapsed time: 37.98 minutes.; Runtime.totalMemory()=23999283200; org.apache.spark.SparkException: Job aborted due to stage failure: Task 20 in stage 1.0 failed 1 times, most",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4725
https://github.com/broadinstitute/gatk/issues/4725:386,Safety,timeout,timeout,386,"PathSeq is failing on the input files I'm using and its difficult for me to interpret the error message. What I'm seeing is,. 1. ERROR LiveListenerBus: SparkListenerBus has already stopped!; 2. Job aborted due to stage failure: Task 20 in stage 1.0 failed 1 times; 3. WARN ShutdownHookManager: ShutdownHook '$anon$2' timeout; 4. WARN ShutdownHookManager: ShutdownHook 'ClientFinalizer' timeout; 5. ERROR ShutdownHookManager: ShutdownHookManger shutdown forcefully.; 6. /var/spool/slurmd/job1619084/slurm_script: line 126: syntax error: unexpected end of file. In that order. I'm running this script in parallel on a SLURM scheduler (four cpus with 8Gb mem/cpu). Here is a sample of the last few lines of STDERR, but I'm also attaching the full error output.; [pathseq_TCGA.slurm.1619078_1.err.txt](https://github.com/broadinstitute/gatk/files/1965063/pathseq_TCGA.slurm.1619078_1.err.txt). Thanks so much for any help you can provide!. `; 18/05/01 14:20:59 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(driver, 10.12.137.46, 39719, None),broadcast_1_piece0,StorageLevel(memory, 1 replicas),127561,0)); 18/05/01 14:21:18 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 18/05/01 14:23:29 INFO MemoryStore: MemoryStore cleared; 18/05/01 14:23:29 INFO BlockManager: BlockManager stopped; 18/05/01 14:23:29 INFO BlockManagerMaster: BlockManagerMaster stopped; 18/05/01 14:24:38 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/05/01 14:25:36 INFO SparkContext: Successfully stopped SparkContext; 14:25:37.027 INFO PathSeqPipelineSpark - Shutting down engine; [May 1, 2018 2:25:37 PM EDT] org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark done. Elapsed time: 37.98 minutes.; Runtime.totalMemory()=23999283200; org.apache.spark.SparkException: Job aborted due to stage failure: Task 20 in stage 1.0 failed 1 times, most",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4725
https://github.com/broadinstitute/gatk/issues/4725:1930,Safety,abort,aborted,1930,"/01 14:20:59 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(driver, 10.12.137.46, 39719, None),broadcast_1_piece0,StorageLevel(memory, 1 replicas),127561,0)); 18/05/01 14:21:18 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 18/05/01 14:23:29 INFO MemoryStore: MemoryStore cleared; 18/05/01 14:23:29 INFO BlockManager: BlockManager stopped; 18/05/01 14:23:29 INFO BlockManagerMaster: BlockManagerMaster stopped; 18/05/01 14:24:38 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/05/01 14:25:36 INFO SparkContext: Successfully stopped SparkContext; 14:25:37.027 INFO PathSeqPipelineSpark - Shutting down engine; [May 1, 2018 2:25:37 PM EDT] org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark done. Elapsed time: 37.98 minutes.; Runtime.totalMemory()=23999283200; org.apache.spark.SparkException: Job aborted due to stage failure: Task 20 in stage 1.0 failed 1 times, most recent failure: Lost task 20.0 in stage 1.0 (TID 891, localhost, executor driver): ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 131031 ms; Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.schedul",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4725
https://github.com/broadinstitute/gatk/issues/4725:2431,Safety,abort,abortStage,2431,"lockManagerMaster: BlockManagerMaster stopped; 18/05/01 14:24:38 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/05/01 14:25:36 INFO SparkContext: Successfully stopped SparkContext; 14:25:37.027 INFO PathSeqPipelineSpark - Shutting down engine; [May 1, 2018 2:25:37 PM EDT] org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark done. Elapsed time: 37.98 minutes.; Runtime.totalMemory()=23999283200; org.apache.spark.SparkException: Job aborted due to stage failure: Task 20 in stage 1.0 failed 1 times, most recent failure: Lost task 20.0 in stage 1.0 (TID 891, localhost, executor driver): ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 131031 ms; Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSche",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4725
https://github.com/broadinstitute/gatk/issues/4725:2529,Safety,abort,abortStage,2529,"utCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/05/01 14:25:36 INFO SparkContext: Successfully stopped SparkContext; 14:25:37.027 INFO PathSeqPipelineSpark - Shutting down engine; [May 1, 2018 2:25:37 PM EDT] org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark done. Elapsed time: 37.98 minutes.; Runtime.totalMemory()=23999283200; org.apache.spark.SparkException: Job aborted due to stage failure: Task 20 in stage 1.0 failed 1 times, most recent failure: Lost task 20.0 in stage 1.0 (TID 891, localhost, executor driver): ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 131031 ms; Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.ap",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4725
https://github.com/broadinstitute/gatk/issues/4725:2774,Safety,abort,abortStage,2774,".spark.pathseq.PathSeqPipelineSpark done. Elapsed time: 37.98 minutes.; Runtime.totalMemory()=23999283200; org.apache.spark.SparkException: Job aborted due to stage failure: Task 20 in stage 1.0 failed 1 times, most recent failure: Lost task 20.0 in stage 1.0 (TID 891, localhost, executor driver): ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 131031 ms; Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4725
https://github.com/broadinstitute/gatk/issues/4725:4982,Safety,timeout,timeout,4982,"ke$class.count(JavaRDDLike.scala:455); 	at org.apache.spark.api.java.AbstractJavaRDDLike.count(JavaRDDLike.scala:45); 	at org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark.runTool(PathSeqPipelineSpark.java:245); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:387); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); 18/05/01 14:30:35 WARN ShutdownHookManager: ShutdownHook '$anon$2' timeout, java.util.concurrent.TimeoutException; java.util.concurrent.TimeoutException; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:67); 18/05/01 14:31:09 WARN ShutdownHookManager: ShutdownHook 'ClientFinalizer' timeout, java.util.concurrent.TimeoutException; java.util.concurrent.TimeoutException; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:67); 18/05/01 14:31:10 INFO ShutdownHookManager: Shutdown hook called; 18/05/01 14:31:15 INFO ShutdownHookManager: Deleting directory /tmp/abd30/spark-3f28d2e3-59d7-40f9-bba3-42d61eff6c6a; 18/05/01 14:31:20 ERROR ShutdownHookManager: ShutdownHookManger shutdown forcefully.; Using GATK jar /gpfs/fs0/home/abd30/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4725
https://github.com/broadinstitute/gatk/issues/4725:5012,Safety,Timeout,TimeoutException,5012,"ractJavaRDDLike.count(JavaRDDLike.scala:45); 	at org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark.runTool(PathSeqPipelineSpark.java:245); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:387); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); 18/05/01 14:30:35 WARN ShutdownHookManager: ShutdownHook '$anon$2' timeout, java.util.concurrent.TimeoutException; java.util.concurrent.TimeoutException; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:67); 18/05/01 14:31:09 WARN ShutdownHookManager: ShutdownHook 'ClientFinalizer' timeout, java.util.concurrent.TimeoutException; java.util.concurrent.TimeoutException; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:67); 18/05/01 14:31:10 INFO ShutdownHookManager: Shutdown hook called; 18/05/01 14:31:15 INFO ShutdownHookManager: Deleting directory /tmp/abd30/spark-3f28d2e3-59d7-40f9-bba3-42d61eff6c6a; 18/05/01 14:31:20 ERROR ShutdownHookManager: ShutdownHookManger shutdown forcefully.; Using GATK jar /gpfs/fs0/home/abd30/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.co",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4725
https://github.com/broadinstitute/gatk/issues/4725:5051,Safety,Timeout,TimeoutException,5051,"5); 	at org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark.runTool(PathSeqPipelineSpark.java:245); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:387); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); 18/05/01 14:30:35 WARN ShutdownHookManager: ShutdownHook '$anon$2' timeout, java.util.concurrent.TimeoutException; java.util.concurrent.TimeoutException; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:67); 18/05/01 14:31:09 WARN ShutdownHookManager: ShutdownHook 'ClientFinalizer' timeout, java.util.concurrent.TimeoutException; java.util.concurrent.TimeoutException; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:67); 18/05/01 14:31:10 INFO ShutdownHookManager: Shutdown hook called; 18/05/01 14:31:15 INFO ShutdownHookManager: Deleting directory /tmp/abd30/spark-3f28d2e3-59d7-40f9-bba3-42d61eff6c6a; 18/05/01 14:31:20 ERROR ShutdownHookManager: ShutdownHookManger shutdown forcefully.; Using GATK jar /gpfs/fs0/home/abd30/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gpfs/fs0/home/abd",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4725
https://github.com/broadinstitute/gatk/issues/4725:5289,Safety,timeout,timeout,5289,"ne(GATKSparkTool.java:387); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); 18/05/01 14:30:35 WARN ShutdownHookManager: ShutdownHook '$anon$2' timeout, java.util.concurrent.TimeoutException; java.util.concurrent.TimeoutException; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:67); 18/05/01 14:31:09 WARN ShutdownHookManager: ShutdownHook 'ClientFinalizer' timeout, java.util.concurrent.TimeoutException; java.util.concurrent.TimeoutException; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:67); 18/05/01 14:31:10 INFO ShutdownHookManager: Shutdown hook called; 18/05/01 14:31:15 INFO ShutdownHookManager: Deleting directory /tmp/abd30/spark-3f28d2e3-59d7-40f9-bba3-42d61eff6c6a; 18/05/01 14:31:20 ERROR ShutdownHookManager: ShutdownHookManger shutdown forcefully.; Using GATK jar /gpfs/fs0/home/abd30/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gpfs/fs0/home/abd30/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar PathSeqPipelineSpark --input /data/shenlab/abd/TCGA_microbiome/tmp_WXS_colorectal_all/TCGA-AH-6643-11A-01D-1826-10_hg19_Illumina_gdc_realn.bam",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4725
https://github.com/broadinstitute/gatk/issues/4725:5319,Safety,Timeout,TimeoutException,5319,"arkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); 18/05/01 14:30:35 WARN ShutdownHookManager: ShutdownHook '$anon$2' timeout, java.util.concurrent.TimeoutException; java.util.concurrent.TimeoutException; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:67); 18/05/01 14:31:09 WARN ShutdownHookManager: ShutdownHook 'ClientFinalizer' timeout, java.util.concurrent.TimeoutException; java.util.concurrent.TimeoutException; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:67); 18/05/01 14:31:10 INFO ShutdownHookManager: Shutdown hook called; 18/05/01 14:31:15 INFO ShutdownHookManager: Deleting directory /tmp/abd30/spark-3f28d2e3-59d7-40f9-bba3-42d61eff6c6a; 18/05/01 14:31:20 ERROR ShutdownHookManager: ShutdownHookManger shutdown forcefully.; Using GATK jar /gpfs/fs0/home/abd30/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gpfs/fs0/home/abd30/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar PathSeqPipelineSpark --input /data/shenlab/abd/TCGA_microbiome/tmp_WXS_colorectal_all/TCGA-AH-6643-11A-01D-1826-10_hg19_Illumina_gdc_realn.bam --kmer-file /data/shenlab/abd/TCGA_microbiome/pathseq_bundle/host_ref/pathse",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4725
https://github.com/broadinstitute/gatk/issues/4725:5358,Safety,Timeout,TimeoutException,5358,"LineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); 18/05/01 14:30:35 WARN ShutdownHookManager: ShutdownHook '$anon$2' timeout, java.util.concurrent.TimeoutException; java.util.concurrent.TimeoutException; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:67); 18/05/01 14:31:09 WARN ShutdownHookManager: ShutdownHook 'ClientFinalizer' timeout, java.util.concurrent.TimeoutException; java.util.concurrent.TimeoutException; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:67); 18/05/01 14:31:10 INFO ShutdownHookManager: Shutdown hook called; 18/05/01 14:31:15 INFO ShutdownHookManager: Deleting directory /tmp/abd30/spark-3f28d2e3-59d7-40f9-bba3-42d61eff6c6a; 18/05/01 14:31:20 ERROR ShutdownHookManager: ShutdownHookManger shutdown forcefully.; Using GATK jar /gpfs/fs0/home/abd30/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gpfs/fs0/home/abd30/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar PathSeqPipelineSpark --input /data/shenlab/abd/TCGA_microbiome/tmp_WXS_colorectal_all/TCGA-AH-6643-11A-01D-1826-10_hg19_Illumina_gdc_realn.bam --kmer-file /data/shenlab/abd/TCGA_microbiome/pathseq_bundle/host_ref/pathseq_host.bfi --filter-bwa-image /data/shenl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4725
https://github.com/broadinstitute/gatk/issues/4725:1334,Usability,clear,cleared,1334,"meout; 5. ERROR ShutdownHookManager: ShutdownHookManger shutdown forcefully.; 6. /var/spool/slurmd/job1619084/slurm_script: line 126: syntax error: unexpected end of file. In that order. I'm running this script in parallel on a SLURM scheduler (four cpus with 8Gb mem/cpu). Here is a sample of the last few lines of STDERR, but I'm also attaching the full error output.; [pathseq_TCGA.slurm.1619078_1.err.txt](https://github.com/broadinstitute/gatk/files/1965063/pathseq_TCGA.slurm.1619078_1.err.txt). Thanks so much for any help you can provide!. `; 18/05/01 14:20:59 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(driver, 10.12.137.46, 39719, None),broadcast_1_piece0,StorageLevel(memory, 1 replicas),127561,0)); 18/05/01 14:21:18 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 18/05/01 14:23:29 INFO MemoryStore: MemoryStore cleared; 18/05/01 14:23:29 INFO BlockManager: BlockManager stopped; 18/05/01 14:23:29 INFO BlockManagerMaster: BlockManagerMaster stopped; 18/05/01 14:24:38 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/05/01 14:25:36 INFO SparkContext: Successfully stopped SparkContext; 14:25:37.027 INFO PathSeqPipelineSpark - Shutting down engine; [May 1, 2018 2:25:37 PM EDT] org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark done. Elapsed time: 37.98 minutes.; Runtime.totalMemory()=23999283200; org.apache.spark.SparkException: Job aborted due to stage failure: Task 20 in stage 1.0 failed 1 times, most recent failure: Lost task 20.0 in stage 1.0 (TID 891, localhost, executor driver): ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 131031 ms; Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4725
https://github.com/broadinstitute/gatk/issues/4726:269,Usability,simpl,simpler,269,"At some point in the next several quarters @davidbenjamin I'd recommend a collaboration between your team and the engine team on a `Mutect2Spark` tool. Most of the prerequisite infrastructure has already been implemented for `HaplotypeCallerSpark`, so it may be a much simpler task than you'd imagine!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4726
https://github.com/broadinstitute/gatk/issues/4727:198,Availability,error,error,198,"Environment:. * GATK 4.0.3.0; * Python 2.7.14; * Red Hat Enterprise Linux Server release 7.4 (Maipo). I'm using GATK CNNScoreVariants 1D model(CPU mode) for hundreds of VCF files, but the following error occurred in some of them. 00:13:26.470 INFO ProgressMeter - Traversal complete. Processed 4924627 total variants in 79.6 minutes.; 00:14:14.422 INFO CNNScoreVariants - Shutting down engine; [2018/05/02 13:10:44 JST] org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants done. Elapsed time: 79.75 minutes.; Runtime.totalMemory()=1126694912; org.broadinstitute.hellbender.utils.python.PythonScriptExecutorException: Traceback detected: Traceback (most recent call last):; File ""<stdin>"", line1, in <module>; File "".../lib/python2.7/site-packages/vqsr_cnn/inference.py"", line 51, in score_and_write_batch; reference_batch.append(reference_string_to_tensor(fifo_data[4])); File "".../lib/python2.7/site-packages/vqsr_cnn/inference.py"", line 107, in reference_string_to_tensor; raise ValueError('Error! Unknown code:', b); ValueError('Error! Unknown code:', '\x00'); >>>; at org.broadinstitute.hellbender.utils.python.StreamingPythonScriptExecutor.getAccumulatedOutput(StreamingPythonScriptExecutor.java:214); at org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants.onTraversalSuccess(CNNScoreVariants.java:385); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:894); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); ; I tried again, but same message are shown.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4727
https://github.com/broadinstitute/gatk/issues/4727:381,Availability,down,down,381,"Environment:. * GATK 4.0.3.0; * Python 2.7.14; * Red Hat Enterprise Linux Server release 7.4 (Maipo). I'm using GATK CNNScoreVariants 1D model(CPU mode) for hundreds of VCF files, but the following error occurred in some of them. 00:13:26.470 INFO ProgressMeter - Traversal complete. Processed 4924627 total variants in 79.6 minutes.; 00:14:14.422 INFO CNNScoreVariants - Shutting down engine; [2018/05/02 13:10:44 JST] org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants done. Elapsed time: 79.75 minutes.; Runtime.totalMemory()=1126694912; org.broadinstitute.hellbender.utils.python.PythonScriptExecutorException: Traceback detected: Traceback (most recent call last):; File ""<stdin>"", line1, in <module>; File "".../lib/python2.7/site-packages/vqsr_cnn/inference.py"", line 51, in score_and_write_batch; reference_batch.append(reference_string_to_tensor(fifo_data[4])); File "".../lib/python2.7/site-packages/vqsr_cnn/inference.py"", line 107, in reference_string_to_tensor; raise ValueError('Error! Unknown code:', b); ValueError('Error! Unknown code:', '\x00'); >>>; at org.broadinstitute.hellbender.utils.python.StreamingPythonScriptExecutor.getAccumulatedOutput(StreamingPythonScriptExecutor.java:214); at org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants.onTraversalSuccess(CNNScoreVariants.java:385); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:894); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); ; I tried again, but same message are shown.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4727
https://github.com/broadinstitute/gatk/issues/4727:1006,Availability,Error,Error,1006,"Environment:. * GATK 4.0.3.0; * Python 2.7.14; * Red Hat Enterprise Linux Server release 7.4 (Maipo). I'm using GATK CNNScoreVariants 1D model(CPU mode) for hundreds of VCF files, but the following error occurred in some of them. 00:13:26.470 INFO ProgressMeter - Traversal complete. Processed 4924627 total variants in 79.6 minutes.; 00:14:14.422 INFO CNNScoreVariants - Shutting down engine; [2018/05/02 13:10:44 JST] org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants done. Elapsed time: 79.75 minutes.; Runtime.totalMemory()=1126694912; org.broadinstitute.hellbender.utils.python.PythonScriptExecutorException: Traceback detected: Traceback (most recent call last):; File ""<stdin>"", line1, in <module>; File "".../lib/python2.7/site-packages/vqsr_cnn/inference.py"", line 51, in score_and_write_batch; reference_batch.append(reference_string_to_tensor(fifo_data[4])); File "".../lib/python2.7/site-packages/vqsr_cnn/inference.py"", line 107, in reference_string_to_tensor; raise ValueError('Error! Unknown code:', b); ValueError('Error! Unknown code:', '\x00'); >>>; at org.broadinstitute.hellbender.utils.python.StreamingPythonScriptExecutor.getAccumulatedOutput(StreamingPythonScriptExecutor.java:214); at org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants.onTraversalSuccess(CNNScoreVariants.java:385); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:894); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); ; I tried again, but same message are shown.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4727
https://github.com/broadinstitute/gatk/issues/4727:1045,Availability,Error,Error,1045,"Environment:. * GATK 4.0.3.0; * Python 2.7.14; * Red Hat Enterprise Linux Server release 7.4 (Maipo). I'm using GATK CNNScoreVariants 1D model(CPU mode) for hundreds of VCF files, but the following error occurred in some of them. 00:13:26.470 INFO ProgressMeter - Traversal complete. Processed 4924627 total variants in 79.6 minutes.; 00:14:14.422 INFO CNNScoreVariants - Shutting down engine; [2018/05/02 13:10:44 JST] org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants done. Elapsed time: 79.75 minutes.; Runtime.totalMemory()=1126694912; org.broadinstitute.hellbender.utils.python.PythonScriptExecutorException: Traceback detected: Traceback (most recent call last):; File ""<stdin>"", line1, in <module>; File "".../lib/python2.7/site-packages/vqsr_cnn/inference.py"", line 51, in score_and_write_batch; reference_batch.append(reference_string_to_tensor(fifo_data[4])); File "".../lib/python2.7/site-packages/vqsr_cnn/inference.py"", line 107, in reference_string_to_tensor; raise ValueError('Error! Unknown code:', b); ValueError('Error! Unknown code:', '\x00'); >>>; at org.broadinstitute.hellbender.utils.python.StreamingPythonScriptExecutor.getAccumulatedOutput(StreamingPythonScriptExecutor.java:214); at org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants.onTraversalSuccess(CNNScoreVariants.java:385); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:894); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); ; I tried again, but same message are shown.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4727
https://github.com/broadinstitute/gatk/issues/4727:81,Deployability,release,release,81,"Environment:. * GATK 4.0.3.0; * Python 2.7.14; * Red Hat Enterprise Linux Server release 7.4 (Maipo). I'm using GATK CNNScoreVariants 1D model(CPU mode) for hundreds of VCF files, but the following error occurred in some of them. 00:13:26.470 INFO ProgressMeter - Traversal complete. Processed 4924627 total variants in 79.6 minutes.; 00:14:14.422 INFO CNNScoreVariants - Shutting down engine; [2018/05/02 13:10:44 JST] org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants done. Elapsed time: 79.75 minutes.; Runtime.totalMemory()=1126694912; org.broadinstitute.hellbender.utils.python.PythonScriptExecutorException: Traceback detected: Traceback (most recent call last):; File ""<stdin>"", line1, in <module>; File "".../lib/python2.7/site-packages/vqsr_cnn/inference.py"", line 51, in score_and_write_batch; reference_batch.append(reference_string_to_tensor(fifo_data[4])); File "".../lib/python2.7/site-packages/vqsr_cnn/inference.py"", line 107, in reference_string_to_tensor; raise ValueError('Error! Unknown code:', b); ValueError('Error! Unknown code:', '\x00'); >>>; at org.broadinstitute.hellbender.utils.python.StreamingPythonScriptExecutor.getAccumulatedOutput(StreamingPythonScriptExecutor.java:214); at org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants.onTraversalSuccess(CNNScoreVariants.java:385); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:894); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); ; I tried again, but same message are shown.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4727
https://github.com/broadinstitute/gatk/issues/4727:1954,Integrability,message,message,1954,"Environment:. * GATK 4.0.3.0; * Python 2.7.14; * Red Hat Enterprise Linux Server release 7.4 (Maipo). I'm using GATK CNNScoreVariants 1D model(CPU mode) for hundreds of VCF files, but the following error occurred in some of them. 00:13:26.470 INFO ProgressMeter - Traversal complete. Processed 4924627 total variants in 79.6 minutes.; 00:14:14.422 INFO CNNScoreVariants - Shutting down engine; [2018/05/02 13:10:44 JST] org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants done. Elapsed time: 79.75 minutes.; Runtime.totalMemory()=1126694912; org.broadinstitute.hellbender.utils.python.PythonScriptExecutorException: Traceback detected: Traceback (most recent call last):; File ""<stdin>"", line1, in <module>; File "".../lib/python2.7/site-packages/vqsr_cnn/inference.py"", line 51, in score_and_write_batch; reference_batch.append(reference_string_to_tensor(fifo_data[4])); File "".../lib/python2.7/site-packages/vqsr_cnn/inference.py"", line 107, in reference_string_to_tensor; raise ValueError('Error! Unknown code:', b); ValueError('Error! Unknown code:', '\x00'); >>>; at org.broadinstitute.hellbender.utils.python.StreamingPythonScriptExecutor.getAccumulatedOutput(StreamingPythonScriptExecutor.java:214); at org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants.onTraversalSuccess(CNNScoreVariants.java:385); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:894); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); ; I tried again, but same message are shown.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4727
https://github.com/broadinstitute/gatk/issues/4727:640,Safety,detect,detected,640,"Environment:. * GATK 4.0.3.0; * Python 2.7.14; * Red Hat Enterprise Linux Server release 7.4 (Maipo). I'm using GATK CNNScoreVariants 1D model(CPU mode) for hundreds of VCF files, but the following error occurred in some of them. 00:13:26.470 INFO ProgressMeter - Traversal complete. Processed 4924627 total variants in 79.6 minutes.; 00:14:14.422 INFO CNNScoreVariants - Shutting down engine; [2018/05/02 13:10:44 JST] org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants done. Elapsed time: 79.75 minutes.; Runtime.totalMemory()=1126694912; org.broadinstitute.hellbender.utils.python.PythonScriptExecutorException: Traceback detected: Traceback (most recent call last):; File ""<stdin>"", line1, in <module>; File "".../lib/python2.7/site-packages/vqsr_cnn/inference.py"", line 51, in score_and_write_batch; reference_batch.append(reference_string_to_tensor(fifo_data[4])); File "".../lib/python2.7/site-packages/vqsr_cnn/inference.py"", line 107, in reference_string_to_tensor; raise ValueError('Error! Unknown code:', b); ValueError('Error! Unknown code:', '\x00'); >>>; at org.broadinstitute.hellbender.utils.python.StreamingPythonScriptExecutor.getAccumulatedOutput(StreamingPythonScriptExecutor.java:214); at org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants.onTraversalSuccess(CNNScoreVariants.java:385); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:894); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); ; I tried again, but same message are shown.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4727
https://github.com/broadinstitute/gatk/pull/4732:159,Safety,avoid,avoid,159,Added a global sort to the beginning of the tool to ensure we are always working with name grouped bams. In the future we should evaluate if alternatives that avoid sorting are necessary. . Fixes #4701,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4732
https://github.com/broadinstitute/gatk/issues/4733:86,Availability,avail,available,86,I was wondering what version of Picard is bundled with GATK. I assume it's the latest available (2.18.4 at this time) but apparently there's no way to tell:; `$ gatk MergeSamFiles -version` output `Version:4.0.4.0`,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4733
https://github.com/broadinstitute/gatk/issues/4734:517,Availability,robust,robust,517,"Ran into this when trying to create a PoN with 100 samples x 100 bp bins = 1.27 * max int elements. This currently causes issues when truncating outliers, at which point all elements are loaded into an array so that Percentiles can be naively computed, resulting in a `java.lang.NegativeArraySizeException`. Solutions include: 1) simply throwing a message and failing early if the counts matrix is too large (perhaps recommend scattering by contig, see #4728), 2) changing the outlier truncation procedure to be more robust. I'm not sure how important outlier truncation is to the SVD, as it remains to be evaluated, but for now we should be able to get around this with no code changes by simply disabling it (i.e., setting the relevant truncation percentile to 0). Note that file I/O takes about an hour for this case. Also note that this is probably on the extreme end of what we should expect to support on a single machine with all counts in memory, as the SVD is probably sufficiently good with 100 samples and 100 bp is on the order of the read length. #4728 will get around this and also make downstream tasks complete faster in parallel, at the very small expense of reducing a few global parameters to per-contig parameters in the modeling step.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4734
https://github.com/broadinstitute/gatk/issues/4734:1101,Availability,down,downstream,1101,"Ran into this when trying to create a PoN with 100 samples x 100 bp bins = 1.27 * max int elements. This currently causes issues when truncating outliers, at which point all elements are loaded into an array so that Percentiles can be naively computed, resulting in a `java.lang.NegativeArraySizeException`. Solutions include: 1) simply throwing a message and failing early if the counts matrix is too large (perhaps recommend scattering by contig, see #4728), 2) changing the outlier truncation procedure to be more robust. I'm not sure how important outlier truncation is to the SVD, as it remains to be evaluated, but for now we should be able to get around this with no code changes by simply disabling it (i.e., setting the relevant truncation percentile to 0). Note that file I/O takes about an hour for this case. Also note that this is probably on the extreme end of what we should expect to support on a single machine with all counts in memory, as the SVD is probably sufficiently good with 100 samples and 100 bp is on the order of the read length. #4728 will get around this and also make downstream tasks complete faster in parallel, at the very small expense of reducing a few global parameters to per-contig parameters in the modeling step.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4734
https://github.com/broadinstitute/gatk/issues/4734:348,Integrability,message,message,348,"Ran into this when trying to create a PoN with 100 samples x 100 bp bins = 1.27 * max int elements. This currently causes issues when truncating outliers, at which point all elements are loaded into an array so that Percentiles can be naively computed, resulting in a `java.lang.NegativeArraySizeException`. Solutions include: 1) simply throwing a message and failing early if the counts matrix is too large (perhaps recommend scattering by contig, see #4728), 2) changing the outlier truncation procedure to be more robust. I'm not sure how important outlier truncation is to the SVD, as it remains to be evaluated, but for now we should be able to get around this with no code changes by simply disabling it (i.e., setting the relevant truncation percentile to 0). Note that file I/O takes about an hour for this case. Also note that this is probably on the extreme end of what we should expect to support on a single machine with all counts in memory, as the SVD is probably sufficiently good with 100 samples and 100 bp is on the order of the read length. #4728 will get around this and also make downstream tasks complete faster in parallel, at the very small expense of reducing a few global parameters to per-contig parameters in the modeling step.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4734
https://github.com/broadinstitute/gatk/issues/4734:187,Performance,load,loaded,187,"Ran into this when trying to create a PoN with 100 samples x 100 bp bins = 1.27 * max int elements. This currently causes issues when truncating outliers, at which point all elements are loaded into an array so that Percentiles can be naively computed, resulting in a `java.lang.NegativeArraySizeException`. Solutions include: 1) simply throwing a message and failing early if the counts matrix is too large (perhaps recommend scattering by contig, see #4728), 2) changing the outlier truncation procedure to be more robust. I'm not sure how important outlier truncation is to the SVD, as it remains to be evaluated, but for now we should be able to get around this with no code changes by simply disabling it (i.e., setting the relevant truncation percentile to 0). Note that file I/O takes about an hour for this case. Also note that this is probably on the extreme end of what we should expect to support on a single machine with all counts in memory, as the SVD is probably sufficiently good with 100 samples and 100 bp is on the order of the read length. #4728 will get around this and also make downstream tasks complete faster in parallel, at the very small expense of reducing a few global parameters to per-contig parameters in the modeling step.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4734
https://github.com/broadinstitute/gatk/issues/4734:330,Usability,simpl,simply,330,"Ran into this when trying to create a PoN with 100 samples x 100 bp bins = 1.27 * max int elements. This currently causes issues when truncating outliers, at which point all elements are loaded into an array so that Percentiles can be naively computed, resulting in a `java.lang.NegativeArraySizeException`. Solutions include: 1) simply throwing a message and failing early if the counts matrix is too large (perhaps recommend scattering by contig, see #4728), 2) changing the outlier truncation procedure to be more robust. I'm not sure how important outlier truncation is to the SVD, as it remains to be evaluated, but for now we should be able to get around this with no code changes by simply disabling it (i.e., setting the relevant truncation percentile to 0). Note that file I/O takes about an hour for this case. Also note that this is probably on the extreme end of what we should expect to support on a single machine with all counts in memory, as the SVD is probably sufficiently good with 100 samples and 100 bp is on the order of the read length. #4728 will get around this and also make downstream tasks complete faster in parallel, at the very small expense of reducing a few global parameters to per-contig parameters in the modeling step.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4734
https://github.com/broadinstitute/gatk/issues/4734:690,Usability,simpl,simply,690,"Ran into this when trying to create a PoN with 100 samples x 100 bp bins = 1.27 * max int elements. This currently causes issues when truncating outliers, at which point all elements are loaded into an array so that Percentiles can be naively computed, resulting in a `java.lang.NegativeArraySizeException`. Solutions include: 1) simply throwing a message and failing early if the counts matrix is too large (perhaps recommend scattering by contig, see #4728), 2) changing the outlier truncation procedure to be more robust. I'm not sure how important outlier truncation is to the SVD, as it remains to be evaluated, but for now we should be able to get around this with no code changes by simply disabling it (i.e., setting the relevant truncation percentile to 0). Note that file I/O takes about an hour for this case. Also note that this is probably on the extreme end of what we should expect to support on a single machine with all counts in memory, as the SVD is probably sufficiently good with 100 samples and 100 bp is on the order of the read length. #4728 will get around this and also make downstream tasks complete faster in parallel, at the very small expense of reducing a few global parameters to per-contig parameters in the modeling step.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4734
https://github.com/broadinstitute/gatk/pull/4736:48,Deployability,integrat,integration,48,"Two .vcf.idx files used by the haplotype caller integration test had; file name lengths > 144. This is incompatible with ecryptfs, which is; commonly used for encrypted home directories on linux. Renaming the; .vcf and .vcf.idx files and updating references to them fixed the; problem. Fixes #4718.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4736
https://github.com/broadinstitute/gatk/pull/4736:48,Integrability,integrat,integration,48,"Two .vcf.idx files used by the haplotype caller integration test had; file name lengths > 144. This is incompatible with ecryptfs, which is; commonly used for encrypted home directories on linux. Renaming the; .vcf and .vcf.idx files and updating references to them fixed the; problem. Fixes #4718.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4736
https://github.com/broadinstitute/gatk/pull/4736:159,Security,encrypt,encrypted,159,"Two .vcf.idx files used by the haplotype caller integration test had; file name lengths > 144. This is incompatible with ecryptfs, which is; commonly used for encrypted home directories on linux. Renaming the; .vcf and .vcf.idx files and updating references to them fixed the; problem. Fixes #4718.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4736
https://github.com/broadinstitute/gatk/pull/4736:60,Testability,test,test,60,"Two .vcf.idx files used by the haplotype caller integration test had; file name lengths > 144. This is incompatible with ecryptfs, which is; commonly used for encrypted home directories on linux. Renaming the; .vcf and .vcf.idx files and updating references to them fixed the; problem. Fixes #4718.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4736
https://github.com/broadinstitute/gatk/issues/4738:1016,Deployability,rolling,rolling,1016,"With the GATK gCNV having great performance results on the first round of evaluations it is ready to be used to call on ExAC. The following things need to be done first:. - Set up gCNV workflow to run on SGE (since exome samples are stored on prem). - Decide on target filtering strategy. . - Decide on the number of samples to use to learn the model (PoN). - Get some truth data to do QC, for example CNV calls from Genome STRiP on matched genome samples in gnomAD. - Design an interval list for samples in ExAC that do not mention one in their metadata. One possible solution could be to use cluster assignment of a sample to choose the interval list pertaining to that cluster. - (Optional) Consider importing list of common CNV regions into gCNV. To make job of gCNV inference easier we could use the list of common CNV regions that was obtained from Genome STRiP calls. To start @ldgauthier suggested using samples sequenced using latest Illumina capture protocol (Standard_Exome_Sequencing_v4) to get the ball rolling",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4738
https://github.com/broadinstitute/gatk/issues/4738:960,Integrability,protocol,protocol,960,"With the GATK gCNV having great performance results on the first round of evaluations it is ready to be used to call on ExAC. The following things need to be done first:. - Set up gCNV workflow to run on SGE (since exome samples are stored on prem). - Decide on target filtering strategy. . - Decide on the number of samples to use to learn the model (PoN). - Get some truth data to do QC, for example CNV calls from Genome STRiP on matched genome samples in gnomAD. - Design an interval list for samples in ExAC that do not mention one in their metadata. One possible solution could be to use cluster assignment of a sample to choose the interval list pertaining to that cluster. - (Optional) Consider importing list of common CNV regions into gCNV. To make job of gCNV inference easier we could use the list of common CNV regions that was obtained from Genome STRiP calls. To start @ldgauthier suggested using samples sequenced using latest Illumina capture protocol (Standard_Exome_Sequencing_v4) to get the ball rolling",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4738
https://github.com/broadinstitute/gatk/issues/4738:32,Performance,perform,performance,32,"With the GATK gCNV having great performance results on the first round of evaluations it is ready to be used to call on ExAC. The following things need to be done first:. - Set up gCNV workflow to run on SGE (since exome samples are stored on prem). - Decide on target filtering strategy. . - Decide on the number of samples to use to learn the model (PoN). - Get some truth data to do QC, for example CNV calls from Genome STRiP on matched genome samples in gnomAD. - Design an interval list for samples in ExAC that do not mention one in their metadata. One possible solution could be to use cluster assignment of a sample to choose the interval list pertaining to that cluster. - (Optional) Consider importing list of common CNV regions into gCNV. To make job of gCNV inference easier we could use the list of common CNV regions that was obtained from Genome STRiP calls. To start @ldgauthier suggested using samples sequenced using latest Illumina capture protocol (Standard_Exome_Sequencing_v4) to get the ball rolling",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4738
https://github.com/broadinstitute/gatk/issues/4738:335,Usability,learn,learn,335,"With the GATK gCNV having great performance results on the first round of evaluations it is ready to be used to call on ExAC. The following things need to be done first:. - Set up gCNV workflow to run on SGE (since exome samples are stored on prem). - Decide on target filtering strategy. . - Decide on the number of samples to use to learn the model (PoN). - Get some truth data to do QC, for example CNV calls from Genome STRiP on matched genome samples in gnomAD. - Design an interval list for samples in ExAC that do not mention one in their metadata. One possible solution could be to use cluster assignment of a sample to choose the interval list pertaining to that cluster. - (Optional) Consider importing list of common CNV regions into gCNV. To make job of gCNV inference easier we could use the list of common CNV regions that was obtained from Genome STRiP calls. To start @ldgauthier suggested using samples sequenced using latest Illumina capture protocol (Standard_Exome_Sequencing_v4) to get the ball rolling",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4738
https://github.com/broadinstitute/gatk/issues/4739:167,Availability,ERROR,ERROR,167,"The hg19 gencode is missing the transcript `ENST00000515683.1`. It appears in the gtf, but not in the fasta. Using `funcotator_dataSources.v1.2.20180329`. ```; A USER ERROR has occurred: Bad input: Unable to find the given Transcript ID in our transcript list (not in given transcript FASTA file): ENST00000515683.1; ```; ```; gatk Funcotator --output-file-format VCF \; --ref-version hg19 --data-sources-path ~/data/funcotator/funcotator_dataSources.v1.2.20180329/ \; -R ~/data/Homo_sapiens_assembly19.fasta -V 0816201804HC0_R01C01.vcf.gz \; -O 0816201804HC0_R01C01.funcotated.vcf; ```. The error is triggered somewhere near the position`4:60143686`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4739
https://github.com/broadinstitute/gatk/issues/4739:592,Availability,error,error,592,"The hg19 gencode is missing the transcript `ENST00000515683.1`. It appears in the gtf, but not in the fasta. Using `funcotator_dataSources.v1.2.20180329`. ```; A USER ERROR has occurred: Bad input: Unable to find the given Transcript ID in our transcript list (not in given transcript FASTA file): ENST00000515683.1; ```; ```; gatk Funcotator --output-file-format VCF \; --ref-version hg19 --data-sources-path ~/data/funcotator/funcotator_dataSources.v1.2.20180329/ \; -R ~/data/Homo_sapiens_assembly19.fasta -V 0816201804HC0_R01C01.vcf.gz \; -O 0816201804HC0_R01C01.funcotated.vcf; ```. The error is triggered somewhere near the position`4:60143686`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4739
https://github.com/broadinstitute/gatk/issues/4741:1234,Availability,down,download,1234,"ent with the command:; ```; conda env create -n gatk -f scripts/gatkcondaenv.yml; ```; This currently fails with the following message (at least on MacOS):; ```; Requirement 'build/gatkPythonPackageArchive.zip' looks like a filename, but the file does not exist; Processing ./build/gatkPythonPackageArchive.zip; Exception:; Traceback (most recent call last):; File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/basecommand.py"", line 215, in main; status = self.run(options, args); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/commands/install.py"", line 335, in run; wb.build(autobuilding=True); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/wheel.py"", line 749, in build; self.requirement_set.prepare_files(self.finder); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/req/req_set.py"", line 380, in prepare_files; ignore_dependencies=self.ignore_dependencies)); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/req/req_set.py"", line 620, in _prepare_file; session=self.session, hashes=hashes); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/download.py"", line 809, in unpack_url; unpack_file_url(link, location, download_dir, hashes=hashes); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/download.py"", line 715, in unpack_file_url; unpack_file(from_path, location, content_type, link); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/utils/__init__.py"", line 599, in unpack_file; flatten=not filename.endswith('.whl'); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/utils/__init__.py"", line 482, in unzip_file; zipfp = open(filename, 'rb'); FileNotFoundError: [Errno 2] No such file or directory: '/Users/markw/IdeaProjects/gatk/scripts/build/gatkPythonPackageArchive.zip'; ```; Moving gatkcondaenv.yml to the GATK root solves the issue. We can either change the yml location or modify the readme.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4741
https://github.com/broadinstitute/gatk/issues/4741:1405,Availability,down,download,1405,"ent with the command:; ```; conda env create -n gatk -f scripts/gatkcondaenv.yml; ```; This currently fails with the following message (at least on MacOS):; ```; Requirement 'build/gatkPythonPackageArchive.zip' looks like a filename, but the file does not exist; Processing ./build/gatkPythonPackageArchive.zip; Exception:; Traceback (most recent call last):; File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/basecommand.py"", line 215, in main; status = self.run(options, args); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/commands/install.py"", line 335, in run; wb.build(autobuilding=True); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/wheel.py"", line 749, in build; self.requirement_set.prepare_files(self.finder); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/req/req_set.py"", line 380, in prepare_files; ignore_dependencies=self.ignore_dependencies)); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/req/req_set.py"", line 620, in _prepare_file; session=self.session, hashes=hashes); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/download.py"", line 809, in unpack_url; unpack_file_url(link, location, download_dir, hashes=hashes); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/download.py"", line 715, in unpack_file_url; unpack_file(from_path, location, content_type, link); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/utils/__init__.py"", line 599, in unpack_file; flatten=not filename.endswith('.whl'); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/utils/__init__.py"", line 482, in unzip_file; zipfp = open(filename, 'rb'); FileNotFoundError: [Errno 2] No such file or directory: '/Users/markw/IdeaProjects/gatk/scripts/build/gatkPythonPackageArchive.zip'; ```; Moving gatkcondaenv.yml to the GATK root solves the issue. We can either change the yml location or modify the readme.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4741
https://github.com/broadinstitute/gatk/issues/4741:638,Deployability,install,install,638,"README.md instructs developers to create the conda environment with the command:; ```; conda env create -n gatk -f scripts/gatkcondaenv.yml; ```; This currently fails with the following message (at least on MacOS):; ```; Requirement 'build/gatkPythonPackageArchive.zip' looks like a filename, but the file does not exist; Processing ./build/gatkPythonPackageArchive.zip; Exception:; Traceback (most recent call last):; File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/basecommand.py"", line 215, in main; status = self.run(options, args); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/commands/install.py"", line 335, in run; wb.build(autobuilding=True); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/wheel.py"", line 749, in build; self.requirement_set.prepare_files(self.finder); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/req/req_set.py"", line 380, in prepare_files; ignore_dependencies=self.ignore_dependencies)); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/req/req_set.py"", line 620, in _prepare_file; session=self.session, hashes=hashes); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/download.py"", line 809, in unpack_url; unpack_file_url(link, location, download_dir, hashes=hashes); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/download.py"", line 715, in unpack_file_url; unpack_file(from_path, location, content_type, link); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/utils/__init__.py"", line 599, in unpack_file; flatten=not filename.endswith('.whl'); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/utils/__init__.py"", line 482, in unzip_file; zipfp = open(filename, 'rb'); FileNotFoundError: [Errno 2] No such file or directory: '/Users/markw/IdeaProjects/gatk/scripts/build/gatkPythonPackageArchive.zip'; ```; Moving gatkcondaenv.yml to the GATK root solves the issue. W",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4741
https://github.com/broadinstitute/gatk/issues/4741:186,Integrability,message,message,186,"README.md instructs developers to create the conda environment with the command:; ```; conda env create -n gatk -f scripts/gatkcondaenv.yml; ```; This currently fails with the following message (at least on MacOS):; ```; Requirement 'build/gatkPythonPackageArchive.zip' looks like a filename, but the file does not exist; Processing ./build/gatkPythonPackageArchive.zip; Exception:; Traceback (most recent call last):; File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/basecommand.py"", line 215, in main; status = self.run(options, args); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/commands/install.py"", line 335, in run; wb.build(autobuilding=True); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/wheel.py"", line 749, in build; self.requirement_set.prepare_files(self.finder); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/req/req_set.py"", line 380, in prepare_files; ignore_dependencies=self.ignore_dependencies)); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/req/req_set.py"", line 620, in _prepare_file; session=self.session, hashes=hashes); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/download.py"", line 809, in unpack_url; unpack_file_url(link, location, download_dir, hashes=hashes); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/download.py"", line 715, in unpack_file_url; unpack_file(from_path, location, content_type, link); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/utils/__init__.py"", line 599, in unpack_file; flatten=not filename.endswith('.whl'); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/utils/__init__.py"", line 482, in unzip_file; zipfp = open(filename, 'rb'); FileNotFoundError: [Errno 2] No such file or directory: '/Users/markw/IdeaProjects/gatk/scripts/build/gatkPythonPackageArchive.zip'; ```; Moving gatkcondaenv.yml to the GATK root solves the issue. W",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4741
https://github.com/broadinstitute/gatk/issues/4741:1148,Security,hash,hashes,1148,"ent with the command:; ```; conda env create -n gatk -f scripts/gatkcondaenv.yml; ```; This currently fails with the following message (at least on MacOS):; ```; Requirement 'build/gatkPythonPackageArchive.zip' looks like a filename, but the file does not exist; Processing ./build/gatkPythonPackageArchive.zip; Exception:; Traceback (most recent call last):; File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/basecommand.py"", line 215, in main; status = self.run(options, args); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/commands/install.py"", line 335, in run; wb.build(autobuilding=True); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/wheel.py"", line 749, in build; self.requirement_set.prepare_files(self.finder); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/req/req_set.py"", line 380, in prepare_files; ignore_dependencies=self.ignore_dependencies)); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/req/req_set.py"", line 620, in _prepare_file; session=self.session, hashes=hashes); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/download.py"", line 809, in unpack_url; unpack_file_url(link, location, download_dir, hashes=hashes); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/download.py"", line 715, in unpack_file_url; unpack_file(from_path, location, content_type, link); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/utils/__init__.py"", line 599, in unpack_file; flatten=not filename.endswith('.whl'); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/utils/__init__.py"", line 482, in unzip_file; zipfp = open(filename, 'rb'); FileNotFoundError: [Errno 2] No such file or directory: '/Users/markw/IdeaProjects/gatk/scripts/build/gatkPythonPackageArchive.zip'; ```; Moving gatkcondaenv.yml to the GATK root solves the issue. We can either change the yml location or modify the readme.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4741
https://github.com/broadinstitute/gatk/issues/4741:1155,Security,hash,hashes,1155,"ent with the command:; ```; conda env create -n gatk -f scripts/gatkcondaenv.yml; ```; This currently fails with the following message (at least on MacOS):; ```; Requirement 'build/gatkPythonPackageArchive.zip' looks like a filename, but the file does not exist; Processing ./build/gatkPythonPackageArchive.zip; Exception:; Traceback (most recent call last):; File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/basecommand.py"", line 215, in main; status = self.run(options, args); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/commands/install.py"", line 335, in run; wb.build(autobuilding=True); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/wheel.py"", line 749, in build; self.requirement_set.prepare_files(self.finder); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/req/req_set.py"", line 380, in prepare_files; ignore_dependencies=self.ignore_dependencies)); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/req/req_set.py"", line 620, in _prepare_file; session=self.session, hashes=hashes); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/download.py"", line 809, in unpack_url; unpack_file_url(link, location, download_dir, hashes=hashes); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/download.py"", line 715, in unpack_file_url; unpack_file(from_path, location, content_type, link); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/utils/__init__.py"", line 599, in unpack_file; flatten=not filename.endswith('.whl'); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/utils/__init__.py"", line 482, in unzip_file; zipfp = open(filename, 'rb'); FileNotFoundError: [Errno 2] No such file or directory: '/Users/markw/IdeaProjects/gatk/scripts/build/gatkPythonPackageArchive.zip'; ```; Moving gatkcondaenv.yml to the GATK root solves the issue. We can either change the yml location or modify the readme.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4741
https://github.com/broadinstitute/gatk/issues/4741:1319,Security,hash,hashes,1319,"ent with the command:; ```; conda env create -n gatk -f scripts/gatkcondaenv.yml; ```; This currently fails with the following message (at least on MacOS):; ```; Requirement 'build/gatkPythonPackageArchive.zip' looks like a filename, but the file does not exist; Processing ./build/gatkPythonPackageArchive.zip; Exception:; Traceback (most recent call last):; File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/basecommand.py"", line 215, in main; status = self.run(options, args); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/commands/install.py"", line 335, in run; wb.build(autobuilding=True); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/wheel.py"", line 749, in build; self.requirement_set.prepare_files(self.finder); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/req/req_set.py"", line 380, in prepare_files; ignore_dependencies=self.ignore_dependencies)); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/req/req_set.py"", line 620, in _prepare_file; session=self.session, hashes=hashes); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/download.py"", line 809, in unpack_url; unpack_file_url(link, location, download_dir, hashes=hashes); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/download.py"", line 715, in unpack_file_url; unpack_file(from_path, location, content_type, link); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/utils/__init__.py"", line 599, in unpack_file; flatten=not filename.endswith('.whl'); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/utils/__init__.py"", line 482, in unzip_file; zipfp = open(filename, 'rb'); FileNotFoundError: [Errno 2] No such file or directory: '/Users/markw/IdeaProjects/gatk/scripts/build/gatkPythonPackageArchive.zip'; ```; Moving gatkcondaenv.yml to the GATK root solves the issue. We can either change the yml location or modify the readme.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4741
https://github.com/broadinstitute/gatk/issues/4741:1326,Security,hash,hashes,1326,"ent with the command:; ```; conda env create -n gatk -f scripts/gatkcondaenv.yml; ```; This currently fails with the following message (at least on MacOS):; ```; Requirement 'build/gatkPythonPackageArchive.zip' looks like a filename, but the file does not exist; Processing ./build/gatkPythonPackageArchive.zip; Exception:; Traceback (most recent call last):; File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/basecommand.py"", line 215, in main; status = self.run(options, args); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/commands/install.py"", line 335, in run; wb.build(autobuilding=True); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/wheel.py"", line 749, in build; self.requirement_set.prepare_files(self.finder); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/req/req_set.py"", line 380, in prepare_files; ignore_dependencies=self.ignore_dependencies)); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/req/req_set.py"", line 620, in _prepare_file; session=self.session, hashes=hashes); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/download.py"", line 809, in unpack_url; unpack_file_url(link, location, download_dir, hashes=hashes); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/download.py"", line 715, in unpack_file_url; unpack_file(from_path, location, content_type, link); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/utils/__init__.py"", line 599, in unpack_file; flatten=not filename.endswith('.whl'); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/utils/__init__.py"", line 482, in unzip_file; zipfp = open(filename, 'rb'); FileNotFoundError: [Errno 2] No such file or directory: '/Users/markw/IdeaProjects/gatk/scripts/build/gatkPythonPackageArchive.zip'; ```; Moving gatkcondaenv.yml to the GATK root solves the issue. We can either change the yml location or modify the readme.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4741
https://github.com/broadinstitute/gatk/issues/4742:84,Availability,error,error,84,"On OS X El Capitan 10.11.6, when creating the conda environment I get the following error:; ```; gcc -undefined dynamic_lookup -L/Users/markw/anaconda/envs/gatk/lib -L/Users/markw/anaconda/envs/gatk/lib -arch x86_64 build/temp.macosx-10.7-x86_64-3.6/pysam/libchtslib.o build/temp.macosx-10.7-x86_64-3.6/pysam/htslib_util.o build/temp.macosx-10.7-x86_64-3.6/htslib/kfunc.o build/temp.macosx-10.7-x86_64-3.6/htslib/knetfile.o build/temp.macosx-10.7-x86_64-3.6/htslib/kstring.o build/temp.macosx-10.7-x86_64-3.6/htslib/bcf_sr_sort.o build/temp.macosx-10.7-x86_64-3.6/htslib/bgzf.o build/temp.macosx-10.7-x86_64-3.6/htslib/errmod.o build/temp.macosx-10.7-x86_64-3.6/htslib/faidx.o build/temp.macosx-10.7-x86_64-3.6/htslib/hfile.o build/temp.macosx-10.7-x86_64-3.6/htslib/hfile_net.o build/temp.macosx-10.7-x86_64-3.6/htslib/hts.o build/temp.macosx-10.7-x86_64-3.6/htslib/hts_os.o build/temp.macosx-10.7-x86_64-3.6/htslib/md5.o build/temp.macosx-10.7-x86_64-3.6/htslib/multipart.o build/temp.macosx-10.7-x86_64-3.6/htslib/probaln.o build/temp.macosx-10.7-x86_64-3.6/htslib/realn.o build/temp.macosx-10.7-x86_64-3.6/htslib/regidx.o build/temp.macosx-10.7-x86_64-3.6/htslib/sam.o build/temp.macosx-10.7-x86_64-3.6/htslib/synced_bcf_reader.o build/temp.macosx-10.7-x86_64-3.6/htslib/vcf_sweep.o build/temp.macosx-10.7-x86_64-3.6/htslib/tbx.o build/temp.macosx-10.7-x86_64-3.6/htslib/textutils.o build/temp.macosx-10.7-x86_64-3.6/htslib/thread_pool.o build/temp.macosx-10.7-x86_64-3.6/htslib/vcf.o build/temp.macosx-10.7-x86_64-3.6/htslib/vcfutils.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/cram_codecs.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/cram_decode.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/cram_encode.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/cram_external.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/cram_index.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/cram_io.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/cram_samtools.o build/temp.macosx-10.7-x86_64-3.6/htslib/",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4742
https://github.com/broadinstitute/gatk/issues/4742:2965,Availability,error,error,2965,"o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/cram_samtools.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/cram_stats.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/files.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/mFILE.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/open_trace_file.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/pooled_alloc.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/rANS_static.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/sam_header.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/string_alloc.o build/temp.macosx-10.7-x86_64-3.6/htslib/hfile_libcurl.o build/temp.macosx-10.7-x86_64-3.6/htslib/hfile_gcs.o build/temp.macosx-10.7-x86_64-3.6/htslib/hfile_s3.o -Lpysam -L. -Lbuild/lib.macosx-10.7-x86_64-3.6/pysam -Lbuild/lib.macosx-10.7-x86_64-3.6/pysam -L/Users/markw/anaconda/envs/gatk/lib -lz -llzma -lbz2 -lz -lcurl -o build/lib.macosx-10.7-x86_64-3.6/pysam/libchtslib.cpython-36m-darwin.so -dynamiclib -rpath @loader_path -Wl,-headerpad_max_install_names -Wl,-install_name,@rpath/libchtslib.cpython-36m-darwin.so -Wl,-x; gcc: error: @loader_path: No such file or directory; gcc: error: unrecognized command line option -rpath; error: command 'gcc' failed with exit status 1. ----------------------------------------; Command ""/Users/markw/anaconda/envs/gatk/bin/python -u -c ""import setuptools, tokenize;__file__='/private/var/folders/x6/k5tc9mwd2z7dm1dthy4hv9nxmt8q9j/T/pip-build-aybz1ucp/pysam/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record /var/folders/x6/k5tc9mwd2z7dm1dthy4hv9nxmt8q9j/T/pip-z_8e2y_r-record/install-record.txt --single-version-externally-managed --compile"" failed with error code 1 in /private/var/folders/x6/k5tc9mwd2z7dm1dthy4hv9nxmt8q9j/T/pip-build-aybz1ucp/pysam/. CondaValueError: pip returned an error; ```; This appears to have been an issue with pysam in the past: https://github.com/pysam-developers/pysam/issues/323",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4742
https://github.com/broadinstitute/gatk/issues/4742:3018,Availability,error,error,3018,"o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/cram_samtools.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/cram_stats.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/files.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/mFILE.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/open_trace_file.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/pooled_alloc.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/rANS_static.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/sam_header.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/string_alloc.o build/temp.macosx-10.7-x86_64-3.6/htslib/hfile_libcurl.o build/temp.macosx-10.7-x86_64-3.6/htslib/hfile_gcs.o build/temp.macosx-10.7-x86_64-3.6/htslib/hfile_s3.o -Lpysam -L. -Lbuild/lib.macosx-10.7-x86_64-3.6/pysam -Lbuild/lib.macosx-10.7-x86_64-3.6/pysam -L/Users/markw/anaconda/envs/gatk/lib -lz -llzma -lbz2 -lz -lcurl -o build/lib.macosx-10.7-x86_64-3.6/pysam/libchtslib.cpython-36m-darwin.so -dynamiclib -rpath @loader_path -Wl,-headerpad_max_install_names -Wl,-install_name,@rpath/libchtslib.cpython-36m-darwin.so -Wl,-x; gcc: error: @loader_path: No such file or directory; gcc: error: unrecognized command line option -rpath; error: command 'gcc' failed with exit status 1. ----------------------------------------; Command ""/Users/markw/anaconda/envs/gatk/bin/python -u -c ""import setuptools, tokenize;__file__='/private/var/folders/x6/k5tc9mwd2z7dm1dthy4hv9nxmt8q9j/T/pip-build-aybz1ucp/pysam/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record /var/folders/x6/k5tc9mwd2z7dm1dthy4hv9nxmt8q9j/T/pip-z_8e2y_r-record/install-record.txt --single-version-externally-managed --compile"" failed with error code 1 in /private/var/folders/x6/k5tc9mwd2z7dm1dthy4hv9nxmt8q9j/T/pip-build-aybz1ucp/pysam/. CondaValueError: pip returned an error; ```; This appears to have been an issue with pysam in the past: https://github.com/pysam-developers/pysam/issues/323",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4742
https://github.com/broadinstitute/gatk/issues/4742:3068,Availability,error,error,3068,"o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/cram_samtools.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/cram_stats.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/files.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/mFILE.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/open_trace_file.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/pooled_alloc.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/rANS_static.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/sam_header.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/string_alloc.o build/temp.macosx-10.7-x86_64-3.6/htslib/hfile_libcurl.o build/temp.macosx-10.7-x86_64-3.6/htslib/hfile_gcs.o build/temp.macosx-10.7-x86_64-3.6/htslib/hfile_s3.o -Lpysam -L. -Lbuild/lib.macosx-10.7-x86_64-3.6/pysam -Lbuild/lib.macosx-10.7-x86_64-3.6/pysam -L/Users/markw/anaconda/envs/gatk/lib -lz -llzma -lbz2 -lz -lcurl -o build/lib.macosx-10.7-x86_64-3.6/pysam/libchtslib.cpython-36m-darwin.so -dynamiclib -rpath @loader_path -Wl,-headerpad_max_install_names -Wl,-install_name,@rpath/libchtslib.cpython-36m-darwin.so -Wl,-x; gcc: error: @loader_path: No such file or directory; gcc: error: unrecognized command line option -rpath; error: command 'gcc' failed with exit status 1. ----------------------------------------; Command ""/Users/markw/anaconda/envs/gatk/bin/python -u -c ""import setuptools, tokenize;__file__='/private/var/folders/x6/k5tc9mwd2z7dm1dthy4hv9nxmt8q9j/T/pip-build-aybz1ucp/pysam/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record /var/folders/x6/k5tc9mwd2z7dm1dthy4hv9nxmt8q9j/T/pip-z_8e2y_r-record/install-record.txt --single-version-externally-managed --compile"" failed with error code 1 in /private/var/folders/x6/k5tc9mwd2z7dm1dthy4hv9nxmt8q9j/T/pip-build-aybz1ucp/pysam/. CondaValueError: pip returned an error; ```; This appears to have been an issue with pysam in the past: https://github.com/pysam-developers/pysam/issues/323",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4742
https://github.com/broadinstitute/gatk/issues/4742:3640,Availability,error,error,3640,"o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/cram_samtools.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/cram_stats.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/files.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/mFILE.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/open_trace_file.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/pooled_alloc.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/rANS_static.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/sam_header.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/string_alloc.o build/temp.macosx-10.7-x86_64-3.6/htslib/hfile_libcurl.o build/temp.macosx-10.7-x86_64-3.6/htslib/hfile_gcs.o build/temp.macosx-10.7-x86_64-3.6/htslib/hfile_s3.o -Lpysam -L. -Lbuild/lib.macosx-10.7-x86_64-3.6/pysam -Lbuild/lib.macosx-10.7-x86_64-3.6/pysam -L/Users/markw/anaconda/envs/gatk/lib -lz -llzma -lbz2 -lz -lcurl -o build/lib.macosx-10.7-x86_64-3.6/pysam/libchtslib.cpython-36m-darwin.so -dynamiclib -rpath @loader_path -Wl,-headerpad_max_install_names -Wl,-install_name,@rpath/libchtslib.cpython-36m-darwin.so -Wl,-x; gcc: error: @loader_path: No such file or directory; gcc: error: unrecognized command line option -rpath; error: command 'gcc' failed with exit status 1. ----------------------------------------; Command ""/Users/markw/anaconda/envs/gatk/bin/python -u -c ""import setuptools, tokenize;__file__='/private/var/folders/x6/k5tc9mwd2z7dm1dthy4hv9nxmt8q9j/T/pip-build-aybz1ucp/pysam/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record /var/folders/x6/k5tc9mwd2z7dm1dthy4hv9nxmt8q9j/T/pip-z_8e2y_r-record/install-record.txt --single-version-externally-managed --compile"" failed with error code 1 in /private/var/folders/x6/k5tc9mwd2z7dm1dthy4hv9nxmt8q9j/T/pip-build-aybz1ucp/pysam/. CondaValueError: pip returned an error; ```; This appears to have been an issue with pysam in the past: https://github.com/pysam-developers/pysam/issues/323",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4742
https://github.com/broadinstitute/gatk/issues/4742:3773,Availability,error,error,3773,"o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/cram_samtools.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/cram_stats.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/files.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/mFILE.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/open_trace_file.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/pooled_alloc.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/rANS_static.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/sam_header.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/string_alloc.o build/temp.macosx-10.7-x86_64-3.6/htslib/hfile_libcurl.o build/temp.macosx-10.7-x86_64-3.6/htslib/hfile_gcs.o build/temp.macosx-10.7-x86_64-3.6/htslib/hfile_s3.o -Lpysam -L. -Lbuild/lib.macosx-10.7-x86_64-3.6/pysam -Lbuild/lib.macosx-10.7-x86_64-3.6/pysam -L/Users/markw/anaconda/envs/gatk/lib -lz -llzma -lbz2 -lz -lcurl -o build/lib.macosx-10.7-x86_64-3.6/pysam/libchtslib.cpython-36m-darwin.so -dynamiclib -rpath @loader_path -Wl,-headerpad_max_install_names -Wl,-install_name,@rpath/libchtslib.cpython-36m-darwin.so -Wl,-x; gcc: error: @loader_path: No such file or directory; gcc: error: unrecognized command line option -rpath; error: command 'gcc' failed with exit status 1. ----------------------------------------; Command ""/Users/markw/anaconda/envs/gatk/bin/python -u -c ""import setuptools, tokenize;__file__='/private/var/folders/x6/k5tc9mwd2z7dm1dthy4hv9nxmt8q9j/T/pip-build-aybz1ucp/pysam/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record /var/folders/x6/k5tc9mwd2z7dm1dthy4hv9nxmt8q9j/T/pip-z_8e2y_r-record/install-record.txt --single-version-externally-managed --compile"" failed with error code 1 in /private/var/folders/x6/k5tc9mwd2z7dm1dthy4hv9nxmt8q9j/T/pip-build-aybz1ucp/pysam/. CondaValueError: pip returned an error; ```; This appears to have been an issue with pysam in the past: https://github.com/pysam-developers/pysam/issues/323",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4742
https://github.com/broadinstitute/gatk/issues/4742:3476,Deployability,install,install,3476,"o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/cram_samtools.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/cram_stats.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/files.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/mFILE.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/open_trace_file.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/pooled_alloc.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/rANS_static.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/sam_header.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/string_alloc.o build/temp.macosx-10.7-x86_64-3.6/htslib/hfile_libcurl.o build/temp.macosx-10.7-x86_64-3.6/htslib/hfile_gcs.o build/temp.macosx-10.7-x86_64-3.6/htslib/hfile_s3.o -Lpysam -L. -Lbuild/lib.macosx-10.7-x86_64-3.6/pysam -Lbuild/lib.macosx-10.7-x86_64-3.6/pysam -L/Users/markw/anaconda/envs/gatk/lib -lz -llzma -lbz2 -lz -lcurl -o build/lib.macosx-10.7-x86_64-3.6/pysam/libchtslib.cpython-36m-darwin.so -dynamiclib -rpath @loader_path -Wl,-headerpad_max_install_names -Wl,-install_name,@rpath/libchtslib.cpython-36m-darwin.so -Wl,-x; gcc: error: @loader_path: No such file or directory; gcc: error: unrecognized command line option -rpath; error: command 'gcc' failed with exit status 1. ----------------------------------------; Command ""/Users/markw/anaconda/envs/gatk/bin/python -u -c ""import setuptools, tokenize;__file__='/private/var/folders/x6/k5tc9mwd2z7dm1dthy4hv9nxmt8q9j/T/pip-build-aybz1ucp/pysam/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record /var/folders/x6/k5tc9mwd2z7dm1dthy4hv9nxmt8q9j/T/pip-z_8e2y_r-record/install-record.txt --single-version-externally-managed --compile"" failed with error code 1 in /private/var/folders/x6/k5tc9mwd2z7dm1dthy4hv9nxmt8q9j/T/pip-build-aybz1ucp/pysam/. CondaValueError: pip returned an error; ```; This appears to have been an issue with pysam in the past: https://github.com/pysam-developers/pysam/issues/323",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4742
https://github.com/broadinstitute/gatk/issues/4742:3562,Deployability,install,install-record,3562,"o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/cram_samtools.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/cram_stats.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/files.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/mFILE.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/open_trace_file.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/pooled_alloc.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/rANS_static.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/sam_header.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/string_alloc.o build/temp.macosx-10.7-x86_64-3.6/htslib/hfile_libcurl.o build/temp.macosx-10.7-x86_64-3.6/htslib/hfile_gcs.o build/temp.macosx-10.7-x86_64-3.6/htslib/hfile_s3.o -Lpysam -L. -Lbuild/lib.macosx-10.7-x86_64-3.6/pysam -Lbuild/lib.macosx-10.7-x86_64-3.6/pysam -L/Users/markw/anaconda/envs/gatk/lib -lz -llzma -lbz2 -lz -lcurl -o build/lib.macosx-10.7-x86_64-3.6/pysam/libchtslib.cpython-36m-darwin.so -dynamiclib -rpath @loader_path -Wl,-headerpad_max_install_names -Wl,-install_name,@rpath/libchtslib.cpython-36m-darwin.so -Wl,-x; gcc: error: @loader_path: No such file or directory; gcc: error: unrecognized command line option -rpath; error: command 'gcc' failed with exit status 1. ----------------------------------------; Command ""/Users/markw/anaconda/envs/gatk/bin/python -u -c ""import setuptools, tokenize;__file__='/private/var/folders/x6/k5tc9mwd2z7dm1dthy4hv9nxmt8q9j/T/pip-build-aybz1ucp/pysam/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record /var/folders/x6/k5tc9mwd2z7dm1dthy4hv9nxmt8q9j/T/pip-z_8e2y_r-record/install-record.txt --single-version-externally-managed --compile"" failed with error code 1 in /private/var/folders/x6/k5tc9mwd2z7dm1dthy4hv9nxmt8q9j/T/pip-build-aybz1ucp/pysam/. CondaValueError: pip returned an error; ```; This appears to have been an issue with pysam in the past: https://github.com/pysam-developers/pysam/issues/323",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4742
https://github.com/broadinstitute/gatk/issues/4743:28,Availability,error,error,28,"I'm receiving the following error on Mac OS X El Capitan when trying to run gcnv:; ```; 13:32:10.054 DEBUG ScriptExecutor - Executing:; 13:32:10.054 DEBUG ScriptExecutor - python; 13:32:10.054 DEBUG ScriptExecutor - -c; 13:32:10.054 DEBUG ScriptExecutor - import gcnvkernel. Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/gcnvkernel/__init__.py"", line 1, in <module>; from pymc3 import __version__ as pymc3_version; File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pymc3/__init__.py"", line 12, in <module>; from .sampling import *; File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pymc3/sampling.py"", line 14, in <module>; from .plots.traceplot import traceplot; File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pymc3/plots/__init__.py"", line 1, in <module>; from .autocorrplot import autocorrplot; File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pymc3/plots/autocorrplot.py"", line 2, in <module>; import matplotlib.pyplot as plt; File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/matplotlib/pyplot.py"", line 113, in <module>; _backend_mod, new_figure_manager, draw_if_interactive, _show = pylab_setup(); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/matplotlib/backends/__init__.py"", line 60, in pylab_setup; [backend_name], 0); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/matplotlib/backends/backend_macosx.py"", line 19, in <module>; from matplotlib.backends import _macosx; RuntimeError: Python is not installed as a framework. The Mac OS X backend will not be able to function correctly if Python is not installed as a framework. See the Python documentation for more information on installing Python as a framework on Mac OS X. Please either reinstall Python as a framework, or try one of the other backends. If you are using (Ana)Conda please install python.app and replace t",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4743
https://github.com/broadinstitute/gatk/issues/4743:1625,Deployability,install,installed,1625,"nel/__init__.py"", line 1, in <module>; from pymc3 import __version__ as pymc3_version; File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pymc3/__init__.py"", line 12, in <module>; from .sampling import *; File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pymc3/sampling.py"", line 14, in <module>; from .plots.traceplot import traceplot; File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pymc3/plots/__init__.py"", line 1, in <module>; from .autocorrplot import autocorrplot; File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pymc3/plots/autocorrplot.py"", line 2, in <module>; import matplotlib.pyplot as plt; File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/matplotlib/pyplot.py"", line 113, in <module>; _backend_mod, new_figure_manager, draw_if_interactive, _show = pylab_setup(); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/matplotlib/backends/__init__.py"", line 60, in pylab_setup; [backend_name], 0); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/matplotlib/backends/backend_macosx.py"", line 19, in <module>; from matplotlib.backends import _macosx; RuntimeError: Python is not installed as a framework. The Mac OS X backend will not be able to function correctly if Python is not installed as a framework. See the Python documentation for more information on installing Python as a framework on Mac OS X. Please either reinstall Python as a framework, or try one of the other backends. If you are using (Ana)Conda please install python.app and replace the use of 'python' with 'pythonw'. See 'Working with Matplotlib on OSX' in the Matplotlib FAQ for more information.; ```; As it states, there is more information here: https://matplotlib.org/faq/osx_framework.html#osxframework-faq. I can resolve the issue by running `conda install -c anaconda python.app` and changing this line in `PythonExecutorBase.java`:; ```; PYTHON(""python""),; ```; to; ```; PYTHON(""pythonw""),; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4743
https://github.com/broadinstitute/gatk/issues/4743:1728,Deployability,install,installed,1728,"nel/__init__.py"", line 1, in <module>; from pymc3 import __version__ as pymc3_version; File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pymc3/__init__.py"", line 12, in <module>; from .sampling import *; File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pymc3/sampling.py"", line 14, in <module>; from .plots.traceplot import traceplot; File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pymc3/plots/__init__.py"", line 1, in <module>; from .autocorrplot import autocorrplot; File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pymc3/plots/autocorrplot.py"", line 2, in <module>; import matplotlib.pyplot as plt; File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/matplotlib/pyplot.py"", line 113, in <module>; _backend_mod, new_figure_manager, draw_if_interactive, _show = pylab_setup(); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/matplotlib/backends/__init__.py"", line 60, in pylab_setup; [backend_name], 0); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/matplotlib/backends/backend_macosx.py"", line 19, in <module>; from matplotlib.backends import _macosx; RuntimeError: Python is not installed as a framework. The Mac OS X backend will not be able to function correctly if Python is not installed as a framework. See the Python documentation for more information on installing Python as a framework on Mac OS X. Please either reinstall Python as a framework, or try one of the other backends. If you are using (Ana)Conda please install python.app and replace the use of 'python' with 'pythonw'. See 'Working with Matplotlib on OSX' in the Matplotlib FAQ for more information.; ```; As it states, there is more information here: https://matplotlib.org/faq/osx_framework.html#osxframework-faq. I can resolve the issue by running `conda install -c anaconda python.app` and changing this line in `PythonExecutorBase.java`:; ```; PYTHON(""python""),; ```; to; ```; PYTHON(""pythonw""),; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4743
https://github.com/broadinstitute/gatk/issues/4743:1807,Deployability,install,installing,1807,"nel/__init__.py"", line 1, in <module>; from pymc3 import __version__ as pymc3_version; File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pymc3/__init__.py"", line 12, in <module>; from .sampling import *; File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pymc3/sampling.py"", line 14, in <module>; from .plots.traceplot import traceplot; File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pymc3/plots/__init__.py"", line 1, in <module>; from .autocorrplot import autocorrplot; File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pymc3/plots/autocorrplot.py"", line 2, in <module>; import matplotlib.pyplot as plt; File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/matplotlib/pyplot.py"", line 113, in <module>; _backend_mod, new_figure_manager, draw_if_interactive, _show = pylab_setup(); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/matplotlib/backends/__init__.py"", line 60, in pylab_setup; [backend_name], 0); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/matplotlib/backends/backend_macosx.py"", line 19, in <module>; from matplotlib.backends import _macosx; RuntimeError: Python is not installed as a framework. The Mac OS X backend will not be able to function correctly if Python is not installed as a framework. See the Python documentation for more information on installing Python as a framework on Mac OS X. Please either reinstall Python as a framework, or try one of the other backends. If you are using (Ana)Conda please install python.app and replace the use of 'python' with 'pythonw'. See 'Working with Matplotlib on OSX' in the Matplotlib FAQ for more information.; ```; As it states, there is more information here: https://matplotlib.org/faq/osx_framework.html#osxframework-faq. I can resolve the issue by running `conda install -c anaconda python.app` and changing this line in `PythonExecutorBase.java`:; ```; PYTHON(""python""),; ```; to; ```; PYTHON(""pythonw""),; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4743
https://github.com/broadinstitute/gatk/issues/4743:1969,Deployability,install,install,1969,"nel/__init__.py"", line 1, in <module>; from pymc3 import __version__ as pymc3_version; File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pymc3/__init__.py"", line 12, in <module>; from .sampling import *; File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pymc3/sampling.py"", line 14, in <module>; from .plots.traceplot import traceplot; File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pymc3/plots/__init__.py"", line 1, in <module>; from .autocorrplot import autocorrplot; File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pymc3/plots/autocorrplot.py"", line 2, in <module>; import matplotlib.pyplot as plt; File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/matplotlib/pyplot.py"", line 113, in <module>; _backend_mod, new_figure_manager, draw_if_interactive, _show = pylab_setup(); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/matplotlib/backends/__init__.py"", line 60, in pylab_setup; [backend_name], 0); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/matplotlib/backends/backend_macosx.py"", line 19, in <module>; from matplotlib.backends import _macosx; RuntimeError: Python is not installed as a framework. The Mac OS X backend will not be able to function correctly if Python is not installed as a framework. See the Python documentation for more information on installing Python as a framework on Mac OS X. Please either reinstall Python as a framework, or try one of the other backends. If you are using (Ana)Conda please install python.app and replace the use of 'python' with 'pythonw'. See 'Working with Matplotlib on OSX' in the Matplotlib FAQ for more information.; ```; As it states, there is more information here: https://matplotlib.org/faq/osx_framework.html#osxframework-faq. I can resolve the issue by running `conda install -c anaconda python.app` and changing this line in `PythonExecutorBase.java`:; ```; PYTHON(""python""),; ```; to; ```; PYTHON(""pythonw""),; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4743
https://github.com/broadinstitute/gatk/issues/4743:2275,Deployability,install,install,2275,"nel/__init__.py"", line 1, in <module>; from pymc3 import __version__ as pymc3_version; File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pymc3/__init__.py"", line 12, in <module>; from .sampling import *; File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pymc3/sampling.py"", line 14, in <module>; from .plots.traceplot import traceplot; File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pymc3/plots/__init__.py"", line 1, in <module>; from .autocorrplot import autocorrplot; File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pymc3/plots/autocorrplot.py"", line 2, in <module>; import matplotlib.pyplot as plt; File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/matplotlib/pyplot.py"", line 113, in <module>; _backend_mod, new_figure_manager, draw_if_interactive, _show = pylab_setup(); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/matplotlib/backends/__init__.py"", line 60, in pylab_setup; [backend_name], 0); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/matplotlib/backends/backend_macosx.py"", line 19, in <module>; from matplotlib.backends import _macosx; RuntimeError: Python is not installed as a framework. The Mac OS X backend will not be able to function correctly if Python is not installed as a framework. See the Python documentation for more information on installing Python as a framework on Mac OS X. Please either reinstall Python as a framework, or try one of the other backends. If you are using (Ana)Conda please install python.app and replace the use of 'python' with 'pythonw'. See 'Working with Matplotlib on OSX' in the Matplotlib FAQ for more information.; ```; As it states, there is more information here: https://matplotlib.org/faq/osx_framework.html#osxframework-faq. I can resolve the issue by running `conda install -c anaconda python.app` and changing this line in `PythonExecutorBase.java`:; ```; PYTHON(""python""),; ```; to; ```; PYTHON(""pythonw""),; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4743
https://github.com/broadinstitute/gatk/issues/4745:707,Integrability,depend,dependent,707,"Question for @davidbenjamin and @takutosato. . > do you have any documents for default parameters to run in tumor only mode? When I set af-of-alleles-not-in-resource = 0.001, I get no variants (GATK 4.0.0.0). When I set it to 0 (as recommended in this discussion: https://gatkforums.broadinstitute.org/gatk/discussion/10157/gatk4-beta-no-filter-passing-variants-in-mutect2-tumor-only-runs-using-default-parameters) I get over 3000 variants, so I am trying to find what is best way to determine this number for tumor only mode. ---. Hi @shlee,; If I am running Mutect2 with same samples but different parameters, do I really need to repeat the step 5 CollectSequencingArtifactMetrics? It looks like it's not dependent on the previous filtering step but as you emphasized this on the tutorial I wanted to check.; Another question is do you have any documents for default parameters to run in tumor only mode? When I set af-of-alleles-not-in-resource = 0.001, I get no variants (GATK 4.0.0.0). When I set it to 0 (as recommended in this discussion: https://gatkforums.broadinstitute.org/gatk/discussion/10157/gatk4-beta-no-filter-passing-variants-in-mutect2-tumor-only-runs-using-default-parameters) I get over 3000 variants, so I am trying to find what is best way to determine this number for tumor only mode.; Last but not least thanks a lot for this tutorial and documentation. I keep coming back and reading this as its loaded with information.; Best,; Irem. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/comment/48461#Comment_48461",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4745
https://github.com/broadinstitute/gatk/issues/4745:1422,Performance,load,loaded,1422,"Question for @davidbenjamin and @takutosato. . > do you have any documents for default parameters to run in tumor only mode? When I set af-of-alleles-not-in-resource = 0.001, I get no variants (GATK 4.0.0.0). When I set it to 0 (as recommended in this discussion: https://gatkforums.broadinstitute.org/gatk/discussion/10157/gatk4-beta-no-filter-passing-variants-in-mutect2-tumor-only-runs-using-default-parameters) I get over 3000 variants, so I am trying to find what is best way to determine this number for tumor only mode. ---. Hi @shlee,; If I am running Mutect2 with same samples but different parameters, do I really need to repeat the step 5 CollectSequencingArtifactMetrics? It looks like it's not dependent on the previous filtering step but as you emphasized this on the tutorial I wanted to check.; Another question is do you have any documents for default parameters to run in tumor only mode? When I set af-of-alleles-not-in-resource = 0.001, I get no variants (GATK 4.0.0.0). When I set it to 0 (as recommended in this discussion: https://gatkforums.broadinstitute.org/gatk/discussion/10157/gatk4-beta-no-filter-passing-variants-in-mutect2-tumor-only-runs-using-default-parameters) I get over 3000 variants, so I am trying to find what is best way to determine this number for tumor only mode.; Last but not least thanks a lot for this tutorial and documentation. I keep coming back and reading this as its loaded with information.; Best,; Irem. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/comment/48461#Comment_48461",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4745
https://github.com/broadinstitute/gatk/issues/4746:464,Availability,down,downloaded,464,"In some preliminary testing I've done it looks like using native Hadoop libraries can speed up tools running in Spark local mode. In private Spark tools under development (which travers a WGS BAM and then performing several shuffles) I have seen speedups of up to 40% (~ 46 minutes -> 26 minutes). An initial test of `MarkDuplicatesSpark` using a 30GB bam file gave me a 9% speedup (logs are below). It might be good to investigate making this easier for users (I downloaded Hadoop and built it from source, and then set gatk's java opts to load the native library). Two options might be: 1) distribute native libraries for supported architectures with gatk or 2) make sure gatk docker images include the native libraries and are set to use them. Logs for `MarkDuplicatesSpark` without and with native libraries, running on a Broad login server:. Without:. ```; $ ${GATK_DIR}/gatk MarkDuplicatesSpark -I CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam -O CEUTrio.HiSeq.WEx37; .NA12892.readnamesort.dupmarked.bam -- --spark-runner LOCAL --spark-master local[8]; Using GATK wrapper script ${GATK_DIR}/gatk/build/install/gatk/bin/gatk; Running:; ${GATK_DIR}/gatk/build/install/gatk/bin/gatk MarkDuplicatesSpark -I CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam -O CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.dupmarked.bam --spark; -master local[8]; 14:40:21.800 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 14:40:21.889 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:${GATK_DIR}/gatk/build/install/gatk/lib/gkl-0.8.5.jar!/com/intel/gkl/native/libgkl_compression.so; 14:40:21.989 INFO MarkDuplicatesSpark - ------------------------------------------------------------; 14:40:21.990 INFO MarkDuplicatesSpark - The Genome Analysis Toolkit (GATK) v4.0.4.0-7-g46a8661-SNAPSHOT; 14:40:21.990 INFO MarkDuplicatesSpark - For support and documentatio",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4746
https://github.com/broadinstitute/gatk/issues/4746:4282,Availability,down,down,4282," MarkDuplicatesSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 14:40:21.994 WARN MarkDuplicatesSpark -. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: MarkDuplicatesSpark is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 14:40:21.994 INFO MarkDuplicatesSpark - Initializing engine; 14:40:21.994 INFO MarkDuplicatesSpark - Done initializing engine; 14:40:22.338 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 15:24:12.735 INFO ReadsSparkSink - Finished sorting the bam file and dumping read shards to disk, proceeding to merge the shards into a single file using the master thread; 15:41:27.766 INFO ReadsSparkSink - Finished merging shards into a single output bam; 15:41:34.351 INFO MarkDuplicatesSpark - Shutting down engine; [May 7, 2018 3:41:34 PM EDT] org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark done. Elapsed time: 61.21 minutes.; Runtime.totalMemory()=13635682304; ```. With native libraries (note the lack of the usual warning):. ```; $ ${GATK_DIR}/gatk MarkDuplicatesSpark --java-options ""-Djava.library.path=${HADOOP_DIR}/hadoop-2.6.5-src/hadoop-common-project/hadoop-common/target/hadoop-common-2.6.5/lib/native"" -I CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam -O CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.dupmarked_native.bam -- --spark-runner LOCAL --spark-master local[8]; Using GATK wrapper script ${GATK_DIR}/gatk/build/install/gatk/bin/gatk; Running:; ${GATK_DIR}/gatk/build/install/gatk/bin/gatk MarkDuplicatesSpark -I CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam -O CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.dupmarked_native.bam --spark-master local[8]; 21:47:47.494 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLB",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4746
https://github.com/broadinstitute/gatk/issues/4746:7985,Availability,down,down,7985," MarkDuplicatesSpark - ------------------------------------------------------------; 21:47:48.271 INFO MarkDuplicatesSpark - ------------------------------------------------------------; 21:47:48.271 INFO MarkDuplicatesSpark - HTSJDK Version: 2.14.3; 21:47:48.271 INFO MarkDuplicatesSpark - Picard Version: 2.18.2; 21:47:48.271 INFO MarkDuplicatesSpark - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 21:47:48.271 INFO MarkDuplicatesSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 21:47:48.272 INFO MarkDuplicatesSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 21:47:48.272 INFO MarkDuplicatesSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 21:47:48.272 INFO MarkDuplicatesSpark - Deflater: IntelDeflater; 21:47:48.272 INFO MarkDuplicatesSpark - Inflater: IntelInflater; 21:47:48.272 INFO MarkDuplicatesSpark - GCS max retries/reopens: 20; 21:47:48.272 INFO MarkDuplicatesSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 21:47:48.272 WARN MarkDuplicatesSpark -. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: MarkDuplicatesSpark is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 21:47:48.273 INFO MarkDuplicatesSpark - Initializing engine; 21:47:48.273 INFO MarkDuplicatesSpark - Done initializing engine; 22:29:27.746 INFO ReadsSparkSink - Finished sorting the bam file and dumping read shards to disk, proceeding to merge the shards into a single file using the master thread; 22:43:29.758 INFO ReadsSparkSink - Finished merging shards into a single output bam; 22:43:36.475 INFO MarkDuplicatesSpark - Shutting down engine; [May 7, 2018 10:43:36 PM EDT] org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark done. Elapsed time: 55.82 minutes.; Runtime.totalMemory()=12430868480; ```. Created after discussion with @lbergelson",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4746
https://github.com/broadinstitute/gatk/issues/4746:1109,Deployability,install,install,1109,"Spark local mode. In private Spark tools under development (which travers a WGS BAM and then performing several shuffles) I have seen speedups of up to 40% (~ 46 minutes -> 26 minutes). An initial test of `MarkDuplicatesSpark` using a 30GB bam file gave me a 9% speedup (logs are below). It might be good to investigate making this easier for users (I downloaded Hadoop and built it from source, and then set gatk's java opts to load the native library). Two options might be: 1) distribute native libraries for supported architectures with gatk or 2) make sure gatk docker images include the native libraries and are set to use them. Logs for `MarkDuplicatesSpark` without and with native libraries, running on a Broad login server:. Without:. ```; $ ${GATK_DIR}/gatk MarkDuplicatesSpark -I CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam -O CEUTrio.HiSeq.WEx37; .NA12892.readnamesort.dupmarked.bam -- --spark-runner LOCAL --spark-master local[8]; Using GATK wrapper script ${GATK_DIR}/gatk/build/install/gatk/bin/gatk; Running:; ${GATK_DIR}/gatk/build/install/gatk/bin/gatk MarkDuplicatesSpark -I CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam -O CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.dupmarked.bam --spark; -master local[8]; 14:40:21.800 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 14:40:21.889 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:${GATK_DIR}/gatk/build/install/gatk/lib/gkl-0.8.5.jar!/com/intel/gkl/native/libgkl_compression.so; 14:40:21.989 INFO MarkDuplicatesSpark - ------------------------------------------------------------; 14:40:21.990 INFO MarkDuplicatesSpark - The Genome Analysis Toolkit (GATK) v4.0.4.0-7-g46a8661-SNAPSHOT; 14:40:21.990 INFO MarkDuplicatesSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:40:21.991 INFO MarkDuplicatesSpark - Executing as cwhelan@",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4746
https://github.com/broadinstitute/gatk/issues/4746:1165,Deployability,install,install,1165,"Spark local mode. In private Spark tools under development (which travers a WGS BAM and then performing several shuffles) I have seen speedups of up to 40% (~ 46 minutes -> 26 minutes). An initial test of `MarkDuplicatesSpark` using a 30GB bam file gave me a 9% speedup (logs are below). It might be good to investigate making this easier for users (I downloaded Hadoop and built it from source, and then set gatk's java opts to load the native library). Two options might be: 1) distribute native libraries for supported architectures with gatk or 2) make sure gatk docker images include the native libraries and are set to use them. Logs for `MarkDuplicatesSpark` without and with native libraries, running on a Broad login server:. Without:. ```; $ ${GATK_DIR}/gatk MarkDuplicatesSpark -I CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam -O CEUTrio.HiSeq.WEx37; .NA12892.readnamesort.dupmarked.bam -- --spark-runner LOCAL --spark-master local[8]; Using GATK wrapper script ${GATK_DIR}/gatk/build/install/gatk/bin/gatk; Running:; ${GATK_DIR}/gatk/build/install/gatk/bin/gatk MarkDuplicatesSpark -I CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam -O CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.dupmarked.bam --spark; -master local[8]; 14:40:21.800 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 14:40:21.889 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:${GATK_DIR}/gatk/build/install/gatk/lib/gkl-0.8.5.jar!/com/intel/gkl/native/libgkl_compression.so; 14:40:21.989 INFO MarkDuplicatesSpark - ------------------------------------------------------------; 14:40:21.990 INFO MarkDuplicatesSpark - The Genome Analysis Toolkit (GATK) v4.0.4.0-7-g46a8661-SNAPSHOT; 14:40:21.990 INFO MarkDuplicatesSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:40:21.991 INFO MarkDuplicatesSpark - Executing as cwhelan@",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4746
https://github.com/broadinstitute/gatk/issues/4746:1650,Deployability,install,install,1650,"ctures with gatk or 2) make sure gatk docker images include the native libraries and are set to use them. Logs for `MarkDuplicatesSpark` without and with native libraries, running on a Broad login server:. Without:. ```; $ ${GATK_DIR}/gatk MarkDuplicatesSpark -I CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam -O CEUTrio.HiSeq.WEx37; .NA12892.readnamesort.dupmarked.bam -- --spark-runner LOCAL --spark-master local[8]; Using GATK wrapper script ${GATK_DIR}/gatk/build/install/gatk/bin/gatk; Running:; ${GATK_DIR}/gatk/build/install/gatk/bin/gatk MarkDuplicatesSpark -I CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam -O CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.dupmarked.bam --spark; -master local[8]; 14:40:21.800 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 14:40:21.889 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:${GATK_DIR}/gatk/build/install/gatk/lib/gkl-0.8.5.jar!/com/intel/gkl/native/libgkl_compression.so; 14:40:21.989 INFO MarkDuplicatesSpark - ------------------------------------------------------------; 14:40:21.990 INFO MarkDuplicatesSpark - The Genome Analysis Toolkit (GATK) v4.0.4.0-7-g46a8661-SNAPSHOT; 14:40:21.990 INFO MarkDuplicatesSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:40:21.991 INFO MarkDuplicatesSpark - Executing as cwhelan@gsa6.broadinstitute.org on Linux v2.6.32-696.16.1.el6.x86_64 amd64; 14:40:21.991 INFO MarkDuplicatesSpark - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 14:40:21.992 INFO MarkDuplicatesSpark - Start Date/Time: May 7, 2018 2:40:21 PM EDT; 14:40:21.992 INFO MarkDuplicatesSpark - ------------------------------------------------------------; 14:40:21.992 INFO MarkDuplicatesSpark - ------------------------------------------------------------; 14:40:21.992 INFO MarkDuplicatesSpark - HTSJDK Version: 2.14.3; 14:4",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4746
https://github.com/broadinstitute/gatk/issues/4746:3332,Deployability,patch,patch,3332,"me: May 7, 2018 2:40:21 PM EDT; 14:40:21.992 INFO MarkDuplicatesSpark - ------------------------------------------------------------; 14:40:21.992 INFO MarkDuplicatesSpark - ------------------------------------------------------------; 14:40:21.992 INFO MarkDuplicatesSpark - HTSJDK Version: 2.14.3; 14:40:21.992 INFO MarkDuplicatesSpark - Picard Version: 2.18.2; 14:40:21.993 INFO MarkDuplicatesSpark - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 14:40:21.993 INFO MarkDuplicatesSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 14:40:21.993 INFO MarkDuplicatesSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 14:40:21.993 INFO MarkDuplicatesSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 14:40:21.993 INFO MarkDuplicatesSpark - Deflater: IntelDeflater; 14:40:21.993 INFO MarkDuplicatesSpark - Inflater: IntelInflater; 14:40:21.993 INFO MarkDuplicatesSpark - GCS max retries/reopens: 20; 14:40:21.993 INFO MarkDuplicatesSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 14:40:21.994 WARN MarkDuplicatesSpark -. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: MarkDuplicatesSpark is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 14:40:21.994 INFO MarkDuplicatesSpark - Initializing engine; 14:40:21.994 INFO MarkDuplicatesSpark - Done initializing engine; 14:40:22.338 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 15:24:12.735 INFO ReadsSparkSink - Finished sorting the bam file and dumping read shards to disk, proceeding to merge the shards into a single file using the master thread; 15:41:27.766 INFO ReadsSparkSink - Finished merging shards into a single output bam; 15:41:34.351 INFO MarkDuplicatesSpark - Shutting down engine; [May 7, 2018 3:41:34 PM EDT] org.broadinsti",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4746
https://github.com/broadinstitute/gatk/issues/4746:4950,Deployability,install,install,4950,"ses where applicable; 15:24:12.735 INFO ReadsSparkSink - Finished sorting the bam file and dumping read shards to disk, proceeding to merge the shards into a single file using the master thread; 15:41:27.766 INFO ReadsSparkSink - Finished merging shards into a single output bam; 15:41:34.351 INFO MarkDuplicatesSpark - Shutting down engine; [May 7, 2018 3:41:34 PM EDT] org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark done. Elapsed time: 61.21 minutes.; Runtime.totalMemory()=13635682304; ```. With native libraries (note the lack of the usual warning):. ```; $ ${GATK_DIR}/gatk MarkDuplicatesSpark --java-options ""-Djava.library.path=${HADOOP_DIR}/hadoop-2.6.5-src/hadoop-common-project/hadoop-common/target/hadoop-common-2.6.5/lib/native"" -I CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam -O CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.dupmarked_native.bam -- --spark-runner LOCAL --spark-master local[8]; Using GATK wrapper script ${GATK_DIR}/gatk/build/install/gatk/bin/gatk; Running:; ${GATK_DIR}/gatk/build/install/gatk/bin/gatk MarkDuplicatesSpark -I CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam -O CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.dupmarked_native.bam --spark-master local[8]; 21:47:47.494 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 21:47:47.827 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:${GATK_DIR}/gatk/build/install/gatk/lib/gkl-0.8.5.jar!/com/intel/gkl/native/libgkl_compression.so; 21:47:48.268 INFO MarkDuplicatesSpark - ------------------------------------------------------------; 21:47:48.268 INFO MarkDuplicatesSpark - The Genome Analysis Toolkit (GATK) v4.0.4.0-7-g46a8661-SNAPSHOT; 21:47:48.268 INFO MarkDuplicatesSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 21:47:48.270 INFO MarkDuplicatesSpark - Executing as cwh",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4746
https://github.com/broadinstitute/gatk/issues/4746:5006,Deployability,install,install,5006,"ses where applicable; 15:24:12.735 INFO ReadsSparkSink - Finished sorting the bam file and dumping read shards to disk, proceeding to merge the shards into a single file using the master thread; 15:41:27.766 INFO ReadsSparkSink - Finished merging shards into a single output bam; 15:41:34.351 INFO MarkDuplicatesSpark - Shutting down engine; [May 7, 2018 3:41:34 PM EDT] org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark done. Elapsed time: 61.21 minutes.; Runtime.totalMemory()=13635682304; ```. With native libraries (note the lack of the usual warning):. ```; $ ${GATK_DIR}/gatk MarkDuplicatesSpark --java-options ""-Djava.library.path=${HADOOP_DIR}/hadoop-2.6.5-src/hadoop-common-project/hadoop-common/target/hadoop-common-2.6.5/lib/native"" -I CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam -O CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.dupmarked_native.bam -- --spark-runner LOCAL --spark-master local[8]; Using GATK wrapper script ${GATK_DIR}/gatk/build/install/gatk/bin/gatk; Running:; ${GATK_DIR}/gatk/build/install/gatk/bin/gatk MarkDuplicatesSpark -I CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam -O CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.dupmarked_native.bam --spark-master local[8]; 21:47:47.494 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 21:47:47.827 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:${GATK_DIR}/gatk/build/install/gatk/lib/gkl-0.8.5.jar!/com/intel/gkl/native/libgkl_compression.so; 21:47:48.268 INFO MarkDuplicatesSpark - ------------------------------------------------------------; 21:47:48.268 INFO MarkDuplicatesSpark - The Genome Analysis Toolkit (GATK) v4.0.4.0-7-g46a8661-SNAPSHOT; 21:47:48.268 INFO MarkDuplicatesSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 21:47:48.270 INFO MarkDuplicatesSpark - Executing as cwh",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4746
https://github.com/broadinstitute/gatk/issues/4746:5496,Deployability,install,install,5496," With native libraries (note the lack of the usual warning):. ```; $ ${GATK_DIR}/gatk MarkDuplicatesSpark --java-options ""-Djava.library.path=${HADOOP_DIR}/hadoop-2.6.5-src/hadoop-common-project/hadoop-common/target/hadoop-common-2.6.5/lib/native"" -I CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam -O CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.dupmarked_native.bam -- --spark-runner LOCAL --spark-master local[8]; Using GATK wrapper script ${GATK_DIR}/gatk/build/install/gatk/bin/gatk; Running:; ${GATK_DIR}/gatk/build/install/gatk/bin/gatk MarkDuplicatesSpark -I CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam -O CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.dupmarked_native.bam --spark-master local[8]; 21:47:47.494 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 21:47:47.827 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:${GATK_DIR}/gatk/build/install/gatk/lib/gkl-0.8.5.jar!/com/intel/gkl/native/libgkl_compression.so; 21:47:48.268 INFO MarkDuplicatesSpark - ------------------------------------------------------------; 21:47:48.268 INFO MarkDuplicatesSpark - The Genome Analysis Toolkit (GATK) v4.0.4.0-7-g46a8661-SNAPSHOT; 21:47:48.268 INFO MarkDuplicatesSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 21:47:48.270 INFO MarkDuplicatesSpark - Executing as cwhelan@gsa6.broadinstitute.org on Linux v2.6.32-696.16.1.el6.x86_64 amd64; 21:47:48.270 INFO MarkDuplicatesSpark - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 21:47:48.270 INFO MarkDuplicatesSpark - Start Date/Time: May 7, 2018 9:47:47 PM EDT; 21:47:48.270 INFO MarkDuplicatesSpark - ------------------------------------------------------------; 21:47:48.271 INFO MarkDuplicatesSpark - ------------------------------------------------------------; 21:47:48.271 INFO MarkDuplicatesSpark - HTSJDK Version: 2.14.3; 21:4",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4746
https://github.com/broadinstitute/gatk/issues/4746:7178,Deployability,patch,patch,7178,"me: May 7, 2018 9:47:47 PM EDT; 21:47:48.270 INFO MarkDuplicatesSpark - ------------------------------------------------------------; 21:47:48.271 INFO MarkDuplicatesSpark - ------------------------------------------------------------; 21:47:48.271 INFO MarkDuplicatesSpark - HTSJDK Version: 2.14.3; 21:47:48.271 INFO MarkDuplicatesSpark - Picard Version: 2.18.2; 21:47:48.271 INFO MarkDuplicatesSpark - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 21:47:48.271 INFO MarkDuplicatesSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 21:47:48.272 INFO MarkDuplicatesSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 21:47:48.272 INFO MarkDuplicatesSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 21:47:48.272 INFO MarkDuplicatesSpark - Deflater: IntelDeflater; 21:47:48.272 INFO MarkDuplicatesSpark - Inflater: IntelInflater; 21:47:48.272 INFO MarkDuplicatesSpark - GCS max retries/reopens: 20; 21:47:48.272 INFO MarkDuplicatesSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 21:47:48.272 WARN MarkDuplicatesSpark -. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: MarkDuplicatesSpark is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 21:47:48.273 INFO MarkDuplicatesSpark - Initializing engine; 21:47:48.273 INFO MarkDuplicatesSpark - Done initializing engine; 22:29:27.746 INFO ReadsSparkSink - Finished sorting the bam file and dumping read shards to disk, proceeding to merge the shards into a single file using the master thread; 22:43:29.758 INFO ReadsSparkSink - Finished merging shards into a single output bam; 22:43:36.475 INFO MarkDuplicatesSpark - Shutting down engine; [May 7, 2018 10:43:36 PM EDT] org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark done. Elapsed time: 55.82 minutes.; Runtime.totalMemory()=1243086848",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4746
https://github.com/broadinstitute/gatk/issues/4746:1071,Integrability,wrap,wrapper,1071,"Spark local mode. In private Spark tools under development (which travers a WGS BAM and then performing several shuffles) I have seen speedups of up to 40% (~ 46 minutes -> 26 minutes). An initial test of `MarkDuplicatesSpark` using a 30GB bam file gave me a 9% speedup (logs are below). It might be good to investigate making this easier for users (I downloaded Hadoop and built it from source, and then set gatk's java opts to load the native library). Two options might be: 1) distribute native libraries for supported architectures with gatk or 2) make sure gatk docker images include the native libraries and are set to use them. Logs for `MarkDuplicatesSpark` without and with native libraries, running on a Broad login server:. Without:. ```; $ ${GATK_DIR}/gatk MarkDuplicatesSpark -I CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam -O CEUTrio.HiSeq.WEx37; .NA12892.readnamesort.dupmarked.bam -- --spark-runner LOCAL --spark-master local[8]; Using GATK wrapper script ${GATK_DIR}/gatk/build/install/gatk/bin/gatk; Running:; ${GATK_DIR}/gatk/build/install/gatk/bin/gatk MarkDuplicatesSpark -I CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam -O CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.dupmarked.bam --spark; -master local[8]; 14:40:21.800 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 14:40:21.889 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:${GATK_DIR}/gatk/build/install/gatk/lib/gkl-0.8.5.jar!/com/intel/gkl/native/libgkl_compression.so; 14:40:21.989 INFO MarkDuplicatesSpark - ------------------------------------------------------------; 14:40:21.990 INFO MarkDuplicatesSpark - The Genome Analysis Toolkit (GATK) v4.0.4.0-7-g46a8661-SNAPSHOT; 14:40:21.990 INFO MarkDuplicatesSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:40:21.991 INFO MarkDuplicatesSpark - Executing as cwhelan@",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4746
https://github.com/broadinstitute/gatk/issues/4746:4912,Integrability,wrap,wrapper,4912,"ses where applicable; 15:24:12.735 INFO ReadsSparkSink - Finished sorting the bam file and dumping read shards to disk, proceeding to merge the shards into a single file using the master thread; 15:41:27.766 INFO ReadsSparkSink - Finished merging shards into a single output bam; 15:41:34.351 INFO MarkDuplicatesSpark - Shutting down engine; [May 7, 2018 3:41:34 PM EDT] org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark done. Elapsed time: 61.21 minutes.; Runtime.totalMemory()=13635682304; ```. With native libraries (note the lack of the usual warning):. ```; $ ${GATK_DIR}/gatk MarkDuplicatesSpark --java-options ""-Djava.library.path=${HADOOP_DIR}/hadoop-2.6.5-src/hadoop-common-project/hadoop-common/target/hadoop-common-2.6.5/lib/native"" -I CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam -O CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.dupmarked_native.bam -- --spark-runner LOCAL --spark-master local[8]; Using GATK wrapper script ${GATK_DIR}/gatk/build/install/gatk/bin/gatk; Running:; ${GATK_DIR}/gatk/build/install/gatk/bin/gatk MarkDuplicatesSpark -I CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam -O CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.dupmarked_native.bam --spark-master local[8]; 21:47:47.494 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 21:47:47.827 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:${GATK_DIR}/gatk/build/install/gatk/lib/gkl-0.8.5.jar!/com/intel/gkl/native/libgkl_compression.so; 21:47:48.268 INFO MarkDuplicatesSpark - ------------------------------------------------------------; 21:47:48.268 INFO MarkDuplicatesSpark - The Genome Analysis Toolkit (GATK) v4.0.4.0-7-g46a8661-SNAPSHOT; 21:47:48.268 INFO MarkDuplicatesSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 21:47:48.270 INFO MarkDuplicatesSpark - Executing as cwh",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4746
https://github.com/broadinstitute/gatk/issues/4746:1396,Modifiability,variab,variables,1396,"or users (I downloaded Hadoop and built it from source, and then set gatk's java opts to load the native library). Two options might be: 1) distribute native libraries for supported architectures with gatk or 2) make sure gatk docker images include the native libraries and are set to use them. Logs for `MarkDuplicatesSpark` without and with native libraries, running on a Broad login server:. Without:. ```; $ ${GATK_DIR}/gatk MarkDuplicatesSpark -I CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam -O CEUTrio.HiSeq.WEx37; .NA12892.readnamesort.dupmarked.bam -- --spark-runner LOCAL --spark-master local[8]; Using GATK wrapper script ${GATK_DIR}/gatk/build/install/gatk/bin/gatk; Running:; ${GATK_DIR}/gatk/build/install/gatk/bin/gatk MarkDuplicatesSpark -I CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam -O CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.dupmarked.bam --spark; -master local[8]; 14:40:21.800 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 14:40:21.889 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:${GATK_DIR}/gatk/build/install/gatk/lib/gkl-0.8.5.jar!/com/intel/gkl/native/libgkl_compression.so; 14:40:21.989 INFO MarkDuplicatesSpark - ------------------------------------------------------------; 14:40:21.990 INFO MarkDuplicatesSpark - The Genome Analysis Toolkit (GATK) v4.0.4.0-7-g46a8661-SNAPSHOT; 14:40:21.990 INFO MarkDuplicatesSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:40:21.991 INFO MarkDuplicatesSpark - Executing as cwhelan@gsa6.broadinstitute.org on Linux v2.6.32-696.16.1.el6.x86_64 amd64; 14:40:21.991 INFO MarkDuplicatesSpark - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 14:40:21.992 INFO MarkDuplicatesSpark - Start Date/Time: May 7, 2018 2:40:21 PM EDT; 14:40:21.992 INFO MarkDuplicatesSpark - -------------------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4746
https://github.com/broadinstitute/gatk/issues/4746:1522,Modifiability,config,configured,1522,"or users (I downloaded Hadoop and built it from source, and then set gatk's java opts to load the native library). Two options might be: 1) distribute native libraries for supported architectures with gatk or 2) make sure gatk docker images include the native libraries and are set to use them. Logs for `MarkDuplicatesSpark` without and with native libraries, running on a Broad login server:. Without:. ```; $ ${GATK_DIR}/gatk MarkDuplicatesSpark -I CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam -O CEUTrio.HiSeq.WEx37; .NA12892.readnamesort.dupmarked.bam -- --spark-runner LOCAL --spark-master local[8]; Using GATK wrapper script ${GATK_DIR}/gatk/build/install/gatk/bin/gatk; Running:; ${GATK_DIR}/gatk/build/install/gatk/bin/gatk MarkDuplicatesSpark -I CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam -O CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.dupmarked.bam --spark; -master local[8]; 14:40:21.800 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 14:40:21.889 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:${GATK_DIR}/gatk/build/install/gatk/lib/gkl-0.8.5.jar!/com/intel/gkl/native/libgkl_compression.so; 14:40:21.989 INFO MarkDuplicatesSpark - ------------------------------------------------------------; 14:40:21.990 INFO MarkDuplicatesSpark - The Genome Analysis Toolkit (GATK) v4.0.4.0-7-g46a8661-SNAPSHOT; 14:40:21.990 INFO MarkDuplicatesSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:40:21.991 INFO MarkDuplicatesSpark - Executing as cwhelan@gsa6.broadinstitute.org on Linux v2.6.32-696.16.1.el6.x86_64 amd64; 14:40:21.991 INFO MarkDuplicatesSpark - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 14:40:21.992 INFO MarkDuplicatesSpark - Start Date/Time: May 7, 2018 2:40:21 PM EDT; 14:40:21.992 INFO MarkDuplicatesSpark - -------------------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4746
https://github.com/broadinstitute/gatk/issues/4746:5242,Modifiability,variab,variables,5242,"y 7, 2018 3:41:34 PM EDT] org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark done. Elapsed time: 61.21 minutes.; Runtime.totalMemory()=13635682304; ```. With native libraries (note the lack of the usual warning):. ```; $ ${GATK_DIR}/gatk MarkDuplicatesSpark --java-options ""-Djava.library.path=${HADOOP_DIR}/hadoop-2.6.5-src/hadoop-common-project/hadoop-common/target/hadoop-common-2.6.5/lib/native"" -I CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam -O CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.dupmarked_native.bam -- --spark-runner LOCAL --spark-master local[8]; Using GATK wrapper script ${GATK_DIR}/gatk/build/install/gatk/bin/gatk; Running:; ${GATK_DIR}/gatk/build/install/gatk/bin/gatk MarkDuplicatesSpark -I CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam -O CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.dupmarked_native.bam --spark-master local[8]; 21:47:47.494 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 21:47:47.827 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:${GATK_DIR}/gatk/build/install/gatk/lib/gkl-0.8.5.jar!/com/intel/gkl/native/libgkl_compression.so; 21:47:48.268 INFO MarkDuplicatesSpark - ------------------------------------------------------------; 21:47:48.268 INFO MarkDuplicatesSpark - The Genome Analysis Toolkit (GATK) v4.0.4.0-7-g46a8661-SNAPSHOT; 21:47:48.268 INFO MarkDuplicatesSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 21:47:48.270 INFO MarkDuplicatesSpark - Executing as cwhelan@gsa6.broadinstitute.org on Linux v2.6.32-696.16.1.el6.x86_64 amd64; 21:47:48.270 INFO MarkDuplicatesSpark - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 21:47:48.270 INFO MarkDuplicatesSpark - Start Date/Time: May 7, 2018 9:47:47 PM EDT; 21:47:48.270 INFO MarkDuplicatesSpark - -------------------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4746
https://github.com/broadinstitute/gatk/issues/4746:5368,Modifiability,config,configured,5368,"y 7, 2018 3:41:34 PM EDT] org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark done. Elapsed time: 61.21 minutes.; Runtime.totalMemory()=13635682304; ```. With native libraries (note the lack of the usual warning):. ```; $ ${GATK_DIR}/gatk MarkDuplicatesSpark --java-options ""-Djava.library.path=${HADOOP_DIR}/hadoop-2.6.5-src/hadoop-common-project/hadoop-common/target/hadoop-common-2.6.5/lib/native"" -I CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam -O CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.dupmarked_native.bam -- --spark-runner LOCAL --spark-master local[8]; Using GATK wrapper script ${GATK_DIR}/gatk/build/install/gatk/bin/gatk; Running:; ${GATK_DIR}/gatk/build/install/gatk/bin/gatk MarkDuplicatesSpark -I CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam -O CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.dupmarked_native.bam --spark-master local[8]; 21:47:47.494 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 21:47:47.827 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:${GATK_DIR}/gatk/build/install/gatk/lib/gkl-0.8.5.jar!/com/intel/gkl/native/libgkl_compression.so; 21:47:48.268 INFO MarkDuplicatesSpark - ------------------------------------------------------------; 21:47:48.268 INFO MarkDuplicatesSpark - The Genome Analysis Toolkit (GATK) v4.0.4.0-7-g46a8661-SNAPSHOT; 21:47:48.268 INFO MarkDuplicatesSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 21:47:48.270 INFO MarkDuplicatesSpark - Executing as cwhelan@gsa6.broadinstitute.org on Linux v2.6.32-696.16.1.el6.x86_64 amd64; 21:47:48.270 INFO MarkDuplicatesSpark - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 21:47:48.270 INFO MarkDuplicatesSpark - Start Date/Time: May 7, 2018 9:47:47 PM EDT; 21:47:48.270 INFO MarkDuplicatesSpark - -------------------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4746
https://github.com/broadinstitute/gatk/issues/4746:205,Performance,perform,performing,205,"In some preliminary testing I've done it looks like using native Hadoop libraries can speed up tools running in Spark local mode. In private Spark tools under development (which travers a WGS BAM and then performing several shuffles) I have seen speedups of up to 40% (~ 46 minutes -> 26 minutes). An initial test of `MarkDuplicatesSpark` using a 30GB bam file gave me a 9% speedup (logs are below). It might be good to investigate making this easier for users (I downloaded Hadoop and built it from source, and then set gatk's java opts to load the native library). Two options might be: 1) distribute native libraries for supported architectures with gatk or 2) make sure gatk docker images include the native libraries and are set to use them. Logs for `MarkDuplicatesSpark` without and with native libraries, running on a Broad login server:. Without:. ```; $ ${GATK_DIR}/gatk MarkDuplicatesSpark -I CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam -O CEUTrio.HiSeq.WEx37; .NA12892.readnamesort.dupmarked.bam -- --spark-runner LOCAL --spark-master local[8]; Using GATK wrapper script ${GATK_DIR}/gatk/build/install/gatk/bin/gatk; Running:; ${GATK_DIR}/gatk/build/install/gatk/bin/gatk MarkDuplicatesSpark -I CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam -O CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.dupmarked.bam --spark; -master local[8]; 14:40:21.800 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 14:40:21.889 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:${GATK_DIR}/gatk/build/install/gatk/lib/gkl-0.8.5.jar!/com/intel/gkl/native/libgkl_compression.so; 14:40:21.989 INFO MarkDuplicatesSpark - ------------------------------------------------------------; 14:40:21.990 INFO MarkDuplicatesSpark - The Genome Analysis Toolkit (GATK) v4.0.4.0-7-g46a8661-SNAPSHOT; 14:40:21.990 INFO MarkDuplicatesSpark - For support and documentatio",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4746
https://github.com/broadinstitute/gatk/issues/4746:541,Performance,load,load,541,"In some preliminary testing I've done it looks like using native Hadoop libraries can speed up tools running in Spark local mode. In private Spark tools under development (which travers a WGS BAM and then performing several shuffles) I have seen speedups of up to 40% (~ 46 minutes -> 26 minutes). An initial test of `MarkDuplicatesSpark` using a 30GB bam file gave me a 9% speedup (logs are below). It might be good to investigate making this easier for users (I downloaded Hadoop and built it from source, and then set gatk's java opts to load the native library). Two options might be: 1) distribute native libraries for supported architectures with gatk or 2) make sure gatk docker images include the native libraries and are set to use them. Logs for `MarkDuplicatesSpark` without and with native libraries, running on a Broad login server:. Without:. ```; $ ${GATK_DIR}/gatk MarkDuplicatesSpark -I CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam -O CEUTrio.HiSeq.WEx37; .NA12892.readnamesort.dupmarked.bam -- --spark-runner LOCAL --spark-master local[8]; Using GATK wrapper script ${GATK_DIR}/gatk/build/install/gatk/bin/gatk; Running:; ${GATK_DIR}/gatk/build/install/gatk/bin/gatk MarkDuplicatesSpark -I CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam -O CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.dupmarked.bam --spark; -master local[8]; 14:40:21.800 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 14:40:21.889 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:${GATK_DIR}/gatk/build/install/gatk/lib/gkl-0.8.5.jar!/com/intel/gkl/native/libgkl_compression.so; 14:40:21.989 INFO MarkDuplicatesSpark - ------------------------------------------------------------; 14:40:21.990 INFO MarkDuplicatesSpark - The Genome Analysis Toolkit (GATK) v4.0.4.0-7-g46a8661-SNAPSHOT; 14:40:21.990 INFO MarkDuplicatesSpark - For support and documentatio",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4746
https://github.com/broadinstitute/gatk/issues/4746:1583,Performance,Load,Loading,1583,"ght be: 1) distribute native libraries for supported architectures with gatk or 2) make sure gatk docker images include the native libraries and are set to use them. Logs for `MarkDuplicatesSpark` without and with native libraries, running on a Broad login server:. Without:. ```; $ ${GATK_DIR}/gatk MarkDuplicatesSpark -I CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam -O CEUTrio.HiSeq.WEx37; .NA12892.readnamesort.dupmarked.bam -- --spark-runner LOCAL --spark-master local[8]; Using GATK wrapper script ${GATK_DIR}/gatk/build/install/gatk/bin/gatk; Running:; ${GATK_DIR}/gatk/build/install/gatk/bin/gatk MarkDuplicatesSpark -I CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam -O CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.dupmarked.bam --spark; -master local[8]; 14:40:21.800 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 14:40:21.889 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:${GATK_DIR}/gatk/build/install/gatk/lib/gkl-0.8.5.jar!/com/intel/gkl/native/libgkl_compression.so; 14:40:21.989 INFO MarkDuplicatesSpark - ------------------------------------------------------------; 14:40:21.990 INFO MarkDuplicatesSpark - The Genome Analysis Toolkit (GATK) v4.0.4.0-7-g46a8661-SNAPSHOT; 14:40:21.990 INFO MarkDuplicatesSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:40:21.991 INFO MarkDuplicatesSpark - Executing as cwhelan@gsa6.broadinstitute.org on Linux v2.6.32-696.16.1.el6.x86_64 amd64; 14:40:21.991 INFO MarkDuplicatesSpark - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 14:40:21.992 INFO MarkDuplicatesSpark - Start Date/Time: May 7, 2018 2:40:21 PM EDT; 14:40:21.992 INFO MarkDuplicatesSpark - ------------------------------------------------------------; 14:40:21.992 INFO MarkDuplicatesSpark - ------------------------------------------------------------; 14:40:2",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4746
https://github.com/broadinstitute/gatk/issues/4746:3882,Performance,load,load,3882,"1.993 INFO MarkDuplicatesSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 14:40:21.993 INFO MarkDuplicatesSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 14:40:21.993 INFO MarkDuplicatesSpark - Deflater: IntelDeflater; 14:40:21.993 INFO MarkDuplicatesSpark - Inflater: IntelInflater; 14:40:21.993 INFO MarkDuplicatesSpark - GCS max retries/reopens: 20; 14:40:21.993 INFO MarkDuplicatesSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 14:40:21.994 WARN MarkDuplicatesSpark -. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: MarkDuplicatesSpark is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 14:40:21.994 INFO MarkDuplicatesSpark - Initializing engine; 14:40:21.994 INFO MarkDuplicatesSpark - Done initializing engine; 14:40:22.338 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 15:24:12.735 INFO ReadsSparkSink - Finished sorting the bam file and dumping read shards to disk, proceeding to merge the shards into a single file using the master thread; 15:41:27.766 INFO ReadsSparkSink - Finished merging shards into a single output bam; 15:41:34.351 INFO MarkDuplicatesSpark - Shutting down engine; [May 7, 2018 3:41:34 PM EDT] org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark done. Elapsed time: 61.21 minutes.; Runtime.totalMemory()=13635682304; ```. With native libraries (note the lack of the usual warning):. ```; $ ${GATK_DIR}/gatk MarkDuplicatesSpark --java-options ""-Djava.library.path=${HADOOP_DIR}/hadoop-2.6.5-src/hadoop-common-project/hadoop-common/target/hadoop-common-2.6.5/lib/native"" -I CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam -O CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.dupmarked_native.bam -- --spark-runner LOCAL --spark-",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4746
https://github.com/broadinstitute/gatk/issues/4746:5429,Performance,Load,Loading,5429,"ime: 61.21 minutes.; Runtime.totalMemory()=13635682304; ```. With native libraries (note the lack of the usual warning):. ```; $ ${GATK_DIR}/gatk MarkDuplicatesSpark --java-options ""-Djava.library.path=${HADOOP_DIR}/hadoop-2.6.5-src/hadoop-common-project/hadoop-common/target/hadoop-common-2.6.5/lib/native"" -I CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam -O CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.dupmarked_native.bam -- --spark-runner LOCAL --spark-master local[8]; Using GATK wrapper script ${GATK_DIR}/gatk/build/install/gatk/bin/gatk; Running:; ${GATK_DIR}/gatk/build/install/gatk/bin/gatk MarkDuplicatesSpark -I CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam -O CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.dupmarked_native.bam --spark-master local[8]; 21:47:47.494 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 21:47:47.827 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:${GATK_DIR}/gatk/build/install/gatk/lib/gkl-0.8.5.jar!/com/intel/gkl/native/libgkl_compression.so; 21:47:48.268 INFO MarkDuplicatesSpark - ------------------------------------------------------------; 21:47:48.268 INFO MarkDuplicatesSpark - The Genome Analysis Toolkit (GATK) v4.0.4.0-7-g46a8661-SNAPSHOT; 21:47:48.268 INFO MarkDuplicatesSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 21:47:48.270 INFO MarkDuplicatesSpark - Executing as cwhelan@gsa6.broadinstitute.org on Linux v2.6.32-696.16.1.el6.x86_64 amd64; 21:47:48.270 INFO MarkDuplicatesSpark - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 21:47:48.270 INFO MarkDuplicatesSpark - Start Date/Time: May 7, 2018 9:47:47 PM EDT; 21:47:48.270 INFO MarkDuplicatesSpark - ------------------------------------------------------------; 21:47:48.271 INFO MarkDuplicatesSpark - ------------------------------------------------------------; 21:47:4",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4746
https://github.com/broadinstitute/gatk/issues/4746:20,Testability,test,testing,20,"In some preliminary testing I've done it looks like using native Hadoop libraries can speed up tools running in Spark local mode. In private Spark tools under development (which travers a WGS BAM and then performing several shuffles) I have seen speedups of up to 40% (~ 46 minutes -> 26 minutes). An initial test of `MarkDuplicatesSpark` using a 30GB bam file gave me a 9% speedup (logs are below). It might be good to investigate making this easier for users (I downloaded Hadoop and built it from source, and then set gatk's java opts to load the native library). Two options might be: 1) distribute native libraries for supported architectures with gatk or 2) make sure gatk docker images include the native libraries and are set to use them. Logs for `MarkDuplicatesSpark` without and with native libraries, running on a Broad login server:. Without:. ```; $ ${GATK_DIR}/gatk MarkDuplicatesSpark -I CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam -O CEUTrio.HiSeq.WEx37; .NA12892.readnamesort.dupmarked.bam -- --spark-runner LOCAL --spark-master local[8]; Using GATK wrapper script ${GATK_DIR}/gatk/build/install/gatk/bin/gatk; Running:; ${GATK_DIR}/gatk/build/install/gatk/bin/gatk MarkDuplicatesSpark -I CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam -O CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.dupmarked.bam --spark; -master local[8]; 14:40:21.800 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 14:40:21.889 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:${GATK_DIR}/gatk/build/install/gatk/lib/gkl-0.8.5.jar!/com/intel/gkl/native/libgkl_compression.so; 14:40:21.989 INFO MarkDuplicatesSpark - ------------------------------------------------------------; 14:40:21.990 INFO MarkDuplicatesSpark - The Genome Analysis Toolkit (GATK) v4.0.4.0-7-g46a8661-SNAPSHOT; 14:40:21.990 INFO MarkDuplicatesSpark - For support and documentatio",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4746
https://github.com/broadinstitute/gatk/issues/4746:309,Testability,test,test,309,"In some preliminary testing I've done it looks like using native Hadoop libraries can speed up tools running in Spark local mode. In private Spark tools under development (which travers a WGS BAM and then performing several shuffles) I have seen speedups of up to 40% (~ 46 minutes -> 26 minutes). An initial test of `MarkDuplicatesSpark` using a 30GB bam file gave me a 9% speedup (logs are below). It might be good to investigate making this easier for users (I downloaded Hadoop and built it from source, and then set gatk's java opts to load the native library). Two options might be: 1) distribute native libraries for supported architectures with gatk or 2) make sure gatk docker images include the native libraries and are set to use them. Logs for `MarkDuplicatesSpark` without and with native libraries, running on a Broad login server:. Without:. ```; $ ${GATK_DIR}/gatk MarkDuplicatesSpark -I CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam -O CEUTrio.HiSeq.WEx37; .NA12892.readnamesort.dupmarked.bam -- --spark-runner LOCAL --spark-master local[8]; Using GATK wrapper script ${GATK_DIR}/gatk/build/install/gatk/bin/gatk; Running:; ${GATK_DIR}/gatk/build/install/gatk/bin/gatk MarkDuplicatesSpark -I CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam -O CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.dupmarked.bam --spark; -master local[8]; 14:40:21.800 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 14:40:21.889 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:${GATK_DIR}/gatk/build/install/gatk/lib/gkl-0.8.5.jar!/com/intel/gkl/native/libgkl_compression.so; 14:40:21.989 INFO MarkDuplicatesSpark - ------------------------------------------------------------; 14:40:21.990 INFO MarkDuplicatesSpark - The Genome Analysis Toolkit (GATK) v4.0.4.0-7-g46a8661-SNAPSHOT; 14:40:21.990 INFO MarkDuplicatesSpark - For support and documentatio",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4746
https://github.com/broadinstitute/gatk/issues/4746:383,Testability,log,logs,383,"In some preliminary testing I've done it looks like using native Hadoop libraries can speed up tools running in Spark local mode. In private Spark tools under development (which travers a WGS BAM and then performing several shuffles) I have seen speedups of up to 40% (~ 46 minutes -> 26 minutes). An initial test of `MarkDuplicatesSpark` using a 30GB bam file gave me a 9% speedup (logs are below). It might be good to investigate making this easier for users (I downloaded Hadoop and built it from source, and then set gatk's java opts to load the native library). Two options might be: 1) distribute native libraries for supported architectures with gatk or 2) make sure gatk docker images include the native libraries and are set to use them. Logs for `MarkDuplicatesSpark` without and with native libraries, running on a Broad login server:. Without:. ```; $ ${GATK_DIR}/gatk MarkDuplicatesSpark -I CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam -O CEUTrio.HiSeq.WEx37; .NA12892.readnamesort.dupmarked.bam -- --spark-runner LOCAL --spark-master local[8]; Using GATK wrapper script ${GATK_DIR}/gatk/build/install/gatk/bin/gatk; Running:; ${GATK_DIR}/gatk/build/install/gatk/bin/gatk MarkDuplicatesSpark -I CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam -O CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.dupmarked.bam --spark; -master local[8]; 14:40:21.800 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 14:40:21.889 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:${GATK_DIR}/gatk/build/install/gatk/lib/gkl-0.8.5.jar!/com/intel/gkl/native/libgkl_compression.so; 14:40:21.989 INFO MarkDuplicatesSpark - ------------------------------------------------------------; 14:40:21.990 INFO MarkDuplicatesSpark - The Genome Analysis Toolkit (GATK) v4.0.4.0-7-g46a8661-SNAPSHOT; 14:40:21.990 INFO MarkDuplicatesSpark - For support and documentatio",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4746
https://github.com/broadinstitute/gatk/issues/4746:747,Testability,Log,Logs,747,"In some preliminary testing I've done it looks like using native Hadoop libraries can speed up tools running in Spark local mode. In private Spark tools under development (which travers a WGS BAM and then performing several shuffles) I have seen speedups of up to 40% (~ 46 minutes -> 26 minutes). An initial test of `MarkDuplicatesSpark` using a 30GB bam file gave me a 9% speedup (logs are below). It might be good to investigate making this easier for users (I downloaded Hadoop and built it from source, and then set gatk's java opts to load the native library). Two options might be: 1) distribute native libraries for supported architectures with gatk or 2) make sure gatk docker images include the native libraries and are set to use them. Logs for `MarkDuplicatesSpark` without and with native libraries, running on a Broad login server:. Without:. ```; $ ${GATK_DIR}/gatk MarkDuplicatesSpark -I CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam -O CEUTrio.HiSeq.WEx37; .NA12892.readnamesort.dupmarked.bam -- --spark-runner LOCAL --spark-master local[8]; Using GATK wrapper script ${GATK_DIR}/gatk/build/install/gatk/bin/gatk; Running:; ${GATK_DIR}/gatk/build/install/gatk/bin/gatk MarkDuplicatesSpark -I CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam -O CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.dupmarked.bam --spark; -master local[8]; 14:40:21.800 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 14:40:21.889 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:${GATK_DIR}/gatk/build/install/gatk/lib/gkl-0.8.5.jar!/com/intel/gkl/native/libgkl_compression.so; 14:40:21.989 INFO MarkDuplicatesSpark - ------------------------------------------------------------; 14:40:21.990 INFO MarkDuplicatesSpark - The Genome Analysis Toolkit (GATK) v4.0.4.0-7-g46a8661-SNAPSHOT; 14:40:21.990 INFO MarkDuplicatesSpark - For support and documentatio",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4746
https://github.com/broadinstitute/gatk/issues/4746:832,Testability,log,login,832,"In some preliminary testing I've done it looks like using native Hadoop libraries can speed up tools running in Spark local mode. In private Spark tools under development (which travers a WGS BAM and then performing several shuffles) I have seen speedups of up to 40% (~ 46 minutes -> 26 minutes). An initial test of `MarkDuplicatesSpark` using a 30GB bam file gave me a 9% speedup (logs are below). It might be good to investigate making this easier for users (I downloaded Hadoop and built it from source, and then set gatk's java opts to load the native library). Two options might be: 1) distribute native libraries for supported architectures with gatk or 2) make sure gatk docker images include the native libraries and are set to use them. Logs for `MarkDuplicatesSpark` without and with native libraries, running on a Broad login server:. Without:. ```; $ ${GATK_DIR}/gatk MarkDuplicatesSpark -I CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam -O CEUTrio.HiSeq.WEx37; .NA12892.readnamesort.dupmarked.bam -- --spark-runner LOCAL --spark-master local[8]; Using GATK wrapper script ${GATK_DIR}/gatk/build/install/gatk/bin/gatk; Running:; ${GATK_DIR}/gatk/build/install/gatk/bin/gatk MarkDuplicatesSpark -I CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam -O CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.dupmarked.bam --spark; -master local[8]; 14:40:21.800 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 14:40:21.889 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:${GATK_DIR}/gatk/build/install/gatk/lib/gkl-0.8.5.jar!/com/intel/gkl/native/libgkl_compression.so; 14:40:21.989 INFO MarkDuplicatesSpark - ------------------------------------------------------------; 14:40:21.990 INFO MarkDuplicatesSpark - The Genome Analysis Toolkit (GATK) v4.0.4.0-7-g46a8661-SNAPSHOT; 14:40:21.990 INFO MarkDuplicatesSpark - For support and documentatio",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4746
https://github.com/broadinstitute/gatk/issues/4748:298,Integrability,message,messages,298,"I often find that when I get a stack trace it doesn't include sufficient information about the record that triggered it, or even where in the file it was. It would be possible to have GATKTools track their position and most recently `applied` records and then output that information either as log messages during a crash, or by wrapping incoming exceptions. . I think it would probably be useful, but it's also possible that we'd output irrelevant information when the record wasn't what caused the problem. Of course people could also write more complete exception messages, but it seems like the ones I hit never have quite what I want. What do people think about it?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4748
https://github.com/broadinstitute/gatk/issues/4748:329,Integrability,wrap,wrapping,329,"I often find that when I get a stack trace it doesn't include sufficient information about the record that triggered it, or even where in the file it was. It would be possible to have GATKTools track their position and most recently `applied` records and then output that information either as log messages during a crash, or by wrapping incoming exceptions. . I think it would probably be useful, but it's also possible that we'd output irrelevant information when the record wasn't what caused the problem. Of course people could also write more complete exception messages, but it seems like the ones I hit never have quite what I want. What do people think about it?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4748
https://github.com/broadinstitute/gatk/issues/4748:567,Integrability,message,messages,567,"I often find that when I get a stack trace it doesn't include sufficient information about the record that triggered it, or even where in the file it was. It would be possible to have GATKTools track their position and most recently `applied` records and then output that information either as log messages during a crash, or by wrapping incoming exceptions. . I think it would probably be useful, but it's also possible that we'd output irrelevant information when the record wasn't what caused the problem. Of course people could also write more complete exception messages, but it seems like the ones I hit never have quite what I want. What do people think about it?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4748
https://github.com/broadinstitute/gatk/issues/4748:294,Testability,log,log,294,"I often find that when I get a stack trace it doesn't include sufficient information about the record that triggered it, or even where in the file it was. It would be possible to have GATKTools track their position and most recently `applied` records and then output that information either as log messages during a crash, or by wrapping incoming exceptions. . I think it would probably be useful, but it's also possible that we'd output irrelevant information when the record wasn't what caused the problem. Of course people could also write more complete exception messages, but it seems like the ones I hit never have quite what I want. What do people think about it?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4748
https://github.com/broadinstitute/gatk/pull/4749:734,Deployability,update,update,734,"Several changes:. - Fix https://github.com/broadinstitute/gatk/issues/4741, where newer versions of conda appear to treat relative references in the environment yml as being relative to the yml file instead of relative to the cwd (based on observation).; - Add a second conda yml file (`gatkcondaenv.intel.yml`) for environments that use Intel hardware acceleration and Intel Tensorflow package (based on https://github.com/broadinstitute/gatk/pull/4735).; - Add a gradle task (`condaEnvironmentDefinition`) to generate the conda yml files from a single template to ensure that all the environment definitions remain in sync. This task also generates the Python package archive.; - Add a gradle task (`localDevCondaEnv`) to create or update a local (non-Intel) conda environment. This is a shortcut for use during development when you're iteratively changing/testing Python code and want to update the conda env.; - Opportunistically removed the prefix verb ""create"" from the name of the `createPythonPackageArchive` task, which is now called `pythonPackageArchive`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4749
https://github.com/broadinstitute/gatk/pull/4749:891,Deployability,update,update,891,"Several changes:. - Fix https://github.com/broadinstitute/gatk/issues/4741, where newer versions of conda appear to treat relative references in the environment yml as being relative to the yml file instead of relative to the cwd (based on observation).; - Add a second conda yml file (`gatkcondaenv.intel.yml`) for environments that use Intel hardware acceleration and Intel Tensorflow package (based on https://github.com/broadinstitute/gatk/pull/4735).; - Add a gradle task (`condaEnvironmentDefinition`) to generate the conda yml files from a single template to ensure that all the environment definitions remain in sync. This task also generates the Python package archive.; - Add a gradle task (`localDevCondaEnv`) to create or update a local (non-Intel) conda environment. This is a shortcut for use during development when you're iteratively changing/testing Python code and want to update the conda env.; - Opportunistically removed the prefix verb ""create"" from the name of the `createPythonPackageArchive` task, which is now called `pythonPackageArchive`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4749
https://github.com/broadinstitute/gatk/pull/4749:859,Testability,test,testing,859,"Several changes:. - Fix https://github.com/broadinstitute/gatk/issues/4741, where newer versions of conda appear to treat relative references in the environment yml as being relative to the yml file instead of relative to the cwd (based on observation).; - Add a second conda yml file (`gatkcondaenv.intel.yml`) for environments that use Intel hardware acceleration and Intel Tensorflow package (based on https://github.com/broadinstitute/gatk/pull/4735).; - Add a gradle task (`condaEnvironmentDefinition`) to generate the conda yml files from a single template to ensure that all the environment definitions remain in sync. This task also generates the Python package archive.; - Add a gradle task (`localDevCondaEnv`) to create or update a local (non-Intel) conda environment. This is a shortcut for use during development when you're iteratively changing/testing Python code and want to update the conda env.; - Opportunistically removed the prefix verb ""create"" from the name of the `createPythonPackageArchive` task, which is now called `pythonPackageArchive`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4749
https://github.com/broadinstitute/gatk/issues/4753:315,Availability,error,error,315,"@kgururaj @francesperry There's a [thread on the GATK forum](https://gatkforums.broadinstitute.org/gatk/discussion/comment/48287) where people are reporting a number of issues running GenomicsDB. There are a few different issues but they all seem to be edge cases with the file system. . 1. Report of the following error when trying to read from a GenomicsDB that is marked as read only. Is there a reason that the workspace must be writeable in order to read it? Can we avoid that requirement?; ```; terminate called after throwing an instance of 'VariantQueryProcessorException'; 2018-01-10T12:15:04.154547266Z what(): VariantQueryProcessorException : Could not open array genomicsdb_array at workspace: /keep/d22f668d4f44631d98bc650d582975ca+1399/chr22_db; ```. 2. `Could not open array genomicsdb_array at workspace` when working with a small disk. Changing to a larger disk fixed the problem. Possibly we need a better error message for the case where we are out of disk space?. 3. Reports of similar errors using a Lustre filesystem with file locking disabled. Can GenomicsDB run without file locking? If not, can we emit a clear error message when we hit that problem?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4753
https://github.com/broadinstitute/gatk/issues/4753:924,Availability,error,error,924,"@kgururaj @francesperry There's a [thread on the GATK forum](https://gatkforums.broadinstitute.org/gatk/discussion/comment/48287) where people are reporting a number of issues running GenomicsDB. There are a few different issues but they all seem to be edge cases with the file system. . 1. Report of the following error when trying to read from a GenomicsDB that is marked as read only. Is there a reason that the workspace must be writeable in order to read it? Can we avoid that requirement?; ```; terminate called after throwing an instance of 'VariantQueryProcessorException'; 2018-01-10T12:15:04.154547266Z what(): VariantQueryProcessorException : Could not open array genomicsdb_array at workspace: /keep/d22f668d4f44631d98bc650d582975ca+1399/chr22_db; ```. 2. `Could not open array genomicsdb_array at workspace` when working with a small disk. Changing to a larger disk fixed the problem. Possibly we need a better error message for the case where we are out of disk space?. 3. Reports of similar errors using a Lustre filesystem with file locking disabled. Can GenomicsDB run without file locking? If not, can we emit a clear error message when we hit that problem?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4753
https://github.com/broadinstitute/gatk/issues/4753:1006,Availability,error,errors,1006,"@kgururaj @francesperry There's a [thread on the GATK forum](https://gatkforums.broadinstitute.org/gatk/discussion/comment/48287) where people are reporting a number of issues running GenomicsDB. There are a few different issues but they all seem to be edge cases with the file system. . 1. Report of the following error when trying to read from a GenomicsDB that is marked as read only. Is there a reason that the workspace must be writeable in order to read it? Can we avoid that requirement?; ```; terminate called after throwing an instance of 'VariantQueryProcessorException'; 2018-01-10T12:15:04.154547266Z what(): VariantQueryProcessorException : Could not open array genomicsdb_array at workspace: /keep/d22f668d4f44631d98bc650d582975ca+1399/chr22_db; ```. 2. `Could not open array genomicsdb_array at workspace` when working with a small disk. Changing to a larger disk fixed the problem. Possibly we need a better error message for the case where we are out of disk space?. 3. Reports of similar errors using a Lustre filesystem with file locking disabled. Can GenomicsDB run without file locking? If not, can we emit a clear error message when we hit that problem?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4753
https://github.com/broadinstitute/gatk/issues/4753:1136,Availability,error,error,1136,"@kgururaj @francesperry There's a [thread on the GATK forum](https://gatkforums.broadinstitute.org/gatk/discussion/comment/48287) where people are reporting a number of issues running GenomicsDB. There are a few different issues but they all seem to be edge cases with the file system. . 1. Report of the following error when trying to read from a GenomicsDB that is marked as read only. Is there a reason that the workspace must be writeable in order to read it? Can we avoid that requirement?; ```; terminate called after throwing an instance of 'VariantQueryProcessorException'; 2018-01-10T12:15:04.154547266Z what(): VariantQueryProcessorException : Could not open array genomicsdb_array at workspace: /keep/d22f668d4f44631d98bc650d582975ca+1399/chr22_db; ```. 2. `Could not open array genomicsdb_array at workspace` when working with a small disk. Changing to a larger disk fixed the problem. Possibly we need a better error message for the case where we are out of disk space?. 3. Reports of similar errors using a Lustre filesystem with file locking disabled. Can GenomicsDB run without file locking? If not, can we emit a clear error message when we hit that problem?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4753
https://github.com/broadinstitute/gatk/issues/4753:930,Integrability,message,message,930,"@kgururaj @francesperry There's a [thread on the GATK forum](https://gatkforums.broadinstitute.org/gatk/discussion/comment/48287) where people are reporting a number of issues running GenomicsDB. There are a few different issues but they all seem to be edge cases with the file system. . 1. Report of the following error when trying to read from a GenomicsDB that is marked as read only. Is there a reason that the workspace must be writeable in order to read it? Can we avoid that requirement?; ```; terminate called after throwing an instance of 'VariantQueryProcessorException'; 2018-01-10T12:15:04.154547266Z what(): VariantQueryProcessorException : Could not open array genomicsdb_array at workspace: /keep/d22f668d4f44631d98bc650d582975ca+1399/chr22_db; ```. 2. `Could not open array genomicsdb_array at workspace` when working with a small disk. Changing to a larger disk fixed the problem. Possibly we need a better error message for the case where we are out of disk space?. 3. Reports of similar errors using a Lustre filesystem with file locking disabled. Can GenomicsDB run without file locking? If not, can we emit a clear error message when we hit that problem?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4753
https://github.com/broadinstitute/gatk/issues/4753:1142,Integrability,message,message,1142,"@kgururaj @francesperry There's a [thread on the GATK forum](https://gatkforums.broadinstitute.org/gatk/discussion/comment/48287) where people are reporting a number of issues running GenomicsDB. There are a few different issues but they all seem to be edge cases with the file system. . 1. Report of the following error when trying to read from a GenomicsDB that is marked as read only. Is there a reason that the workspace must be writeable in order to read it? Can we avoid that requirement?; ```; terminate called after throwing an instance of 'VariantQueryProcessorException'; 2018-01-10T12:15:04.154547266Z what(): VariantQueryProcessorException : Could not open array genomicsdb_array at workspace: /keep/d22f668d4f44631d98bc650d582975ca+1399/chr22_db; ```. 2. `Could not open array genomicsdb_array at workspace` when working with a small disk. Changing to a larger disk fixed the problem. Possibly we need a better error message for the case where we are out of disk space?. 3. Reports of similar errors using a Lustre filesystem with file locking disabled. Can GenomicsDB run without file locking? If not, can we emit a clear error message when we hit that problem?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4753
https://github.com/broadinstitute/gatk/issues/4753:471,Safety,avoid,avoid,471,"@kgururaj @francesperry There's a [thread on the GATK forum](https://gatkforums.broadinstitute.org/gatk/discussion/comment/48287) where people are reporting a number of issues running GenomicsDB. There are a few different issues but they all seem to be edge cases with the file system. . 1. Report of the following error when trying to read from a GenomicsDB that is marked as read only. Is there a reason that the workspace must be writeable in order to read it? Can we avoid that requirement?; ```; terminate called after throwing an instance of 'VariantQueryProcessorException'; 2018-01-10T12:15:04.154547266Z what(): VariantQueryProcessorException : Could not open array genomicsdb_array at workspace: /keep/d22f668d4f44631d98bc650d582975ca+1399/chr22_db; ```. 2. `Could not open array genomicsdb_array at workspace` when working with a small disk. Changing to a larger disk fixed the problem. Possibly we need a better error message for the case where we are out of disk space?. 3. Reports of similar errors using a Lustre filesystem with file locking disabled. Can GenomicsDB run without file locking? If not, can we emit a clear error message when we hit that problem?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4753
https://github.com/broadinstitute/gatk/issues/4753:1130,Usability,clear,clear,1130,"@kgururaj @francesperry There's a [thread on the GATK forum](https://gatkforums.broadinstitute.org/gatk/discussion/comment/48287) where people are reporting a number of issues running GenomicsDB. There are a few different issues but they all seem to be edge cases with the file system. . 1. Report of the following error when trying to read from a GenomicsDB that is marked as read only. Is there a reason that the workspace must be writeable in order to read it? Can we avoid that requirement?; ```; terminate called after throwing an instance of 'VariantQueryProcessorException'; 2018-01-10T12:15:04.154547266Z what(): VariantQueryProcessorException : Could not open array genomicsdb_array at workspace: /keep/d22f668d4f44631d98bc650d582975ca+1399/chr22_db; ```. 2. `Could not open array genomicsdb_array at workspace` when working with a small disk. Changing to a larger disk fixed the problem. Possibly we need a better error message for the case where we are out of disk space?. 3. Reports of similar errors using a Lustre filesystem with file locking disabled. Can GenomicsDB run without file locking? If not, can we emit a clear error message when we hit that problem?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4753
https://github.com/broadinstitute/gatk/pull/4756:302,Testability,test,test,302,"This is a ~130 MB snippet of the NexPond-377866 NA12878 bam from; https://github.com/broadinstitute/palantir/wiki/Gold-Standard-Datasets; with:. -all reads from chromosomes 20 and 21; -50,000 unmapped reads added to the end. It can be used with the existing human_g1k_v37.20.21.fasta reference in; src/test/resources/large. Tests that use this new file will be coming in a future branch.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4756
https://github.com/broadinstitute/gatk/pull/4756:324,Testability,Test,Tests,324,"This is a ~130 MB snippet of the NexPond-377866 NA12878 bam from; https://github.com/broadinstitute/palantir/wiki/Gold-Standard-Datasets; with:. -all reads from chromosomes 20 and 21; -50,000 unmapped reads added to the end. It can be used with the existing human_g1k_v37.20.21.fasta reference in; src/test/resources/large. Tests that use this new file will be coming in a future branch.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4756
https://github.com/broadinstitute/gatk/pull/4757:419,Deployability,Install,Install,419,"- Fixes https://github.com/broadinstitute/gatk/issues/4696, https://github.com/broadinstitute/gatk/issues/4342, https://github.com/broadinstitute/gatk/issues/4443, https://github.com/broadinstitute/gatk/issues/4444.; - Use a second FIFO for command acknowledgement instead of relying on prompt synchronization.; - Add a Python module for managing the Python side of GATK/Python interaction.; - Removed all timeouts.; - Install a Python exception handler for handling uncaught Python exceptions.; - Update CNNScoreVariants to use the new protocol.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4757
https://github.com/broadinstitute/gatk/pull/4757:498,Deployability,Update,Update,498,"- Fixes https://github.com/broadinstitute/gatk/issues/4696, https://github.com/broadinstitute/gatk/issues/4342, https://github.com/broadinstitute/gatk/issues/4443, https://github.com/broadinstitute/gatk/issues/4444.; - Use a second FIFO for command acknowledgement instead of relying on prompt synchronization.; - Add a Python module for managing the Python side of GATK/Python interaction.; - Removed all timeouts.; - Install a Python exception handler for handling uncaught Python exceptions.; - Update CNNScoreVariants to use the new protocol.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4757
https://github.com/broadinstitute/gatk/pull/4757:294,Integrability,synchroniz,synchronization,294,"- Fixes https://github.com/broadinstitute/gatk/issues/4696, https://github.com/broadinstitute/gatk/issues/4342, https://github.com/broadinstitute/gatk/issues/4443, https://github.com/broadinstitute/gatk/issues/4444.; - Use a second FIFO for command acknowledgement instead of relying on prompt synchronization.; - Add a Python module for managing the Python side of GATK/Python interaction.; - Removed all timeouts.; - Install a Python exception handler for handling uncaught Python exceptions.; - Update CNNScoreVariants to use the new protocol.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4757
https://github.com/broadinstitute/gatk/pull/4757:537,Integrability,protocol,protocol,537,"- Fixes https://github.com/broadinstitute/gatk/issues/4696, https://github.com/broadinstitute/gatk/issues/4342, https://github.com/broadinstitute/gatk/issues/4443, https://github.com/broadinstitute/gatk/issues/4444.; - Use a second FIFO for command acknowledgement instead of relying on prompt synchronization.; - Add a Python module for managing the Python side of GATK/Python interaction.; - Removed all timeouts.; - Install a Python exception handler for handling uncaught Python exceptions.; - Update CNNScoreVariants to use the new protocol.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4757
https://github.com/broadinstitute/gatk/pull/4757:406,Safety,timeout,timeouts,406,"- Fixes https://github.com/broadinstitute/gatk/issues/4696, https://github.com/broadinstitute/gatk/issues/4342, https://github.com/broadinstitute/gatk/issues/4443, https://github.com/broadinstitute/gatk/issues/4444.; - Use a second FIFO for command acknowledgement instead of relying on prompt synchronization.; - Add a Python module for managing the Python side of GATK/Python interaction.; - Removed all timeouts.; - Install a Python exception handler for handling uncaught Python exceptions.; - Update CNNScoreVariants to use the new protocol.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4757
https://github.com/broadinstitute/gatk/pull/4758:150,Security,validat,validation,150,"Unfortunately, projects like TCGA with BAMs from different sequencing centers do not use the exact same sequence dictionary across them. I've relaxed validation in cases where dictionaries are checked across different BAMs so that only a warning is thrown; however, cases where dictionaries should arise from the same BAM still throw an exception.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4758
https://github.com/broadinstitute/gatk/issues/4759:200,Integrability,interface,interface,200,"In order to better match changes between gatk and picard, the gatk has been set to rely on the picard version of the OpticalDuplicateFinder. Unfortunately this means we have to rely on the Picard Log interface which we don't have a good reason to get involved in the gatk. Furthermore, the logger isn't serializable so it might cause problems with spark.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4759
https://github.com/broadinstitute/gatk/issues/4759:196,Testability,Log,Log,196,"In order to better match changes between gatk and picard, the gatk has been set to rely on the picard version of the OpticalDuplicateFinder. Unfortunately this means we have to rely on the Picard Log interface which we don't have a good reason to get involved in the gatk. Furthermore, the logger isn't serializable so it might cause problems with spark.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4759
https://github.com/broadinstitute/gatk/issues/4759:290,Testability,log,logger,290,"In order to better match changes between gatk and picard, the gatk has been set to rely on the picard version of the OpticalDuplicateFinder. Unfortunately this means we have to rely on the Picard Log interface which we don't have a good reason to get involved in the gatk. Furthermore, the logger isn't serializable so it might cause problems with spark.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4759
https://github.com/broadinstitute/gatk/issues/4760:311,Deployability,integrat,integrated,311,"One of the example commands in CalculateGenotypePosteriors describes the usage of the `-supporting` argument and the `--num-reference-samples-if-no-call` argument at the same time:; ```; gatk --java-options ""-Xmx4g"" CalculateGenotypePosteriors \; -V input.vcf.gz \; -O output.vcf.gz \; -supporting 1000G.phase3.integrated.sites_only.no_MATCHED_REV.hg38.vcf.gz \; --num-reference-samples-if-no-call 2504; ```; Calculate the posterior genotypes of a callset, and impose that a variant *not seen* in the external panel is tantamount to being AC=0, AN=5008 within that panel. We don't have any tests that use both of these arguments at the same time, but it looks like the behavior in that case is wrong. PosteriorProbabilitiesUtils adds numRefSamplesFromMissingResources regardless of whether there was an overlapping variant in the panel or not, effectively diluting the AF of all the variants used as priors and making the number of reference alleles very inconsistent across variants.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4760
https://github.com/broadinstitute/gatk/issues/4760:311,Integrability,integrat,integrated,311,"One of the example commands in CalculateGenotypePosteriors describes the usage of the `-supporting` argument and the `--num-reference-samples-if-no-call` argument at the same time:; ```; gatk --java-options ""-Xmx4g"" CalculateGenotypePosteriors \; -V input.vcf.gz \; -O output.vcf.gz \; -supporting 1000G.phase3.integrated.sites_only.no_MATCHED_REV.hg38.vcf.gz \; --num-reference-samples-if-no-call 2504; ```; Calculate the posterior genotypes of a callset, and impose that a variant *not seen* in the external panel is tantamount to being AC=0, AN=5008 within that panel. We don't have any tests that use both of these arguments at the same time, but it looks like the behavior in that case is wrong. PosteriorProbabilitiesUtils adds numRefSamplesFromMissingResources regardless of whether there was an overlapping variant in the panel or not, effectively diluting the AF of all the variants used as priors and making the number of reference alleles very inconsistent across variants.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4760
https://github.com/broadinstitute/gatk/issues/4760:590,Testability,test,tests,590,"One of the example commands in CalculateGenotypePosteriors describes the usage of the `-supporting` argument and the `--num-reference-samples-if-no-call` argument at the same time:; ```; gatk --java-options ""-Xmx4g"" CalculateGenotypePosteriors \; -V input.vcf.gz \; -O output.vcf.gz \; -supporting 1000G.phase3.integrated.sites_only.no_MATCHED_REV.hg38.vcf.gz \; --num-reference-samples-if-no-call 2504; ```; Calculate the posterior genotypes of a callset, and impose that a variant *not seen* in the external panel is tantamount to being AC=0, AN=5008 within that panel. We don't have any tests that use both of these arguments at the same time, but it looks like the behavior in that case is wrong. PosteriorProbabilitiesUtils adds numRefSamplesFromMissingResources regardless of whether there was an overlapping variant in the panel or not, effectively diluting the AF of all the variants used as priors and making the number of reference alleles very inconsistent across variants.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4760
https://github.com/broadinstitute/gatk/issues/4761:66,Availability,error,error,66,"(https://github.com/samtools/htsjdk/issues/1115); I am getting an error using gatk's VariantRecalibrator:. `htsjdk.tribble.TribbleException: Line 104: there aren't enough columns for line entrainScore=0.7203;HW=4.306476E-6 (we expected 9 tokens, and saw 1 ), for input source: file:///share/ScratchGeneral/evaben/cromwell/cromwell-executions/JointGenotyping/2ce78a09-433b-48eb-83d0-1fa1771d1181/call-SNPsVariantRecalibratorCreateModel/inputs/share/ClusterShare/biodata/contrib/evaben/hs37d5x/vcf/1000G_omni2.5.b37.vcf`. I am having trouble getting to the bottom of the issue. I cannot find the string 'entrainScore=0.7203;HW=4.306476E-6' in the vcf file, and Line 104 is part of the header (I think Line 104 refers to tribble source though). Gatks ValidateVariants does not have any issues with the vcf file, and looking visually with bcftools I cannot see an issue either. . Can anyone suggest further diagnosis steps? Should I take this to GATK issue tracker?. The VCF file is here: ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/b37/1000G_omni2.5.b37.vcf. Here is the gatk command line: ; ```; gatk --java-options ""-Xmx100g -Xms100g"" \; VariantRecalibrator \; -V /share/ScratchGeneral/evaben/cromwell/cromwell-executions/JointGenotyping/2ce78a09-433b-48eb-83d0-1fa1771d1181/call-SNPsVariantRecalibratorCreateModel/inputs/share/ScratchGeneral/evaben/cromwell/cromwell-executions/JointGenotyping/2ce78a09-433b-48eb-83d0-1fa1771d1181/call-SitesOnlyGatherVcf/execution/NA12878.sites_only.vcf.gz \; -O NA12878.snps.recal \; --tranches-file NA12878.snps.tranches \; -trust-all-polymorphic \; -tranche 100.0 -tranche 99.95 -tranche 99.9 -tranche 99.8 -tranche 99.6 -tranche 99.5 -tranche 99.4 -tranche 99.3 -tranche 99.0 -tranche 98.0 -tranche 97.0 -tranche 90.0 \; -an QD -an MQRankSum -an ReadPosRankSum -an FS -an MQ -an SOR -an DP \; -mode SNP \; -sample-every 10 \; --output-model NA12878.snps.model.report \; --max-gaussians 6 \; -resource hapmap,known=false,training=true,truth=true,prior=",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4761
https://github.com/broadinstitute/gatk/issues/4761:1583,Modifiability,polymorphi,polymorphic,1583,"find the string 'entrainScore=0.7203;HW=4.306476E-6' in the vcf file, and Line 104 is part of the header (I think Line 104 refers to tribble source though). Gatks ValidateVariants does not have any issues with the vcf file, and looking visually with bcftools I cannot see an issue either. . Can anyone suggest further diagnosis steps? Should I take this to GATK issue tracker?. The VCF file is here: ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/b37/1000G_omni2.5.b37.vcf. Here is the gatk command line: ; ```; gatk --java-options ""-Xmx100g -Xms100g"" \; VariantRecalibrator \; -V /share/ScratchGeneral/evaben/cromwell/cromwell-executions/JointGenotyping/2ce78a09-433b-48eb-83d0-1fa1771d1181/call-SNPsVariantRecalibratorCreateModel/inputs/share/ScratchGeneral/evaben/cromwell/cromwell-executions/JointGenotyping/2ce78a09-433b-48eb-83d0-1fa1771d1181/call-SitesOnlyGatherVcf/execution/NA12878.sites_only.vcf.gz \; -O NA12878.snps.recal \; --tranches-file NA12878.snps.tranches \; -trust-all-polymorphic \; -tranche 100.0 -tranche 99.95 -tranche 99.9 -tranche 99.8 -tranche 99.6 -tranche 99.5 -tranche 99.4 -tranche 99.3 -tranche 99.0 -tranche 98.0 -tranche 97.0 -tranche 90.0 \; -an QD -an MQRankSum -an ReadPosRankSum -an FS -an MQ -an SOR -an DP \; -mode SNP \; -sample-every 10 \; --output-model NA12878.snps.model.report \; --max-gaussians 6 \; -resource hapmap,known=false,training=true,truth=true,prior=15:/share/ScratchGeneral/evaben/cromwell/cromwell-executions/JointGenotyping/2ce78a09-433b-48eb-83d0-1fa1771d1181/call-SNPsVariantRecalibratorCreateModel/inputs/share/ClusterShare/biodata/contrib/evaben/hs37d5x/vcf/hapmap_3.3.b37.vcf \; -resource omni,known=false,training=true,truth=true,prior=12:/share/ScratchGeneral/evaben/cromwell/cromwell-executions/JointGenotyping/2ce78a09-433b-48eb-83d0-1fa1771d1181/call-SNPsVariantRecalibratorCreateModel/inputs/share/ClusterShare/biodata/contrib/evaben/hs37d5x/vcf/1000G_omni2.5.b37.vcf \; -resource 1000G,known=false,training=true,truth=fa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4761
https://github.com/broadinstitute/gatk/issues/4761:748,Security,Validat,ValidateVariants,748,"(https://github.com/samtools/htsjdk/issues/1115); I am getting an error using gatk's VariantRecalibrator:. `htsjdk.tribble.TribbleException: Line 104: there aren't enough columns for line entrainScore=0.7203;HW=4.306476E-6 (we expected 9 tokens, and saw 1 ), for input source: file:///share/ScratchGeneral/evaben/cromwell/cromwell-executions/JointGenotyping/2ce78a09-433b-48eb-83d0-1fa1771d1181/call-SNPsVariantRecalibratorCreateModel/inputs/share/ClusterShare/biodata/contrib/evaben/hs37d5x/vcf/1000G_omni2.5.b37.vcf`. I am having trouble getting to the bottom of the issue. I cannot find the string 'entrainScore=0.7203;HW=4.306476E-6' in the vcf file, and Line 104 is part of the header (I think Line 104 refers to tribble source though). Gatks ValidateVariants does not have any issues with the vcf file, and looking visually with bcftools I cannot see an issue either. . Can anyone suggest further diagnosis steps? Should I take this to GATK issue tracker?. The VCF file is here: ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/b37/1000G_omni2.5.b37.vcf. Here is the gatk command line: ; ```; gatk --java-options ""-Xmx100g -Xms100g"" \; VariantRecalibrator \; -V /share/ScratchGeneral/evaben/cromwell/cromwell-executions/JointGenotyping/2ce78a09-433b-48eb-83d0-1fa1771d1181/call-SNPsVariantRecalibratorCreateModel/inputs/share/ScratchGeneral/evaben/cromwell/cromwell-executions/JointGenotyping/2ce78a09-433b-48eb-83d0-1fa1771d1181/call-SitesOnlyGatherVcf/execution/NA12878.sites_only.vcf.gz \; -O NA12878.snps.recal \; --tranches-file NA12878.snps.tranches \; -trust-all-polymorphic \; -tranche 100.0 -tranche 99.95 -tranche 99.9 -tranche 99.8 -tranche 99.6 -tranche 99.5 -tranche 99.4 -tranche 99.3 -tranche 99.0 -tranche 98.0 -tranche 97.0 -tranche 90.0 \; -an QD -an MQRankSum -an ReadPosRankSum -an FS -an MQ -an SOR -an DP \; -mode SNP \; -sample-every 10 \; --output-model NA12878.snps.model.report \; --max-gaussians 6 \; -resource hapmap,known=false,training=true,truth=true,prior=",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4761
https://github.com/broadinstitute/gatk/issues/4762:82,Deployability,patch,patch,82,"We've had a request to retry 502's in `google-cloud-nio`, so we'll likely have to patch our custom fork again. When we do so, we should move it to a more standard location (broadinstitute org instead of my personal account).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4762
https://github.com/broadinstitute/gatk/pull/4765:115,Energy Efficiency,reduce,reduce,115,* fix partitioning bug by moving edge fixing from coordinateSortReads -> querynameSortReads; * refactor methods to reduce code duplication; * renaming and moving some methods; * disallow duplicate sort order on spark because it doesn't work with headerless reads,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4765
https://github.com/broadinstitute/gatk/pull/4765:95,Modifiability,refactor,refactor,95,* fix partitioning bug by moving edge fixing from coordinateSortReads -> querynameSortReads; * refactor methods to reduce code duplication; * renaming and moving some methods; * disallow duplicate sort order on spark because it doesn't work with headerless reads,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4765
https://github.com/broadinstitute/gatk/pull/4767:151,Availability,error,error,151,"The SV copy results script has to figure out the total number of workers including standard and preemptibles. The current version of the script had an error in parsing the `gcloud compute dataproc clusters list` command, which left a blank field when no preemptibles were used that couldn't be parsed because it was displayed in fixed-width format. This switches to the CSV format option for the cluster list command, making it easier to parse.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4767
https://github.com/broadinstitute/gatk/issues/4768:260,Availability,down,downsampled,260,"We usually use a `ReadsDownsamplingIterator` with a `PositionalDownsampler` that has a `ReservoirDownsampler` inside of it, which works as intended. If, however, you create a `ReadsDownsamplingIterator` with a raw `ReservoirDownsampler` directly, no items get downsampled. This is because the implementation of `ReservoirDownsampler.hasFinalizedItems()` violates the assumptions of the `ReadsDownsamplingIterator`. Reported by @cmnbroad",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4768
https://github.com/broadinstitute/gatk/pull/4769:788,Availability,error,error,788,"Added XGBoostEvidenceFilter, an alternative BreakpointEvidence filter; based on XGBoost classifier.; - Default is still BreakpointDensityFilter. Switch by passing; ""--sv-evidence-filter-type XGBOOST"" instead of ""DENSITY"".; - Decisions based on evidence overlap or coherence are now scaled based; on coverage depth (in both filter types).; - Multiple avenues for supplying saved classifier binary file,; including built-in resource, local file, and google cloud storage.; - BreakpointEvidence updated to carry information necessary for; classifier. Unit tests were correspondingly updated.; - Data from genome tracts used for some classifier features. From the; hg38 genome: gaps, centromeres, and umap s100. Additional changes to convenience scripts; - Updated sanity_checks.sh to return error signal when exiting; - Bugfixes to manage_sv_pipeline.sh for linux compatibility; - Update run_whole_pipeline.sh to detect preemptible workers and; thus set NUM_EXECUTORS correctly",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4769
https://github.com/broadinstitute/gatk/pull/4769:492,Deployability,update,updated,492,"Added XGBoostEvidenceFilter, an alternative BreakpointEvidence filter; based on XGBoost classifier.; - Default is still BreakpointDensityFilter. Switch by passing; ""--sv-evidence-filter-type XGBOOST"" instead of ""DENSITY"".; - Decisions based on evidence overlap or coherence are now scaled based; on coverage depth (in both filter types).; - Multiple avenues for supplying saved classifier binary file,; including built-in resource, local file, and google cloud storage.; - BreakpointEvidence updated to carry information necessary for; classifier. Unit tests were correspondingly updated.; - Data from genome tracts used for some classifier features. From the; hg38 genome: gaps, centromeres, and umap s100. Additional changes to convenience scripts; - Updated sanity_checks.sh to return error signal when exiting; - Bugfixes to manage_sv_pipeline.sh for linux compatibility; - Update run_whole_pipeline.sh to detect preemptible workers and; thus set NUM_EXECUTORS correctly",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4769
https://github.com/broadinstitute/gatk/pull/4769:580,Deployability,update,updated,580,"Added XGBoostEvidenceFilter, an alternative BreakpointEvidence filter; based on XGBoost classifier.; - Default is still BreakpointDensityFilter. Switch by passing; ""--sv-evidence-filter-type XGBOOST"" instead of ""DENSITY"".; - Decisions based on evidence overlap or coherence are now scaled based; on coverage depth (in both filter types).; - Multiple avenues for supplying saved classifier binary file,; including built-in resource, local file, and google cloud storage.; - BreakpointEvidence updated to carry information necessary for; classifier. Unit tests were correspondingly updated.; - Data from genome tracts used for some classifier features. From the; hg38 genome: gaps, centromeres, and umap s100. Additional changes to convenience scripts; - Updated sanity_checks.sh to return error signal when exiting; - Bugfixes to manage_sv_pipeline.sh for linux compatibility; - Update run_whole_pipeline.sh to detect preemptible workers and; thus set NUM_EXECUTORS correctly",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4769
https://github.com/broadinstitute/gatk/pull/4769:753,Deployability,Update,Updated,753,"Added XGBoostEvidenceFilter, an alternative BreakpointEvidence filter; based on XGBoost classifier.; - Default is still BreakpointDensityFilter. Switch by passing; ""--sv-evidence-filter-type XGBOOST"" instead of ""DENSITY"".; - Decisions based on evidence overlap or coherence are now scaled based; on coverage depth (in both filter types).; - Multiple avenues for supplying saved classifier binary file,; including built-in resource, local file, and google cloud storage.; - BreakpointEvidence updated to carry information necessary for; classifier. Unit tests were correspondingly updated.; - Data from genome tracts used for some classifier features. From the; hg38 genome: gaps, centromeres, and umap s100. Additional changes to convenience scripts; - Updated sanity_checks.sh to return error signal when exiting; - Bugfixes to manage_sv_pipeline.sh for linux compatibility; - Update run_whole_pipeline.sh to detect preemptible workers and; thus set NUM_EXECUTORS correctly",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4769
https://github.com/broadinstitute/gatk/pull/4769:878,Deployability,Update,Update,878,"Added XGBoostEvidenceFilter, an alternative BreakpointEvidence filter; based on XGBoost classifier.; - Default is still BreakpointDensityFilter. Switch by passing; ""--sv-evidence-filter-type XGBOOST"" instead of ""DENSITY"".; - Decisions based on evidence overlap or coherence are now scaled based; on coverage depth (in both filter types).; - Multiple avenues for supplying saved classifier binary file,; including built-in resource, local file, and google cloud storage.; - BreakpointEvidence updated to carry information necessary for; classifier. Unit tests were correspondingly updated.; - Data from genome tracts used for some classifier features. From the; hg38 genome: gaps, centromeres, and umap s100. Additional changes to convenience scripts; - Updated sanity_checks.sh to return error signal when exiting; - Bugfixes to manage_sv_pipeline.sh for linux compatibility; - Update run_whole_pipeline.sh to detect preemptible workers and; thus set NUM_EXECUTORS correctly",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4769
https://github.com/broadinstitute/gatk/pull/4769:910,Safety,detect,detect,910,"Added XGBoostEvidenceFilter, an alternative BreakpointEvidence filter; based on XGBoost classifier.; - Default is still BreakpointDensityFilter. Switch by passing; ""--sv-evidence-filter-type XGBOOST"" instead of ""DENSITY"".; - Decisions based on evidence overlap or coherence are now scaled based; on coverage depth (in both filter types).; - Multiple avenues for supplying saved classifier binary file,; including built-in resource, local file, and google cloud storage.; - BreakpointEvidence updated to carry information necessary for; classifier. Unit tests were correspondingly updated.; - Data from genome tracts used for some classifier features. From the; hg38 genome: gaps, centromeres, and umap s100. Additional changes to convenience scripts; - Updated sanity_checks.sh to return error signal when exiting; - Bugfixes to manage_sv_pipeline.sh for linux compatibility; - Update run_whole_pipeline.sh to detect preemptible workers and; thus set NUM_EXECUTORS correctly",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4769
https://github.com/broadinstitute/gatk/pull/4769:553,Testability,test,tests,553,"Added XGBoostEvidenceFilter, an alternative BreakpointEvidence filter; based on XGBoost classifier.; - Default is still BreakpointDensityFilter. Switch by passing; ""--sv-evidence-filter-type XGBOOST"" instead of ""DENSITY"".; - Decisions based on evidence overlap or coherence are now scaled based; on coverage depth (in both filter types).; - Multiple avenues for supplying saved classifier binary file,; including built-in resource, local file, and google cloud storage.; - BreakpointEvidence updated to carry information necessary for; classifier. Unit tests were correspondingly updated.; - Data from genome tracts used for some classifier features. From the; hg38 genome: gaps, centromeres, and umap s100. Additional changes to convenience scripts; - Updated sanity_checks.sh to return error signal when exiting; - Bugfixes to manage_sv_pipeline.sh for linux compatibility; - Update run_whole_pipeline.sh to detect preemptible workers and; thus set NUM_EXECUTORS correctly",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4769
https://github.com/broadinstitute/gatk/pull/4770:26,Availability,down,down,26,Added in a script to pull down the latest Gencode data source.; Fixed an issue in 5' UTR processing that would cause variant alleles with length > 1 to throw an exception (issue 4712).; Added three test cases to prevent regression of issue 4712.; Updated Gencode codec to be compatible with latest Gencode release (v28).; Fixed a bug in the version detection for Funcotator data sources that would prevent newer data source versions from being detected as compatible (date comparison error).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4770
https://github.com/broadinstitute/gatk/pull/4770:484,Availability,error,error,484,Added in a script to pull down the latest Gencode data source.; Fixed an issue in 5' UTR processing that would cause variant alleles with length > 1 to throw an exception (issue 4712).; Added three test cases to prevent regression of issue 4712.; Updated Gencode codec to be compatible with latest Gencode release (v28).; Fixed a bug in the version detection for Funcotator data sources that would prevent newer data source versions from being detected as compatible (date comparison error).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4770
https://github.com/broadinstitute/gatk/pull/4770:247,Deployability,Update,Updated,247,Added in a script to pull down the latest Gencode data source.; Fixed an issue in 5' UTR processing that would cause variant alleles with length > 1 to throw an exception (issue 4712).; Added three test cases to prevent regression of issue 4712.; Updated Gencode codec to be compatible with latest Gencode release (v28).; Fixed a bug in the version detection for Funcotator data sources that would prevent newer data source versions from being detected as compatible (date comparison error).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4770
https://github.com/broadinstitute/gatk/pull/4770:306,Deployability,release,release,306,Added in a script to pull down the latest Gencode data source.; Fixed an issue in 5' UTR processing that would cause variant alleles with length > 1 to throw an exception (issue 4712).; Added three test cases to prevent regression of issue 4712.; Updated Gencode codec to be compatible with latest Gencode release (v28).; Fixed a bug in the version detection for Funcotator data sources that would prevent newer data source versions from being detected as compatible (date comparison error).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4770
https://github.com/broadinstitute/gatk/pull/4770:349,Safety,detect,detection,349,Added in a script to pull down the latest Gencode data source.; Fixed an issue in 5' UTR processing that would cause variant alleles with length > 1 to throw an exception (issue 4712).; Added three test cases to prevent regression of issue 4712.; Updated Gencode codec to be compatible with latest Gencode release (v28).; Fixed a bug in the version detection for Funcotator data sources that would prevent newer data source versions from being detected as compatible (date comparison error).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4770
https://github.com/broadinstitute/gatk/pull/4770:444,Safety,detect,detected,444,Added in a script to pull down the latest Gencode data source.; Fixed an issue in 5' UTR processing that would cause variant alleles with length > 1 to throw an exception (issue 4712).; Added three test cases to prevent regression of issue 4712.; Updated Gencode codec to be compatible with latest Gencode release (v28).; Fixed a bug in the version detection for Funcotator data sources that would prevent newer data source versions from being detected as compatible (date comparison error).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4770
https://github.com/broadinstitute/gatk/pull/4770:198,Testability,test,test,198,Added in a script to pull down the latest Gencode data source.; Fixed an issue in 5' UTR processing that would cause variant alleles with length > 1 to throw an exception (issue 4712).; Added three test cases to prevent regression of issue 4712.; Updated Gencode codec to be compatible with latest Gencode release (v28).; Fixed a bug in the version detection for Funcotator data sources that would prevent newer data source versions from being detected as compatible (date comparison error).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4770
https://github.com/broadinstitute/gatk/issues/4771:67,Availability,down,downstream,67,"Funcotator needs to be able to handle variants in the upstream and downstream flanks of a gene. Right now because we walk over variants that match genes in Gencode, there will be no gene matches for upstream and downstream variants. . To do this we will need to update our features to match +/- the maximum of the padding for each en(5' or 3' padding). This will also affect IGR processing. We will need to update the caching scheme in `FeatureCache` to cache around a locus rather than just in front of it (in a configurable manner). 5' should default to 5000 and 3' should default to zero.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4771
https://github.com/broadinstitute/gatk/issues/4771:212,Availability,down,downstream,212,"Funcotator needs to be able to handle variants in the upstream and downstream flanks of a gene. Right now because we walk over variants that match genes in Gencode, there will be no gene matches for upstream and downstream variants. . To do this we will need to update our features to match +/- the maximum of the padding for each en(5' or 3' padding). This will also affect IGR processing. We will need to update the caching scheme in `FeatureCache` to cache around a locus rather than just in front of it (in a configurable manner). 5' should default to 5000 and 3' should default to zero.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4771
https://github.com/broadinstitute/gatk/issues/4771:262,Deployability,update,update,262,"Funcotator needs to be able to handle variants in the upstream and downstream flanks of a gene. Right now because we walk over variants that match genes in Gencode, there will be no gene matches for upstream and downstream variants. . To do this we will need to update our features to match +/- the maximum of the padding for each en(5' or 3' padding). This will also affect IGR processing. We will need to update the caching scheme in `FeatureCache` to cache around a locus rather than just in front of it (in a configurable manner). 5' should default to 5000 and 3' should default to zero.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4771
https://github.com/broadinstitute/gatk/issues/4771:407,Deployability,update,update,407,"Funcotator needs to be able to handle variants in the upstream and downstream flanks of a gene. Right now because we walk over variants that match genes in Gencode, there will be no gene matches for upstream and downstream variants. . To do this we will need to update our features to match +/- the maximum of the padding for each en(5' or 3' padding). This will also affect IGR processing. We will need to update the caching scheme in `FeatureCache` to cache around a locus rather than just in front of it (in a configurable manner). 5' should default to 5000 and 3' should default to zero.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4771
https://github.com/broadinstitute/gatk/issues/4771:513,Modifiability,config,configurable,513,"Funcotator needs to be able to handle variants in the upstream and downstream flanks of a gene. Right now because we walk over variants that match genes in Gencode, there will be no gene matches for upstream and downstream variants. . To do this we will need to update our features to match +/- the maximum of the padding for each en(5' or 3' padding). This will also affect IGR processing. We will need to update the caching scheme in `FeatureCache` to cache around a locus rather than just in front of it (in a configurable manner). 5' should default to 5000 and 3' should default to zero.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4771
https://github.com/broadinstitute/gatk/issues/4771:454,Performance,cache,cache,454,"Funcotator needs to be able to handle variants in the upstream and downstream flanks of a gene. Right now because we walk over variants that match genes in Gencode, there will be no gene matches for upstream and downstream variants. . To do this we will need to update our features to match +/- the maximum of the padding for each en(5' or 3' padding). This will also affect IGR processing. We will need to update the caching scheme in `FeatureCache` to cache around a locus rather than just in front of it (in a configurable manner). 5' should default to 5000 and 3' should default to zero.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4771
https://github.com/broadinstitute/gatk/issues/4773:111,Deployability,update,updates,111,"According to @vdauwera, the most-requested GenomicsDB feature from GATK users is the ability to do incremental updates to an existing GenomicsDB.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4773
https://github.com/broadinstitute/gatk/issues/4775:1231,Energy Efficiency,schedul,scheduler,1231,readBroadcastBlock(TorrentBroadcast.scala:206); 	at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:66); 	at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66); 	at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96); 	at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSparkUtils.lambda$transformToDuplicateNames$4992d4e$1(MarkDuplicatesSparkUtils.java:140); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$3$1.apply(JavaRDDLike.scala:143); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$3$1.apply(JavaRDDLike.scala:143); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:199); 	at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: java.lang.UnsupportedOperationException; 	at org.broadinstitute.hellbender.relocated.com.google.common.collect.ImmutableMap.put(ImmutableMap.java:326); 	at com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:162); 	at com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:39); 	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:790); 	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:246); 	at org.apache.spark.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4775
https://github.com/broadinstitute/gatk/issues/4775:1311,Energy Efficiency,schedul,scheduler,1311,orrentBroadcast._value$lzycompute(TorrentBroadcast.scala:66); 	at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66); 	at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96); 	at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSparkUtils.lambda$transformToDuplicateNames$4992d4e$1(MarkDuplicatesSparkUtils.java:140); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$3$1.apply(JavaRDDLike.scala:143); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$3$1.apply(JavaRDDLike.scala:143); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:199); 	at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: java.lang.UnsupportedOperationException; 	at org.broadinstitute.hellbender.relocated.com.google.common.collect.ImmutableMap.put(ImmutableMap.java:326); 	at com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:162); 	at com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:39); 	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:790); 	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:246); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$8.apply(TorrentBroadcast.scala:293); 	at org,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4775
https://github.com/broadinstitute/gatk/issues/4775:1391,Energy Efficiency,schedul,scheduler,1391,rk.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66); 	at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96); 	at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSparkUtils.lambda$transformToDuplicateNames$4992d4e$1(MarkDuplicatesSparkUtils.java:140); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$3$1.apply(JavaRDDLike.scala:143); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$3$1.apply(JavaRDDLike.scala:143); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:199); 	at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: java.lang.UnsupportedOperationException; 	at org.broadinstitute.hellbender.relocated.com.google.common.collect.ImmutableMap.put(ImmutableMap.java:326); 	at com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:162); 	at com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:39); 	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:790); 	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:246); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$8.apply(TorrentBroadcast.scala:293); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337); 	at org.apache.s,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4775
https://github.com/broadinstitute/gatk/issues/4775:1516,Performance,concurren,concurrent,1516,rentBroadcast.scala:96); 	at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSparkUtils.lambda$transformToDuplicateNames$4992d4e$1(MarkDuplicatesSparkUtils.java:140); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$3$1.apply(JavaRDDLike.scala:143); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$3$1.apply(JavaRDDLike.scala:143); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:199); 	at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: java.lang.UnsupportedOperationException; 	at org.broadinstitute.hellbender.relocated.com.google.common.collect.ImmutableMap.put(ImmutableMap.java:326); 	at com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:162); 	at com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:39); 	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:790); 	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:246); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$8.apply(TorrentBroadcast.scala:293); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337); 	at org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:294); 	at org.apache.spark.broadcast.TorrentBroadcast,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4775
https://github.com/broadinstitute/gatk/issues/4775:1601,Performance,concurren,concurrent,1601,kduplicates.MarkDuplicatesSparkUtils.lambda$transformToDuplicateNames$4992d4e$1(MarkDuplicatesSparkUtils.java:140); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$3$1.apply(JavaRDDLike.scala:143); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$3$1.apply(JavaRDDLike.scala:143); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:199); 	at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: java.lang.UnsupportedOperationException; 	at org.broadinstitute.hellbender.relocated.com.google.common.collect.ImmutableMap.put(ImmutableMap.java:326); 	at com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:162); 	at com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:39); 	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:790); 	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:246); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$8.apply(TorrentBroadcast.scala:293); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337); 	at org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:294); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:226); 	at org.apache.spar,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4775
https://github.com/broadinstitute/gatk/issues/4775:8,Testability,test,tests,8,jenkins tests caught a MarkDuplicatesBug that our existing tests didnt'. ```; java.lang.UnsupportedOperationException; 	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310); 	at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:206); 	at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:66); 	at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66); 	at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96); 	at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSparkUtils.lambda$transformToDuplicateNames$4992d4e$1(MarkDuplicatesSparkUtils.java:140); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$3$1.apply(JavaRDDLike.scala:143); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$3$1.apply(JavaRDDLike.scala:143); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:199); 	at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: java.lang.UnsupportedOperationException; 	at org.broadinstitute.hellbender.relocated.com.google.common.collect.ImmutableMap.put(ImmutableMap.java:326); 	at com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:162); 	at com.esotericsoftware.kryo.seriali,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4775
https://github.com/broadinstitute/gatk/issues/4775:59,Testability,test,tests,59,jenkins tests caught a MarkDuplicatesBug that our existing tests didnt'. ```; java.lang.UnsupportedOperationException; 	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310); 	at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:206); 	at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:66); 	at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66); 	at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96); 	at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSparkUtils.lambda$transformToDuplicateNames$4992d4e$1(MarkDuplicatesSparkUtils.java:140); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$3$1.apply(JavaRDDLike.scala:143); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$3$1.apply(JavaRDDLike.scala:143); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:199); 	at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: java.lang.UnsupportedOperationException; 	at org.broadinstitute.hellbender.relocated.com.google.common.collect.ImmutableMap.put(ImmutableMap.java:326); 	at com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:162); 	at com.esotericsoftware.kryo.seriali,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4775
https://github.com/broadinstitute/gatk/issues/4775:2790,Testability,test,test,2790,fn$3$1.apply(JavaRDDLike.scala:143); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:199); 	at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: java.lang.UnsupportedOperationException; 	at org.broadinstitute.hellbender.relocated.com.google.common.collect.ImmutableMap.put(ImmutableMap.java:326); 	at com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:162); 	at com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:39); 	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:790); 	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:246); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$8.apply(TorrentBroadcast.scala:293); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337); 	at org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:294); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:226); 	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303); 	... 18 more; ```. We recently added a broadcast of an immutable map which seems to be causing the problem. We should add a dataproc test for markduplicates so that this sort of thing is regularly tested.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4775
https://github.com/broadinstitute/gatk/issues/4775:2854,Testability,test,tested,2854,fn$3$1.apply(JavaRDDLike.scala:143); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:199); 	at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: java.lang.UnsupportedOperationException; 	at org.broadinstitute.hellbender.relocated.com.google.common.collect.ImmutableMap.put(ImmutableMap.java:326); 	at com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:162); 	at com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:39); 	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:790); 	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:246); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$8.apply(TorrentBroadcast.scala:293); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337); 	at org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:294); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:226); 	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303); 	... 18 more; ```. We recently added a broadcast of an immutable map which seems to be causing the problem. We should add a dataproc test for markduplicates so that this sort of thing is regularly tested.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4775
https://github.com/broadinstitute/gatk/issues/4777:164,Security,audit,audited,164,"It looks like picard metrics record at least one more field when the metrics get reported, namely the count of supplementary reads seen. The metrics code should be audited to ensure it matches with picard over the same files.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4777
https://github.com/broadinstitute/gatk/issues/4781:518,Modifiability,config,configure,518,"Now that we have important `VariantWalker` tools that use reads as a side input (such as @lucidtronix 's `CNNScoreVariants`), we need to add caching to `ReadsContext` for good performance on nearby reads queries during a traversal. The caching should be modeled after the existing caching in `FeatureContext` as implemented in the `FeatureCache` class, but it should include the ability to cache ""around"" the current locus, rather than just ahead of it as in `FeatureCache`. Ideally, the tool itself should be able to configure the default caching behavior via arguments to control bases to cache before and after current locus.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4781
https://github.com/broadinstitute/gatk/issues/4781:176,Performance,perform,performance,176,"Now that we have important `VariantWalker` tools that use reads as a side input (such as @lucidtronix 's `CNNScoreVariants`), we need to add caching to `ReadsContext` for good performance on nearby reads queries during a traversal. The caching should be modeled after the existing caching in `FeatureContext` as implemented in the `FeatureCache` class, but it should include the ability to cache ""around"" the current locus, rather than just ahead of it as in `FeatureCache`. Ideally, the tool itself should be able to configure the default caching behavior via arguments to control bases to cache before and after current locus.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4781
https://github.com/broadinstitute/gatk/issues/4781:390,Performance,cache,cache,390,"Now that we have important `VariantWalker` tools that use reads as a side input (such as @lucidtronix 's `CNNScoreVariants`), we need to add caching to `ReadsContext` for good performance on nearby reads queries during a traversal. The caching should be modeled after the existing caching in `FeatureContext` as implemented in the `FeatureCache` class, but it should include the ability to cache ""around"" the current locus, rather than just ahead of it as in `FeatureCache`. Ideally, the tool itself should be able to configure the default caching behavior via arguments to control bases to cache before and after current locus.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4781
https://github.com/broadinstitute/gatk/issues/4781:591,Performance,cache,cache,591,"Now that we have important `VariantWalker` tools that use reads as a side input (such as @lucidtronix 's `CNNScoreVariants`), we need to add caching to `ReadsContext` for good performance on nearby reads queries during a traversal. The caching should be modeled after the existing caching in `FeatureContext` as implemented in the `FeatureCache` class, but it should include the ability to cache ""around"" the current locus, rather than just ahead of it as in `FeatureCache`. Ideally, the tool itself should be able to configure the default caching behavior via arguments to control bases to cache before and after current locus.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4781
https://github.com/broadinstitute/gatk/issues/4782:235,Availability,error,error,235,"Hello,. I'm trying to use the CNVGermline Pipeline with Nextflow and a GATK Singularity image pulled from your Docker image. . As you can see, I'm trying to run the DetermineGermlineContigPloidy with 57 samples, and I got a permission error within my Singularity container. This error is directly related to Singularity permissions to create a directory ('/root/.theano/compiledir_Linux-4.10--generic-x86_64-with-debian-stretch-sid-x86_64-3.6.2-64'), because it's running fine with Singularity when I'm root and Docker (without root). So it's not really a GATK4 problem but more a singularity-GATK4 related problem. Maybe your GATK4 Germline CNV calling pipeline is not designed to work as a Singularity container (which is quite understandable), because for tools like HaplotypeCaller, MarkDuplicates ... It's working fine. I didn't know where to post this error, singularity or GATK github, so why not both ! . Do you have any idea make it run properly ? Change a bit the design of your CNV pipeline to make it compatible with Singularity ?. ## Version of softwares:. Singularity : 2.5.1, GATK : 4.0.4.0. ### Command. Singularity : ; singularity build gatk-4.0.4.0.img docker://broadinstitute/gatk:4.0.4.0. GATK4 : ; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/build/libs/gatk-package-4.0.4.0-local.jar DetermineGermlineContigPloidy --input 2044098202-8046_S5_sample.counts.hdf5 --input 2045946179-9076_S2_sample.counts.hdf5 --input 2045946166-9075_S1_sample.counts.hdf5 --input 2048220927-11022_S4_sample.counts.hdf5 --input 2045599261-9046ci_S1_sample.counts.hdf5 --input 2046745668-1007_S5_sample.counts.hdf5 --input 2044098101-8043_S2_sample.counts.hdf5 --input 2044098168-8044_S3_sample.counts.hdf5 --input 2046746598-1012_S4_sample.counts.hdf5 --input 2044395763-8064ci_S4_sample.counts.hdf5 --input 2044395647-8061ci_S1_sample.counts.hdf5 --input 70-20-CI_S3_sample",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782
https://github.com/broadinstitute/gatk/issues/4782:279,Availability,error,error,279,"Hello,. I'm trying to use the CNVGermline Pipeline with Nextflow and a GATK Singularity image pulled from your Docker image. . As you can see, I'm trying to run the DetermineGermlineContigPloidy with 57 samples, and I got a permission error within my Singularity container. This error is directly related to Singularity permissions to create a directory ('/root/.theano/compiledir_Linux-4.10--generic-x86_64-with-debian-stretch-sid-x86_64-3.6.2-64'), because it's running fine with Singularity when I'm root and Docker (without root). So it's not really a GATK4 problem but more a singularity-GATK4 related problem. Maybe your GATK4 Germline CNV calling pipeline is not designed to work as a Singularity container (which is quite understandable), because for tools like HaplotypeCaller, MarkDuplicates ... It's working fine. I didn't know where to post this error, singularity or GATK github, so why not both ! . Do you have any idea make it run properly ? Change a bit the design of your CNV pipeline to make it compatible with Singularity ?. ## Version of softwares:. Singularity : 2.5.1, GATK : 4.0.4.0. ### Command. Singularity : ; singularity build gatk-4.0.4.0.img docker://broadinstitute/gatk:4.0.4.0. GATK4 : ; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/build/libs/gatk-package-4.0.4.0-local.jar DetermineGermlineContigPloidy --input 2044098202-8046_S5_sample.counts.hdf5 --input 2045946179-9076_S2_sample.counts.hdf5 --input 2045946166-9075_S1_sample.counts.hdf5 --input 2048220927-11022_S4_sample.counts.hdf5 --input 2045599261-9046ci_S1_sample.counts.hdf5 --input 2046745668-1007_S5_sample.counts.hdf5 --input 2044098101-8043_S2_sample.counts.hdf5 --input 2044098168-8044_S3_sample.counts.hdf5 --input 2046746598-1012_S4_sample.counts.hdf5 --input 2044395763-8064ci_S4_sample.counts.hdf5 --input 2044395647-8061ci_S1_sample.counts.hdf5 --input 70-20-CI_S3_sample",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782
https://github.com/broadinstitute/gatk/issues/4782:858,Availability,error,error,858,"Hello,. I'm trying to use the CNVGermline Pipeline with Nextflow and a GATK Singularity image pulled from your Docker image. . As you can see, I'm trying to run the DetermineGermlineContigPloidy with 57 samples, and I got a permission error within my Singularity container. This error is directly related to Singularity permissions to create a directory ('/root/.theano/compiledir_Linux-4.10--generic-x86_64-with-debian-stretch-sid-x86_64-3.6.2-64'), because it's running fine with Singularity when I'm root and Docker (without root). So it's not really a GATK4 problem but more a singularity-GATK4 related problem. Maybe your GATK4 Germline CNV calling pipeline is not designed to work as a Singularity container (which is quite understandable), because for tools like HaplotypeCaller, MarkDuplicates ... It's working fine. I didn't know where to post this error, singularity or GATK github, so why not both ! . Do you have any idea make it run properly ? Change a bit the design of your CNV pipeline to make it compatible with Singularity ?. ## Version of softwares:. Singularity : 2.5.1, GATK : 4.0.4.0. ### Command. Singularity : ; singularity build gatk-4.0.4.0.img docker://broadinstitute/gatk:4.0.4.0. GATK4 : ; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/build/libs/gatk-package-4.0.4.0-local.jar DetermineGermlineContigPloidy --input 2044098202-8046_S5_sample.counts.hdf5 --input 2045946179-9076_S2_sample.counts.hdf5 --input 2045946166-9075_S1_sample.counts.hdf5 --input 2048220927-11022_S4_sample.counts.hdf5 --input 2045599261-9046ci_S1_sample.counts.hdf5 --input 2046745668-1007_S5_sample.counts.hdf5 --input 2044098101-8043_S2_sample.counts.hdf5 --input 2044098168-8044_S3_sample.counts.hdf5 --input 2046746598-1012_S4_sample.counts.hdf5 --input 2044395763-8064ci_S4_sample.counts.hdf5 --input 2044395647-8061ci_S1_sample.counts.hdf5 --input 70-20-CI_S3_sample",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782
https://github.com/broadinstitute/gatk/issues/4782:42,Deployability,Pipeline,Pipeline,42,"Hello,. I'm trying to use the CNVGermline Pipeline with Nextflow and a GATK Singularity image pulled from your Docker image. . As you can see, I'm trying to run the DetermineGermlineContigPloidy with 57 samples, and I got a permission error within my Singularity container. This error is directly related to Singularity permissions to create a directory ('/root/.theano/compiledir_Linux-4.10--generic-x86_64-with-debian-stretch-sid-x86_64-3.6.2-64'), because it's running fine with Singularity when I'm root and Docker (without root). So it's not really a GATK4 problem but more a singularity-GATK4 related problem. Maybe your GATK4 Germline CNV calling pipeline is not designed to work as a Singularity container (which is quite understandable), because for tools like HaplotypeCaller, MarkDuplicates ... It's working fine. I didn't know where to post this error, singularity or GATK github, so why not both ! . Do you have any idea make it run properly ? Change a bit the design of your CNV pipeline to make it compatible with Singularity ?. ## Version of softwares:. Singularity : 2.5.1, GATK : 4.0.4.0. ### Command. Singularity : ; singularity build gatk-4.0.4.0.img docker://broadinstitute/gatk:4.0.4.0. GATK4 : ; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/build/libs/gatk-package-4.0.4.0-local.jar DetermineGermlineContigPloidy --input 2044098202-8046_S5_sample.counts.hdf5 --input 2045946179-9076_S2_sample.counts.hdf5 --input 2045946166-9075_S1_sample.counts.hdf5 --input 2048220927-11022_S4_sample.counts.hdf5 --input 2045599261-9046ci_S1_sample.counts.hdf5 --input 2046745668-1007_S5_sample.counts.hdf5 --input 2044098101-8043_S2_sample.counts.hdf5 --input 2044098168-8044_S3_sample.counts.hdf5 --input 2046746598-1012_S4_sample.counts.hdf5 --input 2044395763-8064ci_S4_sample.counts.hdf5 --input 2044395647-8061ci_S1_sample.counts.hdf5 --input 70-20-CI_S3_sample",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782
https://github.com/broadinstitute/gatk/issues/4782:654,Deployability,pipeline,pipeline,654,"Hello,. I'm trying to use the CNVGermline Pipeline with Nextflow and a GATK Singularity image pulled from your Docker image. . As you can see, I'm trying to run the DetermineGermlineContigPloidy with 57 samples, and I got a permission error within my Singularity container. This error is directly related to Singularity permissions to create a directory ('/root/.theano/compiledir_Linux-4.10--generic-x86_64-with-debian-stretch-sid-x86_64-3.6.2-64'), because it's running fine with Singularity when I'm root and Docker (without root). So it's not really a GATK4 problem but more a singularity-GATK4 related problem. Maybe your GATK4 Germline CNV calling pipeline is not designed to work as a Singularity container (which is quite understandable), because for tools like HaplotypeCaller, MarkDuplicates ... It's working fine. I didn't know where to post this error, singularity or GATK github, so why not both ! . Do you have any idea make it run properly ? Change a bit the design of your CNV pipeline to make it compatible with Singularity ?. ## Version of softwares:. Singularity : 2.5.1, GATK : 4.0.4.0. ### Command. Singularity : ; singularity build gatk-4.0.4.0.img docker://broadinstitute/gatk:4.0.4.0. GATK4 : ; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/build/libs/gatk-package-4.0.4.0-local.jar DetermineGermlineContigPloidy --input 2044098202-8046_S5_sample.counts.hdf5 --input 2045946179-9076_S2_sample.counts.hdf5 --input 2045946166-9075_S1_sample.counts.hdf5 --input 2048220927-11022_S4_sample.counts.hdf5 --input 2045599261-9046ci_S1_sample.counts.hdf5 --input 2046745668-1007_S5_sample.counts.hdf5 --input 2044098101-8043_S2_sample.counts.hdf5 --input 2044098168-8044_S3_sample.counts.hdf5 --input 2046746598-1012_S4_sample.counts.hdf5 --input 2044395763-8064ci_S4_sample.counts.hdf5 --input 2044395647-8061ci_S1_sample.counts.hdf5 --input 70-20-CI_S3_sample",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782
https://github.com/broadinstitute/gatk/issues/4782:993,Deployability,pipeline,pipeline,993,"Hello,. I'm trying to use the CNVGermline Pipeline with Nextflow and a GATK Singularity image pulled from your Docker image. . As you can see, I'm trying to run the DetermineGermlineContigPloidy with 57 samples, and I got a permission error within my Singularity container. This error is directly related to Singularity permissions to create a directory ('/root/.theano/compiledir_Linux-4.10--generic-x86_64-with-debian-stretch-sid-x86_64-3.6.2-64'), because it's running fine with Singularity when I'm root and Docker (without root). So it's not really a GATK4 problem but more a singularity-GATK4 related problem. Maybe your GATK4 Germline CNV calling pipeline is not designed to work as a Singularity container (which is quite understandable), because for tools like HaplotypeCaller, MarkDuplicates ... It's working fine. I didn't know where to post this error, singularity or GATK github, so why not both ! . Do you have any idea make it run properly ? Change a bit the design of your CNV pipeline to make it compatible with Singularity ?. ## Version of softwares:. Singularity : 2.5.1, GATK : 4.0.4.0. ### Command. Singularity : ; singularity build gatk-4.0.4.0.img docker://broadinstitute/gatk:4.0.4.0. GATK4 : ; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/build/libs/gatk-package-4.0.4.0-local.jar DetermineGermlineContigPloidy --input 2044098202-8046_S5_sample.counts.hdf5 --input 2045946179-9076_S2_sample.counts.hdf5 --input 2045946166-9075_S1_sample.counts.hdf5 --input 2048220927-11022_S4_sample.counts.hdf5 --input 2045599261-9046ci_S1_sample.counts.hdf5 --input 2046745668-1007_S5_sample.counts.hdf5 --input 2044098101-8043_S2_sample.counts.hdf5 --input 2044098168-8044_S3_sample.counts.hdf5 --input 2046746598-1012_S4_sample.counts.hdf5 --input 2044395763-8064ci_S4_sample.counts.hdf5 --input 2044395647-8061ci_S1_sample.counts.hdf5 --input 70-20-CI_S3_sample",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782
https://github.com/broadinstitute/gatk/issues/4782:6006,Modifiability,config,config,6006,"le>; from . import timeseries; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/pymc3/distributions/timeseries.py"", line 1, in <module>; import theano.tensor as tt; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/__init__.py"", line 66, in <module>; from theano.compile import (; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/compile/__init__.py"", line 10, in <module>; from theano.compile.function_module import *; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/compile/function_module.py"", line 21, in <module>; import theano.compile.mode; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/compile/mode.py"", line 10, in <module>; import theano.gof.vm; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/vm.py"", line 662, in <module>; from . import lazylinker_c; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 42, in <module>; location = os.path.join(config.compiledir, 'lazylinker_ext'); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/confith.join(config.compiledir, 'lazylinker_ext'); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configparser.py"", line 333, in __get__; self.__set__(cls, val_str); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configparser.py"", line 344, in __set__; self.val = self.filter(val); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configdefaults.py"", line 1745, in filter_compiledir; "" '%s'. Check the permissions."" % path); ValueError: Unable to create the compiledir directory '/root/.theano/compiledir_Linux-4.10--generic-x86_64-with-debian-stretch-sid-x86_64-3.6.2-64'. Check the permissions. at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75); at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCuratedArgs(ScriptExecutor.java:126); at org.broadinstitute.hel",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782
https://github.com/broadinstitute/gatk/issues/4782:6123,Modifiability,config,config,6123,"ies.py"", line 1, in <module>; import theano.tensor as tt; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/__init__.py"", line 66, in <module>; from theano.compile import (; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/compile/__init__.py"", line 10, in <module>; from theano.compile.function_module import *; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/compile/function_module.py"", line 21, in <module>; import theano.compile.mode; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/compile/mode.py"", line 10, in <module>; import theano.gof.vm; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/vm.py"", line 662, in <module>; from . import lazylinker_c; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 42, in <module>; location = os.path.join(config.compiledir, 'lazylinker_ext'); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/confith.join(config.compiledir, 'lazylinker_ext'); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configparser.py"", line 333, in __get__; self.__set__(cls, val_str); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configparser.py"", line 344, in __set__; self.val = self.filter(val); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configdefaults.py"", line 1745, in filter_compiledir; "" '%s'. Check the permissions."" % path); ValueError: Unable to create the compiledir directory '/root/.theano/compiledir_Linux-4.10--generic-x86_64-with-debian-stretch-sid-x86_64-3.6.2-64'. Check the permissions. at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75); at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCuratedArgs(ScriptExecutor.java:126); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeArgs(PythonScriptExecutor.java:170); at org.broadinstitute.hellbende",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782
https://github.com/broadinstitute/gatk/issues/4782:6227,Modifiability,config,configparser,6227,"n3.6/site-packages/theano/__init__.py"", line 66, in <module>; from theano.compile import (; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/compile/__init__.py"", line 10, in <module>; from theano.compile.function_module import *; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/compile/function_module.py"", line 21, in <module>; import theano.compile.mode; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/compile/mode.py"", line 10, in <module>; import theano.gof.vm; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/vm.py"", line 662, in <module>; from . import lazylinker_c; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 42, in <module>; location = os.path.join(config.compiledir, 'lazylinker_ext'); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/confith.join(config.compiledir, 'lazylinker_ext'); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configparser.py"", line 333, in __get__; self.__set__(cls, val_str); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configparser.py"", line 344, in __set__; self.val = self.filter(val); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configdefaults.py"", line 1745, in filter_compiledir; "" '%s'. Check the permissions."" % path); ValueError: Unable to create the compiledir directory '/root/.theano/compiledir_Linux-4.10--generic-x86_64-with-debian-stretch-sid-x86_64-3.6.2-64'. Check the permissions. at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75); at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCuratedArgs(ScriptExecutor.java:126); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeArgs(PythonScriptExecutor.java:170); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeCommand(PythonScriptExecutor.java:79); at org.broadinst",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782
https://github.com/broadinstitute/gatk/issues/4782:6361,Modifiability,config,configparser,6361,".6/site-packages/theano/compile/__init__.py"", line 10, in <module>; from theano.compile.function_module import *; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/compile/function_module.py"", line 21, in <module>; import theano.compile.mode; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/compile/mode.py"", line 10, in <module>; import theano.gof.vm; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/vm.py"", line 662, in <module>; from . import lazylinker_c; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 42, in <module>; location = os.path.join(config.compiledir, 'lazylinker_ext'); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/confith.join(config.compiledir, 'lazylinker_ext'); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configparser.py"", line 333, in __get__; self.__set__(cls, val_str); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configparser.py"", line 344, in __set__; self.val = self.filter(val); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configdefaults.py"", line 1745, in filter_compiledir; "" '%s'. Check the permissions."" % path); ValueError: Unable to create the compiledir directory '/root/.theano/compiledir_Linux-4.10--generic-x86_64-with-debian-stretch-sid-x86_64-3.6.2-64'. Check the permissions. at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75); at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCuratedArgs(ScriptExecutor.java:126); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeArgs(PythonScriptExecutor.java:170); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeCommand(PythonScriptExecutor.java:79); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.checkPythonEnvironmentForPackage(PythonScriptExecutor.java:192); at org.broadinstit",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782
https://github.com/broadinstitute/gatk/issues/4782:6496,Modifiability,config,configdefaults,6496,"nvs/gatk/lib/python3.6/site-packages/theano/compile/function_module.py"", line 21, in <module>; import theano.compile.mode; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/compile/mode.py"", line 10, in <module>; import theano.gof.vm; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/vm.py"", line 662, in <module>; from . import lazylinker_c; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 42, in <module>; location = os.path.join(config.compiledir, 'lazylinker_ext'); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/confith.join(config.compiledir, 'lazylinker_ext'); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configparser.py"", line 333, in __get__; self.__set__(cls, val_str); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configparser.py"", line 344, in __set__; self.val = self.filter(val); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configdefaults.py"", line 1745, in filter_compiledir; "" '%s'. Check the permissions."" % path); ValueError: Unable to create the compiledir directory '/root/.theano/compiledir_Linux-4.10--generic-x86_64-with-debian-stretch-sid-x86_64-3.6.2-64'. Check the permissions. at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75); at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCuratedArgs(ScriptExecutor.java:126); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeArgs(PythonScriptExecutor.java:170); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeCommand(PythonScriptExecutor.java:79); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.checkPythonEnvironmentForPackage(PythonScriptExecutor.java:192); at org.broadinstitute.hellbender.tools.copynumber.DetermineGermlineContigPloidy.onStartup(DetermineGermlineContigPloidy.java:240); at org.broadinstitute.h",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782
https://github.com/broadinstitute/gatk/issues/4786:319,Deployability,update,updated,319,"This is for discussion... @jonn-smith @droazen . Currently, there seems to be a fair amount of code change in order to get Funcotator to work with a new gencode version. . For starters:; - the parser has to recognize the email address and version string; - the MAF output alias `MafOutputRendererConstants` needs to be updated. Is there a way to have this be more seamless? . Ideally, we would have no mandatory code changes. Maybe just warnings that would not block a user. Is this an issue at all? Are we going to scale back the flexibility of datasources?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4786
https://github.com/broadinstitute/gatk/issues/4788:3938,Availability,Avail,Available,3938,"roazen/google-cloud-java/tree/dr_all_nio_fixes. 18:58:01.494 INFO HaplotypeCaller - Initializing engine. 18:58:02.043 INFO HaplotypeCaller - Done initializing engine. 18:58:02.053 INFO HaplotypeCallerEngine - Standard Emitting and Calling confidence set to 0.0 for reference-model confidence output. 18:58:02.053 INFO HaplotypeCallerEngine - All sites annotated with PLs forced to true for reference-model confidence output. 18:58:02.886 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/bigdata/operations/pkgadmin/opt/linux/centos/7.x/x86_64/pkgs/gatk/4.0.1.2/gatk-package-4.0.1.2-local.jar!/com/intel/gkl/native/libgkl_utils.so. 18:58:02.888 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/bigdata/operations/pkgadmin/opt/linux/centos/7.x/x86_64/pkgs/gatk/4.0.1.2/gatk-package-4.0.1.2-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so. 18:58:02.932 WARN IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM. 18:58:02.933 INFO IntelPairHmm - Available threads: 8. 18:58:02.933 INFO IntelPairHmm - Requested threads: 4. 18:58:02.933 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation. 18:58:02.986 INFO ProgressMeter - Starting traversal. 18:58:02.987 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute. 18:58:12.992 INFO ProgressMeter - chr1:6969901 0.2 23290 139712.1. 18:58:22.989 INFO ProgressMeter - chr1:13470130 0.3 44960 134866.5. 18:58:32.991 INFO ProgressMeter - chr1:21393130 0.5 71370 142721.0. INFO 18:58:37,986 HelpFormatter - ---------------------------------------------------------------------------------- . INFO 18:58:37,989 HelpFormatter - The Genome Analysis Toolkit (GATK) v3.8-0-ge9d806836, Compiled 2017/07/28 21:26:50 . INFO 18:58:37,989 HelpFormatter - Copyright (c) 2010-2016 The Broad Institute . INFO 18:58:37,989 HelpFormatter - For support and documentation go to https://software.broadinstitute.org/gatk . INFO 18:58:37,989 HelpFormatt",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4788
https://github.com/broadinstitute/gatk/issues/4788:2864,Deployability,patch,patch,2864,"8:01.493 INFO HaplotypeCaller - Start Date/Time: May 17, 2018 6:58:01 PM PDT. 18:58:01.493 INFO HaplotypeCaller - ------------------------------------------------------------. 18:58:01.493 INFO HaplotypeCaller - ------------------------------------------------------------. 18:58:01.494 INFO HaplotypeCaller - HTSJDK Version: 2.14.1. 18:58:01.494 INFO HaplotypeCaller - Picard Version: 2.17.2. 18:58:01.494 INFO HaplotypeCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 1. 18:58:01.494 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false. 18:58:01.494 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true. 18:58:01.494 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false. 18:58:01.494 INFO HaplotypeCaller - Deflater: IntelDeflater. 18:58:01.494 INFO HaplotypeCaller - Inflater: IntelInflater. 18:58:01.494 INFO HaplotypeCaller - GCS max retries/reopens: 20. 18:58:01.494 INFO HaplotypeCaller - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes. 18:58:01.494 INFO HaplotypeCaller - Initializing engine. 18:58:02.043 INFO HaplotypeCaller - Done initializing engine. 18:58:02.053 INFO HaplotypeCallerEngine - Standard Emitting and Calling confidence set to 0.0 for reference-model confidence output. 18:58:02.053 INFO HaplotypeCallerEngine - All sites annotated with PLs forced to true for reference-model confidence output. 18:58:02.886 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/bigdata/operations/pkgadmin/opt/linux/centos/7.x/x86_64/pkgs/gatk/4.0.1.2/gatk-package-4.0.1.2-local.jar!/com/intel/gkl/native/libgkl_utils.so. 18:58:02.888 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/bigdata/operations/pkgadmin/opt/linux/centos/7.x/x86_64/pkgs/gatk/4.0.1.2/gatk-package-4.0.1.2-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so. 18:58:02.932 WARN IntelPairHmm - Flush-to-zero (FTZ) ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4788
https://github.com/broadinstitute/gatk/issues/4788:3401,Performance,Load,Loading,3401,_ASYNC_IO_READ_FOR_SAMTOOLS : false. 18:58:01.494 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true. 18:58:01.494 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false. 18:58:01.494 INFO HaplotypeCaller - Deflater: IntelDeflater. 18:58:01.494 INFO HaplotypeCaller - Inflater: IntelInflater. 18:58:01.494 INFO HaplotypeCaller - GCS max retries/reopens: 20. 18:58:01.494 INFO HaplotypeCaller - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes. 18:58:01.494 INFO HaplotypeCaller - Initializing engine. 18:58:02.043 INFO HaplotypeCaller - Done initializing engine. 18:58:02.053 INFO HaplotypeCallerEngine - Standard Emitting and Calling confidence set to 0.0 for reference-model confidence output. 18:58:02.053 INFO HaplotypeCallerEngine - All sites annotated with PLs forced to true for reference-model confidence output. 18:58:02.886 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/bigdata/operations/pkgadmin/opt/linux/centos/7.x/x86_64/pkgs/gatk/4.0.1.2/gatk-package-4.0.1.2-local.jar!/com/intel/gkl/native/libgkl_utils.so. 18:58:02.888 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/bigdata/operations/pkgadmin/opt/linux/centos/7.x/x86_64/pkgs/gatk/4.0.1.2/gatk-package-4.0.1.2-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so. 18:58:02.932 WARN IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM. 18:58:02.933 INFO IntelPairHmm - Available threads: 8. 18:58:02.933 INFO IntelPairHmm - Requested threads: 4. 18:58:02.933 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation. 18:58:02.986 INFO ProgressMeter - Starting traversal. 18:58:02.987 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute. 18:58:12.992 INFO ProgressMeter - chr1:6969901 0.2 23290 139712.1. 18:58:22.989 INFO ProgressMeter - chr1:13470130 0.3 449,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4788
https://github.com/broadinstitute/gatk/issues/4788:3624,Performance,Load,Loading,3624,":01.494 INFO HaplotypeCaller - Deflater: IntelDeflater. 18:58:01.494 INFO HaplotypeCaller - Inflater: IntelInflater. 18:58:01.494 INFO HaplotypeCaller - GCS max retries/reopens: 20. 18:58:01.494 INFO HaplotypeCaller - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes. 18:58:01.494 INFO HaplotypeCaller - Initializing engine. 18:58:02.043 INFO HaplotypeCaller - Done initializing engine. 18:58:02.053 INFO HaplotypeCallerEngine - Standard Emitting and Calling confidence set to 0.0 for reference-model confidence output. 18:58:02.053 INFO HaplotypeCallerEngine - All sites annotated with PLs forced to true for reference-model confidence output. 18:58:02.886 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/bigdata/operations/pkgadmin/opt/linux/centos/7.x/x86_64/pkgs/gatk/4.0.1.2/gatk-package-4.0.1.2-local.jar!/com/intel/gkl/native/libgkl_utils.so. 18:58:02.888 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/bigdata/operations/pkgadmin/opt/linux/centos/7.x/x86_64/pkgs/gatk/4.0.1.2/gatk-package-4.0.1.2-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so. 18:58:02.932 WARN IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM. 18:58:02.933 INFO IntelPairHmm - Available threads: 8. 18:58:02.933 INFO IntelPairHmm - Requested threads: 4. 18:58:02.933 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation. 18:58:02.986 INFO ProgressMeter - Starting traversal. 18:58:02.987 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute. 18:58:12.992 INFO ProgressMeter - chr1:6969901 0.2 23290 139712.1. 18:58:22.989 INFO ProgressMeter - chr1:13470130 0.3 44960 134866.5. 18:58:32.991 INFO ProgressMeter - chr1:21393130 0.5 71370 142721.0. INFO 18:58:37,986 HelpFormatter - ---------------------------------------------------------------------------------- . INFO 18:58:37,989 HelpForm",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4788
https://github.com/broadinstitute/gatk/issues/4788:4060,Performance,multi-thread,multi-threaded,4060,"er - Done initializing engine. 18:58:02.053 INFO HaplotypeCallerEngine - Standard Emitting and Calling confidence set to 0.0 for reference-model confidence output. 18:58:02.053 INFO HaplotypeCallerEngine - All sites annotated with PLs forced to true for reference-model confidence output. 18:58:02.886 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/bigdata/operations/pkgadmin/opt/linux/centos/7.x/x86_64/pkgs/gatk/4.0.1.2/gatk-package-4.0.1.2-local.jar!/com/intel/gkl/native/libgkl_utils.so. 18:58:02.888 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/bigdata/operations/pkgadmin/opt/linux/centos/7.x/x86_64/pkgs/gatk/4.0.1.2/gatk-package-4.0.1.2-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so. 18:58:02.932 WARN IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM. 18:58:02.933 INFO IntelPairHmm - Available threads: 8. 18:58:02.933 INFO IntelPairHmm - Requested threads: 4. 18:58:02.933 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation. 18:58:02.986 INFO ProgressMeter - Starting traversal. 18:58:02.987 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute. 18:58:12.992 INFO ProgressMeter - chr1:6969901 0.2 23290 139712.1. 18:58:22.989 INFO ProgressMeter - chr1:13470130 0.3 44960 134866.5. 18:58:32.991 INFO ProgressMeter - chr1:21393130 0.5 71370 142721.0. INFO 18:58:37,986 HelpFormatter - ---------------------------------------------------------------------------------- . INFO 18:58:37,989 HelpFormatter - The Genome Analysis Toolkit (GATK) v3.8-0-ge9d806836, Compiled 2017/07/28 21:26:50 . INFO 18:58:37,989 HelpFormatter - Copyright (c) 2010-2016 The Broad Institute . INFO 18:58:37,989 HelpFormatter - For support and documentation go to https://software.broadinstitute.org/gatk . INFO 18:58:37,989 HelpFormatter - [Thu May 17 18:58:37 PDT 2018] Executing on Linux 3.10.0-693.11.6.el7.x86_64 amd64 . INFO 18:58:37,989 HelpFormatter - Java HotSpot",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4788
https://github.com/broadinstitute/gatk/issues/4788:410,Testability,log,log,410,"Here is my command:; `gatk --java-options ""-Xmx32g"" HaplotypeCaller \; -R ucsc.hg19.fasta \; -I ${result}/${outf}_recal_reads.bam \; -O ${result}/${outf}.g.vcf.gz \; -G StandardAnnotation \; -G StandardHCAnnotation \; -genotyping-mode DISCOVERY \; -ERC GVCF \; -bamout ${result}/${outf}_HCbamout.bam`. I suspect there could be issue of memory. Is there any other alternative causes?; The last few lines of the log seems like HaplotypeCaller got cut short and VariantCalibrator jumped in.; Here is VariantRecalibrator command:; java -Xmx6g -jar GenomeAnalysisTK.jar \; -T VariantRecalibrator \; -R ${refgen} \; -input ${result}/${outf}.g.vcf.gz \; -resource:hapmap,known=false,training=true,truth=true,prior=15.0 ${hapmap} \; -resource:omni,known=false,training=true,truth=false,prior=12.0 ${omni} \; -resource:1000G,known=false,training=true,truth=false,prior=10.0 ${phaseonesnp} \; -resource:dbsnp,known=true,training=false,truth=false,prior=2.0 ${dbsnp} \; -an DP \; -an QD \; -an FS \; -an MQRankSum \; -an ReadPosRankSum \; -mode SNP \; -tranche 100.0 \; -tranche 99.9 \; -tranche 99.0 \; -tranche 90.0 \; -recalFile ${result}/${outf}_recalibrate_SNP.recal \; -tranchesFile ${result}/${outf}_recalibrate_SNP.tranches \; -rscriptFile ${result}/${outf}_recalibrate_SNP_plots.R. Here is the log:; `package-4.0.1.2-local.jar!/com/intel/gkl/native/libgkl_compression.so. 18:58:01.493 INFO HaplotypeCaller - ------------------------------------------------------------. 18:58:01.493 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.0.1.2. 18:58:01.493 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/. 18:58:01.493 INFO HaplotypeCaller - Executing as tranahg@i23 on Linux v3.10.0-693.11.6.el7.x86_64 amd64. 18:58:01.493 INFO HaplotypeCaller - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_45-b14. 18:58:01.493 INFO HaplotypeCaller - Start Date/Time: May 17, 2018 6:58:01 PM PDT. 18:58:01.493 INFO HaplotypeCaller - ---------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4788
https://github.com/broadinstitute/gatk/issues/4788:1292,Testability,log,log,1292,"t.bam`. I suspect there could be issue of memory. Is there any other alternative causes?; The last few lines of the log seems like HaplotypeCaller got cut short and VariantCalibrator jumped in.; Here is VariantRecalibrator command:; java -Xmx6g -jar GenomeAnalysisTK.jar \; -T VariantRecalibrator \; -R ${refgen} \; -input ${result}/${outf}.g.vcf.gz \; -resource:hapmap,known=false,training=true,truth=true,prior=15.0 ${hapmap} \; -resource:omni,known=false,training=true,truth=false,prior=12.0 ${omni} \; -resource:1000G,known=false,training=true,truth=false,prior=10.0 ${phaseonesnp} \; -resource:dbsnp,known=true,training=false,truth=false,prior=2.0 ${dbsnp} \; -an DP \; -an QD \; -an FS \; -an MQRankSum \; -an ReadPosRankSum \; -mode SNP \; -tranche 100.0 \; -tranche 99.9 \; -tranche 99.0 \; -tranche 90.0 \; -recalFile ${result}/${outf}_recalibrate_SNP.recal \; -tranchesFile ${result}/${outf}_recalibrate_SNP.tranches \; -rscriptFile ${result}/${outf}_recalibrate_SNP_plots.R. Here is the log:; `package-4.0.1.2-local.jar!/com/intel/gkl/native/libgkl_compression.so. 18:58:01.493 INFO HaplotypeCaller - ------------------------------------------------------------. 18:58:01.493 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.0.1.2. 18:58:01.493 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/. 18:58:01.493 INFO HaplotypeCaller - Executing as tranahg@i23 on Linux v3.10.0-693.11.6.el7.x86_64 amd64. 18:58:01.493 INFO HaplotypeCaller - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_45-b14. 18:58:01.493 INFO HaplotypeCaller - Start Date/Time: May 17, 2018 6:58:01 PM PDT. 18:58:01.493 INFO HaplotypeCaller - ------------------------------------------------------------. 18:58:01.493 INFO HaplotypeCaller - ------------------------------------------------------------. 18:58:01.494 INFO HaplotypeCaller - HTSJDK Version: 2.14.1. 18:58:01.494 INFO HaplotypeCaller - Picard Version: 2.17.2. 18:58:01.494 INFO HaplotypeC",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4788
https://github.com/broadinstitute/gatk/pull/4789:3257,Availability,down,down,3257,"code, theoretically, allows for arbitrarily complex rearrangement; shown above on the right is a table of inversion calls (TP/FP 12/7 using PacBio calls on CHM-1 & 13 cell lines as truth) extracted from the `<CPX>` calls, they were extracted by the tool proposed in PR #4602. #### stages:. * Primitive filter on breakpoints ; * low MQ of assembly contigs' mappings that evidenced the BND records, ; * suspiciously large distance between mates (mate pairs whose distance are over $10^5$bp (~1/3 of input, see blelow) are more likely to be artifact or dispersed/segmental duplications). <p align=""center""><img src=""https://user-images.githubusercontent.com/16310888/40271740-6daa2b9e-5b6f-11e8-9dbb-89085450db6d.png"" width=""420"" height=""420"" ></p>; * if overlaps with CPX (supposedly they should be captured already, or is more complex than what can be comprehended by the logic proposed here). The mates are then converted to intervals bounded by the mates' locations. These ""normal sized"" variants are sent down for further analysis. * Filtering based on overlap signatures. Here we have several possible scenarios (total ~130 pairs of mates):; * no overlappers (~ 50 mate pairs, balanced between ++/--); * multiple overlappers (~ 10 mate pairs, balanced between ++/--); * unique overlapping pairs of mate pairs (~ 60 mate pairs); * which overlaps with same type (++/++ or --/--, ~ 10 mate pairs); * which overlaps with opposite type (++/-- overlap, ~ 50 mate pairs). These are the overlapping pairs sent down for breakpoint linking, expecting a maximum of 20~30 inversion calls. * Type inference. Here we have four possible cases, each signaling what could be involved (primed block is inverted):; * INV55 interval left/right boundary upstream of INV33 interval's left/right boundary: `ABC -> B'`; * INV33 interval left/right boundary upstream of INV55 interval's left/right boundary: `ABC -> AC'B'A'C`; * INV33 interval contains INV55 interval: `ABC -> ABA' or AB'A'`; * INV55 interval contains INV",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4789
https://github.com/broadinstitute/gatk/pull/4789:3755,Availability,down,down,3755,"e likely to be artifact or dispersed/segmental duplications). <p align=""center""><img src=""https://user-images.githubusercontent.com/16310888/40271740-6daa2b9e-5b6f-11e8-9dbb-89085450db6d.png"" width=""420"" height=""420"" ></p>; * if overlaps with CPX (supposedly they should be captured already, or is more complex than what can be comprehended by the logic proposed here). The mates are then converted to intervals bounded by the mates' locations. These ""normal sized"" variants are sent down for further analysis. * Filtering based on overlap signatures. Here we have several possible scenarios (total ~130 pairs of mates):; * no overlappers (~ 50 mate pairs, balanced between ++/--); * multiple overlappers (~ 10 mate pairs, balanced between ++/--); * unique overlapping pairs of mate pairs (~ 60 mate pairs); * which overlaps with same type (++/++ or --/--, ~ 10 mate pairs); * which overlaps with opposite type (++/-- overlap, ~ 50 mate pairs). These are the overlapping pairs sent down for breakpoint linking, expecting a maximum of 20~30 inversion calls. * Type inference. Here we have four possible cases, each signaling what could be involved (primed block is inverted):; * INV55 interval left/right boundary upstream of INV33 interval's left/right boundary: `ABC -> B'`; * INV33 interval left/right boundary upstream of INV55 interval's left/right boundary: `ABC -> AC'B'A'C`; * INV33 interval contains INV55 interval: `ABC -> ABA' or AB'A'`; * INV55 interval contains INV33 interval: `ABC -> C'BC or C'B'C`. Note that for the last two cases, where the inverted dispersed duplication is guaranteed, the two possible alternate alleles are reverse complement&mdash;inversion&mdash;of each other, hence signatures of contig alignments along is not enough, and alignments of short reads within the affected region cannot break the degeneracy either.; So we need to attach left and right flanking regions to the affected region, and align short reads back to these two haplotypes and study the pair o",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4789
https://github.com/broadinstitute/gatk/pull/4789:278,Deployability,pipeline,pipeline,278,"This is for demo purpose only, so the code is not ready yet to be merged:. ## Description. ### background & goal; Currently, there are two parallel code path for structural variation breakpoint location and type inference using local assembly contig alignment signatures in the pipeline `StructuralVariationDiscoveryPipelineSpark`. * the stable code path: scanning neighbor chimeric alignment pairs of a contig iteratively and outputs inversion breakpoints as symbolic variant `<INV>`, annotated with `INV55` and `INV33` for signaling if it is the left or right breakpoint of the assumed inversion.; * the experimental code path that separates the alignment pre-processing step from the inference step, and studying the alignments in whole; this code path, in addition to outputting insertion, deletion and small duplication calls as does the stable path, outputs ; * BND records representing assembled breakpoints for which type could not be completely determined using only the contig alignments; this includes supposedly inversion breakpoints; * complex (`<CPX>`) variants from assembly contigs with more than 2 alignments; ; The tool proposed in this PR is based on [manual review](https://github.com/broadinstitute/dsde-methods-sv/tree/sh_inv_filter_init/docs/knowledgeBase/variantReview/inversion/chm) of a callset generated a long time ago (but still useful for studying filtering inversion breakpoints), and is designed to be integrated with the experimental code path. ### proposed algo. #### input:; * the ""INV55/INV33""-annotated `BND` records output by the upstream experimental code path; * BND's have related concepts of `MATE` and `PARTNER` (see figure below, left); * `MATE`: novel adjacency, i.e. contiguity on sample that is absent on reference (e.g. mobile element insertions, deletions); * `PARTNER`: novel disruption, i.e. contiguity on reference disrupted on sample (e.g. insertions, deletions). ![inversion_demo](https://user-images.githubusercontent.com/16310888/40271739-6d999b",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4789
https://github.com/broadinstitute/gatk/pull/4789:1434,Deployability,integrat,integrated,1434,"table code path: scanning neighbor chimeric alignment pairs of a contig iteratively and outputs inversion breakpoints as symbolic variant `<INV>`, annotated with `INV55` and `INV33` for signaling if it is the left or right breakpoint of the assumed inversion.; * the experimental code path that separates the alignment pre-processing step from the inference step, and studying the alignments in whole; this code path, in addition to outputting insertion, deletion and small duplication calls as does the stable path, outputs ; * BND records representing assembled breakpoints for which type could not be completely determined using only the contig alignments; this includes supposedly inversion breakpoints; * complex (`<CPX>`) variants from assembly contigs with more than 2 alignments; ; The tool proposed in this PR is based on [manual review](https://github.com/broadinstitute/dsde-methods-sv/tree/sh_inv_filter_init/docs/knowledgeBase/variantReview/inversion/chm) of a callset generated a long time ago (but still useful for studying filtering inversion breakpoints), and is designed to be integrated with the experimental code path. ### proposed algo. #### input:; * the ""INV55/INV33""-annotated `BND` records output by the upstream experimental code path; * BND's have related concepts of `MATE` and `PARTNER` (see figure below, left); * `MATE`: novel adjacency, i.e. contiguity on sample that is absent on reference (e.g. mobile element insertions, deletions); * `PARTNER`: novel disruption, i.e. contiguity on reference disrupted on sample (e.g. insertions, deletions). ![inversion_demo](https://user-images.githubusercontent.com/16310888/40271739-6d999b30-5b6f-11e8-86db-78fa11db4305.png). * complex variants detected by the upstream experimental code path; the reason is that sometimes inversion calls are incorporated as part of a larger, more complex event and the logic implemented in the upstream code, theoretically, allows for arbitrarily complex rearrangement; shown above on the rig",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4789
https://github.com/broadinstitute/gatk/pull/4789:1434,Integrability,integrat,integrated,1434,"table code path: scanning neighbor chimeric alignment pairs of a contig iteratively and outputs inversion breakpoints as symbolic variant `<INV>`, annotated with `INV55` and `INV33` for signaling if it is the left or right breakpoint of the assumed inversion.; * the experimental code path that separates the alignment pre-processing step from the inference step, and studying the alignments in whole; this code path, in addition to outputting insertion, deletion and small duplication calls as does the stable path, outputs ; * BND records representing assembled breakpoints for which type could not be completely determined using only the contig alignments; this includes supposedly inversion breakpoints; * complex (`<CPX>`) variants from assembly contigs with more than 2 alignments; ; The tool proposed in this PR is based on [manual review](https://github.com/broadinstitute/dsde-methods-sv/tree/sh_inv_filter_init/docs/knowledgeBase/variantReview/inversion/chm) of a callset generated a long time ago (but still useful for studying filtering inversion breakpoints), and is designed to be integrated with the experimental code path. ### proposed algo. #### input:; * the ""INV55/INV33""-annotated `BND` records output by the upstream experimental code path; * BND's have related concepts of `MATE` and `PARTNER` (see figure below, left); * `MATE`: novel adjacency, i.e. contiguity on sample that is absent on reference (e.g. mobile element insertions, deletions); * `PARTNER`: novel disruption, i.e. contiguity on reference disrupted on sample (e.g. insertions, deletions). ![inversion_demo](https://user-images.githubusercontent.com/16310888/40271739-6d999b30-5b6f-11e8-86db-78fa11db4305.png). * complex variants detected by the upstream experimental code path; the reason is that sometimes inversion calls are incorporated as part of a larger, more complex event and the logic implemented in the upstream code, theoretically, allows for arbitrarily complex rearrangement; shown above on the rig",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4789
https://github.com/broadinstitute/gatk/pull/4789:2057,Safety,detect,detected,2057,"riantReview/inversion/chm) of a callset generated a long time ago (but still useful for studying filtering inversion breakpoints), and is designed to be integrated with the experimental code path. ### proposed algo. #### input:; * the ""INV55/INV33""-annotated `BND` records output by the upstream experimental code path; * BND's have related concepts of `MATE` and `PARTNER` (see figure below, left); * `MATE`: novel adjacency, i.e. contiguity on sample that is absent on reference (e.g. mobile element insertions, deletions); * `PARTNER`: novel disruption, i.e. contiguity on reference disrupted on sample (e.g. insertions, deletions). ![inversion_demo](https://user-images.githubusercontent.com/16310888/40271739-6d999b30-5b6f-11e8-86db-78fa11db4305.png). * complex variants detected by the upstream experimental code path; the reason is that sometimes inversion calls are incorporated as part of a larger, more complex event and the logic implemented in the upstream code, theoretically, allows for arbitrarily complex rearrangement; shown above on the right is a table of inversion calls (TP/FP 12/7 using PacBio calls on CHM-1 & 13 cell lines as truth) extracted from the `<CPX>` calls, they were extracted by the tool proposed in PR #4602. #### stages:. * Primitive filter on breakpoints ; * low MQ of assembly contigs' mappings that evidenced the BND records, ; * suspiciously large distance between mates (mate pairs whose distance are over $10^5$bp (~1/3 of input, see blelow) are more likely to be artifact or dispersed/segmental duplications). <p align=""center""><img src=""https://user-images.githubusercontent.com/16310888/40271740-6daa2b9e-5b6f-11e8-9dbb-89085450db6d.png"" width=""420"" height=""420"" ></p>; * if overlaps with CPX (supposedly they should be captured already, or is more complex than what can be comprehended by the logic proposed here). The mates are then converted to intervals bounded by the mates' locations. These ""normal sized"" variants are sent down for further analysi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4789
https://github.com/broadinstitute/gatk/pull/4789:2216,Testability,log,logic,2216,"riantReview/inversion/chm) of a callset generated a long time ago (but still useful for studying filtering inversion breakpoints), and is designed to be integrated with the experimental code path. ### proposed algo. #### input:; * the ""INV55/INV33""-annotated `BND` records output by the upstream experimental code path; * BND's have related concepts of `MATE` and `PARTNER` (see figure below, left); * `MATE`: novel adjacency, i.e. contiguity on sample that is absent on reference (e.g. mobile element insertions, deletions); * `PARTNER`: novel disruption, i.e. contiguity on reference disrupted on sample (e.g. insertions, deletions). ![inversion_demo](https://user-images.githubusercontent.com/16310888/40271739-6d999b30-5b6f-11e8-86db-78fa11db4305.png). * complex variants detected by the upstream experimental code path; the reason is that sometimes inversion calls are incorporated as part of a larger, more complex event and the logic implemented in the upstream code, theoretically, allows for arbitrarily complex rearrangement; shown above on the right is a table of inversion calls (TP/FP 12/7 using PacBio calls on CHM-1 & 13 cell lines as truth) extracted from the `<CPX>` calls, they were extracted by the tool proposed in PR #4602. #### stages:. * Primitive filter on breakpoints ; * low MQ of assembly contigs' mappings that evidenced the BND records, ; * suspiciously large distance between mates (mate pairs whose distance are over $10^5$bp (~1/3 of input, see blelow) are more likely to be artifact or dispersed/segmental duplications). <p align=""center""><img src=""https://user-images.githubusercontent.com/16310888/40271740-6daa2b9e-5b6f-11e8-9dbb-89085450db6d.png"" width=""420"" height=""420"" ></p>; * if overlaps with CPX (supposedly they should be captured already, or is more complex than what can be comprehended by the logic proposed here). The mates are then converted to intervals bounded by the mates' locations. These ""normal sized"" variants are sent down for further analysi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4789
https://github.com/broadinstitute/gatk/pull/4789:3121,Testability,log,logic,3121,"iants detected by the upstream experimental code path; the reason is that sometimes inversion calls are incorporated as part of a larger, more complex event and the logic implemented in the upstream code, theoretically, allows for arbitrarily complex rearrangement; shown above on the right is a table of inversion calls (TP/FP 12/7 using PacBio calls on CHM-1 & 13 cell lines as truth) extracted from the `<CPX>` calls, they were extracted by the tool proposed in PR #4602. #### stages:. * Primitive filter on breakpoints ; * low MQ of assembly contigs' mappings that evidenced the BND records, ; * suspiciously large distance between mates (mate pairs whose distance are over $10^5$bp (~1/3 of input, see blelow) are more likely to be artifact or dispersed/segmental duplications). <p align=""center""><img src=""https://user-images.githubusercontent.com/16310888/40271740-6daa2b9e-5b6f-11e8-9dbb-89085450db6d.png"" width=""420"" height=""420"" ></p>; * if overlaps with CPX (supposedly they should be captured already, or is more complex than what can be comprehended by the logic proposed here). The mates are then converted to intervals bounded by the mates' locations. These ""normal sized"" variants are sent down for further analysis. * Filtering based on overlap signatures. Here we have several possible scenarios (total ~130 pairs of mates):; * no overlappers (~ 50 mate pairs, balanced between ++/--); * multiple overlappers (~ 10 mate pairs, balanced between ++/--); * unique overlapping pairs of mate pairs (~ 60 mate pairs); * which overlaps with same type (++/++ or --/--, ~ 10 mate pairs); * which overlaps with opposite type (++/-- overlap, ~ 50 mate pairs). These are the overlapping pairs sent down for breakpoint linking, expecting a maximum of 20~30 inversion calls. * Type inference. Here we have four possible cases, each signaling what could be involved (primed block is inverted):; * INV55 interval left/right boundary upstream of INV33 interval's left/right boundary: `ABC -> B'`; * ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4789
https://github.com/broadinstitute/gatk/pull/4789:5422,Usability,undo,undoubtedly,5422,"INV55 interval's left/right boundary: `ABC -> AC'B'A'C`; * INV33 interval contains INV55 interval: `ABC -> ABA' or AB'A'`; * INV55 interval contains INV33 interval: `ABC -> C'BC or C'B'C`. Note that for the last two cases, where the inverted dispersed duplication is guaranteed, the two possible alternate alleles are reverse complement&mdash;inversion&mdash;of each other, hence signatures of contig alignments along is not enough, and alignments of short reads within the affected region cannot break the degeneracy either.; So we need to attach left and right flanking regions to the affected region, and align short reads back to these two haplotypes and study the pair orientations of the alignments to break the degeneracy. #### output:. * VCF containing the inversion and flanking deletion and dispersed duplication calls (together with CPX-derived inversion calls, total number of `INV` calls are ~ 30). * BED file on filtered BND's citing reason for filtering. Relevant files, including an IGV session can be found in this [bucket](https://console.cloud.google.com/storage/browser/broad-dsde-methods/shuang/archive/inversion-algo-demo/?project=broad-dsde-methods&organizationId=548622027621). ## Todo:. * Large inversions. The primitive size-based filtering step and the filtering step requiring matched mate pairs undoubtedly will cause us false negatives, as sometimes we don't expect assembly of all breakpoints for inversions complicated by copy number events. The inversions currently captured tend to be small inversions; ; ```; Min. 1st Qu. Median Mean 3rd Qu. Max.; 77.0 188.0 359.0 1144.5 823.2 12697.0; ```; ; * run check against reference annotation: known Seg. Dup., RC-STR, centromere, as well as consistent pair support from short reads; ; * __Question: is this pre-filtering a bad idea; if not, how to report?__. * __Question: is the number $10^5$ reasonable?__; ; * __Question: any other suggestion on filtering criteria?__; ; * Corner cases here & there in the proposed tool",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4789
https://github.com/broadinstitute/gatk/issues/4791:198,Modifiability,config,config,198,"This is because the GencodeFuncotationFactory will force the datasource name (`getName()`) to return ""Gencode"". However, some areas of the code (e.g. `Funcotator.java`) will query the name from the config file. If these do not match, confusion ensues. You will get IGRs for everything, since all queries into the datasource will yield no found features/transcripts.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4791
https://github.com/broadinstitute/gatk/pull/4793:22,Energy Efficiency,reduce,reduce,22,"HC priors will let us reduce the WEx GVCF footprint. As a consequence, CalculateGenotypePosteriors now supports indels. I fixed a bug and changed the args for CGP. We didn't have great tests, but CGP results will be fixed/improved in some cases.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4793
https://github.com/broadinstitute/gatk/pull/4793:185,Testability,test,tests,185,"HC priors will let us reduce the WEx GVCF footprint. As a consequence, CalculateGenotypePosteriors now supports indels. I fixed a bug and changed the args for CGP. We didn't have great tests, but CGP results will be fixed/improved in some cases.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4793
https://github.com/broadinstitute/gatk/issues/4794:603,Availability,error,error,603,"I am trying to use FilterVariantTranches in GATK 4.0.3.0 after running CNNScoreVariants. ```; ./gatk FilterVariantTranches \; -V test.cnnscore.vcf \; --snp-truth-vcf hapmap_3.3.hg19.sites.vcf \; --indel-truth-vcf Mills_and_1000G_gold_standard.indels.hg19.sites.vcf \; --info-key CNN_1D \; --tranche 99.9 --tranche 99.0 --tranche 95 \; --max-sites 8000 \; -O test.cnnscore.filtered.vcf; ```. There are also index files in the directory. ```; test.cnnscore.vcf.idx (generated by CNNScoreVariants); hapmap_3.3.hg19.sites.vcf.idx; Mills_and_1000G_gold_standard.indels.hg19.sites.vcf.idx; ```. and I got the error. ```; Traceback (most recent call last):; File ""/tmp/zzxzxzzxz/tranches.5887233932112211461.py"", line 124, in <module>; run(); File ""/tmp/zzxzxzzxz/tranches.5887233932112211461.py"", line 10, in run; write_tranches(args); File ""/tmp/zzxzxzzxz/tranches.5887233932112211461.py"", line 34, in write_tranches; v_scored = allele_in_vcf(allele, variant, vcf_reader); File ""/tmp/zzxzxzzxz/tranches.5887233932112211461.py"", line 84, in allele_in_vcf; variants = vcf_ram.fetch(variant.contig, variant.pos-1, variant.pos); File ""pysam/libcbcf.pyx"", line 4321, in pysam.libcbcf.VariantFile.fetch; ValueError: fetch requires an index. at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75); at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCuratedArgs(ScriptExecutor.java:126); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeArgs(PythonScriptExecutor.java:170); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeScript(PythonScriptExecutor.java:151); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeScript(PythonScriptExecutor.java:121); at org.broadinstitute.hellbender.tools.walkers.vqsr.FilterVariantTranches.doWork(FilterVariantTranches.java:134); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); at",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4794
https://github.com/broadinstitute/gatk/issues/4794:129,Testability,test,test,129,"I am trying to use FilterVariantTranches in GATK 4.0.3.0 after running CNNScoreVariants. ```; ./gatk FilterVariantTranches \; -V test.cnnscore.vcf \; --snp-truth-vcf hapmap_3.3.hg19.sites.vcf \; --indel-truth-vcf Mills_and_1000G_gold_standard.indels.hg19.sites.vcf \; --info-key CNN_1D \; --tranche 99.9 --tranche 99.0 --tranche 95 \; --max-sites 8000 \; -O test.cnnscore.filtered.vcf; ```. There are also index files in the directory. ```; test.cnnscore.vcf.idx (generated by CNNScoreVariants); hapmap_3.3.hg19.sites.vcf.idx; Mills_and_1000G_gold_standard.indels.hg19.sites.vcf.idx; ```. and I got the error. ```; Traceback (most recent call last):; File ""/tmp/zzxzxzzxz/tranches.5887233932112211461.py"", line 124, in <module>; run(); File ""/tmp/zzxzxzzxz/tranches.5887233932112211461.py"", line 10, in run; write_tranches(args); File ""/tmp/zzxzxzzxz/tranches.5887233932112211461.py"", line 34, in write_tranches; v_scored = allele_in_vcf(allele, variant, vcf_reader); File ""/tmp/zzxzxzzxz/tranches.5887233932112211461.py"", line 84, in allele_in_vcf; variants = vcf_ram.fetch(variant.contig, variant.pos-1, variant.pos); File ""pysam/libcbcf.pyx"", line 4321, in pysam.libcbcf.VariantFile.fetch; ValueError: fetch requires an index. at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75); at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCuratedArgs(ScriptExecutor.java:126); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeArgs(PythonScriptExecutor.java:170); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeScript(PythonScriptExecutor.java:151); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeScript(PythonScriptExecutor.java:121); at org.broadinstitute.hellbender.tools.walkers.vqsr.FilterVariantTranches.doWork(FilterVariantTranches.java:134); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); at",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4794
https://github.com/broadinstitute/gatk/issues/4794:358,Testability,test,test,358,"I am trying to use FilterVariantTranches in GATK 4.0.3.0 after running CNNScoreVariants. ```; ./gatk FilterVariantTranches \; -V test.cnnscore.vcf \; --snp-truth-vcf hapmap_3.3.hg19.sites.vcf \; --indel-truth-vcf Mills_and_1000G_gold_standard.indels.hg19.sites.vcf \; --info-key CNN_1D \; --tranche 99.9 --tranche 99.0 --tranche 95 \; --max-sites 8000 \; -O test.cnnscore.filtered.vcf; ```. There are also index files in the directory. ```; test.cnnscore.vcf.idx (generated by CNNScoreVariants); hapmap_3.3.hg19.sites.vcf.idx; Mills_and_1000G_gold_standard.indels.hg19.sites.vcf.idx; ```. and I got the error. ```; Traceback (most recent call last):; File ""/tmp/zzxzxzzxz/tranches.5887233932112211461.py"", line 124, in <module>; run(); File ""/tmp/zzxzxzzxz/tranches.5887233932112211461.py"", line 10, in run; write_tranches(args); File ""/tmp/zzxzxzzxz/tranches.5887233932112211461.py"", line 34, in write_tranches; v_scored = allele_in_vcf(allele, variant, vcf_reader); File ""/tmp/zzxzxzzxz/tranches.5887233932112211461.py"", line 84, in allele_in_vcf; variants = vcf_ram.fetch(variant.contig, variant.pos-1, variant.pos); File ""pysam/libcbcf.pyx"", line 4321, in pysam.libcbcf.VariantFile.fetch; ValueError: fetch requires an index. at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75); at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCuratedArgs(ScriptExecutor.java:126); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeArgs(PythonScriptExecutor.java:170); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeScript(PythonScriptExecutor.java:151); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeScript(PythonScriptExecutor.java:121); at org.broadinstitute.hellbender.tools.walkers.vqsr.FilterVariantTranches.doWork(FilterVariantTranches.java:134); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); at",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4794
https://github.com/broadinstitute/gatk/issues/4794:441,Testability,test,test,441,"I am trying to use FilterVariantTranches in GATK 4.0.3.0 after running CNNScoreVariants. ```; ./gatk FilterVariantTranches \; -V test.cnnscore.vcf \; --snp-truth-vcf hapmap_3.3.hg19.sites.vcf \; --indel-truth-vcf Mills_and_1000G_gold_standard.indels.hg19.sites.vcf \; --info-key CNN_1D \; --tranche 99.9 --tranche 99.0 --tranche 95 \; --max-sites 8000 \; -O test.cnnscore.filtered.vcf; ```. There are also index files in the directory. ```; test.cnnscore.vcf.idx (generated by CNNScoreVariants); hapmap_3.3.hg19.sites.vcf.idx; Mills_and_1000G_gold_standard.indels.hg19.sites.vcf.idx; ```. and I got the error. ```; Traceback (most recent call last):; File ""/tmp/zzxzxzzxz/tranches.5887233932112211461.py"", line 124, in <module>; run(); File ""/tmp/zzxzxzzxz/tranches.5887233932112211461.py"", line 10, in run; write_tranches(args); File ""/tmp/zzxzxzzxz/tranches.5887233932112211461.py"", line 34, in write_tranches; v_scored = allele_in_vcf(allele, variant, vcf_reader); File ""/tmp/zzxzxzzxz/tranches.5887233932112211461.py"", line 84, in allele_in_vcf; variants = vcf_ram.fetch(variant.contig, variant.pos-1, variant.pos); File ""pysam/libcbcf.pyx"", line 4321, in pysam.libcbcf.VariantFile.fetch; ValueError: fetch requires an index. at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75); at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCuratedArgs(ScriptExecutor.java:126); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeArgs(PythonScriptExecutor.java:170); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeScript(PythonScriptExecutor.java:151); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeScript(PythonScriptExecutor.java:121); at org.broadinstitute.hellbender.tools.walkers.vqsr.FilterVariantTranches.doWork(FilterVariantTranches.java:134); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); at",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4794
https://github.com/broadinstitute/gatk/issues/4795:58,Deployability,pipeline,pipeline,58,"Allele-specific annotations are going into the production pipeline when we kick off exome reprocessing in a month or so. Lots of projects subset samples (and I think we may even do it in production for people we really like), which will lead to subsetting alleles, which will lead to invalid AS-annotations since they're not going to be split at all. Expected behavior is that any time alleles are subset (e.g. `SelectVariants -sn` or `VariantsToTable --split-mutli-allelic`), AS* annotations are also subset. (GenotypeGVCFs does this already when alleles are subset by QUAL score.)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4795
https://github.com/broadinstitute/gatk/issues/4797:47,Integrability,interface,interface,47,`serializeToVcfString` should not be be in the interface for Funcotation (see `Funcotation.java`). That should be the job of the VCFOutputRenderer to sanitize any strings. A Funcotation should not care whether it is being rendered to a VCF or MAF. It's poor separation of concerns.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4797
https://github.com/broadinstitute/gatk/issues/4797:150,Security,sanitiz,sanitize,150,`serializeToVcfString` should not be be in the interface for Funcotation (see `Funcotation.java`). That should be the job of the VCFOutputRenderer to sanitize any strings. A Funcotation should not care whether it is being rendered to a VCF or MAF. It's poor separation of concerns.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4797
https://github.com/broadinstitute/gatk/issues/4798:153,Safety,Detect,Detect,153,"Every datasource is checked twice, once w/o ""chr"" and once with it. This seems like overkill and inefficient. Several solutions could be implemented:. - Detect which is relevant and only search those. ; - Limit the searching to GENCODE only -- as opposed to all datasources.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4798
https://github.com/broadinstitute/gatk/pull/4800:57,Integrability,depend,dependencies,57,Rewrite of the FilterVariantTranches tool without python dependencies. Uses @takutosato's shiny new TwoPassVariantWalker. @takutosato or @cmnbroad care to review?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4800
https://github.com/broadinstitute/gatk/pull/4800:0,Modifiability,Rewrite,Rewrite,0,Rewrite of the FilterVariantTranches tool without python dependencies. Uses @takutosato's shiny new TwoPassVariantWalker. @takutosato or @cmnbroad care to review?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4800
https://github.com/broadinstitute/gatk/issues/4802:139,Availability,error,errors,139,"Before our cromwell/WDL tests even start to build the docker image, we could run womtool to validate the WDL. This will catch some obvious errors in much less time.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4802
https://github.com/broadinstitute/gatk/issues/4802:92,Security,validat,validate,92,"Before our cromwell/WDL tests even start to build the docker image, we could run womtool to validate the WDL. This will catch some obvious errors in much less time.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4802
https://github.com/broadinstitute/gatk/issues/4802:24,Testability,test,tests,24,"Before our cromwell/WDL tests even start to build the docker image, we could run womtool to validate the WDL. This will catch some obvious errors in much less time.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4802
https://github.com/broadinstitute/gatk/pull/4805:24,Deployability,update,updates,24,"- matches the bootstrap updates for formatting ; - minor text tweaks, displays version clearly; - adds version switching menu. These changes will make it easier to upload new version docs quickly. This should definitely go in before the next version release. . Resulting docs are live at https://software.broadinstitute.org/gatk/documentation/tooldocs",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4805
https://github.com/broadinstitute/gatk/pull/4805:250,Deployability,release,release,250,"- matches the bootstrap updates for formatting ; - minor text tweaks, displays version clearly; - adds version switching menu. These changes will make it easier to upload new version docs quickly. This should definitely go in before the next version release. . Resulting docs are live at https://software.broadinstitute.org/gatk/documentation/tooldocs",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4805
https://github.com/broadinstitute/gatk/pull/4805:87,Usability,clear,clearly,87,"- matches the bootstrap updates for formatting ; - minor text tweaks, displays version clearly; - adds version switching menu. These changes will make it easier to upload new version docs quickly. This should definitely go in before the next version release. . Resulting docs are live at https://software.broadinstitute.org/gatk/documentation/tooldocs",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4805
https://github.com/broadinstitute/gatk/issues/4806:316,Deployability,release,release,316,"Just realized CNV WDLs are not using NIO in FireCloud. This is as simple as changing `File` to `String` for supported files. Not sure if these need to live in our repo (I see we have a M2 NIO WDL), I'd be fine with them just living in FireCloud. @bshifaw would you be OK making the changes in FireCloud for the next release? If not, I can add an NIO version to the repo. @LeeTL1220 perhaps something to add to the style guide (if it's not already there)?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4806
https://github.com/broadinstitute/gatk/issues/4806:66,Usability,simpl,simple,66,"Just realized CNV WDLs are not using NIO in FireCloud. This is as simple as changing `File` to `String` for supported files. Not sure if these need to live in our repo (I see we have a M2 NIO WDL), I'd be fine with them just living in FireCloud. @bshifaw would you be OK making the changes in FireCloud for the next release? If not, I can add an NIO version to the repo. @LeeTL1220 perhaps something to add to the style guide (if it's not already there)?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4806
https://github.com/broadinstitute/gatk/issues/4806:420,Usability,guid,guide,420,"Just realized CNV WDLs are not using NIO in FireCloud. This is as simple as changing `File` to `String` for supported files. Not sure if these need to live in our repo (I see we have a M2 NIO WDL), I'd be fine with them just living in FireCloud. @bshifaw would you be OK making the changes in FireCloud for the next release? If not, I can add an NIO version to the repo. @LeeTL1220 perhaps something to add to the style guide (if it's not already there)?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4806
https://github.com/broadinstitute/gatk/issues/4807:60,Availability,error,errors,60,Change the json to enable running of funcotator and fix any errors that occur.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4807
https://github.com/broadinstitute/gatk/issues/4810:122,Availability,error,error,122,"- Appears in `VCFOutputRenderer`... not sure if this is an issue in the `MAFOutputRenderer`. I am not sure what the exact error is here, but how can I have multiple IGR funcotations for a single variant? I believe that a single variant can be IGR only or not IGR at all, even when looking at multiple transcripts. Perhaps the error is that the variant is being queried by largest gene footprint and then being rendered for transcripts that do not overlap the variant?. For example:; ```; |hg19|chr19|9091811|9091811|IGR||SNP|G|G|A|||+||||||0.5286783042394015|GAGGGTTTCAGCATGGACAGG|||hg19|chr19|9091811|9091811|IGR||SNP|G|G|A|||+||||||0.5286783042394015|GAGGGTTTCAGCATGGACAGG||MUC16|hg19|chr19|9091811|9091811|SILENT||SNP|G|G|A|g.chr19:9091811G>A|ENST00000397910.4|-|1|208|c.4C>T|c.(4-6)Ctg>Ttg|p.L2L|0.5286783042394015|GAGGGTTTCAGCATGGACAGG|IGR_ANNOTATON_%3B_IGR_ANNOTATON||hg19|chr19|9091811|9091811|IGR||SNP|G|G|A|||+||||||0.5286783042394015|GAGGGTTTCAGCATGGACAGG|||hg19|chr19|9091811|9091811|IGR||SNP|G|G|A|||+||||||0.5286783042394015|GAGGGTTTCAGCATGGACAGG||MUC16|hg19|chr19|9091811|9091811|SILENT||SNP|G|G|A|g.chr19:9091811G>A|ENST00000397910.4|-|1|208|c.4C>T|c.(4-6)Ctg>Ttg|p.L2L|0.5286783042394015|GAGGGTTTCAGCATGGACAGG|IGR_ANNOTATON_%3B_IGR_ANNOTATON; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4810
https://github.com/broadinstitute/gatk/issues/4810:326,Availability,error,error,326,"- Appears in `VCFOutputRenderer`... not sure if this is an issue in the `MAFOutputRenderer`. I am not sure what the exact error is here, but how can I have multiple IGR funcotations for a single variant? I believe that a single variant can be IGR only or not IGR at all, even when looking at multiple transcripts. Perhaps the error is that the variant is being queried by largest gene footprint and then being rendered for transcripts that do not overlap the variant?. For example:; ```; |hg19|chr19|9091811|9091811|IGR||SNP|G|G|A|||+||||||0.5286783042394015|GAGGGTTTCAGCATGGACAGG|||hg19|chr19|9091811|9091811|IGR||SNP|G|G|A|||+||||||0.5286783042394015|GAGGGTTTCAGCATGGACAGG||MUC16|hg19|chr19|9091811|9091811|SILENT||SNP|G|G|A|g.chr19:9091811G>A|ENST00000397910.4|-|1|208|c.4C>T|c.(4-6)Ctg>Ttg|p.L2L|0.5286783042394015|GAGGGTTTCAGCATGGACAGG|IGR_ANNOTATON_%3B_IGR_ANNOTATON||hg19|chr19|9091811|9091811|IGR||SNP|G|G|A|||+||||||0.5286783042394015|GAGGGTTTCAGCATGGACAGG|||hg19|chr19|9091811|9091811|IGR||SNP|G|G|A|||+||||||0.5286783042394015|GAGGGTTTCAGCATGGACAGG||MUC16|hg19|chr19|9091811|9091811|SILENT||SNP|G|G|A|g.chr19:9091811G>A|ENST00000397910.4|-|1|208|c.4C>T|c.(4-6)Ctg>Ttg|p.L2L|0.5286783042394015|GAGGGTTTCAGCATGGACAGG|IGR_ANNOTATON_%3B_IGR_ANNOTATON; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4810
https://github.com/broadinstitute/gatk/issues/4812:593,Usability,clear,clearly,593,"Some of the data in the COSMIC database contain invalid protein position information. For example, the protein position field here indicates a stop position before the start:; ```; CEBPA, ENST00000498907, 1077, 1833, 2404659, 2404659, 2267510, haematopoietic_and_lymphoid_tissue, NS, NS, NS, haematopoietic_neoplasm, acute_myeloid_leukaemia, NS, NS, n, COSM5065102, c.926_927ins24, p.V308_P39insDKAKQRNV, Insertion - In frame, het, u, 37, 19:33792394-33792395, -, , -, , , Variant of unknown origin, 20439648, , blood-bone marrow, NS, , ; ```. Specifically: **p.V308_P39insDKAKQRNV**. This is clearly a typo. However, for the moment we are ignoring these data and throwing a warning, rather than attempting to fix them or include them.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4812
https://github.com/broadinstitute/gatk/issues/4813:147,Usability,simpl,simply,147,"Hi there,. I am using ASEReadCounter and it works fine, but I did not find any other output than count tables. I'd like to count by haplotypes, or simply to be able to merge several snp counts together (because I know they are in LD, or they come from the same haplotype); If the tool could export read names per allele, instead of counts, I would be able to merge them together. But here I cannot merge the counts, since they can originate from the same read. Could you add an option to allow users to count per haplotypes/LD snps/group of snps, or simply output read names instead of count tables?. Thanks",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4813
https://github.com/broadinstitute/gatk/issues/4813:550,Usability,simpl,simply,550,"Hi there,. I am using ASEReadCounter and it works fine, but I did not find any other output than count tables. I'd like to count by haplotypes, or simply to be able to merge several snp counts together (because I know they are in LD, or they come from the same haplotype); If the tool could export read names per allele, instead of counts, I would be able to merge them together. But here I cannot merge the counts, since they can originate from the same read. Could you add an option to allow users to count per haplotypes/LD snps/group of snps, or simply output read names instead of count tables?. Thanks",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4813
https://github.com/broadinstitute/gatk/issues/4816:251,Availability,error,error,251,"If you look at the approximation going from equation 34 to 35 in https://github.com/broadinstitute/gatk/blob/master/docs/mutect/mutect.pdf you will find that we replace f(1 - e) + (1 - f)e by just f(1 - e), where f is the allele fraction and e is the error rate. When f is much bigger than e this is okay but when they are comparable (consider mitochondrial or cfDNA calling with f = 1% and base qualities of 25) the approximation breaks down and we significantly underestimate the log odds, thereby failing to consider a region active. This must be fixed!. @meganshand",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4816
https://github.com/broadinstitute/gatk/issues/4816:438,Availability,down,down,438,"If you look at the approximation going from equation 34 to 35 in https://github.com/broadinstitute/gatk/blob/master/docs/mutect/mutect.pdf you will find that we replace f(1 - e) + (1 - f)e by just f(1 - e), where f is the allele fraction and e is the error rate. When f is much bigger than e this is okay but when they are comparable (consider mitochondrial or cfDNA calling with f = 1% and base qualities of 25) the approximation breaks down and we significantly underestimate the log odds, thereby failing to consider a region active. This must be fixed!. @meganshand",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4816
https://github.com/broadinstitute/gatk/issues/4816:482,Testability,log,log,482,"If you look at the approximation going from equation 34 to 35 in https://github.com/broadinstitute/gatk/blob/master/docs/mutect/mutect.pdf you will find that we replace f(1 - e) + (1 - f)e by just f(1 - e), where f is the allele fraction and e is the error rate. When f is much bigger than e this is okay but when they are comparable (consider mitochondrial or cfDNA calling with f = 1% and base qualities of 25) the approximation breaks down and we significantly underestimate the log odds, thereby failing to consider a region active. This must be fixed!. @meganshand",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4816
https://github.com/broadinstitute/gatk/pull/4817:302,Availability,Repair,Repair,302,"Fixes #4739; Refactored UTR VariantClassification handling.; Added warning statement when a transcript in the UTR has no sequence info (now is the same behavior as in protein coding regions).; Added tests to prevent regression on data source date comparison bug.; Now can run on large data.; Fixed DNA Repair Genes getter script.; Fixed an issue in COSMIC to make it robust to bad COSMIC data.; Gencode no longer crashes when given an indel that starts just before an exon.; Fixed the SimpleKeyXsvFuncotationFactory to allow any characters to work as delimiters (including characters used in regular expressions, such as pipes).; Modified several methods to allow for negative start positions in; preparation for allowing indels that start outside exons.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4817
https://github.com/broadinstitute/gatk/pull/4817:367,Availability,robust,robust,367,"Fixes #4739; Refactored UTR VariantClassification handling.; Added warning statement when a transcript in the UTR has no sequence info (now is the same behavior as in protein coding regions).; Added tests to prevent regression on data source date comparison bug.; Now can run on large data.; Fixed DNA Repair Genes getter script.; Fixed an issue in COSMIC to make it robust to bad COSMIC data.; Gencode no longer crashes when given an indel that starts just before an exon.; Fixed the SimpleKeyXsvFuncotationFactory to allow any characters to work as delimiters (including characters used in regular expressions, such as pipes).; Modified several methods to allow for negative start positions in; preparation for allowing indels that start outside exons.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4817
https://github.com/broadinstitute/gatk/pull/4817:13,Modifiability,Refactor,Refactored,13,"Fixes #4739; Refactored UTR VariantClassification handling.; Added warning statement when a transcript in the UTR has no sequence info (now is the same behavior as in protein coding regions).; Added tests to prevent regression on data source date comparison bug.; Now can run on large data.; Fixed DNA Repair Genes getter script.; Fixed an issue in COSMIC to make it robust to bad COSMIC data.; Gencode no longer crashes when given an indel that starts just before an exon.; Fixed the SimpleKeyXsvFuncotationFactory to allow any characters to work as delimiters (including characters used in regular expressions, such as pipes).; Modified several methods to allow for negative start positions in; preparation for allowing indels that start outside exons.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4817
https://github.com/broadinstitute/gatk/pull/4817:199,Testability,test,tests,199,"Fixes #4739; Refactored UTR VariantClassification handling.; Added warning statement when a transcript in the UTR has no sequence info (now is the same behavior as in protein coding regions).; Added tests to prevent regression on data source date comparison bug.; Now can run on large data.; Fixed DNA Repair Genes getter script.; Fixed an issue in COSMIC to make it robust to bad COSMIC data.; Gencode no longer crashes when given an indel that starts just before an exon.; Fixed the SimpleKeyXsvFuncotationFactory to allow any characters to work as delimiters (including characters used in regular expressions, such as pipes).; Modified several methods to allow for negative start positions in; preparation for allowing indels that start outside exons.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4817
https://github.com/broadinstitute/gatk/pull/4817:485,Usability,Simpl,SimpleKeyXsvFuncotationFactory,485,"Fixes #4739; Refactored UTR VariantClassification handling.; Added warning statement when a transcript in the UTR has no sequence info (now is the same behavior as in protein coding regions).; Added tests to prevent regression on data source date comparison bug.; Now can run on large data.; Fixed DNA Repair Genes getter script.; Fixed an issue in COSMIC to make it robust to bad COSMIC data.; Gencode no longer crashes when given an indel that starts just before an exon.; Fixed the SimpleKeyXsvFuncotationFactory to allow any characters to work as delimiters (including characters used in regular expressions, such as pipes).; Modified several methods to allow for negative start positions in; preparation for allowing indels that start outside exons.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4817
https://github.com/broadinstitute/gatk/issues/4819:488,Availability,down,downstream,488,"Using https://jitpack.io/#broadinstitute/gatk for building SNAPSHOTS (independent on your jfrog repository, which has a expired date for snapshots - https://github.com/broadinstitute/gatk/issues/4565) is not working any longer due to the requirement of git-lfs (e.g., https://jitpack.io/com/github/broadinstitute/gatk/4.0.4.0/build.log). It will be nice to add a `jitpack.yml` file (see https://jitpack.io/docs/BUILDING/#custom-commands) to install dependencies - this will be useful for downstream project depending on SNAPSHOTS.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4819
https://github.com/broadinstitute/gatk/issues/4819:441,Deployability,install,install,441,"Using https://jitpack.io/#broadinstitute/gatk for building SNAPSHOTS (independent on your jfrog repository, which has a expired date for snapshots - https://github.com/broadinstitute/gatk/issues/4565) is not working any longer due to the requirement of git-lfs (e.g., https://jitpack.io/com/github/broadinstitute/gatk/4.0.4.0/build.log). It will be nice to add a `jitpack.yml` file (see https://jitpack.io/docs/BUILDING/#custom-commands) to install dependencies - this will be useful for downstream project depending on SNAPSHOTS.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4819
https://github.com/broadinstitute/gatk/issues/4819:449,Integrability,depend,dependencies,449,"Using https://jitpack.io/#broadinstitute/gatk for building SNAPSHOTS (independent on your jfrog repository, which has a expired date for snapshots - https://github.com/broadinstitute/gatk/issues/4565) is not working any longer due to the requirement of git-lfs (e.g., https://jitpack.io/com/github/broadinstitute/gatk/4.0.4.0/build.log). It will be nice to add a `jitpack.yml` file (see https://jitpack.io/docs/BUILDING/#custom-commands) to install dependencies - this will be useful for downstream project depending on SNAPSHOTS.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4819
https://github.com/broadinstitute/gatk/issues/4819:507,Integrability,depend,depending,507,"Using https://jitpack.io/#broadinstitute/gatk for building SNAPSHOTS (independent on your jfrog repository, which has a expired date for snapshots - https://github.com/broadinstitute/gatk/issues/4565) is not working any longer due to the requirement of git-lfs (e.g., https://jitpack.io/com/github/broadinstitute/gatk/4.0.4.0/build.log). It will be nice to add a `jitpack.yml` file (see https://jitpack.io/docs/BUILDING/#custom-commands) to install dependencies - this will be useful for downstream project depending on SNAPSHOTS.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4819
https://github.com/broadinstitute/gatk/issues/4819:332,Testability,log,log,332,"Using https://jitpack.io/#broadinstitute/gatk for building SNAPSHOTS (independent on your jfrog repository, which has a expired date for snapshots - https://github.com/broadinstitute/gatk/issues/4565) is not working any longer due to the requirement of git-lfs (e.g., https://jitpack.io/com/github/broadinstitute/gatk/4.0.4.0/build.log). It will be nice to add a `jitpack.yml` file (see https://jitpack.io/docs/BUILDING/#custom-commands) to install dependencies - this will be useful for downstream project depending on SNAPSHOTS.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4819
https://github.com/broadinstitute/gatk/issues/4820:714,Availability,down,download,714,"Hi,; I use your software with docker swarm where is deploy spark and hadoop the configuration for docker image is this:; ```; FROM bde2020/spark-master:2.2.0-hadoop2.8-hive-java8. MAINTAINER Jhonattan Loza <toro.ryan.jcl@gmail.com>. COPY picard.jar /; COPY GenomeAnalysisTK_v3.8-0-ge9d806836.jar /. RUN curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | bash; RUN apt-get install -y git-lfs; RUN git lfs install; RUN apt-get install unzip; RUN apt-get install wget; RUN apt-get install git. RUN mkdir /gatk; RUN apt-get update && apt-get install -y python git mlocate htop && export JAVA_TOOL_OPTIONS=-Dfile.encoding=UTF8 && \; wget https://github.com/broadinstitute/gatk/releases/download/4.0.4.0/gatk-4.0.4.0.zip && unzip gatk-4.0.4.0.zip -d tmp && mv tmp/gatk-4.0.4.0/* /gatk && cp /spark/conf/spark-defaults.conf.template /spark/conf/spark-defaults.conf && \; echo ""spark.eventLog.enabled true"" >> /spark/conf/spark-defaults.conf && \; echo ""spark.eventLog.dir file:///spark/logs/"" >> /spark/conf/spark-defaults.conf. ENV PATH=""$PATH:/spark/bin""; ```; I have this configurations for docker-compose:; - Spark. ```; version: '3'; services:; spark-master:; image: atahualpa/spark-master:GATK4.0.4; networks:; - workbench; deploy:; replicas: 1; mode: replicated; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 8080; env_file:; - ./hadoop.env; ports:; - 8333:8080; - 4040:4040; - 6066:6066; - 7077:7077; volumes:; - /data0/reference/hg19-ucsc/:/reference/hg19-ucsc/; - /data0/fastq/:/fastq/; - /data0/NGS-SparkGATK/NGS-SparkGATK/:/NGS-SparkGATK/; - /data/ngs/:/ngs/; - /data0/output/:/output/; spark-worker:; image: bde2020/spark-worker:2.2.0-hadoop2.8-hive-java8; networks:; - workbench; environment:; - SPARK_MASTER=spark://spark-master:7077; deploy:; mode: global; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 8081. env_file:; - ./hadoop.env; volumes:; - referen",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820
https://github.com/broadinstitute/gatk/issues/4820:897,Availability,echo,echo,897,"Hi,; I use your software with docker swarm where is deploy spark and hadoop the configuration for docker image is this:; ```; FROM bde2020/spark-master:2.2.0-hadoop2.8-hive-java8. MAINTAINER Jhonattan Loza <toro.ryan.jcl@gmail.com>. COPY picard.jar /; COPY GenomeAnalysisTK_v3.8-0-ge9d806836.jar /. RUN curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | bash; RUN apt-get install -y git-lfs; RUN git lfs install; RUN apt-get install unzip; RUN apt-get install wget; RUN apt-get install git. RUN mkdir /gatk; RUN apt-get update && apt-get install -y python git mlocate htop && export JAVA_TOOL_OPTIONS=-Dfile.encoding=UTF8 && \; wget https://github.com/broadinstitute/gatk/releases/download/4.0.4.0/gatk-4.0.4.0.zip && unzip gatk-4.0.4.0.zip -d tmp && mv tmp/gatk-4.0.4.0/* /gatk && cp /spark/conf/spark-defaults.conf.template /spark/conf/spark-defaults.conf && \; echo ""spark.eventLog.enabled true"" >> /spark/conf/spark-defaults.conf && \; echo ""spark.eventLog.dir file:///spark/logs/"" >> /spark/conf/spark-defaults.conf. ENV PATH=""$PATH:/spark/bin""; ```; I have this configurations for docker-compose:; - Spark. ```; version: '3'; services:; spark-master:; image: atahualpa/spark-master:GATK4.0.4; networks:; - workbench; deploy:; replicas: 1; mode: replicated; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 8080; env_file:; - ./hadoop.env; ports:; - 8333:8080; - 4040:4040; - 6066:6066; - 7077:7077; volumes:; - /data0/reference/hg19-ucsc/:/reference/hg19-ucsc/; - /data0/fastq/:/fastq/; - /data0/NGS-SparkGATK/NGS-SparkGATK/:/NGS-SparkGATK/; - /data/ngs/:/ngs/; - /data0/output/:/output/; spark-worker:; image: bde2020/spark-worker:2.2.0-hadoop2.8-hive-java8; networks:; - workbench; environment:; - SPARK_MASTER=spark://spark-master:7077; deploy:; mode: global; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 8081. env_file:; - ./hadoop.env; volumes:; - referen",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820
https://github.com/broadinstitute/gatk/issues/4820:973,Availability,echo,echo,973,"Hi,; I use your software with docker swarm where is deploy spark and hadoop the configuration for docker image is this:; ```; FROM bde2020/spark-master:2.2.0-hadoop2.8-hive-java8. MAINTAINER Jhonattan Loza <toro.ryan.jcl@gmail.com>. COPY picard.jar /; COPY GenomeAnalysisTK_v3.8-0-ge9d806836.jar /. RUN curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | bash; RUN apt-get install -y git-lfs; RUN git lfs install; RUN apt-get install unzip; RUN apt-get install wget; RUN apt-get install git. RUN mkdir /gatk; RUN apt-get update && apt-get install -y python git mlocate htop && export JAVA_TOOL_OPTIONS=-Dfile.encoding=UTF8 && \; wget https://github.com/broadinstitute/gatk/releases/download/4.0.4.0/gatk-4.0.4.0.zip && unzip gatk-4.0.4.0.zip -d tmp && mv tmp/gatk-4.0.4.0/* /gatk && cp /spark/conf/spark-defaults.conf.template /spark/conf/spark-defaults.conf && \; echo ""spark.eventLog.enabled true"" >> /spark/conf/spark-defaults.conf && \; echo ""spark.eventLog.dir file:///spark/logs/"" >> /spark/conf/spark-defaults.conf. ENV PATH=""$PATH:/spark/bin""; ```; I have this configurations for docker-compose:; - Spark. ```; version: '3'; services:; spark-master:; image: atahualpa/spark-master:GATK4.0.4; networks:; - workbench; deploy:; replicas: 1; mode: replicated; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 8080; env_file:; - ./hadoop.env; ports:; - 8333:8080; - 4040:4040; - 6066:6066; - 7077:7077; volumes:; - /data0/reference/hg19-ucsc/:/reference/hg19-ucsc/; - /data0/fastq/:/fastq/; - /data0/NGS-SparkGATK/NGS-SparkGATK/:/NGS-SparkGATK/; - /data/ngs/:/ngs/; - /data0/output/:/output/; spark-worker:; image: bde2020/spark-worker:2.2.0-hadoop2.8-hive-java8; networks:; - workbench; environment:; - SPARK_MASTER=spark://spark-master:7077; deploy:; mode: global; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 8081. env_file:; - ./hadoop.env; volumes:; - referen",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820
https://github.com/broadinstitute/gatk/issues/4820:1327,Availability,failure,failure,1327,".jar /. RUN curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | bash; RUN apt-get install -y git-lfs; RUN git lfs install; RUN apt-get install unzip; RUN apt-get install wget; RUN apt-get install git. RUN mkdir /gatk; RUN apt-get update && apt-get install -y python git mlocate htop && export JAVA_TOOL_OPTIONS=-Dfile.encoding=UTF8 && \; wget https://github.com/broadinstitute/gatk/releases/download/4.0.4.0/gatk-4.0.4.0.zip && unzip gatk-4.0.4.0.zip -d tmp && mv tmp/gatk-4.0.4.0/* /gatk && cp /spark/conf/spark-defaults.conf.template /spark/conf/spark-defaults.conf && \; echo ""spark.eventLog.enabled true"" >> /spark/conf/spark-defaults.conf && \; echo ""spark.eventLog.dir file:///spark/logs/"" >> /spark/conf/spark-defaults.conf. ENV PATH=""$PATH:/spark/bin""; ```; I have this configurations for docker-compose:; - Spark. ```; version: '3'; services:; spark-master:; image: atahualpa/spark-master:GATK4.0.4; networks:; - workbench; deploy:; replicas: 1; mode: replicated; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 8080; env_file:; - ./hadoop.env; ports:; - 8333:8080; - 4040:4040; - 6066:6066; - 7077:7077; volumes:; - /data0/reference/hg19-ucsc/:/reference/hg19-ucsc/; - /data0/fastq/:/fastq/; - /data0/NGS-SparkGATK/NGS-SparkGATK/:/NGS-SparkGATK/; - /data/ngs/:/ngs/; - /data0/output/:/output/; spark-worker:; image: bde2020/spark-worker:2.2.0-hadoop2.8-hive-java8; networks:; - workbench; environment:; - SPARK_MASTER=spark://spark-master:7077; deploy:; mode: global; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 8081. env_file:; - ./hadoop.env; volumes:; - reference-image:/reference_image. reference:; image: vzzarr/reference:hg19_img; networks:; - workbench; deploy:; mode: global; restart_policy:; condition: on-failure; tty: true #keeps the container alive; volumes:; - reference-image:/reference_image. volumes:; reference-image:. networks:; workben",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820
https://github.com/broadinstitute/gatk/issues/4820:1882,Availability,failure,failure,1882,"ark/conf/spark-defaults.conf.template /spark/conf/spark-defaults.conf && \; echo ""spark.eventLog.enabled true"" >> /spark/conf/spark-defaults.conf && \; echo ""spark.eventLog.dir file:///spark/logs/"" >> /spark/conf/spark-defaults.conf. ENV PATH=""$PATH:/spark/bin""; ```; I have this configurations for docker-compose:; - Spark. ```; version: '3'; services:; spark-master:; image: atahualpa/spark-master:GATK4.0.4; networks:; - workbench; deploy:; replicas: 1; mode: replicated; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 8080; env_file:; - ./hadoop.env; ports:; - 8333:8080; - 4040:4040; - 6066:6066; - 7077:7077; volumes:; - /data0/reference/hg19-ucsc/:/reference/hg19-ucsc/; - /data0/fastq/:/fastq/; - /data0/NGS-SparkGATK/NGS-SparkGATK/:/NGS-SparkGATK/; - /data/ngs/:/ngs/; - /data0/output/:/output/; spark-worker:; image: bde2020/spark-worker:2.2.0-hadoop2.8-hive-java8; networks:; - workbench; environment:; - SPARK_MASTER=spark://spark-master:7077; deploy:; mode: global; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 8081. env_file:; - ./hadoop.env; volumes:; - reference-image:/reference_image. reference:; image: vzzarr/reference:hg19_img; networks:; - workbench; deploy:; mode: global; restart_policy:; condition: on-failure; tty: true #keeps the container alive; volumes:; - reference-image:/reference_image. volumes:; reference-image:. networks:; workbench:; external: true; ```; - Hadoop:; ```; version: '3'; services:; namenode:; image: bde2020/hadoop-namenode:2.0.0-hadoop2.7.4-java8; networks:; - workbench; volumes:; - namenode:/hadoop/dfs/name; environment:; - CLUSTER_NAME=test; env_file:; - ./hadoop.env; deploy:; mode: replicated; replicas: 1; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 50070; ports:; - 8334:50070; volumes:; - /data0/reference/hg19-ucsc/:/reference/hg19-ucsc/; - /data0/output/:/output/; - /data/ngs/:/",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820
https://github.com/broadinstitute/gatk/issues/4820:2152,Availability,failure,failure,2152,"; - Spark. ```; version: '3'; services:; spark-master:; image: atahualpa/spark-master:GATK4.0.4; networks:; - workbench; deploy:; replicas: 1; mode: replicated; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 8080; env_file:; - ./hadoop.env; ports:; - 8333:8080; - 4040:4040; - 6066:6066; - 7077:7077; volumes:; - /data0/reference/hg19-ucsc/:/reference/hg19-ucsc/; - /data0/fastq/:/fastq/; - /data0/NGS-SparkGATK/NGS-SparkGATK/:/NGS-SparkGATK/; - /data/ngs/:/ngs/; - /data0/output/:/output/; spark-worker:; image: bde2020/spark-worker:2.2.0-hadoop2.8-hive-java8; networks:; - workbench; environment:; - SPARK_MASTER=spark://spark-master:7077; deploy:; mode: global; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 8081. env_file:; - ./hadoop.env; volumes:; - reference-image:/reference_image. reference:; image: vzzarr/reference:hg19_img; networks:; - workbench; deploy:; mode: global; restart_policy:; condition: on-failure; tty: true #keeps the container alive; volumes:; - reference-image:/reference_image. volumes:; reference-image:. networks:; workbench:; external: true; ```; - Hadoop:; ```; version: '3'; services:; namenode:; image: bde2020/hadoop-namenode:2.0.0-hadoop2.7.4-java8; networks:; - workbench; volumes:; - namenode:/hadoop/dfs/name; environment:; - CLUSTER_NAME=test; env_file:; - ./hadoop.env; deploy:; mode: replicated; replicas: 1; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 50070; ports:; - 8334:50070; volumes:; - /data0/reference/hg19-ucsc/:/reference/hg19-ucsc/; - /data0/output/:/output/; - /data/ngs/:/ngs/; datanode:; image: bde2020/hadoop-datanode:2.0.0-hadoop2.7.4-java8; networks:; - workbench; volumes:; - datanode:/hadoop/dfs/data; environment:; SERVICE_PRECONDITION: ""namenode:50070""; # depends_on:; # - namenode; env_file:; - ./hadoop.env; deploy:; mode: global; restart_policy:; condition: on-failure; labels",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820
https://github.com/broadinstitute/gatk/issues/4820:2192,Availability,alive,alive,2192,"; - Spark. ```; version: '3'; services:; spark-master:; image: atahualpa/spark-master:GATK4.0.4; networks:; - workbench; deploy:; replicas: 1; mode: replicated; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 8080; env_file:; - ./hadoop.env; ports:; - 8333:8080; - 4040:4040; - 6066:6066; - 7077:7077; volumes:; - /data0/reference/hg19-ucsc/:/reference/hg19-ucsc/; - /data0/fastq/:/fastq/; - /data0/NGS-SparkGATK/NGS-SparkGATK/:/NGS-SparkGATK/; - /data/ngs/:/ngs/; - /data0/output/:/output/; spark-worker:; image: bde2020/spark-worker:2.2.0-hadoop2.8-hive-java8; networks:; - workbench; environment:; - SPARK_MASTER=spark://spark-master:7077; deploy:; mode: global; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 8081. env_file:; - ./hadoop.env; volumes:; - reference-image:/reference_image. reference:; image: vzzarr/reference:hg19_img; networks:; - workbench; deploy:; mode: global; restart_policy:; condition: on-failure; tty: true #keeps the container alive; volumes:; - reference-image:/reference_image. volumes:; reference-image:. networks:; workbench:; external: true; ```; - Hadoop:; ```; version: '3'; services:; namenode:; image: bde2020/hadoop-namenode:2.0.0-hadoop2.7.4-java8; networks:; - workbench; volumes:; - namenode:/hadoop/dfs/name; environment:; - CLUSTER_NAME=test; env_file:; - ./hadoop.env; deploy:; mode: replicated; replicas: 1; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 50070; ports:; - 8334:50070; volumes:; - /data0/reference/hg19-ucsc/:/reference/hg19-ucsc/; - /data0/output/:/output/; - /data/ngs/:/ngs/; datanode:; image: bde2020/hadoop-datanode:2.0.0-hadoop2.7.4-java8; networks:; - workbench; volumes:; - datanode:/hadoop/dfs/data; environment:; SERVICE_PRECONDITION: ""namenode:50070""; # depends_on:; # - namenode; env_file:; - ./hadoop.env; deploy:; mode: global; restart_policy:; condition: on-failure; labels",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820
https://github.com/broadinstitute/gatk/issues/4820:2621,Availability,failure,failure,2621,"K/NGS-SparkGATK/:/NGS-SparkGATK/; - /data/ngs/:/ngs/; - /data0/output/:/output/; spark-worker:; image: bde2020/spark-worker:2.2.0-hadoop2.8-hive-java8; networks:; - workbench; environment:; - SPARK_MASTER=spark://spark-master:7077; deploy:; mode: global; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 8081. env_file:; - ./hadoop.env; volumes:; - reference-image:/reference_image. reference:; image: vzzarr/reference:hg19_img; networks:; - workbench; deploy:; mode: global; restart_policy:; condition: on-failure; tty: true #keeps the container alive; volumes:; - reference-image:/reference_image. volumes:; reference-image:. networks:; workbench:; external: true; ```; - Hadoop:; ```; version: '3'; services:; namenode:; image: bde2020/hadoop-namenode:2.0.0-hadoop2.7.4-java8; networks:; - workbench; volumes:; - namenode:/hadoop/dfs/name; environment:; - CLUSTER_NAME=test; env_file:; - ./hadoop.env; deploy:; mode: replicated; replicas: 1; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 50070; ports:; - 8334:50070; volumes:; - /data0/reference/hg19-ucsc/:/reference/hg19-ucsc/; - /data0/output/:/output/; - /data/ngs/:/ngs/; datanode:; image: bde2020/hadoop-datanode:2.0.0-hadoop2.7.4-java8; networks:; - workbench; volumes:; - datanode:/hadoop/dfs/data; environment:; SERVICE_PRECONDITION: ""namenode:50070""; # depends_on:; # - namenode; env_file:; - ./hadoop.env; deploy:; mode: global; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 50075. volumes:; datanode:; namenode:. networks:; workbench:; external: true; ```; the datanodes and namenode and spark master and workers are all working.; My hardware resources are:; 16 core and 1Tb memory ssd and 56Gb ram for 3 machines. I have this problem when I launch the version(GATK) v4.0.4.0 but not with this version v4.0.2.0-4-gb59d863-SNAPSHOT:. >java.lang.IllegalStateException: Duplicate key -",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820
https://github.com/broadinstitute/gatk/issues/4820:3121,Availability,failure,failure,3121,":; mode: global; restart_policy:; condition: on-failure; tty: true #keeps the container alive; volumes:; - reference-image:/reference_image. volumes:; reference-image:. networks:; workbench:; external: true; ```; - Hadoop:; ```; version: '3'; services:; namenode:; image: bde2020/hadoop-namenode:2.0.0-hadoop2.7.4-java8; networks:; - workbench; volumes:; - namenode:/hadoop/dfs/name; environment:; - CLUSTER_NAME=test; env_file:; - ./hadoop.env; deploy:; mode: replicated; replicas: 1; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 50070; ports:; - 8334:50070; volumes:; - /data0/reference/hg19-ucsc/:/reference/hg19-ucsc/; - /data0/output/:/output/; - /data/ngs/:/ngs/; datanode:; image: bde2020/hadoop-datanode:2.0.0-hadoop2.7.4-java8; networks:; - workbench; volumes:; - datanode:/hadoop/dfs/data; environment:; SERVICE_PRECONDITION: ""namenode:50070""; # depends_on:; # - namenode; env_file:; - ./hadoop.env; deploy:; mode: global; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 50075. volumes:; datanode:; namenode:. networks:; workbench:; external: true; ```; the datanodes and namenode and spark master and workers are all working.; My hardware resources are:; 16 core and 1Tb memory ssd and 56Gb ram for 3 machines. I have this problem when I launch the version(GATK) v4.0.4.0 but not with this version v4.0.2.0-4-gb59d863-SNAPSHOT:. >java.lang.IllegalStateException: Duplicate key -1; 	at java.util.stream.Collectors.lambda$throwingMerger$0(Collectors.java:133); 	at java.util.HashMap.merge(HashMap.java:1253); 	at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320); 	at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.uti",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820
https://github.com/broadinstitute/gatk/issues/4820:52,Deployability,deploy,deploy,52,"Hi,; I use your software with docker swarm where is deploy spark and hadoop the configuration for docker image is this:; ```; FROM bde2020/spark-master:2.2.0-hadoop2.8-hive-java8. MAINTAINER Jhonattan Loza <toro.ryan.jcl@gmail.com>. COPY picard.jar /; COPY GenomeAnalysisTK_v3.8-0-ge9d806836.jar /. RUN curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | bash; RUN apt-get install -y git-lfs; RUN git lfs install; RUN apt-get install unzip; RUN apt-get install wget; RUN apt-get install git. RUN mkdir /gatk; RUN apt-get update && apt-get install -y python git mlocate htop && export JAVA_TOOL_OPTIONS=-Dfile.encoding=UTF8 && \; wget https://github.com/broadinstitute/gatk/releases/download/4.0.4.0/gatk-4.0.4.0.zip && unzip gatk-4.0.4.0.zip -d tmp && mv tmp/gatk-4.0.4.0/* /gatk && cp /spark/conf/spark-defaults.conf.template /spark/conf/spark-defaults.conf && \; echo ""spark.eventLog.enabled true"" >> /spark/conf/spark-defaults.conf && \; echo ""spark.eventLog.dir file:///spark/logs/"" >> /spark/conf/spark-defaults.conf. ENV PATH=""$PATH:/spark/bin""; ```; I have this configurations for docker-compose:; - Spark. ```; version: '3'; services:; spark-master:; image: atahualpa/spark-master:GATK4.0.4; networks:; - workbench; deploy:; replicas: 1; mode: replicated; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 8080; env_file:; - ./hadoop.env; ports:; - 8333:8080; - 4040:4040; - 6066:6066; - 7077:7077; volumes:; - /data0/reference/hg19-ucsc/:/reference/hg19-ucsc/; - /data0/fastq/:/fastq/; - /data0/NGS-SparkGATK/NGS-SparkGATK/:/NGS-SparkGATK/; - /data/ngs/:/ngs/; - /data0/output/:/output/; spark-worker:; image: bde2020/spark-worker:2.2.0-hadoop2.8-hive-java8; networks:; - workbench; environment:; - SPARK_MASTER=spark://spark-master:7077; deploy:; mode: global; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 8081. env_file:; - ./hadoop.env; volumes:; - referen",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820
https://github.com/broadinstitute/gatk/issues/4820:80,Deployability,configurat,configuration,80,"Hi,; I use your software with docker swarm where is deploy spark and hadoop the configuration for docker image is this:; ```; FROM bde2020/spark-master:2.2.0-hadoop2.8-hive-java8. MAINTAINER Jhonattan Loza <toro.ryan.jcl@gmail.com>. COPY picard.jar /; COPY GenomeAnalysisTK_v3.8-0-ge9d806836.jar /. RUN curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | bash; RUN apt-get install -y git-lfs; RUN git lfs install; RUN apt-get install unzip; RUN apt-get install wget; RUN apt-get install git. RUN mkdir /gatk; RUN apt-get update && apt-get install -y python git mlocate htop && export JAVA_TOOL_OPTIONS=-Dfile.encoding=UTF8 && \; wget https://github.com/broadinstitute/gatk/releases/download/4.0.4.0/gatk-4.0.4.0.zip && unzip gatk-4.0.4.0.zip -d tmp && mv tmp/gatk-4.0.4.0/* /gatk && cp /spark/conf/spark-defaults.conf.template /spark/conf/spark-defaults.conf && \; echo ""spark.eventLog.enabled true"" >> /spark/conf/spark-defaults.conf && \; echo ""spark.eventLog.dir file:///spark/logs/"" >> /spark/conf/spark-defaults.conf. ENV PATH=""$PATH:/spark/bin""; ```; I have this configurations for docker-compose:; - Spark. ```; version: '3'; services:; spark-master:; image: atahualpa/spark-master:GATK4.0.4; networks:; - workbench; deploy:; replicas: 1; mode: replicated; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 8080; env_file:; - ./hadoop.env; ports:; - 8333:8080; - 4040:4040; - 6066:6066; - 7077:7077; volumes:; - /data0/reference/hg19-ucsc/:/reference/hg19-ucsc/; - /data0/fastq/:/fastq/; - /data0/NGS-SparkGATK/NGS-SparkGATK/:/NGS-SparkGATK/; - /data/ngs/:/ngs/; - /data0/output/:/output/; spark-worker:; image: bde2020/spark-worker:2.2.0-hadoop2.8-hive-java8; networks:; - workbench; environment:; - SPARK_MASTER=spark://spark-master:7077; deploy:; mode: global; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 8081. env_file:; - ./hadoop.env; volumes:; - referen",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820
https://github.com/broadinstitute/gatk/issues/4820:335,Deployability,install,install,335,"Hi,; I use your software with docker swarm where is deploy spark and hadoop the configuration for docker image is this:; ```; FROM bde2020/spark-master:2.2.0-hadoop2.8-hive-java8. MAINTAINER Jhonattan Loza <toro.ryan.jcl@gmail.com>. COPY picard.jar /; COPY GenomeAnalysisTK_v3.8-0-ge9d806836.jar /. RUN curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | bash; RUN apt-get install -y git-lfs; RUN git lfs install; RUN apt-get install unzip; RUN apt-get install wget; RUN apt-get install git. RUN mkdir /gatk; RUN apt-get update && apt-get install -y python git mlocate htop && export JAVA_TOOL_OPTIONS=-Dfile.encoding=UTF8 && \; wget https://github.com/broadinstitute/gatk/releases/download/4.0.4.0/gatk-4.0.4.0.zip && unzip gatk-4.0.4.0.zip -d tmp && mv tmp/gatk-4.0.4.0/* /gatk && cp /spark/conf/spark-defaults.conf.template /spark/conf/spark-defaults.conf && \; echo ""spark.eventLog.enabled true"" >> /spark/conf/spark-defaults.conf && \; echo ""spark.eventLog.dir file:///spark/logs/"" >> /spark/conf/spark-defaults.conf. ENV PATH=""$PATH:/spark/bin""; ```; I have this configurations for docker-compose:; - Spark. ```; version: '3'; services:; spark-master:; image: atahualpa/spark-master:GATK4.0.4; networks:; - workbench; deploy:; replicas: 1; mode: replicated; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 8080; env_file:; - ./hadoop.env; ports:; - 8333:8080; - 4040:4040; - 6066:6066; - 7077:7077; volumes:; - /data0/reference/hg19-ucsc/:/reference/hg19-ucsc/; - /data0/fastq/:/fastq/; - /data0/NGS-SparkGATK/NGS-SparkGATK/:/NGS-SparkGATK/; - /data/ngs/:/ngs/; - /data0/output/:/output/; spark-worker:; image: bde2020/spark-worker:2.2.0-hadoop2.8-hive-java8; networks:; - workbench; environment:; - SPARK_MASTER=spark://spark-master:7077; deploy:; mode: global; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 8081. env_file:; - ./hadoop.env; volumes:; - referen",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820
https://github.com/broadinstitute/gatk/issues/4820:405,Deployability,install,install,405,"Hi,; I use your software with docker swarm where is deploy spark and hadoop the configuration for docker image is this:; ```; FROM bde2020/spark-master:2.2.0-hadoop2.8-hive-java8. MAINTAINER Jhonattan Loza <toro.ryan.jcl@gmail.com>. COPY picard.jar /; COPY GenomeAnalysisTK_v3.8-0-ge9d806836.jar /. RUN curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | bash; RUN apt-get install -y git-lfs; RUN git lfs install; RUN apt-get install unzip; RUN apt-get install wget; RUN apt-get install git. RUN mkdir /gatk; RUN apt-get update && apt-get install -y python git mlocate htop && export JAVA_TOOL_OPTIONS=-Dfile.encoding=UTF8 && \; wget https://github.com/broadinstitute/gatk/releases/download/4.0.4.0/gatk-4.0.4.0.zip && unzip gatk-4.0.4.0.zip -d tmp && mv tmp/gatk-4.0.4.0/* /gatk && cp /spark/conf/spark-defaults.conf.template /spark/conf/spark-defaults.conf && \; echo ""spark.eventLog.enabled true"" >> /spark/conf/spark-defaults.conf && \; echo ""spark.eventLog.dir file:///spark/logs/"" >> /spark/conf/spark-defaults.conf. ENV PATH=""$PATH:/spark/bin""; ```; I have this configurations for docker-compose:; - Spark. ```; version: '3'; services:; spark-master:; image: atahualpa/spark-master:GATK4.0.4; networks:; - workbench; deploy:; replicas: 1; mode: replicated; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 8080; env_file:; - ./hadoop.env; ports:; - 8333:8080; - 4040:4040; - 6066:6066; - 7077:7077; volumes:; - /data0/reference/hg19-ucsc/:/reference/hg19-ucsc/; - /data0/fastq/:/fastq/; - /data0/NGS-SparkGATK/NGS-SparkGATK/:/NGS-SparkGATK/; - /data/ngs/:/ngs/; - /data0/output/:/output/; spark-worker:; image: bde2020/spark-worker:2.2.0-hadoop2.8-hive-java8; networks:; - workbench; environment:; - SPARK_MASTER=spark://spark-master:7077; deploy:; mode: global; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 8081. env_file:; - ./hadoop.env; volumes:; - referen",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820
https://github.com/broadinstitute/gatk/issues/4820:437,Deployability,install,install,437,"Hi,; I use your software with docker swarm where is deploy spark and hadoop the configuration for docker image is this:; ```; FROM bde2020/spark-master:2.2.0-hadoop2.8-hive-java8. MAINTAINER Jhonattan Loza <toro.ryan.jcl@gmail.com>. COPY picard.jar /; COPY GenomeAnalysisTK_v3.8-0-ge9d806836.jar /. RUN curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | bash; RUN apt-get install -y git-lfs; RUN git lfs install; RUN apt-get install unzip; RUN apt-get install wget; RUN apt-get install git. RUN mkdir /gatk; RUN apt-get update && apt-get install -y python git mlocate htop && export JAVA_TOOL_OPTIONS=-Dfile.encoding=UTF8 && \; wget https://github.com/broadinstitute/gatk/releases/download/4.0.4.0/gatk-4.0.4.0.zip && unzip gatk-4.0.4.0.zip -d tmp && mv tmp/gatk-4.0.4.0/* /gatk && cp /spark/conf/spark-defaults.conf.template /spark/conf/spark-defaults.conf && \; echo ""spark.eventLog.enabled true"" >> /spark/conf/spark-defaults.conf && \; echo ""spark.eventLog.dir file:///spark/logs/"" >> /spark/conf/spark-defaults.conf. ENV PATH=""$PATH:/spark/bin""; ```; I have this configurations for docker-compose:; - Spark. ```; version: '3'; services:; spark-master:; image: atahualpa/spark-master:GATK4.0.4; networks:; - workbench; deploy:; replicas: 1; mode: replicated; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 8080; env_file:; - ./hadoop.env; ports:; - 8333:8080; - 4040:4040; - 6066:6066; - 7077:7077; volumes:; - /data0/reference/hg19-ucsc/:/reference/hg19-ucsc/; - /data0/fastq/:/fastq/; - /data0/NGS-SparkGATK/NGS-SparkGATK/:/NGS-SparkGATK/; - /data/ngs/:/ngs/; - /data0/output/:/output/; spark-worker:; image: bde2020/spark-worker:2.2.0-hadoop2.8-hive-java8; networks:; - workbench; environment:; - SPARK_MASTER=spark://spark-master:7077; deploy:; mode: global; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 8081. env_file:; - ./hadoop.env; volumes:; - referen",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820
https://github.com/broadinstitute/gatk/issues/4820:458,Deployability,install,install,458,"Hi,; I use your software with docker swarm where is deploy spark and hadoop the configuration for docker image is this:; ```; FROM bde2020/spark-master:2.2.0-hadoop2.8-hive-java8. MAINTAINER Jhonattan Loza <toro.ryan.jcl@gmail.com>. COPY picard.jar /; COPY GenomeAnalysisTK_v3.8-0-ge9d806836.jar /. RUN curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | bash; RUN apt-get install -y git-lfs; RUN git lfs install; RUN apt-get install unzip; RUN apt-get install wget; RUN apt-get install git. RUN mkdir /gatk; RUN apt-get update && apt-get install -y python git mlocate htop && export JAVA_TOOL_OPTIONS=-Dfile.encoding=UTF8 && \; wget https://github.com/broadinstitute/gatk/releases/download/4.0.4.0/gatk-4.0.4.0.zip && unzip gatk-4.0.4.0.zip -d tmp && mv tmp/gatk-4.0.4.0/* /gatk && cp /spark/conf/spark-defaults.conf.template /spark/conf/spark-defaults.conf && \; echo ""spark.eventLog.enabled true"" >> /spark/conf/spark-defaults.conf && \; echo ""spark.eventLog.dir file:///spark/logs/"" >> /spark/conf/spark-defaults.conf. ENV PATH=""$PATH:/spark/bin""; ```; I have this configurations for docker-compose:; - Spark. ```; version: '3'; services:; spark-master:; image: atahualpa/spark-master:GATK4.0.4; networks:; - workbench; deploy:; replicas: 1; mode: replicated; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 8080; env_file:; - ./hadoop.env; ports:; - 8333:8080; - 4040:4040; - 6066:6066; - 7077:7077; volumes:; - /data0/reference/hg19-ucsc/:/reference/hg19-ucsc/; - /data0/fastq/:/fastq/; - /data0/NGS-SparkGATK/NGS-SparkGATK/:/NGS-SparkGATK/; - /data/ngs/:/ngs/; - /data0/output/:/output/; spark-worker:; image: bde2020/spark-worker:2.2.0-hadoop2.8-hive-java8; networks:; - workbench; environment:; - SPARK_MASTER=spark://spark-master:7077; deploy:; mode: global; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 8081. env_file:; - ./hadoop.env; volumes:; - referen",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820
https://github.com/broadinstitute/gatk/issues/4820:485,Deployability,install,install,485,"Hi,; I use your software with docker swarm where is deploy spark and hadoop the configuration for docker image is this:; ```; FROM bde2020/spark-master:2.2.0-hadoop2.8-hive-java8. MAINTAINER Jhonattan Loza <toro.ryan.jcl@gmail.com>. COPY picard.jar /; COPY GenomeAnalysisTK_v3.8-0-ge9d806836.jar /. RUN curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | bash; RUN apt-get install -y git-lfs; RUN git lfs install; RUN apt-get install unzip; RUN apt-get install wget; RUN apt-get install git. RUN mkdir /gatk; RUN apt-get update && apt-get install -y python git mlocate htop && export JAVA_TOOL_OPTIONS=-Dfile.encoding=UTF8 && \; wget https://github.com/broadinstitute/gatk/releases/download/4.0.4.0/gatk-4.0.4.0.zip && unzip gatk-4.0.4.0.zip -d tmp && mv tmp/gatk-4.0.4.0/* /gatk && cp /spark/conf/spark-defaults.conf.template /spark/conf/spark-defaults.conf && \; echo ""spark.eventLog.enabled true"" >> /spark/conf/spark-defaults.conf && \; echo ""spark.eventLog.dir file:///spark/logs/"" >> /spark/conf/spark-defaults.conf. ENV PATH=""$PATH:/spark/bin""; ```; I have this configurations for docker-compose:; - Spark. ```; version: '3'; services:; spark-master:; image: atahualpa/spark-master:GATK4.0.4; networks:; - workbench; deploy:; replicas: 1; mode: replicated; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 8080; env_file:; - ./hadoop.env; ports:; - 8333:8080; - 4040:4040; - 6066:6066; - 7077:7077; volumes:; - /data0/reference/hg19-ucsc/:/reference/hg19-ucsc/; - /data0/fastq/:/fastq/; - /data0/NGS-SparkGATK/NGS-SparkGATK/:/NGS-SparkGATK/; - /data/ngs/:/ngs/; - /data0/output/:/output/; spark-worker:; image: bde2020/spark-worker:2.2.0-hadoop2.8-hive-java8; networks:; - workbench; environment:; - SPARK_MASTER=spark://spark-master:7077; deploy:; mode: global; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 8081. env_file:; - ./hadoop.env; volumes:; - referen",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820
https://github.com/broadinstitute/gatk/issues/4820:511,Deployability,install,install,511,"Hi,; I use your software with docker swarm where is deploy spark and hadoop the configuration for docker image is this:; ```; FROM bde2020/spark-master:2.2.0-hadoop2.8-hive-java8. MAINTAINER Jhonattan Loza <toro.ryan.jcl@gmail.com>. COPY picard.jar /; COPY GenomeAnalysisTK_v3.8-0-ge9d806836.jar /. RUN curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | bash; RUN apt-get install -y git-lfs; RUN git lfs install; RUN apt-get install unzip; RUN apt-get install wget; RUN apt-get install git. RUN mkdir /gatk; RUN apt-get update && apt-get install -y python git mlocate htop && export JAVA_TOOL_OPTIONS=-Dfile.encoding=UTF8 && \; wget https://github.com/broadinstitute/gatk/releases/download/4.0.4.0/gatk-4.0.4.0.zip && unzip gatk-4.0.4.0.zip -d tmp && mv tmp/gatk-4.0.4.0/* /gatk && cp /spark/conf/spark-defaults.conf.template /spark/conf/spark-defaults.conf && \; echo ""spark.eventLog.enabled true"" >> /spark/conf/spark-defaults.conf && \; echo ""spark.eventLog.dir file:///spark/logs/"" >> /spark/conf/spark-defaults.conf. ENV PATH=""$PATH:/spark/bin""; ```; I have this configurations for docker-compose:; - Spark. ```; version: '3'; services:; spark-master:; image: atahualpa/spark-master:GATK4.0.4; networks:; - workbench; deploy:; replicas: 1; mode: replicated; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 8080; env_file:; - ./hadoop.env; ports:; - 8333:8080; - 4040:4040; - 6066:6066; - 7077:7077; volumes:; - /data0/reference/hg19-ucsc/:/reference/hg19-ucsc/; - /data0/fastq/:/fastq/; - /data0/NGS-SparkGATK/NGS-SparkGATK/:/NGS-SparkGATK/; - /data/ngs/:/ngs/; - /data0/output/:/output/; spark-worker:; image: bde2020/spark-worker:2.2.0-hadoop2.8-hive-java8; networks:; - workbench; environment:; - SPARK_MASTER=spark://spark-master:7077; deploy:; mode: global; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 8081. env_file:; - ./hadoop.env; volumes:; - referen",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820
https://github.com/broadinstitute/gatk/issues/4820:553,Deployability,update,update,553,"Hi,; I use your software with docker swarm where is deploy spark and hadoop the configuration for docker image is this:; ```; FROM bde2020/spark-master:2.2.0-hadoop2.8-hive-java8. MAINTAINER Jhonattan Loza <toro.ryan.jcl@gmail.com>. COPY picard.jar /; COPY GenomeAnalysisTK_v3.8-0-ge9d806836.jar /. RUN curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | bash; RUN apt-get install -y git-lfs; RUN git lfs install; RUN apt-get install unzip; RUN apt-get install wget; RUN apt-get install git. RUN mkdir /gatk; RUN apt-get update && apt-get install -y python git mlocate htop && export JAVA_TOOL_OPTIONS=-Dfile.encoding=UTF8 && \; wget https://github.com/broadinstitute/gatk/releases/download/4.0.4.0/gatk-4.0.4.0.zip && unzip gatk-4.0.4.0.zip -d tmp && mv tmp/gatk-4.0.4.0/* /gatk && cp /spark/conf/spark-defaults.conf.template /spark/conf/spark-defaults.conf && \; echo ""spark.eventLog.enabled true"" >> /spark/conf/spark-defaults.conf && \; echo ""spark.eventLog.dir file:///spark/logs/"" >> /spark/conf/spark-defaults.conf. ENV PATH=""$PATH:/spark/bin""; ```; I have this configurations for docker-compose:; - Spark. ```; version: '3'; services:; spark-master:; image: atahualpa/spark-master:GATK4.0.4; networks:; - workbench; deploy:; replicas: 1; mode: replicated; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 8080; env_file:; - ./hadoop.env; ports:; - 8333:8080; - 4040:4040; - 6066:6066; - 7077:7077; volumes:; - /data0/reference/hg19-ucsc/:/reference/hg19-ucsc/; - /data0/fastq/:/fastq/; - /data0/NGS-SparkGATK/NGS-SparkGATK/:/NGS-SparkGATK/; - /data/ngs/:/ngs/; - /data0/output/:/output/; spark-worker:; image: bde2020/spark-worker:2.2.0-hadoop2.8-hive-java8; networks:; - workbench; environment:; - SPARK_MASTER=spark://spark-master:7077; deploy:; mode: global; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 8081. env_file:; - ./hadoop.env; volumes:; - referen",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820
https://github.com/broadinstitute/gatk/issues/4820:571,Deployability,install,install,571,"Hi,; I use your software with docker swarm where is deploy spark and hadoop the configuration for docker image is this:; ```; FROM bde2020/spark-master:2.2.0-hadoop2.8-hive-java8. MAINTAINER Jhonattan Loza <toro.ryan.jcl@gmail.com>. COPY picard.jar /; COPY GenomeAnalysisTK_v3.8-0-ge9d806836.jar /. RUN curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | bash; RUN apt-get install -y git-lfs; RUN git lfs install; RUN apt-get install unzip; RUN apt-get install wget; RUN apt-get install git. RUN mkdir /gatk; RUN apt-get update && apt-get install -y python git mlocate htop && export JAVA_TOOL_OPTIONS=-Dfile.encoding=UTF8 && \; wget https://github.com/broadinstitute/gatk/releases/download/4.0.4.0/gatk-4.0.4.0.zip && unzip gatk-4.0.4.0.zip -d tmp && mv tmp/gatk-4.0.4.0/* /gatk && cp /spark/conf/spark-defaults.conf.template /spark/conf/spark-defaults.conf && \; echo ""spark.eventLog.enabled true"" >> /spark/conf/spark-defaults.conf && \; echo ""spark.eventLog.dir file:///spark/logs/"" >> /spark/conf/spark-defaults.conf. ENV PATH=""$PATH:/spark/bin""; ```; I have this configurations for docker-compose:; - Spark. ```; version: '3'; services:; spark-master:; image: atahualpa/spark-master:GATK4.0.4; networks:; - workbench; deploy:; replicas: 1; mode: replicated; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 8080; env_file:; - ./hadoop.env; ports:; - 8333:8080; - 4040:4040; - 6066:6066; - 7077:7077; volumes:; - /data0/reference/hg19-ucsc/:/reference/hg19-ucsc/; - /data0/fastq/:/fastq/; - /data0/NGS-SparkGATK/NGS-SparkGATK/:/NGS-SparkGATK/; - /data/ngs/:/ngs/; - /data0/output/:/output/; spark-worker:; image: bde2020/spark-worker:2.2.0-hadoop2.8-hive-java8; networks:; - workbench; environment:; - SPARK_MASTER=spark://spark-master:7077; deploy:; mode: global; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 8081. env_file:; - ./hadoop.env; volumes:; - referen",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820
https://github.com/broadinstitute/gatk/issues/4820:705,Deployability,release,releases,705,"Hi,; I use your software with docker swarm where is deploy spark and hadoop the configuration for docker image is this:; ```; FROM bde2020/spark-master:2.2.0-hadoop2.8-hive-java8. MAINTAINER Jhonattan Loza <toro.ryan.jcl@gmail.com>. COPY picard.jar /; COPY GenomeAnalysisTK_v3.8-0-ge9d806836.jar /. RUN curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | bash; RUN apt-get install -y git-lfs; RUN git lfs install; RUN apt-get install unzip; RUN apt-get install wget; RUN apt-get install git. RUN mkdir /gatk; RUN apt-get update && apt-get install -y python git mlocate htop && export JAVA_TOOL_OPTIONS=-Dfile.encoding=UTF8 && \; wget https://github.com/broadinstitute/gatk/releases/download/4.0.4.0/gatk-4.0.4.0.zip && unzip gatk-4.0.4.0.zip -d tmp && mv tmp/gatk-4.0.4.0/* /gatk && cp /spark/conf/spark-defaults.conf.template /spark/conf/spark-defaults.conf && \; echo ""spark.eventLog.enabled true"" >> /spark/conf/spark-defaults.conf && \; echo ""spark.eventLog.dir file:///spark/logs/"" >> /spark/conf/spark-defaults.conf. ENV PATH=""$PATH:/spark/bin""; ```; I have this configurations for docker-compose:; - Spark. ```; version: '3'; services:; spark-master:; image: atahualpa/spark-master:GATK4.0.4; networks:; - workbench; deploy:; replicas: 1; mode: replicated; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 8080; env_file:; - ./hadoop.env; ports:; - 8333:8080; - 4040:4040; - 6066:6066; - 7077:7077; volumes:; - /data0/reference/hg19-ucsc/:/reference/hg19-ucsc/; - /data0/fastq/:/fastq/; - /data0/NGS-SparkGATK/NGS-SparkGATK/:/NGS-SparkGATK/; - /data/ngs/:/ngs/; - /data0/output/:/output/; spark-worker:; image: bde2020/spark-worker:2.2.0-hadoop2.8-hive-java8; networks:; - workbench; environment:; - SPARK_MASTER=spark://spark-master:7077; deploy:; mode: global; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 8081. env_file:; - ./hadoop.env; volumes:; - referen",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820
https://github.com/broadinstitute/gatk/issues/4820:1101,Deployability,configurat,configurations,1101,"ocker image is this:; ```; FROM bde2020/spark-master:2.2.0-hadoop2.8-hive-java8. MAINTAINER Jhonattan Loza <toro.ryan.jcl@gmail.com>. COPY picard.jar /; COPY GenomeAnalysisTK_v3.8-0-ge9d806836.jar /. RUN curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | bash; RUN apt-get install -y git-lfs; RUN git lfs install; RUN apt-get install unzip; RUN apt-get install wget; RUN apt-get install git. RUN mkdir /gatk; RUN apt-get update && apt-get install -y python git mlocate htop && export JAVA_TOOL_OPTIONS=-Dfile.encoding=UTF8 && \; wget https://github.com/broadinstitute/gatk/releases/download/4.0.4.0/gatk-4.0.4.0.zip && unzip gatk-4.0.4.0.zip -d tmp && mv tmp/gatk-4.0.4.0/* /gatk && cp /spark/conf/spark-defaults.conf.template /spark/conf/spark-defaults.conf && \; echo ""spark.eventLog.enabled true"" >> /spark/conf/spark-defaults.conf && \; echo ""spark.eventLog.dir file:///spark/logs/"" >> /spark/conf/spark-defaults.conf. ENV PATH=""$PATH:/spark/bin""; ```; I have this configurations for docker-compose:; - Spark. ```; version: '3'; services:; spark-master:; image: atahualpa/spark-master:GATK4.0.4; networks:; - workbench; deploy:; replicas: 1; mode: replicated; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 8080; env_file:; - ./hadoop.env; ports:; - 8333:8080; - 4040:4040; - 6066:6066; - 7077:7077; volumes:; - /data0/reference/hg19-ucsc/:/reference/hg19-ucsc/; - /data0/fastq/:/fastq/; - /data0/NGS-SparkGATK/NGS-SparkGATK/:/NGS-SparkGATK/; - /data/ngs/:/ngs/; - /data0/output/:/output/; spark-worker:; image: bde2020/spark-worker:2.2.0-hadoop2.8-hive-java8; networks:; - workbench; environment:; - SPARK_MASTER=spark://spark-master:7077; deploy:; mode: global; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 8081. env_file:; - ./hadoop.env; volumes:; - reference-image:/reference_image. reference:; image: vzzarr/reference:hg19_img; networks:; - workbench; de",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820
https://github.com/broadinstitute/gatk/issues/4820:1256,Deployability,deploy,deploy,1256,".jar /. RUN curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | bash; RUN apt-get install -y git-lfs; RUN git lfs install; RUN apt-get install unzip; RUN apt-get install wget; RUN apt-get install git. RUN mkdir /gatk; RUN apt-get update && apt-get install -y python git mlocate htop && export JAVA_TOOL_OPTIONS=-Dfile.encoding=UTF8 && \; wget https://github.com/broadinstitute/gatk/releases/download/4.0.4.0/gatk-4.0.4.0.zip && unzip gatk-4.0.4.0.zip -d tmp && mv tmp/gatk-4.0.4.0/* /gatk && cp /spark/conf/spark-defaults.conf.template /spark/conf/spark-defaults.conf && \; echo ""spark.eventLog.enabled true"" >> /spark/conf/spark-defaults.conf && \; echo ""spark.eventLog.dir file:///spark/logs/"" >> /spark/conf/spark-defaults.conf. ENV PATH=""$PATH:/spark/bin""; ```; I have this configurations for docker-compose:; - Spark. ```; version: '3'; services:; spark-master:; image: atahualpa/spark-master:GATK4.0.4; networks:; - workbench; deploy:; replicas: 1; mode: replicated; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 8080; env_file:; - ./hadoop.env; ports:; - 8333:8080; - 4040:4040; - 6066:6066; - 7077:7077; volumes:; - /data0/reference/hg19-ucsc/:/reference/hg19-ucsc/; - /data0/fastq/:/fastq/; - /data0/NGS-SparkGATK/NGS-SparkGATK/:/NGS-SparkGATK/; - /data/ngs/:/ngs/; - /data0/output/:/output/; spark-worker:; image: bde2020/spark-worker:2.2.0-hadoop2.8-hive-java8; networks:; - workbench; environment:; - SPARK_MASTER=spark://spark-master:7077; deploy:; mode: global; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 8081. env_file:; - ./hadoop.env; volumes:; - reference-image:/reference_image. reference:; image: vzzarr/reference:hg19_img; networks:; - workbench; deploy:; mode: global; restart_policy:; condition: on-failure; tty: true #keeps the container alive; volumes:; - reference-image:/reference_image. volumes:; reference-image:. networks:; workben",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820
https://github.com/broadinstitute/gatk/issues/4820:1828,Deployability,deploy,deploy,1828,"ark/conf/spark-defaults.conf.template /spark/conf/spark-defaults.conf && \; echo ""spark.eventLog.enabled true"" >> /spark/conf/spark-defaults.conf && \; echo ""spark.eventLog.dir file:///spark/logs/"" >> /spark/conf/spark-defaults.conf. ENV PATH=""$PATH:/spark/bin""; ```; I have this configurations for docker-compose:; - Spark. ```; version: '3'; services:; spark-master:; image: atahualpa/spark-master:GATK4.0.4; networks:; - workbench; deploy:; replicas: 1; mode: replicated; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 8080; env_file:; - ./hadoop.env; ports:; - 8333:8080; - 4040:4040; - 6066:6066; - 7077:7077; volumes:; - /data0/reference/hg19-ucsc/:/reference/hg19-ucsc/; - /data0/fastq/:/fastq/; - /data0/NGS-SparkGATK/NGS-SparkGATK/:/NGS-SparkGATK/; - /data/ngs/:/ngs/; - /data0/output/:/output/; spark-worker:; image: bde2020/spark-worker:2.2.0-hadoop2.8-hive-java8; networks:; - workbench; environment:; - SPARK_MASTER=spark://spark-master:7077; deploy:; mode: global; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 8081. env_file:; - ./hadoop.env; volumes:; - reference-image:/reference_image. reference:; image: vzzarr/reference:hg19_img; networks:; - workbench; deploy:; mode: global; restart_policy:; condition: on-failure; tty: true #keeps the container alive; volumes:; - reference-image:/reference_image. volumes:; reference-image:. networks:; workbench:; external: true; ```; - Hadoop:; ```; version: '3'; services:; namenode:; image: bde2020/hadoop-namenode:2.0.0-hadoop2.7.4-java8; networks:; - workbench; volumes:; - namenode:/hadoop/dfs/name; environment:; - CLUSTER_NAME=test; env_file:; - ./hadoop.env; deploy:; mode: replicated; replicas: 1; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 50070; ports:; - 8334:50070; volumes:; - /data0/reference/hg19-ucsc/:/reference/hg19-ucsc/; - /data0/output/:/output/; - /data/ngs/:/",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820
https://github.com/broadinstitute/gatk/issues/4820:2098,Deployability,deploy,deploy,2098,"; - Spark. ```; version: '3'; services:; spark-master:; image: atahualpa/spark-master:GATK4.0.4; networks:; - workbench; deploy:; replicas: 1; mode: replicated; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 8080; env_file:; - ./hadoop.env; ports:; - 8333:8080; - 4040:4040; - 6066:6066; - 7077:7077; volumes:; - /data0/reference/hg19-ucsc/:/reference/hg19-ucsc/; - /data0/fastq/:/fastq/; - /data0/NGS-SparkGATK/NGS-SparkGATK/:/NGS-SparkGATK/; - /data/ngs/:/ngs/; - /data0/output/:/output/; spark-worker:; image: bde2020/spark-worker:2.2.0-hadoop2.8-hive-java8; networks:; - workbench; environment:; - SPARK_MASTER=spark://spark-master:7077; deploy:; mode: global; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 8081. env_file:; - ./hadoop.env; volumes:; - reference-image:/reference_image. reference:; image: vzzarr/reference:hg19_img; networks:; - workbench; deploy:; mode: global; restart_policy:; condition: on-failure; tty: true #keeps the container alive; volumes:; - reference-image:/reference_image. volumes:; reference-image:. networks:; workbench:; external: true; ```; - Hadoop:; ```; version: '3'; services:; namenode:; image: bde2020/hadoop-namenode:2.0.0-hadoop2.7.4-java8; networks:; - workbench; volumes:; - namenode:/hadoop/dfs/name; environment:; - CLUSTER_NAME=test; env_file:; - ./hadoop.env; deploy:; mode: replicated; replicas: 1; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 50070; ports:; - 8334:50070; volumes:; - /data0/reference/hg19-ucsc/:/reference/hg19-ucsc/; - /data0/output/:/output/; - /data/ngs/:/ngs/; datanode:; image: bde2020/hadoop-datanode:2.0.0-hadoop2.7.4-java8; networks:; - workbench; volumes:; - datanode:/hadoop/dfs/data; environment:; SERVICE_PRECONDITION: ""namenode:50070""; # depends_on:; # - namenode; env_file:; - ./hadoop.env; deploy:; mode: global; restart_policy:; condition: on-failure; labels",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820
https://github.com/broadinstitute/gatk/issues/4820:2550,Deployability,deploy,deploy,2550,"K/NGS-SparkGATK/:/NGS-SparkGATK/; - /data/ngs/:/ngs/; - /data0/output/:/output/; spark-worker:; image: bde2020/spark-worker:2.2.0-hadoop2.8-hive-java8; networks:; - workbench; environment:; - SPARK_MASTER=spark://spark-master:7077; deploy:; mode: global; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 8081. env_file:; - ./hadoop.env; volumes:; - reference-image:/reference_image. reference:; image: vzzarr/reference:hg19_img; networks:; - workbench; deploy:; mode: global; restart_policy:; condition: on-failure; tty: true #keeps the container alive; volumes:; - reference-image:/reference_image. volumes:; reference-image:. networks:; workbench:; external: true; ```; - Hadoop:; ```; version: '3'; services:; namenode:; image: bde2020/hadoop-namenode:2.0.0-hadoop2.7.4-java8; networks:; - workbench; volumes:; - namenode:/hadoop/dfs/name; environment:; - CLUSTER_NAME=test; env_file:; - ./hadoop.env; deploy:; mode: replicated; replicas: 1; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 50070; ports:; - 8334:50070; volumes:; - /data0/reference/hg19-ucsc/:/reference/hg19-ucsc/; - /data0/output/:/output/; - /data/ngs/:/ngs/; datanode:; image: bde2020/hadoop-datanode:2.0.0-hadoop2.7.4-java8; networks:; - workbench; volumes:; - datanode:/hadoop/dfs/data; environment:; SERVICE_PRECONDITION: ""namenode:50070""; # depends_on:; # - namenode; env_file:; - ./hadoop.env; deploy:; mode: global; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 50075. volumes:; datanode:; namenode:. networks:; workbench:; external: true; ```; the datanodes and namenode and spark master and workers are all working.; My hardware resources are:; 16 core and 1Tb memory ssd and 56Gb ram for 3 machines. I have this problem when I launch the version(GATK) v4.0.4.0 but not with this version v4.0.2.0-4-gb59d863-SNAPSHOT:. >java.lang.IllegalStateException: Duplicate key -",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820
https://github.com/broadinstitute/gatk/issues/4820:3067,Deployability,deploy,deploy,3067,":; mode: global; restart_policy:; condition: on-failure; tty: true #keeps the container alive; volumes:; - reference-image:/reference_image. volumes:; reference-image:. networks:; workbench:; external: true; ```; - Hadoop:; ```; version: '3'; services:; namenode:; image: bde2020/hadoop-namenode:2.0.0-hadoop2.7.4-java8; networks:; - workbench; volumes:; - namenode:/hadoop/dfs/name; environment:; - CLUSTER_NAME=test; env_file:; - ./hadoop.env; deploy:; mode: replicated; replicas: 1; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 50070; ports:; - 8334:50070; volumes:; - /data0/reference/hg19-ucsc/:/reference/hg19-ucsc/; - /data0/output/:/output/; - /data/ngs/:/ngs/; datanode:; image: bde2020/hadoop-datanode:2.0.0-hadoop2.7.4-java8; networks:; - workbench; volumes:; - datanode:/hadoop/dfs/data; environment:; SERVICE_PRECONDITION: ""namenode:50070""; # depends_on:; # - namenode; env_file:; - ./hadoop.env; deploy:; mode: global; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 50075. volumes:; datanode:; namenode:. networks:; workbench:; external: true; ```; the datanodes and namenode and spark master and workers are all working.; My hardware resources are:; 16 core and 1Tb memory ssd and 56Gb ram for 3 machines. I have this problem when I launch the version(GATK) v4.0.4.0 but not with this version v4.0.2.0-4-gb59d863-SNAPSHOT:. >java.lang.IllegalStateException: Duplicate key -1; 	at java.util.stream.Collectors.lambda$throwingMerger$0(Collectors.java:133); 	at java.util.HashMap.merge(HashMap.java:1253); 	at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320); 	at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.uti",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820
https://github.com/broadinstitute/gatk/issues/4820:13081,Deployability,patch,patch,13081,NFO BwaAndMarkDuplicatesPipelineSpark - ------------------------------------------------------------; 16:33:45.590 INFO BwaAndMarkDuplicatesPipelineSpark - HTSJDK Version: 2.14.3; 16:33:45.590 INFO BwaAndMarkDuplicatesPipelineSpark - Picard Version: 2.17.2; 16:33:45.590 INFO BwaAndMarkDuplicatesPipelineSpark - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 16:33:45.590 INFO BwaAndMarkDuplicatesPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 16:33:45.590 INFO BwaAndMarkDuplicatesPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 16:33:45.590 INFO BwaAndMarkDuplicatesPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:33:45.590 INFO BwaAndMarkDuplicatesPipelineSpark - Deflater: IntelDeflater; 16:33:45.591 INFO BwaAndMarkDuplicatesPipelineSpark - Inflater: IntelInflater; 16:33:45.591 INFO BwaAndMarkDuplicatesPipelineSpark - GCS max retries/reopens: 20; 16:33:45.591 INFO BwaAndMarkDuplicatesPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 16:33:45.591 WARN BwaAndMarkDuplicatesPipelineSpark - ; ```; - (GATK) v4.0.4.0. ```; Using GATK jar /gatk/gatk-package-4.0.4.0-spark.jar; Running:; /spark//bin/spark-submit --master spark://926a0516ccf6:7077 --conf spark.driver.userClassPathFirst=false --conf spark.io.compression.codec=lzf --conf spark.driver.maxResultSize=0 --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverh,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820
https://github.com/broadinstitute/gatk/issues/4820:16807,Deployability,patch,patch,16807,NFO BwaAndMarkDuplicatesPipelineSpark - ------------------------------------------------------------; 11:01:49.336 INFO BwaAndMarkDuplicatesPipelineSpark - HTSJDK Version: 2.14.3; 11:01:49.336 INFO BwaAndMarkDuplicatesPipelineSpark - Picard Version: 2.18.2; 11:01:49.336 INFO BwaAndMarkDuplicatesPipelineSpark - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 11:01:49.336 INFO BwaAndMarkDuplicatesPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 11:01:49.336 INFO BwaAndMarkDuplicatesPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 11:01:49.336 INFO BwaAndMarkDuplicatesPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 11:01:49.337 INFO BwaAndMarkDuplicatesPipelineSpark - Deflater: IntelDeflater; 11:01:49.337 INFO BwaAndMarkDuplicatesPipelineSpark - Inflater: IntelInflater; 11:01:49.337 INFO BwaAndMarkDuplicatesPipelineSpark - GCS max retries/reopens: 20; 11:01:49.337 INFO BwaAndMarkDuplicatesPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 11:01:49.337 WARN BwaAndMarkDuplicatesPipelineSpark - ; ```; - (GATK) v4.0.4.0-23-g6e1cc8c-SNAPSHOT; ```; Using GATK jar /gatk/build/libs/gatk-spark.jar; Running:; /spark//bin/spark-submit --master spark://973f3a3a3407:7077 --conf spark.driver.userClassPathFirst=false --conf spark.io.compression.codec=lzf --conf spark.driver.maxResultSize=0 --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.exec,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820
https://github.com/broadinstitute/gatk/issues/4820:20559,Deployability,patch,patch,20559,"esPipelineSpark - ------------------------------------------------------------; 13:47:29.831 INFO BwaAndMarkDuplicatesPipelineSpark - The Genome Analysis Toolkit (GATK) v4.0.4.0-23-g6e1cc8c-SNAPSHOT; 13:47:29.831 INFO BwaAndMarkDuplicatesPipelineSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 13:47:29.832 INFO BwaAndMarkDuplicatesPipelineSpark - Executing as root@973f3a3a3407 on Linux v4.4.0-124-generic amd64; 13:47:29.832 INFO BwaAndMarkDuplicatesPipelineSpark - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_131-8u131-b11-1~bpo8+1-b11; 13:47:29.832 INFO BwaAndMarkDuplicatesPipelineSpark - Start Date/Time: May 21, 2018 1:47:29 PM UTC; 13:47:29.832 INFO BwaAndMarkDuplicatesPipelineSpark - ------------------------------------------------------------; 13:47:29.832 INFO BwaAndMarkDuplicatesPipelineSpark - ------------------------------------------------------------; 13:47:29.833 INFO BwaAndMarkDuplicatesPipelineSpark - HTSJDK Version: 2.14.3; 13:47:29.833 INFO BwaAndMarkDuplicatesPipelineSpark - Picard Version: 2.18.2; 13:47:29.833 INFO BwaAndMarkDuplicatesPipelineSpark - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 13:47:29.833 INFO BwaAndMarkDuplicatesPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 13:47:29.834 INFO BwaAndMarkDuplicatesPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 13:47:29.834 INFO BwaAndMarkDuplicatesPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 13:47:29.834 INFO BwaAndMarkDuplicatesPipelineSpark - Deflater: IntelDeflater; 13:47:29.834 INFO BwaAndMarkDuplicatesPipelineSpark - Inflater: IntelInflater; 13:47:29.834 INFO BwaAndMarkDuplicatesPipelineSpark - GCS max retries/reopens: 20; 13:47:29.834 INFO BwaAndMarkDuplicatesPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 13:47:29.834 WARN BwaAndMarkDuplicatesPipelineSpark - ; ```; thanks.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820
https://github.com/broadinstitute/gatk/issues/4820:3817,Energy Efficiency,Reduce,ReduceOps,3817,"tanode:; image: bde2020/hadoop-datanode:2.0.0-hadoop2.7.4-java8; networks:; - workbench; volumes:; - datanode:/hadoop/dfs/data; environment:; SERVICE_PRECONDITION: ""namenode:50070""; # depends_on:; # - namenode; env_file:; - ./hadoop.env; deploy:; mode: global; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 50075. volumes:; datanode:; namenode:. networks:; workbench:; external: true; ```; the datanodes and namenode and spark master and workers are all working.; My hardware resources are:; 16 core and 1Tb memory ssd and 56Gb ram for 3 machines. I have this problem when I launch the version(GATK) v4.0.4.0 but not with this version v4.0.2.0-4-gb59d863-SNAPSHOT:. >java.lang.IllegalStateException: Duplicate key -1; 	at java.util.stream.Collectors.lambda$throwingMerger$0(Collectors.java:133); 	at java.util.HashMap.merge(HashMap.java:1253); 	at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320); 	at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark.lambda$mark$2142e97f$1(MarkDuplicatesSpark.java:82); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$10$1.apply(JavaRDDLike.scala:319); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$10$1.apply(JavaRDDLike.scala:319); 	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89); 	at org.apache.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820
https://github.com/broadinstitute/gatk/issues/4820:3848,Energy Efficiency,Reduce,ReduceOps,3848,"020/hadoop-datanode:2.0.0-hadoop2.7.4-java8; networks:; - workbench; volumes:; - datanode:/hadoop/dfs/data; environment:; SERVICE_PRECONDITION: ""namenode:50070""; # depends_on:; # - namenode; env_file:; - ./hadoop.env; deploy:; mode: global; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 50075. volumes:; datanode:; namenode:. networks:; workbench:; external: true; ```; the datanodes and namenode and spark master and workers are all working.; My hardware resources are:; 16 core and 1Tb memory ssd and 56Gb ram for 3 machines. I have this problem when I launch the version(GATK) v4.0.4.0 but not with this version v4.0.2.0-4-gb59d863-SNAPSHOT:. >java.lang.IllegalStateException: Duplicate key -1; 	at java.util.stream.Collectors.lambda$throwingMerger$0(Collectors.java:133); 	at java.util.HashMap.merge(HashMap.java:1253); 	at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320); 	at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark.lambda$mark$2142e97f$1(MarkDuplicatesSpark.java:82); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$10$1.apply(JavaRDDLike.scala:319); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$10$1.apply(JavaRDDLike.scala:319); 	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89); 	at org.apache.spark.rdd.RDD.compute",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820
https://github.com/broadinstitute/gatk/issues/4820:4196,Energy Efficiency,Reduce,ReduceOps,4196,datanode:; namenode:. networks:; workbench:; external: true; ```; the datanodes and namenode and spark master and workers are all working.; My hardware resources are:; 16 core and 1Tb memory ssd and 56Gb ram for 3 machines. I have this problem when I launch the version(GATK) v4.0.4.0 but not with this version v4.0.2.0-4-gb59d863-SNAPSHOT:. >java.lang.IllegalStateException: Duplicate key -1; 	at java.util.stream.Collectors.lambda$throwingMerger$0(Collectors.java:133); 	at java.util.HashMap.merge(HashMap.java:1253); 	at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320); 	at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark.lambda$mark$2142e97f$1(MarkDuplicatesSpark.java:82); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$10$1.apply(JavaRDDLike.scala:319); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$10$1.apply(JavaRDDLike.scala:319); 	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.s,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820
https://github.com/broadinstitute/gatk/issues/4820:4206,Energy Efficiency,Reduce,ReduceOp,4206,datanode:; namenode:. networks:; workbench:; external: true; ```; the datanodes and namenode and spark master and workers are all working.; My hardware resources are:; 16 core and 1Tb memory ssd and 56Gb ram for 3 machines. I have this problem when I launch the version(GATK) v4.0.4.0 but not with this version v4.0.2.0-4-gb59d863-SNAPSHOT:. >java.lang.IllegalStateException: Duplicate key -1; 	at java.util.stream.Collectors.lambda$throwingMerger$0(Collectors.java:133); 	at java.util.HashMap.merge(HashMap.java:1253); 	at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320); 	at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark.lambda$mark$2142e97f$1(MarkDuplicatesSpark.java:82); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$10$1.apply(JavaRDDLike.scala:319); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$10$1.apply(JavaRDDLike.scala:319); 	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.s,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820
https://github.com/broadinstitute/gatk/issues/4820:4234,Energy Efficiency,Reduce,ReduceOps,4234,tworks:; workbench:; external: true; ```; the datanodes and namenode and spark master and workers are all working.; My hardware resources are:; 16 core and 1Tb memory ssd and 56Gb ram for 3 machines. I have this problem when I launch the version(GATK) v4.0.4.0 but not with this version v4.0.2.0-4-gb59d863-SNAPSHOT:. >java.lang.IllegalStateException: Duplicate key -1; 	at java.util.stream.Collectors.lambda$throwingMerger$0(Collectors.java:133); 	at java.util.HashMap.merge(HashMap.java:1253); 	at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320); 	at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark.lambda$mark$2142e97f$1(MarkDuplicatesSpark.java:82); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$10$1.apply(JavaRDDLike.scala:319); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$10$1.apply(JavaRDDLike.scala:319); 	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820
https://github.com/broadinstitute/gatk/issues/4820:5762,Energy Efficiency,schedul,scheduler,5762,onsRDD2.compute(ZippedPartitionsRDD.scala:89); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). I don't understand why if the command is the same:; ```; $GATK_PATH BwaAndMarkDuplicatesPipelineSpark --bam-partition-size 64000000 or 4000000 \; --input hdfs://namenode:8020/$dir_prepro$ubam \; --reference hdfs://namenode:8020/hg19-ucsc/ucsc.hg19.2bit \; --bwa-mem-index-image /reference_image/ucsc.hg19.fasta.img \; --disable-sequence-dictionary-validation true \; --output hdfs://namenode:8020/$dir_prepro$output -- \; --spark-runner SPARK --spark-master spark://$SPARK_MASTER_HOST:7077 \; --driver-memory 20g --executor-cores 4 --executor-memory 8g; ```. Furthermore I have this problem with this version ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820
https://github.com/broadinstitute/gatk/issues/4820:5834,Energy Efficiency,schedul,scheduler,5834,RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). I don't understand why if the command is the same:; ```; $GATK_PATH BwaAndMarkDuplicatesPipelineSpark --bam-partition-size 64000000 or 4000000 \; --input hdfs://namenode:8020/$dir_prepro$ubam \; --reference hdfs://namenode:8020/hg19-ucsc/ucsc.hg19.2bit \; --bwa-mem-index-image /reference_image/ucsc.hg19.fasta.img \; --disable-sequence-dictionary-validation true \; --output hdfs://namenode:8020/$dir_prepro$output -- \; --spark-runner SPARK --spark-master spark://$SPARK_MASTER_HOST:7077 \; --driver-memory 20g --executor-cores 4 --executor-memory 8g; ```. Furthermore I have this problem with this version v4.0.4.0-23-g6e1cc8c-SNAPSHOT. > mark duplicate records objects correspo,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820
https://github.com/broadinstitute/gatk/issues/4820:7213,Energy Efficiency,Reduce,ReduceOps,7213," BwaAndMarkDuplicatesPipelineSpark --bam-partition-size 64000000 or 4000000 \; --input hdfs://namenode:8020/$dir_prepro$ubam \; --reference hdfs://namenode:8020/hg19-ucsc/ucsc.hg19.2bit \; --bwa-mem-index-image /reference_image/ucsc.hg19.fasta.img \; --disable-sequence-dictionary-validation true \; --output hdfs://namenode:8020/$dir_prepro$output -- \; --spark-runner SPARK --spark-master spark://$SPARK_MASTER_HOST:7077 \; --driver-memory 20g --executor-cores 4 --executor-memory 8g; ```. Furthermore I have this problem with this version v4.0.4.0-23-g6e1cc8c-SNAPSHOT. > mark duplicate records objects corresponding to read with name, this could be the result of readnames spanning more than one partition; 	at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark.lambda$null$0(MarkDuplicatesSpark.java:109); 	at java.util.HashMap.merge(HashMap.java:1253); 	at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320); 	at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark.lambda$mark$62928560$1(MarkDuplicatesSpark.java:109); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$10$1.apply(JavaRDDLike.scala:319); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$10$1.apply(JavaRDDLike.scala:319); 	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89); 	at org.apache",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820
https://github.com/broadinstitute/gatk/issues/4820:7244,Energy Efficiency,Reduce,ReduceOps,7244,"sPipelineSpark --bam-partition-size 64000000 or 4000000 \; --input hdfs://namenode:8020/$dir_prepro$ubam \; --reference hdfs://namenode:8020/hg19-ucsc/ucsc.hg19.2bit \; --bwa-mem-index-image /reference_image/ucsc.hg19.fasta.img \; --disable-sequence-dictionary-validation true \; --output hdfs://namenode:8020/$dir_prepro$output -- \; --spark-runner SPARK --spark-master spark://$SPARK_MASTER_HOST:7077 \; --driver-memory 20g --executor-cores 4 --executor-memory 8g; ```. Furthermore I have this problem with this version v4.0.4.0-23-g6e1cc8c-SNAPSHOT. > mark duplicate records objects corresponding to read with name, this could be the result of readnames spanning more than one partition; 	at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark.lambda$null$0(MarkDuplicatesSpark.java:109); 	at java.util.HashMap.merge(HashMap.java:1253); 	at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320); 	at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark.lambda$mark$62928560$1(MarkDuplicatesSpark.java:109); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$10$1.apply(JavaRDDLike.scala:319); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$10$1.apply(JavaRDDLike.scala:319); 	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89); 	at org.apache.spark.rdd.RDD.comput",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820
https://github.com/broadinstitute/gatk/issues/4820:7592,Energy Efficiency,Reduce,ReduceOps,7592,"--spark-master spark://$SPARK_MASTER_HOST:7077 \; --driver-memory 20g --executor-cores 4 --executor-memory 8g; ```. Furthermore I have this problem with this version v4.0.4.0-23-g6e1cc8c-SNAPSHOT. > mark duplicate records objects corresponding to read with name, this could be the result of readnames spanning more than one partition; 	at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark.lambda$null$0(MarkDuplicatesSpark.java:109); 	at java.util.HashMap.merge(HashMap.java:1253); 	at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320); 	at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark.lambda$mark$62928560$1(MarkDuplicatesSpark.java:109); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$10$1.apply(JavaRDDLike.scala:319); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$10$1.apply(JavaRDDLike.scala:319); 	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820
https://github.com/broadinstitute/gatk/issues/4820:7602,Energy Efficiency,Reduce,ReduceOp,7602,"--spark-master spark://$SPARK_MASTER_HOST:7077 \; --driver-memory 20g --executor-cores 4 --executor-memory 8g; ```. Furthermore I have this problem with this version v4.0.4.0-23-g6e1cc8c-SNAPSHOT. > mark duplicate records objects corresponding to read with name, this could be the result of readnames spanning more than one partition; 	at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark.lambda$null$0(MarkDuplicatesSpark.java:109); 	at java.util.HashMap.merge(HashMap.java:1253); 	at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320); 	at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark.lambda$mark$62928560$1(MarkDuplicatesSpark.java:109); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$10$1.apply(JavaRDDLike.scala:319); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$10$1.apply(JavaRDDLike.scala:319); 	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820
https://github.com/broadinstitute/gatk/issues/4820:7630,Energy Efficiency,Reduce,ReduceOps,7630,"SPARK_MASTER_HOST:7077 \; --driver-memory 20g --executor-cores 4 --executor-memory 8g; ```. Furthermore I have this problem with this version v4.0.4.0-23-g6e1cc8c-SNAPSHOT. > mark duplicate records objects corresponding to read with name, this could be the result of readnames spanning more than one partition; 	at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark.lambda$null$0(MarkDuplicatesSpark.java:109); 	at java.util.HashMap.merge(HashMap.java:1253); 	at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320); 	at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark.lambda$mark$62928560$1(MarkDuplicatesSpark.java:109); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$10$1.apply(JavaRDDLike.scala:319); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$10$1.apply(JavaRDDLike.scala:319); 	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apach",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820
https://github.com/broadinstitute/gatk/issues/4820:9159,Energy Efficiency,schedul,scheduler,9159,onsRDD2.compute(ZippedPartitionsRDD.scala:89); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748); for more information:. - v4.0.2.0-4-gb59d863-SNAPSHOT; ```; /spark//bin/spark-submit --master spark://680776067ebd:7077 --conf spark.driver.userClassPathFirst=false --conf spark.io.compression.codec=lzf --conf spark.driver.maxResultSize=0 --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_a,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820
https://github.com/broadinstitute/gatk/issues/4820:9231,Energy Efficiency,schedul,scheduler,9231,RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748); for more information:. - v4.0.2.0-4-gb59d863-SNAPSHOT; ```; /spark//bin/spark-submit --master spark://680776067ebd:7077 --conf spark.driver.userClassPathFirst=false --conf spark.io.compression.codec=lzf --conf spark.driver.maxResultSize=0 --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820
https://github.com/broadinstitute/gatk/issues/4820:4131,Integrability,wrap,wrapAndCopyInto,4131,docker.network: workbench; traefik.port: 50075. volumes:; datanode:; namenode:. networks:; workbench:; external: true; ```; the datanodes and namenode and spark master and workers are all working.; My hardware resources are:; 16 core and 1Tb memory ssd and 56Gb ram for 3 machines. I have this problem when I launch the version(GATK) v4.0.4.0 but not with this version v4.0.2.0-4-gb59d863-SNAPSHOT:. >java.lang.IllegalStateException: Duplicate key -1; 	at java.util.stream.Collectors.lambda$throwingMerger$0(Collectors.java:133); 	at java.util.HashMap.merge(HashMap.java:1253); 	at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320); 	at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark.lambda$mark$2142e97f$1(MarkDuplicatesSpark.java:82); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$10$1.apply(JavaRDDLike.scala:319); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$10$1.apply(JavaRDDLike.scala:319); 	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.ap,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820
https://github.com/broadinstitute/gatk/issues/4820:7527,Integrability,wrap,wrapAndCopyInto,7527,"menode:8020/$dir_prepro$output -- \; --spark-runner SPARK --spark-master spark://$SPARK_MASTER_HOST:7077 \; --driver-memory 20g --executor-cores 4 --executor-memory 8g; ```. Furthermore I have this problem with this version v4.0.4.0-23-g6e1cc8c-SNAPSHOT. > mark duplicate records objects corresponding to read with name, this could be the result of readnames spanning more than one partition; 	at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark.lambda$null$0(MarkDuplicatesSpark.java:109); 	at java.util.HashMap.merge(HashMap.java:1253); 	at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320); 	at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark.lambda$mark$62928560$1(MarkDuplicatesSpark.java:109); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$10$1.apply(JavaRDDLike.scala:319); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$10$1.apply(JavaRDDLike.scala:319); 	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820
https://github.com/broadinstitute/gatk/issues/4820:80,Modifiability,config,configuration,80,"Hi,; I use your software with docker swarm where is deploy spark and hadoop the configuration for docker image is this:; ```; FROM bde2020/spark-master:2.2.0-hadoop2.8-hive-java8. MAINTAINER Jhonattan Loza <toro.ryan.jcl@gmail.com>. COPY picard.jar /; COPY GenomeAnalysisTK_v3.8-0-ge9d806836.jar /. RUN curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | bash; RUN apt-get install -y git-lfs; RUN git lfs install; RUN apt-get install unzip; RUN apt-get install wget; RUN apt-get install git. RUN mkdir /gatk; RUN apt-get update && apt-get install -y python git mlocate htop && export JAVA_TOOL_OPTIONS=-Dfile.encoding=UTF8 && \; wget https://github.com/broadinstitute/gatk/releases/download/4.0.4.0/gatk-4.0.4.0.zip && unzip gatk-4.0.4.0.zip -d tmp && mv tmp/gatk-4.0.4.0/* /gatk && cp /spark/conf/spark-defaults.conf.template /spark/conf/spark-defaults.conf && \; echo ""spark.eventLog.enabled true"" >> /spark/conf/spark-defaults.conf && \; echo ""spark.eventLog.dir file:///spark/logs/"" >> /spark/conf/spark-defaults.conf. ENV PATH=""$PATH:/spark/bin""; ```; I have this configurations for docker-compose:; - Spark. ```; version: '3'; services:; spark-master:; image: atahualpa/spark-master:GATK4.0.4; networks:; - workbench; deploy:; replicas: 1; mode: replicated; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 8080; env_file:; - ./hadoop.env; ports:; - 8333:8080; - 4040:4040; - 6066:6066; - 7077:7077; volumes:; - /data0/reference/hg19-ucsc/:/reference/hg19-ucsc/; - /data0/fastq/:/fastq/; - /data0/NGS-SparkGATK/NGS-SparkGATK/:/NGS-SparkGATK/; - /data/ngs/:/ngs/; - /data0/output/:/output/; spark-worker:; image: bde2020/spark-worker:2.2.0-hadoop2.8-hive-java8; networks:; - workbench; environment:; - SPARK_MASTER=spark://spark-master:7077; deploy:; mode: global; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 8081. env_file:; - ./hadoop.env; volumes:; - referen",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820
https://github.com/broadinstitute/gatk/issues/4820:1101,Modifiability,config,configurations,1101,"ocker image is this:; ```; FROM bde2020/spark-master:2.2.0-hadoop2.8-hive-java8. MAINTAINER Jhonattan Loza <toro.ryan.jcl@gmail.com>. COPY picard.jar /; COPY GenomeAnalysisTK_v3.8-0-ge9d806836.jar /. RUN curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | bash; RUN apt-get install -y git-lfs; RUN git lfs install; RUN apt-get install unzip; RUN apt-get install wget; RUN apt-get install git. RUN mkdir /gatk; RUN apt-get update && apt-get install -y python git mlocate htop && export JAVA_TOOL_OPTIONS=-Dfile.encoding=UTF8 && \; wget https://github.com/broadinstitute/gatk/releases/download/4.0.4.0/gatk-4.0.4.0.zip && unzip gatk-4.0.4.0.zip -d tmp && mv tmp/gatk-4.0.4.0/* /gatk && cp /spark/conf/spark-defaults.conf.template /spark/conf/spark-defaults.conf && \; echo ""spark.eventLog.enabled true"" >> /spark/conf/spark-defaults.conf && \; echo ""spark.eventLog.dir file:///spark/logs/"" >> /spark/conf/spark-defaults.conf. ENV PATH=""$PATH:/spark/bin""; ```; I have this configurations for docker-compose:; - Spark. ```; version: '3'; services:; spark-master:; image: atahualpa/spark-master:GATK4.0.4; networks:; - workbench; deploy:; replicas: 1; mode: replicated; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 8080; env_file:; - ./hadoop.env; ports:; - 8333:8080; - 4040:4040; - 6066:6066; - 7077:7077; volumes:; - /data0/reference/hg19-ucsc/:/reference/hg19-ucsc/; - /data0/fastq/:/fastq/; - /data0/NGS-SparkGATK/NGS-SparkGATK/:/NGS-SparkGATK/; - /data/ngs/:/ngs/; - /data0/output/:/output/; spark-worker:; image: bde2020/spark-worker:2.2.0-hadoop2.8-hive-java8; networks:; - workbench; environment:; - SPARK_MASTER=spark://spark-master:7077; deploy:; mode: global; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 8081. env_file:; - ./hadoop.env; volumes:; - reference-image:/reference_image. reference:; image: vzzarr/reference:hg19_img; networks:; - workbench; de",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820
https://github.com/broadinstitute/gatk/issues/4820:10926,Modifiability,variab,variables,10926,"rite_tribble=false -Dsamjdk.compression_level=1 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverhead=600 --driver-memory 15g --executor-cores 2 --executor-memory 8g /gatk/build/libs/gatk-spark.jar BwaAndMarkDuplicatesPipelineSpark --bam-partition-size 4000000 --input hdfs://namenode:8020/PREPROCESSING/PFC_0028_SW_CGTACG_R_fastqtosam.bam --reference hdfs://namenode:8020/hg19-ucsc/ucsc.hg19.2bit --bwa-mem-index-image /reference_image/ucsc.hg19.fasta.img --disable-sequence-dictionary-validation true --output hdfs://namenode:8020/PREPROCESSING/PFC_0028_SW_CGTACG_R_dedup_reads.bam --spark-master spark://680776067ebd:7077; 16:33:44.918 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 16:33:45.152 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/build/libs/gatk-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 16:33:45.587 INFO BwaAndMarkDuplicatesPipelineSpark - ------------------------------------------------------------; 16:33:45.587 INFO BwaAndMarkDuplicatesPipelineSpark - The Genome Analysis Toolkit (GATK) v4.0.2.0-4-gb59d863-SNAPSHOT; 16:33:45.588 INFO BwaAndMarkDuplicatesPipelineSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 16:33:45.588 INFO BwaAndMarkDuplicatesPipelineSpark - Executing as root@680776067ebd on Linux v4.4.0-127-generic amd64; 16:33:45.588 INFO BwaAndMarkDuplicatesPipelineSpark - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_131-8u131-b11-1~bpo8+1-b11; 16:33:45.589 INFO BwaAndMarkDuplicatesPipelineSpark - Start Date/Time: May 26, 2018 4:33:45 PM UTC; 16:33:45.589 INFO BwaAndMarkDupli",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820
https://github.com/broadinstitute/gatk/issues/4820:11052,Modifiability,config,configured,11052,"rite_tribble=false -Dsamjdk.compression_level=1 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverhead=600 --driver-memory 15g --executor-cores 2 --executor-memory 8g /gatk/build/libs/gatk-spark.jar BwaAndMarkDuplicatesPipelineSpark --bam-partition-size 4000000 --input hdfs://namenode:8020/PREPROCESSING/PFC_0028_SW_CGTACG_R_fastqtosam.bam --reference hdfs://namenode:8020/hg19-ucsc/ucsc.hg19.2bit --bwa-mem-index-image /reference_image/ucsc.hg19.fasta.img --disable-sequence-dictionary-validation true --output hdfs://namenode:8020/PREPROCESSING/PFC_0028_SW_CGTACG_R_dedup_reads.bam --spark-master spark://680776067ebd:7077; 16:33:44.918 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 16:33:45.152 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/build/libs/gatk-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 16:33:45.587 INFO BwaAndMarkDuplicatesPipelineSpark - ------------------------------------------------------------; 16:33:45.587 INFO BwaAndMarkDuplicatesPipelineSpark - The Genome Analysis Toolkit (GATK) v4.0.2.0-4-gb59d863-SNAPSHOT; 16:33:45.588 INFO BwaAndMarkDuplicatesPipelineSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 16:33:45.588 INFO BwaAndMarkDuplicatesPipelineSpark - Executing as root@680776067ebd on Linux v4.4.0-127-generic amd64; 16:33:45.588 INFO BwaAndMarkDuplicatesPipelineSpark - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_131-8u131-b11-1~bpo8+1-b11; 16:33:45.589 INFO BwaAndMarkDuplicatesPipelineSpark - Start Date/Time: May 26, 2018 4:33:45 PM UTC; 16:33:45.589 INFO BwaAndMarkDupli",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820
https://github.com/broadinstitute/gatk/issues/4820:14666,Modifiability,variab,variables,14666,"ribble=false -Dsamjdk.compression_level=2 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverhead=600 --driver-memory 20g --executor-cores 4 --executor-memory 8g /gatk/gatk-package-4.0.4.0-spark.jar BwaAndMarkDuplicatesPipelineSpark --bam-partition-size 64000000 --input hdfs://namenode:8020/PREPROCESSING/PFC_0028_SW_CGTACG_R_fastqtosam.bam --reference hdfs://namenode:8020/hg19-ucsc/ucsc.hg19.2bit --bwa-mem-index-image /reference_image/ucsc.hg19.fasta.img --disable-sequence-dictionary-validation true --output hdfs://namenode:8020/PREPROCESSING/PFC_0028_SW_CGTACG_R_dedup_reads.bam --spark-master spark://926a0516ccf6:7077; 11:01:48.445 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 11:01:48.743 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.0.4.0-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 11:01:49.333 INFO BwaAndMarkDuplicatesPipelineSpark - ------------------------------------------------------------; 11:01:49.334 INFO BwaAndMarkDuplicatesPipelineSpark - The Genome Analysis Toolkit (GATK) v4.0.4.0; 11:01:49.334 INFO BwaAndMarkDuplicatesPipelineSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:01:49.334 INFO BwaAndMarkDuplicatesPipelineSpark - Executing as root@926a0516ccf6 on Linux v4.4.0-127-generic amd64; 11:01:49.335 INFO BwaAndMarkDuplicatesPipelineSpark - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_131-8u131-b11-1~bpo8+1-b11; 11:01:49.335 INFO BwaAndMarkDuplicatesPipelineSpark - Start Date/Time: May 28, 2018 11:01:48 AM UTC; 11:01:49.335 INFO BwaAndMarkDuplicatesPipelineS",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820
https://github.com/broadinstitute/gatk/issues/4820:14792,Modifiability,config,configured,14792,"ribble=false -Dsamjdk.compression_level=2 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverhead=600 --driver-memory 20g --executor-cores 4 --executor-memory 8g /gatk/gatk-package-4.0.4.0-spark.jar BwaAndMarkDuplicatesPipelineSpark --bam-partition-size 64000000 --input hdfs://namenode:8020/PREPROCESSING/PFC_0028_SW_CGTACG_R_fastqtosam.bam --reference hdfs://namenode:8020/hg19-ucsc/ucsc.hg19.2bit --bwa-mem-index-image /reference_image/ucsc.hg19.fasta.img --disable-sequence-dictionary-validation true --output hdfs://namenode:8020/PREPROCESSING/PFC_0028_SW_CGTACG_R_dedup_reads.bam --spark-master spark://926a0516ccf6:7077; 11:01:48.445 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 11:01:48.743 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.0.4.0-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 11:01:49.333 INFO BwaAndMarkDuplicatesPipelineSpark - ------------------------------------------------------------; 11:01:49.334 INFO BwaAndMarkDuplicatesPipelineSpark - The Genome Analysis Toolkit (GATK) v4.0.4.0; 11:01:49.334 INFO BwaAndMarkDuplicatesPipelineSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:01:49.334 INFO BwaAndMarkDuplicatesPipelineSpark - Executing as root@926a0516ccf6 on Linux v4.4.0-127-generic amd64; 11:01:49.335 INFO BwaAndMarkDuplicatesPipelineSpark - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_131-8u131-b11-1~bpo8+1-b11; 11:01:49.335 INFO BwaAndMarkDuplicatesPipelineSpark - Start Date/Time: May 28, 2018 11:01:48 AM UTC; 11:01:49.335 INFO BwaAndMarkDuplicatesPipelineS",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820
https://github.com/broadinstitute/gatk/issues/4820:18403,Modifiability,variab,variables,18403,"ite_tribble=false -Dsamjdk.compression_level=2 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverhead=600 --driver-memory 15g --executor-cores 2 --executor-memory 8g /gatk/build/libs/gatk-spark.jar BwaAndMarkDuplicatesPipelineSpark --bam-partition-size 64000000 --input hdfs://namenode:8020/PREPROCESSING/PFC_0028_SW_CGTACG_R_fastqtosam.bam --reference hdfs://namenode:8020/hg19-ucsc/ucsc.hg19.2bit --bwa-mem-index-image /reference_image/ucsc.hg19.fasta.img --disable-sequence-dictionary-validation true --output hdfs://namenode:8020/PREPROCESSING/PFC_0028_SW_CGTACG_R_dedup_reads.bam --spark-master spark://973f3a3a3407:7077; 13:47:29.376 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 13:47:29.548 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/build/libs/gatk-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 13:47:29.831 INFO BwaAndMarkDuplicatesPipelineSpark - ------------------------------------------------------------; 13:47:29.831 INFO BwaAndMarkDuplicatesPipelineSpark - The Genome Analysis Toolkit (GATK) v4.0.4.0-23-g6e1cc8c-SNAPSHOT; 13:47:29.831 INFO BwaAndMarkDuplicatesPipelineSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 13:47:29.832 INFO BwaAndMarkDuplicatesPipelineSpark - Executing as root@973f3a3a3407 on Linux v4.4.0-124-generic amd64; 13:47:29.832 INFO BwaAndMarkDuplicatesPipelineSpark - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_131-8u131-b11-1~bpo8+1-b11; 13:47:29.832 INFO BwaAndMarkDuplicatesPipelineSpark - Start Date/Time: May 21, 2018 1:47:29 PM UTC; 13:47:29.832 INFO BwaAndMarkDupl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820
https://github.com/broadinstitute/gatk/issues/4820:18529,Modifiability,config,configured,18529,"ite_tribble=false -Dsamjdk.compression_level=2 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverhead=600 --driver-memory 15g --executor-cores 2 --executor-memory 8g /gatk/build/libs/gatk-spark.jar BwaAndMarkDuplicatesPipelineSpark --bam-partition-size 64000000 --input hdfs://namenode:8020/PREPROCESSING/PFC_0028_SW_CGTACG_R_fastqtosam.bam --reference hdfs://namenode:8020/hg19-ucsc/ucsc.hg19.2bit --bwa-mem-index-image /reference_image/ucsc.hg19.fasta.img --disable-sequence-dictionary-validation true --output hdfs://namenode:8020/PREPROCESSING/PFC_0028_SW_CGTACG_R_dedup_reads.bam --spark-master spark://973f3a3a3407:7077; 13:47:29.376 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 13:47:29.548 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/build/libs/gatk-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 13:47:29.831 INFO BwaAndMarkDuplicatesPipelineSpark - ------------------------------------------------------------; 13:47:29.831 INFO BwaAndMarkDuplicatesPipelineSpark - The Genome Analysis Toolkit (GATK) v4.0.4.0-23-g6e1cc8c-SNAPSHOT; 13:47:29.831 INFO BwaAndMarkDuplicatesPipelineSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 13:47:29.832 INFO BwaAndMarkDuplicatesPipelineSpark - Executing as root@973f3a3a3407 on Linux v4.4.0-124-generic amd64; 13:47:29.832 INFO BwaAndMarkDuplicatesPipelineSpark - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_131-8u131-b11-1~bpo8+1-b11; 13:47:29.832 INFO BwaAndMarkDuplicatesPipelineSpark - Start Date/Time: May 21, 2018 1:47:29 PM UTC; 13:47:29.832 INFO BwaAndMarkDupl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820
https://github.com/broadinstitute/gatk/issues/4820:5959,Performance,concurren,concurrent,5959,"pPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). I don't understand why if the command is the same:; ```; $GATK_PATH BwaAndMarkDuplicatesPipelineSpark --bam-partition-size 64000000 or 4000000 \; --input hdfs://namenode:8020/$dir_prepro$ubam \; --reference hdfs://namenode:8020/hg19-ucsc/ucsc.hg19.2bit \; --bwa-mem-index-image /reference_image/ucsc.hg19.fasta.img \; --disable-sequence-dictionary-validation true \; --output hdfs://namenode:8020/$dir_prepro$output -- \; --spark-runner SPARK --spark-master spark://$SPARK_MASTER_HOST:7077 \; --driver-memory 20g --executor-cores 4 --executor-memory 8g; ```. Furthermore I have this problem with this version v4.0.4.0-23-g6e1cc8c-SNAPSHOT. > mark duplicate records objects corresponding to read with name, this could be the result of readnames spanning more than one partition; 	at org.broadinstitute.hellbe",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820
https://github.com/broadinstitute/gatk/issues/4820:6044,Performance,concurren,concurrent,6044,"teOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). I don't understand why if the command is the same:; ```; $GATK_PATH BwaAndMarkDuplicatesPipelineSpark --bam-partition-size 64000000 or 4000000 \; --input hdfs://namenode:8020/$dir_prepro$ubam \; --reference hdfs://namenode:8020/hg19-ucsc/ucsc.hg19.2bit \; --bwa-mem-index-image /reference_image/ucsc.hg19.fasta.img \; --disable-sequence-dictionary-validation true \; --output hdfs://namenode:8020/$dir_prepro$output -- \; --spark-runner SPARK --spark-master spark://$SPARK_MASTER_HOST:7077 \; --driver-memory 20g --executor-cores 4 --executor-memory 8g; ```. Furthermore I have this problem with this version v4.0.4.0-23-g6e1cc8c-SNAPSHOT. > mark duplicate records objects corresponding to read with name, this could be the result of readnames spanning more than one partition; 	at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark.lambda$null$0(MarkDupl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820
https://github.com/broadinstitute/gatk/issues/4820:9356,Performance,concurren,concurrent,9356,pPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748); for more information:. - v4.0.2.0-4-gb59d863-SNAPSHOT; ```; /spark//bin/spark-submit --master spark://680776067ebd:7077 --conf spark.driver.userClassPathFirst=false --conf spark.io.compression.codec=lzf --conf spark.driver.maxResultSize=0 --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverhead=600 --driver,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820
https://github.com/broadinstitute/gatk/issues/4820:9441,Performance,concurren,concurrent,9441,teOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748); for more information:. - v4.0.2.0-4-gb59d863-SNAPSHOT; ```; /spark//bin/spark-submit --master spark://680776067ebd:7077 --conf spark.driver.userClassPathFirst=false --conf spark.io.compression.codec=lzf --conf spark.driver.maxResultSize=0 --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverhead=600 --driver-memory 15g --executor-cores 2 --executor-memory 8g /gatk/build/libs/gatk-spark.jar B,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820
https://github.com/broadinstitute/gatk/issues/4820:11113,Performance,Load,Loading,11113,"amjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverhead=600 --driver-memory 15g --executor-cores 2 --executor-memory 8g /gatk/build/libs/gatk-spark.jar BwaAndMarkDuplicatesPipelineSpark --bam-partition-size 4000000 --input hdfs://namenode:8020/PREPROCESSING/PFC_0028_SW_CGTACG_R_fastqtosam.bam --reference hdfs://namenode:8020/hg19-ucsc/ucsc.hg19.2bit --bwa-mem-index-image /reference_image/ucsc.hg19.fasta.img --disable-sequence-dictionary-validation true --output hdfs://namenode:8020/PREPROCESSING/PFC_0028_SW_CGTACG_R_dedup_reads.bam --spark-master spark://680776067ebd:7077; 16:33:44.918 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 16:33:45.152 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/build/libs/gatk-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 16:33:45.587 INFO BwaAndMarkDuplicatesPipelineSpark - ------------------------------------------------------------; 16:33:45.587 INFO BwaAndMarkDuplicatesPipelineSpark - The Genome Analysis Toolkit (GATK) v4.0.2.0-4-gb59d863-SNAPSHOT; 16:33:45.588 INFO BwaAndMarkDuplicatesPipelineSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 16:33:45.588 INFO BwaAndMarkDuplicatesPipelineSpark - Executing as root@680776067ebd on Linux v4.4.0-127-generic amd64; 16:33:45.588 INFO BwaAndMarkDuplicatesPipelineSpark - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_131-8u131-b11-1~bpo8+1-b11; 16:33:45.589 INFO BwaAndMarkDuplicatesPipelineSpark - Start Date/Time: May 26, 2018 4:33:45 PM UTC; 16:33:45.589 INFO BwaAndMarkDuplicatesPipelineSpark - ------------------------------------------------------------; 16:33:45.589 INFO BwaAndMarkDuplicatesPipelin",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820
https://github.com/broadinstitute/gatk/issues/4820:14853,Performance,Load,Loading,14853,"use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverhead=600 --driver-memory 20g --executor-cores 4 --executor-memory 8g /gatk/gatk-package-4.0.4.0-spark.jar BwaAndMarkDuplicatesPipelineSpark --bam-partition-size 64000000 --input hdfs://namenode:8020/PREPROCESSING/PFC_0028_SW_CGTACG_R_fastqtosam.bam --reference hdfs://namenode:8020/hg19-ucsc/ucsc.hg19.2bit --bwa-mem-index-image /reference_image/ucsc.hg19.fasta.img --disable-sequence-dictionary-validation true --output hdfs://namenode:8020/PREPROCESSING/PFC_0028_SW_CGTACG_R_dedup_reads.bam --spark-master spark://926a0516ccf6:7077; 11:01:48.445 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 11:01:48.743 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.0.4.0-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 11:01:49.333 INFO BwaAndMarkDuplicatesPipelineSpark - ------------------------------------------------------------; 11:01:49.334 INFO BwaAndMarkDuplicatesPipelineSpark - The Genome Analysis Toolkit (GATK) v4.0.4.0; 11:01:49.334 INFO BwaAndMarkDuplicatesPipelineSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:01:49.334 INFO BwaAndMarkDuplicatesPipelineSpark - Executing as root@926a0516ccf6 on Linux v4.4.0-127-generic amd64; 11:01:49.335 INFO BwaAndMarkDuplicatesPipelineSpark - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_131-8u131-b11-1~bpo8+1-b11; 11:01:49.335 INFO BwaAndMarkDuplicatesPipelineSpark - Start Date/Time: May 28, 2018 11:01:48 AM UTC; 11:01:49.335 INFO BwaAndMarkDuplicatesPipelineSpark - ------------------------------------------------------------; 11:01:49.335 INFO BwaAndMarkDuplicatesPipelineSpark - -----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820
https://github.com/broadinstitute/gatk/issues/4820:18590,Performance,Load,Loading,18590,"mjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverhead=600 --driver-memory 15g --executor-cores 2 --executor-memory 8g /gatk/build/libs/gatk-spark.jar BwaAndMarkDuplicatesPipelineSpark --bam-partition-size 64000000 --input hdfs://namenode:8020/PREPROCESSING/PFC_0028_SW_CGTACG_R_fastqtosam.bam --reference hdfs://namenode:8020/hg19-ucsc/ucsc.hg19.2bit --bwa-mem-index-image /reference_image/ucsc.hg19.fasta.img --disable-sequence-dictionary-validation true --output hdfs://namenode:8020/PREPROCESSING/PFC_0028_SW_CGTACG_R_dedup_reads.bam --spark-master spark://973f3a3a3407:7077; 13:47:29.376 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 13:47:29.548 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/build/libs/gatk-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 13:47:29.831 INFO BwaAndMarkDuplicatesPipelineSpark - ------------------------------------------------------------; 13:47:29.831 INFO BwaAndMarkDuplicatesPipelineSpark - The Genome Analysis Toolkit (GATK) v4.0.4.0-23-g6e1cc8c-SNAPSHOT; 13:47:29.831 INFO BwaAndMarkDuplicatesPipelineSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 13:47:29.832 INFO BwaAndMarkDuplicatesPipelineSpark - Executing as root@973f3a3a3407 on Linux v4.4.0-124-generic amd64; 13:47:29.832 INFO BwaAndMarkDuplicatesPipelineSpark - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_131-8u131-b11-1~bpo8+1-b11; 13:47:29.832 INFO BwaAndMarkDuplicatesPipelineSpark - Start Date/Time: May 21, 2018 1:47:29 PM UTC; 13:47:29.832 INFO BwaAndMarkDuplicatesPipelineSpark - ------------------------------------------------------------; 13:47:29.832 INFO BwaAndMarkDuplicatesPipeli",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820
https://github.com/broadinstitute/gatk/issues/4820:3691,Security,Hash,HashMap,3691,"ports:; - 8334:50070; volumes:; - /data0/reference/hg19-ucsc/:/reference/hg19-ucsc/; - /data0/output/:/output/; - /data/ngs/:/ngs/; datanode:; image: bde2020/hadoop-datanode:2.0.0-hadoop2.7.4-java8; networks:; - workbench; volumes:; - datanode:/hadoop/dfs/data; environment:; SERVICE_PRECONDITION: ""namenode:50070""; # depends_on:; # - namenode; env_file:; - ./hadoop.env; deploy:; mode: global; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 50075. volumes:; datanode:; namenode:. networks:; workbench:; external: true; ```; the datanodes and namenode and spark master and workers are all working.; My hardware resources are:; 16 core and 1Tb memory ssd and 56Gb ram for 3 machines. I have this problem when I launch the version(GATK) v4.0.4.0 but not with this version v4.0.2.0-4-gb59d863-SNAPSHOT:. >java.lang.IllegalStateException: Duplicate key -1; 	at java.util.stream.Collectors.lambda$throwingMerger$0(Collectors.java:133); 	at java.util.HashMap.merge(HashMap.java:1253); 	at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320); 	at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark.lambda$mark$2142e97f$1(MarkDuplicatesSpark.java:82); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$10$1.apply(JavaRDDLike.scala:319); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820
https://github.com/broadinstitute/gatk/issues/4820:3705,Security,Hash,HashMap,3705,"334:50070; volumes:; - /data0/reference/hg19-ucsc/:/reference/hg19-ucsc/; - /data0/output/:/output/; - /data/ngs/:/ngs/; datanode:; image: bde2020/hadoop-datanode:2.0.0-hadoop2.7.4-java8; networks:; - workbench; volumes:; - datanode:/hadoop/dfs/data; environment:; SERVICE_PRECONDITION: ""namenode:50070""; # depends_on:; # - namenode; env_file:; - ./hadoop.env; deploy:; mode: global; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 50075. volumes:; datanode:; namenode:. networks:; workbench:; external: true; ```; the datanodes and namenode and spark master and workers are all working.; My hardware resources are:; 16 core and 1Tb memory ssd and 56Gb ram for 3 machines. I have this problem when I launch the version(GATK) v4.0.4.0 but not with this version v4.0.2.0-4-gb59d863-SNAPSHOT:. >java.lang.IllegalStateException: Duplicate key -1; 	at java.util.stream.Collectors.lambda$throwingMerger$0(Collectors.java:133); 	at java.util.HashMap.merge(HashMap.java:1253); 	at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320); 	at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark.lambda$mark$2142e97f$1(MarkDuplicatesSpark.java:82); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$10$1.apply(JavaRDDLike.scala:319); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$10$1.apply(J",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820
https://github.com/broadinstitute/gatk/issues/4820:6506,Security,validat,validation,6506,"pPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). I don't understand why if the command is the same:; ```; $GATK_PATH BwaAndMarkDuplicatesPipelineSpark --bam-partition-size 64000000 or 4000000 \; --input hdfs://namenode:8020/$dir_prepro$ubam \; --reference hdfs://namenode:8020/hg19-ucsc/ucsc.hg19.2bit \; --bwa-mem-index-image /reference_image/ucsc.hg19.fasta.img \; --disable-sequence-dictionary-validation true \; --output hdfs://namenode:8020/$dir_prepro$output -- \; --spark-runner SPARK --spark-master spark://$SPARK_MASTER_HOST:7077 \; --driver-memory 20g --executor-cores 4 --executor-memory 8g; ```. Furthermore I have this problem with this version v4.0.4.0-23-g6e1cc8c-SNAPSHOT. > mark duplicate records objects corresponding to read with name, this could be the result of readnames spanning more than one partition; 	at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark.lambda$null$0(MarkDuplicatesSpark.java:109); 	at java.util.HashMap.merge(HashMap.java:1253); 	at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320); 	at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.R",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820
https://github.com/broadinstitute/gatk/issues/4820:7087,Security,Hash,HashMap,7087,"PoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). I don't understand why if the command is the same:; ```; $GATK_PATH BwaAndMarkDuplicatesPipelineSpark --bam-partition-size 64000000 or 4000000 \; --input hdfs://namenode:8020/$dir_prepro$ubam \; --reference hdfs://namenode:8020/hg19-ucsc/ucsc.hg19.2bit \; --bwa-mem-index-image /reference_image/ucsc.hg19.fasta.img \; --disable-sequence-dictionary-validation true \; --output hdfs://namenode:8020/$dir_prepro$output -- \; --spark-runner SPARK --spark-master spark://$SPARK_MASTER_HOST:7077 \; --driver-memory 20g --executor-cores 4 --executor-memory 8g; ```. Furthermore I have this problem with this version v4.0.4.0-23-g6e1cc8c-SNAPSHOT. > mark duplicate records objects corresponding to read with name, this could be the result of readnames spanning more than one partition; 	at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark.lambda$null$0(MarkDuplicatesSpark.java:109); 	at java.util.HashMap.merge(HashMap.java:1253); 	at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320); 	at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark.lambda$mark$62928560$1(MarkDuplicatesSpark.java:109); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$10$1.apply(JavaRDDLike.scala:319); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820
https://github.com/broadinstitute/gatk/issues/4820:7101,Security,Hash,HashMap,7101,"r.java:617); 	at java.lang.Thread.run(Thread.java:748). I don't understand why if the command is the same:; ```; $GATK_PATH BwaAndMarkDuplicatesPipelineSpark --bam-partition-size 64000000 or 4000000 \; --input hdfs://namenode:8020/$dir_prepro$ubam \; --reference hdfs://namenode:8020/hg19-ucsc/ucsc.hg19.2bit \; --bwa-mem-index-image /reference_image/ucsc.hg19.fasta.img \; --disable-sequence-dictionary-validation true \; --output hdfs://namenode:8020/$dir_prepro$output -- \; --spark-runner SPARK --spark-master spark://$SPARK_MASTER_HOST:7077 \; --driver-memory 20g --executor-cores 4 --executor-memory 8g; ```. Furthermore I have this problem with this version v4.0.4.0-23-g6e1cc8c-SNAPSHOT. > mark duplicate records objects corresponding to read with name, this could be the result of readnames spanning more than one partition; 	at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark.lambda$null$0(MarkDuplicatesSpark.java:109); 	at java.util.HashMap.merge(HashMap.java:1253); 	at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320); 	at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark.lambda$mark$62928560$1(MarkDuplicatesSpark.java:109); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$10$1.apply(JavaRDDLike.scala:319); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$10$1.apply(",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820
https://github.com/broadinstitute/gatk/issues/4820:10735,Security,validat,validation,10735, spark.driver.maxResultSize=0 --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverhead=600 --driver-memory 15g --executor-cores 2 --executor-memory 8g /gatk/build/libs/gatk-spark.jar BwaAndMarkDuplicatesPipelineSpark --bam-partition-size 4000000 --input hdfs://namenode:8020/PREPROCESSING/PFC_0028_SW_CGTACG_R_fastqtosam.bam --reference hdfs://namenode:8020/hg19-ucsc/ucsc.hg19.2bit --bwa-mem-index-image /reference_image/ucsc.hg19.fasta.img --disable-sequence-dictionary-validation true --output hdfs://namenode:8020/PREPROCESSING/PFC_0028_SW_CGTACG_R_dedup_reads.bam --spark-master spark://680776067ebd:7077; 16:33:44.918 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 16:33:45.152 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/build/libs/gatk-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 16:33:45.587 INFO BwaAndMarkDuplicatesPipelineSpark - ------------------------------------------------------------; 16:33:45.587 INFO BwaAndMarkDuplicatesPipelineSpark - The Genome Analysis Toolkit (GATK) v4.0.2.0-4-gb59d863-SNAPSHOT; 16:33:45.588 INFO BwaAndMarkDuplicatesPipelineSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 16:33:45.588 INFO BwaAndMarkDuplicatesPipelineSpark - Executing as root@680776067ebd on Linux v4.4.0-127-generic amd64; 16:33:45.588 INFO BwaAndMarkDuplicatesPipel,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820
https://github.com/broadinstitute/gatk/issues/4820:14475,Security,validat,validation,14475,.driver.maxResultSize=0 --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverhead=600 --driver-memory 20g --executor-cores 4 --executor-memory 8g /gatk/gatk-package-4.0.4.0-spark.jar BwaAndMarkDuplicatesPipelineSpark --bam-partition-size 64000000 --input hdfs://namenode:8020/PREPROCESSING/PFC_0028_SW_CGTACG_R_fastqtosam.bam --reference hdfs://namenode:8020/hg19-ucsc/ucsc.hg19.2bit --bwa-mem-index-image /reference_image/ucsc.hg19.fasta.img --disable-sequence-dictionary-validation true --output hdfs://namenode:8020/PREPROCESSING/PFC_0028_SW_CGTACG_R_dedup_reads.bam --spark-master spark://926a0516ccf6:7077; 11:01:48.445 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 11:01:48.743 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.0.4.0-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 11:01:49.333 INFO BwaAndMarkDuplicatesPipelineSpark - ------------------------------------------------------------; 11:01:49.334 INFO BwaAndMarkDuplicatesPipelineSpark - The Genome Analysis Toolkit (GATK) v4.0.4.0; 11:01:49.334 INFO BwaAndMarkDuplicatesPipelineSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:01:49.334 INFO BwaAndMarkDuplicatesPipelineSpark - Executing as root@926a0516ccf6 on Linux v4.4.0-127-generic amd64; 11:01:49.335 INFO BwaAndMarkDuplicatesPipelineSpark - Java,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820
https://github.com/broadinstitute/gatk/issues/4820:18212,Security,validat,validation,18212,spark.driver.maxResultSize=0 --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverhead=600 --driver-memory 15g --executor-cores 2 --executor-memory 8g /gatk/build/libs/gatk-spark.jar BwaAndMarkDuplicatesPipelineSpark --bam-partition-size 64000000 --input hdfs://namenode:8020/PREPROCESSING/PFC_0028_SW_CGTACG_R_fastqtosam.bam --reference hdfs://namenode:8020/hg19-ucsc/ucsc.hg19.2bit --bwa-mem-index-image /reference_image/ucsc.hg19.fasta.img --disable-sequence-dictionary-validation true --output hdfs://namenode:8020/PREPROCESSING/PFC_0028_SW_CGTACG_R_dedup_reads.bam --spark-master spark://973f3a3a3407:7077; 13:47:29.376 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 13:47:29.548 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/build/libs/gatk-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 13:47:29.831 INFO BwaAndMarkDuplicatesPipelineSpark - ------------------------------------------------------------; 13:47:29.831 INFO BwaAndMarkDuplicatesPipelineSpark - The Genome Analysis Toolkit (GATK) v4.0.4.0-23-g6e1cc8c-SNAPSHOT; 13:47:29.831 INFO BwaAndMarkDuplicatesPipelineSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 13:47:29.832 INFO BwaAndMarkDuplicatesPipelineSpark - Executing as root@973f3a3a3407 on Linux v4.4.0-124-generic amd64; 13:47:29.832 INFO BwaAndMarkDuplicatesPipe,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820
https://github.com/broadinstitute/gatk/issues/4820:1012,Testability,log,logs,1012,"are with docker swarm where is deploy spark and hadoop the configuration for docker image is this:; ```; FROM bde2020/spark-master:2.2.0-hadoop2.8-hive-java8. MAINTAINER Jhonattan Loza <toro.ryan.jcl@gmail.com>. COPY picard.jar /; COPY GenomeAnalysisTK_v3.8-0-ge9d806836.jar /. RUN curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | bash; RUN apt-get install -y git-lfs; RUN git lfs install; RUN apt-get install unzip; RUN apt-get install wget; RUN apt-get install git. RUN mkdir /gatk; RUN apt-get update && apt-get install -y python git mlocate htop && export JAVA_TOOL_OPTIONS=-Dfile.encoding=UTF8 && \; wget https://github.com/broadinstitute/gatk/releases/download/4.0.4.0/gatk-4.0.4.0.zip && unzip gatk-4.0.4.0.zip -d tmp && mv tmp/gatk-4.0.4.0/* /gatk && cp /spark/conf/spark-defaults.conf.template /spark/conf/spark-defaults.conf && \; echo ""spark.eventLog.enabled true"" >> /spark/conf/spark-defaults.conf && \; echo ""spark.eventLog.dir file:///spark/logs/"" >> /spark/conf/spark-defaults.conf. ENV PATH=""$PATH:/spark/bin""; ```; I have this configurations for docker-compose:; - Spark. ```; version: '3'; services:; spark-master:; image: atahualpa/spark-master:GATK4.0.4; networks:; - workbench; deploy:; replicas: 1; mode: replicated; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 8080; env_file:; - ./hadoop.env; ports:; - 8333:8080; - 4040:4040; - 6066:6066; - 7077:7077; volumes:; - /data0/reference/hg19-ucsc/:/reference/hg19-ucsc/; - /data0/fastq/:/fastq/; - /data0/NGS-SparkGATK/NGS-SparkGATK/:/NGS-SparkGATK/; - /data/ngs/:/ngs/; - /data0/output/:/output/; spark-worker:; image: bde2020/spark-worker:2.2.0-hadoop2.8-hive-java8; networks:; - workbench; environment:; - SPARK_MASTER=spark://spark-master:7077; deploy:; mode: global; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 8081. env_file:; - ./hadoop.env; volumes:; - reference-image:/reference_i",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820
https://github.com/broadinstitute/gatk/issues/4820:2517,Testability,test,test,2517,"7077:7077; volumes:; - /data0/reference/hg19-ucsc/:/reference/hg19-ucsc/; - /data0/fastq/:/fastq/; - /data0/NGS-SparkGATK/NGS-SparkGATK/:/NGS-SparkGATK/; - /data/ngs/:/ngs/; - /data0/output/:/output/; spark-worker:; image: bde2020/spark-worker:2.2.0-hadoop2.8-hive-java8; networks:; - workbench; environment:; - SPARK_MASTER=spark://spark-master:7077; deploy:; mode: global; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 8081. env_file:; - ./hadoop.env; volumes:; - reference-image:/reference_image. reference:; image: vzzarr/reference:hg19_img; networks:; - workbench; deploy:; mode: global; restart_policy:; condition: on-failure; tty: true #keeps the container alive; volumes:; - reference-image:/reference_image. volumes:; reference-image:. networks:; workbench:; external: true; ```; - Hadoop:; ```; version: '3'; services:; namenode:; image: bde2020/hadoop-namenode:2.0.0-hadoop2.7.4-java8; networks:; - workbench; volumes:; - namenode:/hadoop/dfs/name; environment:; - CLUSTER_NAME=test; env_file:; - ./hadoop.env; deploy:; mode: replicated; replicas: 1; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 50070; ports:; - 8334:50070; volumes:; - /data0/reference/hg19-ucsc/:/reference/hg19-ucsc/; - /data0/output/:/output/; - /data/ngs/:/ngs/; datanode:; image: bde2020/hadoop-datanode:2.0.0-hadoop2.7.4-java8; networks:; - workbench; volumes:; - datanode:/hadoop/dfs/data; environment:; SERVICE_PRECONDITION: ""namenode:50070""; # depends_on:; # - namenode; env_file:; - ./hadoop.env; deploy:; mode: global; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 50075. volumes:; datanode:; namenode:. networks:; workbench:; external: true; ```; the datanodes and namenode and spark master and workers are all working.; My hardware resources are:; 16 core and 1Tb memory ssd and 56Gb ram for 3 machines. I have this problem when I launch the version(GA",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820
https://github.com/broadinstitute/gatk/issues/4821:1441,Availability,heartbeat,heartbeatInterval,1441,"ug/' \; HaplotypeCallerSpark \; --reference /projects/rdocking_prj/software/bcbio-nextgen/data/genomes/Hsapiens/hg19/ucsc/hg19.2bit \; --annotation MappingQualityRankSumTest --annotation MappingQualityZero \; --annotation QualByDepth --annotation ReadPosRankSumTest \; --annotation RMSMappingQuality --annotation BaseQualityRankSumTest \; --annotation FisherStrand --annotation MappingQuality \; --annotation DepthPerAlleleBySample --annotation Coverage \; -I /projects/karsanscratch/rdocking/KARSANBIO-1390_rna_seq_runs/molm13_replicate_one_small/work/align/MOLM13_rep1/MOLM13_rep1-dedup.splitN.bam \; -L /projects/karsanlab/rdocking/KARSANBIO-1254_pipeline/KARSANBIO-1390_rna_seq_runs/data/gatk_debug/chr1_70k.bed \; --interval-set-rule INTERSECTION \; --spark-master local[12] \; --conf spark.local.dir=/projects/karsanscratch/rdocking/KARSANBIO-1390_rna_seq_runs/molm13_replicate_one_small/debug \; --conf spark.driver.host=localhost \; --conf spark.network.timeout=800 \; --conf spark.executor.heartbeatInterval=100 \; --annotation ClippingRankSumTest --annotation DepthPerSampleHC \; --emit-ref-confidence GVCF -GQB 10 -GQB 20 -GQB 30 -GQB 40 -GQB 60 -GQB 80 \; --output MOLM13_rep1-chr1-70k-gatk-haplotype.vcf; ```. When I run this command on a single chromosome with `-Xmx94349m`, the command completes successfully, but the resulting VCF header does not contain this expected header line:. ```; ##INFO=<ID=END,Number=1,Type=Integer,Description=""Stop position of the interval"">; ```. (along with most of the other header lines associated with gVCF output). When I up the memory request to 110g for the same input files, the proper VCF header is present. I discovered this in the context of running GATK within the bcbio pipeline, the original descriptions are at: https://github.com/bcbio/bcbio-nextgen/issues/2375. On the linked issue, I have examples of GATK output from runs that produced correct and incorrect output - please let me know if there's any other information you need. Thanks!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4821
https://github.com/broadinstitute/gatk/issues/4821:2170,Deployability,pipeline,pipeline,2170,"ug/' \; HaplotypeCallerSpark \; --reference /projects/rdocking_prj/software/bcbio-nextgen/data/genomes/Hsapiens/hg19/ucsc/hg19.2bit \; --annotation MappingQualityRankSumTest --annotation MappingQualityZero \; --annotation QualByDepth --annotation ReadPosRankSumTest \; --annotation RMSMappingQuality --annotation BaseQualityRankSumTest \; --annotation FisherStrand --annotation MappingQuality \; --annotation DepthPerAlleleBySample --annotation Coverage \; -I /projects/karsanscratch/rdocking/KARSANBIO-1390_rna_seq_runs/molm13_replicate_one_small/work/align/MOLM13_rep1/MOLM13_rep1-dedup.splitN.bam \; -L /projects/karsanlab/rdocking/KARSANBIO-1254_pipeline/KARSANBIO-1390_rna_seq_runs/data/gatk_debug/chr1_70k.bed \; --interval-set-rule INTERSECTION \; --spark-master local[12] \; --conf spark.local.dir=/projects/karsanscratch/rdocking/KARSANBIO-1390_rna_seq_runs/molm13_replicate_one_small/debug \; --conf spark.driver.host=localhost \; --conf spark.network.timeout=800 \; --conf spark.executor.heartbeatInterval=100 \; --annotation ClippingRankSumTest --annotation DepthPerSampleHC \; --emit-ref-confidence GVCF -GQB 10 -GQB 20 -GQB 30 -GQB 40 -GQB 60 -GQB 80 \; --output MOLM13_rep1-chr1-70k-gatk-haplotype.vcf; ```. When I run this command on a single chromosome with `-Xmx94349m`, the command completes successfully, but the resulting VCF header does not contain this expected header line:. ```; ##INFO=<ID=END,Number=1,Type=Integer,Description=""Stop position of the interval"">; ```. (along with most of the other header lines associated with gVCF output). When I up the memory request to 110g for the same input files, the proper VCF header is present. I discovered this in the context of running GATK within the bcbio pipeline, the original descriptions are at: https://github.com/bcbio/bcbio-nextgen/issues/2375. On the linked issue, I have examples of GATK output from runs that produced correct and incorrect output - please let me know if there's any other information you need. Thanks!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4821
https://github.com/broadinstitute/gatk/issues/4821:1404,Safety,timeout,timeout,1404,"_replicate_one_small/debug/' \; HaplotypeCallerSpark \; --reference /projects/rdocking_prj/software/bcbio-nextgen/data/genomes/Hsapiens/hg19/ucsc/hg19.2bit \; --annotation MappingQualityRankSumTest --annotation MappingQualityZero \; --annotation QualByDepth --annotation ReadPosRankSumTest \; --annotation RMSMappingQuality --annotation BaseQualityRankSumTest \; --annotation FisherStrand --annotation MappingQuality \; --annotation DepthPerAlleleBySample --annotation Coverage \; -I /projects/karsanscratch/rdocking/KARSANBIO-1390_rna_seq_runs/molm13_replicate_one_small/work/align/MOLM13_rep1/MOLM13_rep1-dedup.splitN.bam \; -L /projects/karsanlab/rdocking/KARSANBIO-1254_pipeline/KARSANBIO-1390_rna_seq_runs/data/gatk_debug/chr1_70k.bed \; --interval-set-rule INTERSECTION \; --spark-master local[12] \; --conf spark.local.dir=/projects/karsanscratch/rdocking/KARSANBIO-1390_rna_seq_runs/molm13_replicate_one_small/debug \; --conf spark.driver.host=localhost \; --conf spark.network.timeout=800 \; --conf spark.executor.heartbeatInterval=100 \; --annotation ClippingRankSumTest --annotation DepthPerSampleHC \; --emit-ref-confidence GVCF -GQB 10 -GQB 20 -GQB 30 -GQB 40 -GQB 60 -GQB 80 \; --output MOLM13_rep1-chr1-70k-gatk-haplotype.vcf; ```. When I run this command on a single chromosome with `-Xmx94349m`, the command completes successfully, but the resulting VCF header does not contain this expected header line:. ```; ##INFO=<ID=END,Number=1,Type=Integer,Description=""Stop position of the interval"">; ```. (along with most of the other header lines associated with gVCF output). When I up the memory request to 110g for the same input files, the proper VCF header is present. I discovered this in the context of running GATK within the bcbio pipeline, the original descriptions are at: https://github.com/bcbio/bcbio-nextgen/issues/2375. On the linked issue, I have examples of GATK output from runs that produced correct and incorrect output - please let me know if there's any other infor",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4821
https://github.com/broadinstitute/gatk/issues/4822:27,Availability,error,error,27,I am getting the following error when creating the gatk environment with conda env create -n gatk -f scripts/gatkcondaenv.yml. NoPackagesFoundError: Package missing in current linux-64 channels:; - intel-openmp 2018.0.0*. Below is the complete listing. Any suggestions?. .```; /gradlew createPythonPackageArchive; :createPythonPackageArchive UP-TO-DATE. BUILD SUCCESSFUL. Total time: 13.183 secs. conda env create -n gatk -f scripts/gatkcondaenv.yml; Using Anaconda API: https://api.anaconda.org; Fetching package metadata ............. NoPackagesFoundError: Package missing in current linux-64 channels:; - intel-openmp 2018.0.0*; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4822
https://github.com/broadinstitute/gatk/pull/4823:79,Deployability,Update,Updated,79,Fixes #3956. Now gencode data sources have names preserved from config files.; Updated MafOutputRenderer to put a space and delimiter between the date and first funcotation factory information.; Updated some test cases to be correct with the new Gencode name preservation and MAF renderer update.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4823
https://github.com/broadinstitute/gatk/pull/4823:195,Deployability,Update,Updated,195,Fixes #3956. Now gencode data sources have names preserved from config files.; Updated MafOutputRenderer to put a space and delimiter between the date and first funcotation factory information.; Updated some test cases to be correct with the new Gencode name preservation and MAF renderer update.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4823
https://github.com/broadinstitute/gatk/pull/4823:289,Deployability,update,update,289,Fixes #3956. Now gencode data sources have names preserved from config files.; Updated MafOutputRenderer to put a space and delimiter between the date and first funcotation factory information.; Updated some test cases to be correct with the new Gencode name preservation and MAF renderer update.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4823
https://github.com/broadinstitute/gatk/pull/4823:64,Modifiability,config,config,64,Fixes #3956. Now gencode data sources have names preserved from config files.; Updated MafOutputRenderer to put a space and delimiter between the date and first funcotation factory information.; Updated some test cases to be correct with the new Gencode name preservation and MAF renderer update.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4823
https://github.com/broadinstitute/gatk/pull/4823:208,Testability,test,test,208,Fixes #3956. Now gencode data sources have names preserved from config files.; Updated MafOutputRenderer to put a space and delimiter between the date and first funcotation factory information.; Updated some test cases to be correct with the new Gencode name preservation and MAF renderer update.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4823
https://github.com/broadinstitute/gatk/issues/4824:209,Availability,error,error,209,"@samuelklee @asmirnov239 @mbabadi I tried to run a 30-sample cohort through gCNV on all canonical chromosomes with 250bp bins sharded in 10k-interval blocks, but PostprocessGermlineCNVCalls gave the following error:. ```...; 19:26:14.967 INFO PostprocessGermlineCNVCalls - Analyzing shard 223...; 19:26:15.107 INFO PostprocessGermlineCNVCalls - Analyzing shard 224...; 19:26:15.259 INFO PostprocessGermlineCNVCalls - Analyzing shard 225...; 19:26:15.260 INFO PostprocessGermlineCNVCalls - Shutting down engine; [May 29, 2018 7:26:15 PM UTC] org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls done. Elapsed time: 3.34 minutes.; Runtime.totalMemory()=39753089024; ***********************************************************************. A USER ERROR has occurred: Bad input: Validation error occurred on line %d of the posterior file: Posterior probabilities for at at least one posterior record do not sum up to one.; ```. After inspecting the output from shard 225, it seems that the model starts producing nan values after ~1600 warmup iterations (looking at the ELBO log). This shard corresponds to a pericentromeric region chr3:91540501-94090250. . It would be nice to have the option to bypass this error in PostprocessGermlineCNVCalls. Here is the model config for the shard:. ```""p_alt"": 1e-06,; ""p_active"": 0.01,; ""cnv_coherence_length"": 10000.0,; ""class_coherence_length"": 10000.0,; ""max_copy_number"": 5,; ""num_calling_processes"": 1,; ""num_copy_number_states"": 6,; ""num_copy_number_classes"": 2; ""max_bias_factors"": 5,; ""mapping_error_rate"": 0.01,; ""psi_t_scale"": 0.001,; ""psi_s_scale"": 0.0001,; ""depth_correction_tau"": 10000.0,; ""log_mean_bias_std"": 0.1,; ""init_ard_rel_unexplained_variance"": 0.1,; ""num_gc_bins"": 20,; ""gc_curve_sd"": 1.0,; ""q_c_expectation_mode"": ""hybrid"",; ""active_class_padding_hybrid_mode"": 50000,; ""enable_bias_factors"": false,; ""enable_explicit_gc_bias_modeling"": false,; ""disable_bias_factors_in_active_class"": false; ""version"": ""0.7""; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4824
https://github.com/broadinstitute/gatk/issues/4824:498,Availability,down,down,498,"@samuelklee @asmirnov239 @mbabadi I tried to run a 30-sample cohort through gCNV on all canonical chromosomes with 250bp bins sharded in 10k-interval blocks, but PostprocessGermlineCNVCalls gave the following error:. ```...; 19:26:14.967 INFO PostprocessGermlineCNVCalls - Analyzing shard 223...; 19:26:15.107 INFO PostprocessGermlineCNVCalls - Analyzing shard 224...; 19:26:15.259 INFO PostprocessGermlineCNVCalls - Analyzing shard 225...; 19:26:15.260 INFO PostprocessGermlineCNVCalls - Shutting down engine; [May 29, 2018 7:26:15 PM UTC] org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls done. Elapsed time: 3.34 minutes.; Runtime.totalMemory()=39753089024; ***********************************************************************. A USER ERROR has occurred: Bad input: Validation error occurred on line %d of the posterior file: Posterior probabilities for at at least one posterior record do not sum up to one.; ```. After inspecting the output from shard 225, it seems that the model starts producing nan values after ~1600 warmup iterations (looking at the ELBO log). This shard corresponds to a pericentromeric region chr3:91540501-94090250. . It would be nice to have the option to bypass this error in PostprocessGermlineCNVCalls. Here is the model config for the shard:. ```""p_alt"": 1e-06,; ""p_active"": 0.01,; ""cnv_coherence_length"": 10000.0,; ""class_coherence_length"": 10000.0,; ""max_copy_number"": 5,; ""num_calling_processes"": 1,; ""num_copy_number_states"": 6,; ""num_copy_number_classes"": 2; ""max_bias_factors"": 5,; ""mapping_error_rate"": 0.01,; ""psi_t_scale"": 0.001,; ""psi_s_scale"": 0.0001,; ""depth_correction_tau"": 10000.0,; ""log_mean_bias_std"": 0.1,; ""init_ard_rel_unexplained_variance"": 0.1,; ""num_gc_bins"": 20,; ""gc_curve_sd"": 1.0,; ""q_c_expectation_mode"": ""hybrid"",; ""active_class_padding_hybrid_mode"": 50000,; ""enable_bias_factors"": false,; ""enable_explicit_gc_bias_modeling"": false,; ""disable_bias_factors_in_active_class"": false; ""version"": ""0.7""; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4824
https://github.com/broadinstitute/gatk/issues/4824:766,Availability,ERROR,ERROR,766,"@samuelklee @asmirnov239 @mbabadi I tried to run a 30-sample cohort through gCNV on all canonical chromosomes with 250bp bins sharded in 10k-interval blocks, but PostprocessGermlineCNVCalls gave the following error:. ```...; 19:26:14.967 INFO PostprocessGermlineCNVCalls - Analyzing shard 223...; 19:26:15.107 INFO PostprocessGermlineCNVCalls - Analyzing shard 224...; 19:26:15.259 INFO PostprocessGermlineCNVCalls - Analyzing shard 225...; 19:26:15.260 INFO PostprocessGermlineCNVCalls - Shutting down engine; [May 29, 2018 7:26:15 PM UTC] org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls done. Elapsed time: 3.34 minutes.; Runtime.totalMemory()=39753089024; ***********************************************************************. A USER ERROR has occurred: Bad input: Validation error occurred on line %d of the posterior file: Posterior probabilities for at at least one posterior record do not sum up to one.; ```. After inspecting the output from shard 225, it seems that the model starts producing nan values after ~1600 warmup iterations (looking at the ELBO log). This shard corresponds to a pericentromeric region chr3:91540501-94090250. . It would be nice to have the option to bypass this error in PostprocessGermlineCNVCalls. Here is the model config for the shard:. ```""p_alt"": 1e-06,; ""p_active"": 0.01,; ""cnv_coherence_length"": 10000.0,; ""class_coherence_length"": 10000.0,; ""max_copy_number"": 5,; ""num_calling_processes"": 1,; ""num_copy_number_states"": 6,; ""num_copy_number_classes"": 2; ""max_bias_factors"": 5,; ""mapping_error_rate"": 0.01,; ""psi_t_scale"": 0.001,; ""psi_s_scale"": 0.0001,; ""depth_correction_tau"": 10000.0,; ""log_mean_bias_std"": 0.1,; ""init_ard_rel_unexplained_variance"": 0.1,; ""num_gc_bins"": 20,; ""gc_curve_sd"": 1.0,; ""q_c_expectation_mode"": ""hybrid"",; ""active_class_padding_hybrid_mode"": 50000,; ""enable_bias_factors"": false,; ""enable_explicit_gc_bias_modeling"": false,; ""disable_bias_factors_in_active_class"": false; ""version"": ""0.7""; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4824
https://github.com/broadinstitute/gatk/issues/4824:808,Availability,error,error,808,"@samuelklee @asmirnov239 @mbabadi I tried to run a 30-sample cohort through gCNV on all canonical chromosomes with 250bp bins sharded in 10k-interval blocks, but PostprocessGermlineCNVCalls gave the following error:. ```...; 19:26:14.967 INFO PostprocessGermlineCNVCalls - Analyzing shard 223...; 19:26:15.107 INFO PostprocessGermlineCNVCalls - Analyzing shard 224...; 19:26:15.259 INFO PostprocessGermlineCNVCalls - Analyzing shard 225...; 19:26:15.260 INFO PostprocessGermlineCNVCalls - Shutting down engine; [May 29, 2018 7:26:15 PM UTC] org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls done. Elapsed time: 3.34 minutes.; Runtime.totalMemory()=39753089024; ***********************************************************************. A USER ERROR has occurred: Bad input: Validation error occurred on line %d of the posterior file: Posterior probabilities for at at least one posterior record do not sum up to one.; ```. After inspecting the output from shard 225, it seems that the model starts producing nan values after ~1600 warmup iterations (looking at the ELBO log). This shard corresponds to a pericentromeric region chr3:91540501-94090250. . It would be nice to have the option to bypass this error in PostprocessGermlineCNVCalls. Here is the model config for the shard:. ```""p_alt"": 1e-06,; ""p_active"": 0.01,; ""cnv_coherence_length"": 10000.0,; ""class_coherence_length"": 10000.0,; ""max_copy_number"": 5,; ""num_calling_processes"": 1,; ""num_copy_number_states"": 6,; ""num_copy_number_classes"": 2; ""max_bias_factors"": 5,; ""mapping_error_rate"": 0.01,; ""psi_t_scale"": 0.001,; ""psi_s_scale"": 0.0001,; ""depth_correction_tau"": 10000.0,; ""log_mean_bias_std"": 0.1,; ""init_ard_rel_unexplained_variance"": 0.1,; ""num_gc_bins"": 20,; ""gc_curve_sd"": 1.0,; ""q_c_expectation_mode"": ""hybrid"",; ""active_class_padding_hybrid_mode"": 50000,; ""enable_bias_factors"": false,; ""enable_explicit_gc_bias_modeling"": false,; ""disable_bias_factors_in_active_class"": false; ""version"": ""0.7""; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4824
https://github.com/broadinstitute/gatk/issues/4824:1227,Availability,error,error,1227,"@samuelklee @asmirnov239 @mbabadi I tried to run a 30-sample cohort through gCNV on all canonical chromosomes with 250bp bins sharded in 10k-interval blocks, but PostprocessGermlineCNVCalls gave the following error:. ```...; 19:26:14.967 INFO PostprocessGermlineCNVCalls - Analyzing shard 223...; 19:26:15.107 INFO PostprocessGermlineCNVCalls - Analyzing shard 224...; 19:26:15.259 INFO PostprocessGermlineCNVCalls - Analyzing shard 225...; 19:26:15.260 INFO PostprocessGermlineCNVCalls - Shutting down engine; [May 29, 2018 7:26:15 PM UTC] org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls done. Elapsed time: 3.34 minutes.; Runtime.totalMemory()=39753089024; ***********************************************************************. A USER ERROR has occurred: Bad input: Validation error occurred on line %d of the posterior file: Posterior probabilities for at at least one posterior record do not sum up to one.; ```. After inspecting the output from shard 225, it seems that the model starts producing nan values after ~1600 warmup iterations (looking at the ELBO log). This shard corresponds to a pericentromeric region chr3:91540501-94090250. . It would be nice to have the option to bypass this error in PostprocessGermlineCNVCalls. Here is the model config for the shard:. ```""p_alt"": 1e-06,; ""p_active"": 0.01,; ""cnv_coherence_length"": 10000.0,; ""class_coherence_length"": 10000.0,; ""max_copy_number"": 5,; ""num_calling_processes"": 1,; ""num_copy_number_states"": 6,; ""num_copy_number_classes"": 2; ""max_bias_factors"": 5,; ""mapping_error_rate"": 0.01,; ""psi_t_scale"": 0.001,; ""psi_s_scale"": 0.0001,; ""depth_correction_tau"": 10000.0,; ""log_mean_bias_std"": 0.1,; ""init_ard_rel_unexplained_variance"": 0.1,; ""num_gc_bins"": 20,; ""gc_curve_sd"": 1.0,; ""q_c_expectation_mode"": ""hybrid"",; ""active_class_padding_hybrid_mode"": 50000,; ""enable_bias_factors"": false,; ""enable_explicit_gc_bias_modeling"": false,; ""disable_bias_factors_in_active_class"": false; ""version"": ""0.7""; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4824
https://github.com/broadinstitute/gatk/issues/4824:1283,Modifiability,config,config,1283,"@samuelklee @asmirnov239 @mbabadi I tried to run a 30-sample cohort through gCNV on all canonical chromosomes with 250bp bins sharded in 10k-interval blocks, but PostprocessGermlineCNVCalls gave the following error:. ```...; 19:26:14.967 INFO PostprocessGermlineCNVCalls - Analyzing shard 223...; 19:26:15.107 INFO PostprocessGermlineCNVCalls - Analyzing shard 224...; 19:26:15.259 INFO PostprocessGermlineCNVCalls - Analyzing shard 225...; 19:26:15.260 INFO PostprocessGermlineCNVCalls - Shutting down engine; [May 29, 2018 7:26:15 PM UTC] org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls done. Elapsed time: 3.34 minutes.; Runtime.totalMemory()=39753089024; ***********************************************************************. A USER ERROR has occurred: Bad input: Validation error occurred on line %d of the posterior file: Posterior probabilities for at at least one posterior record do not sum up to one.; ```. After inspecting the output from shard 225, it seems that the model starts producing nan values after ~1600 warmup iterations (looking at the ELBO log). This shard corresponds to a pericentromeric region chr3:91540501-94090250. . It would be nice to have the option to bypass this error in PostprocessGermlineCNVCalls. Here is the model config for the shard:. ```""p_alt"": 1e-06,; ""p_active"": 0.01,; ""cnv_coherence_length"": 10000.0,; ""class_coherence_length"": 10000.0,; ""max_copy_number"": 5,; ""num_calling_processes"": 1,; ""num_copy_number_states"": 6,; ""num_copy_number_classes"": 2; ""max_bias_factors"": 5,; ""mapping_error_rate"": 0.01,; ""psi_t_scale"": 0.001,; ""psi_s_scale"": 0.0001,; ""depth_correction_tau"": 10000.0,; ""log_mean_bias_std"": 0.1,; ""init_ard_rel_unexplained_variance"": 0.1,; ""num_gc_bins"": 20,; ""gc_curve_sd"": 1.0,; ""q_c_expectation_mode"": ""hybrid"",; ""active_class_padding_hybrid_mode"": 50000,; ""enable_bias_factors"": false,; ""enable_explicit_gc_bias_modeling"": false,; ""disable_bias_factors_in_active_class"": false; ""version"": ""0.7""; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4824
https://github.com/broadinstitute/gatk/issues/4824:797,Security,Validat,Validation,797,"@samuelklee @asmirnov239 @mbabadi I tried to run a 30-sample cohort through gCNV on all canonical chromosomes with 250bp bins sharded in 10k-interval blocks, but PostprocessGermlineCNVCalls gave the following error:. ```...; 19:26:14.967 INFO PostprocessGermlineCNVCalls - Analyzing shard 223...; 19:26:15.107 INFO PostprocessGermlineCNVCalls - Analyzing shard 224...; 19:26:15.259 INFO PostprocessGermlineCNVCalls - Analyzing shard 225...; 19:26:15.260 INFO PostprocessGermlineCNVCalls - Shutting down engine; [May 29, 2018 7:26:15 PM UTC] org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls done. Elapsed time: 3.34 minutes.; Runtime.totalMemory()=39753089024; ***********************************************************************. A USER ERROR has occurred: Bad input: Validation error occurred on line %d of the posterior file: Posterior probabilities for at at least one posterior record do not sum up to one.; ```. After inspecting the output from shard 225, it seems that the model starts producing nan values after ~1600 warmup iterations (looking at the ELBO log). This shard corresponds to a pericentromeric region chr3:91540501-94090250. . It would be nice to have the option to bypass this error in PostprocessGermlineCNVCalls. Here is the model config for the shard:. ```""p_alt"": 1e-06,; ""p_active"": 0.01,; ""cnv_coherence_length"": 10000.0,; ""class_coherence_length"": 10000.0,; ""max_copy_number"": 5,; ""num_calling_processes"": 1,; ""num_copy_number_states"": 6,; ""num_copy_number_classes"": 2; ""max_bias_factors"": 5,; ""mapping_error_rate"": 0.01,; ""psi_t_scale"": 0.001,; ""psi_s_scale"": 0.0001,; ""depth_correction_tau"": 10000.0,; ""log_mean_bias_std"": 0.1,; ""init_ard_rel_unexplained_variance"": 0.1,; ""num_gc_bins"": 20,; ""gc_curve_sd"": 1.0,; ""q_c_expectation_mode"": ""hybrid"",; ""active_class_padding_hybrid_mode"": 50000,; ""enable_bias_factors"": false,; ""enable_explicit_gc_bias_modeling"": false,; ""disable_bias_factors_in_active_class"": false; ""version"": ""0.7""; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4824
https://github.com/broadinstitute/gatk/issues/4824:1093,Testability,log,log,1093,"@samuelklee @asmirnov239 @mbabadi I tried to run a 30-sample cohort through gCNV on all canonical chromosomes with 250bp bins sharded in 10k-interval blocks, but PostprocessGermlineCNVCalls gave the following error:. ```...; 19:26:14.967 INFO PostprocessGermlineCNVCalls - Analyzing shard 223...; 19:26:15.107 INFO PostprocessGermlineCNVCalls - Analyzing shard 224...; 19:26:15.259 INFO PostprocessGermlineCNVCalls - Analyzing shard 225...; 19:26:15.260 INFO PostprocessGermlineCNVCalls - Shutting down engine; [May 29, 2018 7:26:15 PM UTC] org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls done. Elapsed time: 3.34 minutes.; Runtime.totalMemory()=39753089024; ***********************************************************************. A USER ERROR has occurred: Bad input: Validation error occurred on line %d of the posterior file: Posterior probabilities for at at least one posterior record do not sum up to one.; ```. After inspecting the output from shard 225, it seems that the model starts producing nan values after ~1600 warmup iterations (looking at the ELBO log). This shard corresponds to a pericentromeric region chr3:91540501-94090250. . It would be nice to have the option to bypass this error in PostprocessGermlineCNVCalls. Here is the model config for the shard:. ```""p_alt"": 1e-06,; ""p_active"": 0.01,; ""cnv_coherence_length"": 10000.0,; ""class_coherence_length"": 10000.0,; ""max_copy_number"": 5,; ""num_calling_processes"": 1,; ""num_copy_number_states"": 6,; ""num_copy_number_classes"": 2; ""max_bias_factors"": 5,; ""mapping_error_rate"": 0.01,; ""psi_t_scale"": 0.001,; ""psi_s_scale"": 0.0001,; ""depth_correction_tau"": 10000.0,; ""log_mean_bias_std"": 0.1,; ""init_ard_rel_unexplained_variance"": 0.1,; ""num_gc_bins"": 20,; ""gc_curve_sd"": 1.0,; ""q_c_expectation_mode"": ""hybrid"",; ""active_class_padding_hybrid_mode"": 50000,; ""enable_bias_factors"": false,; ""enable_explicit_gc_bias_modeling"": false,; ""disable_bias_factors_in_active_class"": false; ""version"": ""0.7""; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4824
https://github.com/broadinstitute/gatk/issues/4825:1135,Availability,down,down,1135,"/io_commons.py"", line 98, in assert_output_path_writable; filehandle = open(filename, 'w'); PermissionError: [Errno 13] Permission denied: '/home/shlee/gcc/hc24_soohee1k_chr1-model/write_tester'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/shlee/cohort_denoising_calling.7832183760446168530.py"", line 151, in <module>; args.output_model_path)(); File ""/home/shlee/anaconda3/envs/gatk/lib/python3.6/site-packages/gcnvkernel/io/io_denoising_calling.py"", line 28, in __init__; io_commons.assert_output_path_writable(output_path); File ""/home/shlee/anaconda3/envs/gatk/lib/python3.6/site-packages/gcnvkernel/io/io_commons.py"", line 102, in assert_output_path_writable; raise IOError(""The output path \""{0}\"" is not writeable"".format(output_path)); OSError: The output path ""/home/shlee/gcc/hc24_soohee1k_chr1-model"" is not writeable; 16:26:00.659 DEBUG ScriptExecutor - Result: 1; 16:26:00.662 INFO GermlineCNVCaller - Shutting down engine; [May 27, 2018 4:26:00 PM UTC] org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller done. Elapsed time: 2,255.34 minutes.; Runtime.totalMemory()=8207728640; org.broadinstitute.hellbender.utils.python.PythonScriptExecutorException: ; python exited with 1; Command Line: python /tmp/shlee/cohort_denoising_calling.7832183760446168530.py --ploidy_calls_path=/home/shlee/gcnv/coverage_1k/hc24_soohee1kall_ploidy-calls --output_calls_path=/home/shlee/gcc/hc24_soohee1k_chr1-calls --modeling_interval_list=/tmp/shlee/intervals1147946183347323472.tsv --output_model_path=/home/shlee/gcc/hc24_soohee1k_chr1-model --enable_explicit_gc_bias_modeling=False --read_count_tsv_files /tmp/shlee/sample-04516283083315244626.tsv /tmp/shlee/sample-17497576995757363646.tsv /tmp/shlee/sample-21271002324475135098.tsv /tmp/shlee/sample-36985602309924438312.tsv /tmp/shlee/sample-44773997237633003175.tsv /tmp/shlee/sample-55563425618633690228.tsv /tmp/shlee/sample-66588087553393228850.tsv /tmp/shlee/",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4825
https://github.com/broadinstitute/gatk/issues/4826:6339,Availability,down,down,6339,".soohee1k.hdf5 (15 / 24); 21:55:02.822 INFO GermlineCNVCaller - Aggregating read-count file /home/shlee/gcnv/low_coverage_1k/HG03642.lc.soohee1k.hdf5 (16 / 24); 21:55:04.931 INFO GermlineCNVCaller - Aggregating read-count file /home/shlee/gcnv/low_coverage_1k/HG03742.lc.soohee1k.hdf5 (17 / 24); 21:55:06.457 INFO GermlineCNVCaller - Aggregating read-count file /home/shlee/gcnv/low_coverage_1k/NA18525.lc.soohee1k.hdf5 (18 / 24); 21:55:07.933 INFO GermlineCNVCaller - Aggregating read-count file /home/shlee/gcnv/low_coverage_1k/NA18939.lc.soohee1k.hdf5 (19 / 24); 21:55:09.347 INFO GermlineCNVCaller - Aggregating read-count file /home/shlee/gcnv/low_coverage_1k/NA19017.lc.soohee1k.hdf5 (20 / 24); 21:55:11.068 INFO GermlineCNVCaller - Aggregating read-count file /home/shlee/gcnv/low_coverage_1k/NA19625.lc.soohee1k.hdf5 (21 / 24); 21:55:13.479 INFO GermlineCNVCaller - Aggregating read-count file /home/shlee/gcnv/low_coverage_1k/NA19648.lc.soohee1k.hdf5 (22 / 24); 21:55:15.323 INFO GermlineCNVCaller - Aggregating read-count file /home/shlee/gcnv/low_coverage_1k/NA20502.lc.soohee1k.hdf5 (23 / 24); 21:55:17.219 INFO GermlineCNVCaller - Aggregating read-count file /home/shlee/gcnv/low_coverage_1k/NA20845.lc.soohee1k.hdf5 (24 / 24); 01:27:10.674 INFO GermlineCNVCaller - Germline denoising and CNV calling complete.; 01:27:10.676 INFO GermlineCNVCaller - Shutting down engine; [May 29, 2018 1:27:10 AM UTC] org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller done. Elapsed time: 212.70 minutes.; Runtime.totalMemory()=5764022272; Tool returned:; SUCCESS; ```. Would be great to have a summary of useful information, e.g. (but not limited to):. - Total number of epochs (INFO level); - ELBO value and SNR value etc for the final epochs (INFO level); - Whether convergence was achieved or not (WARN if not). Currently, the only way to get all three pieces of information is through setting `--verbosity DEBUG`, which makes for very long stdouts that go well beyond what tmux saves.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4826
https://github.com/broadinstitute/gatk/issues/4826:1785,Deployability,patch,patch,1785,"Caller - Start Date/Time: May 28, 2018 9:54:28 PM UTC; 21:54:28.625 INFO GermlineCNVCaller - ------------------------------------------------------------; 21:54:28.625 INFO GermlineCNVCaller - ------------------------------------------------------------; 21:54:28.626 INFO GermlineCNVCaller - HTSJDK Version: 2.14.3; 21:54:28.626 INFO GermlineCNVCaller - Picard Version: 2.18.2; 21:54:28.626 INFO GermlineCNVCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 21:54:28.626 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 21:54:28.626 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 21:54:28.626 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 21:54:28.627 INFO GermlineCNVCaller - Deflater: IntelDeflater; 21:54:28.627 INFO GermlineCNVCaller - Inflater: IntelInflater; 21:54:28.627 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 21:54:28.627 INFO GermlineCNVCaller - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 21:54:28.627 WARN GermlineCNVCaller - . !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: GermlineCNVCaller is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 21:54:28.627 INFO GermlineCNVCaller - Initializing engine; 21:54:31.994 INFO GermlineCNVCaller - Done initializing engine; log4j:WARN No appenders could be found for logger (org.broadinstitute.hdf5.HDF5Library).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 21:54:33.457 INFO GermlineCNVCaller - Intervals specified...; 21:54:34.113 INFO IntervalArgumentCollection - Processing 10999816 bp from intervals; 21:54:34.145 INFO GermlineCNVCaller - No GC-content annotations for intervals found; explicit GC-bias correction will not be performed",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4826
https://github.com/broadinstitute/gatk/issues/4826:103,Performance,Load,Loading,103,"All that you get is the recapitulated command and, e.g.:; ```; 21:54:28.439 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/shlee/gatk-4.0.4.0/gatk-package-4.0.4.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 21:54:28.625 INFO GermlineCNVCaller - ------------------------------------------------------------; 21:54:28.625 INFO GermlineCNVCaller - The Genome Analysis Toolkit (GATK) v4.0.4.0; 21:54:28.625 INFO GermlineCNVCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 21:54:28.625 INFO GermlineCNVCaller - Executing as shlee@brie on Linux v4.13.0-1017-gcp amd64; 21:54:28.625 INFO GermlineCNVCaller - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_171-8u171-b11-0ubuntu0.16.04.1-b11; 21:54:28.625 INFO GermlineCNVCaller - Start Date/Time: May 28, 2018 9:54:28 PM UTC; 21:54:28.625 INFO GermlineCNVCaller - ------------------------------------------------------------; 21:54:28.625 INFO GermlineCNVCaller - ------------------------------------------------------------; 21:54:28.626 INFO GermlineCNVCaller - HTSJDK Version: 2.14.3; 21:54:28.626 INFO GermlineCNVCaller - Picard Version: 2.18.2; 21:54:28.626 INFO GermlineCNVCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 21:54:28.626 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 21:54:28.626 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 21:54:28.626 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 21:54:28.627 INFO GermlineCNVCaller - Deflater: IntelDeflater; 21:54:28.627 INFO GermlineCNVCaller - Inflater: IntelInflater; 21:54:28.627 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 21:54:28.627 INFO GermlineCNVCaller - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 21:54:28.627 WARN GermlineCNVCaller - . !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4826
https://github.com/broadinstitute/gatk/issues/4826:2783,Performance,perform,performed,2783,8.627 INFO GermlineCNVCaller - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 21:54:28.627 WARN GermlineCNVCaller - . !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: GermlineCNVCaller is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 21:54:28.627 INFO GermlineCNVCaller - Initializing engine; 21:54:31.994 INFO GermlineCNVCaller - Done initializing engine; log4j:WARN No appenders could be found for logger (org.broadinstitute.hdf5.HDF5Library).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 21:54:33.457 INFO GermlineCNVCaller - Intervals specified...; 21:54:34.113 INFO IntervalArgumentCollection - Processing 10999816 bp from intervals; 21:54:34.145 INFO GermlineCNVCaller - No GC-content annotations for intervals found; explicit GC-bias correction will not be performed...; 21:54:34.217 INFO GermlineCNVCaller - Running the tool in the COHORT mode...; 21:54:34.217 INFO GermlineCNVCaller - Validating and aggregating data from input read-count files...; 21:54:34.241 INFO GermlineCNVCaller - Aggregating read-count file /home/shlee/gcnv/low_coverage_1k/HG00096.lc.soohee1k.hdf5 (1 / 24); 21:54:36.539 INFO GermlineCNVCaller - Aggregating read-count file /home/shlee/gcnv/low_coverage_1k/HG00268.lc.soohee1k.hdf5 (2 / 24); 21:54:37.967 INFO GermlineCNVCaller - Aggregating read-count file /home/shlee/gcnv/low_coverage_1k/HG00419.lc.soohee1k.hdf5 (3 / 24); 21:54:40.147 INFO GermlineCNVCaller - Aggregating read-count file /home/shlee/gcnv/low_coverage_1k/HG00759.lc.soohee1k.hdf5 (4 / 24); 21:54:41.782 INFO GermlineCNVCaller - Aggregating read-count file /home/shlee/gcnv/low_coverage_1k/HG01051.lc.soohee1k.hdf5 (5 / 24); 21:54:43.197 INFO GermlineCNVCaller - Aggregating read-count file /home/shlee/gcnv/,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4826
https://github.com/broadinstitute/gatk/issues/4826:2913,Security,Validat,Validating,2913,ineCNVCaller - . !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: GermlineCNVCaller is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 21:54:28.627 INFO GermlineCNVCaller - Initializing engine; 21:54:31.994 INFO GermlineCNVCaller - Done initializing engine; log4j:WARN No appenders could be found for logger (org.broadinstitute.hdf5.HDF5Library).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 21:54:33.457 INFO GermlineCNVCaller - Intervals specified...; 21:54:34.113 INFO IntervalArgumentCollection - Processing 10999816 bp from intervals; 21:54:34.145 INFO GermlineCNVCaller - No GC-content annotations for intervals found; explicit GC-bias correction will not be performed...; 21:54:34.217 INFO GermlineCNVCaller - Running the tool in the COHORT mode...; 21:54:34.217 INFO GermlineCNVCaller - Validating and aggregating data from input read-count files...; 21:54:34.241 INFO GermlineCNVCaller - Aggregating read-count file /home/shlee/gcnv/low_coverage_1k/HG00096.lc.soohee1k.hdf5 (1 / 24); 21:54:36.539 INFO GermlineCNVCaller - Aggregating read-count file /home/shlee/gcnv/low_coverage_1k/HG00268.lc.soohee1k.hdf5 (2 / 24); 21:54:37.967 INFO GermlineCNVCaller - Aggregating read-count file /home/shlee/gcnv/low_coverage_1k/HG00419.lc.soohee1k.hdf5 (3 / 24); 21:54:40.147 INFO GermlineCNVCaller - Aggregating read-count file /home/shlee/gcnv/low_coverage_1k/HG00759.lc.soohee1k.hdf5 (4 / 24); 21:54:41.782 INFO GermlineCNVCaller - Aggregating read-count file /home/shlee/gcnv/low_coverage_1k/HG01051.lc.soohee1k.hdf5 (5 / 24); 21:54:43.197 INFO GermlineCNVCaller - Aggregating read-count file /home/shlee/gcnv/low_coverage_1k/HG01112.lc.soohee1k.hdf5 (6 / 24); 21:54:45.169 INFO GermlineCNVCaller - Aggregating read-count file /home/shlee/gcnv/low_coverage_1k/HG01500.lc.soohee1k.hdf5 (7 / 24); 21:54:46.852 I,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4826
https://github.com/broadinstitute/gatk/issues/4826:2321,Testability,log,logger,2321,aller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 21:54:28.626 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 21:54:28.626 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 21:54:28.627 INFO GermlineCNVCaller - Deflater: IntelDeflater; 21:54:28.627 INFO GermlineCNVCaller - Inflater: IntelInflater; 21:54:28.627 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 21:54:28.627 INFO GermlineCNVCaller - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 21:54:28.627 WARN GermlineCNVCaller - . !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: GermlineCNVCaller is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 21:54:28.627 INFO GermlineCNVCaller - Initializing engine; 21:54:31.994 INFO GermlineCNVCaller - Done initializing engine; log4j:WARN No appenders could be found for logger (org.broadinstitute.hdf5.HDF5Library).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 21:54:33.457 INFO GermlineCNVCaller - Intervals specified...; 21:54:34.113 INFO IntervalArgumentCollection - Processing 10999816 bp from intervals; 21:54:34.145 INFO GermlineCNVCaller - No GC-content annotations for intervals found; explicit GC-bias correction will not be performed...; 21:54:34.217 INFO GermlineCNVCaller - Running the tool in the COHORT mode...; 21:54:34.217 INFO GermlineCNVCaller - Validating and aggregating data from input read-count files...; 21:54:34.241 INFO GermlineCNVCaller - Aggregating read-count file /home/shlee/gcnv/low_coverage_1k/HG00096.lc.soohee1k.hdf5 (1 / 24); 21:54:36.539 INFO GermlineCNVCaller - Aggregating read-count file /home/shlee/gcnv/low_coverage_1k/HG00268.lc.soohee1k.hdf5 (2 / 24); 21:54:37.967 INFO GermlineCNVCall,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4826
https://github.com/broadinstitute/gatk/issues/4826:2447,Testability,log,logging,2447,54:28.626 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 21:54:28.627 INFO GermlineCNVCaller - Deflater: IntelDeflater; 21:54:28.627 INFO GermlineCNVCaller - Inflater: IntelInflater; 21:54:28.627 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 21:54:28.627 INFO GermlineCNVCaller - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 21:54:28.627 WARN GermlineCNVCaller - . !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: GermlineCNVCaller is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 21:54:28.627 INFO GermlineCNVCaller - Initializing engine; 21:54:31.994 INFO GermlineCNVCaller - Done initializing engine; log4j:WARN No appenders could be found for logger (org.broadinstitute.hdf5.HDF5Library).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 21:54:33.457 INFO GermlineCNVCaller - Intervals specified...; 21:54:34.113 INFO IntervalArgumentCollection - Processing 10999816 bp from intervals; 21:54:34.145 INFO GermlineCNVCaller - No GC-content annotations for intervals found; explicit GC-bias correction will not be performed...; 21:54:34.217 INFO GermlineCNVCaller - Running the tool in the COHORT mode...; 21:54:34.217 INFO GermlineCNVCaller - Validating and aggregating data from input read-count files...; 21:54:34.241 INFO GermlineCNVCaller - Aggregating read-count file /home/shlee/gcnv/low_coverage_1k/HG00096.lc.soohee1k.hdf5 (1 / 24); 21:54:36.539 INFO GermlineCNVCaller - Aggregating read-count file /home/shlee/gcnv/low_coverage_1k/HG00268.lc.soohee1k.hdf5 (2 / 24); 21:54:37.967 INFO GermlineCNVCaller - Aggregating read-count file /home/shlee/gcnv/low_coverage_1k/HG00419.lc.soohee1k.hdf5 (3 / 24); 21:54:40.147 INFO GermlineCNVCaller - Aggregating read-count,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4826
https://github.com/broadinstitute/gatk/pull/4827:4,Testability,log,log-normal,4,nd log-normal distributed) parametrized by. insert size mean and stddev,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4827
https://github.com/broadinstitute/gatk/issues/4829:1696,Testability,log,log,1696,"HaplotypeCaller and Mutect2:. ![screen shot 2018-05-30 at 2 36 58 pm](https://user-images.githubusercontent.com/8438890/40759148-b860a5da-645e-11e8-8f94-21534ed9ab48.png). Notice how all of the reads that appear to support a single-base deletion actually do not because they do not span the poly-T homopolymer. This makes the bamout harder to interpret. In this particular case (of a false positive M2 insertion) it is a red herring because it suggests that the multiallelicness of the site is relevant to the false positive deletion. One could ask what in the GATK engine is responsible. * The assembly engine, perhaps? No, it is the assembly engine's job to propose possible haplotypes, not to call them. In any case, there *is* one spanning read with the deletion above the reads shown, so it is a valid path in the graph.; * Pair-HMM? This one confused me for a while, but no. The engine is *not* saying that these reads' best alignment to the reference has a deletion, which would be false because there is a gap opening penalty. Rather, it says that they align equally well (with no deletions) to the ref haplotype and to the deletion haplotype. The deletion shown in IGV is the deletion of the alt haplotype relative to the reference, not of the reads relative to their best haplotype.; * The bamout writer? Nope, that code is really straightforward and does the right thing. So what's the issue? Well, the bamout writer gets its read alignments from the `readLikelihoods` after the reads have been realigned to their best haplotype. In these cases, it turns out that the alignment of the reads to their best haplotype, the deletion has a log likelihood better than the alignment to the ref haplotype by about 0.00001. The simplest solution would be to give an extremely modest prior in favor of the reference and break these near-ties in favor of the reference. @droazen @ldgauthier @yfarjoun if you think this is a good idea I can fix it for both HC and M2. Otherwise I'll do an M2-only fix.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4829
https://github.com/broadinstitute/gatk/issues/4829:1780,Usability,simpl,simplest,1780,"HaplotypeCaller and Mutect2:. ![screen shot 2018-05-30 at 2 36 58 pm](https://user-images.githubusercontent.com/8438890/40759148-b860a5da-645e-11e8-8f94-21534ed9ab48.png). Notice how all of the reads that appear to support a single-base deletion actually do not because they do not span the poly-T homopolymer. This makes the bamout harder to interpret. In this particular case (of a false positive M2 insertion) it is a red herring because it suggests that the multiallelicness of the site is relevant to the false positive deletion. One could ask what in the GATK engine is responsible. * The assembly engine, perhaps? No, it is the assembly engine's job to propose possible haplotypes, not to call them. In any case, there *is* one spanning read with the deletion above the reads shown, so it is a valid path in the graph.; * Pair-HMM? This one confused me for a while, but no. The engine is *not* saying that these reads' best alignment to the reference has a deletion, which would be false because there is a gap opening penalty. Rather, it says that they align equally well (with no deletions) to the ref haplotype and to the deletion haplotype. The deletion shown in IGV is the deletion of the alt haplotype relative to the reference, not of the reads relative to their best haplotype.; * The bamout writer? Nope, that code is really straightforward and does the right thing. So what's the issue? Well, the bamout writer gets its read alignments from the `readLikelihoods` after the reads have been realigned to their best haplotype. In these cases, it turns out that the alignment of the reads to their best haplotype, the deletion has a log likelihood better than the alignment to the ref haplotype by about 0.00001. The simplest solution would be to give an extremely modest prior in favor of the reference and break these near-ties in favor of the reference. @droazen @ldgauthier @yfarjoun if you think this is a good idea I can fix it for both HC and M2. Otherwise I'll do an M2-only fix.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4829
https://github.com/broadinstitute/gatk/pull/4835:887,Modifiability,refactor,refactored,887,"This is to overhaul tests on SV assembly-based non-complex breakpoint and type inference code. ------. What is done:. * new AssemblyBasedSVDiscoveryTestDataProvider classes which hold manually computed expected values. Unit tests simply load the expected values and compare with actual values calculated on the fly; * fix bugs in BND formatted variants and tested (using the structure above). Classes affected in `main` ; * `AnnotatedVariantProducer`: methods are grouped together and renamed to reflect that the annotations added are assembly-specific or using short reads; * `BreakEndVariantType`: more detailed types (mostly about how alt allele, with the ref bases and square brackets); * `BreakpointComplications` and `BreakpointsInference`: mostly to add trivial methods used only in tests; `DiscoverVariantsFromContigAlignmentsSAMSpark`: now a thin CLI, where functionalities are refactored into new class `ContigChimericAlignmentIterativeInterpreter`; * `SimpleChimera`: new documented and tested method `firstContigRegionRefSpanAfterSecond ` and trivial test-related code",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4835
https://github.com/broadinstitute/gatk/pull/4835:237,Performance,load,load,237,"This is to overhaul tests on SV assembly-based non-complex breakpoint and type inference code. ------. What is done:. * new AssemblyBasedSVDiscoveryTestDataProvider classes which hold manually computed expected values. Unit tests simply load the expected values and compare with actual values calculated on the fly; * fix bugs in BND formatted variants and tested (using the structure above). Classes affected in `main` ; * `AnnotatedVariantProducer`: methods are grouped together and renamed to reflect that the annotations added are assembly-specific or using short reads; * `BreakEndVariantType`: more detailed types (mostly about how alt allele, with the ref bases and square brackets); * `BreakpointComplications` and `BreakpointsInference`: mostly to add trivial methods used only in tests; `DiscoverVariantsFromContigAlignmentsSAMSpark`: now a thin CLI, where functionalities are refactored into new class `ContigChimericAlignmentIterativeInterpreter`; * `SimpleChimera`: new documented and tested method `firstContigRegionRefSpanAfterSecond ` and trivial test-related code",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4835
https://github.com/broadinstitute/gatk/pull/4835:20,Testability,test,tests,20,"This is to overhaul tests on SV assembly-based non-complex breakpoint and type inference code. ------. What is done:. * new AssemblyBasedSVDiscoveryTestDataProvider classes which hold manually computed expected values. Unit tests simply load the expected values and compare with actual values calculated on the fly; * fix bugs in BND formatted variants and tested (using the structure above). Classes affected in `main` ; * `AnnotatedVariantProducer`: methods are grouped together and renamed to reflect that the annotations added are assembly-specific or using short reads; * `BreakEndVariantType`: more detailed types (mostly about how alt allele, with the ref bases and square brackets); * `BreakpointComplications` and `BreakpointsInference`: mostly to add trivial methods used only in tests; `DiscoverVariantsFromContigAlignmentsSAMSpark`: now a thin CLI, where functionalities are refactored into new class `ContigChimericAlignmentIterativeInterpreter`; * `SimpleChimera`: new documented and tested method `firstContigRegionRefSpanAfterSecond ` and trivial test-related code",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4835
https://github.com/broadinstitute/gatk/pull/4835:224,Testability,test,tests,224,"This is to overhaul tests on SV assembly-based non-complex breakpoint and type inference code. ------. What is done:. * new AssemblyBasedSVDiscoveryTestDataProvider classes which hold manually computed expected values. Unit tests simply load the expected values and compare with actual values calculated on the fly; * fix bugs in BND formatted variants and tested (using the structure above). Classes affected in `main` ; * `AnnotatedVariantProducer`: methods are grouped together and renamed to reflect that the annotations added are assembly-specific or using short reads; * `BreakEndVariantType`: more detailed types (mostly about how alt allele, with the ref bases and square brackets); * `BreakpointComplications` and `BreakpointsInference`: mostly to add trivial methods used only in tests; `DiscoverVariantsFromContigAlignmentsSAMSpark`: now a thin CLI, where functionalities are refactored into new class `ContigChimericAlignmentIterativeInterpreter`; * `SimpleChimera`: new documented and tested method `firstContigRegionRefSpanAfterSecond ` and trivial test-related code",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4835
https://github.com/broadinstitute/gatk/pull/4835:357,Testability,test,tested,357,"This is to overhaul tests on SV assembly-based non-complex breakpoint and type inference code. ------. What is done:. * new AssemblyBasedSVDiscoveryTestDataProvider classes which hold manually computed expected values. Unit tests simply load the expected values and compare with actual values calculated on the fly; * fix bugs in BND formatted variants and tested (using the structure above). Classes affected in `main` ; * `AnnotatedVariantProducer`: methods are grouped together and renamed to reflect that the annotations added are assembly-specific or using short reads; * `BreakEndVariantType`: more detailed types (mostly about how alt allele, with the ref bases and square brackets); * `BreakpointComplications` and `BreakpointsInference`: mostly to add trivial methods used only in tests; `DiscoverVariantsFromContigAlignmentsSAMSpark`: now a thin CLI, where functionalities are refactored into new class `ContigChimericAlignmentIterativeInterpreter`; * `SimpleChimera`: new documented and tested method `firstContigRegionRefSpanAfterSecond ` and trivial test-related code",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4835
https://github.com/broadinstitute/gatk/pull/4835:790,Testability,test,tests,790,"This is to overhaul tests on SV assembly-based non-complex breakpoint and type inference code. ------. What is done:. * new AssemblyBasedSVDiscoveryTestDataProvider classes which hold manually computed expected values. Unit tests simply load the expected values and compare with actual values calculated on the fly; * fix bugs in BND formatted variants and tested (using the structure above). Classes affected in `main` ; * `AnnotatedVariantProducer`: methods are grouped together and renamed to reflect that the annotations added are assembly-specific or using short reads; * `BreakEndVariantType`: more detailed types (mostly about how alt allele, with the ref bases and square brackets); * `BreakpointComplications` and `BreakpointsInference`: mostly to add trivial methods used only in tests; `DiscoverVariantsFromContigAlignmentsSAMSpark`: now a thin CLI, where functionalities are refactored into new class `ContigChimericAlignmentIterativeInterpreter`; * `SimpleChimera`: new documented and tested method `firstContigRegionRefSpanAfterSecond ` and trivial test-related code",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4835
https://github.com/broadinstitute/gatk/pull/4835:998,Testability,test,tested,998,"This is to overhaul tests on SV assembly-based non-complex breakpoint and type inference code. ------. What is done:. * new AssemblyBasedSVDiscoveryTestDataProvider classes which hold manually computed expected values. Unit tests simply load the expected values and compare with actual values calculated on the fly; * fix bugs in BND formatted variants and tested (using the structure above). Classes affected in `main` ; * `AnnotatedVariantProducer`: methods are grouped together and renamed to reflect that the annotations added are assembly-specific or using short reads; * `BreakEndVariantType`: more detailed types (mostly about how alt allele, with the ref bases and square brackets); * `BreakpointComplications` and `BreakpointsInference`: mostly to add trivial methods used only in tests; `DiscoverVariantsFromContigAlignmentsSAMSpark`: now a thin CLI, where functionalities are refactored into new class `ContigChimericAlignmentIterativeInterpreter`; * `SimpleChimera`: new documented and tested method `firstContigRegionRefSpanAfterSecond ` and trivial test-related code",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4835
https://github.com/broadinstitute/gatk/pull/4835:1063,Testability,test,test-related,1063,"This is to overhaul tests on SV assembly-based non-complex breakpoint and type inference code. ------. What is done:. * new AssemblyBasedSVDiscoveryTestDataProvider classes which hold manually computed expected values. Unit tests simply load the expected values and compare with actual values calculated on the fly; * fix bugs in BND formatted variants and tested (using the structure above). Classes affected in `main` ; * `AnnotatedVariantProducer`: methods are grouped together and renamed to reflect that the annotations added are assembly-specific or using short reads; * `BreakEndVariantType`: more detailed types (mostly about how alt allele, with the ref bases and square brackets); * `BreakpointComplications` and `BreakpointsInference`: mostly to add trivial methods used only in tests; `DiscoverVariantsFromContigAlignmentsSAMSpark`: now a thin CLI, where functionalities are refactored into new class `ContigChimericAlignmentIterativeInterpreter`; * `SimpleChimera`: new documented and tested method `firstContigRegionRefSpanAfterSecond ` and trivial test-related code",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4835
https://github.com/broadinstitute/gatk/pull/4835:230,Usability,simpl,simply,230,"This is to overhaul tests on SV assembly-based non-complex breakpoint and type inference code. ------. What is done:. * new AssemblyBasedSVDiscoveryTestDataProvider classes which hold manually computed expected values. Unit tests simply load the expected values and compare with actual values calculated on the fly; * fix bugs in BND formatted variants and tested (using the structure above). Classes affected in `main` ; * `AnnotatedVariantProducer`: methods are grouped together and renamed to reflect that the annotations added are assembly-specific or using short reads; * `BreakEndVariantType`: more detailed types (mostly about how alt allele, with the ref bases and square brackets); * `BreakpointComplications` and `BreakpointsInference`: mostly to add trivial methods used only in tests; `DiscoverVariantsFromContigAlignmentsSAMSpark`: now a thin CLI, where functionalities are refactored into new class `ContigChimericAlignmentIterativeInterpreter`; * `SimpleChimera`: new documented and tested method `firstContigRegionRefSpanAfterSecond ` and trivial test-related code",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4835
https://github.com/broadinstitute/gatk/pull/4835:963,Usability,Simpl,SimpleChimera,963,"This is to overhaul tests on SV assembly-based non-complex breakpoint and type inference code. ------. What is done:. * new AssemblyBasedSVDiscoveryTestDataProvider classes which hold manually computed expected values. Unit tests simply load the expected values and compare with actual values calculated on the fly; * fix bugs in BND formatted variants and tested (using the structure above). Classes affected in `main` ; * `AnnotatedVariantProducer`: methods are grouped together and renamed to reflect that the annotations added are assembly-specific or using short reads; * `BreakEndVariantType`: more detailed types (mostly about how alt allele, with the ref bases and square brackets); * `BreakpointComplications` and `BreakpointsInference`: mostly to add trivial methods used only in tests; `DiscoverVariantsFromContigAlignmentsSAMSpark`: now a thin CLI, where functionalities are refactored into new class `ContigChimericAlignmentIterativeInterpreter`; * `SimpleChimera`: new documented and tested method `firstContigRegionRefSpanAfterSecond ` and trivial test-related code",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4835
https://github.com/broadinstitute/gatk/pull/4838:85,Modifiability,refactor,refactoring,85,Closes #3842 ; Closes #4808 ; Closes #4811 ; Closes #4810 ; Closes #4839. - A lot of refactoring to create `FuncotationMap` and use that to send to OutputRenderers.; - VCF and MAF will honor the canonical vs. all vs best effect.; - MAF entries will now be rendered as `AnnotatedInterval` via tribble,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4838
https://github.com/broadinstitute/gatk/issues/4839:12,Deployability,integrat,integration,12,This causes integration tests to fail.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4839
https://github.com/broadinstitute/gatk/issues/4839:12,Integrability,integrat,integration,12,This causes integration tests to fail.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4839
https://github.com/broadinstitute/gatk/issues/4839:24,Testability,test,tests,24,This causes integration tests to fail.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4839
https://github.com/broadinstitute/gatk/issues/4840:78,Availability,error,errors,78,Random and unplaced contigs are a part of the primary assembly. The tool also errors on decoy contigs but no other GRCh38 contig type. This ticket is to investigate why this error is happening for this subset of the data. Error is; ```; Stdout: 22:01:54.365 INFO segment_gcnv_calls - Loading ploidy calls...; 22:01:54.366 INFO gcnvkernel.io.io_metadata - Loading germline contig ploidy and global read depth metadata...; 22:01:54.381 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chrM). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.381 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270706v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.381 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270707v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.381 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270708v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.381 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270709v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270710v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270711v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270712v1_random). Germline contig ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4840
https://github.com/broadinstitute/gatk/issues/4840:174,Availability,error,error,174,Random and unplaced contigs are a part of the primary assembly. The tool also errors on decoy contigs but no other GRCh38 contig type. This ticket is to investigate why this error is happening for this subset of the data. Error is; ```; Stdout: 22:01:54.365 INFO segment_gcnv_calls - Loading ploidy calls...; 22:01:54.366 INFO gcnvkernel.io.io_metadata - Loading germline contig ploidy and global read depth metadata...; 22:01:54.381 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chrM). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.381 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270706v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.381 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270707v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.381 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270708v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.381 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270709v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270710v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270711v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270712v1_random). Germline contig ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4840
https://github.com/broadinstitute/gatk/issues/4840:222,Availability,Error,Error,222,Random and unplaced contigs are a part of the primary assembly. The tool also errors on decoy contigs but no other GRCh38 contig type. This ticket is to investigate why this error is happening for this subset of the data. Error is; ```; Stdout: 22:01:54.365 INFO segment_gcnv_calls - Loading ploidy calls...; 22:01:54.366 INFO gcnvkernel.io.io_metadata - Loading germline contig ploidy and global read depth metadata...; 22:01:54.381 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chrM). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.381 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270706v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.381 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270707v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.381 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270708v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.381 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270709v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270710v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270711v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270712v1_random). Germline contig ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4840
https://github.com/broadinstitute/gatk/issues/4840:570,Availability,reliab,reliable,570,Random and unplaced contigs are a part of the primary assembly. The tool also errors on decoy contigs but no other GRCh38 contig type. This ticket is to investigate why this error is happening for this subset of the data. Error is; ```; Stdout: 22:01:54.365 INFO segment_gcnv_calls - Loading ploidy calls...; 22:01:54.366 INFO gcnvkernel.io.io_metadata - Loading germline contig ploidy and global read depth metadata...; 22:01:54.381 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chrM). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.381 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270706v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.381 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270707v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.381 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270708v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.381 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270709v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270710v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270711v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270712v1_random). Germline contig ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4840
https://github.com/broadinstitute/gatk/issues/4840:779,Availability,reliab,reliable,779,Random and unplaced contigs are a part of the primary assembly. The tool also errors on decoy contigs but no other GRCh38 contig type. This ticket is to investigate why this error is happening for this subset of the data. Error is; ```; Stdout: 22:01:54.365 INFO segment_gcnv_calls - Loading ploidy calls...; 22:01:54.366 INFO gcnvkernel.io.io_metadata - Loading germline contig ploidy and global read depth metadata...; 22:01:54.381 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chrM). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.381 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270706v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.381 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270707v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.381 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270708v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.381 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270709v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270710v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270711v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270712v1_random). Germline contig ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4840
https://github.com/broadinstitute/gatk/issues/4840:988,Availability,reliab,reliable,988,Random and unplaced contigs are a part of the primary assembly. The tool also errors on decoy contigs but no other GRCh38 contig type. This ticket is to investigate why this error is happening for this subset of the data. Error is; ```; Stdout: 22:01:54.365 INFO segment_gcnv_calls - Loading ploidy calls...; 22:01:54.366 INFO gcnvkernel.io.io_metadata - Loading germline contig ploidy and global read depth metadata...; 22:01:54.381 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chrM). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.381 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270706v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.381 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270707v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.381 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270708v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.381 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270709v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270710v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270711v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270712v1_random). Germline contig ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4840
https://github.com/broadinstitute/gatk/issues/4840:1197,Availability,reliab,reliable,1197, for this subset of the data. Error is; ```; Stdout: 22:01:54.365 INFO segment_gcnv_calls - Loading ploidy calls...; 22:01:54.366 INFO gcnvkernel.io.io_metadata - Loading germline contig ploidy and global read depth metadata...; 22:01:54.381 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chrM). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.381 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270706v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.381 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270707v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.381 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270708v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.381 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270709v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270710v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270711v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270712v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270713v1_random).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4840
https://github.com/broadinstitute/gatk/issues/4840:1406,Availability,reliab,reliable,1406, depth metadata...; 22:01:54.381 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chrM). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.381 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270706v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.381 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270707v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.381 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270708v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.381 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270709v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270710v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270711v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270712v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270713v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270714v1_random).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4840
https://github.com/broadinstitute/gatk/issues/4840:1615,Availability,reliab,reliable,1615,; 22:01:54.381 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270706v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.381 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270707v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.381 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270708v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.381 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270709v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270710v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270711v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270712v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270713v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270714v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr2_KI270715v1_random).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4840
https://github.com/broadinstitute/gatk/issues/4840:1824,Availability,reliab,reliable,1824,; 22:01:54.381 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270707v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.381 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270708v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.381 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270709v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270710v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270711v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270712v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270713v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270714v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr2_KI270715v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr2_KI270716v1_random).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4840
https://github.com/broadinstitute/gatk/issues/4840:2033,Availability,reliab,reliable,2033,; 22:01:54.381 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270708v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.381 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270709v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270710v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270711v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270712v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270713v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270714v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr2_KI270715v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr2_KI270716v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr3_GL000221v1_random).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4840
https://github.com/broadinstitute/gatk/issues/4840:2242,Availability,reliab,reliable,2242,; 22:01:54.381 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270709v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270710v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270711v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270712v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270713v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270714v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr2_KI270715v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr2_KI270716v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr3_GL000221v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr4_GL000008v2_random).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4840
https://github.com/broadinstitute/gatk/issues/4840:2451,Availability,reliab,reliable,2451,; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270710v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270711v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270712v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270713v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270714v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr2_KI270715v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr2_KI270716v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr3_GL000221v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr4_GL000008v2_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr5_GL000208v1_random).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4840
https://github.com/broadinstitute/gatk/issues/4840:2660,Availability,reliab,reliable,2660,; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270711v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270712v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270713v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270714v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr2_KI270715v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr2_KI270716v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr3_GL000221v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr4_GL000008v2_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr5_GL000208v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr9_KI270717v1_random).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4840
https://github.com/broadinstitute/gatk/issues/4840:2869,Availability,reliab,reliable,2869,; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270712v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270713v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270714v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr2_KI270715v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr2_KI270716v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr3_GL000221v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr4_GL000008v2_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr5_GL000208v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr9_KI270717v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr9_KI270718v1_random).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4840
https://github.com/broadinstitute/gatk/issues/4840:3078,Availability,reliab,reliable,3078,; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270713v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270714v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr2_KI270715v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr2_KI270716v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr3_GL000221v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr4_GL000008v2_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr5_GL000208v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr9_KI270717v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr9_KI270718v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr9_KI270719v1_random).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4840
https://github.com/broadinstitute/gatk/issues/4840:3287,Availability,reliab,reliable,3287,; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270714v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr2_KI270715v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr2_KI270716v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr3_GL000221v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr4_GL000008v2_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr5_GL000208v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr9_KI270717v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr9_KI270718v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr9_KI270719v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr9_KI270720v1_random).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4840
https://github.com/broadinstitute/gatk/issues/4840:3496,Availability,reliab,reliable,3496,; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr2_KI270715v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr2_KI270716v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr3_GL000221v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr4_GL000008v2_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr5_GL000208v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr9_KI270717v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr9_KI270718v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr9_KI270719v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr9_KI270720v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr11_KI270721v1_random),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4840
https://github.com/broadinstitute/gatk/issues/4840:3705,Availability,reliab,reliable,3705,; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr2_KI270716v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr3_GL000221v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr4_GL000008v2_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr5_GL000208v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr9_KI270717v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr9_KI270718v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr9_KI270719v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr9_KI270720v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr11_KI270721v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr14_GL000009v2_random,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4840
https://github.com/broadinstitute/gatk/issues/4840:3914,Availability,reliab,reliable,3914,; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr3_GL000221v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr4_GL000008v2_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr5_GL000208v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr9_KI270717v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr9_KI270718v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr9_KI270719v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr9_KI270720v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr11_KI270721v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr14_GL000009v2_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr14_GL000225v1_rando,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4840
https://github.com/broadinstitute/gatk/issues/4840:4123,Availability,reliab,reliable,4123,; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr4_GL000008v2_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr5_GL000208v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr9_KI270717v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr9_KI270718v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr9_KI270719v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr9_KI270720v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr11_KI270721v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr14_GL000009v2_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr14_GL000225v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr14_KI270722v1_rand,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4840
https://github.com/broadinstitute/gatk/issues/4840:4332,Availability,reliab,reliable,4332,; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr5_GL000208v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr9_KI270717v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr9_KI270718v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr9_KI270719v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr9_KI270720v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr11_KI270721v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr14_GL000009v2_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr14_GL000225v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr14_KI270722v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr14_GL000194v1_ran,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4840
https://github.com/broadinstitute/gatk/issues/4840:4542,Availability,reliab,reliable,4542, 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr9_KI270717v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr9_KI270718v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr9_KI270719v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr9_KI270720v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr11_KI270721v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr14_GL000009v2_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr14_GL000225v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr14_KI270722v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr14_GL000194v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr14_KI270723v1_ran,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4840
https://github.com/broadinstitute/gatk/issues/4840:4752,Availability,reliab,reliable,4752,22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr9_KI270718v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr9_KI270719v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr9_KI270720v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr11_KI270721v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr14_GL000009v2_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr14_GL000225v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr14_KI270722v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr14_GL000194v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr14_KI270723v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr14_KI270724v1_ran,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4840
https://github.com/broadinstitute/gatk/issues/4840:4962,Availability,reliab,reliable,4962,2:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr9_KI270719v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr9_KI270720v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr11_KI270721v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr14_GL000009v2_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr14_GL000225v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr14_KI270722v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr14_GL000194v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr14_KI270723v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr14_KI270724v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr14_KI270725v1_ran,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4840
https://github.com/broadinstitute/gatk/issues/4840:5172,Availability,reliab,reliable,5172,:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr9_KI270720v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr11_KI270721v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr14_GL000009v2_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr14_GL000225v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr14_KI270722v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr14_GL000194v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr14_KI270723v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr14_KI270724v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr14_KI270725v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr14_KI270726v1_ran,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4840
https://github.com/broadinstitute/gatk/issues/4840:5382,Availability,reliab,reliable,5382,01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr11_KI270721v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr14_GL000009v2_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr14_GL000225v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr14_KI270722v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr14_GL000194v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr14_KI270723v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr14_KI270724v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr14_KI270725v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr14_KI270726v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr15_KI270727v1_ran,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4840
https://github.com/broadinstitute/gatk/issues/4840:5592,Availability,reliab,reliable,5592,01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr14_GL000009v2_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr14_GL000225v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr14_KI270722v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr14_GL000194v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr14_KI270723v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr14_KI270724v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr14_KI270725v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr14_KI270726v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr15_KI270727v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr16_KI270728v1_ran,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4840
https://github.com/broadinstitute/gatk/issues/4840:5802,Availability,reliab,reliable,5802,01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr14_GL000225v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr14_KI270722v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr14_GL000194v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr14_KI270723v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr14_KI270724v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr14_KI270725v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr14_KI270726v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr15_KI270727v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr16_KI270728v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr17_GL000205v2_ran,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4840
https://github.com/broadinstitute/gatk/issues/4840:6012,Availability,reliab,reliable,6012,01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr14_KI270722v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr14_GL000194v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr14_KI270723v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr14_KI270724v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr14_KI270725v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr14_KI270726v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr15_KI270727v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr16_KI270728v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr17_GL000205v2_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr17_KI270729v1_ran,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4840
https://github.com/broadinstitute/gatk/issues/4840:6222,Availability,reliab,reliable,6222,01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr14_GL000194v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr14_KI270723v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr14_KI270724v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr14_KI270725v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr14_KI270726v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr15_KI270727v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr16_KI270728v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr17_GL000205v2_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr17_KI270729v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr17_KI270730v1_ran,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4840
https://github.com/broadinstitute/gatk/issues/4840:6432,Availability,reliab,reliable,6432,01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr14_KI270723v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr14_KI270724v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr14_KI270725v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr14_KI270726v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr15_KI270727v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr16_KI270728v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr17_GL000205v2_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr17_KI270729v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr17_KI270730v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.384 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr22_KI270731v1_ran,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4840
https://github.com/broadinstitute/gatk/issues/4840:6642,Availability,reliab,reliable,6642,01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr14_KI270724v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr14_KI270725v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr14_KI270726v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr15_KI270727v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr16_KI270728v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr17_GL000205v2_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr17_KI270729v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr17_KI270730v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.384 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr22_KI270731v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.384 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr22_KI270732v1_ran,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4840
https://github.com/broadinstitute/gatk/issues/4840:6852,Availability,reliab,reliable,6852,01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr14_KI270725v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr14_KI270726v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr15_KI270727v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr16_KI270728v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr17_GL000205v2_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr17_KI270729v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr17_KI270730v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.384 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr22_KI270731v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.384 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr22_KI270732v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.384 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr22_KI270733v1_ran,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4840
https://github.com/broadinstitute/gatk/issues/4840:7062,Availability,reliab,reliable,7062,01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr14_KI270726v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr15_KI270727v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr16_KI270728v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr17_GL000205v2_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr17_KI270729v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr17_KI270730v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.384 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr22_KI270731v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.384 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr22_KI270732v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.384 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr22_KI270733v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.384 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr22_KI270734v1_ran,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4840
https://github.com/broadinstitute/gatk/issues/4840:7272,Availability,reliab,reliable,7272,01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr15_KI270727v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr16_KI270728v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr17_GL000205v2_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr17_KI270729v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr17_KI270730v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.384 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr22_KI270731v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.384 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr22_KI270732v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.384 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr22_KI270733v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.384 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr22_KI270734v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.384 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr22_KI270735v1_ran,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4840
https://github.com/broadinstitute/gatk/issues/4840:7482,Availability,reliab,reliable,7482,01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr16_KI270728v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr17_GL000205v2_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr17_KI270729v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr17_KI270730v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.384 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr22_KI270731v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.384 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr22_KI270732v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.384 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr22_KI270733v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.384 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr22_KI270734v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.384 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr22_KI270735v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.384 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr22_KI270736v1_ra;,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4840
https://github.com/broadinstitute/gatk/issues/4840:7692,Availability,reliab,reliable,7692,"01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr17_GL000205v2_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr17_KI270729v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr17_KI270730v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.384 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr22_KI270731v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.384 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr22_KI270732v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.384 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr22_KI270733v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.384 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr22_KI270734v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.384 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr22_KI270735v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.384 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr22_KI270736v1_ra; Stderr: Traceback (most recent call last):; File ""/home/shlee/anaconda3/envs/gatk/lib/python3.6/site-packages/theano/compile/function_module.py"", line 884, in __call__; self.fn() if output_subset is None else\",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4840
https://github.com/broadinstitute/gatk/issues/4840:7902,Availability,reliab,reliable,7902,"01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr17_KI270729v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr17_KI270730v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.384 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr22_KI270731v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.384 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr22_KI270732v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.384 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr22_KI270733v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.384 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr22_KI270734v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.384 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr22_KI270735v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.384 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr22_KI270736v1_ra; Stderr: Traceback (most recent call last):; File ""/home/shlee/anaconda3/envs/gatk/lib/python3.6/site-packages/theano/compile/function_module.py"", line 884, in __call__; self.fn() if output_subset is None else\; File ""/home/shlee/anaconda3/envs/gatk/lib/python3.6/site-packages/theano/scan_module/scan_op.py"", line 989, in rval; r = p(n, [x[0] for x in i], o); File ""/home/shlee/anaconda3/envs/gatk/lib/python3.6/site-pa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4840
https://github.com/broadinstitute/gatk/issues/4840:8112,Availability,reliab,reliable,8112,"01:54.383 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr17_KI270730v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.384 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr22_KI270731v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.384 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr22_KI270732v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.384 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr22_KI270733v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.384 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr22_KI270734v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.384 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr22_KI270735v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.384 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr22_KI270736v1_ra; Stderr: Traceback (most recent call last):; File ""/home/shlee/anaconda3/envs/gatk/lib/python3.6/site-packages/theano/compile/function_module.py"", line 884, in __call__; self.fn() if output_subset is None else\; File ""/home/shlee/anaconda3/envs/gatk/lib/python3.6/site-packages/theano/scan_module/scan_op.py"", line 989, in rval; r = p(n, [x[0] for x in i], o); File ""/home/shlee/anaconda3/envs/gatk/lib/python3.6/site-packages/theano/scan_module/scan_op.py"", line 978, in p; self, node); File ""theano/scan_module/scan_perform.pyx"", line 215, in theano.scan_module.scan_perform.perform (/home/shlee/.theano/compiledir_Linux-4.13--g",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4840
https://github.com/broadinstitute/gatk/issues/4840:8322,Availability,reliab,reliable,8322,"01:54.384 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr22_KI270731v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.384 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr22_KI270732v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.384 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr22_KI270733v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.384 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr22_KI270734v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.384 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr22_KI270735v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.384 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr22_KI270736v1_ra; Stderr: Traceback (most recent call last):; File ""/home/shlee/anaconda3/envs/gatk/lib/python3.6/site-packages/theano/compile/function_module.py"", line 884, in __call__; self.fn() if output_subset is None else\; File ""/home/shlee/anaconda3/envs/gatk/lib/python3.6/site-packages/theano/scan_module/scan_op.py"", line 989, in rval; r = p(n, [x[0] for x in i], o); File ""/home/shlee/anaconda3/envs/gatk/lib/python3.6/site-packages/theano/scan_module/scan_op.py"", line 978, in p; self, node); File ""theano/scan_module/scan_perform.pyx"", line 215, in theano.scan_module.scan_perform.perform (/home/shlee/.theano/compiledir_Linux-4.13--gcp-x86_64-with-debian-stretch-sid-x86_64-3.6.2-64/scan_perform/mod.cpp:2628); NotImplementedError: We didn't implemented yet the case where scan do 0 iteration. During handling of the above exception, another e",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4840
https://github.com/broadinstitute/gatk/issues/4840:11467,Availability,error,error,11467,"nvs/gatk/lib/python3.6/site-packages/theano/gof/link.py"", line 325, in raise_with_op; reraise(exc_type, exc_value, exc_trace); File ""/home/shlee/anaconda3/envs/gatk/lib/python3.6/site-packages/six.py"", line 692, in reraise; raise value.with_traceback(tb); File ""/home/shlee/anaconda3/envs/gatk/lib/python3.6/site-packages/theano/compile/function_module.py"", line 884, in __call__; self.fn() if output_subset is None else\; File ""/home/shlee/anaconda3/envs/gatk/lib/python3.6/site-packages/theano/scan_module/scan_op.py"", line 989, in rval; r = p(n, [x[0] for x in i], o); File ""/home/shlee/anaconda3/envs/gatk/lib/python3.6/site-packages/theano/scan_module/scan_op.py"", line 978, in p; self, node); File ""theano/scan_module/scan_perform.pyx"", line 215, in theano.scan_module.scan_perform.perform (/home/shlee/.theano/compiledir_Linux-4.13--gcp-x86_64-with-debian-stretch-sid-x86_64-3.6.2-64/scan_perform/mod.cpp:2628); NotImplementedError: We didn't implemented yet the case where scan do 0 iteration; Apply node that caused the error: forall_inplace,cpu,scan_fn}(Elemwise{minimum,no_inplace}.0, InplaceDimShuffle{0,2,1}.0, Subtensor{int64:int64:int64}.0, IncSubtensor{InplaceSet;:int64:}.0, Shape_i{0}.0); Toposort index: 97; Inputs types: [TensorType(int64, scalar), TensorType(float64, 3D), TensorType(float64, matrix), TensorType(float64, matrix), TensorType(int64, scalar)]; Inputs shapes: [(), (0, 6, 6), (0, 6), (2, 6), ()]; Inputs strides: [(), (288, 8, 48), (48, 8), (48, 8), ()]; Inputs values: [array(0), array([], shape=(0, 6, 6), dtype=float64), array([], shape=(0, 6), dtype=float64), 'not shown', array(6)]; Outputs clients: [[Subtensor{int64:int64:int8}(forall_inplace,cpu,scan_fn}.0, ScalarFromTensor.0, ScalarFromTensor.0, Constant{1})]]. HINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be d; ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4840
https://github.com/broadinstitute/gatk/issues/4840:14257,Availability,error,error,14257,"node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be d; isabled with 'optimizer=None'.; HINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node. at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75); at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCuratedArgs(ScriptExecutor.java:126); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeArgs(PythonScriptExecutor.java:170); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeScript(PythonScriptExecutor.java:151); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeScript(PythonScriptExecutor.java:121); at org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls.executeSegmentGermlineCNVCallsPythonScript(PostprocessGermlineCNVCalls.java:500); at org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls.generateSegmentsVCFFileFromAllShards(PostprocessGermlineCNVCalls.java:436); at org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls.traverse(PostprocessGermlineCNVCalls.java:297); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:892); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. Let me know what data files we need to look into to figure out the cause of error so I can make them available.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4840
https://github.com/broadinstitute/gatk/issues/4840:14282,Availability,avail,available,14282,"node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be d; isabled with 'optimizer=None'.; HINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node. at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75); at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCuratedArgs(ScriptExecutor.java:126); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeArgs(PythonScriptExecutor.java:170); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeScript(PythonScriptExecutor.java:151); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeScript(PythonScriptExecutor.java:121); at org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls.executeSegmentGermlineCNVCallsPythonScript(PostprocessGermlineCNVCalls.java:500); at org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls.generateSegmentsVCFFileFromAllShards(PostprocessGermlineCNVCalls.java:436); at org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls.traverse(PostprocessGermlineCNVCalls.java:297); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:892); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. Let me know what data files we need to look into to figure out the cause of error so I can make them available.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4840
https://github.com/broadinstitute/gatk/issues/4840:284,Performance,Load,Loading,284,Random and unplaced contigs are a part of the primary assembly. The tool also errors on decoy contigs but no other GRCh38 contig type. This ticket is to investigate why this error is happening for this subset of the data. Error is; ```; Stdout: 22:01:54.365 INFO segment_gcnv_calls - Loading ploidy calls...; 22:01:54.366 INFO gcnvkernel.io.io_metadata - Loading germline contig ploidy and global read depth metadata...; 22:01:54.381 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chrM). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.381 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270706v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.381 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270707v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.381 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270708v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.381 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270709v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270710v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270711v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270712v1_random). Germline contig ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4840
https://github.com/broadinstitute/gatk/issues/4840:355,Performance,Load,Loading,355,Random and unplaced contigs are a part of the primary assembly. The tool also errors on decoy contigs but no other GRCh38 contig type. This ticket is to investigate why this error is happening for this subset of the data. Error is; ```; Stdout: 22:01:54.365 INFO segment_gcnv_calls - Loading ploidy calls...; 22:01:54.366 INFO gcnvkernel.io.io_metadata - Loading germline contig ploidy and global read depth metadata...; 22:01:54.381 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chrM). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.381 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270706v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.381 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270707v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.381 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270708v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.381 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270709v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270710v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270711v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270712v1_random). Germline contig ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4840
https://github.com/broadinstitute/gatk/issues/4840:9055,Performance,perform,perform,9055,"rmline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.384 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr22_KI270735v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.384 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr22_KI270736v1_ra; Stderr: Traceback (most recent call last):; File ""/home/shlee/anaconda3/envs/gatk/lib/python3.6/site-packages/theano/compile/function_module.py"", line 884, in __call__; self.fn() if output_subset is None else\; File ""/home/shlee/anaconda3/envs/gatk/lib/python3.6/site-packages/theano/scan_module/scan_op.py"", line 989, in rval; r = p(n, [x[0] for x in i], o); File ""/home/shlee/anaconda3/envs/gatk/lib/python3.6/site-packages/theano/scan_module/scan_op.py"", line 978, in p; self, node); File ""theano/scan_module/scan_perform.pyx"", line 215, in theano.scan_module.scan_perform.perform (/home/shlee/.theano/compiledir_Linux-4.13--gcp-x86_64-with-debian-stretch-sid-x86_64-3.6.2-64/scan_perform/mod.cpp:2628); NotImplementedError: We didn't implemented yet the case where scan do 0 iteration. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/shlee/segment_gcnv_calls.5861176430020419759.py"", line 73, in <module>; viterbi_engine.write_copy_number_segments_for_single_sample(args.sample_index); File ""/home/shlee/anaconda3/envs/gatk/lib/python3.6/site-packages/gcnvkernel/postprocess/viterbi_segmentation.py"", line 265, in write_copy_number_segments_for_single_sample; for segment in self._viterbi_segments_generator_for_single_sample(sample_index):; File ""/home/shlee/anaconda3/envs/gatk/lib/python3.6/site-packages/gcnvkernel/postprocess/viterbi_segmentation.py"", line 188, in _viterbi_segments_generator_for_single_sample; log_prior_c, log_trans_contig_tcc, copy_number_log_emission_contig_tc); File ""/home/shlee/anaconda",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4840
https://github.com/broadinstitute/gatk/issues/4840:11226,Performance,perform,perform,11226,"""/home/shlee/anaconda3/envs/gatk/lib/python3.6/site-packages/theano/compile/function_module.py"", line 898, in __call__; storage_map=getattr(self.fn, 'storage_map', None)); File ""/home/shlee/anaconda3/envs/gatk/lib/python3.6/site-packages/theano/gof/link.py"", line 325, in raise_with_op; reraise(exc_type, exc_value, exc_trace); File ""/home/shlee/anaconda3/envs/gatk/lib/python3.6/site-packages/six.py"", line 692, in reraise; raise value.with_traceback(tb); File ""/home/shlee/anaconda3/envs/gatk/lib/python3.6/site-packages/theano/compile/function_module.py"", line 884, in __call__; self.fn() if output_subset is None else\; File ""/home/shlee/anaconda3/envs/gatk/lib/python3.6/site-packages/theano/scan_module/scan_op.py"", line 989, in rval; r = p(n, [x[0] for x in i], o); File ""/home/shlee/anaconda3/envs/gatk/lib/python3.6/site-packages/theano/scan_module/scan_op.py"", line 978, in p; self, node); File ""theano/scan_module/scan_perform.pyx"", line 215, in theano.scan_module.scan_perform.perform (/home/shlee/.theano/compiledir_Linux-4.13--gcp-x86_64-with-debian-stretch-sid-x86_64-3.6.2-64/scan_perform/mod.cpp:2628); NotImplementedError: We didn't implemented yet the case where scan do 0 iteration; Apply node that caused the error: forall_inplace,cpu,scan_fn}(Elemwise{minimum,no_inplace}.0, InplaceDimShuffle{0,2,1}.0, Subtensor{int64:int64:int64}.0, IncSubtensor{InplaceSet;:int64:}.0, Shape_i{0}.0); Toposort index: 97; Inputs types: [TensorType(int64, scalar), TensorType(float64, 3D), TensorType(float64, matrix), TensorType(float64, matrix), TensorType(int64, scalar)]; Inputs shapes: [(), (0, 6, 6), (0, 6), (2, 6), ()]; Inputs strides: [(), (288, 8, 48), (48, 8), (48, 8), ()]; Inputs values: [array(0), array([], shape=(0, 6, 6), dtype=float64), array([], shape=(0, 6), dtype=float64), 'not shown', array(6)]; Outputs clients: [[Subtensor{int64:int64:int8}(forall_inplace,cpu,scan_fn}.0, ScalarFromTensor.0, ScalarFromTensor.0, Constant{1})]]. HINT: Re-running with most Theano optimiza",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4840
https://github.com/broadinstitute/gatk/issues/4840:12229,Performance,optimiz,optimization,12229,"ano/compiledir_Linux-4.13--gcp-x86_64-with-debian-stretch-sid-x86_64-3.6.2-64/scan_perform/mod.cpp:2628); NotImplementedError: We didn't implemented yet the case where scan do 0 iteration; Apply node that caused the error: forall_inplace,cpu,scan_fn}(Elemwise{minimum,no_inplace}.0, InplaceDimShuffle{0,2,1}.0, Subtensor{int64:int64:int64}.0, IncSubtensor{InplaceSet;:int64:}.0, Shape_i{0}.0); Toposort index: 97; Inputs types: [TensorType(int64, scalar), TensorType(float64, 3D), TensorType(float64, matrix), TensorType(float64, matrix), TensorType(int64, scalar)]; Inputs shapes: [(), (0, 6, 6), (0, 6), (2, 6), ()]; Inputs strides: [(), (288, 8, 48), (48, 8), (48, 8), ()]; Inputs values: [array(0), array([], shape=(0, 6, 6), dtype=float64), array([], shape=(0, 6), dtype=float64), 'not shown', array(6)]; Outputs clients: [[Subtensor{int64:int64:int8}(forall_inplace,cpu,scan_fn}.0, ScalarFromTensor.0, ScalarFromTensor.0, Constant{1})]]. HINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be d; isabled with 'optimizer=None'.; HINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node. at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75); at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCuratedArgs(ScriptExecutor.java:126); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeArgs(PythonScriptExecutor.java:170); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeScript(PythonScriptExecutor.java:151); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeScript(PythonScriptExecutor.java:121); at org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls.executeSegmentGermlineCNVCalls",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4840
https://github.com/broadinstitute/gatk/issues/4840:12360,Performance,optimiz,optimizer,12360,"cpp:2628); NotImplementedError: We didn't implemented yet the case where scan do 0 iteration; Apply node that caused the error: forall_inplace,cpu,scan_fn}(Elemwise{minimum,no_inplace}.0, InplaceDimShuffle{0,2,1}.0, Subtensor{int64:int64:int64}.0, IncSubtensor{InplaceSet;:int64:}.0, Shape_i{0}.0); Toposort index: 97; Inputs types: [TensorType(int64, scalar), TensorType(float64, 3D), TensorType(float64, matrix), TensorType(float64, matrix), TensorType(int64, scalar)]; Inputs shapes: [(), (0, 6, 6), (0, 6), (2, 6), ()]; Inputs strides: [(), (288, 8, 48), (48, 8), (48, 8), ()]; Inputs values: [array(0), array([], shape=(0, 6, 6), dtype=float64), array([], shape=(0, 6), dtype=float64), 'not shown', array(6)]; Outputs clients: [[Subtensor{int64:int64:int8}(forall_inplace,cpu,scan_fn}.0, ScalarFromTensor.0, ScalarFromTensor.0, Constant{1})]]. HINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be d; isabled with 'optimizer=None'.; HINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node. at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75); at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCuratedArgs(ScriptExecutor.java:126); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeArgs(PythonScriptExecutor.java:170); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeScript(PythonScriptExecutor.java:151); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeScript(PythonScriptExecutor.java:121); at org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls.executeSegmentGermlineCNVCallsPythonScript(PostprocessGermlineCNVCalls.java:500); at org.broadinstitute.hellbender.tools.copy",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4840
https://github.com/broadinstitute/gatk/issues/4840:12415,Performance,optimiz,optimizations,12415," 0 iteration; Apply node that caused the error: forall_inplace,cpu,scan_fn}(Elemwise{minimum,no_inplace}.0, InplaceDimShuffle{0,2,1}.0, Subtensor{int64:int64:int64}.0, IncSubtensor{InplaceSet;:int64:}.0, Shape_i{0}.0); Toposort index: 97; Inputs types: [TensorType(int64, scalar), TensorType(float64, 3D), TensorType(float64, matrix), TensorType(float64, matrix), TensorType(int64, scalar)]; Inputs shapes: [(), (0, 6, 6), (0, 6), (2, 6), ()]; Inputs strides: [(), (288, 8, 48), (48, 8), (48, 8), ()]; Inputs values: [array(0), array([], shape=(0, 6, 6), dtype=float64), array([], shape=(0, 6), dtype=float64), 'not shown', array(6)]; Outputs clients: [[Subtensor{int64:int64:int8}(forall_inplace,cpu,scan_fn}.0, ScalarFromTensor.0, ScalarFromTensor.0, Constant{1})]]. HINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be d; isabled with 'optimizer=None'.; HINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node. at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75); at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCuratedArgs(ScriptExecutor.java:126); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeArgs(PythonScriptExecutor.java:170); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeScript(PythonScriptExecutor.java:151); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeScript(PythonScriptExecutor.java:121); at org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls.executeSegmentGermlineCNVCallsPythonScript(PostprocessGermlineCNVCalls.java:500); at org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls.generateSegmentsVCFFileFromAllShards(Postproc",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4840
https://github.com/broadinstitute/gatk/issues/4840:12453,Performance,optimiz,optimizer,12453," 0 iteration; Apply node that caused the error: forall_inplace,cpu,scan_fn}(Elemwise{minimum,no_inplace}.0, InplaceDimShuffle{0,2,1}.0, Subtensor{int64:int64:int64}.0, IncSubtensor{InplaceSet;:int64:}.0, Shape_i{0}.0); Toposort index: 97; Inputs types: [TensorType(int64, scalar), TensorType(float64, 3D), TensorType(float64, matrix), TensorType(float64, matrix), TensorType(int64, scalar)]; Inputs shapes: [(), (0, 6, 6), (0, 6), (2, 6), ()]; Inputs strides: [(), (288, 8, 48), (48, 8), (48, 8), ()]; Inputs values: [array(0), array([], shape=(0, 6, 6), dtype=float64), array([], shape=(0, 6), dtype=float64), 'not shown', array(6)]; Outputs clients: [[Subtensor{int64:int64:int8}(forall_inplace,cpu,scan_fn}.0, ScalarFromTensor.0, ScalarFromTensor.0, Constant{1})]]. HINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be d; isabled with 'optimizer=None'.; HINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node. at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75); at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCuratedArgs(ScriptExecutor.java:126); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeArgs(PythonScriptExecutor.java:170); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeScript(PythonScriptExecutor.java:151); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeScript(PythonScriptExecutor.java:121); at org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls.executeSegmentGermlineCNVCallsPythonScript(PostprocessGermlineCNVCalls.java:500); at org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls.generateSegmentsVCFFileFromAllShards(Postproc",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4840
https://github.com/broadinstitute/gatk/pull/4843:35,Testability,test,tests,35,@droazen Here are a couple of unit tests for some heavily-uses and previously untested methods.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4843
https://github.com/broadinstitute/gatk/pull/4845:105,Energy Efficiency,reduce,reduced,105,"@takutosato Here's another little one. This didn't affect sensitivity in Hapmap or DREAM and in DREAM it reduced indel false positives by about half. Not a bad short-term improvement for MC3, although deep learning will handle it better.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4845
https://github.com/broadinstitute/gatk/pull/4845:206,Usability,learn,learning,206,"@takutosato Here's another little one. This didn't affect sensitivity in Hapmap or DREAM and in DREAM it reduced indel false positives by about half. Not a bad short-term improvement for MC3, although deep learning will handle it better.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4845
https://github.com/broadinstitute/gatk/pull/4848:47,Deployability,release,release,47,@LeeTL1220 This is worth getting in before the release.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4848
https://github.com/broadinstitute/gatk/issues/4849:48,Deployability,pipeline,pipelines,48,"Different traversal options might optimize some pipelines and give flexibility to the API user to have more efficient tools. Some of the different traversal per-pass that I am interested are:; * Allow different filters applied in each pass. Example usage: collect some metric with all reads in the first pass, and use it for the second pass only on the filtered reads.; * Allow different transformers in each pass. Example usage: in the first pass, untransformed reads are used to collect a metric; based on that metric, the second pass transform the reads to be output.; * Allow different intervals applied in each pass. Example usage: iterate over the user intervals to get some information about the reads, and then in the second pass use that information to iterate over a different set of intervals. I came out with this feature in the `TwoPassReadWalker` to implement a tool for pair-end data. The use case is to iterate in the first pass to the user intervals/filters, collecting where the mates are located (and/or read names), and in the second pass iterate over the reads and its mates independently of where they are located. If giving this flexibility to the `TwoPassReadWalker` is not desired, another option is to implement another walker-type `ReadAndMatesWalker` which implement the first pass in a private method and the second as a `ReadWalker.apply` for the reads and its mates. Which one is the preferred option for the GATK engine team, @droazen?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4849
https://github.com/broadinstitute/gatk/issues/4849:108,Energy Efficiency,efficient,efficient,108,"Different traversal options might optimize some pipelines and give flexibility to the API user to have more efficient tools. Some of the different traversal per-pass that I am interested are:; * Allow different filters applied in each pass. Example usage: collect some metric with all reads in the first pass, and use it for the second pass only on the filtered reads.; * Allow different transformers in each pass. Example usage: in the first pass, untransformed reads are used to collect a metric; based on that metric, the second pass transform the reads to be output.; * Allow different intervals applied in each pass. Example usage: iterate over the user intervals to get some information about the reads, and then in the second pass use that information to iterate over a different set of intervals. I came out with this feature in the `TwoPassReadWalker` to implement a tool for pair-end data. The use case is to iterate in the first pass to the user intervals/filters, collecting where the mates are located (and/or read names), and in the second pass iterate over the reads and its mates independently of where they are located. If giving this flexibility to the `TwoPassReadWalker` is not desired, another option is to implement another walker-type `ReadAndMatesWalker` which implement the first pass in a private method and the second as a `ReadWalker.apply` for the reads and its mates. Which one is the preferred option for the GATK engine team, @droazen?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4849
https://github.com/broadinstitute/gatk/issues/4849:34,Performance,optimiz,optimize,34,"Different traversal options might optimize some pipelines and give flexibility to the API user to have more efficient tools. Some of the different traversal per-pass that I am interested are:; * Allow different filters applied in each pass. Example usage: collect some metric with all reads in the first pass, and use it for the second pass only on the filtered reads.; * Allow different transformers in each pass. Example usage: in the first pass, untransformed reads are used to collect a metric; based on that metric, the second pass transform the reads to be output.; * Allow different intervals applied in each pass. Example usage: iterate over the user intervals to get some information about the reads, and then in the second pass use that information to iterate over a different set of intervals. I came out with this feature in the `TwoPassReadWalker` to implement a tool for pair-end data. The use case is to iterate in the first pass to the user intervals/filters, collecting where the mates are located (and/or read names), and in the second pass iterate over the reads and its mates independently of where they are located. If giving this flexibility to the `TwoPassReadWalker` is not desired, another option is to implement another walker-type `ReadAndMatesWalker` which implement the first pass in a private method and the second as a `ReadWalker.apply` for the reads and its mates. Which one is the preferred option for the GATK engine team, @droazen?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4849
https://github.com/broadinstitute/gatk/issues/4852:395,Availability,error,errors,395,"## Bug Report. ### Affected tool(s) or class(es). All Spark tools that takes parameter `-L`. ### Affected version(s); - [x] Latest public release version [4.0.4.0]; - [x] Latest master branch as of [2018-06-30]. ### Description . When running a Spark tool and passing in interval arguments via the standard `-L` argument, if the interval file (only BED file is tested) is stored in HDFS, we see errors like below. ```; org.broadinstitute.hellbender.exceptions.UserException$MalformedGenomeLoc: Badly formed genome unclippedLoc: Query interval ""hdfs://shuang-g94794-chmi-chmi3-wgs1-cram-bam-feature-m:8020/data/merged_commonFPDel.bed"" is not valid for this input.; 	at org.broadinstitute.hellbender.utils.GenomeLocParser.getUnambiguousInterval(GenomeLocParser.java:350); 	at org.broadinstitute.hellbender.utils.GenomeLocParser.parseGenomeLoc(GenomeLocParser.java:309); 	at org.broadinstitute.hellbender.utils.IntervalUtils.parseIntervalArguments(IntervalUtils.java:300); 	at org.broadinstitute.hellbender.utils.IntervalUtils.loadIntervals(IntervalUtils.java:226); 	at org.broadinstitute.hellbender.cmdline.argumentcollections.IntervalArgumentCollection.parseIntervals(IntervalArgumentCollection.java:174); 	at org.broadinstitute.hellbender.cmdline.argumentcollections.IntervalArgumentCollection.getTraversalParameters(IntervalArgumentCollection.java:155); 	at org.broadinstitute.hellbender.cmdline.argumentcollections.IntervalArgumentCollection.getIntervals(IntervalArgumentCollection.java:111); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeIntervals(GATKSparkTool.java:514); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:451); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:439); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLinePro",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4852
https://github.com/broadinstitute/gatk/issues/4852:3127,Availability,ERROR,ERROR,3127,roadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:451); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:439); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:135); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:180); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:199); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:775); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ERROR: (gcloud.dataproc.jobs.submit.spark) Job [91a5d7391a4647a89e50717b96eb50e0] entered state [ERROR] while waiting for [DONE]. ```. #### Steps to reproduce; Run a tool in the following way. ```; gatk ToolNameSpark \; -I hdfs://path/to/bam/test.bam \; -L hdfs://path/to/interval/file/interval.bed \; -O hdfs://path/to/output \; ....; ```. #### Expected behavior; Intervals to be parsed correctly. #### Actual behavior; Engine tries to interpret the file name as an actual interval.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4852
https://github.com/broadinstitute/gatk/issues/4852:3224,Availability,ERROR,ERROR,3224,roadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:451); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:439); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:135); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:180); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:199); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:775); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ERROR: (gcloud.dataproc.jobs.submit.spark) Job [91a5d7391a4647a89e50717b96eb50e0] entered state [ERROR] while waiting for [DONE]. ```. #### Steps to reproduce; Run a tool in the following way. ```; gatk ToolNameSpark \; -I hdfs://path/to/bam/test.bam \; -L hdfs://path/to/interval/file/interval.bed \; -O hdfs://path/to/output \; ....; ```. #### Expected behavior; Intervals to be parsed correctly. #### Actual behavior; Engine tries to interpret the file name as an actual interval.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4852
https://github.com/broadinstitute/gatk/issues/4852:138,Deployability,release,release,138,"## Bug Report. ### Affected tool(s) or class(es). All Spark tools that takes parameter `-L`. ### Affected version(s); - [x] Latest public release version [4.0.4.0]; - [x] Latest master branch as of [2018-06-30]. ### Description . When running a Spark tool and passing in interval arguments via the standard `-L` argument, if the interval file (only BED file is tested) is stored in HDFS, we see errors like below. ```; org.broadinstitute.hellbender.exceptions.UserException$MalformedGenomeLoc: Badly formed genome unclippedLoc: Query interval ""hdfs://shuang-g94794-chmi-chmi3-wgs1-cram-bam-feature-m:8020/data/merged_commonFPDel.bed"" is not valid for this input.; 	at org.broadinstitute.hellbender.utils.GenomeLocParser.getUnambiguousInterval(GenomeLocParser.java:350); 	at org.broadinstitute.hellbender.utils.GenomeLocParser.parseGenomeLoc(GenomeLocParser.java:309); 	at org.broadinstitute.hellbender.utils.IntervalUtils.parseIntervalArguments(IntervalUtils.java:300); 	at org.broadinstitute.hellbender.utils.IntervalUtils.loadIntervals(IntervalUtils.java:226); 	at org.broadinstitute.hellbender.cmdline.argumentcollections.IntervalArgumentCollection.parseIntervals(IntervalArgumentCollection.java:174); 	at org.broadinstitute.hellbender.cmdline.argumentcollections.IntervalArgumentCollection.getTraversalParameters(IntervalArgumentCollection.java:155); 	at org.broadinstitute.hellbender.cmdline.argumentcollections.IntervalArgumentCollection.getIntervals(IntervalArgumentCollection.java:111); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeIntervals(GATKSparkTool.java:514); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:451); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:439); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLinePro",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4852
https://github.com/broadinstitute/gatk/issues/4852:2754,Deployability,deploy,deploy,2754,roadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:451); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:439); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:135); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:180); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:199); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:775); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ERROR: (gcloud.dataproc.jobs.submit.spark) Job [91a5d7391a4647a89e50717b96eb50e0] entered state [ERROR] while waiting for [DONE]. ```. #### Steps to reproduce; Run a tool in the following way. ```; gatk ToolNameSpark \; -I hdfs://path/to/bam/test.bam \; -L hdfs://path/to/interval/file/interval.bed \; -O hdfs://path/to/output \; ....; ```. #### Expected behavior; Intervals to be parsed correctly. #### Actual behavior; Engine tries to interpret the file name as an actual interval.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4852
https://github.com/broadinstitute/gatk/issues/4852:2791,Deployability,deploy,deploy,2791,roadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:451); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:439); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:135); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:180); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:199); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:775); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ERROR: (gcloud.dataproc.jobs.submit.spark) Job [91a5d7391a4647a89e50717b96eb50e0] entered state [ERROR] while waiting for [DONE]. ```. #### Steps to reproduce; Run a tool in the following way. ```; gatk ToolNameSpark \; -I hdfs://path/to/bam/test.bam \; -L hdfs://path/to/interval/file/interval.bed \; -O hdfs://path/to/output \; ....; ```. #### Expected behavior; Intervals to be parsed correctly. #### Actual behavior; Engine tries to interpret the file name as an actual interval.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4852
https://github.com/broadinstitute/gatk/issues/4852:2864,Deployability,deploy,deploy,2864,roadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:451); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:439); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:135); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:180); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:199); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:775); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ERROR: (gcloud.dataproc.jobs.submit.spark) Job [91a5d7391a4647a89e50717b96eb50e0] entered state [ERROR] while waiting for [DONE]. ```. #### Steps to reproduce; Run a tool in the following way. ```; gatk ToolNameSpark \; -I hdfs://path/to/bam/test.bam \; -L hdfs://path/to/interval/file/interval.bed \; -O hdfs://path/to/output \; ....; ```. #### Expected behavior; Intervals to be parsed correctly. #### Actual behavior; Engine tries to interpret the file name as an actual interval.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4852
https://github.com/broadinstitute/gatk/issues/4852:2941,Deployability,deploy,deploy,2941,roadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:451); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:439); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:135); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:180); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:199); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:775); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ERROR: (gcloud.dataproc.jobs.submit.spark) Job [91a5d7391a4647a89e50717b96eb50e0] entered state [ERROR] while waiting for [DONE]. ```. #### Steps to reproduce; Run a tool in the following way. ```; gatk ToolNameSpark \; -I hdfs://path/to/bam/test.bam \; -L hdfs://path/to/interval/file/interval.bed \; -O hdfs://path/to/output \; ....; ```. #### Expected behavior; Intervals to be parsed correctly. #### Actual behavior; Engine tries to interpret the file name as an actual interval.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4852
https://github.com/broadinstitute/gatk/issues/4852:3013,Deployability,deploy,deploy,3013,roadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:451); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:439); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:135); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:180); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:199); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:775); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ERROR: (gcloud.dataproc.jobs.submit.spark) Job [91a5d7391a4647a89e50717b96eb50e0] entered state [ERROR] while waiting for [DONE]. ```. #### Steps to reproduce; Run a tool in the following way. ```; gatk ToolNameSpark \; -I hdfs://path/to/bam/test.bam \; -L hdfs://path/to/interval/file/interval.bed \; -O hdfs://path/to/output \; ....; ```. #### Expected behavior; Intervals to be parsed correctly. #### Actual behavior; Engine tries to interpret the file name as an actual interval.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4852
https://github.com/broadinstitute/gatk/issues/4852:3083,Deployability,deploy,deploy,3083,roadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:451); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:439); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:135); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:180); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:199); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:775); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ERROR: (gcloud.dataproc.jobs.submit.spark) Job [91a5d7391a4647a89e50717b96eb50e0] entered state [ERROR] while waiting for [DONE]. ```. #### Steps to reproduce; Run a tool in the following way. ```; gatk ToolNameSpark \; -I hdfs://path/to/bam/test.bam \; -L hdfs://path/to/interval/file/interval.bed \; -O hdfs://path/to/output \; ....; ```. #### Expected behavior; Intervals to be parsed correctly. #### Actual behavior; Engine tries to interpret the file name as an actual interval.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4852
https://github.com/broadinstitute/gatk/issues/4852:1024,Performance,load,loadIntervals,1024," class(es). All Spark tools that takes parameter `-L`. ### Affected version(s); - [x] Latest public release version [4.0.4.0]; - [x] Latest master branch as of [2018-06-30]. ### Description . When running a Spark tool and passing in interval arguments via the standard `-L` argument, if the interval file (only BED file is tested) is stored in HDFS, we see errors like below. ```; org.broadinstitute.hellbender.exceptions.UserException$MalformedGenomeLoc: Badly formed genome unclippedLoc: Query interval ""hdfs://shuang-g94794-chmi-chmi3-wgs1-cram-bam-feature-m:8020/data/merged_commonFPDel.bed"" is not valid for this input.; 	at org.broadinstitute.hellbender.utils.GenomeLocParser.getUnambiguousInterval(GenomeLocParser.java:350); 	at org.broadinstitute.hellbender.utils.GenomeLocParser.parseGenomeLoc(GenomeLocParser.java:309); 	at org.broadinstitute.hellbender.utils.IntervalUtils.parseIntervalArguments(IntervalUtils.java:300); 	at org.broadinstitute.hellbender.utils.IntervalUtils.loadIntervals(IntervalUtils.java:226); 	at org.broadinstitute.hellbender.cmdline.argumentcollections.IntervalArgumentCollection.parseIntervals(IntervalArgumentCollection.java:174); 	at org.broadinstitute.hellbender.cmdline.argumentcollections.IntervalArgumentCollection.getTraversalParameters(IntervalArgumentCollection.java:155); 	at org.broadinstitute.hellbender.cmdline.argumentcollections.IntervalArgumentCollection.getIntervals(IntervalArgumentCollection.java:111); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeIntervals(GATKSparkTool.java:514); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:451); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:439); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:135); 	at org.broadinstitut",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4852
https://github.com/broadinstitute/gatk/issues/4852:361,Testability,test,tested,361,"## Bug Report. ### Affected tool(s) or class(es). All Spark tools that takes parameter `-L`. ### Affected version(s); - [x] Latest public release version [4.0.4.0]; - [x] Latest master branch as of [2018-06-30]. ### Description . When running a Spark tool and passing in interval arguments via the standard `-L` argument, if the interval file (only BED file is tested) is stored in HDFS, we see errors like below. ```; org.broadinstitute.hellbender.exceptions.UserException$MalformedGenomeLoc: Badly formed genome unclippedLoc: Query interval ""hdfs://shuang-g94794-chmi-chmi3-wgs1-cram-bam-feature-m:8020/data/merged_commonFPDel.bed"" is not valid for this input.; 	at org.broadinstitute.hellbender.utils.GenomeLocParser.getUnambiguousInterval(GenomeLocParser.java:350); 	at org.broadinstitute.hellbender.utils.GenomeLocParser.parseGenomeLoc(GenomeLocParser.java:309); 	at org.broadinstitute.hellbender.utils.IntervalUtils.parseIntervalArguments(IntervalUtils.java:300); 	at org.broadinstitute.hellbender.utils.IntervalUtils.loadIntervals(IntervalUtils.java:226); 	at org.broadinstitute.hellbender.cmdline.argumentcollections.IntervalArgumentCollection.parseIntervals(IntervalArgumentCollection.java:174); 	at org.broadinstitute.hellbender.cmdline.argumentcollections.IntervalArgumentCollection.getTraversalParameters(IntervalArgumentCollection.java:155); 	at org.broadinstitute.hellbender.cmdline.argumentcollections.IntervalArgumentCollection.getIntervals(IntervalArgumentCollection.java:111); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeIntervals(GATKSparkTool.java:514); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:451); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:439); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLinePro",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4852
https://github.com/broadinstitute/gatk/issues/4852:3369,Testability,test,test,3369,roadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:451); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:439); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:135); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:180); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:199); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:775); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ERROR: (gcloud.dataproc.jobs.submit.spark) Job [91a5d7391a4647a89e50717b96eb50e0] entered state [ERROR] while waiting for [DONE]. ```. #### Steps to reproduce; Run a tool in the following way. ```; gatk ToolNameSpark \; -I hdfs://path/to/bam/test.bam \; -L hdfs://path/to/interval/file/interval.bed \; -O hdfs://path/to/output \; ....; ```. #### Expected behavior; Intervals to be parsed correctly. #### Actual behavior; Engine tries to interpret the file name as an actual interval.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4852
https://github.com/broadinstitute/gatk/pull/4853:0,Integrability,Depend,Depends,0,Depends on #4545 . Fixes #4818,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4853
https://github.com/broadinstitute/gatk/issues/4857:590,Security,validat,validated,590,"### Affected tool(s) or class(es); Funcotator. ### Affected version(s); - [ ] Latest master branch as of June 7, 2018. ### Description ; Currently, the metadata for `GencodeFuncotation` will always be default, unknown for all fields. However, we know what each field is, so we could populate . This cannot be seen anywhere by a user, yet. However, future requirements will probably cause the metadata to be rendered into a VCF header or a MAF comment, so this should be populated. #### Expected behavior; The Gencode funcotations should not have unknown types and descriptions. This can be validated with an automated test. #### Actual behavior; The Gencode funcotations have unknown types and descriptions.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4857
https://github.com/broadinstitute/gatk/issues/4857:618,Testability,test,test,618,"### Affected tool(s) or class(es); Funcotator. ### Affected version(s); - [ ] Latest master branch as of June 7, 2018. ### Description ; Currently, the metadata for `GencodeFuncotation` will always be default, unknown for all fields. However, we know what each field is, so we could populate . This cannot be seen anywhere by a user, yet. However, future requirements will probably cause the metadata to be rendered into a VCF header or a MAF comment, so this should be populated. #### Expected behavior; The Gencode funcotations should not have unknown types and descriptions. This can be validated with an automated test. #### Actual behavior; The Gencode funcotations have unknown types and descriptions.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4857
https://github.com/broadinstitute/gatk/pull/4858:287,Availability,down,downsampling,287,"Closes #4829. Also cleans up many other implausible deletions in the bamout, a few tens of thousands per genome. @ldgauthier I would especially like your thoughts on the ungainly use of `useReferenceIfUninformative`. I wanted to be conservative about switching things like contamination downsampling to the new version, but it's inelegant.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4858
https://github.com/broadinstitute/gatk/issues/4859:207,Safety,avoid,avoid,207,"Currently ReadsSparkSource automatically sorts reads before writing them out according to the sort order of the header (or without sorting if the header sort order is unrecognized). There should be a way to avoid this for tools where the reads RDD ordering can be guarinteed at write time, or in which we don't want to change the sort ordering anyway like PrintReadsSpark.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4859
https://github.com/broadinstitute/gatk/issues/4860:241,Performance,perform,perform,241,"Currently the best way to select a subset of reads overlapping an interval is to provide a list of -L reader interval arguments. Unfortunately, this requires that the bam be sorted and indexed, which can often be a pain point when trying to perform quick tests, as a sort is often slow and requires copying the bam a second time. To alleviate this, it would be nice to add a ReadFilter level interval argument, for example an `IntervalOverlapReadFilter` or some such which can take similarly formatted arguments to -L. This would allow one to use `PrintReads` to select an interval over unsorted/unindexed bams more easily.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4860
https://github.com/broadinstitute/gatk/issues/4860:255,Testability,test,tests,255,"Currently the best way to select a subset of reads overlapping an interval is to provide a list of -L reader interval arguments. Unfortunately, this requires that the bam be sorted and indexed, which can often be a pain point when trying to perform quick tests, as a sort is often slow and requires copying the bam a second time. To alleviate this, it would be nice to add a ReadFilter level interval argument, for example an `IntervalOverlapReadFilter` or some such which can take similarly formatted arguments to -L. This would allow one to use `PrintReads` to select an interval over unsorted/unindexed bams more easily.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4860
https://github.com/broadinstitute/gatk/issues/4861:7,Availability,error,error,7,In the error handling for `catch ( final FuncotatorUtils.TranscriptCodingSequenceException ex )` in `GencodeFuncotationFactory` we should add an `otherTranscript` to the output annotations with the transcript ID and an ERROR statement if an error occurred during processing. This will make the user more likely to see it if there was a problem creating funcotations for a particular transcript.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4861
https://github.com/broadinstitute/gatk/issues/4861:219,Availability,ERROR,ERROR,219,In the error handling for `catch ( final FuncotatorUtils.TranscriptCodingSequenceException ex )` in `GencodeFuncotationFactory` we should add an `otherTranscript` to the output annotations with the transcript ID and an ERROR statement if an error occurred during processing. This will make the user more likely to see it if there was a problem creating funcotations for a particular transcript.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4861
https://github.com/broadinstitute/gatk/issues/4861:241,Availability,error,error,241,In the error handling for `catch ( final FuncotatorUtils.TranscriptCodingSequenceException ex )` in `GencodeFuncotationFactory` we should add an `otherTranscript` to the output annotations with the transcript ID and an ERROR statement if an error occurred during processing. This will make the user more likely to see it if there was a problem creating funcotations for a particular transcript.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4861
https://github.com/broadinstitute/gatk/issues/4866:80,Testability,test,tests,80,`DiploidGenotype` and `ProbabilityVector` are not used except in their own unit tests. Delete? @ldgauthier,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4866
https://github.com/broadinstitute/gatk/issues/4867:140,Availability,error,errors,140,"In high-depth calling (eg @meganshand's work with mitochondria) it is necessary to tweak the `min-pruning` argument. If it is too low, base errors render the assembly graph nearly dense, causing a loss of sensitivity when the assembly engine essentially chooses random haplotypes. If it is too high, we also lose sensitivity because true variants are pruned. Setting the command line argument differently for each sample is not only cumbersome. It also doesn't solve the problem because depth varies within the same bam. Thus, pruning must adapt to each assembly region.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4867
https://github.com/broadinstitute/gatk/issues/4867:540,Energy Efficiency,adapt,adapt,540,"In high-depth calling (eg @meganshand's work with mitochondria) it is necessary to tweak the `min-pruning` argument. If it is too low, base errors render the assembly graph nearly dense, causing a loss of sensitivity when the assembly engine essentially chooses random haplotypes. If it is too high, we also lose sensitivity because true variants are pruned. Setting the command line argument differently for each sample is not only cumbersome. It also doesn't solve the problem because depth varies within the same bam. Thus, pruning must adapt to each assembly region.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4867
https://github.com/broadinstitute/gatk/issues/4867:540,Modifiability,adapt,adapt,540,"In high-depth calling (eg @meganshand's work with mitochondria) it is necessary to tweak the `min-pruning` argument. If it is too low, base errors render the assembly graph nearly dense, causing a loss of sensitivity when the assembly engine essentially chooses random haplotypes. If it is too high, we also lose sensitivity because true variants are pruned. Setting the command line argument differently for each sample is not only cumbersome. It also doesn't solve the problem because depth varies within the same bam. Thus, pruning must adapt to each assembly region.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4867
https://github.com/broadinstitute/gatk/issues/4868:5,Availability,error,error,5,"Read error correction tends to follow the basic strategy of 1) collect kmer counts 2) replace rare kmers with their closest non-rare match. For germline calling where there is a huge gap between error rates and diploid het allele fractions this is sufficient. Mutect, however, must contend with cases where counts alone do not discriminate perfectly between errors and real mutations. Without committing to an approach, it seems like phasing might help. That is, we could construct haplotypes of rare kmers and error correct those. This should work because sequencing errors are unphased and real variants are. There are phased artifacts, of course, but we handle those in downstream filtering.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4868
https://github.com/broadinstitute/gatk/issues/4868:195,Availability,error,error,195,"Read error correction tends to follow the basic strategy of 1) collect kmer counts 2) replace rare kmers with their closest non-rare match. For germline calling where there is a huge gap between error rates and diploid het allele fractions this is sufficient. Mutect, however, must contend with cases where counts alone do not discriminate perfectly between errors and real mutations. Without committing to an approach, it seems like phasing might help. That is, we could construct haplotypes of rare kmers and error correct those. This should work because sequencing errors are unphased and real variants are. There are phased artifacts, of course, but we handle those in downstream filtering.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4868
https://github.com/broadinstitute/gatk/issues/4868:358,Availability,error,errors,358,"Read error correction tends to follow the basic strategy of 1) collect kmer counts 2) replace rare kmers with their closest non-rare match. For germline calling where there is a huge gap between error rates and diploid het allele fractions this is sufficient. Mutect, however, must contend with cases where counts alone do not discriminate perfectly between errors and real mutations. Without committing to an approach, it seems like phasing might help. That is, we could construct haplotypes of rare kmers and error correct those. This should work because sequencing errors are unphased and real variants are. There are phased artifacts, of course, but we handle those in downstream filtering.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4868
https://github.com/broadinstitute/gatk/issues/4868:511,Availability,error,error,511,"Read error correction tends to follow the basic strategy of 1) collect kmer counts 2) replace rare kmers with their closest non-rare match. For germline calling where there is a huge gap between error rates and diploid het allele fractions this is sufficient. Mutect, however, must contend with cases where counts alone do not discriminate perfectly between errors and real mutations. Without committing to an approach, it seems like phasing might help. That is, we could construct haplotypes of rare kmers and error correct those. This should work because sequencing errors are unphased and real variants are. There are phased artifacts, of course, but we handle those in downstream filtering.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4868
https://github.com/broadinstitute/gatk/issues/4868:568,Availability,error,errors,568,"Read error correction tends to follow the basic strategy of 1) collect kmer counts 2) replace rare kmers with their closest non-rare match. For germline calling where there is a huge gap between error rates and diploid het allele fractions this is sufficient. Mutect, however, must contend with cases where counts alone do not discriminate perfectly between errors and real mutations. Without committing to an approach, it seems like phasing might help. That is, we could construct haplotypes of rare kmers and error correct those. This should work because sequencing errors are unphased and real variants are. There are phased artifacts, of course, but we handle those in downstream filtering.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4868
https://github.com/broadinstitute/gatk/issues/4868:673,Availability,down,downstream,673,"Read error correction tends to follow the basic strategy of 1) collect kmer counts 2) replace rare kmers with their closest non-rare match. For germline calling where there is a huge gap between error rates and diploid het allele fractions this is sufficient. Mutect, however, must contend with cases where counts alone do not discriminate perfectly between errors and real mutations. Without committing to an approach, it seems like phasing might help. That is, we could construct haplotypes of rare kmers and error correct those. This should work because sequencing errors are unphased and real variants are. There are phased artifacts, of course, but we handle those in downstream filtering.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4868
https://github.com/broadinstitute/gatk/issues/4869:141,Availability,error,error,141,Currently if you submit a bam with no reads to a spark tool AND it's header indicates that it is queryname sorted it will throw an unhelpful error message in the repartitioning code. This should be fixed somehow.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4869
https://github.com/broadinstitute/gatk/issues/4869:147,Integrability,message,message,147,Currently if you submit a bam with no reads to a spark tool AND it's header indicates that it is queryname sorted it will throw an unhelpful error message in the repartitioning code. This should be fixed somehow.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4869
https://github.com/broadinstitute/gatk/pull/4870:0,Testability,Test,Tested,0,Tested with Cromwell 30.2,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4870
https://github.com/broadinstitute/gatk/pull/4872:605,Security,Sanitiz,Sanitizing,605,"- Created `FuncotationMetadata` class. An instance of this will appear as an attribute on all Funcotations. This describes the fields in the Funcotation. This is also useful for when you need to know all possible field names in formats where these may not be specified in every variant (e.g. VCF); - Due to previous bullet point, Funcotator will now throw an exception if an attribute in a variant is not in the header.; - For now, `FuncotationMetadata` only supports INFO fields in a VCF.; - `tumor_f` was being mapped to an incorrect field. This has been removed, but not fixed. See #4634 and #4871 ; - Sanitizing spaces in funcotation field values.; - Fixes #3895 ; - Fix where other transcript field was being populated incorrectly.; - (Dev) Creating a Funcotation for the input VCF INFO attributes.; - (Dev) Made the `VcfOutputRenderer` ignore VCF input funcotations.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4872
https://github.com/broadinstitute/gatk/pull/4873:56,Safety,avoid,avoid,56,"Simple fix to remove trailing slash in GCS_SAVE_PATH to avoid double slashes in GCS_RESULTS_DIR. Without this, if the `manage_sv_pipeline.sh` is launched with `-s gs://custom/path/to/save/` having the trailing slash, log file and cmd line info will be saved to a strange place.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4873
https://github.com/broadinstitute/gatk/pull/4873:217,Testability,log,log,217,"Simple fix to remove trailing slash in GCS_SAVE_PATH to avoid double slashes in GCS_RESULTS_DIR. Without this, if the `manage_sv_pipeline.sh` is launched with `-s gs://custom/path/to/save/` having the trailing slash, log file and cmd line info will be saved to a strange place.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4873
https://github.com/broadinstitute/gatk/pull/4873:0,Usability,Simpl,Simple,0,"Simple fix to remove trailing slash in GCS_SAVE_PATH to avoid double slashes in GCS_RESULTS_DIR. Without this, if the `manage_sv_pipeline.sh` is launched with `-s gs://custom/path/to/save/` having the trailing slash, log file and cmd line info will be saved to a strange place.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4873
https://github.com/broadinstitute/gatk/pull/4874:13,Integrability,Depend,Depends,13,Fixes #4859. Depends on #4545,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4874
https://github.com/broadinstitute/gatk/issues/4875:661,Availability,error,error,661,"```; /opt/jdk1.8.0_172/bin/java -jar /data-ddn/software/GATK4/4.0.3.0/gatk-package-4.0.3.0-local.jar Mutect2 -h | head. USAGE: Mutect2 [arguments]. Call somatic SNVs and indels via local assembly of haplotypes; Version:4.0.3.0. Required Arguments:. --input,-I:String BAM/SAM/CRAM file containing reads This argument must be specified at least once.; Required. --output,-O:File File to which variants should be written Required. --reference,-R:String Reference sequence file Required. --tumor-sample,-tumor:String BAM sample name of tumor. May be URL-encoded as output by GetSampleName with -encode; argument. Required. ...; ```. while version 4.0.5.0 raises an error :. ```; /opt/jdk1.8.0_172/bin/java -jar /data-ddn/software/GATK4/4.0.5.0/gatk-package-4.0.5.0-local.jar Mutect2 -h; java.lang.IllegalArgumentException: Allowed values request for unrecognized string argument: input; 	at org.broadinstitute.hellbender.cmdline.GATKPlugin.GATKAnnotationPluginDescriptor.getAllowedValuesForDescriptorHelp(GATKAnnotationPluginDescriptor.java:246); 	at org.broadinstitute.barclay.argparser.CommandLineArgumentParser.usageForPluginDescriptorArgumentIfApplicable(CommandLineArgumentParser.java:870); 	at org.broadinstitute.barclay.argparser.CommandLineArgumentParser.makeArgumentDescription(CommandLineArgumentParser.java:847); 	at org.broadinstitute.barclay.argparser.CommandLineArgumentParser.printArgumentUsage(CommandLineArgumentParser.java:791); 	at org.broadinstitute.barclay.argparser.CommandLineArgumentParser.lambda$printArgumentUsageBlock$2(CommandLineArgumentParser.java:276); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); 	at java.util.stream.SortedOps$SizedRefSortingSink.end(SortedOps.java:352); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); 	at java.util.stream.ForEach",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4875
https://github.com/broadinstitute/gatk/issues/4875:1846,Integrability,wrap,wrapAndCopyInto,1846,ing argument: input; 	at org.broadinstitute.hellbender.cmdline.GATKPlugin.GATKAnnotationPluginDescriptor.getAllowedValuesForDescriptorHelp(GATKAnnotationPluginDescriptor.java:246); 	at org.broadinstitute.barclay.argparser.CommandLineArgumentParser.usageForPluginDescriptorArgumentIfApplicable(CommandLineArgumentParser.java:870); 	at org.broadinstitute.barclay.argparser.CommandLineArgumentParser.makeArgumentDescription(CommandLineArgumentParser.java:847); 	at org.broadinstitute.barclay.argparser.CommandLineArgumentParser.printArgumentUsage(CommandLineArgumentParser.java:791); 	at org.broadinstitute.barclay.argparser.CommandLineArgumentParser.lambda$printArgumentUsageBlock$2(CommandLineArgumentParser.java:276); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); 	at java.util.stream.SortedOps$SizedRefSortingSink.end(SortedOps.java:352); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); 	at org.broadinstitute.barclay.argparser.CommandLineArgumentParser.printArgumentUsageBlock(CommandLineArgumentParser.java:276); 	at org.broadinstitute.barclay.argparser.CommandLineArgumentParser.usage(CommandLineArgumentParser.java:308); 	at org.broadinstitute.barclay.argparser.CommandLineArgumentParser.parseArguments(CommandLineArgumentParser.java:417); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.parseArgs(CommandLineProgram.java:221); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:195); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.bro,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4875
https://github.com/broadinstitute/gatk/pull/4876:74,Testability,test,test,74,Fixes https://github.com/broadinstitute/gatk/issues/4875. The newly added test fails without this change. Also opportunistically removed some dead code.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4876
https://github.com/broadinstitute/gatk/pull/4877:22,Testability,test,tests,22,@LeeTL1220 For you if tests pass,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4877
https://github.com/broadinstitute/gatk/pull/4879:15,Integrability,message,message,15,I noticed this message could be slightly more helpful in #4820.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4879
https://github.com/broadinstitute/gatk/pull/4881:23,Testability,test,test,23,- Fixes #4880 ; - Adds test to catch this if it happens again. Offline I ran it against a large hg38 file and it succeeded.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4881
https://github.com/broadinstitute/gatk/issues/4882:89,Availability,ERROR,ERROR,89,"```; 15:52:58.620 INFO ProgressMeter - chr18:3309146 21.0 6473000 308149.3; 15:53:02.175 ERROR GencodeFuncotationFactory - Unable to create a GencodeFuncotation on transcript ENST00000536353.2 for variant: chr18:9887391-9887436(GGAAGCCATCCAGCCCAAGGAGGGTGACATCCCCAAGTCCCCAGAA* -> G); ```. If we do not need to worry about these, can we downgrade to a warning?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4882
https://github.com/broadinstitute/gatk/issues/4882:335,Availability,down,downgrade,335,"```; 15:52:58.620 INFO ProgressMeter - chr18:3309146 21.0 6473000 308149.3; 15:53:02.175 ERROR GencodeFuncotationFactory - Unable to create a GencodeFuncotation on transcript ENST00000536353.2 for variant: chr18:9887391-9887436(GGAAGCCATCCAGCCCAAGGAGGGTGACATCCCCAAGTCCCCAGAA* -> G); ```. If we do not need to worry about these, can we downgrade to a warning?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4882
https://github.com/broadinstitute/gatk/issues/4883:1867,Availability,avail,available,1867,"```; ......AAAAAAAAAA......; ```; Here we have a homology of exactly 10A's.; When we detect the deletion by studying the alignment signature, the alt haplotype would have two alignments mapped to the reference, one ends just before the G-block, one starts just after the G-block, with the A-block on the alt haplotype mapped to two places.; We follow the left-align/left-justify convention, and place the POS 1-bp before the left most A (hence saying `10A10G` was deleted, as opposed to right-justify which would say `10G10A` deleted, in fact without the convention any contiguous substring of 20 bp long of `10A10G10A` would be correct). However, it can be imagined the homologous sequences flanking the G's are not exactly the same, or may not be the same length (small indels), and the alignments would contain small gaps in their CIGARs. By assuming the homologous sequence are of the same length, which is what we are doing now, we could get the breakpoint location wrong. This is generally not a serious problem, but when the accumulated gap sizes are large enough, we can end up too-far off. A similar issue is when inferring SVLEN for small tandem duplications, where we are assuming the extra copies have the same length. This is not always true and when the `DUP_SEQ_CIGARS` annotation is available, it should be easily fixable. When it is not available, one could use the difference between `SEQ_ALT_HAPLOTYPE` and END-POS for that. #### Steps to reproduce; Run the `StructuralVariationDiscoveryPipelineSpark` pipeline on a site with SV event having different homology length around the breakpoints. #### Expected behavior; Breakpoint inference taking into account of small indels in the micro-homology surrounding the breakpoints. #### Actual behavior; Breakpoint inference assuming homologous sequence surrounding the breakpoints having the same length. #### What could be done:; The inference code could use CIGAR's in the alignment to infer how much to left adjust the breakpoint. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4883
https://github.com/broadinstitute/gatk/issues/4883:1922,Availability,avail,available,1922,"```; ......AAAAAAAAAA......; ```; Here we have a homology of exactly 10A's.; When we detect the deletion by studying the alignment signature, the alt haplotype would have two alignments mapped to the reference, one ends just before the G-block, one starts just after the G-block, with the A-block on the alt haplotype mapped to two places.; We follow the left-align/left-justify convention, and place the POS 1-bp before the left most A (hence saying `10A10G` was deleted, as opposed to right-justify which would say `10G10A` deleted, in fact without the convention any contiguous substring of 20 bp long of `10A10G10A` would be correct). However, it can be imagined the homologous sequences flanking the G's are not exactly the same, or may not be the same length (small indels), and the alignments would contain small gaps in their CIGARs. By assuming the homologous sequence are of the same length, which is what we are doing now, we could get the breakpoint location wrong. This is generally not a serious problem, but when the accumulated gap sizes are large enough, we can end up too-far off. A similar issue is when inferring SVLEN for small tandem duplications, where we are assuming the extra copies have the same length. This is not always true and when the `DUP_SEQ_CIGARS` annotation is available, it should be easily fixable. When it is not available, one could use the difference between `SEQ_ALT_HAPLOTYPE` and END-POS for that. #### Steps to reproduce; Run the `StructuralVariationDiscoveryPipelineSpark` pipeline on a site with SV event having different homology length around the breakpoints. #### Expected behavior; Breakpoint inference taking into account of small indels in the micro-homology surrounding the breakpoints. #### Actual behavior; Breakpoint inference assuming homologous sequence surrounding the breakpoints having the same length. #### What could be done:; The inference code could use CIGAR's in the alignment to infer how much to left adjust the breakpoint. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4883
https://github.com/broadinstitute/gatk/issues/4883:209,Deployability,pipeline,pipeline,209,"## Bug Report. ### Affected tool(s) or class(es); `org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.BreakpointsInference`, hence affecting the location of breakpoint output by the SV discovery pipeline. ### Affected version(s); - [x] Latest public release version [version?]; - [x] Latest master branch as of [date of test?]. ### Description . Micro-homology around breakpoints affects where we place breakpoints in the SV.; Take the simplest example of deletion; where (10A10G10A); ```; ......AAAAAAAAAAGGGGGGGGGGAAAAAAAAAA......; ```; becomes (10A); ```; ......AAAAAAAAAA......; ```; Here we have a homology of exactly 10A's.; When we detect the deletion by studying the alignment signature, the alt haplotype would have two alignments mapped to the reference, one ends just before the G-block, one starts just after the G-block, with the A-block on the alt haplotype mapped to two places.; We follow the left-align/left-justify convention, and place the POS 1-bp before the left most A (hence saying `10A10G` was deleted, as opposed to right-justify which would say `10G10A` deleted, in fact without the convention any contiguous substring of 20 bp long of `10A10G10A` would be correct). However, it can be imagined the homologous sequences flanking the G's are not exactly the same, or may not be the same length (small indels), and the alignments would contain small gaps in their CIGARs. By assuming the homologous sequence are of the same length, which is what we are doing now, we could get the breakpoint location wrong. This is generally not a serious problem, but when the accumulated gap sizes are large enough, we can end up too-far off. A similar issue is when inferring SVLEN for small tandem duplications, where we are assuming the extra copies have the same length. This is not always true and when the `DUP_SEQ_CIGARS` annotation is available, it should be easily fixable. When it is not available, one could use the difference between `SEQ_ALT_HAPLOTYPE` and END-POS",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4883
https://github.com/broadinstitute/gatk/issues/4883:264,Deployability,release,release,264,"## Bug Report. ### Affected tool(s) or class(es); `org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.BreakpointsInference`, hence affecting the location of breakpoint output by the SV discovery pipeline. ### Affected version(s); - [x] Latest public release version [version?]; - [x] Latest master branch as of [date of test?]. ### Description . Micro-homology around breakpoints affects where we place breakpoints in the SV.; Take the simplest example of deletion; where (10A10G10A); ```; ......AAAAAAAAAAGGGGGGGGGGAAAAAAAAAA......; ```; becomes (10A); ```; ......AAAAAAAAAA......; ```; Here we have a homology of exactly 10A's.; When we detect the deletion by studying the alignment signature, the alt haplotype would have two alignments mapped to the reference, one ends just before the G-block, one starts just after the G-block, with the A-block on the alt haplotype mapped to two places.; We follow the left-align/left-justify convention, and place the POS 1-bp before the left most A (hence saying `10A10G` was deleted, as opposed to right-justify which would say `10G10A` deleted, in fact without the convention any contiguous substring of 20 bp long of `10A10G10A` would be correct). However, it can be imagined the homologous sequences flanking the G's are not exactly the same, or may not be the same length (small indels), and the alignments would contain small gaps in their CIGARs. By assuming the homologous sequence are of the same length, which is what we are doing now, we could get the breakpoint location wrong. This is generally not a serious problem, but when the accumulated gap sizes are large enough, we can end up too-far off. A similar issue is when inferring SVLEN for small tandem duplications, where we are assuming the extra copies have the same length. This is not always true and when the `DUP_SEQ_CIGARS` annotation is available, it should be easily fixable. When it is not available, one could use the difference between `SEQ_ALT_HAPLOTYPE` and END-POS",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4883
https://github.com/broadinstitute/gatk/issues/4883:2089,Deployability,pipeline,pipeline,2089,"```; ......AAAAAAAAAA......; ```; Here we have a homology of exactly 10A's.; When we detect the deletion by studying the alignment signature, the alt haplotype would have two alignments mapped to the reference, one ends just before the G-block, one starts just after the G-block, with the A-block on the alt haplotype mapped to two places.; We follow the left-align/left-justify convention, and place the POS 1-bp before the left most A (hence saying `10A10G` was deleted, as opposed to right-justify which would say `10G10A` deleted, in fact without the convention any contiguous substring of 20 bp long of `10A10G10A` would be correct). However, it can be imagined the homologous sequences flanking the G's are not exactly the same, or may not be the same length (small indels), and the alignments would contain small gaps in their CIGARs. By assuming the homologous sequence are of the same length, which is what we are doing now, we could get the breakpoint location wrong. This is generally not a serious problem, but when the accumulated gap sizes are large enough, we can end up too-far off. A similar issue is when inferring SVLEN for small tandem duplications, where we are assuming the extra copies have the same length. This is not always true and when the `DUP_SEQ_CIGARS` annotation is available, it should be easily fixable. When it is not available, one could use the difference between `SEQ_ALT_HAPLOTYPE` and END-POS for that. #### Steps to reproduce; Run the `StructuralVariationDiscoveryPipelineSpark` pipeline on a site with SV event having different homology length around the breakpoints. #### Expected behavior; Breakpoint inference taking into account of small indels in the micro-homology surrounding the breakpoints. #### Actual behavior; Breakpoint inference assuming homologous sequence surrounding the breakpoints having the same length. #### What could be done:; The inference code could use CIGAR's in the alignment to infer how much to left adjust the breakpoint. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4883
https://github.com/broadinstitute/gatk/issues/4883:653,Safety,detect,detect,653,"## Bug Report. ### Affected tool(s) or class(es); `org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.BreakpointsInference`, hence affecting the location of breakpoint output by the SV discovery pipeline. ### Affected version(s); - [x] Latest public release version [version?]; - [x] Latest master branch as of [date of test?]. ### Description . Micro-homology around breakpoints affects where we place breakpoints in the SV.; Take the simplest example of deletion; where (10A10G10A); ```; ......AAAAAAAAAAGGGGGGGGGGAAAAAAAAAA......; ```; becomes (10A); ```; ......AAAAAAAAAA......; ```; Here we have a homology of exactly 10A's.; When we detect the deletion by studying the alignment signature, the alt haplotype would have two alignments mapped to the reference, one ends just before the G-block, one starts just after the G-block, with the A-block on the alt haplotype mapped to two places.; We follow the left-align/left-justify convention, and place the POS 1-bp before the left most A (hence saying `10A10G` was deleted, as opposed to right-justify which would say `10G10A` deleted, in fact without the convention any contiguous substring of 20 bp long of `10A10G10A` would be correct). However, it can be imagined the homologous sequences flanking the G's are not exactly the same, or may not be the same length (small indels), and the alignments would contain small gaps in their CIGARs. By assuming the homologous sequence are of the same length, which is what we are doing now, we could get the breakpoint location wrong. This is generally not a serious problem, but when the accumulated gap sizes are large enough, we can end up too-far off. A similar issue is when inferring SVLEN for small tandem duplications, where we are assuming the extra copies have the same length. This is not always true and when the `DUP_SEQ_CIGARS` annotation is available, it should be easily fixable. When it is not available, one could use the difference between `SEQ_ALT_HAPLOTYPE` and END-POS",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4883
https://github.com/broadinstitute/gatk/issues/4883:334,Testability,test,test,334,"## Bug Report. ### Affected tool(s) or class(es); `org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.BreakpointsInference`, hence affecting the location of breakpoint output by the SV discovery pipeline. ### Affected version(s); - [x] Latest public release version [version?]; - [x] Latest master branch as of [date of test?]. ### Description . Micro-homology around breakpoints affects where we place breakpoints in the SV.; Take the simplest example of deletion; where (10A10G10A); ```; ......AAAAAAAAAAGGGGGGGGGGAAAAAAAAAA......; ```; becomes (10A); ```; ......AAAAAAAAAA......; ```; Here we have a homology of exactly 10A's.; When we detect the deletion by studying the alignment signature, the alt haplotype would have two alignments mapped to the reference, one ends just before the G-block, one starts just after the G-block, with the A-block on the alt haplotype mapped to two places.; We follow the left-align/left-justify convention, and place the POS 1-bp before the left most A (hence saying `10A10G` was deleted, as opposed to right-justify which would say `10G10A` deleted, in fact without the convention any contiguous substring of 20 bp long of `10A10G10A` would be correct). However, it can be imagined the homologous sequences flanking the G's are not exactly the same, or may not be the same length (small indels), and the alignments would contain small gaps in their CIGARs. By assuming the homologous sequence are of the same length, which is what we are doing now, we could get the breakpoint location wrong. This is generally not a serious problem, but when the accumulated gap sizes are large enough, we can end up too-far off. A similar issue is when inferring SVLEN for small tandem duplications, where we are assuming the extra copies have the same length. This is not always true and when the `DUP_SEQ_CIGARS` annotation is available, it should be easily fixable. When it is not available, one could use the difference between `SEQ_ALT_HAPLOTYPE` and END-POS",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4883
https://github.com/broadinstitute/gatk/issues/4883:450,Usability,simpl,simplest,450,"## Bug Report. ### Affected tool(s) or class(es); `org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.BreakpointsInference`, hence affecting the location of breakpoint output by the SV discovery pipeline. ### Affected version(s); - [x] Latest public release version [version?]; - [x] Latest master branch as of [date of test?]. ### Description . Micro-homology around breakpoints affects where we place breakpoints in the SV.; Take the simplest example of deletion; where (10A10G10A); ```; ......AAAAAAAAAAGGGGGGGGGGAAAAAAAAAA......; ```; becomes (10A); ```; ......AAAAAAAAAA......; ```; Here we have a homology of exactly 10A's.; When we detect the deletion by studying the alignment signature, the alt haplotype would have two alignments mapped to the reference, one ends just before the G-block, one starts just after the G-block, with the A-block on the alt haplotype mapped to two places.; We follow the left-align/left-justify convention, and place the POS 1-bp before the left most A (hence saying `10A10G` was deleted, as opposed to right-justify which would say `10G10A` deleted, in fact without the convention any contiguous substring of 20 bp long of `10A10G10A` would be correct). However, it can be imagined the homologous sequences flanking the G's are not exactly the same, or may not be the same length (small indels), and the alignments would contain small gaps in their CIGARs. By assuming the homologous sequence are of the same length, which is what we are doing now, we could get the breakpoint location wrong. This is generally not a serious problem, but when the accumulated gap sizes are large enough, we can end up too-far off. A similar issue is when inferring SVLEN for small tandem duplications, where we are assuming the extra copies have the same length. This is not always true and when the `DUP_SEQ_CIGARS` annotation is available, it should be easily fixable. When it is not available, one could use the difference between `SEQ_ALT_HAPLOTYPE` and END-POS",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4883
https://github.com/broadinstitute/gatk/issues/4888:55,Availability,error,errors,55,"We've had reports of two kinds of intermittent network errors when using NIO that are not currently caught by google-cloud-nio:. * `UnknownHostException`; * 502 Bad Gateway. We should patch the library to add these errors to the list of retryable/reopenable errors. We can start by patching the GATK fork of `google-cloud-java`, and then concurrently submit a PR against `google-cloud-java` proper.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4888
https://github.com/broadinstitute/gatk/issues/4888:215,Availability,error,errors,215,"We've had reports of two kinds of intermittent network errors when using NIO that are not currently caught by google-cloud-nio:. * `UnknownHostException`; * 502 Bad Gateway. We should patch the library to add these errors to the list of retryable/reopenable errors. We can start by patching the GATK fork of `google-cloud-java`, and then concurrently submit a PR against `google-cloud-java` proper.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4888
https://github.com/broadinstitute/gatk/issues/4888:258,Availability,error,errors,258,"We've had reports of two kinds of intermittent network errors when using NIO that are not currently caught by google-cloud-nio:. * `UnknownHostException`; * 502 Bad Gateway. We should patch the library to add these errors to the list of retryable/reopenable errors. We can start by patching the GATK fork of `google-cloud-java`, and then concurrently submit a PR against `google-cloud-java` proper.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4888
https://github.com/broadinstitute/gatk/issues/4888:184,Deployability,patch,patch,184,"We've had reports of two kinds of intermittent network errors when using NIO that are not currently caught by google-cloud-nio:. * `UnknownHostException`; * 502 Bad Gateway. We should patch the library to add these errors to the list of retryable/reopenable errors. We can start by patching the GATK fork of `google-cloud-java`, and then concurrently submit a PR against `google-cloud-java` proper.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4888
https://github.com/broadinstitute/gatk/issues/4888:282,Deployability,patch,patching,282,"We've had reports of two kinds of intermittent network errors when using NIO that are not currently caught by google-cloud-nio:. * `UnknownHostException`; * 502 Bad Gateway. We should patch the library to add these errors to the list of retryable/reopenable errors. We can start by patching the GATK fork of `google-cloud-java`, and then concurrently submit a PR against `google-cloud-java` proper.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4888
https://github.com/broadinstitute/gatk/issues/4888:338,Performance,concurren,concurrently,338,"We've had reports of two kinds of intermittent network errors when using NIO that are not currently caught by google-cloud-nio:. * `UnknownHostException`; * 502 Bad Gateway. We should patch the library to add these errors to the list of retryable/reopenable errors. We can start by patching the GATK fork of `google-cloud-java`, and then concurrently submit a PR against `google-cloud-java` proper.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4888
https://github.com/broadinstitute/gatk/issues/4892:474,Availability,repair,repair,474,"@yfarjoun and I recently discovered the following artifact in 30 FFPE samples:. The artifact occurs in DNA transposons with sequence 5'[S1][ITR][transposase][ITR'][S2]3', where S1 and S2 are arbitrary flanking sequence, ITR and ITR' are the inverted tandem repeat and its reverse complement (on the *same* strand). The artifact occurs when FFPE damage in the ITR sequence disrupts base pairing, causing ITR to pair with ITR', thereby creating a single-strand loop. Then end-repair (that is, filling in the 3' underhang) copies sequence from S1 into S2, yielding an insertion artifact. We verified a few signatures of this artifact:; * The insertion is always on read 2; * the fragment size for any given artifact is constant + 2 * # of soft-clipped bases; * When the artifact occurs upstream (downstream) of the insertion it is on a forward (reverse) strand read.; * the fragments 3' and 5' sequences are reverse complements of each other. The most natural way to filter would be with the read and its mate, but that's tricky in the GATK engine so we can also implement a filter that knows the reference context. This could also work as a hard-clipping read transformer.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4892
https://github.com/broadinstitute/gatk/issues/4892:793,Availability,down,downstream,793,"@yfarjoun and I recently discovered the following artifact in 30 FFPE samples:. The artifact occurs in DNA transposons with sequence 5'[S1][ITR][transposase][ITR'][S2]3', where S1 and S2 are arbitrary flanking sequence, ITR and ITR' are the inverted tandem repeat and its reverse complement (on the *same* strand). The artifact occurs when FFPE damage in the ITR sequence disrupts base pairing, causing ITR to pair with ITR', thereby creating a single-strand loop. Then end-repair (that is, filling in the 3' underhang) copies sequence from S1 into S2, yielding an insertion artifact. We verified a few signatures of this artifact:; * The insertion is always on read 2; * the fragment size for any given artifact is constant + 2 * # of soft-clipped bases; * When the artifact occurs upstream (downstream) of the insertion it is on a forward (reverse) strand read.; * the fragments 3' and 5' sequences are reverse complements of each other. The most natural way to filter would be with the read and its mate, but that's tricky in the GATK engine so we can also implement a filter that knows the reference context. This could also work as a hard-clipping read transformer.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4892
https://github.com/broadinstitute/gatk/issues/4896:460,Safety,avoid,avoid,460,"@jamesemery has found that he's been able to make `MarkDuplicatesSpark` call directly into Picard code for much core functionality. As a result, `MarkDuplicatesGATK` has gotten out-of-date relative to both the Spark version and the Picard version. I think that we should remove `MarkDuplicatesGATK` for now, as it's likely just confusing users. Longer term, we should still discuss migrating development of the Picard `MarkDuplicates` tool to GATK in order to avoid constant divergence between that version and the Spark version, but if we end up doing this we'd likely want to re-port from scratch anyway rather than use `MarkDuplicatesGATK` as a starting point.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4896
https://github.com/broadinstitute/gatk/issues/4897:332,Deployability,configurat,configuration,332,"### Affected tool(s) or class(es); Funcotator, but could be future tools as well. ### Affected version(s); - [ ] Latest master branch as of [June 14, 2018]. ### Description ; Currently, if you want to read a MAF the GATK will use AnnotatedIntervalCodec. This is fine in the majority of cases. However, under the hood, it is using a configuration setup that has an aliasing scheme. This alias scheme is fairly permissive and can lead to conflicts. For example, if a MAF has a column named ""END"", the MAF will not parse, since the default configuration will attempt to use the ""END"" column instead of ""End_Position"". This can be fixed if we have a MAF codec, but some decisions need to be made. For example, should it produce AnnotatedIntervals? Variant may be too difficult.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4897
https://github.com/broadinstitute/gatk/issues/4897:537,Deployability,configurat,configuration,537,"### Affected tool(s) or class(es); Funcotator, but could be future tools as well. ### Affected version(s); - [ ] Latest master branch as of [June 14, 2018]. ### Description ; Currently, if you want to read a MAF the GATK will use AnnotatedIntervalCodec. This is fine in the majority of cases. However, under the hood, it is using a configuration setup that has an aliasing scheme. This alias scheme is fairly permissive and can lead to conflicts. For example, if a MAF has a column named ""END"", the MAF will not parse, since the default configuration will attempt to use the ""END"" column instead of ""End_Position"". This can be fixed if we have a MAF codec, but some decisions need to be made. For example, should it produce AnnotatedIntervals? Variant may be too difficult.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4897
https://github.com/broadinstitute/gatk/issues/4897:332,Modifiability,config,configuration,332,"### Affected tool(s) or class(es); Funcotator, but could be future tools as well. ### Affected version(s); - [ ] Latest master branch as of [June 14, 2018]. ### Description ; Currently, if you want to read a MAF the GATK will use AnnotatedIntervalCodec. This is fine in the majority of cases. However, under the hood, it is using a configuration setup that has an aliasing scheme. This alias scheme is fairly permissive and can lead to conflicts. For example, if a MAF has a column named ""END"", the MAF will not parse, since the default configuration will attempt to use the ""END"" column instead of ""End_Position"". This can be fixed if we have a MAF codec, but some decisions need to be made. For example, should it produce AnnotatedIntervals? Variant may be too difficult.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4897
https://github.com/broadinstitute/gatk/issues/4897:537,Modifiability,config,configuration,537,"### Affected tool(s) or class(es); Funcotator, but could be future tools as well. ### Affected version(s); - [ ] Latest master branch as of [June 14, 2018]. ### Description ; Currently, if you want to read a MAF the GATK will use AnnotatedIntervalCodec. This is fine in the majority of cases. However, under the hood, it is using a configuration setup that has an aliasing scheme. This alias scheme is fairly permissive and can lead to conflicts. For example, if a MAF has a column named ""END"", the MAF will not parse, since the default configuration will attempt to use the ""END"" column instead of ""End_Position"". This can be fixed if we have a MAF codec, but some decisions need to be made. For example, should it produce AnnotatedIntervals? Variant may be too difficult.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4897
https://github.com/broadinstitute/gatk/issues/4898:1306,Deployability,release,release,1306,"gatk).; - Search the existing github issues to see if your issue (or something similar) has already been reported. If the issue already exists, you may comment there to inquire about the progress.; - Determine whether your issue is a **bug report**, a **feature request**, or a **documentation request** (for tool/class javadoc only -- for forum docs please post there); - Consider if your ""issue"" is better addressed on the GATK forum: http://gatkforums.broadinstitute.org/gatk Post there if you have questions about expected tool behavior, output format, unexpected results, or generally any question that does not fit into the categories above; - Use a **concise** yet **descriptive** title; - Choose the corresponding template block below and fill it in, replacing or deleting text in italics (surrounded by `_`) as appropriate; - Delete the other template blocks and this header. ----. ## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_. ### Affected version(s); - [ ] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._. #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_. #### Expected behavior; _Tell us what should happen_. #### Actual behavior; _Tell us what happens instead_. ----. ## Feature request. ### Tool(s) or class(es) involved; _Tool/class name(s), special parameters?_. ### Description; _Specify whether you want a modification of an existing behavior or addition of a new capability._; _Provide **examples**, **screenshots**, where appropriate._. ----. ## Documentation request. ### Tool(s) or class(es) involved; _Tool/class name(s), parameters?_. ### Description ; _Describe what needs to be added or modified._. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4898
https://github.com/broadinstitute/gatk/issues/4898:1376,Testability,test,test,1376,"gatk).; - Search the existing github issues to see if your issue (or something similar) has already been reported. If the issue already exists, you may comment there to inquire about the progress.; - Determine whether your issue is a **bug report**, a **feature request**, or a **documentation request** (for tool/class javadoc only -- for forum docs please post there); - Consider if your ""issue"" is better addressed on the GATK forum: http://gatkforums.broadinstitute.org/gatk Post there if you have questions about expected tool behavior, output format, unexpected results, or generally any question that does not fit into the categories above; - Use a **concise** yet **descriptive** title; - Choose the corresponding template block below and fill it in, replacing or deleting text in italics (surrounded by `_`) as appropriate; - Delete the other template blocks and this header. ----. ## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_. ### Affected version(s); - [ ] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._. #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_. #### Expected behavior; _Tell us what should happen_. #### Actual behavior; _Tell us what happens instead_. ----. ## Feature request. ### Tool(s) or class(es) involved; _Tool/class name(s), special parameters?_. ### Description; _Specify whether you want a modification of an existing behavior or addition of a new capability._; _Provide **examples**, **screenshots**, where appropriate._. ----. ## Documentation request. ### Tool(s) or class(es) involved; _Tool/class name(s), parameters?_. ### Description ; _Describe what needs to be added or modified._. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4898
https://github.com/broadinstitute/gatk/issues/4898:1476,Testability,log,logs,1476,"gatk).; - Search the existing github issues to see if your issue (or something similar) has already been reported. If the issue already exists, you may comment there to inquire about the progress.; - Determine whether your issue is a **bug report**, a **feature request**, or a **documentation request** (for tool/class javadoc only -- for forum docs please post there); - Consider if your ""issue"" is better addressed on the GATK forum: http://gatkforums.broadinstitute.org/gatk Post there if you have questions about expected tool behavior, output format, unexpected results, or generally any question that does not fit into the categories above; - Use a **concise** yet **descriptive** title; - Choose the corresponding template block below and fill it in, replacing or deleting text in italics (surrounded by `_`) as appropriate; - Delete the other template blocks and this header. ----. ## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_. ### Affected version(s); - [ ] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._. #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_. #### Expected behavior; _Tell us what should happen_. #### Actual behavior; _Tell us what happens instead_. ----. ## Feature request. ### Tool(s) or class(es) involved; _Tool/class name(s), special parameters?_. ### Description; _Specify whether you want a modification of an existing behavior or addition of a new capability._; _Provide **examples**, **screenshots**, where appropriate._. ----. ## Documentation request. ### Tool(s) or class(es) involved; _Tool/class name(s), parameters?_. ### Description ; _Describe what needs to be added or modified._. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4898
https://github.com/broadinstitute/gatk/pull/4902:227,Performance,cache,cache,227,"Allow tools to opt-in for read caching (enabled in this PR for CNNScoreVariants and FilterAlignmentArtifacts) when issuing forward-only queries such as those used to query a ReadsContext for all reads backing each variant. The cache matches the (sometimes surprising - see https://github.com/broadinstitute/gatk/issues/4901) results that are returned when caching is not used, specifically that an unmapped but placed read that is mated with a mapped read is only returned if the *start position* overlaps the interval query interval, whereas the mapped read is returned if any part of the read overlaps the query interval.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4902
https://github.com/broadinstitute/gatk/pull/4905:15,Usability,simpl,simple,15,This is a very simple change to allow subclasses of `AbstractConcordanceWalker` to use NIO for their truth callset.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4905
https://github.com/broadinstitute/gatk/issues/4906:92,Integrability,depend,dependent,92,"For multi-allelic variants, funcotator should cache all the annotations that are not allele-dependent. This will give a minor speed improvement because it will not have to mechanically create the new annotations from scratch for each allele.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4906
https://github.com/broadinstitute/gatk/issues/4906:46,Performance,cache,cache,46,"For multi-allelic variants, funcotator should cache all the annotations that are not allele-dependent. This will give a minor speed improvement because it will not have to mechanically create the new annotations from scratch for each allele.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4906
https://github.com/broadinstitute/gatk/issues/4907:308,Integrability,message,message,308,Occasionally Funcotator detects that a given reference allele is different from the underlying reference genome. In these cases a new funcotation should be added to the variant that details the nature of the difference between the given reference allele and the reference genome content. Currently a warning message is generated:. ```; 16:34:57.133 WARN FuncotatorUtils:1461 - Reference allele is different than the reference coding sequence! Substituting given allele for sequence code (GGC->AGC); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4907
https://github.com/broadinstitute/gatk/issues/4907:24,Safety,detect,detects,24,Occasionally Funcotator detects that a given reference allele is different from the underlying reference genome. In these cases a new funcotation should be added to the variant that details the nature of the difference between the given reference allele and the reference genome content. Currently a warning message is generated:. ```; 16:34:57.133 WARN FuncotatorUtils:1461 - Reference allele is different than the reference coding sequence! Substituting given allele for sequence code (GGC->AGC); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4907
https://github.com/broadinstitute/gatk/issues/4912:176,Integrability,protocol,protocol,176,"## Feature request. ### Tool(s) or class(es) involved; Funcotator. ### Description; Currently, if you attempt to specify more than one pair in an input VCF, there is no agreed protocol for such a thing. As a result, if Funcotator ever determines that there is more than one pair in an input VCF, it will fail. Please note that the current implementation is unlikely to find more than one pair in the wild. This is an issue being filed to allow more than one pair to be specified and to have a protocol for doing so in a VCF. -[ ] Determine protocol(s) for specifying more than one pair.; -[ ] Alter `MafOutputRenderer` to use the pair information to populate the sample fields correctly and grab the annotations only applicable to that pair. There are other concerns here, such as checking the genotype fields (why add an entry in the MAF for a tumor 0/0 genotype). This is probably not a trivial issue to address. FYI: @davidbenjamin and @jonn-smith",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4912
https://github.com/broadinstitute/gatk/issues/4912:493,Integrability,protocol,protocol,493,"## Feature request. ### Tool(s) or class(es) involved; Funcotator. ### Description; Currently, if you attempt to specify more than one pair in an input VCF, there is no agreed protocol for such a thing. As a result, if Funcotator ever determines that there is more than one pair in an input VCF, it will fail. Please note that the current implementation is unlikely to find more than one pair in the wild. This is an issue being filed to allow more than one pair to be specified and to have a protocol for doing so in a VCF. -[ ] Determine protocol(s) for specifying more than one pair.; -[ ] Alter `MafOutputRenderer` to use the pair information to populate the sample fields correctly and grab the annotations only applicable to that pair. There are other concerns here, such as checking the genotype fields (why add an entry in the MAF for a tumor 0/0 genotype). This is probably not a trivial issue to address. FYI: @davidbenjamin and @jonn-smith",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4912
https://github.com/broadinstitute/gatk/issues/4912:540,Integrability,protocol,protocol,540,"## Feature request. ### Tool(s) or class(es) involved; Funcotator. ### Description; Currently, if you attempt to specify more than one pair in an input VCF, there is no agreed protocol for such a thing. As a result, if Funcotator ever determines that there is more than one pair in an input VCF, it will fail. Please note that the current implementation is unlikely to find more than one pair in the wild. This is an issue being filed to allow more than one pair to be specified and to have a protocol for doing so in a VCF. -[ ] Determine protocol(s) for specifying more than one pair.; -[ ] Alter `MafOutputRenderer` to use the pair information to populate the sample fields correctly and grab the annotations only applicable to that pair. There are other concerns here, such as checking the genotype fields (why add an entry in the MAF for a tumor 0/0 genotype). This is probably not a trivial issue to address. FYI: @davidbenjamin and @jonn-smith",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4912
https://github.com/broadinstitute/gatk/pull/4914:0,Deployability,Update,Update,0,Update to htsjdk 2.16.0. This only updates gatk to fix the compile warnings from deprecations. An additional PR is needed in order to support fasta.gz files. fixes #4039,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4914
https://github.com/broadinstitute/gatk/pull/4914:35,Deployability,update,updates,35,Update to htsjdk 2.16.0. This only updates gatk to fix the compile warnings from deprecations. An additional PR is needed in order to support fasta.gz files. fixes #4039,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4914
https://github.com/broadinstitute/gatk/pull/4915:90,Modifiability,config,config,90,Fixes #4193. `XsvTableFeature` no longer removes an extra column if start and end in; the config file for a `LocatableXsv` data source are the same.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4915
https://github.com/broadinstitute/gatk/pull/4916:0,Testability,Test,Test,0,Test to verify fix for https://github.com/broadinstitute/gatk/issues/3669 now that the htsjdk fix (https://github.com/samtools/htsjdk/pull/1125) is in.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4916
https://github.com/broadinstitute/gatk/issues/4920:46,Testability,test,tests,46,`bestToZero` is always `false` except in unit tests. Can we eliminate it and the code paths for when it is `true`? @vruano can you think of a reason to keep this argument?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4920
https://github.com/broadinstitute/gatk/issues/4921:41,Availability,error,error,41,"For some reason, this user is getting an error in 4.0.5.1 when this was supposed to have been fixed in GATK3. ----; User Report; ----; I'm trying to get a set of robust variants to use to recalibrate quality scores. I called variants using gatk4, and then tried to perform VariantFiltration:. gatk-4.0.5.1/gatk VariantFiltration -R data/genome.fasta -V variants/6753_12-15-2015_first_pass_filtered.vcf -filter 'QD > 2 && FS > 60 && SOR < 3 && MQ > 40 && MQRankSum > -3 && ReadPosRankSum > -4' -output variants/6753_12-15-2015_second_pass_filtered.vcf -filter-name ""default"" . However, it complains with a java.lang.NumberFormatException:. Using GATK jar gatk-4.0.5.1/gatk-package-4.0.5.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar gatk-4.0.5.1/gatk-package-4.0.5.1-local.jar VariantFiltration -R data/genome.fasta -V variants/6753_12-15-2015_first_pass_filtered.vcf -filter QD > 2 && FS > 60 && SOR < 3 && MQ > 40 && MQRankSum > -3 && ReadPosRankSum > -4 -output variants/6753_12-15-2015_second_pass_filtered.vcf -filter-name default; 15:42:33.964 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:gatk-4.0.5.1/gatk-package-4.0.5.1-local.jar!/com/intel/gkl/native/libgkl_compression.dylib; 15:42:34.114 INFO VariantFiltration - ------------------------------------------------------------; 15:42:34.115 INFO VariantFiltration - The Genome Analysis Toolkit (GATK) v4.0.5.1; 15:42:34.115 INFO VariantFiltration - For support and documentation go to https://software.broadinstitute.org/gatk/; 15:42:34.115 INFO VariantFiltration - Executing as sherlock@DN52ehae.SUNet on Mac OS X v10.13.5 x86_64; 15:42:34.116 INFO VariantFiltration - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_91-b14; 15:42:34.116 INFO VariantFiltration - Start Date/Time: June 15, 2018 3:42:33 PM PDT; 15:42:34.116 INFO VariantFiltration - ----------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4921
https://github.com/broadinstitute/gatk/issues/4921:162,Availability,robust,robust,162,"For some reason, this user is getting an error in 4.0.5.1 when this was supposed to have been fixed in GATK3. ----; User Report; ----; I'm trying to get a set of robust variants to use to recalibrate quality scores. I called variants using gatk4, and then tried to perform VariantFiltration:. gatk-4.0.5.1/gatk VariantFiltration -R data/genome.fasta -V variants/6753_12-15-2015_first_pass_filtered.vcf -filter 'QD > 2 && FS > 60 && SOR < 3 && MQ > 40 && MQRankSum > -3 && ReadPosRankSum > -4' -output variants/6753_12-15-2015_second_pass_filtered.vcf -filter-name ""default"" . However, it complains with a java.lang.NumberFormatException:. Using GATK jar gatk-4.0.5.1/gatk-package-4.0.5.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar gatk-4.0.5.1/gatk-package-4.0.5.1-local.jar VariantFiltration -R data/genome.fasta -V variants/6753_12-15-2015_first_pass_filtered.vcf -filter QD > 2 && FS > 60 && SOR < 3 && MQ > 40 && MQRankSum > -3 && ReadPosRankSum > -4 -output variants/6753_12-15-2015_second_pass_filtered.vcf -filter-name default; 15:42:33.964 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:gatk-4.0.5.1/gatk-package-4.0.5.1-local.jar!/com/intel/gkl/native/libgkl_compression.dylib; 15:42:34.114 INFO VariantFiltration - ------------------------------------------------------------; 15:42:34.115 INFO VariantFiltration - The Genome Analysis Toolkit (GATK) v4.0.5.1; 15:42:34.115 INFO VariantFiltration - For support and documentation go to https://software.broadinstitute.org/gatk/; 15:42:34.115 INFO VariantFiltration - Executing as sherlock@DN52ehae.SUNet on Mac OS X v10.13.5 x86_64; 15:42:34.116 INFO VariantFiltration - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_91-b14; 15:42:34.116 INFO VariantFiltration - Start Date/Time: June 15, 2018 3:42:33 PM PDT; 15:42:34.116 INFO VariantFiltration - ----------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4921
https://github.com/broadinstitute/gatk/issues/4921:3496,Availability,down,down,3496,"SYNC_IO_WRITE_FOR_SAMTOOLS : true; 15:42:34.118 INFO VariantFiltration - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 15:42:34.118 INFO VariantFiltration - Deflater: IntelDeflater; 15:42:34.119 INFO VariantFiltration - Inflater: IntelInflater; 15:42:34.119 INFO VariantFiltration - GCS max retries/reopens: 20; 15:42:34.119 INFO VariantFiltration - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 15:42:34.119 INFO VariantFiltration - Initializing engine; 15:42:34.634 INFO FeatureManager - Using codec VCFCodec to read file file:///Users/sherlock/dev/Bhatt_lab/crassphage/variants/6753_12-15-2015_first_pass_filtered.vcf; 15:42:34.663 INFO VariantFiltration - Done initializing engine; 15:42:34.750 INFO ProgressMeter - Starting traversal; 15:42:34.750 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 15:42:34.781 INFO VariantFiltration - Shutting down engine; [June 15, 2018 3:42:34 PM PDT] org.broadinstitute.hellbender.tools.walkers.filters.VariantFiltration done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=342884352; java.lang.NumberFormatException: For input string: ""26.67""; 	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65); 	at java.lang.Long.parseLong(Long.java:589); 	at java.lang.Long.parseLong(Long.java:631); 	at org.apache.commons.jexl2.JexlArithmetic.toLong(JexlArithmetic.java:906); 	at org.apache.commons.jexl2.JexlArithmetic.compare(JexlArithmetic.java:718); 	at org.apache.commons.jexl2.JexlArithmetic.greaterThan(JexlArithmetic.java:790); 	at org.apache.commons.jexl2.Interpreter.visit(Interpreter.java:796); 	at org.apache.commons.jexl2.parser.ASTGTNode.jjtAccept(ASTGTNode.java:18); 	at org.apache.commons.jexl2.Interpreter.visit(Interpreter.java:449); 	at org.apache.commons.jexl2.parser.ASTAndNode.jjtAccept(ASTAndNode.java:18); 	at org.apache.commons.jexl2.Interpreter.visit(Interpreter.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4921
https://github.com/broadinstitute/gatk/issues/4921:7469,Availability,error,errors,7469,"rencePipeline.java:175); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); 	at org.broadinstitute.hellbender.engine.VariantWalkerBase.traverse(VariantWalkerBase.java:107); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:994); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:135); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:180); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:199); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289). I don't really know how to fix it. ValidateVariants gives no errors, and I am able to perform variant selection, e.g.:. gatk-4.0.5.1/gatk SelectVariants -R data/genome.fasta -V variants/6753_12-15-2015_first_pass_raw.vcf -select 'vc.getGenotype(""6753_12-15-2015"").getAD().1/vc.getGenotype(""6753_12-15-2015"").getDP() > 0.9 ' -output variants/6753_12-15-2015_first_pass_filtered.vcf. with no problems. Insights would be gratefully appreciated.; Thanks!; Gavin. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/12223/java-lang-numberformatexception-when-trying-to-perform-variantfiltration/p1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4921
https://github.com/broadinstitute/gatk/issues/4921:2885,Deployability,patch,patch,2885,"ation - Start Date/Time: June 15, 2018 3:42:33 PM PDT; 15:42:34.116 INFO VariantFiltration - ------------------------------------------------------------; 15:42:34.116 INFO VariantFiltration - ------------------------------------------------------------; 15:42:34.117 INFO VariantFiltration - HTSJDK Version: 2.15.1; 15:42:34.118 INFO VariantFiltration - Picard Version: 2.18.2; 15:42:34.118 INFO VariantFiltration - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 15:42:34.118 INFO VariantFiltration - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 15:42:34.118 INFO VariantFiltration - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 15:42:34.118 INFO VariantFiltration - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 15:42:34.118 INFO VariantFiltration - Deflater: IntelDeflater; 15:42:34.119 INFO VariantFiltration - Inflater: IntelInflater; 15:42:34.119 INFO VariantFiltration - GCS max retries/reopens: 20; 15:42:34.119 INFO VariantFiltration - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 15:42:34.119 INFO VariantFiltration - Initializing engine; 15:42:34.634 INFO FeatureManager - Using codec VCFCodec to read file file:///Users/sherlock/dev/Bhatt_lab/crassphage/variants/6753_12-15-2015_first_pass_filtered.vcf; 15:42:34.663 INFO VariantFiltration - Done initializing engine; 15:42:34.750 INFO ProgressMeter - Starting traversal; 15:42:34.750 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 15:42:34.781 INFO VariantFiltration - Shutting down engine; [June 15, 2018 3:42:34 PM PDT] org.broadinstitute.hellbender.tools.walkers.filters.VariantFiltration done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=342884352; java.lang.NumberFormatException: For input string: ""26.67""; 	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65); 	at java.lang.Long.parseLong(Long.java:589); 	at java.lang.Long.parseLo",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4921
https://github.com/broadinstitute/gatk/issues/4921:6345,Integrability,wrap,wrapAndCopyInto,6345,xt.JEXLMap.get(JEXLMap.java:15); 	at htsjdk.variant.variantcontext.VariantContextUtils.match(VariantContextUtils.java:338); 	at org.broadinstitute.hellbender.tools.walkers.filters.VariantFiltration.matchesFilter(VariantFiltration.java:380); 	at org.broadinstitute.hellbender.tools.walkers.filters.VariantFiltration.filter(VariantFiltration.java:339); 	at org.broadinstitute.hellbender.tools.walkers.filters.VariantFiltration.apply(VariantFiltration.java:299); 	at org.broadinstitute.hellbender.engine.VariantWalkerBase.lambda$traverse$0(VariantWalkerBase.java:109); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); 	at org.broadinstitute.hellbender.engine.VariantWalkerBase.traverse(VariantWalkerBase.java:107); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:994); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:135); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:180); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:199); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadi,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4921
https://github.com/broadinstitute/gatk/issues/4921:265,Performance,perform,perform,265,"For some reason, this user is getting an error in 4.0.5.1 when this was supposed to have been fixed in GATK3. ----; User Report; ----; I'm trying to get a set of robust variants to use to recalibrate quality scores. I called variants using gatk4, and then tried to perform VariantFiltration:. gatk-4.0.5.1/gatk VariantFiltration -R data/genome.fasta -V variants/6753_12-15-2015_first_pass_filtered.vcf -filter 'QD > 2 && FS > 60 && SOR < 3 && MQ > 40 && MQRankSum > -3 && ReadPosRankSum > -4' -output variants/6753_12-15-2015_second_pass_filtered.vcf -filter-name ""default"" . However, it complains with a java.lang.NumberFormatException:. Using GATK jar gatk-4.0.5.1/gatk-package-4.0.5.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar gatk-4.0.5.1/gatk-package-4.0.5.1-local.jar VariantFiltration -R data/genome.fasta -V variants/6753_12-15-2015_first_pass_filtered.vcf -filter QD > 2 && FS > 60 && SOR < 3 && MQ > 40 && MQRankSum > -3 && ReadPosRankSum > -4 -output variants/6753_12-15-2015_second_pass_filtered.vcf -filter-name default; 15:42:33.964 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:gatk-4.0.5.1/gatk-package-4.0.5.1-local.jar!/com/intel/gkl/native/libgkl_compression.dylib; 15:42:34.114 INFO VariantFiltration - ------------------------------------------------------------; 15:42:34.115 INFO VariantFiltration - The Genome Analysis Toolkit (GATK) v4.0.5.1; 15:42:34.115 INFO VariantFiltration - For support and documentation go to https://software.broadinstitute.org/gatk/; 15:42:34.115 INFO VariantFiltration - Executing as sherlock@DN52ehae.SUNet on Mac OS X v10.13.5 x86_64; 15:42:34.116 INFO VariantFiltration - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_91-b14; 15:42:34.116 INFO VariantFiltration - Start Date/Time: June 15, 2018 3:42:33 PM PDT; 15:42:34.116 INFO VariantFiltration - ----------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4921
https://github.com/broadinstitute/gatk/issues/4921:1218,Performance,Load,Loading,1218,"I called variants using gatk4, and then tried to perform VariantFiltration:. gatk-4.0.5.1/gatk VariantFiltration -R data/genome.fasta -V variants/6753_12-15-2015_first_pass_filtered.vcf -filter 'QD > 2 && FS > 60 && SOR < 3 && MQ > 40 && MQRankSum > -3 && ReadPosRankSum > -4' -output variants/6753_12-15-2015_second_pass_filtered.vcf -filter-name ""default"" . However, it complains with a java.lang.NumberFormatException:. Using GATK jar gatk-4.0.5.1/gatk-package-4.0.5.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar gatk-4.0.5.1/gatk-package-4.0.5.1-local.jar VariantFiltration -R data/genome.fasta -V variants/6753_12-15-2015_first_pass_filtered.vcf -filter QD > 2 && FS > 60 && SOR < 3 && MQ > 40 && MQRankSum > -3 && ReadPosRankSum > -4 -output variants/6753_12-15-2015_second_pass_filtered.vcf -filter-name default; 15:42:33.964 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:gatk-4.0.5.1/gatk-package-4.0.5.1-local.jar!/com/intel/gkl/native/libgkl_compression.dylib; 15:42:34.114 INFO VariantFiltration - ------------------------------------------------------------; 15:42:34.115 INFO VariantFiltration - The Genome Analysis Toolkit (GATK) v4.0.5.1; 15:42:34.115 INFO VariantFiltration - For support and documentation go to https://software.broadinstitute.org/gatk/; 15:42:34.115 INFO VariantFiltration - Executing as sherlock@DN52ehae.SUNet on Mac OS X v10.13.5 x86_64; 15:42:34.116 INFO VariantFiltration - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_91-b14; 15:42:34.116 INFO VariantFiltration - Start Date/Time: June 15, 2018 3:42:33 PM PDT; 15:42:34.116 INFO VariantFiltration - ------------------------------------------------------------; 15:42:34.116 INFO VariantFiltration - ------------------------------------------------------------; 15:42:34.117 INFO VariantFiltration - HTSJDK Version: 2.15.1; 15:42:3",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4921
https://github.com/broadinstitute/gatk/issues/4921:7494,Performance,perform,perform,7494,"rencePipeline.java:175); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); 	at org.broadinstitute.hellbender.engine.VariantWalkerBase.traverse(VariantWalkerBase.java:107); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:994); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:135); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:180); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:199); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289). I don't really know how to fix it. ValidateVariants gives no errors, and I am able to perform variant selection, e.g.:. gatk-4.0.5.1/gatk SelectVariants -R data/genome.fasta -V variants/6753_12-15-2015_first_pass_raw.vcf -select 'vc.getGenotype(""6753_12-15-2015"").getAD().1/vc.getGenotype(""6753_12-15-2015"").getDP() > 0.9 ' -output variants/6753_12-15-2015_first_pass_filtered.vcf. with no problems. Insights would be gratefully appreciated.; Thanks!; Gavin. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/12223/java-lang-numberformatexception-when-trying-to-perform-variantfiltration/p1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4921
https://github.com/broadinstitute/gatk/issues/4921:8030,Performance,perform,perform-variantfiltration,8030,"rencePipeline.java:175); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); 	at org.broadinstitute.hellbender.engine.VariantWalkerBase.traverse(VariantWalkerBase.java:107); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:994); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:135); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:180); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:199); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289). I don't really know how to fix it. ValidateVariants gives no errors, and I am able to perform variant selection, e.g.:. gatk-4.0.5.1/gatk SelectVariants -R data/genome.fasta -V variants/6753_12-15-2015_first_pass_raw.vcf -select 'vc.getGenotype(""6753_12-15-2015"").getAD().1/vc.getGenotype(""6753_12-15-2015"").getDP() > 0.9 ' -output variants/6753_12-15-2015_first_pass_filtered.vcf. with no problems. Insights would be gratefully appreciated.; Thanks!; Gavin. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/12223/java-lang-numberformatexception-when-trying-to-perform-variantfiltration/p1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4921
https://github.com/broadinstitute/gatk/issues/4921:7443,Security,Validat,ValidateVariants,7443,"rencePipeline.java:175); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); 	at org.broadinstitute.hellbender.engine.VariantWalkerBase.traverse(VariantWalkerBase.java:107); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:994); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:135); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:180); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:199); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289). I don't really know how to fix it. ValidateVariants gives no errors, and I am able to perform variant selection, e.g.:. gatk-4.0.5.1/gatk SelectVariants -R data/genome.fasta -V variants/6753_12-15-2015_first_pass_raw.vcf -select 'vc.getGenotype(""6753_12-15-2015"").getAD().1/vc.getGenotype(""6753_12-15-2015"").getDP() > 0.9 ' -output variants/6753_12-15-2015_first_pass_filtered.vcf. with no problems. Insights would be gratefully appreciated.; Thanks!; Gavin. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/12223/java-lang-numberformatexception-when-trying-to-perform-variantfiltration/p1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4921
https://github.com/broadinstitute/gatk/issues/4922:196,Deployability,pipeline,pipeline,196,"## Feature request. ### Tool(s) or class(es) involved; _FindBreakpointEvidenceSpark_, _StructuralVariationDiscoveryPipelineSpark_, when using _XGBoostEvidenceFilter.java_. ### Description; The SV pipeline filters BreakpointEvidence based on BreakpointDensityFilter, or optionally XGBoostEvidenceFilter. The XGBoostEvidenceFilter uses a saved classifier model trained with Python code external to the GATK. This poses two main problems:; 1) The external Python code was designed for proof-of-principle and method development, not maintainability or ease of use. Additionally, GATK users and developers are assumed to be familiar with Java, not necessarily Python.; 2) The external Python code must share heterogeneous data with Java for unit/integration tests (supplying test BreakpointEvidence, expected classifier features, and expected classifier probabilities). Currently this is done via JSON files organized to (invertibly) store Pandas or Numpy objects. The resulting code to load these JSON files in on the Java side is complex.; These problems can be resolved by; 1) Replacing external python code by porting to an **experimental** tool in the GATK.; 2) Replacing JSON files with a serialization strategy currently supported by the GATK (e.g. Kryo). Additional benefits can be obtained by ensuring that the classifier-training subroutines are sufficiently general to speed development for other projects that may want to use boosted decision trees for classification.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4922
https://github.com/broadinstitute/gatk/issues/4922:741,Deployability,integrat,integration,741,"## Feature request. ### Tool(s) or class(es) involved; _FindBreakpointEvidenceSpark_, _StructuralVariationDiscoveryPipelineSpark_, when using _XGBoostEvidenceFilter.java_. ### Description; The SV pipeline filters BreakpointEvidence based on BreakpointDensityFilter, or optionally XGBoostEvidenceFilter. The XGBoostEvidenceFilter uses a saved classifier model trained with Python code external to the GATK. This poses two main problems:; 1) The external Python code was designed for proof-of-principle and method development, not maintainability or ease of use. Additionally, GATK users and developers are assumed to be familiar with Java, not necessarily Python.; 2) The external Python code must share heterogeneous data with Java for unit/integration tests (supplying test BreakpointEvidence, expected classifier features, and expected classifier probabilities). Currently this is done via JSON files organized to (invertibly) store Pandas or Numpy objects. The resulting code to load these JSON files in on the Java side is complex.; These problems can be resolved by; 1) Replacing external python code by porting to an **experimental** tool in the GATK.; 2) Replacing JSON files with a serialization strategy currently supported by the GATK (e.g. Kryo). Additional benefits can be obtained by ensuring that the classifier-training subroutines are sufficiently general to speed development for other projects that may want to use boosted decision trees for classification.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4922
