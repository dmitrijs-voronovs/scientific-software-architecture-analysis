id,quality_attribute,keyword,matched_word,match_idx,sentence,source,filename,author,repo,version,wiki,url
https://hail.is/docs/0.1/_modules/hail/dataset.html:154588,Modifiability,inherit,inheritance,154588,"r missing) key per variant,; rather than as a collection of keys. :param str agg_expr: Sample aggregation expression (per key). :param str test: Statistical test, one of: 'wald', 'lrt', 'score', or 'firth'. :param str y: Response expression. :param covariates: list of covariate expressions.; :type covariates: list of str. :return: Tuple of logistic regression key table and sample aggregation key table.; :rtype: (:py:class:`.KeyTable`, :py:class:`.KeyTable`); """""". r = self._jvdf.logregBurden(key_name, variant_keys, single_key, agg_expr, test, y, jarray(Env.jvm().java.lang.String, covariates)); logreg_kt = KeyTable(self.hc, r._1()); sample_kt = KeyTable(self.hc, r._2()). return logreg_kt, sample_kt. [docs] @handle_py4j; @requireTGenotype; @typecheck_method(pedigree=Pedigree); def mendel_errors(self, pedigree):; """"""Find Mendel errors; count per variant, individual and nuclear; family. .. include:: requireTGenotype.rst. **Examples**. Find all violations of Mendelian inheritance in each (dad,; mom, kid) trio in a pedigree and return four tables:. >>> ped = Pedigree.read('data/trios.fam'); >>> all, per_fam, per_sample, per_variant = vds.mendel_errors(ped); ; Export all mendel errors to a text file:; ; >>> all.export('output/all_mendel_errors.tsv'). Annotate samples with the number of Mendel errors:; ; >>> annotated_vds = vds.annotate_samples_table(per_sample, root=""sa.mendel""); ; Annotate variants with the number of Mendel errors:; ; >>> annotated_vds = vds.annotate_variants_table(per_variant, root=""va.mendel""); ; **Notes**; ; This method assumes all contigs apart from X and Y are fully autosomal;; mitochondria, decoys, etc. are not given special treatment. The example above returns four tables, which contain Mendelian violations grouped in; various ways. These tables are modeled after the ; `PLINK mendel formats <https://www.cog-genomics.org/plink2/formats#mendel>`_. The four; tables contain the following columns:; ; **First table:** all Mendel errors. This table contain",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:158077,Modifiability,extend,extending,158077,"r of Mendel errors found involving this individual at SNPs.; - **error** (*String*) -- Readable representation of Mendel error.; ; **Fourth table:** errors per variant. This table contains one row per variant in the dataset.; ; Columns:; ; - **v** (*Variant*) -- Variant (key column).; - **nErrors** (*Int*) -- Number of Mendel errors in this variant.; ; **PLINK Mendel error formats:**. - ``*.mendel`` -- all mendel errors: FID KID CHR SNP CODE ERROR; - ``*.fmendel`` -- error count per nuclear family: FID PAT MAT CHLD N; - ``*.imendel`` -- error count per individual: FID IID N; - ``*.lmendel`` -- error count per variant: CHR SNP N; ; In the PLINK formats, **FID**, **KID**, **PAT**, **MAT**, and **IID** refer to family, kid,; dad, mom, and individual ID, respectively, with missing values set to ``0``. SNP denotes ; the variant identifier ``chr:pos:ref:alt``. N is the error count. CHLD is the number of ; children in a nuclear family. The CODE of each Mendel error is determined by the table below,; extending the `Plink; classification <https://www.cog-genomics.org/plink2/basic_stats#mendel>`__. Those individuals implicated by each code are in bold. The copy state of a locus with respect to a trio is defined as follows,; where PAR is the `pseudoautosomal region <https://en.wikipedia.org/wiki/Pseudoautosomal_region>`__ (PAR). - HemiX -- in non-PAR of X, male child; - HemiY -- in non-PAR of Y, male child; - Auto -- otherwise (in autosome or PAR, or female child). Any refers to :math:`\{ HomRef, Het, HomVar, NoCall \}` and ! denotes complement in this set. +--------+------------+------------+----------+------------------+; |Code | Dad | Mom | Kid | Copy State |; +========+============+============+==========+==================+; | 1 | HomVar | HomVar | Het | Auto |; +--------+------------+------------+----------+------------------+; | 2 | HomRef | HomRef | Het | Auto |; +--------+------------+------------+----------+------------------+; | 3 | HomRef | ! HomRef | HomVar | Auto",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:170285,Modifiability,inherit,inherited,170285,"quency filter of 0.01:. >>> rel = vds.pc_relate(5, 0.01). Calculate values as above, but when performing distributed matrix; multiplications use a matrix-block-size of 1024 by 1024. >>> rel = vds.pc_relate(5, 0.01, 1024). Calculate values as above, excluding sample-pairs with kinship less; than 0.1. This is more efficient than producing the full key table and; filtering using :py:meth:`~hail.KeyTable.filter`. >>> rel = vds.pc_relate(5, 0.01, min_kinship=0.1). **Method**. The traditional estimator for kinship between a pair of individuals; :math:`i` and :math:`j`, sharing the set :math:`S_{ij}` of; single-nucleotide variants, from a population with allele frequencies; :math:`p_s`, is given by:. .. math::. \\widehat{\phi_{ij}} := \\frac{1}{|S_{ij}|}\\sum_{s \in S_{ij}}\\frac{(g_{is} - 2 p_s) (g_{js} - 2 p_s)}{4 * \sum_{s \in S_{ij} p_s (1 - p_s)}}. This estimator is true under the model that the sharing of common; (relative to the population) alleles is not very informative to; relatedness (because they're common) and the sharing of rare alleles; suggests a recent common ancestor from which the allele was inherited by; descent. When multiple ancestry groups are mixed in a sample, this model breaks; down. Alleles that are rare in all but one ancestry group are treated as; very informative to relatedness. However, these alleles are simply; markers of the ancestry group. The PC-Relate method corrects for this; situation and the related situation of admixed individuals. PC-Relate slightly modifies the usual estimator for relatedness:; occurences of population allele frequency are replaced with an; ""individual-specific allele frequency"". This modification allows the; method to correctly weight an allele according to an individual's unique; ancestry profile. The ""individual-specific allele frequency"" at a given genetic locus is; modeled by PC-Relate as a linear function of their first ``k`` principal; component coordinates. As such, the efficacy of this method rests on two; ",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:221158,Modifiability,config,config,221158,"|; +---------------------------+--------+--------------------------------------------------------+; | ``gqMean`` | Double | The average genotype quality across all samples |; +---------------------------+--------+--------------------------------------------------------+; | ``gqStDev`` | Double | Genotype quality standard deviation across all samples |; +---------------------------+--------+--------------------------------------------------------+. Missing values ``NA`` may result (for example, due to division by zero) and are handled properly ; in filtering and written as ""NA"" in export modules. The empirical standard deviation is computed; with zero degrees of freedom. :param str root: Variant annotation root for computed struct. :return: Annotated variant dataset with new variant QC annotations.; :rtype: :py:class:`.VariantDataset`; """""". jvds = self._jvdf.variantQC(root); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(config=strlike,; block_size=integral,; root=strlike,; csq=bool); def vep(self, config, block_size=1000, root='va.vep', csq=False):; """"""Annotate variants with VEP. :py:meth:`~hail.VariantDataset.vep` runs `Variant Effect Predictor <http://www.ensembl.org/info/docs/tools/vep/index.html>`__ with; the `LOFTEE plugin <https://github.com/konradjk/loftee>`__; on the current variant dataset and adds the result as a variant annotation. **Examples**. Add VEP annotations to the dataset:. >>> vds_result = vds.vep(""data/vep.properties"") # doctest: +SKIP. **Configuration**. :py:meth:`~hail.VariantDataset.vep` needs a configuration file to tell it how to run; VEP. The format is a `.properties file <https://en.wikipedia.org/wiki/.properties>`__.; Roughly, each line defines a property as a key-value pair of the form `key = value`. `vep` supports the following properties:. - **hail.vep.perl** -- Location of Perl. Optional, default: perl.; - **hail.vep.perl5lib** -- Value for the PERL5LIB environment variable when invoking VEP. Optional, by",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:221237,Modifiability,config,config,221237,"|; +---------------------------+--------+--------------------------------------------------------+; | ``gqMean`` | Double | The average genotype quality across all samples |; +---------------------------+--------+--------------------------------------------------------+; | ``gqStDev`` | Double | Genotype quality standard deviation across all samples |; +---------------------------+--------+--------------------------------------------------------+. Missing values ``NA`` may result (for example, due to division by zero) and are handled properly ; in filtering and written as ""NA"" in export modules. The empirical standard deviation is computed; with zero degrees of freedom. :param str root: Variant annotation root for computed struct. :return: Annotated variant dataset with new variant QC annotations.; :rtype: :py:class:`.VariantDataset`; """""". jvds = self._jvdf.variantQC(root); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(config=strlike,; block_size=integral,; root=strlike,; csq=bool); def vep(self, config, block_size=1000, root='va.vep', csq=False):; """"""Annotate variants with VEP. :py:meth:`~hail.VariantDataset.vep` runs `Variant Effect Predictor <http://www.ensembl.org/info/docs/tools/vep/index.html>`__ with; the `LOFTEE plugin <https://github.com/konradjk/loftee>`__; on the current variant dataset and adds the result as a variant annotation. **Examples**. Add VEP annotations to the dataset:. >>> vds_result = vds.vep(""data/vep.properties"") # doctest: +SKIP. **Configuration**. :py:meth:`~hail.VariantDataset.vep` needs a configuration file to tell it how to run; VEP. The format is a `.properties file <https://en.wikipedia.org/wiki/.properties>`__.; Roughly, each line defines a property as a key-value pair of the form `key = value`. `vep` supports the following properties:. - **hail.vep.perl** -- Location of Perl. Optional, default: perl.; - **hail.vep.perl5lib** -- Value for the PERL5LIB environment variable when invoking VEP. Optional, by",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:221465,Modifiability,plugin,plugin,221465,"+; | ``gqStDev`` | Double | Genotype quality standard deviation across all samples |; +---------------------------+--------+--------------------------------------------------------+. Missing values ``NA`` may result (for example, due to division by zero) and are handled properly ; in filtering and written as ""NA"" in export modules. The empirical standard deviation is computed; with zero degrees of freedom. :param str root: Variant annotation root for computed struct. :return: Annotated variant dataset with new variant QC annotations.; :rtype: :py:class:`.VariantDataset`; """""". jvds = self._jvdf.variantQC(root); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(config=strlike,; block_size=integral,; root=strlike,; csq=bool); def vep(self, config, block_size=1000, root='va.vep', csq=False):; """"""Annotate variants with VEP. :py:meth:`~hail.VariantDataset.vep` runs `Variant Effect Predictor <http://www.ensembl.org/info/docs/tools/vep/index.html>`__ with; the `LOFTEE plugin <https://github.com/konradjk/loftee>`__; on the current variant dataset and adds the result as a variant annotation. **Examples**. Add VEP annotations to the dataset:. >>> vds_result = vds.vep(""data/vep.properties"") # doctest: +SKIP. **Configuration**. :py:meth:`~hail.VariantDataset.vep` needs a configuration file to tell it how to run; VEP. The format is a `.properties file <https://en.wikipedia.org/wiki/.properties>`__.; Roughly, each line defines a property as a key-value pair of the form `key = value`. `vep` supports the following properties:. - **hail.vep.perl** -- Location of Perl. Optional, default: perl.; - **hail.vep.perl5lib** -- Value for the PERL5LIB environment variable when invoking VEP. Optional, by default PERL5LIB is not set.; - **hail.vep.path** -- Value of the PATH environment variable when invoking VEP. Optional, by default PATH is not set.; - **hail.vep.location** -- Location of the VEP Perl script. Required.; - **hail.vep.cache_dir** -- Location of the VE",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:221769,Modifiability,config,configuration,221769,"rt modules. The empirical standard deviation is computed; with zero degrees of freedom. :param str root: Variant annotation root for computed struct. :return: Annotated variant dataset with new variant QC annotations.; :rtype: :py:class:`.VariantDataset`; """""". jvds = self._jvdf.variantQC(root); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(config=strlike,; block_size=integral,; root=strlike,; csq=bool); def vep(self, config, block_size=1000, root='va.vep', csq=False):; """"""Annotate variants with VEP. :py:meth:`~hail.VariantDataset.vep` runs `Variant Effect Predictor <http://www.ensembl.org/info/docs/tools/vep/index.html>`__ with; the `LOFTEE plugin <https://github.com/konradjk/loftee>`__; on the current variant dataset and adds the result as a variant annotation. **Examples**. Add VEP annotations to the dataset:. >>> vds_result = vds.vep(""data/vep.properties"") # doctest: +SKIP. **Configuration**. :py:meth:`~hail.VariantDataset.vep` needs a configuration file to tell it how to run; VEP. The format is a `.properties file <https://en.wikipedia.org/wiki/.properties>`__.; Roughly, each line defines a property as a key-value pair of the form `key = value`. `vep` supports the following properties:. - **hail.vep.perl** -- Location of Perl. Optional, default: perl.; - **hail.vep.perl5lib** -- Value for the PERL5LIB environment variable when invoking VEP. Optional, by default PERL5LIB is not set.; - **hail.vep.path** -- Value of the PATH environment variable when invoking VEP. Optional, by default PATH is not set.; - **hail.vep.location** -- Location of the VEP Perl script. Required.; - **hail.vep.cache_dir** -- Location of the VEP cache dir, passed to VEP with the `--dir` option. Required.; - **hail.vep.fasta** -- Location of the FASTA file to use to look up the reference sequence, passed to VEP with the `--fasta` option. Required.; - **hail.vep.assembly** -- Genome assembly version to use. Optional, default: GRCh37; - **hail.vep.plugin** -- VEP ",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:222155,Modifiability,variab,variable,222155,"echeck_method(config=strlike,; block_size=integral,; root=strlike,; csq=bool); def vep(self, config, block_size=1000, root='va.vep', csq=False):; """"""Annotate variants with VEP. :py:meth:`~hail.VariantDataset.vep` runs `Variant Effect Predictor <http://www.ensembl.org/info/docs/tools/vep/index.html>`__ with; the `LOFTEE plugin <https://github.com/konradjk/loftee>`__; on the current variant dataset and adds the result as a variant annotation. **Examples**. Add VEP annotations to the dataset:. >>> vds_result = vds.vep(""data/vep.properties"") # doctest: +SKIP. **Configuration**. :py:meth:`~hail.VariantDataset.vep` needs a configuration file to tell it how to run; VEP. The format is a `.properties file <https://en.wikipedia.org/wiki/.properties>`__.; Roughly, each line defines a property as a key-value pair of the form `key = value`. `vep` supports the following properties:. - **hail.vep.perl** -- Location of Perl. Optional, default: perl.; - **hail.vep.perl5lib** -- Value for the PERL5LIB environment variable when invoking VEP. Optional, by default PERL5LIB is not set.; - **hail.vep.path** -- Value of the PATH environment variable when invoking VEP. Optional, by default PATH is not set.; - **hail.vep.location** -- Location of the VEP Perl script. Required.; - **hail.vep.cache_dir** -- Location of the VEP cache dir, passed to VEP with the `--dir` option. Required.; - **hail.vep.fasta** -- Location of the FASTA file to use to look up the reference sequence, passed to VEP with the `--fasta` option. Required.; - **hail.vep.assembly** -- Genome assembly version to use. Optional, default: GRCh37; - **hail.vep.plugin** -- VEP plugin, passed to VEP with the `--plugin` option. Optional. Overrides `hail.vep.lof.human_ancestor` and `hail.vep.lof.conservation_file`.; - **hail.vep.lof.human_ancestor** -- Location of the human ancestor file for the LOFTEE plugin. Ignored if `hail.vep.plugin` is set. Required otherwise.; - **hail.vep.lof.conservation_file** -- Location of the conservat",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:222279,Modifiability,variab,variable,222279,"ep', csq=False):; """"""Annotate variants with VEP. :py:meth:`~hail.VariantDataset.vep` runs `Variant Effect Predictor <http://www.ensembl.org/info/docs/tools/vep/index.html>`__ with; the `LOFTEE plugin <https://github.com/konradjk/loftee>`__; on the current variant dataset and adds the result as a variant annotation. **Examples**. Add VEP annotations to the dataset:. >>> vds_result = vds.vep(""data/vep.properties"") # doctest: +SKIP. **Configuration**. :py:meth:`~hail.VariantDataset.vep` needs a configuration file to tell it how to run; VEP. The format is a `.properties file <https://en.wikipedia.org/wiki/.properties>`__.; Roughly, each line defines a property as a key-value pair of the form `key = value`. `vep` supports the following properties:. - **hail.vep.perl** -- Location of Perl. Optional, default: perl.; - **hail.vep.perl5lib** -- Value for the PERL5LIB environment variable when invoking VEP. Optional, by default PERL5LIB is not set.; - **hail.vep.path** -- Value of the PATH environment variable when invoking VEP. Optional, by default PATH is not set.; - **hail.vep.location** -- Location of the VEP Perl script. Required.; - **hail.vep.cache_dir** -- Location of the VEP cache dir, passed to VEP with the `--dir` option. Required.; - **hail.vep.fasta** -- Location of the FASTA file to use to look up the reference sequence, passed to VEP with the `--fasta` option. Required.; - **hail.vep.assembly** -- Genome assembly version to use. Optional, default: GRCh37; - **hail.vep.plugin** -- VEP plugin, passed to VEP with the `--plugin` option. Optional. Overrides `hail.vep.lof.human_ancestor` and `hail.vep.lof.conservation_file`.; - **hail.vep.lof.human_ancestor** -- Location of the human ancestor file for the LOFTEE plugin. Ignored if `hail.vep.plugin` is set. Required otherwise.; - **hail.vep.lof.conservation_file** -- Location of the conservation file for the LOFTEE plugin. Ignored if `hail.vep.plugin` is set. Required otherwise. Here is an example `vep.properties` conf",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:222770,Modifiability,plugin,plugin,222770," to run; VEP. The format is a `.properties file <https://en.wikipedia.org/wiki/.properties>`__.; Roughly, each line defines a property as a key-value pair of the form `key = value`. `vep` supports the following properties:. - **hail.vep.perl** -- Location of Perl. Optional, default: perl.; - **hail.vep.perl5lib** -- Value for the PERL5LIB environment variable when invoking VEP. Optional, by default PERL5LIB is not set.; - **hail.vep.path** -- Value of the PATH environment variable when invoking VEP. Optional, by default PATH is not set.; - **hail.vep.location** -- Location of the VEP Perl script. Required.; - **hail.vep.cache_dir** -- Location of the VEP cache dir, passed to VEP with the `--dir` option. Required.; - **hail.vep.fasta** -- Location of the FASTA file to use to look up the reference sequence, passed to VEP with the `--fasta` option. Required.; - **hail.vep.assembly** -- Genome assembly version to use. Optional, default: GRCh37; - **hail.vep.plugin** -- VEP plugin, passed to VEP with the `--plugin` option. Optional. Overrides `hail.vep.lof.human_ancestor` and `hail.vep.lof.conservation_file`.; - **hail.vep.lof.human_ancestor** -- Location of the human ancestor file for the LOFTEE plugin. Ignored if `hail.vep.plugin` is set. Required otherwise.; - **hail.vep.lof.conservation_file** -- Location of the conservation file for the LOFTEE plugin. Ignored if `hail.vep.plugin` is set. Required otherwise. Here is an example `vep.properties` configuration file. .. code-block:: text. hail.vep.perl = /usr/bin/perl; hail.vep.path = /usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin; hail.vep.location = /path/to/vep/ensembl-tools-release-81/scripts/variant_effect_predictor/variant_effect_predictor.pl; hail.vep.cache_dir = /path/to/vep; hail.vep.lof.human_ancestor = /path/to/loftee_data/human_ancestor.fa.gz; hail.vep.lof.conservation_file = /path/to/loftee_data//phylocsf.sql. **VEP Invocation**. .. code-block:: text. <hail.vep.perl>; <hail.vep.location>; --format vcf; --json; ",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:222786,Modifiability,plugin,plugin,222786," to run; VEP. The format is a `.properties file <https://en.wikipedia.org/wiki/.properties>`__.; Roughly, each line defines a property as a key-value pair of the form `key = value`. `vep` supports the following properties:. - **hail.vep.perl** -- Location of Perl. Optional, default: perl.; - **hail.vep.perl5lib** -- Value for the PERL5LIB environment variable when invoking VEP. Optional, by default PERL5LIB is not set.; - **hail.vep.path** -- Value of the PATH environment variable when invoking VEP. Optional, by default PATH is not set.; - **hail.vep.location** -- Location of the VEP Perl script. Required.; - **hail.vep.cache_dir** -- Location of the VEP cache dir, passed to VEP with the `--dir` option. Required.; - **hail.vep.fasta** -- Location of the FASTA file to use to look up the reference sequence, passed to VEP with the `--fasta` option. Required.; - **hail.vep.assembly** -- Genome assembly version to use. Optional, default: GRCh37; - **hail.vep.plugin** -- VEP plugin, passed to VEP with the `--plugin` option. Optional. Overrides `hail.vep.lof.human_ancestor` and `hail.vep.lof.conservation_file`.; - **hail.vep.lof.human_ancestor** -- Location of the human ancestor file for the LOFTEE plugin. Ignored if `hail.vep.plugin` is set. Required otherwise.; - **hail.vep.lof.conservation_file** -- Location of the conservation file for the LOFTEE plugin. Ignored if `hail.vep.plugin` is set. Required otherwise. Here is an example `vep.properties` configuration file. .. code-block:: text. hail.vep.perl = /usr/bin/perl; hail.vep.path = /usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin; hail.vep.location = /path/to/vep/ensembl-tools-release-81/scripts/variant_effect_predictor/variant_effect_predictor.pl; hail.vep.cache_dir = /path/to/vep; hail.vep.lof.human_ancestor = /path/to/loftee_data/human_ancestor.fa.gz; hail.vep.lof.conservation_file = /path/to/loftee_data//phylocsf.sql. **VEP Invocation**. .. code-block:: text. <hail.vep.perl>; <hail.vep.location>; --format vcf; --json; ",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:222820,Modifiability,plugin,plugin,222820," to run; VEP. The format is a `.properties file <https://en.wikipedia.org/wiki/.properties>`__.; Roughly, each line defines a property as a key-value pair of the form `key = value`. `vep` supports the following properties:. - **hail.vep.perl** -- Location of Perl. Optional, default: perl.; - **hail.vep.perl5lib** -- Value for the PERL5LIB environment variable when invoking VEP. Optional, by default PERL5LIB is not set.; - **hail.vep.path** -- Value of the PATH environment variable when invoking VEP. Optional, by default PATH is not set.; - **hail.vep.location** -- Location of the VEP Perl script. Required.; - **hail.vep.cache_dir** -- Location of the VEP cache dir, passed to VEP with the `--dir` option. Required.; - **hail.vep.fasta** -- Location of the FASTA file to use to look up the reference sequence, passed to VEP with the `--fasta` option. Required.; - **hail.vep.assembly** -- Genome assembly version to use. Optional, default: GRCh37; - **hail.vep.plugin** -- VEP plugin, passed to VEP with the `--plugin` option. Optional. Overrides `hail.vep.lof.human_ancestor` and `hail.vep.lof.conservation_file`.; - **hail.vep.lof.human_ancestor** -- Location of the human ancestor file for the LOFTEE plugin. Ignored if `hail.vep.plugin` is set. Required otherwise.; - **hail.vep.lof.conservation_file** -- Location of the conservation file for the LOFTEE plugin. Ignored if `hail.vep.plugin` is set. Required otherwise. Here is an example `vep.properties` configuration file. .. code-block:: text. hail.vep.perl = /usr/bin/perl; hail.vep.path = /usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin; hail.vep.location = /path/to/vep/ensembl-tools-release-81/scripts/variant_effect_predictor/variant_effect_predictor.pl; hail.vep.cache_dir = /path/to/vep; hail.vep.lof.human_ancestor = /path/to/loftee_data/human_ancestor.fa.gz; hail.vep.lof.conservation_file = /path/to/loftee_data//phylocsf.sql. **VEP Invocation**. .. code-block:: text. <hail.vep.perl>; <hail.vep.location>; --format vcf; --json; ",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:223013,Modifiability,plugin,plugin,223013,"`. `vep` supports the following properties:. - **hail.vep.perl** -- Location of Perl. Optional, default: perl.; - **hail.vep.perl5lib** -- Value for the PERL5LIB environment variable when invoking VEP. Optional, by default PERL5LIB is not set.; - **hail.vep.path** -- Value of the PATH environment variable when invoking VEP. Optional, by default PATH is not set.; - **hail.vep.location** -- Location of the VEP Perl script. Required.; - **hail.vep.cache_dir** -- Location of the VEP cache dir, passed to VEP with the `--dir` option. Required.; - **hail.vep.fasta** -- Location of the FASTA file to use to look up the reference sequence, passed to VEP with the `--fasta` option. Required.; - **hail.vep.assembly** -- Genome assembly version to use. Optional, default: GRCh37; - **hail.vep.plugin** -- VEP plugin, passed to VEP with the `--plugin` option. Optional. Overrides `hail.vep.lof.human_ancestor` and `hail.vep.lof.conservation_file`.; - **hail.vep.lof.human_ancestor** -- Location of the human ancestor file for the LOFTEE plugin. Ignored if `hail.vep.plugin` is set. Required otherwise.; - **hail.vep.lof.conservation_file** -- Location of the conservation file for the LOFTEE plugin. Ignored if `hail.vep.plugin` is set. Required otherwise. Here is an example `vep.properties` configuration file. .. code-block:: text. hail.vep.perl = /usr/bin/perl; hail.vep.path = /usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin; hail.vep.location = /path/to/vep/ensembl-tools-release-81/scripts/variant_effect_predictor/variant_effect_predictor.pl; hail.vep.cache_dir = /path/to/vep; hail.vep.lof.human_ancestor = /path/to/loftee_data/human_ancestor.fa.gz; hail.vep.lof.conservation_file = /path/to/loftee_data//phylocsf.sql. **VEP Invocation**. .. code-block:: text. <hail.vep.perl>; <hail.vep.location>; --format vcf; --json; --everything; --allele_number; --no_stats; --cache --offline; --dir <hail.vep.cache_dir>; --fasta <hail.vep.fasta>; --minimal; --assembly <hail.vep.assembly>; --plugin LoF,human_",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:223042,Modifiability,plugin,plugin,223042,"Location of Perl. Optional, default: perl.; - **hail.vep.perl5lib** -- Value for the PERL5LIB environment variable when invoking VEP. Optional, by default PERL5LIB is not set.; - **hail.vep.path** -- Value of the PATH environment variable when invoking VEP. Optional, by default PATH is not set.; - **hail.vep.location** -- Location of the VEP Perl script. Required.; - **hail.vep.cache_dir** -- Location of the VEP cache dir, passed to VEP with the `--dir` option. Required.; - **hail.vep.fasta** -- Location of the FASTA file to use to look up the reference sequence, passed to VEP with the `--fasta` option. Required.; - **hail.vep.assembly** -- Genome assembly version to use. Optional, default: GRCh37; - **hail.vep.plugin** -- VEP plugin, passed to VEP with the `--plugin` option. Optional. Overrides `hail.vep.lof.human_ancestor` and `hail.vep.lof.conservation_file`.; - **hail.vep.lof.human_ancestor** -- Location of the human ancestor file for the LOFTEE plugin. Ignored if `hail.vep.plugin` is set. Required otherwise.; - **hail.vep.lof.conservation_file** -- Location of the conservation file for the LOFTEE plugin. Ignored if `hail.vep.plugin` is set. Required otherwise. Here is an example `vep.properties` configuration file. .. code-block:: text. hail.vep.perl = /usr/bin/perl; hail.vep.path = /usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin; hail.vep.location = /path/to/vep/ensembl-tools-release-81/scripts/variant_effect_predictor/variant_effect_predictor.pl; hail.vep.cache_dir = /path/to/vep; hail.vep.lof.human_ancestor = /path/to/loftee_data/human_ancestor.fa.gz; hail.vep.lof.conservation_file = /path/to/loftee_data//phylocsf.sql. **VEP Invocation**. .. code-block:: text. <hail.vep.perl>; <hail.vep.location>; --format vcf; --json; --everything; --allele_number; --no_stats; --cache --offline; --dir <hail.vep.cache_dir>; --fasta <hail.vep.fasta>; --minimal; --assembly <hail.vep.assembly>; --plugin LoF,human_ancestor_fa:$<hail.vep.lof.human_ancestor>,filter_position:0.05,min_i",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:223168,Modifiability,plugin,plugin,223168,"ERL5LIB environment variable when invoking VEP. Optional, by default PERL5LIB is not set.; - **hail.vep.path** -- Value of the PATH environment variable when invoking VEP. Optional, by default PATH is not set.; - **hail.vep.location** -- Location of the VEP Perl script. Required.; - **hail.vep.cache_dir** -- Location of the VEP cache dir, passed to VEP with the `--dir` option. Required.; - **hail.vep.fasta** -- Location of the FASTA file to use to look up the reference sequence, passed to VEP with the `--fasta` option. Required.; - **hail.vep.assembly** -- Genome assembly version to use. Optional, default: GRCh37; - **hail.vep.plugin** -- VEP plugin, passed to VEP with the `--plugin` option. Optional. Overrides `hail.vep.lof.human_ancestor` and `hail.vep.lof.conservation_file`.; - **hail.vep.lof.human_ancestor** -- Location of the human ancestor file for the LOFTEE plugin. Ignored if `hail.vep.plugin` is set. Required otherwise.; - **hail.vep.lof.conservation_file** -- Location of the conservation file for the LOFTEE plugin. Ignored if `hail.vep.plugin` is set. Required otherwise. Here is an example `vep.properties` configuration file. .. code-block:: text. hail.vep.perl = /usr/bin/perl; hail.vep.path = /usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin; hail.vep.location = /path/to/vep/ensembl-tools-release-81/scripts/variant_effect_predictor/variant_effect_predictor.pl; hail.vep.cache_dir = /path/to/vep; hail.vep.lof.human_ancestor = /path/to/loftee_data/human_ancestor.fa.gz; hail.vep.lof.conservation_file = /path/to/loftee_data//phylocsf.sql. **VEP Invocation**. .. code-block:: text. <hail.vep.perl>; <hail.vep.location>; --format vcf; --json; --everything; --allele_number; --no_stats; --cache --offline; --dir <hail.vep.cache_dir>; --fasta <hail.vep.fasta>; --minimal; --assembly <hail.vep.assembly>; --plugin LoF,human_ancestor_fa:$<hail.vep.lof.human_ancestor>,filter_position:0.05,min_intron_size:15,conservation_file:<hail.vep.lof.conservation_file>; -o STDOUT. **Annotat",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:223197,Modifiability,plugin,plugin,223197,"PERL5LIB is not set.; - **hail.vep.path** -- Value of the PATH environment variable when invoking VEP. Optional, by default PATH is not set.; - **hail.vep.location** -- Location of the VEP Perl script. Required.; - **hail.vep.cache_dir** -- Location of the VEP cache dir, passed to VEP with the `--dir` option. Required.; - **hail.vep.fasta** -- Location of the FASTA file to use to look up the reference sequence, passed to VEP with the `--fasta` option. Required.; - **hail.vep.assembly** -- Genome assembly version to use. Optional, default: GRCh37; - **hail.vep.plugin** -- VEP plugin, passed to VEP with the `--plugin` option. Optional. Overrides `hail.vep.lof.human_ancestor` and `hail.vep.lof.conservation_file`.; - **hail.vep.lof.human_ancestor** -- Location of the human ancestor file for the LOFTEE plugin. Ignored if `hail.vep.plugin` is set. Required otherwise.; - **hail.vep.lof.conservation_file** -- Location of the conservation file for the LOFTEE plugin. Ignored if `hail.vep.plugin` is set. Required otherwise. Here is an example `vep.properties` configuration file. .. code-block:: text. hail.vep.perl = /usr/bin/perl; hail.vep.path = /usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin; hail.vep.location = /path/to/vep/ensembl-tools-release-81/scripts/variant_effect_predictor/variant_effect_predictor.pl; hail.vep.cache_dir = /path/to/vep; hail.vep.lof.human_ancestor = /path/to/loftee_data/human_ancestor.fa.gz; hail.vep.lof.conservation_file = /path/to/loftee_data//phylocsf.sql. **VEP Invocation**. .. code-block:: text. <hail.vep.perl>; <hail.vep.location>; --format vcf; --json; --everything; --allele_number; --no_stats; --cache --offline; --dir <hail.vep.cache_dir>; --fasta <hail.vep.fasta>; --minimal; --assembly <hail.vep.assembly>; --plugin LoF,human_ancestor_fa:$<hail.vep.lof.human_ancestor>,filter_position:0.05,min_intron_size:15,conservation_file:<hail.vep.lof.conservation_file>; -o STDOUT. **Annotations**. Annotations with the following schema are placed in the locat",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:223269,Modifiability,config,configuration,223269,"onment variable when invoking VEP. Optional, by default PATH is not set.; - **hail.vep.location** -- Location of the VEP Perl script. Required.; - **hail.vep.cache_dir** -- Location of the VEP cache dir, passed to VEP with the `--dir` option. Required.; - **hail.vep.fasta** -- Location of the FASTA file to use to look up the reference sequence, passed to VEP with the `--fasta` option. Required.; - **hail.vep.assembly** -- Genome assembly version to use. Optional, default: GRCh37; - **hail.vep.plugin** -- VEP plugin, passed to VEP with the `--plugin` option. Optional. Overrides `hail.vep.lof.human_ancestor` and `hail.vep.lof.conservation_file`.; - **hail.vep.lof.human_ancestor** -- Location of the human ancestor file for the LOFTEE plugin. Ignored if `hail.vep.plugin` is set. Required otherwise.; - **hail.vep.lof.conservation_file** -- Location of the conservation file for the LOFTEE plugin. Ignored if `hail.vep.plugin` is set. Required otherwise. Here is an example `vep.properties` configuration file. .. code-block:: text. hail.vep.perl = /usr/bin/perl; hail.vep.path = /usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin; hail.vep.location = /path/to/vep/ensembl-tools-release-81/scripts/variant_effect_predictor/variant_effect_predictor.pl; hail.vep.cache_dir = /path/to/vep; hail.vep.lof.human_ancestor = /path/to/loftee_data/human_ancestor.fa.gz; hail.vep.lof.conservation_file = /path/to/loftee_data//phylocsf.sql. **VEP Invocation**. .. code-block:: text. <hail.vep.perl>; <hail.vep.location>; --format vcf; --json; --everything; --allele_number; --no_stats; --cache --offline; --dir <hail.vep.cache_dir>; --fasta <hail.vep.fasta>; --minimal; --assembly <hail.vep.assembly>; --plugin LoF,human_ancestor_fa:$<hail.vep.lof.human_ancestor>,filter_position:0.05,min_intron_size:15,conservation_file:<hail.vep.lof.conservation_file>; -o STDOUT. **Annotations**. Annotations with the following schema are placed in the location specified by ``root``.; The full resulting dataset schema can be",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:223964,Modifiability,plugin,plugin,223964,"he human ancestor file for the LOFTEE plugin. Ignored if `hail.vep.plugin` is set. Required otherwise.; - **hail.vep.lof.conservation_file** -- Location of the conservation file for the LOFTEE plugin. Ignored if `hail.vep.plugin` is set. Required otherwise. Here is an example `vep.properties` configuration file. .. code-block:: text. hail.vep.perl = /usr/bin/perl; hail.vep.path = /usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin; hail.vep.location = /path/to/vep/ensembl-tools-release-81/scripts/variant_effect_predictor/variant_effect_predictor.pl; hail.vep.cache_dir = /path/to/vep; hail.vep.lof.human_ancestor = /path/to/loftee_data/human_ancestor.fa.gz; hail.vep.lof.conservation_file = /path/to/loftee_data//phylocsf.sql. **VEP Invocation**. .. code-block:: text. <hail.vep.perl>; <hail.vep.location>; --format vcf; --json; --everything; --allele_number; --no_stats; --cache --offline; --dir <hail.vep.cache_dir>; --fasta <hail.vep.fasta>; --minimal; --assembly <hail.vep.assembly>; --plugin LoF,human_ancestor_fa:$<hail.vep.lof.human_ancestor>,filter_position:0.05,min_intron_size:15,conservation_file:<hail.vep.lof.conservation_file>; -o STDOUT. **Annotations**. Annotations with the following schema are placed in the location specified by ``root``.; The full resulting dataset schema can be queried with :py:attr:`~hail.VariantDataset.variant_schema`. .. code-block:: text. Struct{; assembly_name: String,; allele_string: String,; colocated_variants: Array[Struct{; aa_allele: String,; aa_maf: Double,; afr_allele: String,; afr_maf: Double,; allele_string: String,; amr_allele: String,; amr_maf: Double,; clin_sig: Array[String],; end: Int,; eas_allele: String,; eas_maf: Double,; ea_allele: String,,; ea_maf: Double,; eur_allele: String,; eur_maf: Double,; exac_adj_allele: String,; exac_adj_maf: Double,; exac_allele: String,; exac_afr_allele: String,; exac_afr_maf: Double,; exac_amr_allele: String,; exac_amr_maf: Double,; exac_eas_allele: String,; exac_eas_maf: Double,; exac_fin_allele: ",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:227096,Modifiability,config,config,227096,"_region_name: String,; start: Int,; strand: Int,; transcript_consequences: Array[Struct{; allele_num: Int,; amino_acids: String,; biotype: String,; canonical: Int,; ccds: String,; cdna_start: Int,; cdna_end: Int,; cds_end: Int,; cds_start: Int,; codons: String,; consequence_terms: Array[String],; distance: Int,; domains: Array[Struct{; db: String; name: String; }],; exon: String,; gene_id: String,; gene_pheno: Int,; gene_symbol: String,; gene_symbol_source: String,; hgnc_id: String,; hgvsc: String,; hgvsp: String,; hgvs_offset: Int,; impact: String,; intron: String,; lof: String,; lof_flags: String,; lof_filter: String,; lof_info: String,; minimised: Int,; polyphen_prediction: String,; polyphen_score: Double,; protein_end: Int,; protein_start: Int,; protein_id: String,; sift_prediction: String,; sift_score: Double,; strand: Int,; swissprot: String,; transcript_id: String,; trembl: String,; uniparc: String,; variant_allele: String; }],; variant_class: String; }. :param str config: Path to VEP configuration file. :param block_size: Number of variants to annotate per VEP invocation.; :type block_size: int. :param str root: Variant annotation path to store VEP output. :param bool csq: If ``True``, annotates VCF CSQ field as a String.; If ``False``, annotates with the full nested struct schema. :return: An annotated with variant annotations from VEP.; :rtype: :py:class:`.VariantDataset`; """""". jvds = self._jvds.vep(config, root, csq, block_size); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; def variants_table(self):; """"""Convert variants and variant annotations to a KeyTable. The resulting KeyTable has schema:. .. code-block:: text. Struct {; v: Variant; va: variant annotations; }. with a single key ``v``. :return: Key table with variants and variant annotations.; :rtype: :class:`.KeyTable`; """""". return KeyTable(self.hc, self._jvds.variantsKT()). [docs] @handle_py4j; def samples_table(self):; """"""Convert samples and sample annotations to KeyTable. The resulting",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:227116,Modifiability,config,configuration,227116,"_region_name: String,; start: Int,; strand: Int,; transcript_consequences: Array[Struct{; allele_num: Int,; amino_acids: String,; biotype: String,; canonical: Int,; ccds: String,; cdna_start: Int,; cdna_end: Int,; cds_end: Int,; cds_start: Int,; codons: String,; consequence_terms: Array[String],; distance: Int,; domains: Array[Struct{; db: String; name: String; }],; exon: String,; gene_id: String,; gene_pheno: Int,; gene_symbol: String,; gene_symbol_source: String,; hgnc_id: String,; hgvsc: String,; hgvsp: String,; hgvs_offset: Int,; impact: String,; intron: String,; lof: String,; lof_flags: String,; lof_filter: String,; lof_info: String,; minimised: Int,; polyphen_prediction: String,; polyphen_score: Double,; protein_end: Int,; protein_start: Int,; protein_id: String,; sift_prediction: String,; sift_score: Double,; strand: Int,; swissprot: String,; transcript_id: String,; trembl: String,; uniparc: String,; variant_allele: String; }],; variant_class: String; }. :param str config: Path to VEP configuration file. :param block_size: Number of variants to annotate per VEP invocation.; :type block_size: int. :param str root: Variant annotation path to store VEP output. :param bool csq: If ``True``, annotates VCF CSQ field as a String.; If ``False``, annotates with the full nested struct schema. :return: An annotated with variant annotations from VEP.; :rtype: :py:class:`.VariantDataset`; """""". jvds = self._jvds.vep(config, root, csq, block_size); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; def variants_table(self):; """"""Convert variants and variant annotations to a KeyTable. The resulting KeyTable has schema:. .. code-block:: text. Struct {; v: Variant; va: variant annotations; }. with a single key ``v``. :return: Key table with variants and variant annotations.; :rtype: :class:`.KeyTable`; """""". return KeyTable(self.hc, self._jvds.variantsKT()). [docs] @handle_py4j; def samples_table(self):; """"""Convert samples and sample annotations to KeyTable. The resulting",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:227542,Modifiability,config,config,227542,": String,; hgnc_id: String,; hgvsc: String,; hgvsp: String,; hgvs_offset: Int,; impact: String,; intron: String,; lof: String,; lof_flags: String,; lof_filter: String,; lof_info: String,; minimised: Int,; polyphen_prediction: String,; polyphen_score: Double,; protein_end: Int,; protein_start: Int,; protein_id: String,; sift_prediction: String,; sift_score: Double,; strand: Int,; swissprot: String,; transcript_id: String,; trembl: String,; uniparc: String,; variant_allele: String; }],; variant_class: String; }. :param str config: Path to VEP configuration file. :param block_size: Number of variants to annotate per VEP invocation.; :type block_size: int. :param str root: Variant annotation path to store VEP output. :param bool csq: If ``True``, annotates VCF CSQ field as a String.; If ``False``, annotates with the full nested struct schema. :return: An annotated with variant annotations from VEP.; :rtype: :py:class:`.VariantDataset`; """""". jvds = self._jvds.vep(config, root, csq, block_size); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; def variants_table(self):; """"""Convert variants and variant annotations to a KeyTable. The resulting KeyTable has schema:. .. code-block:: text. Struct {; v: Variant; va: variant annotations; }. with a single key ``v``. :return: Key table with variants and variant annotations.; :rtype: :class:`.KeyTable`; """""". return KeyTable(self.hc, self._jvds.variantsKT()). [docs] @handle_py4j; def samples_table(self):; """"""Convert samples and sample annotations to KeyTable. The resulting KeyTable has schema:. .. code-block:: text. Struct {; s: Sample; sa: sample annotations; }. with a single key ``s``. :return: Key table with samples and sample annotations.; :rtype: :class:`.KeyTable`; """""". return KeyTable(self.hc, self._jvds.samplesKT()). [docs] @handle_py4j; def genotypes_table(self):; """"""Generate a fully expanded genotype table. **Examples**. >>> gs = vds.genotypes_table(). **Notes**. This produces a (massive) flat table from all the; ",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:2004,Performance,load,load,2004,"c, vds._jvdf.toVDS()); return func(coerced_vds, *args, **kwargs); else:; raise TypeError(""genotype signature must be Genotype, but found '%s'"" % type(vds.genotype_schema)). return func(vds, *args, **kwargs). @decorator; def convertVDS(func, vds, *args, **kwargs):; if vds._is_generic_genotype:; if isinstance(vds.genotype_schema, TGenotype):; vds = VariantDataset(vds.hc, vds._jvdf.toVDS()). return func(vds, *args, **kwargs). vds_type = lazy(). [docs]class VariantDataset(object):; """"""Hail's primary representation of genomic data, a matrix keyed by sample and variant. Variant datasets may be generated from other formats using the :py:class:`.HailContext` import methods,; constructed from a variant-keyed :py:class:`KeyTable` using :py:meth:`.VariantDataset.from_table`,; and simulated using :py:meth:`~hail.HailContext.balding_nichols_model`. Once a variant dataset has been written to disk with :py:meth:`~hail.VariantDataset.write`,; use :py:meth:`~hail.HailContext.read` to load the variant dataset into the environment. >>> vds = hc.read(""data/example.vds""). :ivar hc: Hail Context.; :vartype hc: :class:`.HailContext`; """""". def __init__(self, hc, jvds):; self.hc = hc; self._jvds = jvds. self._globals = None; self._sample_annotations = None; self._colkey_schema = None; self._sa_schema = None; self._rowkey_schema = None; self._va_schema = None; self._global_schema = None; self._genotype_schema = None; self._sample_ids = None; self._num_samples = None; self._jvdf_cache = None. [docs] @staticmethod; @handle_py4j; @typecheck(table=KeyTable); def from_table(table):; """"""Construct a sites-only variant dataset from a key table. **Examples**. Import a text table and construct a sites-only VDS:. >>> table = hc.import_table('data/variant-lof.tsv', types={'v': TVariant()}).key_by('v'); >>> sites_vds = VariantDataset.from_table(table). **Notes**. The key table must be keyed by one column of type :py:class:`.TVariant`. All columns in the key table become variant annotations in the result.;",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:10603,Performance,perform,performance,10603," specified by the left-hand side (must; begin with ``g``). This is analogous to :py:meth:`~hail.VariantDataset.annotate_variants_expr` and; :py:meth:`~hail.VariantDataset.annotate_samples_expr` where the annotation paths are ``va`` and ``sa`` respectively. ``expr`` is in genotype context so the following symbols are in scope:. - ``g``: genotype annotation; - ``v`` (*Variant*): :ref:`variant`; - ``va``: variant annotations; - ``s`` (*Sample*): sample; - ``sa``: sample annotations; - ``global``: global annotations. For more information, see the documentation on writing `expressions <overview.html#expressions>`__; and using the `Hail Expression Language <exprlang.html>`__. .. warning::. - If the resulting genotype schema is not :py:class:`~hail.expr.TGenotype`,; subsequent function calls on the annotated variant dataset may not work such as; :py:meth:`~hail.VariantDataset.pca` and :py:meth:`~hail.VariantDataset.linreg`. - Hail performance may be significantly slower if the annotated variant dataset does not have a; genotype schema equal to :py:class:`~hail.expr.TGenotype`. - Genotypes are immutable. For example, if ``g`` is initially of type ``Genotype``, the expression; ``g.gt = g.gt + 1`` will return a ``Struct`` with one field ``gt`` of type ``Int`` and **NOT** a ``Genotype``; with the ``gt`` incremented by 1. :param expr: Annotation expression.; :type expr: str or list of str. :return: Annotated variant dataset.; :rtype: :py:class:`.VariantDataset`; """""". if isinstance(expr, list):; expr = "","".join(expr). jvds = self._jvdf.annotateGenotypesExpr(expr); vds = VariantDataset(self.hc, jvds); if isinstance(vds.genotype_schema, TGenotype):; return VariantDataset(self.hc, vds._jvdf.toVDS()); else:; return vds. [docs] @handle_py4j; @typecheck_method(expr=oneof(strlike, listof(strlike))); def annotate_global_expr(self, expr):; """"""Annotate global with expression. **Example**. Annotate global with an array of populations:. >>> vds = vds.annotate_global_expr('global.pops = [""FI",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:14718,Performance,perform,performance,14718,":. >>> vds_result = (vds.annotate_samples_expr('sa.gqHetStats = gs.filter(g => g.isHet()).map(g => g.gq).stats()'); ... .export_samples('output/samples.txt', 'sample = s, het_gq_mean = sa.gqHetStats.mean')). Compute the list of genes with a singleton LOF per sample:. >>> variant_annotations_table = hc.import_table('data/consequence.tsv', impute=True).key_by('Variant'); >>> vds_result = (vds.annotate_variants_table(variant_annotations_table, root='va.consequence'); ... .annotate_variants_expr('va.isSingleton = gs.map(g => g.nNonRefAlleles()).sum() == 1'); ... .annotate_samples_expr('sa.LOF_genes = gs.filter(g => va.isSingleton && g.isHet() && va.consequence == ""LOF"").map(g => va.gene).collect()')). To create an annotation for only a subset of samples based on an existing annotation:. >>> vds_result = vds.annotate_samples_expr('sa.newpheno = if (sa.pheno.cohortName == ""cohort1"") sa.pheno.bloodPressure else NA: Double'). .. note::. For optimal performance, be sure to explicitly give the alternative (``NA``) the same type as the consequent (``sa.pheno.bloodPressure``). **Notes**. ``expr`` is in sample context so the following symbols are in scope:. - ``s`` (*Sample*): sample; - ``sa``: sample annotations; - ``global``: global annotations; - ``gs`` (*Aggregable[Genotype]*): aggregable of :ref:`genotype` for sample ``s``. :param expr: Annotation expression.; :type expr: str or list of str. :return: Annotated variant dataset.; :rtype: :class:`.VariantDataset`; """""". if isinstance(expr, list):; expr = ','.join(expr). jvds = self._jvds.annotateSamplesExpr(expr); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(table=KeyTable,; root=nullable(strlike),; expr=nullable(strlike),; vds_key=nullable(strlike),; product=bool); def annotate_samples_table(self, table, root=None, expr=None, vds_key=None, product=False):; """"""Annotate samples with a key table. **Examples**. To annotates samples using `samples1.tsv` with type imputation::. >>> table = hc.import_t",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:35207,Performance,load,load,35207,"d VEP annotations to your VDS, make sure to add the initialization action ; :code:`gs://hail-common/vep/vep/vep85-init.sh` when starting your cluster. :param annotations: List of annotations to import from the database.; :type annotations: str or list of str . :param gene_key: Existing variant annotation used to map variants to gene symbols if importing gene-level ; annotations. If not provided, the method will add VEP annotations and parse them as described in the ; database documentation to obtain one gene symbol per variant.; :type gene_key: str. :return: Annotated variant dataset.; :rtype: :py:class:`.VariantDataset`; """""". # import modules needed by this function; import sqlite3. # collect user-supplied annotations, converting str -> list if necessary and dropping duplicates; annotations = list(set(wrap_to_list(annotations))). # open connection to in-memory SQLite database; conn = sqlite3.connect(':memory:'). # load database with annotation metadata, print error if not on Google Cloud Platform; try:; f = hadoop_read('gs://annotationdb/ADMIN/annotationdb.sql'); except FatalError:; raise EnvironmentError('Cannot read from Google Storage. Must be running on Google Cloud Platform to use annotation database.'); else:; curs = conn.executescript(f.read()); f.close(). # parameter substitution string to put in SQL query; like = ' OR '.join('a.annotation LIKE ?' for i in xrange(2*len(annotations))). # query to extract path of all needed database files and their respective annotation exprs ; qry = """"""SELECT file_path, annotation, file_type, file_element, f.file_id; FROM files AS f INNER JOIN annotations AS a ON f.file_id = a.file_id; WHERE {}"""""".format(like). # run query and collect results in a file_path: expr dictionary; results = curs.execute(qry, [x + '.%' for x in annotations] + annotations).fetchall(). # all file_ids to be used; file_ids = list(set([x[4] for x in results])). # parameter substitution string; sub = ','.join('?' for x in file_ids). # query to fetch coun",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:40712,Performance,cache,cache,40712," = drop(va, vep)'). # subset VEP annotations if needed; subset = ','.join([x.rsplit('.')[-1] for x in annotations if x.startswith('va.vep.')]); if subset:; self = self.annotate_variants_expr('va.vep = select(va.vep, {})'.format(subset)). # iterate through files, selected annotations from each file; for db_file, expr in file_exprs.iteritems():. # if database file is a VDS; if db_file.endswith('.vds'):. # annotate analysis VDS with database VDS; self = self.annotate_variants_vds(self.hc.read(db_file), expr=expr). # if database file is a keytable; elif db_file.endswith('.kt'):. # join on gene symbol for gene annotations; if db_file == 'gs://annotationdb/gene/gene.kt':; if gene_key:; vds_key = gene_key; else:; vds_key = 'va.gene.transcript.gene_symbol'; else:; vds_key = None. # annotate analysis VDS with database keytable; self = self.annotate_variants_table(self.hc.read_table(db_file), expr=expr, vds_key=vds_key). else:; continue. return self. [docs] @handle_py4j; def cache(self):; """"""Mark this variant dataset to be cached in memory. :py:meth:`~hail.VariantDataset.cache` is the same as :func:`persist(""MEMORY_ONLY"") <hail.VariantDataset.persist>`.; ; :rtype: :class:`.VariantDataset`; """""". return VariantDataset(self.hc, self._jvdf.cache()). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(right=vds_type); def concordance(self, right):; """"""Calculate call concordance with another variant dataset. .. include:: requireTGenotype.rst. **Example**; ; >>> comparison_vds = hc.read('data/example2.vds'); >>> summary, samples, variants = vds.concordance(comparison_vds). **Notes**. This method computes the genotype call concordance between two bialellic variant datasets. ; It performs an inner join on samples (only samples in both datasets will be considered), and an outer join; on variants. If a variant is only in one dataset, then each genotype is treated as ""no data"" in the other.; This method returns a tuple of three objects: a nested list of list of int with global conc",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:40761,Performance,cache,cached,40761," = drop(va, vep)'). # subset VEP annotations if needed; subset = ','.join([x.rsplit('.')[-1] for x in annotations if x.startswith('va.vep.')]); if subset:; self = self.annotate_variants_expr('va.vep = select(va.vep, {})'.format(subset)). # iterate through files, selected annotations from each file; for db_file, expr in file_exprs.iteritems():. # if database file is a VDS; if db_file.endswith('.vds'):. # annotate analysis VDS with database VDS; self = self.annotate_variants_vds(self.hc.read(db_file), expr=expr). # if database file is a keytable; elif db_file.endswith('.kt'):. # join on gene symbol for gene annotations; if db_file == 'gs://annotationdb/gene/gene.kt':; if gene_key:; vds_key = gene_key; else:; vds_key = 'va.gene.transcript.gene_symbol'; else:; vds_key = None. # annotate analysis VDS with database keytable; self = self.annotate_variants_table(self.hc.read_table(db_file), expr=expr, vds_key=vds_key). else:; continue. return self. [docs] @handle_py4j; def cache(self):; """"""Mark this variant dataset to be cached in memory. :py:meth:`~hail.VariantDataset.cache` is the same as :func:`persist(""MEMORY_ONLY"") <hail.VariantDataset.persist>`.; ; :rtype: :class:`.VariantDataset`; """""". return VariantDataset(self.hc, self._jvdf.cache()). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(right=vds_type); def concordance(self, right):; """"""Calculate call concordance with another variant dataset. .. include:: requireTGenotype.rst. **Example**; ; >>> comparison_vds = hc.read('data/example2.vds'); >>> summary, samples, variants = vds.concordance(comparison_vds). **Notes**. This method computes the genotype call concordance between two bialellic variant datasets. ; It performs an inner join on samples (only samples in both datasets will be considered), and an outer join; on variants. If a variant is only in one dataset, then each genotype is treated as ""no data"" in the other.; This method returns a tuple of three objects: a nested list of list of int with global conc",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:40810,Performance,cache,cache,40810,"ations if x.startswith('va.vep.')]); if subset:; self = self.annotate_variants_expr('va.vep = select(va.vep, {})'.format(subset)). # iterate through files, selected annotations from each file; for db_file, expr in file_exprs.iteritems():. # if database file is a VDS; if db_file.endswith('.vds'):. # annotate analysis VDS with database VDS; self = self.annotate_variants_vds(self.hc.read(db_file), expr=expr). # if database file is a keytable; elif db_file.endswith('.kt'):. # join on gene symbol for gene annotations; if db_file == 'gs://annotationdb/gene/gene.kt':; if gene_key:; vds_key = gene_key; else:; vds_key = 'va.gene.transcript.gene_symbol'; else:; vds_key = None. # annotate analysis VDS with database keytable; self = self.annotate_variants_table(self.hc.read_table(db_file), expr=expr, vds_key=vds_key). else:; continue. return self. [docs] @handle_py4j; def cache(self):; """"""Mark this variant dataset to be cached in memory. :py:meth:`~hail.VariantDataset.cache` is the same as :func:`persist(""MEMORY_ONLY"") <hail.VariantDataset.persist>`.; ; :rtype: :class:`.VariantDataset`; """""". return VariantDataset(self.hc, self._jvdf.cache()). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(right=vds_type); def concordance(self, right):; """"""Calculate call concordance with another variant dataset. .. include:: requireTGenotype.rst. **Example**; ; >>> comparison_vds = hc.read('data/example2.vds'); >>> summary, samples, variants = vds.concordance(comparison_vds). **Notes**. This method computes the genotype call concordance between two bialellic variant datasets. ; It performs an inner join on samples (only samples in both datasets will be considered), and an outer join; on variants. If a variant is only in one dataset, then each genotype is treated as ""no data"" in the other.; This method returns a tuple of three objects: a nested list of list of int with global concordance; summary statistics, a key table with sample concordance statistics, and a key table with variant c",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:40978,Performance,cache,cache,40978,"rough files, selected annotations from each file; for db_file, expr in file_exprs.iteritems():. # if database file is a VDS; if db_file.endswith('.vds'):. # annotate analysis VDS with database VDS; self = self.annotate_variants_vds(self.hc.read(db_file), expr=expr). # if database file is a keytable; elif db_file.endswith('.kt'):. # join on gene symbol for gene annotations; if db_file == 'gs://annotationdb/gene/gene.kt':; if gene_key:; vds_key = gene_key; else:; vds_key = 'va.gene.transcript.gene_symbol'; else:; vds_key = None. # annotate analysis VDS with database keytable; self = self.annotate_variants_table(self.hc.read_table(db_file), expr=expr, vds_key=vds_key). else:; continue. return self. [docs] @handle_py4j; def cache(self):; """"""Mark this variant dataset to be cached in memory. :py:meth:`~hail.VariantDataset.cache` is the same as :func:`persist(""MEMORY_ONLY"") <hail.VariantDataset.persist>`.; ; :rtype: :class:`.VariantDataset`; """""". return VariantDataset(self.hc, self._jvdf.cache()). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(right=vds_type); def concordance(self, right):; """"""Calculate call concordance with another variant dataset. .. include:: requireTGenotype.rst. **Example**; ; >>> comparison_vds = hc.read('data/example2.vds'); >>> summary, samples, variants = vds.concordance(comparison_vds). **Notes**. This method computes the genotype call concordance between two bialellic variant datasets. ; It performs an inner join on samples (only samples in both datasets will be considered), and an outer join; on variants. If a variant is only in one dataset, then each genotype is treated as ""no data"" in the other.; This method returns a tuple of three objects: a nested list of list of int with global concordance; summary statistics, a key table with sample concordance statistics, and a key table with variant concordance ; statistics.; ; **Using the global summary result**; ; The global summary is a list of list of int (conceptually a 5 by 5 matrix), ;",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:41428,Performance,perform,performs,41428,"symbol'; else:; vds_key = None. # annotate analysis VDS with database keytable; self = self.annotate_variants_table(self.hc.read_table(db_file), expr=expr, vds_key=vds_key). else:; continue. return self. [docs] @handle_py4j; def cache(self):; """"""Mark this variant dataset to be cached in memory. :py:meth:`~hail.VariantDataset.cache` is the same as :func:`persist(""MEMORY_ONLY"") <hail.VariantDataset.persist>`.; ; :rtype: :class:`.VariantDataset`; """""". return VariantDataset(self.hc, self._jvdf.cache()). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(right=vds_type); def concordance(self, right):; """"""Calculate call concordance with another variant dataset. .. include:: requireTGenotype.rst. **Example**; ; >>> comparison_vds = hc.read('data/example2.vds'); >>> summary, samples, variants = vds.concordance(comparison_vds). **Notes**. This method computes the genotype call concordance between two bialellic variant datasets. ; It performs an inner join on samples (only samples in both datasets will be considered), and an outer join; on variants. If a variant is only in one dataset, then each genotype is treated as ""no data"" in the other.; This method returns a tuple of three objects: a nested list of list of int with global concordance; summary statistics, a key table with sample concordance statistics, and a key table with variant concordance ; statistics.; ; **Using the global summary result**; ; The global summary is a list of list of int (conceptually a 5 by 5 matrix), ; where the indices have special meaning:. 0. No Data (missing variant); 1. No Call (missing genotype call); 2. Hom Ref; 3. Heterozygous; 4. Hom Var; ; The first index is the state in the left dataset (the one on which concordance was called), and the second; index is the state in the right dataset (the argument to the concordance method call). Typical uses of ; the summary list are shown below.; ; >>> summary, samples, variants = vds.concordance(hc.read('data/example2.vds')); >>> left_homref_righ",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:74346,Performance,perform,performs,74346,"rval.parse('17:38449840-38530994')); ; Another way of writing this same query:; ; >>> vds_result = vds.filter_intervals(Interval(Locus('17', 38449840), Locus('17', 38530994))); ; Two identical ways of parsing a list of intervals:; ; >>> intervals = map(Interval.parse, ['1:50M-75M', '2:START-400000', '3-22']); >>> intervals = [Interval.parse(x) for x in ['1:50M-75M', '2:START-400000', '3-22']]; ; Use this interval list to filter:; ; >>> vds_result = vds.filter_intervals(intervals); ; **Notes**; ; This method takes an argument of :class:`.Interval` or list of :class:`.Interval`. Based on the ``keep`` argument, this method will either restrict to variants in the; supplied interval ranges, or remove all variants in those ranges. Note that intervals; are left-inclusive, and right-exclusive. The below interval includes the locus; ``15:100000`` but not ``15:101000``. >>> interval = Interval.parse('15:100000-101000'). This method performs predicate pushdown when ``keep=True``, meaning that data shards; that don't overlap any supplied interval will not be loaded at all. This property; enables ``filter_intervals`` to be used for reasonably low-latency queries of small ranges; of the genome, even on large datasets. Suppose we are interested in variants on ; chromosome 15 between 100000 and 200000. This implementation with :py:meth:`.filter_variants_expr`; may come to mind first:; ; >>> vds_filtered = vds.filter_variants_expr('v.contig == ""15"" && v.start >= 100000 && v.start < 200000'); ; However, it is **much** faster (and easier!) to use this method:; ; >>> vds_filtered = vds.filter_intervals(Interval.parse('15:100000-200000')). .. note::. A :py:class:`.KeyTable` keyed by interval can be used to filter a dataset efficiently as well.; See the documentation for :py:meth:`.filter_variants_table` for an example. This is useful for; using interval files to filter a dataset. :param intervals: Interval(s) to keep or remove.; :type intervals: :class:`.Interval` or list of :class:`.In",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:74473,Performance,load,loaded,74473,"rval.parse('17:38449840-38530994')); ; Another way of writing this same query:; ; >>> vds_result = vds.filter_intervals(Interval(Locus('17', 38449840), Locus('17', 38530994))); ; Two identical ways of parsing a list of intervals:; ; >>> intervals = map(Interval.parse, ['1:50M-75M', '2:START-400000', '3-22']); >>> intervals = [Interval.parse(x) for x in ['1:50M-75M', '2:START-400000', '3-22']]; ; Use this interval list to filter:; ; >>> vds_result = vds.filter_intervals(intervals); ; **Notes**; ; This method takes an argument of :class:`.Interval` or list of :class:`.Interval`. Based on the ``keep`` argument, this method will either restrict to variants in the; supplied interval ranges, or remove all variants in those ranges. Note that intervals; are left-inclusive, and right-exclusive. The below interval includes the locus; ``15:100000`` but not ``15:101000``. >>> interval = Interval.parse('15:100000-101000'). This method performs predicate pushdown when ``keep=True``, meaning that data shards; that don't overlap any supplied interval will not be loaded at all. This property; enables ``filter_intervals`` to be used for reasonably low-latency queries of small ranges; of the genome, even on large datasets. Suppose we are interested in variants on ; chromosome 15 between 100000 and 200000. This implementation with :py:meth:`.filter_variants_expr`; may come to mind first:; ; >>> vds_filtered = vds.filter_variants_expr('v.contig == ""15"" && v.start >= 100000 && v.start < 200000'); ; However, it is **much** faster (and easier!) to use this method:; ; >>> vds_filtered = vds.filter_intervals(Interval.parse('15:100000-200000')). .. note::. A :py:class:`.KeyTable` keyed by interval can be used to filter a dataset efficiently as well.; See the documentation for :py:meth:`.filter_variants_table` for an example. This is useful for; using interval files to filter a dataset. :param intervals: Interval(s) to keep or remove.; :type intervals: :class:`.Interval` or list of :class:`.In",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:74562,Performance,latency,latency,74562,", Locus('17', 38530994))); ; Two identical ways of parsing a list of intervals:; ; >>> intervals = map(Interval.parse, ['1:50M-75M', '2:START-400000', '3-22']); >>> intervals = [Interval.parse(x) for x in ['1:50M-75M', '2:START-400000', '3-22']]; ; Use this interval list to filter:; ; >>> vds_result = vds.filter_intervals(intervals); ; **Notes**; ; This method takes an argument of :class:`.Interval` or list of :class:`.Interval`. Based on the ``keep`` argument, this method will either restrict to variants in the; supplied interval ranges, or remove all variants in those ranges. Note that intervals; are left-inclusive, and right-exclusive. The below interval includes the locus; ``15:100000`` but not ``15:101000``. >>> interval = Interval.parse('15:100000-101000'). This method performs predicate pushdown when ``keep=True``, meaning that data shards; that don't overlap any supplied interval will not be loaded at all. This property; enables ``filter_intervals`` to be used for reasonably low-latency queries of small ranges; of the genome, even on large datasets. Suppose we are interested in variants on ; chromosome 15 between 100000 and 200000. This implementation with :py:meth:`.filter_variants_expr`; may come to mind first:; ; >>> vds_filtered = vds.filter_variants_expr('v.contig == ""15"" && v.start >= 100000 && v.start < 200000'); ; However, it is **much** faster (and easier!) to use this method:; ; >>> vds_filtered = vds.filter_intervals(Interval.parse('15:100000-200000')). .. note::. A :py:class:`.KeyTable` keyed by interval can be used to filter a dataset efficiently as well.; See the documentation for :py:meth:`.filter_variants_table` for an example. This is useful for; using interval files to filter a dataset. :param intervals: Interval(s) to keep or remove.; :type intervals: :class:`.Interval` or list of :class:`.Interval`. :param bool keep: Keep variants overlapping an interval if ``True``, remove variants overlapping; an interval if ``False``. :return: Filtered",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:76155,Performance,perform,performs,76155,"e` for an example. This is useful for; using interval files to filter a dataset. :param intervals: Interval(s) to keep or remove.; :type intervals: :class:`.Interval` or list of :class:`.Interval`. :param bool keep: Keep variants overlapping an interval if ``True``, remove variants overlapping; an interval if ``False``. :return: Filtered variant dataset.; :rtype: :py:class:`.VariantDataset`; """""". intervals = wrap_to_list(intervals). jvds = self._jvds.filterIntervals([x._jrep for x in intervals], keep); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(variants=listof(Variant),; keep=bool); def filter_variants_list(self, variants, keep=True):; """"""Filter variants with a list of variants. **Examples**. Filter VDS down to a list of variants:. >>> vds_filtered = vds.filter_variants_list([Variant.parse('20:10626633:G:GC'), ; ... Variant.parse('20:10019093:A:G')], keep=True); ; **Notes**. This method performs predicate pushdown when ``keep=True``, meaning that data shards; that don't overlap with any supplied variant will not be loaded at all. This property; enables ``filter_variants_list`` to be used for reasonably low-latency queries of one; or more variants, even on large datasets. ; ; :param variants: List of variants to keep or remove.; :type variants: list of :py:class:`~hail.representation.Variant`. :param bool keep: If true, keep variants in ``variants``, otherwise remove them. :return: Filtered variant dataset.; :rtype: :py:class:`.VariantDataset`; """""". return VariantDataset(; self.hc, self._jvds.filterVariantsList(; [TVariant()._convert_to_j(v) for v in variants], keep)). [docs] @handle_py4j; @typecheck_method(table=KeyTable,; keep=bool); def filter_variants_table(self, table, keep=True):; """"""Filter variants with a Variant keyed key table. **Example**. Filter variants of a VDS to those appearing in a text file:. >>> kt = hc.import_table('data/sample_variants.txt', key='Variant', impute=True); >>> filtered_vds = vds.filter_variants_table",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:76286,Performance,load,loaded,76286,"e` for an example. This is useful for; using interval files to filter a dataset. :param intervals: Interval(s) to keep or remove.; :type intervals: :class:`.Interval` or list of :class:`.Interval`. :param bool keep: Keep variants overlapping an interval if ``True``, remove variants overlapping; an interval if ``False``. :return: Filtered variant dataset.; :rtype: :py:class:`.VariantDataset`; """""". intervals = wrap_to_list(intervals). jvds = self._jvds.filterIntervals([x._jrep for x in intervals], keep); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(variants=listof(Variant),; keep=bool); def filter_variants_list(self, variants, keep=True):; """"""Filter variants with a list of variants. **Examples**. Filter VDS down to a list of variants:. >>> vds_filtered = vds.filter_variants_list([Variant.parse('20:10626633:G:GC'), ; ... Variant.parse('20:10019093:A:G')], keep=True); ; **Notes**. This method performs predicate pushdown when ``keep=True``, meaning that data shards; that don't overlap with any supplied variant will not be loaded at all. This property; enables ``filter_variants_list`` to be used for reasonably low-latency queries of one; or more variants, even on large datasets. ; ; :param variants: List of variants to keep or remove.; :type variants: list of :py:class:`~hail.representation.Variant`. :param bool keep: If true, keep variants in ``variants``, otherwise remove them. :return: Filtered variant dataset.; :rtype: :py:class:`.VariantDataset`; """""". return VariantDataset(; self.hc, self._jvds.filterVariantsList(; [TVariant()._convert_to_j(v) for v in variants], keep)). [docs] @handle_py4j; @typecheck_method(table=KeyTable,; keep=bool); def filter_variants_table(self, table, keep=True):; """"""Filter variants with a Variant keyed key table. **Example**. Filter variants of a VDS to those appearing in a text file:. >>> kt = hc.import_table('data/sample_variants.txt', key='Variant', impute=True); >>> filtered_vds = vds.filter_variants_table",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:76379,Performance,latency,latency,76379,"ass:`.Interval` or list of :class:`.Interval`. :param bool keep: Keep variants overlapping an interval if ``True``, remove variants overlapping; an interval if ``False``. :return: Filtered variant dataset.; :rtype: :py:class:`.VariantDataset`; """""". intervals = wrap_to_list(intervals). jvds = self._jvds.filterIntervals([x._jrep for x in intervals], keep); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(variants=listof(Variant),; keep=bool); def filter_variants_list(self, variants, keep=True):; """"""Filter variants with a list of variants. **Examples**. Filter VDS down to a list of variants:. >>> vds_filtered = vds.filter_variants_list([Variant.parse('20:10626633:G:GC'), ; ... Variant.parse('20:10019093:A:G')], keep=True); ; **Notes**. This method performs predicate pushdown when ``keep=True``, meaning that data shards; that don't overlap with any supplied variant will not be loaded at all. This property; enables ``filter_variants_list`` to be used for reasonably low-latency queries of one; or more variants, even on large datasets. ; ; :param variants: List of variants to keep or remove.; :type variants: list of :py:class:`~hail.representation.Variant`. :param bool keep: If true, keep variants in ``variants``, otherwise remove them. :return: Filtered variant dataset.; :rtype: :py:class:`.VariantDataset`; """""". return VariantDataset(; self.hc, self._jvds.filterVariantsList(; [TVariant()._convert_to_j(v) for v in variants], keep)). [docs] @handle_py4j; @typecheck_method(table=KeyTable,; keep=bool); def filter_variants_table(self, table, keep=True):; """"""Filter variants with a Variant keyed key table. **Example**. Filter variants of a VDS to those appearing in a text file:. >>> kt = hc.import_table('data/sample_variants.txt', key='Variant', impute=True); >>> filtered_vds = vds.filter_variants_table(kt, keep=True); ; Keep all variants whose chromosome and position (locus) appear in a file with ; a chromosome:position column:; ; >>> kt = hc.import_",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:82590,Performance,perform,perform,82590,"dCalls()). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(maf=nullable(strlike),; bounded=bool,; min=nullable(numeric),; max=nullable(numeric)); def ibd(self, maf=None, bounded=True, min=None, max=None):; """"""Compute matrix of identity-by-descent estimations. .. include:: requireTGenotype.rst. **Examples**. To calculate a full IBD matrix, using minor allele frequencies computed; from the variant dataset itself:. >>> vds.ibd(). To calculate an IBD matrix containing only pairs of samples with; ``PI_HAT`` in [0.2, 0.9], using minor allele frequencies stored in; ``va.panel_maf``:. >>> vds.ibd(maf='va.panel_maf', min=0.2, max=0.9). **Notes**. The implementation is based on the IBD algorithm described in the `PLINK; paper <http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1950838>`__. :py:meth:`~hail.VariantDataset.ibd` requires the dataset to be; bi-allelic (otherwise run :py:meth:`~hail.VariantDataset.split_multi` or otherwise run :py:meth:`~hail.VariantDataset.filter_multi`); and does not perform LD pruning. Linkage disequilibrium may bias the; result so consider filtering variants first. The resulting :py:class:`.KeyTable` entries have the type: *{ i: String,; j: String, ibd: { Z0: Double, Z1: Double, Z2: Double, PI_HAT: Double },; ibs0: Long, ibs1: Long, ibs2: Long }*. The key list is: `*i: String, j:; String*`. Conceptually, the output is a symmetric, sample-by-sample matrix. The; output key table has the following form. .. code-block:: text. i		j	ibd.Z0	ibd.Z1	ibd.Z2	ibd.PI_HAT ibs0	ibs1	ibs2; sample1	sample2	1.0000	0.0000	0.0000	0.0000 ...; sample1	sample3	1.0000	0.0000	0.0000	0.0000 ...; sample1	sample4	0.6807	0.0000	0.3193	0.3193 ...; sample1	sample5	0.1966	0.0000	0.8034	0.8034 ... :param maf: Expression for the minor allele frequency.; :type maf: str or None. :param bool bounded: Forces the estimations for Z0, Z1, Z2,; and PI_HAT to take on biologically meaningful values; (in the range [0,1]). :param min: Sample pairs with a PI_HAT below this value will; no",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:90574,Performance,perform,performs,90574,"nts considered; - **nCalled** (*Long*) -- Number of variants with a genotype call; - **expectedHoms** (*Double*) -- Expected number of homozygotes; - **observedHoms** (*Long*) -- Observed number of homozygotes. :param float maf_threshold: Minimum minor allele frequency threshold. :param bool include_par: Include pseudoautosomal regions. :param float female_threshold: Samples are called females if F < femaleThreshold. :param float male_threshold: Samples are called males if F > maleThreshold. :param str pop_freq: Variant annotation for estimate of MAF.; If None, MAF will be computed. :return: Annotated dataset.; :rtype: :py:class:`.VariantDataset`; """""". jvds = self._jvdf.imputeSex(maf_threshold, include_par, female_threshold, male_threshold, joption(pop_freq)); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(right=vds_type); def join(self, right):; """"""Join two variant datasets. **Notes**. This method performs an inner join on variants,; concatenates samples, and takes variant and; global annotations from the left dataset (self). The datasets must have distinct samples, the same sample schema, and the same split status (both split or both multi-allelic). :param right: right-hand variant dataset; :type right: :py:class:`.VariantDataset`. :return: Joined variant dataset; :rtype: :py:class:`.VariantDataset`; """""". return VariantDataset(self.hc, self._jvds.join(right._jvds)). [docs] @handle_py4j; @typecheck(datasets=tupleof(vds_type)); def union(*datasets):; """"""Take the union of datasets vertically (include all variants). **Examples**. .. testsetup::. vds_autosomal = vds; vds_chromX = vds; vds_chromY = vds. Union two datasets:. >>> vds_union = vds_autosomal.union(vds_chromX). Given a list of datasets, union them all:. >>> all_vds = [vds_autosomal, vds_chromX, vds_chromY]. The following three syntaxes are equivalent:. >>> vds_union1 = vds_autosomal.union(vds_chromX, vds_chromY); >>> vds_union2 = all_vds[0].union(*all_vds[1:]); >>> vds_union3 = Va",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:97028,Performance,perform,performance,97028,"ue between variants i and j, defined as; `Pearson's correlation coefficient <https://en.wikipedia.org/wiki/Pearson_correlation_coefficient>`__; :math:`\\rho_{x_i,x_j}` between the two genotype vectors :math:`x_i` and :math:`x_j`. .. math::. \\rho_{x_i,x_j} = \\frac{\\mathrm{Cov}(X_i,X_j)}{\\sigma_{X_i} \\sigma_{X_j}}. Also note that variants with zero variance (:math:`\\sigma = 0`) will be dropped from the matrix. .. caution::. The matrix returned by this function can easily be very large with most entries near zero; (for example, entries between variants on different chromosomes in a homogenous population).; Most likely you'll want to reduce the number of variants with methods like; :py:meth:`.sample_variants`, :py:meth:`.filter_variants_expr`, or :py:meth:`.ld_prune` before; calling this unless your dataset is very small. :param bool force_local: If true, the LD matrix is computed using local matrix multiplication on the Spark driver. This may improve performance when the genotype matrix is small enough to easily fit in local memory. If false, the LD matrix is computed using distributed matrix multiplication if the number of genotypes exceeds :math:`5000^2` and locally otherwise. :return: Matrix of r values between pairs of variants.; :rtype: :py:class:`LDMatrix`; """""". jldm = self._jvdf.ldMatrix(force_local); return LDMatrix(jldm). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(y=strlike,; covariates=listof(strlike),; root=strlike,; use_dosages=bool,; min_ac=integral,; min_af=numeric); def linreg(self, y, covariates=[], root='va.linreg', use_dosages=False, min_ac=1, min_af=0.0):; r""""""Test each variant for association using linear regression. .. include:: requireTGenotype.rst. **Examples**. Run linear regression per variant using a phenotype and two covariates stored in sample annotations:. >>> vds_result = vds.linreg('sa.pheno.height', covariates=['sa.pheno.age', 'sa.pheno.isFemale']). **Notes**. The :py:meth:`.linreg` method computes, for each variant, ",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:114002,Performance,perform,perform,114002,"-- number of samples used; - **va.linreg.AC** (*Double*) -- sum of the genotype values ``x``; - **va.linreg.ytx** (*Array[Double]*) -- array of dot products of each phenotype vector ``y`` with the genotype vector ``x``; - **va.linreg.beta** (*Array[Double]*) -- array of fit genotype coefficients, :math:`\hat\beta_1`; - **va.linreg.se** (*Array[Double]*) -- array of estimated standard errors, :math:`\widehat{\mathrm{se}}`; - **va.linreg.tstat** (*Array[Double]*) -- array of :math:`t`-statistics, equal to :math:`\hat\beta_1 / \widehat{\mathrm{se}}`; - **va.linreg.pval** (*Array[Double]*) -- array of :math:`p`-values. :param ys: list of one or more response expressions.; :type covariates: list of str. :param covariates: list of covariate expressions.; :type covariates: list of str. :param str root: Variant annotation path to store result of linear regression. :param bool use_dosages: If true, use dosage genotypes rather than hard call genotypes. :param int variant_block_size: Number of variant regressions to perform simultaneously. Larger block size requires more memmory. :return: Variant dataset with linear regression variant annotations.; :rtype: :py:class:`.VariantDataset`. """""". jvds = self._jvdf.linreg3(jarray(Env.jvm().java.lang.String, ys),; jarray(Env.jvm().java.lang.String, covariates), root, use_dosages, variant_block_size); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(kinshipMatrix=KinshipMatrix,; y=strlike,; covariates=listof(strlike),; global_root=strlike,; va_root=strlike,; run_assoc=bool,; use_ml=bool,; delta=nullable(numeric),; sparsity_threshold=numeric,; use_dosages=bool,; n_eigs=nullable(integral),; dropped_variance_fraction=(nullable(float))); def lmmreg(self, kinshipMatrix, y, covariates=[], global_root=""global.lmmreg"", va_root=""va.lmmreg"",; run_assoc=True, use_ml=False, delta=None, sparsity_threshold=1.0, use_dosages=False,; n_eigs=None, dropped_variance_fraction=None):; """"""Use a kinship-based line",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:122434,Performance,perform,performed,122434,"``global.lmmreg.fit.logDeltaGrid`` | Array[Double] | values of :math:`\\mathrm{ln}(\delta)` used in the grid search |; +----------------------------------------------+----------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+; | ``global.lmmreg.fit.logLkhdVals`` | Array[Double] | (restricted) log likelihood of :math:`y` given :math:`X` and :math:`\\mathrm{ln}(\delta)` at the (RE)ML fit of :math:`\\beta` and :math:`\sigma_g^2` |; +----------------------------------------------+----------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+. These global annotations are also added to ``hail.log``, with the ranked evals and :math:`\delta` grid with values in .tsv tabular form. Use ``grep 'lmmreg:' hail.log`` to find the lines just above each table. If Step 5 is performed, :py:meth:`.lmmreg` also adds four linear regression variant annotations. +------------------------+--------+-------------------------------------------------------------------------+; | Annotation | Type | Value |; +========================+========+=========================================================================+; | ``va.lmmreg.beta`` | Double | fit genotype coefficient, :math:`\hat\\beta_0` |; +------------------------+--------+-------------------------------------------------------------------------+; | ``va.lmmreg.sigmaG2`` | Double | fit coefficient of genetic variance component, :math:`\hat{\sigma}_g^2` |; +------------------------+--------+-------------------------------------------------------------------------+; | ``va.lmmreg.chi2`` | Double | :math:`\chi^2` statistic of the likelihood ratio test |; +------------------------+--------+-------------------------------------------------------------------------+; | ``va.lmmreg.pval`` | Double | :math:`p`-value |; +-",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:125277,Performance,perform,performant,125277,"ail's initial version of :py:meth:`.lmmreg` scales beyond 15k samples and to an essentially unbounded number of variants, making it particularly well-suited to modern sequencing studies and complementary to tools designed for SNP arrays. Analysts have used :py:meth:`.lmmreg` in research to compute kinship from 100k common variants and test 32 million non-rare variants on 8k whole genomes in about 10 minutes on `Google cloud <http://discuss.hail.is/t/using-hail-on-the-google-cloud-platform/80>`__. While :py:meth:`.lmmreg` computes the kinship matrix :math:`K` using distributed matrix multiplication (Step 2), the full `eigendecomposition <https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix>`__ (Step 3) is currently run on a single core of master using the `LAPACK routine DSYEVD <http://www.netlib.org/lapack/explore-html/d2/d8a/group__double_s_yeigen_ga694ddc6e5527b6223748e3462013d867.html>`__, which we empirically find to be the most performant of the four available routines; laptop performance plots showing cubic complexity in :math:`n` are available `here <https://github.com/hail-is/hail/pull/906>`__. On Google cloud, eigendecomposition takes about 2 seconds for 2535 sampes and 1 minute for 8185 samples. If you see worse performance, check that LAPACK natives are being properly loaded (see ""BLAS and LAPACK"" in Getting Started). Given the eigendecomposition, fitting the global model (Step 4) takes on the order of a few seconds on master. Association testing (Step 5) is fully distributed by variant with per-variant time complexity that is completely independent of the number of sample covariates and dominated by multiplication of the genotype vector :math:`v` by the matrix of eigenvectors :math:`U^T` as described below, which we accelerate with a sparse representation of :math:`v`. The matrix :math:`U^T` has size about :math:`8n^2` bytes and is currently broadcast to each Spark executor. For example, with 15k samples, storing :math:`U^T` consumes about 3.6GB o",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:125327,Performance,perform,performance,125327,"ail's initial version of :py:meth:`.lmmreg` scales beyond 15k samples and to an essentially unbounded number of variants, making it particularly well-suited to modern sequencing studies and complementary to tools designed for SNP arrays. Analysts have used :py:meth:`.lmmreg` in research to compute kinship from 100k common variants and test 32 million non-rare variants on 8k whole genomes in about 10 minutes on `Google cloud <http://discuss.hail.is/t/using-hail-on-the-google-cloud-platform/80>`__. While :py:meth:`.lmmreg` computes the kinship matrix :math:`K` using distributed matrix multiplication (Step 2), the full `eigendecomposition <https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix>`__ (Step 3) is currently run on a single core of master using the `LAPACK routine DSYEVD <http://www.netlib.org/lapack/explore-html/d2/d8a/group__double_s_yeigen_ga694ddc6e5527b6223748e3462013d867.html>`__, which we empirically find to be the most performant of the four available routines; laptop performance plots showing cubic complexity in :math:`n` are available `here <https://github.com/hail-is/hail/pull/906>`__. On Google cloud, eigendecomposition takes about 2 seconds for 2535 sampes and 1 minute for 8185 samples. If you see worse performance, check that LAPACK natives are being properly loaded (see ""BLAS and LAPACK"" in Getting Started). Given the eigendecomposition, fitting the global model (Step 4) takes on the order of a few seconds on master. Association testing (Step 5) is fully distributed by variant with per-variant time complexity that is completely independent of the number of sample covariates and dominated by multiplication of the genotype vector :math:`v` by the matrix of eigenvectors :math:`U^T` as described below, which we accelerate with a sparse representation of :math:`v`. The matrix :math:`U^T` has size about :math:`8n^2` bytes and is currently broadcast to each Spark executor. For example, with 15k samples, storing :math:`U^T` consumes about 3.6GB o",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:125572,Performance,perform,performance,125572,"ute kinship from 100k common variants and test 32 million non-rare variants on 8k whole genomes in about 10 minutes on `Google cloud <http://discuss.hail.is/t/using-hail-on-the-google-cloud-platform/80>`__. While :py:meth:`.lmmreg` computes the kinship matrix :math:`K` using distributed matrix multiplication (Step 2), the full `eigendecomposition <https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix>`__ (Step 3) is currently run on a single core of master using the `LAPACK routine DSYEVD <http://www.netlib.org/lapack/explore-html/d2/d8a/group__double_s_yeigen_ga694ddc6e5527b6223748e3462013d867.html>`__, which we empirically find to be the most performant of the four available routines; laptop performance plots showing cubic complexity in :math:`n` are available `here <https://github.com/hail-is/hail/pull/906>`__. On Google cloud, eigendecomposition takes about 2 seconds for 2535 sampes and 1 minute for 8185 samples. If you see worse performance, check that LAPACK natives are being properly loaded (see ""BLAS and LAPACK"" in Getting Started). Given the eigendecomposition, fitting the global model (Step 4) takes on the order of a few seconds on master. Association testing (Step 5) is fully distributed by variant with per-variant time complexity that is completely independent of the number of sample covariates and dominated by multiplication of the genotype vector :math:`v` by the matrix of eigenvectors :math:`U^T` as described below, which we accelerate with a sparse representation of :math:`v`. The matrix :math:`U^T` has size about :math:`8n^2` bytes and is currently broadcast to each Spark executor. For example, with 15k samples, storing :math:`U^T` consumes about 3.6GB of memory on a 16-core worker node with two 8-core executors. So for large :math:`n`, we recommend using a high-memory configuration such as ``highmem`` workers. **Linear mixed model**. :py:meth:`.lmmreg` estimates the genetic proportion of residual phenotypic variance (narrow-sense heritabilit",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:125630,Performance,load,loaded,125630,"ute kinship from 100k common variants and test 32 million non-rare variants on 8k whole genomes in about 10 minutes on `Google cloud <http://discuss.hail.is/t/using-hail-on-the-google-cloud-platform/80>`__. While :py:meth:`.lmmreg` computes the kinship matrix :math:`K` using distributed matrix multiplication (Step 2), the full `eigendecomposition <https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix>`__ (Step 3) is currently run on a single core of master using the `LAPACK routine DSYEVD <http://www.netlib.org/lapack/explore-html/d2/d8a/group__double_s_yeigen_ga694ddc6e5527b6223748e3462013d867.html>`__, which we empirically find to be the most performant of the four available routines; laptop performance plots showing cubic complexity in :math:`n` are available `here <https://github.com/hail-is/hail/pull/906>`__. On Google cloud, eigendecomposition takes about 2 seconds for 2535 sampes and 1 minute for 8185 samples. If you see worse performance, check that LAPACK natives are being properly loaded (see ""BLAS and LAPACK"" in Getting Started). Given the eigendecomposition, fitting the global model (Step 4) takes on the order of a few seconds on master. Association testing (Step 5) is fully distributed by variant with per-variant time complexity that is completely independent of the number of sample covariates and dominated by multiplication of the genotype vector :math:`v` by the matrix of eigenvectors :math:`U^T` as described below, which we accelerate with a sparse representation of :math:`v`. The matrix :math:`U^T` has size about :math:`8n^2` bytes and is currently broadcast to each Spark executor. For example, with 15k samples, storing :math:`U^T` consumes about 3.6GB of memory on a 16-core worker node with two 8-core executors. So for large :math:`n`, we recommend using a high-memory configuration such as ``highmem`` workers. **Linear mixed model**. :py:meth:`.lmmreg` estimates the genetic proportion of residual phenotypic variance (narrow-sense heritabilit",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:130282,Performance,optimiz,optimization,130282,"order. :math:`S_{ii}` is the eigenvalue of eigenvector :math:`U_{:,i}`; - :math:`U^T = n \\times n` orthonormal matrix, the transpose (and inverse) of :math:`U`. A bit of matrix algebra on the multivariate normal density shows that the linear mixed model above is mathematically equivalent to the model. .. math::. U^Ty \\sim \mathrm{N}\\left(U^TX\\beta, \sigma_g^2 (S + \delta I)\\right). for which the covariance is diagonal (e.g., unmixed). That is, rotating the phenotype vector (:math:`y`) and covariate vectors (columns of :math:`X`) in :math:`\mathbb{R}^n` by :math:`U^T` transforms the model to one with independent residuals. For any particular value of :math:`\delta`, the restricted maximum likelihood (REML) solution for the latter model can be solved exactly in time complexity that is linear rather than cubic in :math:`n`. In particular, having rotated, we can run a very efficient 1-dimensional optimization procedure over :math:`\delta` to find the REML estimate :math:`(\hat{\delta}, \\hat{\\beta}, \\hat{\sigma}_g^2)` of the triple :math:`(\delta, \\beta, \sigma_g^2)`, which in turn determines :math:`\\hat{\sigma}_e^2` and :math:`\\hat{h}^2`. We first compute the maximum log likelihood on a :math:`\delta`-grid that is uniform on the log scale, with :math:`\\mathrm{ln}(\delta)` running from -8 to 8 by 0.01, corresponding to :math:`h^2` decreasing from 0.9995 to 0.0005. If :math:`h^2` is maximized at the lower boundary then standard linear regression would be more appropriate and Hail will exit; more generally, consider using standard linear regression when :math:`\\hat{h}^2` is very small. A maximum at the upper boundary is highly suspicious and will also cause Hail to exit. In any case, the log file records the table of grid values for further inspection, beginning under the info line containing ""lmmreg: table of delta"". If the optimal grid point falls in the interior of the grid as expected, we then use `Brent's method <https://en.wikipedia.org/wiki/Brent%27s_me",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:136237,Performance,perform,performance,136237,"\beta^0_v, \\beta^1_v, \\ldots, \\beta^c_v), \sigma_{g_v}^2)`, with :math:`\delta` fixed at :math:`\\hat\delta` in both. The latter fit is simply that of the global model, :math:`((0, \\hat{\\beta}^1, \\ldots, \\hat{\\beta}^c), \\hat{\sigma}_g^2)`. The likelihood ratio test statistic is given by. .. math::. \chi^2 = n \\, \\mathrm{ln}\left(\\frac{\hat{\sigma}^2_g}{\\hat{\sigma}_{g,v}^2}\\right). and follows a chi-squared distribution with one degree of freedom. Here the ratio :math:`\\hat{\sigma}^2_g / \\hat{\sigma}_{g,v}^2` captures the degree to which adding the variant :math:`v` to the global model reduces the residual phenotypic variance. **Kinship Matrix**. FastLMM uses the Realized Relationship Matrix (RRM) for kinship. This can be computed with :py:meth:`~hail.VariantDataset.rrm`. However, any instance of :py:class:`KinshipMatrix` may be used, so long as ``sample_list`` contains the complete samples of the caller variant dataset in the same order. **Low-rank approximation of kinship for improved performance**. :py:meth:`.lmmreg` can implicitly use a low-rank approximation of the kinship matrix to more rapidly fit delta and the statistics for each variant. The computational complexity per variant is proportional to the number of eigenvectors used. This number can be specified in two ways. Specify the parameter ``n_eigs`` to use only the top ``n_eigs`` eigenvectors. Alternatively, specify ``dropped_variance_fraction`` to use as many eigenvectors as necessary to capture all but at most this fraction of the sample variance (also known as the trace, or the sum of the eigenvalues). For example, ``dropped_variance_fraction=0.01`` will use the minimal number of eigenvectors to account for 99% of the sample variance. Specifying both parameters will apply the more stringent (fewest eigenvectors) of the two. **Further background**. For the history and mathematics of linear mixed models in genetics, including `FastLMM <https://www.microsoft.com/en-us/research/project/fas",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:139765,Performance,perform,performs,139765,"tDataset`; """""". jvds = self._jvdf.lmmreg(kinshipMatrix._jkm, y, jarray(Env.jvm().java.lang.String, covariates),; use_ml, global_root, va_root, run_assoc, joption(delta), sparsity_threshold,; use_dosages, joption(n_eigs), joption(dropped_variance_fraction)); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(test=strlike,; y=strlike,; covariates=listof(strlike),; root=strlike,; use_dosages=bool); def logreg(self, test, y, covariates=[], root='va.logreg', use_dosages=False):; """"""Test each variant for association using logistic regression. .. include:: requireTGenotype.rst. **Examples**. Run the logistic regression Wald test per variant using a Boolean phenotype and two covariates stored; in sample annotations:. >>> vds_result = vds.logreg('wald', 'sa.pheno.isCase', covariates=['sa.pheno.age', 'sa.pheno.isFemale']). **Notes**. The :py:meth:`~hail.VariantDataset.logreg` method performs,; for each variant, a significance test of the genotype in; predicting a binary (case-control) phenotype based on the; logistic regression model. The phenotype type must either be numeric (with all; present values 0 or 1) or Boolean, in which case true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'), Rao score test ('score'),; and Firth test ('firth'). Hail only includes samples for which the phenotype and all covariates are; defined. For each variant, Hail imputes missing genotypes as the mean of called genotypes. By default, genotypes values are given by hard call genotypes (``g.gt``).; If ``use_dosages=True``, then genotype values are defined by the dosage; :math:`\mathrm{P}(\mathrm{Het}) + 2 \cdot \mathrm{P}(\mathrm{HomVar})`. For Phred-scaled values,; :math:`\mathrm{P}(\mathrm{Het})` and :math:`\mathrm{P}(\mathrm{HomVar})` are; calculated by normalizing the PL likelihoods (converted from the Phred-scale) to sum to 1. The example above considers a model of the form. .. math::.",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:161652,Performance,load,loadings,161652,"), KeyTable(self.hc, kts._2()), \; KeyTable(self.hc, kts._3()), KeyTable(self.hc, kts._4()). [docs] @handle_py4j; @typecheck_method(max_shift=integral); def min_rep(self, max_shift=100):; """"""; Gives minimal, left-aligned representation of alleles. Note that this can change the variant position. **Examples**. 1. Simple trimming of a multi-allelic site, no change in variant position; `1:10000:TAA:TAA,AA` => `1:10000:TA:T,A`. 2. Trimming of a bi-allelic site leading to a change in position; `1:10000:AATAA,AAGAA` => `1:10002:T:G`. :param int max_shift: maximum number of base pairs by which; a split variant can move. Affects memory usage, and will; cause Hail to throw an error if a variant that moves further; is encountered. :rtype: :class:`.VariantDataset`; """""". jvds = self._jvds.minRep(max_shift); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(scores=strlike,; loadings=nullable(strlike),; eigenvalues=nullable(strlike),; k=integral,; as_array=bool); def pca(self, scores, loadings=None, eigenvalues=None, k=10, as_array=False):; """"""Run Principal Component Analysis (PCA) on the matrix of genotypes. .. include:: requireTGenotype.rst. **Examples**. Compute the top 5 principal component scores, stored as sample annotations ``sa.scores.PC1``, ..., ``sa.scores.PC5`` of type Double:. >>> vds_result = vds.pca('sa.scores', k=5). Compute the top 5 principal component scores, loadings, and eigenvalues, stored as annotations ``sa.scores``, ``va.loadings``, and ``global.evals`` of type Array[Double]:. >>> vds_result = vds.pca('sa.scores', 'va.loadings', 'global.evals', 5, as_array=True). **Notes**. Hail supports principal component analysis (PCA) of genotype data, a now-standard procedure `Patterson, Price and Reich, 2006 <http://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.0020190>`__. This method expects a variant dataset with biallelic autosomal variants. Scores are computed and stored as sample annotations of type ",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:161764,Performance,load,loadings,161764,"), KeyTable(self.hc, kts._2()), \; KeyTable(self.hc, kts._3()), KeyTable(self.hc, kts._4()). [docs] @handle_py4j; @typecheck_method(max_shift=integral); def min_rep(self, max_shift=100):; """"""; Gives minimal, left-aligned representation of alleles. Note that this can change the variant position. **Examples**. 1. Simple trimming of a multi-allelic site, no change in variant position; `1:10000:TAA:TAA,AA` => `1:10000:TA:T,A`. 2. Trimming of a bi-allelic site leading to a change in position; `1:10000:AATAA,AAGAA` => `1:10002:T:G`. :param int max_shift: maximum number of base pairs by which; a split variant can move. Affects memory usage, and will; cause Hail to throw an error if a variant that moves further; is encountered. :rtype: :class:`.VariantDataset`; """""". jvds = self._jvds.minRep(max_shift); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(scores=strlike,; loadings=nullable(strlike),; eigenvalues=nullable(strlike),; k=integral,; as_array=bool); def pca(self, scores, loadings=None, eigenvalues=None, k=10, as_array=False):; """"""Run Principal Component Analysis (PCA) on the matrix of genotypes. .. include:: requireTGenotype.rst. **Examples**. Compute the top 5 principal component scores, stored as sample annotations ``sa.scores.PC1``, ..., ``sa.scores.PC5`` of type Double:. >>> vds_result = vds.pca('sa.scores', k=5). Compute the top 5 principal component scores, loadings, and eigenvalues, stored as annotations ``sa.scores``, ``va.loadings``, and ``global.evals`` of type Array[Double]:. >>> vds_result = vds.pca('sa.scores', 'va.loadings', 'global.evals', 5, as_array=True). **Notes**. Hail supports principal component analysis (PCA) of genotype data, a now-standard procedure `Patterson, Price and Reich, 2006 <http://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.0020190>`__. This method expects a variant dataset with biallelic autosomal variants. Scores are computed and stored as sample annotations of type ",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:162164,Performance,load,loadings,162164,"ming of a bi-allelic site leading to a change in position; `1:10000:AATAA,AAGAA` => `1:10002:T:G`. :param int max_shift: maximum number of base pairs by which; a split variant can move. Affects memory usage, and will; cause Hail to throw an error if a variant that moves further; is encountered. :rtype: :class:`.VariantDataset`; """""". jvds = self._jvds.minRep(max_shift); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(scores=strlike,; loadings=nullable(strlike),; eigenvalues=nullable(strlike),; k=integral,; as_array=bool); def pca(self, scores, loadings=None, eigenvalues=None, k=10, as_array=False):; """"""Run Principal Component Analysis (PCA) on the matrix of genotypes. .. include:: requireTGenotype.rst. **Examples**. Compute the top 5 principal component scores, stored as sample annotations ``sa.scores.PC1``, ..., ``sa.scores.PC5`` of type Double:. >>> vds_result = vds.pca('sa.scores', k=5). Compute the top 5 principal component scores, loadings, and eigenvalues, stored as annotations ``sa.scores``, ``va.loadings``, and ``global.evals`` of type Array[Double]:. >>> vds_result = vds.pca('sa.scores', 'va.loadings', 'global.evals', 5, as_array=True). **Notes**. Hail supports principal component analysis (PCA) of genotype data, a now-standard procedure `Patterson, Price and Reich, 2006 <http://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.0020190>`__. This method expects a variant dataset with biallelic autosomal variants. Scores are computed and stored as sample annotations of type Struct by default; variant loadings and eigenvalues can optionally be computed and stored in variant and global annotations, respectively. PCA is based on the singular value decomposition (SVD) of a standardized genotype matrix :math:`M`, computed as follows. An :math:`n \\times m` matrix :math:`C` records raw genotypes, with rows indexed by :math:`n` samples and columns indexed by :math:`m` bialellic autosomal variants; :math:`C_{",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:162233,Performance,load,loadings,162233,"A` => `1:10002:T:G`. :param int max_shift: maximum number of base pairs by which; a split variant can move. Affects memory usage, and will; cause Hail to throw an error if a variant that moves further; is encountered. :rtype: :class:`.VariantDataset`; """""". jvds = self._jvds.minRep(max_shift); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(scores=strlike,; loadings=nullable(strlike),; eigenvalues=nullable(strlike),; k=integral,; as_array=bool); def pca(self, scores, loadings=None, eigenvalues=None, k=10, as_array=False):; """"""Run Principal Component Analysis (PCA) on the matrix of genotypes. .. include:: requireTGenotype.rst. **Examples**. Compute the top 5 principal component scores, stored as sample annotations ``sa.scores.PC1``, ..., ``sa.scores.PC5`` of type Double:. >>> vds_result = vds.pca('sa.scores', k=5). Compute the top 5 principal component scores, loadings, and eigenvalues, stored as annotations ``sa.scores``, ``va.loadings``, and ``global.evals`` of type Array[Double]:. >>> vds_result = vds.pca('sa.scores', 'va.loadings', 'global.evals', 5, as_array=True). **Notes**. Hail supports principal component analysis (PCA) of genotype data, a now-standard procedure `Patterson, Price and Reich, 2006 <http://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.0020190>`__. This method expects a variant dataset with biallelic autosomal variants. Scores are computed and stored as sample annotations of type Struct by default; variant loadings and eigenvalues can optionally be computed and stored in variant and global annotations, respectively. PCA is based on the singular value decomposition (SVD) of a standardized genotype matrix :math:`M`, computed as follows. An :math:`n \\times m` matrix :math:`C` records raw genotypes, with rows indexed by :math:`n` samples and columns indexed by :math:`m` bialellic autosomal variants; :math:`C_{ij}` is the number of alternate alleles of variant :math:`j` carried by sample",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:162332,Performance,load,loadings,162332,"t can move. Affects memory usage, and will; cause Hail to throw an error if a variant that moves further; is encountered. :rtype: :class:`.VariantDataset`; """""". jvds = self._jvds.minRep(max_shift); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(scores=strlike,; loadings=nullable(strlike),; eigenvalues=nullable(strlike),; k=integral,; as_array=bool); def pca(self, scores, loadings=None, eigenvalues=None, k=10, as_array=False):; """"""Run Principal Component Analysis (PCA) on the matrix of genotypes. .. include:: requireTGenotype.rst. **Examples**. Compute the top 5 principal component scores, stored as sample annotations ``sa.scores.PC1``, ..., ``sa.scores.PC5`` of type Double:. >>> vds_result = vds.pca('sa.scores', k=5). Compute the top 5 principal component scores, loadings, and eigenvalues, stored as annotations ``sa.scores``, ``va.loadings``, and ``global.evals`` of type Array[Double]:. >>> vds_result = vds.pca('sa.scores', 'va.loadings', 'global.evals', 5, as_array=True). **Notes**. Hail supports principal component analysis (PCA) of genotype data, a now-standard procedure `Patterson, Price and Reich, 2006 <http://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.0020190>`__. This method expects a variant dataset with biallelic autosomal variants. Scores are computed and stored as sample annotations of type Struct by default; variant loadings and eigenvalues can optionally be computed and stored in variant and global annotations, respectively. PCA is based on the singular value decomposition (SVD) of a standardized genotype matrix :math:`M`, computed as follows. An :math:`n \\times m` matrix :math:`C` records raw genotypes, with rows indexed by :math:`n` samples and columns indexed by :math:`m` bialellic autosomal variants; :math:`C_{ij}` is the number of alternate alleles of variant :math:`j` carried by sample :math:`i`, which can be 0, 1, 2, or missing. For each variant :math:`j`, the sample alternate a",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:162760,Performance,load,loadings,162760,"s=None, eigenvalues=None, k=10, as_array=False):; """"""Run Principal Component Analysis (PCA) on the matrix of genotypes. .. include:: requireTGenotype.rst. **Examples**. Compute the top 5 principal component scores, stored as sample annotations ``sa.scores.PC1``, ..., ``sa.scores.PC5`` of type Double:. >>> vds_result = vds.pca('sa.scores', k=5). Compute the top 5 principal component scores, loadings, and eigenvalues, stored as annotations ``sa.scores``, ``va.loadings``, and ``global.evals`` of type Array[Double]:. >>> vds_result = vds.pca('sa.scores', 'va.loadings', 'global.evals', 5, as_array=True). **Notes**. Hail supports principal component analysis (PCA) of genotype data, a now-standard procedure `Patterson, Price and Reich, 2006 <http://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.0020190>`__. This method expects a variant dataset with biallelic autosomal variants. Scores are computed and stored as sample annotations of type Struct by default; variant loadings and eigenvalues can optionally be computed and stored in variant and global annotations, respectively. PCA is based on the singular value decomposition (SVD) of a standardized genotype matrix :math:`M`, computed as follows. An :math:`n \\times m` matrix :math:`C` records raw genotypes, with rows indexed by :math:`n` samples and columns indexed by :math:`m` bialellic autosomal variants; :math:`C_{ij}` is the number of alternate alleles of variant :math:`j` carried by sample :math:`i`, which can be 0, 1, 2, or missing. For each variant :math:`j`, the sample alternate allele frequency :math:`p_j` is computed as half the mean of the non-missing entries of column :math:`j`. Entries of :math:`M` are then mean-centered and variance-normalized as. .. math::. M_{ij} = \\frac{C_{ij}-2p_j}{\sqrt{2p_j(1-p_j)m}},. with :math:`M_{ij} = 0` for :math:`C_{ij}` missing (i.e. mean genotype imputation). This scaling normalizes genotype variances to a common value :math:`1/m` for variants in Hardy-Weinberg e",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:165002,Performance,load,loadings,165002,"h minor allele frequency below some cutoff.) The factor :math:`1/m` gives each sample row approximately unit total variance (assuming linkage equilibrium) and yields the sample correlation or genetic relationship matrix (GRM) as simply :math:`MM^T`. PCA then computes the SVD. .. math::. M = USV^T. where columns of :math:`U` are left singular vectors (orthonormal in :math:`\mathbb{R}^n`), columns of :math:`V` are right singular vectors (orthonormal in :math:`\mathbb{R}^m`), and :math:`S=\mathrm{diag}(s_1, s_2, \ldots)` with ordered singular values :math:`s_1 \ge s_2 \ge \cdots \ge 0`. Typically one computes only the first :math:`k` singular vectors and values, yielding the best rank :math:`k` approximation :math:`U_k S_k V_k^T` of :math:`M`; the truncations :math:`U_k`, :math:`S_k` and :math:`V_k` are :math:`n \\times k`, :math:`k \\times k` and :math:`m \\times k` respectively. From the perspective of the samples or rows of :math:`M` as data, :math:`V_k` contains the variant loadings for the first :math:`k` PCs while :math:`MV_k = U_k S_k` contains the first :math:`k` PC scores of each sample. The loadings represent a new basis of features while the scores represent the projected data on those features. The eigenvalues of the GRM :math:`MM^T` are the squares of the singular values :math:`s_1^2, s_2^2, \ldots`, which represent the variances carried by the respective PCs. By default, Hail only computes the loadings if the ``loadings`` parameter is specified. *Note:* In PLINK/GCTA the GRM is taken as the starting point and it is computed slightly differently with regard to missing data. Here the :math:`ij` entry of :math:`MM^T` is simply the dot product of rows :math:`i` and :math:`j` of :math:`M`; in terms of :math:`C` it is. .. math::. \\frac{1}{m}\sum_{l\in\mathcal{C}_i\cap\mathcal{C}_j}\\frac{(C_{il}-2p_l)(C_{jl} - 2p_l)}{2p_l(1-p_l)}. where :math:`\mathcal{C}_i = \{l \mid C_{il} \\text{ is non-missing}\}`. In PLINK/GCTA the denominator :math:`m` is replaced with t",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:165127,Performance,load,loadings,165127,"the sample correlation or genetic relationship matrix (GRM) as simply :math:`MM^T`. PCA then computes the SVD. .. math::. M = USV^T. where columns of :math:`U` are left singular vectors (orthonormal in :math:`\mathbb{R}^n`), columns of :math:`V` are right singular vectors (orthonormal in :math:`\mathbb{R}^m`), and :math:`S=\mathrm{diag}(s_1, s_2, \ldots)` with ordered singular values :math:`s_1 \ge s_2 \ge \cdots \ge 0`. Typically one computes only the first :math:`k` singular vectors and values, yielding the best rank :math:`k` approximation :math:`U_k S_k V_k^T` of :math:`M`; the truncations :math:`U_k`, :math:`S_k` and :math:`V_k` are :math:`n \\times k`, :math:`k \\times k` and :math:`m \\times k` respectively. From the perspective of the samples or rows of :math:`M` as data, :math:`V_k` contains the variant loadings for the first :math:`k` PCs while :math:`MV_k = U_k S_k` contains the first :math:`k` PC scores of each sample. The loadings represent a new basis of features while the scores represent the projected data on those features. The eigenvalues of the GRM :math:`MM^T` are the squares of the singular values :math:`s_1^2, s_2^2, \ldots`, which represent the variances carried by the respective PCs. By default, Hail only computes the loadings if the ``loadings`` parameter is specified. *Note:* In PLINK/GCTA the GRM is taken as the starting point and it is computed slightly differently with regard to missing data. Here the :math:`ij` entry of :math:`MM^T` is simply the dot product of rows :math:`i` and :math:`j` of :math:`M`; in terms of :math:`C` it is. .. math::. \\frac{1}{m}\sum_{l\in\mathcal{C}_i\cap\mathcal{C}_j}\\frac{(C_{il}-2p_l)(C_{jl} - 2p_l)}{2p_l(1-p_l)}. where :math:`\mathcal{C}_i = \{l \mid C_{il} \\text{ is non-missing}\}`. In PLINK/GCTA the denominator :math:`m` is replaced with the number of terms in the sum :math:`\\lvert\mathcal{C}_i\cap\\mathcal{C}_j\\rvert`, i.e. the number of variants where both samples have non-missing genotypes. While ",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:165440,Performance,load,loadings,165440,"rs (orthonormal in :math:`\mathbb{R}^m`), and :math:`S=\mathrm{diag}(s_1, s_2, \ldots)` with ordered singular values :math:`s_1 \ge s_2 \ge \cdots \ge 0`. Typically one computes only the first :math:`k` singular vectors and values, yielding the best rank :math:`k` approximation :math:`U_k S_k V_k^T` of :math:`M`; the truncations :math:`U_k`, :math:`S_k` and :math:`V_k` are :math:`n \\times k`, :math:`k \\times k` and :math:`m \\times k` respectively. From the perspective of the samples or rows of :math:`M` as data, :math:`V_k` contains the variant loadings for the first :math:`k` PCs while :math:`MV_k = U_k S_k` contains the first :math:`k` PC scores of each sample. The loadings represent a new basis of features while the scores represent the projected data on those features. The eigenvalues of the GRM :math:`MM^T` are the squares of the singular values :math:`s_1^2, s_2^2, \ldots`, which represent the variances carried by the respective PCs. By default, Hail only computes the loadings if the ``loadings`` parameter is specified. *Note:* In PLINK/GCTA the GRM is taken as the starting point and it is computed slightly differently with regard to missing data. Here the :math:`ij` entry of :math:`MM^T` is simply the dot product of rows :math:`i` and :math:`j` of :math:`M`; in terms of :math:`C` it is. .. math::. \\frac{1}{m}\sum_{l\in\mathcal{C}_i\cap\mathcal{C}_j}\\frac{(C_{il}-2p_l)(C_{jl} - 2p_l)}{2p_l(1-p_l)}. where :math:`\mathcal{C}_i = \{l \mid C_{il} \\text{ is non-missing}\}`. In PLINK/GCTA the denominator :math:`m` is replaced with the number of terms in the sum :math:`\\lvert\mathcal{C}_i\cap\\mathcal{C}_j\\rvert`, i.e. the number of variants where both samples have non-missing genotypes. While this is arguably a better estimator of the true GRM (trading shrinkage for noise), it has the drawback that one loses the clean interpretation of the loadings and scores as features and projections. Separately, for the PCs PLINK/GCTA output the eigenvectors of the GRM; ",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:165458,Performance,load,loadings,165458,"rs (orthonormal in :math:`\mathbb{R}^m`), and :math:`S=\mathrm{diag}(s_1, s_2, \ldots)` with ordered singular values :math:`s_1 \ge s_2 \ge \cdots \ge 0`. Typically one computes only the first :math:`k` singular vectors and values, yielding the best rank :math:`k` approximation :math:`U_k S_k V_k^T` of :math:`M`; the truncations :math:`U_k`, :math:`S_k` and :math:`V_k` are :math:`n \\times k`, :math:`k \\times k` and :math:`m \\times k` respectively. From the perspective of the samples or rows of :math:`M` as data, :math:`V_k` contains the variant loadings for the first :math:`k` PCs while :math:`MV_k = U_k S_k` contains the first :math:`k` PC scores of each sample. The loadings represent a new basis of features while the scores represent the projected data on those features. The eigenvalues of the GRM :math:`MM^T` are the squares of the singular values :math:`s_1^2, s_2^2, \ldots`, which represent the variances carried by the respective PCs. By default, Hail only computes the loadings if the ``loadings`` parameter is specified. *Note:* In PLINK/GCTA the GRM is taken as the starting point and it is computed slightly differently with regard to missing data. Here the :math:`ij` entry of :math:`MM^T` is simply the dot product of rows :math:`i` and :math:`j` of :math:`M`; in terms of :math:`C` it is. .. math::. \\frac{1}{m}\sum_{l\in\mathcal{C}_i\cap\mathcal{C}_j}\\frac{(C_{il}-2p_l)(C_{jl} - 2p_l)}{2p_l(1-p_l)}. where :math:`\mathcal{C}_i = \{l \mid C_{il} \\text{ is non-missing}\}`. In PLINK/GCTA the denominator :math:`m` is replaced with the number of terms in the sum :math:`\\lvert\mathcal{C}_i\cap\\mathcal{C}_j\\rvert`, i.e. the number of variants where both samples have non-missing genotypes. While this is arguably a better estimator of the true GRM (trading shrinkage for noise), it has the drawback that one loses the clean interpretation of the loadings and scores as features and projections. Separately, for the PCs PLINK/GCTA output the eigenvectors of the GRM; ",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:166328,Performance,load,loadings,166328,"` are the squares of the singular values :math:`s_1^2, s_2^2, \ldots`, which represent the variances carried by the respective PCs. By default, Hail only computes the loadings if the ``loadings`` parameter is specified. *Note:* In PLINK/GCTA the GRM is taken as the starting point and it is computed slightly differently with regard to missing data. Here the :math:`ij` entry of :math:`MM^T` is simply the dot product of rows :math:`i` and :math:`j` of :math:`M`; in terms of :math:`C` it is. .. math::. \\frac{1}{m}\sum_{l\in\mathcal{C}_i\cap\mathcal{C}_j}\\frac{(C_{il}-2p_l)(C_{jl} - 2p_l)}{2p_l(1-p_l)}. where :math:`\mathcal{C}_i = \{l \mid C_{il} \\text{ is non-missing}\}`. In PLINK/GCTA the denominator :math:`m` is replaced with the number of terms in the sum :math:`\\lvert\mathcal{C}_i\cap\\mathcal{C}_j\\rvert`, i.e. the number of variants where both samples have non-missing genotypes. While this is arguably a better estimator of the true GRM (trading shrinkage for noise), it has the drawback that one loses the clean interpretation of the loadings and scores as features and projections. Separately, for the PCs PLINK/GCTA output the eigenvectors of the GRM; even ignoring the above discrepancy that means the left singular vectors :math:`U_k` instead of the component scores :math:`U_k S_k`. While this is just a matter of the scale on each PC, the scores have the advantage of representing true projections of the data onto features with the variance of a score reflecting the variance explained by the corresponding feature. (In PC bi-plots this amounts to a change in aspect ratio; for use of PCs as covariates in regression it is immaterial.). **Annotations**. Given root ``scores='sa.scores'`` and ``as_array=False``, :py:meth:`~hail.VariantDataset.pca` adds a Struct to sample annotations:. - **sa.scores** (*Struct*) -- Struct of sample scores. With ``k=3``, the Struct has three field:. - **sa.scores.PC1** (*Double*) -- Score from first PC. - **sa.scores.PC2** (*Double*) -- ",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:167437,Performance,load,loadings,167437,"e eigenvectors of the GRM; even ignoring the above discrepancy that means the left singular vectors :math:`U_k` instead of the component scores :math:`U_k S_k`. While this is just a matter of the scale on each PC, the scores have the advantage of representing true projections of the data onto features with the variance of a score reflecting the variance explained by the corresponding feature. (In PC bi-plots this amounts to a change in aspect ratio; for use of PCs as covariates in regression it is immaterial.). **Annotations**. Given root ``scores='sa.scores'`` and ``as_array=False``, :py:meth:`~hail.VariantDataset.pca` adds a Struct to sample annotations:. - **sa.scores** (*Struct*) -- Struct of sample scores. With ``k=3``, the Struct has three field:. - **sa.scores.PC1** (*Double*) -- Score from first PC. - **sa.scores.PC2** (*Double*) -- Score from second PC. - **sa.scores.PC3** (*Double*) -- Score from third PC. Analogous variant and global annotations of type Struct are added by specifying the ``loadings`` and ``eigenvalues`` arguments, respectively. Given roots ``scores='sa.scores'``, ``loadings='va.loadings'``, and ``eigenvalues='global.evals'``, and ``as_array=True``, :py:meth:`~hail.VariantDataset.pca` adds the following annotations:. - **sa.scores** (*Array[Double]*) -- Array of sample scores from the top k PCs. - **va.loadings** (*Array[Double]*) -- Array of variant loadings in the top k PCs. - **global.evals** (*Array[Double]*) -- Array of the top k eigenvalues. :param str scores: Sample annotation path to store scores. :param loadings: Variant annotation path to store site loadings.; :type loadings: str or None. :param eigenvalues: Global annotation path to store eigenvalues.; :type eigenvalues: str or None. :param k: Number of principal components.; :type k: int or None. :param bool as_array: Store annotations as type Array rather than Struct; :type k: bool or None. :return: Dataset with new PCA annotations.; :rtype: :class:`.VariantDataset`; """""". jvds",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:167531,Performance,load,loadings,167531,"` instead of the component scores :math:`U_k S_k`. While this is just a matter of the scale on each PC, the scores have the advantage of representing true projections of the data onto features with the variance of a score reflecting the variance explained by the corresponding feature. (In PC bi-plots this amounts to a change in aspect ratio; for use of PCs as covariates in regression it is immaterial.). **Annotations**. Given root ``scores='sa.scores'`` and ``as_array=False``, :py:meth:`~hail.VariantDataset.pca` adds a Struct to sample annotations:. - **sa.scores** (*Struct*) -- Struct of sample scores. With ``k=3``, the Struct has three field:. - **sa.scores.PC1** (*Double*) -- Score from first PC. - **sa.scores.PC2** (*Double*) -- Score from second PC. - **sa.scores.PC3** (*Double*) -- Score from third PC. Analogous variant and global annotations of type Struct are added by specifying the ``loadings`` and ``eigenvalues`` arguments, respectively. Given roots ``scores='sa.scores'``, ``loadings='va.loadings'``, and ``eigenvalues='global.evals'``, and ``as_array=True``, :py:meth:`~hail.VariantDataset.pca` adds the following annotations:. - **sa.scores** (*Array[Double]*) -- Array of sample scores from the top k PCs. - **va.loadings** (*Array[Double]*) -- Array of variant loadings in the top k PCs. - **global.evals** (*Array[Double]*) -- Array of the top k eigenvalues. :param str scores: Sample annotation path to store scores. :param loadings: Variant annotation path to store site loadings.; :type loadings: str or None. :param eigenvalues: Global annotation path to store eigenvalues.; :type eigenvalues: str or None. :param k: Number of principal components.; :type k: int or None. :param bool as_array: Store annotations as type Array rather than Struct; :type k: bool or None. :return: Dataset with new PCA annotations.; :rtype: :class:`.VariantDataset`; """""". jvds = self._jvdf.pca(scores, k, joption(loadings), joption(eigenvalues), as_array); return VariantDataset(self.hc",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:167544,Performance,load,loadings,167544,"s :math:`U_k S_k`. While this is just a matter of the scale on each PC, the scores have the advantage of representing true projections of the data onto features with the variance of a score reflecting the variance explained by the corresponding feature. (In PC bi-plots this amounts to a change in aspect ratio; for use of PCs as covariates in regression it is immaterial.). **Annotations**. Given root ``scores='sa.scores'`` and ``as_array=False``, :py:meth:`~hail.VariantDataset.pca` adds a Struct to sample annotations:. - **sa.scores** (*Struct*) -- Struct of sample scores. With ``k=3``, the Struct has three field:. - **sa.scores.PC1** (*Double*) -- Score from first PC. - **sa.scores.PC2** (*Double*) -- Score from second PC. - **sa.scores.PC3** (*Double*) -- Score from third PC. Analogous variant and global annotations of type Struct are added by specifying the ``loadings`` and ``eigenvalues`` arguments, respectively. Given roots ``scores='sa.scores'``, ``loadings='va.loadings'``, and ``eigenvalues='global.evals'``, and ``as_array=True``, :py:meth:`~hail.VariantDataset.pca` adds the following annotations:. - **sa.scores** (*Array[Double]*) -- Array of sample scores from the top k PCs. - **va.loadings** (*Array[Double]*) -- Array of variant loadings in the top k PCs. - **global.evals** (*Array[Double]*) -- Array of the top k eigenvalues. :param str scores: Sample annotation path to store scores. :param loadings: Variant annotation path to store site loadings.; :type loadings: str or None. :param eigenvalues: Global annotation path to store eigenvalues.; :type eigenvalues: str or None. :param k: Number of principal components.; :type k: int or None. :param bool as_array: Store annotations as type Array rather than Struct; :type k: bool or None. :return: Dataset with new PCA annotations.; :rtype: :class:`.VariantDataset`; """""". jvds = self._jvdf.pca(scores, k, joption(loadings), joption(eigenvalues), as_array); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @ty",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:167772,Performance,load,loadings,167772,"eature. (In PC bi-plots this amounts to a change in aspect ratio; for use of PCs as covariates in regression it is immaterial.). **Annotations**. Given root ``scores='sa.scores'`` and ``as_array=False``, :py:meth:`~hail.VariantDataset.pca` adds a Struct to sample annotations:. - **sa.scores** (*Struct*) -- Struct of sample scores. With ``k=3``, the Struct has three field:. - **sa.scores.PC1** (*Double*) -- Score from first PC. - **sa.scores.PC2** (*Double*) -- Score from second PC. - **sa.scores.PC3** (*Double*) -- Score from third PC. Analogous variant and global annotations of type Struct are added by specifying the ``loadings`` and ``eigenvalues`` arguments, respectively. Given roots ``scores='sa.scores'``, ``loadings='va.loadings'``, and ``eigenvalues='global.evals'``, and ``as_array=True``, :py:meth:`~hail.VariantDataset.pca` adds the following annotations:. - **sa.scores** (*Array[Double]*) -- Array of sample scores from the top k PCs. - **va.loadings** (*Array[Double]*) -- Array of variant loadings in the top k PCs. - **global.evals** (*Array[Double]*) -- Array of the top k eigenvalues. :param str scores: Sample annotation path to store scores. :param loadings: Variant annotation path to store site loadings.; :type loadings: str or None. :param eigenvalues: Global annotation path to store eigenvalues.; :type eigenvalues: str or None. :param k: Number of principal components.; :type k: int or None. :param bool as_array: Store annotations as type Array rather than Struct; :type k: bool or None. :return: Dataset with new PCA annotations.; :rtype: :class:`.VariantDataset`; """""". jvds = self._jvdf.pca(scores, k, joption(loadings), joption(eigenvalues), as_array); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(k=integral,; maf=numeric,; block_size=integral,; min_kinship=numeric,; statistics=enumeration(""phi"", ""phik2"", ""phik2k0"", ""all"")); def pc_relate(self, k, maf, block_size=512, min_kinship=-float(""inf""), statistics=""all""):; """"""Compute",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:167821,Performance,load,loadings,167821,"eature. (In PC bi-plots this amounts to a change in aspect ratio; for use of PCs as covariates in regression it is immaterial.). **Annotations**. Given root ``scores='sa.scores'`` and ``as_array=False``, :py:meth:`~hail.VariantDataset.pca` adds a Struct to sample annotations:. - **sa.scores** (*Struct*) -- Struct of sample scores. With ``k=3``, the Struct has three field:. - **sa.scores.PC1** (*Double*) -- Score from first PC. - **sa.scores.PC2** (*Double*) -- Score from second PC. - **sa.scores.PC3** (*Double*) -- Score from third PC. Analogous variant and global annotations of type Struct are added by specifying the ``loadings`` and ``eigenvalues`` arguments, respectively. Given roots ``scores='sa.scores'``, ``loadings='va.loadings'``, and ``eigenvalues='global.evals'``, and ``as_array=True``, :py:meth:`~hail.VariantDataset.pca` adds the following annotations:. - **sa.scores** (*Array[Double]*) -- Array of sample scores from the top k PCs. - **va.loadings** (*Array[Double]*) -- Array of variant loadings in the top k PCs. - **global.evals** (*Array[Double]*) -- Array of the top k eigenvalues. :param str scores: Sample annotation path to store scores. :param loadings: Variant annotation path to store site loadings.; :type loadings: str or None. :param eigenvalues: Global annotation path to store eigenvalues.; :type eigenvalues: str or None. :param k: Number of principal components.; :type k: int or None. :param bool as_array: Store annotations as type Array rather than Struct; :type k: bool or None. :return: Dataset with new PCA annotations.; :rtype: :class:`.VariantDataset`; """""". jvds = self._jvdf.pca(scores, k, joption(loadings), joption(eigenvalues), as_array); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(k=integral,; maf=numeric,; block_size=integral,; min_kinship=numeric,; statistics=enumeration(""phi"", ""phik2"", ""phik2k0"", ""all"")); def pc_relate(self, k, maf, block_size=512, min_kinship=-float(""inf""), statistics=""all""):; """"""Compute",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:167986,Performance,load,loadings,167986,"`, :py:meth:`~hail.VariantDataset.pca` adds a Struct to sample annotations:. - **sa.scores** (*Struct*) -- Struct of sample scores. With ``k=3``, the Struct has three field:. - **sa.scores.PC1** (*Double*) -- Score from first PC. - **sa.scores.PC2** (*Double*) -- Score from second PC. - **sa.scores.PC3** (*Double*) -- Score from third PC. Analogous variant and global annotations of type Struct are added by specifying the ``loadings`` and ``eigenvalues`` arguments, respectively. Given roots ``scores='sa.scores'``, ``loadings='va.loadings'``, and ``eigenvalues='global.evals'``, and ``as_array=True``, :py:meth:`~hail.VariantDataset.pca` adds the following annotations:. - **sa.scores** (*Array[Double]*) -- Array of sample scores from the top k PCs. - **va.loadings** (*Array[Double]*) -- Array of variant loadings in the top k PCs. - **global.evals** (*Array[Double]*) -- Array of the top k eigenvalues. :param str scores: Sample annotation path to store scores. :param loadings: Variant annotation path to store site loadings.; :type loadings: str or None. :param eigenvalues: Global annotation path to store eigenvalues.; :type eigenvalues: str or None. :param k: Number of principal components.; :type k: int or None. :param bool as_array: Store annotations as type Array rather than Struct; :type k: bool or None. :return: Dataset with new PCA annotations.; :rtype: :class:`.VariantDataset`; """""". jvds = self._jvdf.pca(scores, k, joption(loadings), joption(eigenvalues), as_array); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(k=integral,; maf=numeric,; block_size=integral,; min_kinship=numeric,; statistics=enumeration(""phi"", ""phik2"", ""phik2k0"", ""all"")); def pc_relate(self, k, maf, block_size=512, min_kinship=-float(""inf""), statistics=""all""):; """"""Compute relatedness estimates between individuals using a variant of the; PC-Relate method. .. include:: experimental.rst. **Examples**. Estimate kinship, identity-by-descent two, identity-by-descent one, and",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:168034,Performance,load,loadings,168034,"`, :py:meth:`~hail.VariantDataset.pca` adds a Struct to sample annotations:. - **sa.scores** (*Struct*) -- Struct of sample scores. With ``k=3``, the Struct has three field:. - **sa.scores.PC1** (*Double*) -- Score from first PC. - **sa.scores.PC2** (*Double*) -- Score from second PC. - **sa.scores.PC3** (*Double*) -- Score from third PC. Analogous variant and global annotations of type Struct are added by specifying the ``loadings`` and ``eigenvalues`` arguments, respectively. Given roots ``scores='sa.scores'``, ``loadings='va.loadings'``, and ``eigenvalues='global.evals'``, and ``as_array=True``, :py:meth:`~hail.VariantDataset.pca` adds the following annotations:. - **sa.scores** (*Array[Double]*) -- Array of sample scores from the top k PCs. - **va.loadings** (*Array[Double]*) -- Array of variant loadings in the top k PCs. - **global.evals** (*Array[Double]*) -- Array of the top k eigenvalues. :param str scores: Sample annotation path to store scores. :param loadings: Variant annotation path to store site loadings.; :type loadings: str or None. :param eigenvalues: Global annotation path to store eigenvalues.; :type eigenvalues: str or None. :param k: Number of principal components.; :type k: int or None. :param bool as_array: Store annotations as type Array rather than Struct; :type k: bool or None. :return: Dataset with new PCA annotations.; :rtype: :class:`.VariantDataset`; """""". jvds = self._jvdf.pca(scores, k, joption(loadings), joption(eigenvalues), as_array); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(k=integral,; maf=numeric,; block_size=integral,; min_kinship=numeric,; statistics=enumeration(""phi"", ""phik2"", ""phik2k0"", ""all"")); def pc_relate(self, k, maf, block_size=512, min_kinship=-float(""inf""), statistics=""all""):; """"""Compute relatedness estimates between individuals using a variant of the; PC-Relate method. .. include:: experimental.rst. **Examples**. Estimate kinship, identity-by-descent two, identity-by-descent one, and",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:168051,Performance,load,loadings,168051,"ruct to sample annotations:. - **sa.scores** (*Struct*) -- Struct of sample scores. With ``k=3``, the Struct has three field:. - **sa.scores.PC1** (*Double*) -- Score from first PC. - **sa.scores.PC2** (*Double*) -- Score from second PC. - **sa.scores.PC3** (*Double*) -- Score from third PC. Analogous variant and global annotations of type Struct are added by specifying the ``loadings`` and ``eigenvalues`` arguments, respectively. Given roots ``scores='sa.scores'``, ``loadings='va.loadings'``, and ``eigenvalues='global.evals'``, and ``as_array=True``, :py:meth:`~hail.VariantDataset.pca` adds the following annotations:. - **sa.scores** (*Array[Double]*) -- Array of sample scores from the top k PCs. - **va.loadings** (*Array[Double]*) -- Array of variant loadings in the top k PCs. - **global.evals** (*Array[Double]*) -- Array of the top k eigenvalues. :param str scores: Sample annotation path to store scores. :param loadings: Variant annotation path to store site loadings.; :type loadings: str or None. :param eigenvalues: Global annotation path to store eigenvalues.; :type eigenvalues: str or None. :param k: Number of principal components.; :type k: int or None. :param bool as_array: Store annotations as type Array rather than Struct; :type k: bool or None. :return: Dataset with new PCA annotations.; :rtype: :class:`.VariantDataset`; """""". jvds = self._jvdf.pca(scores, k, joption(loadings), joption(eigenvalues), as_array); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(k=integral,; maf=numeric,; block_size=integral,; min_kinship=numeric,; statistics=enumeration(""phi"", ""phik2"", ""phik2k0"", ""all"")); def pc_relate(self, k, maf, block_size=512, min_kinship=-float(""inf""), statistics=""all""):; """"""Compute relatedness estimates between individuals using a variant of the; PC-Relate method. .. include:: experimental.rst. **Examples**. Estimate kinship, identity-by-descent two, identity-by-descent one, and; identity-by-descent zero for every pair of sa",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:168458,Performance,load,loadings,168458,"pectively. Given roots ``scores='sa.scores'``, ``loadings='va.loadings'``, and ``eigenvalues='global.evals'``, and ``as_array=True``, :py:meth:`~hail.VariantDataset.pca` adds the following annotations:. - **sa.scores** (*Array[Double]*) -- Array of sample scores from the top k PCs. - **va.loadings** (*Array[Double]*) -- Array of variant loadings in the top k PCs. - **global.evals** (*Array[Double]*) -- Array of the top k eigenvalues. :param str scores: Sample annotation path to store scores. :param loadings: Variant annotation path to store site loadings.; :type loadings: str or None. :param eigenvalues: Global annotation path to store eigenvalues.; :type eigenvalues: str or None. :param k: Number of principal components.; :type k: int or None. :param bool as_array: Store annotations as type Array rather than Struct; :type k: bool or None. :return: Dataset with new PCA annotations.; :rtype: :class:`.VariantDataset`; """""". jvds = self._jvdf.pca(scores, k, joption(loadings), joption(eigenvalues), as_array); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(k=integral,; maf=numeric,; block_size=integral,; min_kinship=numeric,; statistics=enumeration(""phi"", ""phik2"", ""phik2k0"", ""all"")); def pc_relate(self, k, maf, block_size=512, min_kinship=-float(""inf""), statistics=""all""):; """"""Compute relatedness estimates between individuals using a variant of the; PC-Relate method. .. include:: experimental.rst. **Examples**. Estimate kinship, identity-by-descent two, identity-by-descent one, and; identity-by-descent zero for every pair of samples, using 5 prinicpal; components to correct for ancestral populations, and a minimum minor; allele frequency filter of 0.01:. >>> rel = vds.pc_relate(5, 0.01). Calculate values as above, but when performing distributed matrix; multiplications use a matrix-block-size of 1024 by 1024. >>> rel = vds.pc_relate(5, 0.01, 1024). Calculate values as above, excluding sample-pairs with kinship less; than 0.1. This is more effi",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:169258,Performance,perform,performing,169258,"e Array rather than Struct; :type k: bool or None. :return: Dataset with new PCA annotations.; :rtype: :class:`.VariantDataset`; """""". jvds = self._jvdf.pca(scores, k, joption(loadings), joption(eigenvalues), as_array); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(k=integral,; maf=numeric,; block_size=integral,; min_kinship=numeric,; statistics=enumeration(""phi"", ""phik2"", ""phik2k0"", ""all"")); def pc_relate(self, k, maf, block_size=512, min_kinship=-float(""inf""), statistics=""all""):; """"""Compute relatedness estimates between individuals using a variant of the; PC-Relate method. .. include:: experimental.rst. **Examples**. Estimate kinship, identity-by-descent two, identity-by-descent one, and; identity-by-descent zero for every pair of samples, using 5 prinicpal; components to correct for ancestral populations, and a minimum minor; allele frequency filter of 0.01:. >>> rel = vds.pc_relate(5, 0.01). Calculate values as above, but when performing distributed matrix; multiplications use a matrix-block-size of 1024 by 1024. >>> rel = vds.pc_relate(5, 0.01, 1024). Calculate values as above, excluding sample-pairs with kinship less; than 0.1. This is more efficient than producing the full key table and; filtering using :py:meth:`~hail.KeyTable.filter`. >>> rel = vds.pc_relate(5, 0.01, min_kinship=0.1). **Method**. The traditional estimator for kinship between a pair of individuals; :math:`i` and :math:`j`, sharing the set :math:`S_{ij}` of; single-nucleotide variants, from a population with allele frequencies; :math:`p_s`, is given by:. .. math::. \\widehat{\phi_{ij}} := \\frac{1}{|S_{ij}|}\\sum_{s \in S_{ij}}\\frac{(g_{is} - 2 p_s) (g_{js} - 2 p_s)}{4 * \sum_{s \in S_{ij} p_s (1 - p_s)}}. This estimator is true under the model that the sharing of common; (relative to the population) alleles is not very informative to; relatedness (because they're common) and the sharing of rare alleles; suggests a recent common ancestor from which the allele wa",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:174389,Performance,perform,perform,174389,"BS}^{(0)}_{ij}}; {\sum_{s \in S_{ij}} \\widehat{\\mu_{is}}^2(1 - \\widehat{\\mu_{js}})^2 + (1 - \\widehat{\\mu_{is}})^2\\widehat{\\mu_{js}}^2}; & \\widehat{\phi_{ij}} > 2^{-5/2} \\\\; 1 - 4 \\widehat{\phi_{ij}} + k^{(2)}_{ij}; & \\widehat{\phi_{ij}} \le 2^{-5/2}; \\end{cases}. The estimator for identity-by-descent one is given by:. .. math::. \\widehat{k^{(1)}_{ij}} := 1 - \\widehat{k^{(2)}_{ij}} - \\widehat{k^{(0)}_{ij}}. **Details**. The PC-Relate method is described in ""Model-free Estimation of Recent; Genetic Relatedness"". Conomos MP, Reiner AP, Weir BS, Thornton TA. in; American Journal of Human Genetics. 2016 Jan 7. The reference; implementation is available in the `GENESIS Bioconductor package; <https://bioconductor.org/packages/release/bioc/html/GENESIS.html>`_ . :py:meth:`~hail.VariantDataset.pc_relate` differs from the reference; implementation in a couple key ways:. - the principal components analysis does not use an unrelated set of; individuals. - the estimators do not perform small sample correction. - the algorithm does not provide an option to use population-wide; allele frequency estimates. - the algorithm does not provide an option to not use ""overall; standardization"" (see R ``pcrelate`` documentation). **Notes**. The ``block_size`` controls memory usage and parallelism. If it is large; enough to hold an entire sample-by-sample matrix of 64-bit doubles in; memory, then only one Spark worker node can be used to compute matrix; operations. If it is too small, communication overhead will begin to; dominate the computation's time. The author has found that on Google; Dataproc (where each core has about 3.75GB of memory), setting; ``block_size`` larger than 512 tends to cause memory exhaustion errors. The minimum allele frequency filter is applied per-pair: if either of; the two individual's individual-specific minor allele frequency is below; the threshold, then the variant's contribution to relatedness estimates; is zero. Under the PC-Relate model, ki",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:178055,Performance,cache,cache,178055,"identity-by-descent two; statistics, 'phik2k0' will compute the kinship; statistics and both identity-by-descent two and; zero, 'all' computes the kinship statistic and; all three identity-by-descent statistics. :return: A :py:class:`.KeyTable` mapping pairs of samples to estimations; of their kinship and identity-by-descent zero, one, and two.; :rtype: :py:class:`.KeyTable`. """""". intstatistics = { ""phi"" : 0, ""phik2"" : 1, ""phik2k0"" : 2, ""all"" : 3 }[statistics]. return KeyTable(self.hc, self._jvdf.pcRelate(k, maf, block_size, min_kinship, intstatistics)). [docs] @handle_py4j; @typecheck_method(storage_level=strlike); def persist(self, storage_level=""MEMORY_AND_DISK""):; """"""Persist this variant dataset to memory and/or disk. **Examples**. Persist the variant dataset to both memory and disk:. >>> vds_result = vds.persist(). **Notes**. The :py:meth:`~hail.VariantDataset.persist` and :py:meth:`~hail.VariantDataset.cache` methods ; allow you to store the current dataset on disk or in memory to avoid redundant computation and ; improve the performance of Hail pipelines. :py:meth:`~hail.VariantDataset.cache` is an alias for ; :func:`persist(""MEMORY_ONLY"") <hail.VariantDataset.persist>`. Most users will want ""MEMORY_AND_DISK"".; See the `Spark documentation <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__ ; for a more in-depth discussion of persisting data.; ; .. warning ::; ; Persist, like all other :class:`.VariantDataset` functions, is functional.; Its output must be captured. This is wrong:; ; >>> vds = vds.linreg('sa.phenotype') # doctest: +SKIP; >>> vds.persist() # doctest: +SKIP; ; The above code does NOT persist ``vds``. Instead, it copies ``vds`` and persists that result. ; The proper usage is this:; ; >>> vds = vds.pca().persist() # doctest: +SKIP. :param storage_level: Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DI",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:178181,Performance,perform,performance,178181,"identity-by-descent two; statistics, 'phik2k0' will compute the kinship; statistics and both identity-by-descent two and; zero, 'all' computes the kinship statistic and; all three identity-by-descent statistics. :return: A :py:class:`.KeyTable` mapping pairs of samples to estimations; of their kinship and identity-by-descent zero, one, and two.; :rtype: :py:class:`.KeyTable`. """""". intstatistics = { ""phi"" : 0, ""phik2"" : 1, ""phik2k0"" : 2, ""all"" : 3 }[statistics]. return KeyTable(self.hc, self._jvdf.pcRelate(k, maf, block_size, min_kinship, intstatistics)). [docs] @handle_py4j; @typecheck_method(storage_level=strlike); def persist(self, storage_level=""MEMORY_AND_DISK""):; """"""Persist this variant dataset to memory and/or disk. **Examples**. Persist the variant dataset to both memory and disk:. >>> vds_result = vds.persist(). **Notes**. The :py:meth:`~hail.VariantDataset.persist` and :py:meth:`~hail.VariantDataset.cache` methods ; allow you to store the current dataset on disk or in memory to avoid redundant computation and ; improve the performance of Hail pipelines. :py:meth:`~hail.VariantDataset.cache` is an alias for ; :func:`persist(""MEMORY_ONLY"") <hail.VariantDataset.persist>`. Most users will want ""MEMORY_AND_DISK"".; See the `Spark documentation <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__ ; for a more in-depth discussion of persisting data.; ; .. warning ::; ; Persist, like all other :class:`.VariantDataset` functions, is functional.; Its output must be captured. This is wrong:; ; >>> vds = vds.linreg('sa.phenotype') # doctest: +SKIP; >>> vds.persist() # doctest: +SKIP; ; The above code does NOT persist ``vds``. Instead, it copies ``vds`` and persists that result. ; The proper usage is this:; ; >>> vds = vds.pca().persist() # doctest: +SKIP. :param storage_level: Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DI",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:178243,Performance,cache,cache,178243,"es the kinship statistic and; all three identity-by-descent statistics. :return: A :py:class:`.KeyTable` mapping pairs of samples to estimations; of their kinship and identity-by-descent zero, one, and two.; :rtype: :py:class:`.KeyTable`. """""". intstatistics = { ""phi"" : 0, ""phik2"" : 1, ""phik2k0"" : 2, ""all"" : 3 }[statistics]. return KeyTable(self.hc, self._jvdf.pcRelate(k, maf, block_size, min_kinship, intstatistics)). [docs] @handle_py4j; @typecheck_method(storage_level=strlike); def persist(self, storage_level=""MEMORY_AND_DISK""):; """"""Persist this variant dataset to memory and/or disk. **Examples**. Persist the variant dataset to both memory and disk:. >>> vds_result = vds.persist(). **Notes**. The :py:meth:`~hail.VariantDataset.persist` and :py:meth:`~hail.VariantDataset.cache` methods ; allow you to store the current dataset on disk or in memory to avoid redundant computation and ; improve the performance of Hail pipelines. :py:meth:`~hail.VariantDataset.cache` is an alias for ; :func:`persist(""MEMORY_ONLY"") <hail.VariantDataset.persist>`. Most users will want ""MEMORY_AND_DISK"".; See the `Spark documentation <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__ ; for a more in-depth discussion of persisting data.; ; .. warning ::; ; Persist, like all other :class:`.VariantDataset` functions, is functional.; Its output must be captured. This is wrong:; ; >>> vds = vds.linreg('sa.phenotype') # doctest: +SKIP; >>> vds.persist() # doctest: +SKIP; ; The above code does NOT persist ``vds``. Instead, it copies ``vds`` and persists that result. ; The proper usage is this:; ; >>> vds = vds.pca().persist() # doctest: +SKIP. :param storage_level: Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP; ; :rtype: :class:`.VariantDataset`; """""". return VariantDataset(self.hc, self._jvdf.persist(storage_l",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:179613,Performance,perform,performed,179613,"Persist, like all other :class:`.VariantDataset` functions, is functional.; Its output must be captured. This is wrong:; ; >>> vds = vds.linreg('sa.phenotype') # doctest: +SKIP; >>> vds.persist() # doctest: +SKIP; ; The above code does NOT persist ``vds``. Instead, it copies ``vds`` and persists that result. ; The proper usage is this:; ; >>> vds = vds.pca().persist() # doctest: +SKIP. :param storage_level: Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP; ; :rtype: :class:`.VariantDataset`; """""". return VariantDataset(self.hc, self._jvdf.persist(storage_level)). [docs] def unpersist(self):; """"""; Unpersists this VDS from memory/disk.; ; **Notes**; This function will have no effect on a VDS that was not previously persisted.; ; There's nothing stopping you from continuing to use a VDS that has been unpersisted, but doing so will result in; all previous steps taken to compute the VDS being performed again since the VDS must be recomputed. Only unpersist; a VDS when you are done with it.; ; """"""; self._jvds.unpersist(). @property; @handle_py4j; def global_schema(self):; """"""; Returns the signature of the global annotations contained in this VDS. **Examples**. >>> print(vds.global_schema). The ``pprint`` module can be used to print the schema in a more human-readable format:. >>> from pprint import pprint; >>> pprint(vds.global_schema). :rtype: :class:`.Type`; """""". if self._global_schema is None:; self._global_schema = Type._from_java(self._jvds.globalSignature()); return self._global_schema. @property; @handle_py4j; def colkey_schema(self):; """"""; Returns the signature of the column key (sample) contained in this VDS. **Examples**. >>> print(vds.colkey_schema). The ``pprint`` module can be used to print the schema in a more human-readable format:. >>> from pprint import pprint; >>> pprint(vds.colkey_schema). :rtype: ",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:192087,Performance,perform,performance,192087,"m old to new sample IDs. :return: Dataset with remapped sample IDs.; :rtype: :class:`.VariantDataset`; """""". jvds = self._jvds.renameSamples(mapping); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(num_partitions=integral,; shuffle=bool); def repartition(self, num_partitions, shuffle=True):; """"""Increase or decrease the number of variant dataset partitions. **Examples**. Repartition the variant dataset to have 500 partitions:. >>> vds_result = vds.repartition(500). **Notes**. Check the current number of partitions with :py:meth:`.num_partitions`. The data in a variant dataset is divided into chunks called partitions, which may be stored together or across a network, so that each partition may be read and processed in parallel by available cores. When a variant dataset with :math:`M` variants is first imported, each of the :math:`k` partition will contain about :math:`M/k` of the variants. Since each partition has some computational overhead, decreasing the number of partitions can improve performance after significant filtering. Since it's recommended to have at least 2 - 4 partitions per core, increasing the number of partitions can allow one to take advantage of more cores. Partitions are a core concept of distributed computation in Spark, see `here <http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds>`__ for details. With ``shuffle=True``, Hail does a full shuffle of the data and creates equal sized partitions. With ``shuffle=False``, Hail combines existing partitions to avoid a full shuffle. These algorithms correspond to the ``repartition`` and ``coalesce`` commands in Spark, respectively. In particular, when ``shuffle=False``, ``num_partitions`` cannot exceed current number of partitions. :param int num_partitions: Desired number of partitions, must be less than the current number if ``shuffle=False``. :param bool shuffle: If true, use full shuffle to repartition. :return: Variant dataset wi",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:222465,Performance,cache,cache,222465,"<https://github.com/konradjk/loftee>`__; on the current variant dataset and adds the result as a variant annotation. **Examples**. Add VEP annotations to the dataset:. >>> vds_result = vds.vep(""data/vep.properties"") # doctest: +SKIP. **Configuration**. :py:meth:`~hail.VariantDataset.vep` needs a configuration file to tell it how to run; VEP. The format is a `.properties file <https://en.wikipedia.org/wiki/.properties>`__.; Roughly, each line defines a property as a key-value pair of the form `key = value`. `vep` supports the following properties:. - **hail.vep.perl** -- Location of Perl. Optional, default: perl.; - **hail.vep.perl5lib** -- Value for the PERL5LIB environment variable when invoking VEP. Optional, by default PERL5LIB is not set.; - **hail.vep.path** -- Value of the PATH environment variable when invoking VEP. Optional, by default PATH is not set.; - **hail.vep.location** -- Location of the VEP Perl script. Required.; - **hail.vep.cache_dir** -- Location of the VEP cache dir, passed to VEP with the `--dir` option. Required.; - **hail.vep.fasta** -- Location of the FASTA file to use to look up the reference sequence, passed to VEP with the `--fasta` option. Required.; - **hail.vep.assembly** -- Genome assembly version to use. Optional, default: GRCh37; - **hail.vep.plugin** -- VEP plugin, passed to VEP with the `--plugin` option. Optional. Overrides `hail.vep.lof.human_ancestor` and `hail.vep.lof.conservation_file`.; - **hail.vep.lof.human_ancestor** -- Location of the human ancestor file for the LOFTEE plugin. Ignored if `hail.vep.plugin` is set. Required otherwise.; - **hail.vep.lof.conservation_file** -- Location of the conservation file for the LOFTEE plugin. Ignored if `hail.vep.plugin` is set. Required otherwise. Here is an example `vep.properties` configuration file. .. code-block:: text. hail.vep.perl = /usr/bin/perl; hail.vep.path = /usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin; hail.vep.location = /path/to/vep/ensembl-tools-release-81/scripts/va",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:223848,Performance,cache,cache,223848,"gin` option. Optional. Overrides `hail.vep.lof.human_ancestor` and `hail.vep.lof.conservation_file`.; - **hail.vep.lof.human_ancestor** -- Location of the human ancestor file for the LOFTEE plugin. Ignored if `hail.vep.plugin` is set. Required otherwise.; - **hail.vep.lof.conservation_file** -- Location of the conservation file for the LOFTEE plugin. Ignored if `hail.vep.plugin` is set. Required otherwise. Here is an example `vep.properties` configuration file. .. code-block:: text. hail.vep.perl = /usr/bin/perl; hail.vep.path = /usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin; hail.vep.location = /path/to/vep/ensembl-tools-release-81/scripts/variant_effect_predictor/variant_effect_predictor.pl; hail.vep.cache_dir = /path/to/vep; hail.vep.lof.human_ancestor = /path/to/loftee_data/human_ancestor.fa.gz; hail.vep.lof.conservation_file = /path/to/loftee_data//phylocsf.sql. **VEP Invocation**. .. code-block:: text. <hail.vep.perl>; <hail.vep.location>; --format vcf; --json; --everything; --allele_number; --no_stats; --cache --offline; --dir <hail.vep.cache_dir>; --fasta <hail.vep.fasta>; --minimal; --assembly <hail.vep.assembly>; --plugin LoF,human_ancestor_fa:$<hail.vep.lof.human_ancestor>,filter_position:0.05,min_intron_size:15,conservation_file:<hail.vep.lof.conservation_file>; -o STDOUT. **Annotations**. Annotations with the following schema are placed in the location specified by ``root``.; The full resulting dataset schema can be queried with :py:attr:`~hail.VariantDataset.variant_schema`. .. code-block:: text. Struct{; assembly_name: String,; allele_string: String,; colocated_variants: Array[Struct{; aa_allele: String,; aa_maf: Double,; afr_allele: String,; afr_maf: Double,; allele_string: String,; amr_allele: String,; amr_maf: Double,; clin_sig: Array[String],; end: Int,; eas_allele: String,; eas_maf: Double,; ea_allele: String,,; ea_maf: Double,; eur_allele: String,; eur_maf: Double,; exac_adj_allele: String,; exac_adj_maf: Double,; exac_allele: String,; exac_afr_all",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:139834,Safety,predict,predicting,139834,"tDataset`; """""". jvds = self._jvdf.lmmreg(kinshipMatrix._jkm, y, jarray(Env.jvm().java.lang.String, covariates),; use_ml, global_root, va_root, run_assoc, joption(delta), sparsity_threshold,; use_dosages, joption(n_eigs), joption(dropped_variance_fraction)); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(test=strlike,; y=strlike,; covariates=listof(strlike),; root=strlike,; use_dosages=bool); def logreg(self, test, y, covariates=[], root='va.logreg', use_dosages=False):; """"""Test each variant for association using logistic regression. .. include:: requireTGenotype.rst. **Examples**. Run the logistic regression Wald test per variant using a Boolean phenotype and two covariates stored; in sample annotations:. >>> vds_result = vds.logreg('wald', 'sa.pheno.isCase', covariates=['sa.pheno.age', 'sa.pheno.isFemale']). **Notes**. The :py:meth:`~hail.VariantDataset.logreg` method performs,; for each variant, a significance test of the genotype in; predicting a binary (case-control) phenotype based on the; logistic regression model. The phenotype type must either be numeric (with all; present values 0 or 1) or Boolean, in which case true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'), Rao score test ('score'),; and Firth test ('firth'). Hail only includes samples for which the phenotype and all covariates are; defined. For each variant, Hail imputes missing genotypes as the mean of called genotypes. By default, genotypes values are given by hard call genotypes (``g.gt``).; If ``use_dosages=True``, then genotype values are defined by the dosage; :math:`\mathrm{P}(\mathrm{Het}) + 2 \cdot \mathrm{P}(\mathrm{HomVar})`. For Phred-scaled values,; :math:`\mathrm{P}(\mathrm{Het})` and :math:`\mathrm{P}(\mathrm{HomVar})` are; calculated by normalizing the PL likelihoods (converted from the Phred-scale) to sum to 1. The example above considers a model of the form. .. math::.",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:178135,Safety,avoid,avoid,178135,"identity-by-descent two; statistics, 'phik2k0' will compute the kinship; statistics and both identity-by-descent two and; zero, 'all' computes the kinship statistic and; all three identity-by-descent statistics. :return: A :py:class:`.KeyTable` mapping pairs of samples to estimations; of their kinship and identity-by-descent zero, one, and two.; :rtype: :py:class:`.KeyTable`. """""". intstatistics = { ""phi"" : 0, ""phik2"" : 1, ""phik2k0"" : 2, ""all"" : 3 }[statistics]. return KeyTable(self.hc, self._jvdf.pcRelate(k, maf, block_size, min_kinship, intstatistics)). [docs] @handle_py4j; @typecheck_method(storage_level=strlike); def persist(self, storage_level=""MEMORY_AND_DISK""):; """"""Persist this variant dataset to memory and/or disk. **Examples**. Persist the variant dataset to both memory and disk:. >>> vds_result = vds.persist(). **Notes**. The :py:meth:`~hail.VariantDataset.persist` and :py:meth:`~hail.VariantDataset.cache` methods ; allow you to store the current dataset on disk or in memory to avoid redundant computation and ; improve the performance of Hail pipelines. :py:meth:`~hail.VariantDataset.cache` is an alias for ; :func:`persist(""MEMORY_ONLY"") <hail.VariantDataset.persist>`. Most users will want ""MEMORY_AND_DISK"".; See the `Spark documentation <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__ ; for a more in-depth discussion of persisting data.; ; .. warning ::; ; Persist, like all other :class:`.VariantDataset` functions, is functional.; Its output must be captured. This is wrong:; ; >>> vds = vds.linreg('sa.phenotype') # doctest: +SKIP; >>> vds.persist() # doctest: +SKIP; ; The above code does NOT persist ``vds``. Instead, it copies ``vds`` and persists that result. ; The proper usage is this:; ; >>> vds = vds.pca().persist() # doctest: +SKIP. :param storage_level: Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DI",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:178141,Safety,redund,redundant,178141,"identity-by-descent two; statistics, 'phik2k0' will compute the kinship; statistics and both identity-by-descent two and; zero, 'all' computes the kinship statistic and; all three identity-by-descent statistics. :return: A :py:class:`.KeyTable` mapping pairs of samples to estimations; of their kinship and identity-by-descent zero, one, and two.; :rtype: :py:class:`.KeyTable`. """""". intstatistics = { ""phi"" : 0, ""phik2"" : 1, ""phik2k0"" : 2, ""all"" : 3 }[statistics]. return KeyTable(self.hc, self._jvdf.pcRelate(k, maf, block_size, min_kinship, intstatistics)). [docs] @handle_py4j; @typecheck_method(storage_level=strlike); def persist(self, storage_level=""MEMORY_AND_DISK""):; """"""Persist this variant dataset to memory and/or disk. **Examples**. Persist the variant dataset to both memory and disk:. >>> vds_result = vds.persist(). **Notes**. The :py:meth:`~hail.VariantDataset.persist` and :py:meth:`~hail.VariantDataset.cache` methods ; allow you to store the current dataset on disk or in memory to avoid redundant computation and ; improve the performance of Hail pipelines. :py:meth:`~hail.VariantDataset.cache` is an alias for ; :func:`persist(""MEMORY_ONLY"") <hail.VariantDataset.persist>`. Most users will want ""MEMORY_AND_DISK"".; See the `Spark documentation <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__ ; for a more in-depth discussion of persisting data.; ; .. warning ::; ; Persist, like all other :class:`.VariantDataset` functions, is functional.; Its output must be captured. This is wrong:; ; >>> vds = vds.linreg('sa.phenotype') # doctest: +SKIP; >>> vds.persist() # doctest: +SKIP; ; The above code does NOT persist ``vds``. Instead, it copies ``vds`` and persists that result. ; The proper usage is this:; ; >>> vds = vds.pca().persist() # doctest: +SKIP. :param storage_level: Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DI",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:192625,Safety,avoid,avoid,192625,"ith :py:meth:`.num_partitions`. The data in a variant dataset is divided into chunks called partitions, which may be stored together or across a network, so that each partition may be read and processed in parallel by available cores. When a variant dataset with :math:`M` variants is first imported, each of the :math:`k` partition will contain about :math:`M/k` of the variants. Since each partition has some computational overhead, decreasing the number of partitions can improve performance after significant filtering. Since it's recommended to have at least 2 - 4 partitions per core, increasing the number of partitions can allow one to take advantage of more cores. Partitions are a core concept of distributed computation in Spark, see `here <http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds>`__ for details. With ``shuffle=True``, Hail does a full shuffle of the data and creates equal sized partitions. With ``shuffle=False``, Hail combines existing partitions to avoid a full shuffle. These algorithms correspond to the ``repartition`` and ``coalesce`` commands in Spark, respectively. In particular, when ``shuffle=False``, ``num_partitions`` cannot exceed current number of partitions. :param int num_partitions: Desired number of partitions, must be less than the current number if ``shuffle=False``. :param bool shuffle: If true, use full shuffle to repartition. :return: Variant dataset with the number of partitions equal to at most ``num_partitions``; :rtype: :class:`.VariantDataset`; """""". jvds = self._jvdf.coalesce(num_partitions, shuffle); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(max_partitions=integral); def naive_coalesce(self, max_partitions):; """"""Naively descrease the number of partitions. .. warning ::. :py:meth:`~hail.VariantDataset.naive_coalesce` simply combines adjacent partitions to achieve the desired number. It does not attempt to rebalance, unlike :py:meth:`~hail.VariantDatase",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:12834,Security,access,accessed,12834,"e_global_expr('global.pops = [""FIN"", ""AFR"", ""EAS"", ""NFE""]'); >>> vds = vds.annotate_global_expr('global.pops = drop(global, pops)'). The expression namespace contains only one variable:. - ``global``: global annotations. :param expr: Annotation expression; :type expr: str or list of str. :return: Annotated variant dataset.; :rtype: :py:class:`.VariantDataset`; """""". if isinstance(expr, list):; expr = ','.join(expr). jvds = self._jvds.annotateGlobalExpr(expr); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(path=strlike,; annotation=anytype,; annotation_type=Type); def annotate_global(self, path, annotation, annotation_type):; """"""Add global annotations from Python objects. **Examples**. Add populations as a global annotation:; ; >>> vds_result = vds.annotate_global('global.populations',; ... ['EAS', 'AFR', 'EUR', 'SAS', 'AMR'],; ... TArray(TString())). **Notes**. This method registers new global annotations in a VDS. These annotations; can then be accessed through expressions in downstream operations. The; Hail data type must be provided and must match the given ``annotation``; parameter. :param str path: annotation path starting in 'global'. :param annotation: annotation to add to global. :param annotation_type: Hail type of annotation; :type annotation_type: :py:class:`.Type`. :return: Annotated variant dataset.; :rtype: :py:class:`.VariantDataset`; """""". annotation_type._typecheck(annotation). annotated = self._jvds.annotateGlobal(annotation_type._convert_to_j(annotation), annotation_type._jtype, path); assert annotated.globalSignature().typeCheck(annotated.globalAnnotation()), 'error in java type checking'; return VariantDataset(self.hc, annotated). [docs] @handle_py4j; @typecheck_method(expr=oneof(strlike, listof(strlike))); def annotate_samples_expr(self, expr):; """"""Annotate samples with expression. **Examples**. Compute per-sample GQ statistics for hets:. >>> vds_result = (vds.annotate_samples_expr('sa.gqHetStats = gs.filter(g => g.i",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:32875,Security,access,accessible,32875,"0012:A:T`` or ``22:140012:A:TTT``. - The variant ``22:140012:A:T`` will not be annotated by; ``22:140012:A:T,TTT``. It is possible that an unsplit variant dataset contains no multiallelic; variants, so ignore any warnings Hail prints if you know that to be the; case. Otherwise, run :py:meth:`.split_multi` before :py:meth:`.annotate_variants_vds`. :param VariantDataset other: Variant dataset to annotate with. :param str root: Sample annotation path to add variant annotations. :param str expr: Annotation expression. :return: Annotated variant dataset.; :rtype: :py:class:`.VariantDataset`; '''. jvds = self._jvds.annotateVariantsVDS(other._jvds, joption(root), joption(expr)). return VariantDataset(self.hc, jvds). [docs] def annotate_variants_db(self, annotations, gene_key=None):; """"""; Annotate variants using the Hail annotation database. .. warning::. Experimental. Supported only while running Hail on the Google Cloud Platform. Documentation describing the annotations that are accessible through this method can be found :ref:`here <sec-annotationdb>`. **Examples**. Annotate variants with CADD raw and PHRED scores:. >>> vds = vds.annotate_variants_db(['va.cadd.RawScore', 'va.cadd.PHRED']) # doctest: +SKIP. Annotate variants with gene-level PLI score, using the VEP-generated gene symbol to map variants to genes: . >>> pli_vds = vds.annotate_variants_db('va.gene.constraint.pli') # doctest: +SKIP. Again annotate variants with gene-level PLI score, this time using the existing ``va.gene_symbol`` annotation ; to map variants to genes:. >>> vds = vds.annotate_variants_db('va.gene.constraint.pli', gene_key='va.gene_symbol') # doctest: +SKIP. **Notes**. Annotations in the database are bi-allelic, so splitting multi-allelic variants in the VDS before using this ; method is recommended to capture all appropriate annotations from the database. To do this, run :py:meth:`split_multi` ; prior to annotating variants with this method:. >>> vds = vds.split_multi().annotate_variants_db(['",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:49427,Security,access,accessible,49427,"ith identifiers that form the header:. >>> vds.export_genotypes('output/genotypes.tsv', 'SAMPLE=s, VARIANT=v, GQ=g.gq, DP=g.dp, ANNO1=va.anno1, ANNO2=va.anno2'). Export the same information without identifiers, resulting in a file with no header:. >>> vds.export_genotypes('output/genotypes.tsv', 's, v, g.gq, g.dp, va.anno1, va.anno2'). **Notes**. :py:meth:`~hail.VariantDataset.export_genotypes` outputs one line per cell (genotype) in the data set, though HomRef and missing genotypes are not output by default if the genotype schema is equal to :py:class:`~hail.expr.TGenotype`. Use the ``export_ref`` and ``export_missing`` parameters to force export of HomRef and missing genotypes, respectively. The ``expr`` argument is a comma-separated list of fields or expressions, all of which must be of the form ``IDENTIFIER = <expression>``, or else of the form ``<expression>``. If some fields have identifiers and some do not, Hail will throw an exception. The accessible namespace includes ``g``, ``s``, ``sa``, ``v``, ``va``, and ``global``. .. warning::. If the genotype schema does not have the type :py:class:`~hail.expr.TGenotype`, all genotypes will be exported unless the value of ``g`` is missing.; Use :py:meth:`~hail.VariantDataset.filter_genotypes` to filter out genotypes based on an expression before exporting. :param str output: Output path. :param str expr: Export expression for values to export. :param bool types: Write types of exported columns to a file at (output + "".types""). :param bool export_ref: If true, export reference genotypes. Only applicable if the genotype schema is :py:class:`~hail.expr.TGenotype`. :param bool export_missing: If true, export missing genotypes. :param bool parallel: If true, writes a set of files (one per partition) rather than serially concatenating these files.; """""". if self._is_generic_genotype:; self._jvdf.exportGenotypes(output, expr, types, export_missing, parallel); else:; self._jvdf.exportGenotypes(output, expr, types, export_ref, ",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:55465,Security,access,accessible,55465,"in the; struct, and names them according to the struct field name. For example, the following invocation (assuming ``va.qc`` was generated; by :py:meth:`.variant_qc`):. >>> vds.export_variants('output/file.tsv', 'variant = v, va.qc.*'). will produce the following set of columns:. .. code-block:: text. variant callRate AC AF nCalled ... Note that using the ``.*`` syntax always results in named arguments, so it; is not possible to export header-less files in this manner. However,; naming the ""splatted"" struct will apply the name in front of each column; like so:. >>> vds.export_variants('output/file.tsv', 'variant = v, QC = va.qc.*'). which produces these columns:. .. code-block:: text. variant QC.callRate QC.AC QC.AF QC.nCalled ... **Notes**. This module takes a comma-delimited list of fields or expressions to; print. These fields will be printed in the order they appear in the; expression in the header and on each line. One line per variant in the VDS will be printed. The accessible namespace includes:. - ``v`` (*Variant*): :ref:`variant`; - ``va``: variant annotations; - ``global``: global annotations; - ``gs`` (*Aggregable[Genotype]*): aggregable of :ref:`genotype` for variant ``v``. **Designating output with an expression**. Much like the filtering methods, this method uses the Hail expression language.; While the filtering methods expect an; expression that evaluates to true or false, this method expects a; comma-separated list of fields to print. These fields take the; form ``IDENTIFIER = <expression>``. :param str output: Output file. :param str expr: Export expression for values to export. :param bool types: Write types of exported columns to a file at (output + "".types""). :param bool parallel: If true, writes a set of files (one per partition) rather than serially concatenating these files.; """""". self._jvds.exportVariants(output, expr, types, parallel). [docs] @handle_py4j; @typecheck_method(output=strlike,; append_to_header=nullable(strlike),; export_pp=boo",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:60256,Security,access,accessible,60256,"aram bool export_pp: If true, export linear-scaled probabilities (Hail's `pp` field on genotype) as the VCF PP FORMAT field. :param bool parallel: If true, return a set of VCF files (one per partition) rather than serially concatenating these files.; """""". self._jvdf.exportVCF(output, joption(append_to_header), export_pp, parallel). [docs] @handle_py4j; @convertVDS; @typecheck_method(output=strlike,; overwrite=bool,; parquet_genotypes=bool); def write(self, output, overwrite=False, parquet_genotypes=False):; """"""Write variant dataset as VDS file. **Examples**. Import data from a VCF file and then write the data to a VDS file:. >>> vds.write(""output/sample.vds""). :param str output: Path of VDS file to write. :param bool overwrite: If true, overwrite any existing VDS file. Cannot be used to read from and write to the same path. :param bool parquet_genotypes: If true, store genotypes as Parquet rather than Hail's serialization. The resulting VDS will be larger and slower in Hail but the genotypes will be accessible from other tools that support Parquet. """""". if self._is_generic_genotype:; self._jvdf.write(output, overwrite); else:; self._jvdf.write(output, overwrite, parquet_genotypes). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(expr=strlike,; annotation=strlike,; subset=bool,; keep=bool,; filter_altered_genotypes=bool,; max_shift=integral,; keep_star=bool); def filter_alleles(self, expr, annotation='va = va', subset=True, keep=True,; filter_altered_genotypes=False, max_shift=100, keep_star=False):; """"""Filter a user-defined set of alternate alleles for each variant.; If all alternate alleles of a variant are filtered, the; variant itself is filtered. The expr expression is; evaluated for each alternate allele, but not for; the reference allele (i.e. ``aIndex`` will never be zero). .. include:: requireTGenotype.rst. **Examples**. To remove alternate alleles with zero allele count and; update the alternate allele count annotation with the new; indices:. >>> ",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:89399,Security,access,accessed,89399,"somal region `(X:60001-2699520) || (X:154931044-155260560)` are included if the ``include_par`` optional parameter is set to true.; 4. The minor allele frequency (maf) per variant is calculated.; 5. For each variant and sample with a non-missing genotype call, :math:`E`, the expected number of homozygotes (from population MAF), is computed as :math:`1.0 - (2.0*maf*(1.0-maf))`.; 6. For each variant and sample with a non-missing genotype call, :math:`O`, the observed number of homozygotes, is computed as `0 = heterozygote; 1 = homozygote`; 7. For each variant and sample with a non-missing genotype call, :math:`N` is incremented by 1; 8. For each sample, :math:`E`, :math:`O`, and :math:`N` are combined across variants; 9. :math:`F` is calculated by :math:`(O - E) / (N - E)`; 10. A sex is assigned to each sample with the following criteria: `F < 0.2 => Female; F > 0.8 => Male`. Use ``female-threshold`` and ``male-threshold`` to change this behavior. **Annotations**. The below annotations can be accessed with ``sa.imputesex``. - **isFemale** (*Boolean*) -- True if the imputed sex is female, false if male, missing if undetermined; - **Fstat** (*Double*) -- Inbreeding coefficient; - **nTotal** (*Long*) -- Total number of variants considered; - **nCalled** (*Long*) -- Number of variants with a genotype call; - **expectedHoms** (*Double*) -- Expected number of homozygotes; - **observedHoms** (*Long*) -- Observed number of homozygotes. :param float maf_threshold: Minimum minor allele frequency threshold. :param bool include_par: Include pseudoautosomal regions. :param float female_threshold: Samples are called females if F < femaleThreshold. :param float male_threshold: Samples are called males if F > maleThreshold. :param str pop_freq: Variant annotation for estimate of MAF.; If None, MAF will be computed. :return: Annotated dataset.; :rtype: :py:class:`.VariantDataset`; """""". jvds = self._jvdf.imputeSex(maf_threshold, include_par, female_threshold, male_threshold, joption(pop",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:105414,Security,access,accessible,105414,"up. With ``single_key=True``, ``variant_keys`` expects a; variant annotation whose value is itself the key of interest. In bose cases, variants with missing keys are; ignored. **Notes**. This method modifies :py:meth:`.linreg` by replacing the genotype covariate per variant and sample with; an aggregated (i.e., collapsed) score per key and sample. This numeric score is computed from the sample's; genotypes and annotations over all variants with that key. Conceptually, the method proceeds as follows:. 1) Filter to the set of samples for which all phenotype and covariates are defined. 2) For each key and sample, aggregate genotypes across variants with that key to produce a numeric score.; ``agg_expr`` must be of numeric type and has the following symbols are in scope:. - ``s`` (*Sample*): sample; - ``sa``: sample annotations; - ``global``: global annotations; - ``gs`` (*Aggregable[Genotype]*): aggregable of :ref:`genotype` for sample ``s``. Note that ``v``, ``va``, and ``g`` are accessible through; `Aggregable methods <https://hail.is/hail/types.html#aggregable>`_ on ``gs``. The resulting **sample key table** has key column ``key_name`` and a numeric column of scores for each sample; named by the sample ID. 3) For each key, fit the linear regression model using the supplied phenotype and covariates.; The model is that of :py:meth:`.linreg` with sample genotype ``gt`` replaced by the score in the sample; key table. For each key, missing scores are mean-imputed across all samples. The resulting **linear regression key table** has the following columns:. - value of ``key_name`` (*String*) -- descriptor of variant group key (key column); - **beta** (*Double*) -- fit coefficient, :math:`\hat\beta_1`; - **se** (*Double*) -- estimated standard error, :math:`\widehat{\mathrm{se}}`; - **tstat** (*Double*) -- :math:`t`-statistic, equal to :math:`\hat\beta_1 / \widehat{\mathrm{se}}`; - **pval** (*Double*) -- :math:`p`-value. :py:meth:`.linreg_burden` returns both the linear reg",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:152483,Security,access,accessible,152483,"ated (i.e., collapsed) score per key and sample. This numeric score is computed from the sample's; genotypes and annotations over all variants with that key. The phenotype type must either be numeric; (with all present values 0 or 1) or Boolean, in which case true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'), Rao score test ('score'),; and Firth test ('firth') as the ``test`` parameter. Conceptually, the method proceeds as follows:. 1) Filter to the set of samples for which all phenotype and covariates are defined. 2) For each key and sample, aggregate genotypes across variants with that key to produce a numeric score.; ``agg_expr`` must be of numeric type and has the following symbols are in scope:. - ``s`` (*Sample*): sample; - ``sa``: sample annotations; - ``global``: global annotations; - ``gs`` (*Aggregable[Genotype]*): aggregable of :ref:`genotype` for sample ``s``. Note that ``v``, ``va``, and ``g`` are accessible through; `Aggregable methods <https://hail.is/hail/types.html#aggregable>`_ on ``gs``. The resulting **sample key table** has key column ``key_name`` and a numeric column of scores for each sample; named by the sample ID. 3) For each key, fit the logistic regression model using the supplied phenotype, covariates, and test.; The model and tests are those of :py:meth:`.logreg` with sample genotype ``gt`` replaced by the; score in the sample key table. For each key, missing scores are mean-imputed across all samples. The resulting **logistic regression key table** has key column of type String given by the ``key_name``; parameter and additional columns corresponding to the fields of the ``va.logreg`` schema given for ``test``; in :py:meth:`.logreg`. :py:meth:`.logreg_burden` returns both the logistic regression key table and the sample key table. :param str key_name: Name to assign to key column of returned key tables. :param str variant_keys: Variant annotation path for the TArray or",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:197232,Security,access,accessed,197232,"numeric); def same(self, other, tolerance=1e-6):; """"""True if the two variant datasets have the same variants, samples, genotypes, and annotation schemata and values. **Examples**. This will return True:. >>> vds.same(vds). **Notes**. The ``tolerance`` parameter sets the tolerance for equality when comparing floating-point fields. More precisely, :math:`x` and :math:`y` are equal if. .. math::. \abs{x - y} \leq tolerance * \max{\abs{x}, \abs{y}}. :param other: variant dataset to compare against; :type other: :class:`.VariantDataset`. :param float tolerance: floating-point tolerance for equality. :rtype: bool; """""". return self._jvds.same(other._jvds, tolerance). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(root=strlike,; keep_star=bool); def sample_qc(self, root='sa.qc', keep_star=False):; """"""Compute per-sample QC metrics. .. include:: requireTGenotype.rst. **Annotations**. :py:meth:`~hail.VariantDataset.sample_qc` computes 20 sample statistics from the ; genotype data and stores the results as sample annotations that can be accessed with; ``sa.qc.<identifier>`` (or ``<root>.<identifier>`` if a non-default root was passed):. +---------------------------+--------+----------------------------------------------------------+; | Name | Type | Description |; +===========================+========+==========================================================+; | ``callRate`` | Double | Fraction of genotypes called |; +---------------------------+--------+----------------------------------------------------------+; | ``nHomRef`` | Int | Number of homozygous reference genotypes |; +---------------------------+--------+----------------------------------------------------------+; | ``nHet`` | Int | Number of heterozygous genotypes |; +---------------------------+--------+----------------------------------------------------------+; | ``nHomVar`` | Int | Number of homozygous alternate genotypes |; +---------------------------+--------+--------------------------------------",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:217372,Security,access,accessed,217372,".tdt.chi2** (*Double*) -- TDT statistic. - **va.tdt.pval** (*Double*) -- p-value. :param pedigree: Sample pedigree.; :type pedigree: :class:`~hail.representation.Pedigree`. :param root: Variant annotation root to store TDT result. :return: Variant dataset with TDT association results added to variant annotations.; :rtype: :py:class:`.VariantDataset`; """""". jvds = self._jvdf.tdt(pedigree._jrep, root); return VariantDataset(self.hc, jvds). @handle_py4j; def _typecheck(self):; """"""Check if all sample, variant and global annotations are consistent with the schema."""""". self._jvds.typecheck(). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(root=strlike); def variant_qc(self, root='va.qc'):; """"""Compute common variant statistics (quality control metrics). .. include:: requireTGenotype.rst. **Examples**. >>> vds_result = vds.variant_qc(). .. _variantqc_annotations:. **Annotations**. :py:meth:`~hail.VariantDataset.variant_qc` computes 18 variant statistics from the ; genotype data and stores the results as variant annotations that can be accessed ; with ``va.qc.<identifier>`` (or ``<root>.<identifier>`` if a non-default root was passed):. +---------------------------+--------+--------------------------------------------------------+; | Name | Type | Description |; +===========================+========+========================================================+; | ``callRate`` | Double | Fraction of samples with called genotypes |; +---------------------------+--------+--------------------------------------------------------+; | ``AF`` | Double | Calculated alternate allele frequency (q) |; +---------------------------+--------+--------------------------------------------------------+; | ``AC`` | Int | Count of alternate alleles |; +---------------------------+--------+--------------------------------------------------------+; | ``rHeterozygosity`` | Double | Proportion of heterozygotes |; +---------------------------+--------+--------------------------------------------",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:6740,Testability,test,test,6740," the variant dataset was imported with :py:meth:`~hail.HailContext.import_plink`, :py:meth:`~hail.HailContext.import_gen`,; or :py:meth:`~hail.HailContext.import_bgen`, or if the variant dataset was simulated with :py:meth:`~hail.HailContext.balding_nichols_model`. :rtype: bool; """""". return self._jvds.wasSplit(). [docs] @handle_py4j; def file_version(self):; """"""File version of variant dataset. :rtype: int; """""". return self._jvds.fileVersion(). [docs] @handle_py4j; @typecheck_method(key_exprs=oneof(strlike, listof(strlike)),; agg_exprs=oneof(strlike, listof(strlike))); def aggregate_by_key(self, key_exprs, agg_exprs):; """"""Aggregate by user-defined key and aggregation expressions to produce a KeyTable.; Equivalent to a group-by operation in SQL. **Examples**. Compute the number of LOF heterozygote calls per gene per sample:. >>> kt_result = (vds; ... .aggregate_by_key(['Sample = s', 'Gene = va.gene'],; ... 'nHet = g.filter(g => g.isHet() && va.consequence == ""LOF"").count()'); ... .export(""test.tsv"")). This will produce a :class:`KeyTable` with 3 columns (`Sample`, `Gene`, `nHet`). :param key_exprs: Named expression(s) for which fields are keys.; :type key_exprs: str or list of str. :param agg_exprs: Named aggregation expression(s).; :type agg_exprs: str or list of str. :rtype: :class:`.KeyTable`; """""". if isinstance(key_exprs, list):; key_exprs = "","".join(key_exprs); if isinstance(agg_exprs, list):; agg_exprs = "","".join(agg_exprs). return KeyTable(self.hc, self._jvds.aggregateByKey(key_exprs, agg_exprs)). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(expr=oneof(strlike, listof(strlike)),; propagate_gq=bool); def annotate_alleles_expr(self, expr, propagate_gq=False):; """"""Annotate alleles with expression. .. include:: requireTGenotype.rst. **Examples**. To create a variant annotation ``va.nNonRefSamples: Array[Int]`` where the ith entry of; the array is the number of samples carrying the ith alternate allele:. >>> vds_result = vds.annotate_alleles_expr('va.nNo",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:13403,Testability,assert,assert,13403,"tation=anytype,; annotation_type=Type); def annotate_global(self, path, annotation, annotation_type):; """"""Add global annotations from Python objects. **Examples**. Add populations as a global annotation:; ; >>> vds_result = vds.annotate_global('global.populations',; ... ['EAS', 'AFR', 'EUR', 'SAS', 'AMR'],; ... TArray(TString())). **Notes**. This method registers new global annotations in a VDS. These annotations; can then be accessed through expressions in downstream operations. The; Hail data type must be provided and must match the given ``annotation``; parameter. :param str path: annotation path starting in 'global'. :param annotation: annotation to add to global. :param annotation_type: Hail type of annotation; :type annotation_type: :py:class:`.Type`. :return: Annotated variant dataset.; :rtype: :py:class:`.VariantDataset`; """""". annotation_type._typecheck(annotation). annotated = self._jvds.annotateGlobal(annotation_type._convert_to_j(annotation), annotation_type._jtype, path); assert annotated.globalSignature().typeCheck(annotated.globalAnnotation()), 'error in java type checking'; return VariantDataset(self.hc, annotated). [docs] @handle_py4j; @typecheck_method(expr=oneof(strlike, listof(strlike))); def annotate_samples_expr(self, expr):; """"""Annotate samples with expression. **Examples**. Compute per-sample GQ statistics for hets:. >>> vds_result = (vds.annotate_samples_expr('sa.gqHetStats = gs.filter(g => g.isHet()).map(g => g.gq).stats()'); ... .export_samples('output/samples.txt', 'sample = s, het_gq_mean = sa.gqHetStats.mean')). Compute the list of genes with a singleton LOF per sample:. >>> variant_annotations_table = hc.import_table('data/consequence.tsv', impute=True).key_by('Variant'); >>> vds_result = (vds.annotate_variants_table(variant_annotations_table, root='va.consequence'); ... .annotate_variants_expr('va.isSingleton = gs.map(g => g.nNonRefAlleles()).sum() == 1'); ... .annotate_samples_expr('sa.LOF_genes = gs.filter(g => va.isSingleton && g.is",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:63508,Testability,log,log,63508,"f; that the filtered alleles are not real so we should discard any; probability mass associated with them. The subset algorithm would produce the following:. .. code-block:: text. GT: 1/1; GQ: 980; AD: 0,50. 0 | 980; 1 | 980 0; +-----------; 0 1. In summary:. - GT: Set to most likely genotype based on the PLs ignoring the filtered allele(s).; - AD: The filtered alleles' columns are eliminated, e.g., filtering alleles 1 and 2 transforms ``25,5,10,20`` to ``25,20``.; - DP: No change.; - PL: The filtered alleles' columns are eliminated and the remaining columns shifted so the minimum value is 0.; - GQ: The second-lowest PL (after shifting). **Downcode algorithm**. The downcode algorithm (``subset=False``) recodes occurances of filtered alleles; to occurances of the reference allele (e.g. 1 -> 0 in our example). So the depths of filtered alleles in the AD field; are added to the depth of the reference allele. Where downcodeing filtered alleles merges distinct genotypes, the minimum PL is used (since PL is on a log scale, this roughly corresponds to adding probabilities). The PLs; are then re-normalized (shifted) so that the most likely genotype has a PL of 0, and GT is set to this genotype.; If an allele is filtered, this algorithm acts similarly to :py:meth:`~hail.VariantDataset.split_multi`. The downcoding algorithm would produce the following:. .. code-block:: text. GT: 0/1; GQ: 10; AD: 35,50. 0 | 20; 1 | 0 10; +-----------; 0 1. In summary:. - GT: Downcode filtered alleles to reference.; - AD: The filtered alleles' columns are eliminated and their value is added to the reference, e.g., filtering alleles 1 and 2 transforms ``25,5,10,20`` to ``40,20``.; - DP: No change.; - PL: Downcode filtered alleles to reference, combine PLs using minimum for each overloaded genotype, and shift so the overall minimum PL is 0.; - GQ: The second-lowest PL (after shifting). **Expression Variables**. The following symbols are in scope for ``expr``:. - ``v`` (*Variant*): :ref:`variant`; ",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:64570,Testability,test,tested,64570,"cale, this roughly corresponds to adding probabilities). The PLs; are then re-normalized (shifted) so that the most likely genotype has a PL of 0, and GT is set to this genotype.; If an allele is filtered, this algorithm acts similarly to :py:meth:`~hail.VariantDataset.split_multi`. The downcoding algorithm would produce the following:. .. code-block:: text. GT: 0/1; GQ: 10; AD: 35,50. 0 | 20; 1 | 0 10; +-----------; 0 1. In summary:. - GT: Downcode filtered alleles to reference.; - AD: The filtered alleles' columns are eliminated and their value is added to the reference, e.g., filtering alleles 1 and 2 transforms ``25,5,10,20`` to ``40,20``.; - DP: No change.; - PL: Downcode filtered alleles to reference, combine PLs using minimum for each overloaded genotype, and shift so the overall minimum PL is 0.; - GQ: The second-lowest PL (after shifting). **Expression Variables**. The following symbols are in scope for ``expr``:. - ``v`` (*Variant*): :ref:`variant`; - ``va``: variant annotations; - ``aIndex`` (*Int*): the index of the allele being tested. The following symbols are in scope for ``annotation``:. - ``v`` (*Variant*): :ref:`variant`; - ``va``: variant annotations; - ``aIndices`` (*Array[Int]*): the array of old indices (such that ``aIndices[newIndex] = oldIndex`` and ``aIndices[0] = 0``). :param str expr: Boolean filter expression involving v (variant), va (variant annotations), ; and aIndex (allele index). :param str annotation: Annotation modifying expression involving v (new variant), va (old variant annotations),; and aIndices (maps from new to old indices). :param bool subset: If true, subsets PL and AD, otherwise downcodes the PL and AD.; Genotype and GQ are set based on the resulting PLs. :param bool keep: If true, keep variants matching expr. :param bool filter_altered_genotypes: If true, genotypes that contain filtered-out alleles are set to missing. :param int max_shift: maximum number of base pairs by which; a split variant can move. Affects memory ",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:91219,Testability,test,testsetup,91219,"eturn: Annotated dataset.; :rtype: :py:class:`.VariantDataset`; """""". jvds = self._jvdf.imputeSex(maf_threshold, include_par, female_threshold, male_threshold, joption(pop_freq)); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(right=vds_type); def join(self, right):; """"""Join two variant datasets. **Notes**. This method performs an inner join on variants,; concatenates samples, and takes variant and; global annotations from the left dataset (self). The datasets must have distinct samples, the same sample schema, and the same split status (both split or both multi-allelic). :param right: right-hand variant dataset; :type right: :py:class:`.VariantDataset`. :return: Joined variant dataset; :rtype: :py:class:`.VariantDataset`; """""". return VariantDataset(self.hc, self._jvds.join(right._jvds)). [docs] @handle_py4j; @typecheck(datasets=tupleof(vds_type)); def union(*datasets):; """"""Take the union of datasets vertically (include all variants). **Examples**. .. testsetup::. vds_autosomal = vds; vds_chromX = vds; vds_chromY = vds. Union two datasets:. >>> vds_union = vds_autosomal.union(vds_chromX). Given a list of datasets, union them all:. >>> all_vds = [vds_autosomal, vds_chromX, vds_chromY]. The following three syntaxes are equivalent:. >>> vds_union1 = vds_autosomal.union(vds_chromX, vds_chromY); >>> vds_union2 = all_vds[0].union(*all_vds[1:]); >>> vds_union3 = VariantDataset.union(*all_vds). **Notes**. In order to combine two datasets, these requirements must be met:; - the samples must match; - the variant annotation schemas must match (field order within structs matters).; - the cell (genotype) schemas must match (field order within structs matters). The column annotations in the resulting dataset are simply the column annotations; from the first dataset; the column annotation schemas do not need to match. This method can trigger a shuffle, if partitions from two datasets overlap. :param vds_type: Datasets to combine.; :type vds_type: tuple ",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:94286,Testability,test,tests,94286,"argest start position. The LD pruning algorithm is as follows:. .. code-block:: python. pruned_set = []; for v1 in contig:; keep = True; for v2 in pruned_set:; if ((v1.position - v2.position) <= window and correlation(v1, v2) >= r2):; keep = False; if keep:; pruned_set.append(v1). The parameter ``window`` defines the maximum distance in base pairs between two variants to check whether; the variants are independent (:math:`R^2` < ``r2``) where ``r2`` is the maximum :math:`R^2` allowed.; :math:`R^2` is defined as the square of `Pearson's correlation coefficient <https://en.wikipedia.org/wiki/Pearson_correlation_coefficient>`__; :math:`{\\rho}_{x,y}` between the two genotype vectors :math:`{\\mathbf{x}}` and :math:`{\\mathbf{y}}`. .. math::. {\\rho}_{x,y} = \\frac{\\mathrm{Cov}(X,Y)}{\\sigma_X \\sigma_Y}. :py:meth:`.ld_prune` with default arguments is equivalent to ``plink --indep-pairwise 1000kb 1 0.2``.; The list of pruned variants returned by Hail and PLINK will differ because Hail mean-imputes missing values and tests pairs of variants in a different order than PLINK. Be sure to provide enough disk space per worker because :py:meth:`.ld_prune` `persists <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__ up to 3 copies of the data to both memory and disk.; The amount of disk space required will depend on the size and minor allele frequency of the input data and the prune parameters ``r2`` and ``window``. The number of bytes stored in memory per variant is about ``nSamples / 4 + 50``. .. warning::. The variants in the pruned set are not guaranteed to be identical each time :py:meth:`.ld_prune` is run. We recommend running :py:meth:`.ld_prune` once and exporting the list of LD pruned variants using; :py:meth:`.export_variants` for future use. :param float r2: Maximum :math:`R^2` threshold between two variants in the pruned set within a given window. :param int window: Width of window in base-pairs for computing pair-wise :math:`R^2` values",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:98090,Testability,test,test,98090,"rix is computed using distributed matrix multiplication if the number of genotypes exceeds :math:`5000^2` and locally otherwise. :return: Matrix of r values between pairs of variants.; :rtype: :py:class:`LDMatrix`; """""". jldm = self._jvdf.ldMatrix(force_local); return LDMatrix(jldm). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(y=strlike,; covariates=listof(strlike),; root=strlike,; use_dosages=bool,; min_ac=integral,; min_af=numeric); def linreg(self, y, covariates=[], root='va.linreg', use_dosages=False, min_ac=1, min_af=0.0):; r""""""Test each variant for association using linear regression. .. include:: requireTGenotype.rst. **Examples**. Run linear regression per variant using a phenotype and two covariates stored in sample annotations:. >>> vds_result = vds.linreg('sa.pheno.height', covariates=['sa.pheno.age', 'sa.pheno.isFemale']). **Notes**. The :py:meth:`.linreg` method computes, for each variant, statistics of; the :math:`t`-test for the genotype coefficient of the linear function; of best fit from sample genotype and covariates to quantitative; phenotype or case-control status. Hail only includes samples for which; phenotype and all covariates are defined. For each variant, missing genotypes; as the mean of called genotypes. By default, genotypes values are given by hard call genotypes (``g.gt``).; If ``use_dosages=True``, then genotype values are defined by the dosage; :math:`\mathrm{P}(\mathrm{Het}) + 2 \cdot \mathrm{P}(\mathrm{HomVar})`. For Phred-scaled values,; :math:`\mathrm{P}(\mathrm{Het})` and :math:`\mathrm{P}(\mathrm{HomVar})` are; calculated by normalizing the PL likelihoods (converted from the Phred-scale) to sum to 1. Assuming there are sample annotations ``sa.pheno.height``,; ``sa.pheno.age``, ``sa.pheno.isFemale``, and ``sa.cov.PC1``, the code:. >>> vds_result = vds.linreg('sa.pheno.height', covariates=['sa.pheno.age', 'sa.pheno.isFemale', 'sa.cov.PC1']). considers a model of the form. .. math::. \mathrm{height} = \beta_0 + \beta_",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:102703,Testability,test,test,102703," root: Variant annotation path to store result of linear regression. :param bool use_dosages: If true, use dosages genotypes rather than hard call genotypes. :param int min_ac: Minimum alternate allele count. :param float min_af: Minimum alternate allele frequency. :return: Variant dataset with linear regression variant annotations.; :rtype: :py:class:`.VariantDataset`; """""". jvds = self._jvdf.linreg(y, jarray(Env.jvm().java.lang.String, covariates), root, use_dosages, min_ac, min_af); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(key_name=strlike,; variant_keys=strlike,; single_key=bool,; agg_expr=strlike,; y=strlike,; covariates=listof(strlike)); def linreg_burden(self, key_name, variant_keys, single_key, agg_expr, y, covariates=[]):; r""""""Test each keyed group of variants for association by aggregating (collapsing) genotypes and applying the; linear regression model. .. include:: requireTGenotype.rst. **Examples**. Run a gene burden test using linear regression on the maximum genotype per gene. Here ``va.genes`` is a variant; annotation of type Set[String] giving the set of genes containing the variant (see **Extended example** below; for a deep dive):. >>> linreg_kt, sample_kt = (hc.read('data/example_burden.vds'); ... .linreg_burden(key_name='gene',; ... variant_keys='va.genes',; ... single_key=False,; ... agg_expr='gs.map(g => g.gt).max()',; ... y='sa.burden.pheno',; ... covariates=['sa.burden.cov1', 'sa.burden.cov2'])). Run a gene burden test using linear regression on the weighted sum of genotypes per gene. Here ``va.gene`` is; a variant annotation of type String giving a single gene per variant (or no gene if missing), and ``va.weight``; is a numeric variant annotation:. >>> linreg_kt, sample_kt = (hc.read('data/example_burden.vds'); ... .linreg_burden(key_name='gene',; ... variant_keys='va.gene',; ... single_key=True,; ... agg_expr='gs.map(g => va.weight * g.gt).sum()',; ... y='sa.burden.pheno',; ... covariates=['sa.burden.cov1'",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:103222,Testability,test,test,103222,"s). [docs] @handle_py4j; @typecheck_method(key_name=strlike,; variant_keys=strlike,; single_key=bool,; agg_expr=strlike,; y=strlike,; covariates=listof(strlike)); def linreg_burden(self, key_name, variant_keys, single_key, agg_expr, y, covariates=[]):; r""""""Test each keyed group of variants for association by aggregating (collapsing) genotypes and applying the; linear regression model. .. include:: requireTGenotype.rst. **Examples**. Run a gene burden test using linear regression on the maximum genotype per gene. Here ``va.genes`` is a variant; annotation of type Set[String] giving the set of genes containing the variant (see **Extended example** below; for a deep dive):. >>> linreg_kt, sample_kt = (hc.read('data/example_burden.vds'); ... .linreg_burden(key_name='gene',; ... variant_keys='va.genes',; ... single_key=False,; ... agg_expr='gs.map(g => g.gt).max()',; ... y='sa.burden.pheno',; ... covariates=['sa.burden.cov1', 'sa.burden.cov2'])). Run a gene burden test using linear regression on the weighted sum of genotypes per gene. Here ``va.gene`` is; a variant annotation of type String giving a single gene per variant (or no gene if missing), and ``va.weight``; is a numeric variant annotation:. >>> linreg_kt, sample_kt = (hc.read('data/example_burden.vds'); ... .linreg_burden(key_name='gene',; ... variant_keys='va.gene',; ... single_key=True,; ... agg_expr='gs.map(g => va.weight * g.gt).sum()',; ... y='sa.burden.pheno',; ... covariates=['sa.burden.cov1', 'sa.burden.cov2'])). To use a weighted sum of genotypes with missing genotypes mean-imputed rather than ignored, set; ``agg_expr='gs.map(g => va.weight * orElse(g.gt.toDouble, 2 * va.qc.AF)).sum()'`` where ``va.qc.AF``; is the allele frequency over those samples that have no missing phenotype or covariates. .. caution::. With ``single_key=False``, ``variant_keys`` expects a variant annotation of Set or Array type, in order to; allow each variant to have zero, one, or more keys (for example, the same variant may appe",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:115096,Testability,test,test,115096,"rm simultaneously. Larger block size requires more memmory. :return: Variant dataset with linear regression variant annotations.; :rtype: :py:class:`.VariantDataset`. """""". jvds = self._jvdf.linreg3(jarray(Env.jvm().java.lang.String, ys),; jarray(Env.jvm().java.lang.String, covariates), root, use_dosages, variant_block_size); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(kinshipMatrix=KinshipMatrix,; y=strlike,; covariates=listof(strlike),; global_root=strlike,; va_root=strlike,; run_assoc=bool,; use_ml=bool,; delta=nullable(numeric),; sparsity_threshold=numeric,; use_dosages=bool,; n_eigs=nullable(integral),; dropped_variance_fraction=(nullable(float))); def lmmreg(self, kinshipMatrix, y, covariates=[], global_root=""global.lmmreg"", va_root=""va.lmmreg"",; run_assoc=True, use_ml=False, delta=None, sparsity_threshold=1.0, use_dosages=False,; n_eigs=None, dropped_variance_fraction=None):; """"""Use a kinship-based linear mixed model to estimate the genetic component of phenotypic variance (narrow-sense heritability) and optionally test each variant for association. .. include:: requireTGenotype.rst. **Examples**. Suppose the variant dataset saved at *data/example_lmmreg.vds* has a Boolean variant annotation ``va.useInKinship`` and numeric or Boolean sample annotations ``sa.pheno``, ``sa.cov1``, ``sa.cov2``. Then the :py:meth:`.lmmreg` function in. >>> assoc_vds = hc.read(""data/example_lmmreg.vds""); >>> kinship_matrix = assoc_vds.filter_variants_expr('va.useInKinship').rrm(); >>> lmm_vds = assoc_vds.lmmreg(kinship_matrix, 'sa.pheno', ['sa.cov1', 'sa.cov2']). will execute the following four steps in order:. 1) filter to samples in given kinship matrix to those for which ``sa.pheno``, ``sa.cov``, and ``sa.cov2`` are all defined; 2) compute the eigendecomposition :math:`K = USU^T` of the kinship matrix; 3) fit covariate coefficients and variance parameters in the sample-covariates-only (global) model using restricted maximum lik",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:116155,Testability,test,test,116155," Suppose the variant dataset saved at *data/example_lmmreg.vds* has a Boolean variant annotation ``va.useInKinship`` and numeric or Boolean sample annotations ``sa.pheno``, ``sa.cov1``, ``sa.cov2``. Then the :py:meth:`.lmmreg` function in. >>> assoc_vds = hc.read(""data/example_lmmreg.vds""); >>> kinship_matrix = assoc_vds.filter_variants_expr('va.useInKinship').rrm(); >>> lmm_vds = assoc_vds.lmmreg(kinship_matrix, 'sa.pheno', ['sa.cov1', 'sa.cov2']). will execute the following four steps in order:. 1) filter to samples in given kinship matrix to those for which ``sa.pheno``, ``sa.cov``, and ``sa.cov2`` are all defined; 2) compute the eigendecomposition :math:`K = USU^T` of the kinship matrix; 3) fit covariate coefficients and variance parameters in the sample-covariates-only (global) model using restricted maximum likelihood (`REML <https://en.wikipedia.org/wiki/Restricted_maximum_likelihood>`__), storing results in global annotations under ``global.lmmreg``; 4) test each variant for association, storing results under ``va.lmmreg`` in variant annotations. This plan can be modified as follows:. - Set ``run_assoc=False`` to not test any variants for association, i.e. skip Step 5.; - Set ``use_ml=True`` to use maximum likelihood instead of REML in Steps 4 and 5.; - Set the ``delta`` argument to manually set the value of :math:`\delta` rather that fitting :math:`\delta` in Step 4.; - Set the ``global_root`` argument to change the global annotation root in Step 4.; - Set the ``va_root`` argument to change the variant annotation root in Step 5. :py:meth:`.lmmreg` adds 9 or 13 global annotations in Step 4, depending on whether :math:`\delta` is set or fit. +----------------------------------------------+----------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+; | Annotation | Type | Value |; +==============================================+=====================",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:116322,Testability,test,test,116322,"le annotations ``sa.pheno``, ``sa.cov1``, ``sa.cov2``. Then the :py:meth:`.lmmreg` function in. >>> assoc_vds = hc.read(""data/example_lmmreg.vds""); >>> kinship_matrix = assoc_vds.filter_variants_expr('va.useInKinship').rrm(); >>> lmm_vds = assoc_vds.lmmreg(kinship_matrix, 'sa.pheno', ['sa.cov1', 'sa.cov2']). will execute the following four steps in order:. 1) filter to samples in given kinship matrix to those for which ``sa.pheno``, ``sa.cov``, and ``sa.cov2`` are all defined; 2) compute the eigendecomposition :math:`K = USU^T` of the kinship matrix; 3) fit covariate coefficients and variance parameters in the sample-covariates-only (global) model using restricted maximum likelihood (`REML <https://en.wikipedia.org/wiki/Restricted_maximum_likelihood>`__), storing results in global annotations under ``global.lmmreg``; 4) test each variant for association, storing results under ``va.lmmreg`` in variant annotations. This plan can be modified as follows:. - Set ``run_assoc=False`` to not test any variants for association, i.e. skip Step 5.; - Set ``use_ml=True`` to use maximum likelihood instead of REML in Steps 4 and 5.; - Set the ``delta`` argument to manually set the value of :math:`\delta` rather that fitting :math:`\delta` in Step 4.; - Set the ``global_root`` argument to change the global annotation root in Step 4.; - Set the ``va_root`` argument to change the variant annotation root in Step 5. :py:meth:`.lmmreg` adds 9 or 13 global annotations in Step 4, depending on whether :math:`\delta` is set or fit. +----------------------------------------------+----------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+; | Annotation | Type | Value |; +==============================================+======================+==============================================================================================================================================",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:121155,Testability,log,log,121155,"---------------------------------------+; | ``global.lmmreg.fit.seH2`` | Double | standard error of :math:`\\hat{h}^2` under asymptotic normal approximation |; +----------------------------------------------+----------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+; | ``global.lmmreg.fit.normLkhdH2`` | Array[Double] | likelihood function of :math:`h^2` normalized on the discrete grid ``0.01, 0.02, ..., 0.99``. Index ``i`` is the likelihood for percentage ``i``. |; +----------------------------------------------+----------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+; | ``global.lmmreg.fit.maxLogLkhd`` | Double | (restricted) maximum log likelihood corresponding to :math:`\\hat{\delta}` |; +----------------------------------------------+----------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+; | ``global.lmmreg.fit.logDeltaGrid`` | Array[Double] | values of :math:`\\mathrm{ln}(\delta)` used in the grid search |; +----------------------------------------------+----------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+; | ``global.lmmreg.fit.logLkhdVals`` | Array[Double] | (restricted) log likelihood of :math:`y` given :math:`X` and :math:`\\mathrm{ln}(\delta)` at the (RE)ML fit of :math:`\\beta` and :math:`\sigma_g^2` |; +----------------------------------------------+----------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+. These global annotations are also added to ``hail.log``, with the ra",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:121458,Testability,log,logDeltaGrid,121458,"----------------------------------+; | ``global.lmmreg.fit.normLkhdH2`` | Array[Double] | likelihood function of :math:`h^2` normalized on the discrete grid ``0.01, 0.02, ..., 0.99``. Index ``i`` is the likelihood for percentage ``i``. |; +----------------------------------------------+----------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+; | ``global.lmmreg.fit.maxLogLkhd`` | Double | (restricted) maximum log likelihood corresponding to :math:`\\hat{\delta}` |; +----------------------------------------------+----------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+; | ``global.lmmreg.fit.logDeltaGrid`` | Array[Double] | values of :math:`\\mathrm{ln}(\delta)` used in the grid search |; +----------------------------------------------+----------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+; | ``global.lmmreg.fit.logLkhdVals`` | Array[Double] | (restricted) log likelihood of :math:`y` given :math:`X` and :math:`\\mathrm{ln}(\delta)` at the (RE)ML fit of :math:`\\beta` and :math:`\sigma_g^2` |; +----------------------------------------------+----------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+. These global annotations are also added to ``hail.log``, with the ranked evals and :math:`\delta` grid with values in .tsv tabular form. Use ``grep 'lmmreg:' hail.log`` to find the lines just above each table. If Step 5 is performed, :py:meth:`.lmmreg` also adds four linear regression variant annotations. +------------------------+--------+------------------------------------------------------------------------",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:121803,Testability,log,logLkhdVals,121803,"-------------------------------------------------------------------------------+; | ``global.lmmreg.fit.maxLogLkhd`` | Double | (restricted) maximum log likelihood corresponding to :math:`\\hat{\delta}` |; +----------------------------------------------+----------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+; | ``global.lmmreg.fit.logDeltaGrid`` | Array[Double] | values of :math:`\\mathrm{ln}(\delta)` used in the grid search |; +----------------------------------------------+----------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+; | ``global.lmmreg.fit.logLkhdVals`` | Array[Double] | (restricted) log likelihood of :math:`y` given :math:`X` and :math:`\\mathrm{ln}(\delta)` at the (RE)ML fit of :math:`\\beta` and :math:`\sigma_g^2` |; +----------------------------------------------+----------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+. These global annotations are also added to ``hail.log``, with the ranked evals and :math:`\delta` grid with values in .tsv tabular form. Use ``grep 'lmmreg:' hail.log`` to find the lines just above each table. If Step 5 is performed, :py:meth:`.lmmreg` also adds four linear regression variant annotations. +------------------------+--------+-------------------------------------------------------------------------+; | Annotation | Type | Value |; +========================+========+=========================================================================+; | ``va.lmmreg.beta`` | Double | fit genotype coefficient, :math:`\hat\\beta_0` |; +------------------------+--------+-------------------------------------------------------------------------+; | ``va.lmmreg.sigmaG2`` | Double | fit coef",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:121848,Testability,log,log,121848,"-------------------------------------------------------------------------------+; | ``global.lmmreg.fit.maxLogLkhd`` | Double | (restricted) maximum log likelihood corresponding to :math:`\\hat{\delta}` |; +----------------------------------------------+----------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+; | ``global.lmmreg.fit.logDeltaGrid`` | Array[Double] | values of :math:`\\mathrm{ln}(\delta)` used in the grid search |; +----------------------------------------------+----------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+; | ``global.lmmreg.fit.logLkhdVals`` | Array[Double] | (restricted) log likelihood of :math:`y` given :math:`X` and :math:`\\mathrm{ln}(\delta)` at the (RE)ML fit of :math:`\\beta` and :math:`\sigma_g^2` |; +----------------------------------------------+----------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+. These global annotations are also added to ``hail.log``, with the ranked evals and :math:`\delta` grid with values in .tsv tabular form. Use ``grep 'lmmreg:' hail.log`` to find the lines just above each table. If Step 5 is performed, :py:meth:`.lmmreg` also adds four linear regression variant annotations. +------------------------+--------+-------------------------------------------------------------------------+; | Annotation | Type | Value |; +========================+========+=========================================================================+; | ``va.lmmreg.beta`` | Double | fit genotype coefficient, :math:`\hat\\beta_0` |; +------------------------+--------+-------------------------------------------------------------------------+; | ``va.lmmreg.sigmaG2`` | Double | fit coef",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:122261,Testability,log,log,122261,"------------------------------------------------------------------------------------------------------------------------------------------+; | ``global.lmmreg.fit.logDeltaGrid`` | Array[Double] | values of :math:`\\mathrm{ln}(\delta)` used in the grid search |; +----------------------------------------------+----------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+; | ``global.lmmreg.fit.logLkhdVals`` | Array[Double] | (restricted) log likelihood of :math:`y` given :math:`X` and :math:`\\mathrm{ln}(\delta)` at the (RE)ML fit of :math:`\\beta` and :math:`\sigma_g^2` |; +----------------------------------------------+----------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+. These global annotations are also added to ``hail.log``, with the ranked evals and :math:`\delta` grid with values in .tsv tabular form. Use ``grep 'lmmreg:' hail.log`` to find the lines just above each table. If Step 5 is performed, :py:meth:`.lmmreg` also adds four linear regression variant annotations. +------------------------+--------+-------------------------------------------------------------------------+; | Annotation | Type | Value |; +========================+========+=========================================================================+; | ``va.lmmreg.beta`` | Double | fit genotype coefficient, :math:`\hat\\beta_0` |; +------------------------+--------+-------------------------------------------------------------------------+; | ``va.lmmreg.sigmaG2`` | Double | fit coefficient of genetic variance component, :math:`\hat{\sigma}_g^2` |; +------------------------+--------+-------------------------------------------------------------------------+; | ``va.lmmreg.chi2`` | Double | :math:`\chi^2` statistic of the likelihood ratio test |; +---------------------",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:122374,Testability,log,log,122374,"------------------------------------+; | ``global.lmmreg.fit.logDeltaGrid`` | Array[Double] | values of :math:`\\mathrm{ln}(\delta)` used in the grid search |; +----------------------------------------------+----------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+; | ``global.lmmreg.fit.logLkhdVals`` | Array[Double] | (restricted) log likelihood of :math:`y` given :math:`X` and :math:`\\mathrm{ln}(\delta)` at the (RE)ML fit of :math:`\\beta` and :math:`\sigma_g^2` |; +----------------------------------------------+----------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+. These global annotations are also added to ``hail.log``, with the ranked evals and :math:`\delta` grid with values in .tsv tabular form. Use ``grep 'lmmreg:' hail.log`` to find the lines just above each table. If Step 5 is performed, :py:meth:`.lmmreg` also adds four linear regression variant annotations. +------------------------+--------+-------------------------------------------------------------------------+; | Annotation | Type | Value |; +========================+========+=========================================================================+; | ``va.lmmreg.beta`` | Double | fit genotype coefficient, :math:`\hat\\beta_0` |; +------------------------+--------+-------------------------------------------------------------------------+; | ``va.lmmreg.sigmaG2`` | Double | fit coefficient of genetic variance component, :math:`\hat{\sigma}_g^2` |; +------------------------+--------+-------------------------------------------------------------------------+; | ``va.lmmreg.chi2`` | Double | :math:`\chi^2` statistic of the likelihood ratio test |; +------------------------+--------+-------------------------------------------------------------------------+; | ``va.lmmre",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:123266,Testability,test,test,123266,"math:`\delta` grid with values in .tsv tabular form. Use ``grep 'lmmreg:' hail.log`` to find the lines just above each table. If Step 5 is performed, :py:meth:`.lmmreg` also adds four linear regression variant annotations. +------------------------+--------+-------------------------------------------------------------------------+; | Annotation | Type | Value |; +========================+========+=========================================================================+; | ``va.lmmreg.beta`` | Double | fit genotype coefficient, :math:`\hat\\beta_0` |; +------------------------+--------+-------------------------------------------------------------------------+; | ``va.lmmreg.sigmaG2`` | Double | fit coefficient of genetic variance component, :math:`\hat{\sigma}_g^2` |; +------------------------+--------+-------------------------------------------------------------------------+; | ``va.lmmreg.chi2`` | Double | :math:`\chi^2` statistic of the likelihood ratio test |; +------------------------+--------+-------------------------------------------------------------------------+; | ``va.lmmreg.pval`` | Double | :math:`p`-value |; +------------------------+--------+-------------------------------------------------------------------------+. Those variants that don't vary across the included samples (e.g., all genotypes; are HomRef) will have missing annotations. The simplest way to export all resulting annotations is:. >>> lmm_vds.export_variants('output/lmmreg.tsv.bgz', 'variant = v, va.lmmreg.*'); >>> lmmreg_results = lmm_vds.globals['lmmreg']; ; By default, genotypes values are given by hard call genotypes (``g.gt``).; If ``use_dosages=True``, then genotype values for per-variant association are defined by the dosage; :math:`\mathrm{P}(\mathrm{Het}) + 2 \cdot \mathrm{P}(\mathrm{HomVar})`. For Phred-scaled values,; :math:`\mathrm{P}(\mathrm{Het})` and :math:`\mathrm{P}(\mathrm{HomVar})` are; calculated by normalizing the PL likelihoods (converted from the Phred-scale) to su",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:124659,Testability,test,test,124659,"plest way to export all resulting annotations is:. >>> lmm_vds.export_variants('output/lmmreg.tsv.bgz', 'variant = v, va.lmmreg.*'); >>> lmmreg_results = lmm_vds.globals['lmmreg']; ; By default, genotypes values are given by hard call genotypes (``g.gt``).; If ``use_dosages=True``, then genotype values for per-variant association are defined by the dosage; :math:`\mathrm{P}(\mathrm{Het}) + 2 \cdot \mathrm{P}(\mathrm{HomVar})`. For Phred-scaled values,; :math:`\mathrm{P}(\mathrm{Het})` and :math:`\mathrm{P}(\mathrm{HomVar})` are; calculated by normalizing the PL likelihoods (converted from the Phred-scale) to sum to 1. **Performance**. Hail's initial version of :py:meth:`.lmmreg` scales beyond 15k samples and to an essentially unbounded number of variants, making it particularly well-suited to modern sequencing studies and complementary to tools designed for SNP arrays. Analysts have used :py:meth:`.lmmreg` in research to compute kinship from 100k common variants and test 32 million non-rare variants on 8k whole genomes in about 10 minutes on `Google cloud <http://discuss.hail.is/t/using-hail-on-the-google-cloud-platform/80>`__. While :py:meth:`.lmmreg` computes the kinship matrix :math:`K` using distributed matrix multiplication (Step 2), the full `eigendecomposition <https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix>`__ (Step 3) is currently run on a single core of master using the `LAPACK routine DSYEVD <http://www.netlib.org/lapack/explore-html/d2/d8a/group__double_s_yeigen_ga694ddc6e5527b6223748e3462013d867.html>`__, which we empirically find to be the most performant of the four available routines; laptop performance plots showing cubic complexity in :math:`n` are available `here <https://github.com/hail-is/hail/pull/906>`__. On Google cloud, eigendecomposition takes about 2 seconds for 2535 sampes and 1 minute for 8185 samples. If you see worse performance, check that LAPACK natives are being properly loaded (see ""BLAS and LAPACK"" in Getting Started",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:125804,Testability,test,testing,125804,"<https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix>`__ (Step 3) is currently run on a single core of master using the `LAPACK routine DSYEVD <http://www.netlib.org/lapack/explore-html/d2/d8a/group__double_s_yeigen_ga694ddc6e5527b6223748e3462013d867.html>`__, which we empirically find to be the most performant of the four available routines; laptop performance plots showing cubic complexity in :math:`n` are available `here <https://github.com/hail-is/hail/pull/906>`__. On Google cloud, eigendecomposition takes about 2 seconds for 2535 sampes and 1 minute for 8185 samples. If you see worse performance, check that LAPACK natives are being properly loaded (see ""BLAS and LAPACK"" in Getting Started). Given the eigendecomposition, fitting the global model (Step 4) takes on the order of a few seconds on master. Association testing (Step 5) is fully distributed by variant with per-variant time complexity that is completely independent of the number of sample covariates and dominated by multiplication of the genotype vector :math:`v` by the matrix of eigenvectors :math:`U^T` as described below, which we accelerate with a sparse representation of :math:`v`. The matrix :math:`U^T` has size about :math:`8n^2` bytes and is currently broadcast to each Spark executor. For example, with 15k samples, storing :math:`U^T` consumes about 3.6GB of memory on a 16-core worker node with two 8-core executors. So for large :math:`n`, we recommend using a high-memory configuration such as ``highmem`` workers. **Linear mixed model**. :py:meth:`.lmmreg` estimates the genetic proportion of residual phenotypic variance (narrow-sense heritability) under a kinship-based linear mixed model, and then optionally tests each variant for association using the likelihood ratio test. Inference is exact. We first describe the sample-covariates-only model used to estimate heritability, which we simply refer to as the *global model*. With :math:`n` samples and :math:`c` sample covariates, we define:",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:126682,Testability,test,tests,126682,"ed (see ""BLAS and LAPACK"" in Getting Started). Given the eigendecomposition, fitting the global model (Step 4) takes on the order of a few seconds on master. Association testing (Step 5) is fully distributed by variant with per-variant time complexity that is completely independent of the number of sample covariates and dominated by multiplication of the genotype vector :math:`v` by the matrix of eigenvectors :math:`U^T` as described below, which we accelerate with a sparse representation of :math:`v`. The matrix :math:`U^T` has size about :math:`8n^2` bytes and is currently broadcast to each Spark executor. For example, with 15k samples, storing :math:`U^T` consumes about 3.6GB of memory on a 16-core worker node with two 8-core executors. So for large :math:`n`, we recommend using a high-memory configuration such as ``highmem`` workers. **Linear mixed model**. :py:meth:`.lmmreg` estimates the genetic proportion of residual phenotypic variance (narrow-sense heritability) under a kinship-based linear mixed model, and then optionally tests each variant for association using the likelihood ratio test. Inference is exact. We first describe the sample-covariates-only model used to estimate heritability, which we simply refer to as the *global model*. With :math:`n` samples and :math:`c` sample covariates, we define:. - :math:`y = n \\times 1` vector of phenotypes; - :math:`X = n \\times c` matrix of sample covariates and intercept column of ones; - :math:`K = n \\times n` kinship matrix; - :math:`I = n \\times n` identity matrix; - :math:`\\beta = c \\times 1` vector of covariate coefficients; - :math:`\sigma_g^2 =` coefficient of genetic variance component :math:`K`; - :math:`\sigma_e^2 =` coefficient of environmental variance component :math:`I`; - :math:`\delta = \\frac{\sigma_e^2}{\sigma_g^2} =` ratio of environmental and genetic variance component coefficients; - :math:`h^2 = \\frac{\sigma_g^2}{\sigma_g^2 + \sigma_e^2} = \\frac{1}{1 + \delta} =` genetic proportion o",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:126744,Testability,test,test,126744,"ed (see ""BLAS and LAPACK"" in Getting Started). Given the eigendecomposition, fitting the global model (Step 4) takes on the order of a few seconds on master. Association testing (Step 5) is fully distributed by variant with per-variant time complexity that is completely independent of the number of sample covariates and dominated by multiplication of the genotype vector :math:`v` by the matrix of eigenvectors :math:`U^T` as described below, which we accelerate with a sparse representation of :math:`v`. The matrix :math:`U^T` has size about :math:`8n^2` bytes and is currently broadcast to each Spark executor. For example, with 15k samples, storing :math:`U^T` consumes about 3.6GB of memory on a 16-core worker node with two 8-core executors. So for large :math:`n`, we recommend using a high-memory configuration such as ``highmem`` workers. **Linear mixed model**. :py:meth:`.lmmreg` estimates the genetic proportion of residual phenotypic variance (narrow-sense heritability) under a kinship-based linear mixed model, and then optionally tests each variant for association using the likelihood ratio test. Inference is exact. We first describe the sample-covariates-only model used to estimate heritability, which we simply refer to as the *global model*. With :math:`n` samples and :math:`c` sample covariates, we define:. - :math:`y = n \\times 1` vector of phenotypes; - :math:`X = n \\times c` matrix of sample covariates and intercept column of ones; - :math:`K = n \\times n` kinship matrix; - :math:`I = n \\times n` identity matrix; - :math:`\\beta = c \\times 1` vector of covariate coefficients; - :math:`\sigma_g^2 =` coefficient of genetic variance component :math:`K`; - :math:`\sigma_e^2 =` coefficient of environmental variance component :math:`I`; - :math:`\delta = \\frac{\sigma_e^2}{\sigma_g^2} =` ratio of environmental and genetic variance component coefficients; - :math:`h^2 = \\frac{\sigma_g^2}{\sigma_g^2 + \sigma_e^2} = \\frac{1}{1 + \delta} =` genetic proportion o",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:130564,Testability,log,log,130564,"xed model above is mathematically equivalent to the model. .. math::. U^Ty \\sim \mathrm{N}\\left(U^TX\\beta, \sigma_g^2 (S + \delta I)\\right). for which the covariance is diagonal (e.g., unmixed). That is, rotating the phenotype vector (:math:`y`) and covariate vectors (columns of :math:`X`) in :math:`\mathbb{R}^n` by :math:`U^T` transforms the model to one with independent residuals. For any particular value of :math:`\delta`, the restricted maximum likelihood (REML) solution for the latter model can be solved exactly in time complexity that is linear rather than cubic in :math:`n`. In particular, having rotated, we can run a very efficient 1-dimensional optimization procedure over :math:`\delta` to find the REML estimate :math:`(\hat{\delta}, \\hat{\\beta}, \\hat{\sigma}_g^2)` of the triple :math:`(\delta, \\beta, \sigma_g^2)`, which in turn determines :math:`\\hat{\sigma}_e^2` and :math:`\\hat{h}^2`. We first compute the maximum log likelihood on a :math:`\delta`-grid that is uniform on the log scale, with :math:`\\mathrm{ln}(\delta)` running from -8 to 8 by 0.01, corresponding to :math:`h^2` decreasing from 0.9995 to 0.0005. If :math:`h^2` is maximized at the lower boundary then standard linear regression would be more appropriate and Hail will exit; more generally, consider using standard linear regression when :math:`\\hat{h}^2` is very small. A maximum at the upper boundary is highly suspicious and will also cause Hail to exit. In any case, the log file records the table of grid values for further inspection, beginning under the info line containing ""lmmreg: table of delta"". If the optimal grid point falls in the interior of the grid as expected, we then use `Brent's method <https://en.wikipedia.org/wiki/Brent%27s_method>`__ to find the precise location of the maximum over the same range, with initial guess given by the optimal grid point and a tolerance on :math:`\\mathrm{ln}(\delta)` of 1e-6. If this location differs from the optimal grid point by more th",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:130627,Testability,log,log,130627,"xed model above is mathematically equivalent to the model. .. math::. U^Ty \\sim \mathrm{N}\\left(U^TX\\beta, \sigma_g^2 (S + \delta I)\\right). for which the covariance is diagonal (e.g., unmixed). That is, rotating the phenotype vector (:math:`y`) and covariate vectors (columns of :math:`X`) in :math:`\mathbb{R}^n` by :math:`U^T` transforms the model to one with independent residuals. For any particular value of :math:`\delta`, the restricted maximum likelihood (REML) solution for the latter model can be solved exactly in time complexity that is linear rather than cubic in :math:`n`. In particular, having rotated, we can run a very efficient 1-dimensional optimization procedure over :math:`\delta` to find the REML estimate :math:`(\hat{\delta}, \\hat{\\beta}, \\hat{\sigma}_g^2)` of the triple :math:`(\delta, \\beta, \sigma_g^2)`, which in turn determines :math:`\\hat{\sigma}_e^2` and :math:`\\hat{h}^2`. We first compute the maximum log likelihood on a :math:`\delta`-grid that is uniform on the log scale, with :math:`\\mathrm{ln}(\delta)` running from -8 to 8 by 0.01, corresponding to :math:`h^2` decreasing from 0.9995 to 0.0005. If :math:`h^2` is maximized at the lower boundary then standard linear regression would be more appropriate and Hail will exit; more generally, consider using standard linear regression when :math:`\\hat{h}^2` is very small. A maximum at the upper boundary is highly suspicious and will also cause Hail to exit. In any case, the log file records the table of grid values for further inspection, beginning under the info line containing ""lmmreg: table of delta"". If the optimal grid point falls in the interior of the grid as expected, we then use `Brent's method <https://en.wikipedia.org/wiki/Brent%27s_method>`__ to find the precise location of the maximum over the same range, with initial guess given by the optimal grid point and a tolerance on :math:`\\mathrm{ln}(\delta)` of 1e-6. If this location differs from the optimal grid point by more th",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:131094,Testability,log,log,131094,"complexity that is linear rather than cubic in :math:`n`. In particular, having rotated, we can run a very efficient 1-dimensional optimization procedure over :math:`\delta` to find the REML estimate :math:`(\hat{\delta}, \\hat{\\beta}, \\hat{\sigma}_g^2)` of the triple :math:`(\delta, \\beta, \sigma_g^2)`, which in turn determines :math:`\\hat{\sigma}_e^2` and :math:`\\hat{h}^2`. We first compute the maximum log likelihood on a :math:`\delta`-grid that is uniform on the log scale, with :math:`\\mathrm{ln}(\delta)` running from -8 to 8 by 0.01, corresponding to :math:`h^2` decreasing from 0.9995 to 0.0005. If :math:`h^2` is maximized at the lower boundary then standard linear regression would be more appropriate and Hail will exit; more generally, consider using standard linear regression when :math:`\\hat{h}^2` is very small. A maximum at the upper boundary is highly suspicious and will also cause Hail to exit. In any case, the log file records the table of grid values for further inspection, beginning under the info line containing ""lmmreg: table of delta"". If the optimal grid point falls in the interior of the grid as expected, we then use `Brent's method <https://en.wikipedia.org/wiki/Brent%27s_method>`__ to find the precise location of the maximum over the same range, with initial guess given by the optimal grid point and a tolerance on :math:`\\mathrm{ln}(\delta)` of 1e-6. If this location differs from the optimal grid point by more than 0.01, a warning will be displayed and logged, and one would be wise to investigate by plotting the values over the grid. Note that :math:`h^2` is related to :math:`\\mathrm{ln}(\delta)` through the `sigmoid function <https://en.wikipedia.org/wiki/Sigmoid_function>`_. More precisely,. .. math::. h^2 = 1 - \mathrm{sigmoid}(\\mathrm{ln}(\delta)) = \mathrm{sigmoid}(-\\mathrm{ln}(\delta)). Hence one can change variables to extract a high-resolution discretization of the likelihood function of :math:`h^2` over :math:`[0,1]` at the c",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:131657,Testability,log,logged,131657,"from -8 to 8 by 0.01, corresponding to :math:`h^2` decreasing from 0.9995 to 0.0005. If :math:`h^2` is maximized at the lower boundary then standard linear regression would be more appropriate and Hail will exit; more generally, consider using standard linear regression when :math:`\\hat{h}^2` is very small. A maximum at the upper boundary is highly suspicious and will also cause Hail to exit. In any case, the log file records the table of grid values for further inspection, beginning under the info line containing ""lmmreg: table of delta"". If the optimal grid point falls in the interior of the grid as expected, we then use `Brent's method <https://en.wikipedia.org/wiki/Brent%27s_method>`__ to find the precise location of the maximum over the same range, with initial guess given by the optimal grid point and a tolerance on :math:`\\mathrm{ln}(\delta)` of 1e-6. If this location differs from the optimal grid point by more than 0.01, a warning will be displayed and logged, and one would be wise to investigate by plotting the values over the grid. Note that :math:`h^2` is related to :math:`\\mathrm{ln}(\delta)` through the `sigmoid function <https://en.wikipedia.org/wiki/Sigmoid_function>`_. More precisely,. .. math::. h^2 = 1 - \mathrm{sigmoid}(\\mathrm{ln}(\delta)) = \mathrm{sigmoid}(-\\mathrm{ln}(\delta)). Hence one can change variables to extract a high-resolution discretization of the likelihood function of :math:`h^2` over :math:`[0,1]` at the corresponding REML estimators for :math:`\\beta` and :math:`\sigma_g^2`, as well as integrate over the normalized likelihood function using `change of variables <https://en.wikipedia.org/wiki/Integration_by_substitution>`_ and the `sigmoid differential equation <https://en.wikipedia.org/wiki/Sigmoid_function#Properties>`_. For convenience, ``global.lmmreg.fit.normLkhdH2`` records the the likelihood function of :math:`h^2` normalized over the discrete grid ``0.01, 0.02, ..., 0.98, 0.99``. The length of the array is 101 so tha",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:133274,Testability,log,log,133274,"egrate over the normalized likelihood function using `change of variables <https://en.wikipedia.org/wiki/Integration_by_substitution>`_ and the `sigmoid differential equation <https://en.wikipedia.org/wiki/Sigmoid_function#Properties>`_. For convenience, ``global.lmmreg.fit.normLkhdH2`` records the the likelihood function of :math:`h^2` normalized over the discrete grid ``0.01, 0.02, ..., 0.98, 0.99``. The length of the array is 101 so that index ``i`` contains the likelihood at percentage ``i``. The values at indices 0 and 100 are left undefined. By the theory of maximum likelihood estimation, this normalized likelihood function is approximately normally distributed near the maximum likelihood estimate. So we estimate the standard error of the estimator of :math:`h^2` as follows. Let :math:`x_2` be the maximum likelihood estimate of :math:`h^2` and let :math:`x_ 1` and :math:`x_3` be just to the left and right of :math:`x_2`. Let :math:`y_1`, :math:`y_2`, and :math:`y_3` be the corresponding values of the (unnormalized) log likelihood function. Setting equal the leading coefficient of the unique parabola through these points (as given by Lagrange interpolation) and the leading coefficient of the log of the normal distribution, we have:. .. math::. \\frac{x_3 (y_2 - y_1) + x_2 (y_1 - y_3) + x_1 (y_3 - y_2))}{(x_2 - x_1)(x_1 - x_3)(x_3 - x_2)} = -\\frac{1}{2 \sigma^2}. The standard error :math:`\\hat{\sigma}` is then estimated by solving for :math:`\sigma`. Note that the mean and standard deviation of the (discretized or continuous) distribution held in ``global.lmmreg.fit.normLkhdH2`` will not coincide with :math:`\\hat{h}^2` and :math:`\\hat{\sigma}`, since this distribution only becomes normal in the infinite sample limit. One can visually assess normality by plotting this distribution against a normal distribution with the same mean and standard deviation, or use this distribution to approximate credible intervals under a flat prior on :math:`h^2`. **Testing each ",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:133453,Testability,log,log,133453,"ential equation <https://en.wikipedia.org/wiki/Sigmoid_function#Properties>`_. For convenience, ``global.lmmreg.fit.normLkhdH2`` records the the likelihood function of :math:`h^2` normalized over the discrete grid ``0.01, 0.02, ..., 0.98, 0.99``. The length of the array is 101 so that index ``i`` contains the likelihood at percentage ``i``. The values at indices 0 and 100 are left undefined. By the theory of maximum likelihood estimation, this normalized likelihood function is approximately normally distributed near the maximum likelihood estimate. So we estimate the standard error of the estimator of :math:`h^2` as follows. Let :math:`x_2` be the maximum likelihood estimate of :math:`h^2` and let :math:`x_ 1` and :math:`x_3` be just to the left and right of :math:`x_2`. Let :math:`y_1`, :math:`y_2`, and :math:`y_3` be the corresponding values of the (unnormalized) log likelihood function. Setting equal the leading coefficient of the unique parabola through these points (as given by Lagrange interpolation) and the leading coefficient of the log of the normal distribution, we have:. .. math::. \\frac{x_3 (y_2 - y_1) + x_2 (y_1 - y_3) + x_1 (y_3 - y_2))}{(x_2 - x_1)(x_1 - x_3)(x_3 - x_2)} = -\\frac{1}{2 \sigma^2}. The standard error :math:`\\hat{\sigma}` is then estimated by solving for :math:`\sigma`. Note that the mean and standard deviation of the (discretized or continuous) distribution held in ``global.lmmreg.fit.normLkhdH2`` will not coincide with :math:`\\hat{h}^2` and :math:`\\hat{\sigma}`, since this distribution only becomes normal in the infinite sample limit. One can visually assess normality by plotting this distribution against a normal distribution with the same mean and standard deviation, or use this distribution to approximate credible intervals under a flat prior on :math:`h^2`. **Testing each variant for association**. Fixing a single variant, we define:. - :math:`v = n \\times 1` vector of genotypes, with missing genotypes imputed as the mean of c",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:134978,Testability,test,test,134978," approximate credible intervals under a flat prior on :math:`h^2`. **Testing each variant for association**. Fixing a single variant, we define:. - :math:`v = n \\times 1` vector of genotypes, with missing genotypes imputed as the mean of called genotypes; - :math:`X_v = \\left[v | X \\right] = n \\times (1 + c)` matrix concatenating :math:`v` and :math:`X`; - :math:`\\beta_v = (\\beta^0_v, \\beta^1_v, \\ldots, \\beta^c_v) = (1 + c) \\times 1` vector of covariate coefficients. Fixing :math:`\delta` at the global REML estimate :math:`\\hat{\delta}`, we find the REML estimate :math:`(\\hat{\\beta}_v, \\hat{\sigma}_{g,v}^2)` via rotation of the model. .. math::. y \\sim \\mathrm{N}\\left(X_v\\beta_v, \sigma_{g,v}^2 (K + \\hat{\delta} I)\\right). Note that the only new rotation to compute here is :math:`U^T v`. To test the null hypothesis that the genotype coefficient :math:`\\beta^0_v` is zero, we consider the restricted model with parameters :math:`((0, \\beta^1_v, \ldots, \\beta^c_v), \sigma_{g,v}^2)` within the full model with parameters :math:`(\\beta^0_v, \\beta^1_v, \\ldots, \\beta^c_v), \sigma_{g_v}^2)`, with :math:`\delta` fixed at :math:`\\hat\delta` in both. The latter fit is simply that of the global model, :math:`((0, \\hat{\\beta}^1, \\ldots, \\hat{\\beta}^c), \\hat{\sigma}_g^2)`. The likelihood ratio test statistic is given by. .. math::. \chi^2 = n \\, \\mathrm{ln}\left(\\frac{\hat{\sigma}^2_g}{\\hat{\sigma}_{g,v}^2}\\right). and follows a chi-squared distribution with one degree of freedom. Here the ratio :math:`\\hat{\sigma}^2_g / \\hat{\sigma}_{g,v}^2` captures the degree to which adding the variant :math:`v` to the global model reduces the residual phenotypic variance. **Kinship Matrix**. FastLMM uses the Realized Relationship Matrix (RRM) for kinship. This can be computed with :py:meth:`~hail.VariantDataset.rrm`. However, any instance of :py:class:`KinshipMatrix` may be used, so long as ``sample_list`` contains the complete samples of the caller vari",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:135489,Testability,test,test,135489," :math:`v` and :math:`X`; - :math:`\\beta_v = (\\beta^0_v, \\beta^1_v, \\ldots, \\beta^c_v) = (1 + c) \\times 1` vector of covariate coefficients. Fixing :math:`\delta` at the global REML estimate :math:`\\hat{\delta}`, we find the REML estimate :math:`(\\hat{\\beta}_v, \\hat{\sigma}_{g,v}^2)` via rotation of the model. .. math::. y \\sim \\mathrm{N}\\left(X_v\\beta_v, \sigma_{g,v}^2 (K + \\hat{\delta} I)\\right). Note that the only new rotation to compute here is :math:`U^T v`. To test the null hypothesis that the genotype coefficient :math:`\\beta^0_v` is zero, we consider the restricted model with parameters :math:`((0, \\beta^1_v, \ldots, \\beta^c_v), \sigma_{g,v}^2)` within the full model with parameters :math:`(\\beta^0_v, \\beta^1_v, \\ldots, \\beta^c_v), \sigma_{g_v}^2)`, with :math:`\delta` fixed at :math:`\\hat\delta` in both. The latter fit is simply that of the global model, :math:`((0, \\hat{\\beta}^1, \\ldots, \\hat{\\beta}^c), \\hat{\sigma}_g^2)`. The likelihood ratio test statistic is given by. .. math::. \chi^2 = n \\, \\mathrm{ln}\left(\\frac{\hat{\sigma}^2_g}{\\hat{\sigma}_{g,v}^2}\\right). and follows a chi-squared distribution with one degree of freedom. Here the ratio :math:`\\hat{\sigma}^2_g / \\hat{\sigma}_{g,v}^2` captures the degree to which adding the variant :math:`v` to the global model reduces the residual phenotypic variance. **Kinship Matrix**. FastLMM uses the Realized Relationship Matrix (RRM) for kinship. This can be computed with :py:meth:`~hail.VariantDataset.rrm`. However, any instance of :py:class:`KinshipMatrix` may be used, so long as ``sample_list`` contains the complete samples of the caller variant dataset in the same order. **Low-rank approximation of kinship for improved performance**. :py:meth:`.lmmreg` can implicitly use a low-rank approximation of the kinship matrix to more rapidly fit delta and the statistics for each variant. The computational complexity per variant is proportional to the number of eigenvectors used.",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:138089,Testability,test,testing,138089,"istory and mathematics of linear mixed models in genetics, including `FastLMM <https://www.microsoft.com/en-us/research/project/fastlmm/>`__, see `Christoph Lippert's PhD thesis <https://publikationen.uni-tuebingen.de/xmlui/bitstream/handle/10900/50003/pdf/thesis_komplett.pdf>`__. For an investigation of various approaches to defining kinship, see `Comparison of Methods to Account for Relatedness in Genome-Wide Association Studies with Family-Based Data <http://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.1004445>`__. :param kinshipMatrix: Kinship matrix to be used.; :type kinshipMatrix: :class:`KinshipMatrix`. :param str y: Response sample annotation. :param covariates: List of covariate sample annotations.; :type covariates: list of str. :param str global_root: Global annotation root, a period-delimited path starting with `global`. :param str va_root: Variant annotation root, a period-delimited path starting with `va`. :param bool run_assoc: If true, run association testing in addition to fitting the global model. :param bool use_ml: Use ML instead of REML throughout. :param delta: Fixed delta value to use in the global model, overrides fitting delta.; :type delta: float or None. :param float sparsity_threshold: Genotype vector sparsity at or below which to use sparse genotype vector in rotation (advanced). :param bool use_dosages: If true, use dosages rather than hard call genotypes. :param int n_eigs: Number of eigenvectors of the kinship matrix used to fit the model. :param float dropped_variance_fraction: Upper bound on fraction of sample variance lost by dropping eigenvectors with small eigenvalues. :return: Variant dataset with linear mixed regression annotations.; :rtype: :py:class:`.VariantDataset`; """""". jvds = self._jvdf.lmmreg(kinshipMatrix._jkm, y, jarray(Env.jvm().java.lang.String, covariates),; use_ml, global_root, va_root, run_assoc, joption(delta), sparsity_threshold,; use_dosages, joption(n_eigs), joption(dropped_variance_fraction)",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:139188,Testability,test,test,139188,"use in the global model, overrides fitting delta.; :type delta: float or None. :param float sparsity_threshold: Genotype vector sparsity at or below which to use sparse genotype vector in rotation (advanced). :param bool use_dosages: If true, use dosages rather than hard call genotypes. :param int n_eigs: Number of eigenvectors of the kinship matrix used to fit the model. :param float dropped_variance_fraction: Upper bound on fraction of sample variance lost by dropping eigenvectors with small eigenvalues. :return: Variant dataset with linear mixed regression annotations.; :rtype: :py:class:`.VariantDataset`; """""". jvds = self._jvdf.lmmreg(kinshipMatrix._jkm, y, jarray(Env.jvm().java.lang.String, covariates),; use_ml, global_root, va_root, run_assoc, joption(delta), sparsity_threshold,; use_dosages, joption(n_eigs), joption(dropped_variance_fraction)); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(test=strlike,; y=strlike,; covariates=listof(strlike),; root=strlike,; use_dosages=bool); def logreg(self, test, y, covariates=[], root='va.logreg', use_dosages=False):; """"""Test each variant for association using logistic regression. .. include:: requireTGenotype.rst. **Examples**. Run the logistic regression Wald test per variant using a Boolean phenotype and two covariates stored; in sample annotations:. >>> vds_result = vds.logreg('wald', 'sa.pheno.isCase', covariates=['sa.pheno.age', 'sa.pheno.isFemale']). **Notes**. The :py:meth:`~hail.VariantDataset.logreg` method performs,; for each variant, a significance test of the genotype in; predicting a binary (case-control) phenotype based on the; logistic regression model. The phenotype type must either be numeric (with all; present values 0 or 1) or Boolean, in which case true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'), Rao score test ('score'),; and Firth test ('firth'). Hail only includes samples for whi",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:139282,Testability,log,logreg,139282,"use in the global model, overrides fitting delta.; :type delta: float or None. :param float sparsity_threshold: Genotype vector sparsity at or below which to use sparse genotype vector in rotation (advanced). :param bool use_dosages: If true, use dosages rather than hard call genotypes. :param int n_eigs: Number of eigenvectors of the kinship matrix used to fit the model. :param float dropped_variance_fraction: Upper bound on fraction of sample variance lost by dropping eigenvectors with small eigenvalues. :return: Variant dataset with linear mixed regression annotations.; :rtype: :py:class:`.VariantDataset`; """""". jvds = self._jvdf.lmmreg(kinshipMatrix._jkm, y, jarray(Env.jvm().java.lang.String, covariates),; use_ml, global_root, va_root, run_assoc, joption(delta), sparsity_threshold,; use_dosages, joption(n_eigs), joption(dropped_variance_fraction)); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(test=strlike,; y=strlike,; covariates=listof(strlike),; root=strlike,; use_dosages=bool); def logreg(self, test, y, covariates=[], root='va.logreg', use_dosages=False):; """"""Test each variant for association using logistic regression. .. include:: requireTGenotype.rst. **Examples**. Run the logistic regression Wald test per variant using a Boolean phenotype and two covariates stored; in sample annotations:. >>> vds_result = vds.logreg('wald', 'sa.pheno.isCase', covariates=['sa.pheno.age', 'sa.pheno.isFemale']). **Notes**. The :py:meth:`~hail.VariantDataset.logreg` method performs,; for each variant, a significance test of the genotype in; predicting a binary (case-control) phenotype based on the; logistic regression model. The phenotype type must either be numeric (with all; present values 0 or 1) or Boolean, in which case true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'), Rao score test ('score'),; and Firth test ('firth'). Hail only includes samples for whi",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:139295,Testability,test,test,139295,"use in the global model, overrides fitting delta.; :type delta: float or None. :param float sparsity_threshold: Genotype vector sparsity at or below which to use sparse genotype vector in rotation (advanced). :param bool use_dosages: If true, use dosages rather than hard call genotypes. :param int n_eigs: Number of eigenvectors of the kinship matrix used to fit the model. :param float dropped_variance_fraction: Upper bound on fraction of sample variance lost by dropping eigenvectors with small eigenvalues. :return: Variant dataset with linear mixed regression annotations.; :rtype: :py:class:`.VariantDataset`; """""". jvds = self._jvdf.lmmreg(kinshipMatrix._jkm, y, jarray(Env.jvm().java.lang.String, covariates),; use_ml, global_root, va_root, run_assoc, joption(delta), sparsity_threshold,; use_dosages, joption(n_eigs), joption(dropped_variance_fraction)); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(test=strlike,; y=strlike,; covariates=listof(strlike),; root=strlike,; use_dosages=bool); def logreg(self, test, y, covariates=[], root='va.logreg', use_dosages=False):; """"""Test each variant for association using logistic regression. .. include:: requireTGenotype.rst. **Examples**. Run the logistic regression Wald test per variant using a Boolean phenotype and two covariates stored; in sample annotations:. >>> vds_result = vds.logreg('wald', 'sa.pheno.isCase', covariates=['sa.pheno.age', 'sa.pheno.isFemale']). **Notes**. The :py:meth:`~hail.VariantDataset.logreg` method performs,; for each variant, a significance test of the genotype in; predicting a binary (case-control) phenotype based on the; logistic regression model. The phenotype type must either be numeric (with all; present values 0 or 1) or Boolean, in which case true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'), Rao score test ('score'),; and Firth test ('firth'). Hail only includes samples for whi",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:139328,Testability,log,logreg,139328,"ow which to use sparse genotype vector in rotation (advanced). :param bool use_dosages: If true, use dosages rather than hard call genotypes. :param int n_eigs: Number of eigenvectors of the kinship matrix used to fit the model. :param float dropped_variance_fraction: Upper bound on fraction of sample variance lost by dropping eigenvectors with small eigenvalues. :return: Variant dataset with linear mixed regression annotations.; :rtype: :py:class:`.VariantDataset`; """""". jvds = self._jvdf.lmmreg(kinshipMatrix._jkm, y, jarray(Env.jvm().java.lang.String, covariates),; use_ml, global_root, va_root, run_assoc, joption(delta), sparsity_threshold,; use_dosages, joption(n_eigs), joption(dropped_variance_fraction)); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(test=strlike,; y=strlike,; covariates=listof(strlike),; root=strlike,; use_dosages=bool); def logreg(self, test, y, covariates=[], root='va.logreg', use_dosages=False):; """"""Test each variant for association using logistic regression. .. include:: requireTGenotype.rst. **Examples**. Run the logistic regression Wald test per variant using a Boolean phenotype and two covariates stored; in sample annotations:. >>> vds_result = vds.logreg('wald', 'sa.pheno.isCase', covariates=['sa.pheno.age', 'sa.pheno.isFemale']). **Notes**. The :py:meth:`~hail.VariantDataset.logreg` method performs,; for each variant, a significance test of the genotype in; predicting a binary (case-control) phenotype based on the; logistic regression model. The phenotype type must either be numeric (with all; present values 0 or 1) or Boolean, in which case true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'), Rao score test ('score'),; and Firth test ('firth'). Hail only includes samples for which the phenotype and all covariates are; defined. For each variant, Hail imputes missing genotypes as the mean of called genotypes. By default, ge",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:139401,Testability,log,logistic,139401,"ow which to use sparse genotype vector in rotation (advanced). :param bool use_dosages: If true, use dosages rather than hard call genotypes. :param int n_eigs: Number of eigenvectors of the kinship matrix used to fit the model. :param float dropped_variance_fraction: Upper bound on fraction of sample variance lost by dropping eigenvectors with small eigenvalues. :return: Variant dataset with linear mixed regression annotations.; :rtype: :py:class:`.VariantDataset`; """""". jvds = self._jvdf.lmmreg(kinshipMatrix._jkm, y, jarray(Env.jvm().java.lang.String, covariates),; use_ml, global_root, va_root, run_assoc, joption(delta), sparsity_threshold,; use_dosages, joption(n_eigs), joption(dropped_variance_fraction)); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(test=strlike,; y=strlike,; covariates=listof(strlike),; root=strlike,; use_dosages=bool); def logreg(self, test, y, covariates=[], root='va.logreg', use_dosages=False):; """"""Test each variant for association using logistic regression. .. include:: requireTGenotype.rst. **Examples**. Run the logistic regression Wald test per variant using a Boolean phenotype and two covariates stored; in sample annotations:. >>> vds_result = vds.logreg('wald', 'sa.pheno.isCase', covariates=['sa.pheno.age', 'sa.pheno.isFemale']). **Notes**. The :py:meth:`~hail.VariantDataset.logreg` method performs,; for each variant, a significance test of the genotype in; predicting a binary (case-control) phenotype based on the; logistic regression model. The phenotype type must either be numeric (with all; present values 0 or 1) or Boolean, in which case true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'), Rao score test ('score'),; and Firth test ('firth'). Hail only includes samples for which the phenotype and all covariates are; defined. For each variant, Hail imputes missing genotypes as the mean of called genotypes. By default, ge",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:139479,Testability,log,logistic,139479,": Number of eigenvectors of the kinship matrix used to fit the model. :param float dropped_variance_fraction: Upper bound on fraction of sample variance lost by dropping eigenvectors with small eigenvalues. :return: Variant dataset with linear mixed regression annotations.; :rtype: :py:class:`.VariantDataset`; """""". jvds = self._jvdf.lmmreg(kinshipMatrix._jkm, y, jarray(Env.jvm().java.lang.String, covariates),; use_ml, global_root, va_root, run_assoc, joption(delta), sparsity_threshold,; use_dosages, joption(n_eigs), joption(dropped_variance_fraction)); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(test=strlike,; y=strlike,; covariates=listof(strlike),; root=strlike,; use_dosages=bool); def logreg(self, test, y, covariates=[], root='va.logreg', use_dosages=False):; """"""Test each variant for association using logistic regression. .. include:: requireTGenotype.rst. **Examples**. Run the logistic regression Wald test per variant using a Boolean phenotype and two covariates stored; in sample annotations:. >>> vds_result = vds.logreg('wald', 'sa.pheno.isCase', covariates=['sa.pheno.age', 'sa.pheno.isFemale']). **Notes**. The :py:meth:`~hail.VariantDataset.logreg` method performs,; for each variant, a significance test of the genotype in; predicting a binary (case-control) phenotype based on the; logistic regression model. The phenotype type must either be numeric (with all; present values 0 or 1) or Boolean, in which case true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'), Rao score test ('score'),; and Firth test ('firth'). Hail only includes samples for which the phenotype and all covariates are; defined. For each variant, Hail imputes missing genotypes as the mean of called genotypes. By default, genotypes values are given by hard call genotypes (``g.gt``).; If ``use_dosages=True``, then genotype values are defined by the dosage; :math:`\mathrm{P}(\mathrm",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:139504,Testability,test,test,139504,": Number of eigenvectors of the kinship matrix used to fit the model. :param float dropped_variance_fraction: Upper bound on fraction of sample variance lost by dropping eigenvectors with small eigenvalues. :return: Variant dataset with linear mixed regression annotations.; :rtype: :py:class:`.VariantDataset`; """""". jvds = self._jvdf.lmmreg(kinshipMatrix._jkm, y, jarray(Env.jvm().java.lang.String, covariates),; use_ml, global_root, va_root, run_assoc, joption(delta), sparsity_threshold,; use_dosages, joption(n_eigs), joption(dropped_variance_fraction)); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(test=strlike,; y=strlike,; covariates=listof(strlike),; root=strlike,; use_dosages=bool); def logreg(self, test, y, covariates=[], root='va.logreg', use_dosages=False):; """"""Test each variant for association using logistic regression. .. include:: requireTGenotype.rst. **Examples**. Run the logistic regression Wald test per variant using a Boolean phenotype and two covariates stored; in sample annotations:. >>> vds_result = vds.logreg('wald', 'sa.pheno.isCase', covariates=['sa.pheno.age', 'sa.pheno.isFemale']). **Notes**. The :py:meth:`~hail.VariantDataset.logreg` method performs,; for each variant, a significance test of the genotype in; predicting a binary (case-control) phenotype based on the; logistic regression model. The phenotype type must either be numeric (with all; present values 0 or 1) or Boolean, in which case true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'), Rao score test ('score'),; and Firth test ('firth'). Hail only includes samples for which the phenotype and all covariates are; defined. For each variant, Hail imputes missing genotypes as the mean of called genotypes. By default, genotypes values are given by hard call genotypes (``g.gt``).; If ``use_dosages=True``, then genotype values are defined by the dosage; :math:`\mathrm{P}(\mathrm",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:139619,Testability,log,logreg,139619,"ance_fraction: Upper bound on fraction of sample variance lost by dropping eigenvectors with small eigenvalues. :return: Variant dataset with linear mixed regression annotations.; :rtype: :py:class:`.VariantDataset`; """""". jvds = self._jvdf.lmmreg(kinshipMatrix._jkm, y, jarray(Env.jvm().java.lang.String, covariates),; use_ml, global_root, va_root, run_assoc, joption(delta), sparsity_threshold,; use_dosages, joption(n_eigs), joption(dropped_variance_fraction)); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(test=strlike,; y=strlike,; covariates=listof(strlike),; root=strlike,; use_dosages=bool); def logreg(self, test, y, covariates=[], root='va.logreg', use_dosages=False):; """"""Test each variant for association using logistic regression. .. include:: requireTGenotype.rst. **Examples**. Run the logistic regression Wald test per variant using a Boolean phenotype and two covariates stored; in sample annotations:. >>> vds_result = vds.logreg('wald', 'sa.pheno.isCase', covariates=['sa.pheno.age', 'sa.pheno.isFemale']). **Notes**. The :py:meth:`~hail.VariantDataset.logreg` method performs,; for each variant, a significance test of the genotype in; predicting a binary (case-control) phenotype based on the; logistic regression model. The phenotype type must either be numeric (with all; present values 0 or 1) or Boolean, in which case true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'), Rao score test ('score'),; and Firth test ('firth'). Hail only includes samples for which the phenotype and all covariates are; defined. For each variant, Hail imputes missing genotypes as the mean of called genotypes. By default, genotypes values are given by hard call genotypes (``g.gt``).; If ``use_dosages=True``, then genotype values are defined by the dosage; :math:`\mathrm{P}(\mathrm{Het}) + 2 \cdot \mathrm{P}(\mathrm{HomVar})`. For Phred-scaled values,; :math:`\mathrm{P}(\mat",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:139750,Testability,log,logreg,139750,"tDataset`; """""". jvds = self._jvdf.lmmreg(kinshipMatrix._jkm, y, jarray(Env.jvm().java.lang.String, covariates),; use_ml, global_root, va_root, run_assoc, joption(delta), sparsity_threshold,; use_dosages, joption(n_eigs), joption(dropped_variance_fraction)); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(test=strlike,; y=strlike,; covariates=listof(strlike),; root=strlike,; use_dosages=bool); def logreg(self, test, y, covariates=[], root='va.logreg', use_dosages=False):; """"""Test each variant for association using logistic regression. .. include:: requireTGenotype.rst. **Examples**. Run the logistic regression Wald test per variant using a Boolean phenotype and two covariates stored; in sample annotations:. >>> vds_result = vds.logreg('wald', 'sa.pheno.isCase', covariates=['sa.pheno.age', 'sa.pheno.isFemale']). **Notes**. The :py:meth:`~hail.VariantDataset.logreg` method performs,; for each variant, a significance test of the genotype in; predicting a binary (case-control) phenotype based on the; logistic regression model. The phenotype type must either be numeric (with all; present values 0 or 1) or Boolean, in which case true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'), Rao score test ('score'),; and Firth test ('firth'). Hail only includes samples for which the phenotype and all covariates are; defined. For each variant, Hail imputes missing genotypes as the mean of called genotypes. By default, genotypes values are given by hard call genotypes (``g.gt``).; If ``use_dosages=True``, then genotype values are defined by the dosage; :math:`\mathrm{P}(\mathrm{Het}) + 2 \cdot \mathrm{P}(\mathrm{HomVar})`. For Phred-scaled values,; :math:`\mathrm{P}(\mathrm{Het})` and :math:`\mathrm{P}(\mathrm{HomVar})` are; calculated by normalizing the PL likelihoods (converted from the Phred-scale) to sum to 1. The example above considers a model of the form. .. math::.",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:139809,Testability,test,test,139809,"tDataset`; """""". jvds = self._jvdf.lmmreg(kinshipMatrix._jkm, y, jarray(Env.jvm().java.lang.String, covariates),; use_ml, global_root, va_root, run_assoc, joption(delta), sparsity_threshold,; use_dosages, joption(n_eigs), joption(dropped_variance_fraction)); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(test=strlike,; y=strlike,; covariates=listof(strlike),; root=strlike,; use_dosages=bool); def logreg(self, test, y, covariates=[], root='va.logreg', use_dosages=False):; """"""Test each variant for association using logistic regression. .. include:: requireTGenotype.rst. **Examples**. Run the logistic regression Wald test per variant using a Boolean phenotype and two covariates stored; in sample annotations:. >>> vds_result = vds.logreg('wald', 'sa.pheno.isCase', covariates=['sa.pheno.age', 'sa.pheno.isFemale']). **Notes**. The :py:meth:`~hail.VariantDataset.logreg` method performs,; for each variant, a significance test of the genotype in; predicting a binary (case-control) phenotype based on the; logistic regression model. The phenotype type must either be numeric (with all; present values 0 or 1) or Boolean, in which case true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'), Rao score test ('score'),; and Firth test ('firth'). Hail only includes samples for which the phenotype and all covariates are; defined. For each variant, Hail imputes missing genotypes as the mean of called genotypes. By default, genotypes values are given by hard call genotypes (``g.gt``).; If ``use_dosages=True``, then genotype values are defined by the dosage; :math:`\mathrm{P}(\mathrm{Het}) + 2 \cdot \mathrm{P}(\mathrm{HomVar})`. For Phred-scaled values,; :math:`\mathrm{P}(\mathrm{Het})` and :math:`\mathrm{P}(\mathrm{HomVar})` are; calculated by normalizing the PL likelihoods (converted from the Phred-scale) to sum to 1. The example above considers a model of the form. .. math::.",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:139893,Testability,log,logistic,139893,"tDataset`; """""". jvds = self._jvdf.lmmreg(kinshipMatrix._jkm, y, jarray(Env.jvm().java.lang.String, covariates),; use_ml, global_root, va_root, run_assoc, joption(delta), sparsity_threshold,; use_dosages, joption(n_eigs), joption(dropped_variance_fraction)); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(test=strlike,; y=strlike,; covariates=listof(strlike),; root=strlike,; use_dosages=bool); def logreg(self, test, y, covariates=[], root='va.logreg', use_dosages=False):; """"""Test each variant for association using logistic regression. .. include:: requireTGenotype.rst. **Examples**. Run the logistic regression Wald test per variant using a Boolean phenotype and two covariates stored; in sample annotations:. >>> vds_result = vds.logreg('wald', 'sa.pheno.isCase', covariates=['sa.pheno.age', 'sa.pheno.isFemale']). **Notes**. The :py:meth:`~hail.VariantDataset.logreg` method performs,; for each variant, a significance test of the genotype in; predicting a binary (case-control) phenotype based on the; logistic regression model. The phenotype type must either be numeric (with all; present values 0 or 1) or Boolean, in which case true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'), Rao score test ('score'),; and Firth test ('firth'). Hail only includes samples for which the phenotype and all covariates are; defined. For each variant, Hail imputes missing genotypes as the mean of called genotypes. By default, genotypes values are given by hard call genotypes (``g.gt``).; If ``use_dosages=True``, then genotype values are defined by the dosage; :math:`\mathrm{P}(\mathrm{Het}) + 2 \cdot \mathrm{P}(\mathrm{HomVar})`. For Phred-scaled values,; :math:`\mathrm{P}(\mathrm{Het})` and :math:`\mathrm{P}(\mathrm{HomVar})` are; calculated by normalizing the PL likelihoods (converted from the Phred-scale) to sum to 1. The example above considers a model of the form. .. math::.",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:140096,Testability,test,test,140096,"cs] @handle_py4j; @requireTGenotype; @typecheck_method(test=strlike,; y=strlike,; covariates=listof(strlike),; root=strlike,; use_dosages=bool); def logreg(self, test, y, covariates=[], root='va.logreg', use_dosages=False):; """"""Test each variant for association using logistic regression. .. include:: requireTGenotype.rst. **Examples**. Run the logistic regression Wald test per variant using a Boolean phenotype and two covariates stored; in sample annotations:. >>> vds_result = vds.logreg('wald', 'sa.pheno.isCase', covariates=['sa.pheno.age', 'sa.pheno.isFemale']). **Notes**. The :py:meth:`~hail.VariantDataset.logreg` method performs,; for each variant, a significance test of the genotype in; predicting a binary (case-control) phenotype based on the; logistic regression model. The phenotype type must either be numeric (with all; present values 0 or 1) or Boolean, in which case true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'), Rao score test ('score'),; and Firth test ('firth'). Hail only includes samples for which the phenotype and all covariates are; defined. For each variant, Hail imputes missing genotypes as the mean of called genotypes. By default, genotypes values are given by hard call genotypes (``g.gt``).; If ``use_dosages=True``, then genotype values are defined by the dosage; :math:`\mathrm{P}(\mathrm{Het}) + 2 \cdot \mathrm{P}(\mathrm{HomVar})`. For Phred-scaled values,; :math:`\mathrm{P}(\mathrm{Het})` and :math:`\mathrm{P}(\mathrm{HomVar})` are; calculated by normalizing the PL likelihoods (converted from the Phred-scale) to sum to 1. The example above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{isCase}) = \mathrm{sigmoid}(\\beta_0 + \\beta_1 \, \mathrm{gt} + \\beta_2 \, \mathrm{age} + \\beta_3 \, \mathrm{isFemale} + \\varepsilon), \quad \\varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid; function <https://en.wikipedia.org/wiki",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:140128,Testability,test,test,140128,"cs] @handle_py4j; @requireTGenotype; @typecheck_method(test=strlike,; y=strlike,; covariates=listof(strlike),; root=strlike,; use_dosages=bool); def logreg(self, test, y, covariates=[], root='va.logreg', use_dosages=False):; """"""Test each variant for association using logistic regression. .. include:: requireTGenotype.rst. **Examples**. Run the logistic regression Wald test per variant using a Boolean phenotype and two covariates stored; in sample annotations:. >>> vds_result = vds.logreg('wald', 'sa.pheno.isCase', covariates=['sa.pheno.age', 'sa.pheno.isFemale']). **Notes**. The :py:meth:`~hail.VariantDataset.logreg` method performs,; for each variant, a significance test of the genotype in; predicting a binary (case-control) phenotype based on the; logistic regression model. The phenotype type must either be numeric (with all; present values 0 or 1) or Boolean, in which case true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'), Rao score test ('score'),; and Firth test ('firth'). Hail only includes samples for which the phenotype and all covariates are; defined. For each variant, Hail imputes missing genotypes as the mean of called genotypes. By default, genotypes values are given by hard call genotypes (``g.gt``).; If ``use_dosages=True``, then genotype values are defined by the dosage; :math:`\mathrm{P}(\mathrm{Het}) + 2 \cdot \mathrm{P}(\mathrm{HomVar})`. For Phred-scaled values,; :math:`\mathrm{P}(\mathrm{Het})` and :math:`\mathrm{P}(\mathrm{HomVar})` are; calculated by normalizing the PL likelihoods (converted from the Phred-scale) to sum to 1. The example above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{isCase}) = \mathrm{sigmoid}(\\beta_0 + \\beta_1 \, \mathrm{gt} + \\beta_2 \, \mathrm{age} + \\beta_3 \, \mathrm{isFemale} + \\varepsilon), \quad \\varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid; function <https://en.wikipedia.org/wiki",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:140152,Testability,test,test,140152,"cs] @handle_py4j; @requireTGenotype; @typecheck_method(test=strlike,; y=strlike,; covariates=listof(strlike),; root=strlike,; use_dosages=bool); def logreg(self, test, y, covariates=[], root='va.logreg', use_dosages=False):; """"""Test each variant for association using logistic regression. .. include:: requireTGenotype.rst. **Examples**. Run the logistic regression Wald test per variant using a Boolean phenotype and two covariates stored; in sample annotations:. >>> vds_result = vds.logreg('wald', 'sa.pheno.isCase', covariates=['sa.pheno.age', 'sa.pheno.isFemale']). **Notes**. The :py:meth:`~hail.VariantDataset.logreg` method performs,; for each variant, a significance test of the genotype in; predicting a binary (case-control) phenotype based on the; logistic regression model. The phenotype type must either be numeric (with all; present values 0 or 1) or Boolean, in which case true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'), Rao score test ('score'),; and Firth test ('firth'). Hail only includes samples for which the phenotype and all covariates are; defined. For each variant, Hail imputes missing genotypes as the mean of called genotypes. By default, genotypes values are given by hard call genotypes (``g.gt``).; If ``use_dosages=True``, then genotype values are defined by the dosage; :math:`\mathrm{P}(\mathrm{Het}) + 2 \cdot \mathrm{P}(\mathrm{HomVar})`. For Phred-scaled values,; :math:`\mathrm{P}(\mathrm{Het})` and :math:`\mathrm{P}(\mathrm{HomVar})` are; calculated by normalizing the PL likelihoods (converted from the Phred-scale) to sum to 1. The example above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{isCase}) = \mathrm{sigmoid}(\\beta_0 + \\beta_1 \, \mathrm{gt} + \\beta_2 \, \mathrm{age} + \\beta_3 \, \mathrm{isFemale} + \\varepsilon), \quad \\varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid; function <https://en.wikipedia.org/wiki",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:140179,Testability,test,test,140179,"cs] @handle_py4j; @requireTGenotype; @typecheck_method(test=strlike,; y=strlike,; covariates=listof(strlike),; root=strlike,; use_dosages=bool); def logreg(self, test, y, covariates=[], root='va.logreg', use_dosages=False):; """"""Test each variant for association using logistic regression. .. include:: requireTGenotype.rst. **Examples**. Run the logistic regression Wald test per variant using a Boolean phenotype and two covariates stored; in sample annotations:. >>> vds_result = vds.logreg('wald', 'sa.pheno.isCase', covariates=['sa.pheno.age', 'sa.pheno.isFemale']). **Notes**. The :py:meth:`~hail.VariantDataset.logreg` method performs,; for each variant, a significance test of the genotype in; predicting a binary (case-control) phenotype based on the; logistic regression model. The phenotype type must either be numeric (with all; present values 0 or 1) or Boolean, in which case true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'), Rao score test ('score'),; and Firth test ('firth'). Hail only includes samples for which the phenotype and all covariates are; defined. For each variant, Hail imputes missing genotypes as the mean of called genotypes. By default, genotypes values are given by hard call genotypes (``g.gt``).; If ``use_dosages=True``, then genotype values are defined by the dosage; :math:`\mathrm{P}(\mathrm{Het}) + 2 \cdot \mathrm{P}(\mathrm{HomVar})`. For Phred-scaled values,; :math:`\mathrm{P}(\mathrm{Het})` and :math:`\mathrm{P}(\mathrm{HomVar})` are; calculated by normalizing the PL likelihoods (converted from the Phred-scale) to sum to 1. The example above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{isCase}) = \mathrm{sigmoid}(\\beta_0 + \\beta_1 \, \mathrm{gt} + \\beta_2 \, \mathrm{age} + \\beta_3 \, \mathrm{isFemale} + \\varepsilon), \quad \\varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid; function <https://en.wikipedia.org/wiki",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:141446,Testability,test,test,141446,"_dosages=True``, then genotype values are defined by the dosage; :math:`\mathrm{P}(\mathrm{Het}) + 2 \cdot \mathrm{P}(\mathrm{HomVar})`. For Phred-scaled values,; :math:`\mathrm{P}(\mathrm{Het})` and :math:`\mathrm{P}(\mathrm{HomVar})` are; calculated by normalizing the PL likelihoods (converted from the Phred-scale) to sum to 1. The example above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{isCase}) = \mathrm{sigmoid}(\\beta_0 + \\beta_1 \, \mathrm{gt} + \\beta_2 \, \mathrm{age} + \\beta_3 \, \mathrm{isFemale} + \\varepsilon), \quad \\varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid; function <https://en.wikipedia.org/wiki/Sigmoid_function>`__, the; genotype :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for; Het, and 2 for HomVar, and the Boolean covariate; :math:`\mathrm{isFemale}` is coded as 1 for true (female) and; 0 for false (male). The null model sets :math:`\\beta_1 = 0`. The resulting variant annotations depend on the test statistic; as shown in the tables below. ========== =================== ====== =====; Test Annotation Type Value; ========== =================== ====== =====; Wald ``va.logreg.beta`` Double fit genotype coefficient, :math:`\hat\\beta_1`; Wald ``va.logreg.se`` Double estimated standard error, :math:`\widehat{\mathrm{se}}`; Wald ``va.logreg.zstat`` Double Wald :math:`z`-statistic, equal to :math:`\hat\\beta_1 / \widehat{\mathrm{se}}`; Wald ``va.logreg.pval`` Double Wald p-value testing :math:`\\beta_1 = 0`; LRT, Firth ``va.logreg.beta`` Double fit genotype coefficient, :math:`\hat\\beta_1`; LRT, Firth ``va.logreg.chi2`` Double deviance statistic; LRT, Firth ``va.logreg.pval`` Double LRT / Firth p-value testing :math:`\\beta_1 = 0`; Score ``va.logreg.chi2`` Double score statistic; Score ``va.logreg.pval`` Double score p-value testing :math:`\\beta_1 = 0`; ========== =================== ====== =====. For the Wald and likelihood ratio tests, Hail fits the logistic model for each vari",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:141620,Testability,log,logreg,141620,"}(\mathrm{Het})` and :math:`\mathrm{P}(\mathrm{HomVar})` are; calculated by normalizing the PL likelihoods (converted from the Phred-scale) to sum to 1. The example above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{isCase}) = \mathrm{sigmoid}(\\beta_0 + \\beta_1 \, \mathrm{gt} + \\beta_2 \, \mathrm{age} + \\beta_3 \, \mathrm{isFemale} + \\varepsilon), \quad \\varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid; function <https://en.wikipedia.org/wiki/Sigmoid_function>`__, the; genotype :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for; Het, and 2 for HomVar, and the Boolean covariate; :math:`\mathrm{isFemale}` is coded as 1 for true (female) and; 0 for false (male). The null model sets :math:`\\beta_1 = 0`. The resulting variant annotations depend on the test statistic; as shown in the tables below. ========== =================== ====== =====; Test Annotation Type Value; ========== =================== ====== =====; Wald ``va.logreg.beta`` Double fit genotype coefficient, :math:`\hat\\beta_1`; Wald ``va.logreg.se`` Double estimated standard error, :math:`\widehat{\mathrm{se}}`; Wald ``va.logreg.zstat`` Double Wald :math:`z`-statistic, equal to :math:`\hat\\beta_1 / \widehat{\mathrm{se}}`; Wald ``va.logreg.pval`` Double Wald p-value testing :math:`\\beta_1 = 0`; LRT, Firth ``va.logreg.beta`` Double fit genotype coefficient, :math:`\hat\\beta_1`; LRT, Firth ``va.logreg.chi2`` Double deviance statistic; LRT, Firth ``va.logreg.pval`` Double LRT / Firth p-value testing :math:`\\beta_1 = 0`; Score ``va.logreg.chi2`` Double score statistic; Score ``va.logreg.pval`` Double score p-value testing :math:`\\beta_1 = 0`; ========== =================== ====== =====. For the Wald and likelihood ratio tests, Hail fits the logistic model for each variant using Newton iteration and only emits the above annotations when the maximum likelihood estimate of the coefficients converges. The Firth test uses a modified form of Newton it",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:141699,Testability,log,logreg,141699,"malizing the PL likelihoods (converted from the Phred-scale) to sum to 1. The example above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{isCase}) = \mathrm{sigmoid}(\\beta_0 + \\beta_1 \, \mathrm{gt} + \\beta_2 \, \mathrm{age} + \\beta_3 \, \mathrm{isFemale} + \\varepsilon), \quad \\varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid; function <https://en.wikipedia.org/wiki/Sigmoid_function>`__, the; genotype :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for; Het, and 2 for HomVar, and the Boolean covariate; :math:`\mathrm{isFemale}` is coded as 1 for true (female) and; 0 for false (male). The null model sets :math:`\\beta_1 = 0`. The resulting variant annotations depend on the test statistic; as shown in the tables below. ========== =================== ====== =====; Test Annotation Type Value; ========== =================== ====== =====; Wald ``va.logreg.beta`` Double fit genotype coefficient, :math:`\hat\\beta_1`; Wald ``va.logreg.se`` Double estimated standard error, :math:`\widehat{\mathrm{se}}`; Wald ``va.logreg.zstat`` Double Wald :math:`z`-statistic, equal to :math:`\hat\\beta_1 / \widehat{\mathrm{se}}`; Wald ``va.logreg.pval`` Double Wald p-value testing :math:`\\beta_1 = 0`; LRT, Firth ``va.logreg.beta`` Double fit genotype coefficient, :math:`\hat\\beta_1`; LRT, Firth ``va.logreg.chi2`` Double deviance statistic; LRT, Firth ``va.logreg.pval`` Double LRT / Firth p-value testing :math:`\\beta_1 = 0`; Score ``va.logreg.chi2`` Double score statistic; Score ``va.logreg.pval`` Double score p-value testing :math:`\\beta_1 = 0`; ========== =================== ====== =====. For the Wald and likelihood ratio tests, Hail fits the logistic model for each variant using Newton iteration and only emits the above annotations when the maximum likelihood estimate of the coefficients converges. The Firth test uses a modified form of Newton iteration. To help diagnose convergence issues, Hail also emits three variant ann",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:141785,Testability,log,logreg,141785,"above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{isCase}) = \mathrm{sigmoid}(\\beta_0 + \\beta_1 \, \mathrm{gt} + \\beta_2 \, \mathrm{age} + \\beta_3 \, \mathrm{isFemale} + \\varepsilon), \quad \\varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid; function <https://en.wikipedia.org/wiki/Sigmoid_function>`__, the; genotype :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for; Het, and 2 for HomVar, and the Boolean covariate; :math:`\mathrm{isFemale}` is coded as 1 for true (female) and; 0 for false (male). The null model sets :math:`\\beta_1 = 0`. The resulting variant annotations depend on the test statistic; as shown in the tables below. ========== =================== ====== =====; Test Annotation Type Value; ========== =================== ====== =====; Wald ``va.logreg.beta`` Double fit genotype coefficient, :math:`\hat\\beta_1`; Wald ``va.logreg.se`` Double estimated standard error, :math:`\widehat{\mathrm{se}}`; Wald ``va.logreg.zstat`` Double Wald :math:`z`-statistic, equal to :math:`\hat\\beta_1 / \widehat{\mathrm{se}}`; Wald ``va.logreg.pval`` Double Wald p-value testing :math:`\\beta_1 = 0`; LRT, Firth ``va.logreg.beta`` Double fit genotype coefficient, :math:`\hat\\beta_1`; LRT, Firth ``va.logreg.chi2`` Double deviance statistic; LRT, Firth ``va.logreg.pval`` Double LRT / Firth p-value testing :math:`\\beta_1 = 0`; Score ``va.logreg.chi2`` Double score statistic; Score ``va.logreg.pval`` Double score p-value testing :math:`\\beta_1 = 0`; ========== =================== ====== =====. For the Wald and likelihood ratio tests, Hail fits the logistic model for each variant using Newton iteration and only emits the above annotations when the maximum likelihood estimate of the coefficients converges. The Firth test uses a modified form of Newton iteration. To help diagnose convergence issues, Hail also emits three variant annotations which summarize the iterative fitting process:. ================ ============",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:141898,Testability,log,logreg,141898,"ta_1 \, \mathrm{gt} + \\beta_2 \, \mathrm{age} + \\beta_3 \, \mathrm{isFemale} + \\varepsilon), \quad \\varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid; function <https://en.wikipedia.org/wiki/Sigmoid_function>`__, the; genotype :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for; Het, and 2 for HomVar, and the Boolean covariate; :math:`\mathrm{isFemale}` is coded as 1 for true (female) and; 0 for false (male). The null model sets :math:`\\beta_1 = 0`. The resulting variant annotations depend on the test statistic; as shown in the tables below. ========== =================== ====== =====; Test Annotation Type Value; ========== =================== ====== =====; Wald ``va.logreg.beta`` Double fit genotype coefficient, :math:`\hat\\beta_1`; Wald ``va.logreg.se`` Double estimated standard error, :math:`\widehat{\mathrm{se}}`; Wald ``va.logreg.zstat`` Double Wald :math:`z`-statistic, equal to :math:`\hat\\beta_1 / \widehat{\mathrm{se}}`; Wald ``va.logreg.pval`` Double Wald p-value testing :math:`\\beta_1 = 0`; LRT, Firth ``va.logreg.beta`` Double fit genotype coefficient, :math:`\hat\\beta_1`; LRT, Firth ``va.logreg.chi2`` Double deviance statistic; LRT, Firth ``va.logreg.pval`` Double LRT / Firth p-value testing :math:`\\beta_1 = 0`; Score ``va.logreg.chi2`` Double score statistic; Score ``va.logreg.pval`` Double score p-value testing :math:`\\beta_1 = 0`; ========== =================== ====== =====. For the Wald and likelihood ratio tests, Hail fits the logistic model for each variant using Newton iteration and only emits the above annotations when the maximum likelihood estimate of the coefficients converges. The Firth test uses a modified form of Newton iteration. To help diagnose convergence issues, Hail also emits three variant annotations which summarize the iterative fitting process:. ================ =========================== ======= =====; Test Annotation Type Value; ================ =========================== ======= =",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:141932,Testability,test,testing,141932,"m{age} + \\beta_3 \, \mathrm{isFemale} + \\varepsilon), \quad \\varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid; function <https://en.wikipedia.org/wiki/Sigmoid_function>`__, the; genotype :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for; Het, and 2 for HomVar, and the Boolean covariate; :math:`\mathrm{isFemale}` is coded as 1 for true (female) and; 0 for false (male). The null model sets :math:`\\beta_1 = 0`. The resulting variant annotations depend on the test statistic; as shown in the tables below. ========== =================== ====== =====; Test Annotation Type Value; ========== =================== ====== =====; Wald ``va.logreg.beta`` Double fit genotype coefficient, :math:`\hat\\beta_1`; Wald ``va.logreg.se`` Double estimated standard error, :math:`\widehat{\mathrm{se}}`; Wald ``va.logreg.zstat`` Double Wald :math:`z`-statistic, equal to :math:`\hat\\beta_1 / \widehat{\mathrm{se}}`; Wald ``va.logreg.pval`` Double Wald p-value testing :math:`\\beta_1 = 0`; LRT, Firth ``va.logreg.beta`` Double fit genotype coefficient, :math:`\hat\\beta_1`; LRT, Firth ``va.logreg.chi2`` Double deviance statistic; LRT, Firth ``va.logreg.pval`` Double LRT / Firth p-value testing :math:`\\beta_1 = 0`; Score ``va.logreg.chi2`` Double score statistic; Score ``va.logreg.pval`` Double score p-value testing :math:`\\beta_1 = 0`; ========== =================== ====== =====. For the Wald and likelihood ratio tests, Hail fits the logistic model for each variant using Newton iteration and only emits the above annotations when the maximum likelihood estimate of the coefficients converges. The Firth test uses a modified form of Newton iteration. To help diagnose convergence issues, Hail also emits three variant annotations which summarize the iterative fitting process:. ================ =========================== ======= =====; Test Annotation Type Value; ================ =========================== ======= =====; Wald, LRT, Firth ``va.logreg.fit.n",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:141978,Testability,log,logreg,141978," \\varepsilon), \quad \\varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid; function <https://en.wikipedia.org/wiki/Sigmoid_function>`__, the; genotype :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for; Het, and 2 for HomVar, and the Boolean covariate; :math:`\mathrm{isFemale}` is coded as 1 for true (female) and; 0 for false (male). The null model sets :math:`\\beta_1 = 0`. The resulting variant annotations depend on the test statistic; as shown in the tables below. ========== =================== ====== =====; Test Annotation Type Value; ========== =================== ====== =====; Wald ``va.logreg.beta`` Double fit genotype coefficient, :math:`\hat\\beta_1`; Wald ``va.logreg.se`` Double estimated standard error, :math:`\widehat{\mathrm{se}}`; Wald ``va.logreg.zstat`` Double Wald :math:`z`-statistic, equal to :math:`\hat\\beta_1 / \widehat{\mathrm{se}}`; Wald ``va.logreg.pval`` Double Wald p-value testing :math:`\\beta_1 = 0`; LRT, Firth ``va.logreg.beta`` Double fit genotype coefficient, :math:`\hat\\beta_1`; LRT, Firth ``va.logreg.chi2`` Double deviance statistic; LRT, Firth ``va.logreg.pval`` Double LRT / Firth p-value testing :math:`\\beta_1 = 0`; Score ``va.logreg.chi2`` Double score statistic; Score ``va.logreg.pval`` Double score p-value testing :math:`\\beta_1 = 0`; ========== =================== ====== =====. For the Wald and likelihood ratio tests, Hail fits the logistic model for each variant using Newton iteration and only emits the above annotations when the maximum likelihood estimate of the coefficients converges. The Firth test uses a modified form of Newton iteration. To help diagnose convergence issues, Hail also emits three variant annotations which summarize the iterative fitting process:. ================ =========================== ======= =====; Test Annotation Type Value; ================ =========================== ======= =====; Wald, LRT, Firth ``va.logreg.fit.nIter`` Int number of iterations until co",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:142063,Testability,log,logreg,142063,"{sigmoid}` is the `sigmoid; function <https://en.wikipedia.org/wiki/Sigmoid_function>`__, the; genotype :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for; Het, and 2 for HomVar, and the Boolean covariate; :math:`\mathrm{isFemale}` is coded as 1 for true (female) and; 0 for false (male). The null model sets :math:`\\beta_1 = 0`. The resulting variant annotations depend on the test statistic; as shown in the tables below. ========== =================== ====== =====; Test Annotation Type Value; ========== =================== ====== =====; Wald ``va.logreg.beta`` Double fit genotype coefficient, :math:`\hat\\beta_1`; Wald ``va.logreg.se`` Double estimated standard error, :math:`\widehat{\mathrm{se}}`; Wald ``va.logreg.zstat`` Double Wald :math:`z`-statistic, equal to :math:`\hat\\beta_1 / \widehat{\mathrm{se}}`; Wald ``va.logreg.pval`` Double Wald p-value testing :math:`\\beta_1 = 0`; LRT, Firth ``va.logreg.beta`` Double fit genotype coefficient, :math:`\hat\\beta_1`; LRT, Firth ``va.logreg.chi2`` Double deviance statistic; LRT, Firth ``va.logreg.pval`` Double LRT / Firth p-value testing :math:`\\beta_1 = 0`; Score ``va.logreg.chi2`` Double score statistic; Score ``va.logreg.pval`` Double score p-value testing :math:`\\beta_1 = 0`; ========== =================== ====== =====. For the Wald and likelihood ratio tests, Hail fits the logistic model for each variant using Newton iteration and only emits the above annotations when the maximum likelihood estimate of the coefficients converges. The Firth test uses a modified form of Newton iteration. To help diagnose convergence issues, Hail also emits three variant annotations which summarize the iterative fitting process:. ================ =========================== ======= =====; Test Annotation Type Value; ================ =========================== ======= =====; Wald, LRT, Firth ``va.logreg.fit.nIter`` Int number of iterations until convergence, explosion, or reaching the max (25 for Wald, LRT; 100 for Firth); Wald, LR",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:142120,Testability,log,logreg,142120,"a.org/wiki/Sigmoid_function>`__, the; genotype :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for; Het, and 2 for HomVar, and the Boolean covariate; :math:`\mathrm{isFemale}` is coded as 1 for true (female) and; 0 for false (male). The null model sets :math:`\\beta_1 = 0`. The resulting variant annotations depend on the test statistic; as shown in the tables below. ========== =================== ====== =====; Test Annotation Type Value; ========== =================== ====== =====; Wald ``va.logreg.beta`` Double fit genotype coefficient, :math:`\hat\\beta_1`; Wald ``va.logreg.se`` Double estimated standard error, :math:`\widehat{\mathrm{se}}`; Wald ``va.logreg.zstat`` Double Wald :math:`z`-statistic, equal to :math:`\hat\\beta_1 / \widehat{\mathrm{se}}`; Wald ``va.logreg.pval`` Double Wald p-value testing :math:`\\beta_1 = 0`; LRT, Firth ``va.logreg.beta`` Double fit genotype coefficient, :math:`\hat\\beta_1`; LRT, Firth ``va.logreg.chi2`` Double deviance statistic; LRT, Firth ``va.logreg.pval`` Double LRT / Firth p-value testing :math:`\\beta_1 = 0`; Score ``va.logreg.chi2`` Double score statistic; Score ``va.logreg.pval`` Double score p-value testing :math:`\\beta_1 = 0`; ========== =================== ====== =====. For the Wald and likelihood ratio tests, Hail fits the logistic model for each variant using Newton iteration and only emits the above annotations when the maximum likelihood estimate of the coefficients converges. The Firth test uses a modified form of Newton iteration. To help diagnose convergence issues, Hail also emits three variant annotations which summarize the iterative fitting process:. ================ =========================== ======= =====; Test Annotation Type Value; ================ =========================== ======= =====; Wald, LRT, Firth ``va.logreg.fit.nIter`` Int number of iterations until convergence, explosion, or reaching the max (25 for Wald, LRT; 100 for Firth); Wald, LRT, Firth ``va.logreg.fit.converged`` Boolean true if iter",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:142161,Testability,test,testing,142161,"otype :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for; Het, and 2 for HomVar, and the Boolean covariate; :math:`\mathrm{isFemale}` is coded as 1 for true (female) and; 0 for false (male). The null model sets :math:`\\beta_1 = 0`. The resulting variant annotations depend on the test statistic; as shown in the tables below. ========== =================== ====== =====; Test Annotation Type Value; ========== =================== ====== =====; Wald ``va.logreg.beta`` Double fit genotype coefficient, :math:`\hat\\beta_1`; Wald ``va.logreg.se`` Double estimated standard error, :math:`\widehat{\mathrm{se}}`; Wald ``va.logreg.zstat`` Double Wald :math:`z`-statistic, equal to :math:`\hat\\beta_1 / \widehat{\mathrm{se}}`; Wald ``va.logreg.pval`` Double Wald p-value testing :math:`\\beta_1 = 0`; LRT, Firth ``va.logreg.beta`` Double fit genotype coefficient, :math:`\hat\\beta_1`; LRT, Firth ``va.logreg.chi2`` Double deviance statistic; LRT, Firth ``va.logreg.pval`` Double LRT / Firth p-value testing :math:`\\beta_1 = 0`; Score ``va.logreg.chi2`` Double score statistic; Score ``va.logreg.pval`` Double score p-value testing :math:`\\beta_1 = 0`; ========== =================== ====== =====. For the Wald and likelihood ratio tests, Hail fits the logistic model for each variant using Newton iteration and only emits the above annotations when the maximum likelihood estimate of the coefficients converges. The Firth test uses a modified form of Newton iteration. To help diagnose convergence issues, Hail also emits three variant annotations which summarize the iterative fitting process:. ================ =========================== ======= =====; Test Annotation Type Value; ================ =========================== ======= =====; Wald, LRT, Firth ``va.logreg.fit.nIter`` Int number of iterations until convergence, explosion, or reaching the max (25 for Wald, LRT; 100 for Firth); Wald, LRT, Firth ``va.logreg.fit.converged`` Boolean true if iteration converged; Wald, LRT, Firth ``va.lo",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:142202,Testability,log,logreg,142202,"or HomRef, 1 for; Het, and 2 for HomVar, and the Boolean covariate; :math:`\mathrm{isFemale}` is coded as 1 for true (female) and; 0 for false (male). The null model sets :math:`\\beta_1 = 0`. The resulting variant annotations depend on the test statistic; as shown in the tables below. ========== =================== ====== =====; Test Annotation Type Value; ========== =================== ====== =====; Wald ``va.logreg.beta`` Double fit genotype coefficient, :math:`\hat\\beta_1`; Wald ``va.logreg.se`` Double estimated standard error, :math:`\widehat{\mathrm{se}}`; Wald ``va.logreg.zstat`` Double Wald :math:`z`-statistic, equal to :math:`\hat\\beta_1 / \widehat{\mathrm{se}}`; Wald ``va.logreg.pval`` Double Wald p-value testing :math:`\\beta_1 = 0`; LRT, Firth ``va.logreg.beta`` Double fit genotype coefficient, :math:`\hat\\beta_1`; LRT, Firth ``va.logreg.chi2`` Double deviance statistic; LRT, Firth ``va.logreg.pval`` Double LRT / Firth p-value testing :math:`\\beta_1 = 0`; Score ``va.logreg.chi2`` Double score statistic; Score ``va.logreg.pval`` Double score p-value testing :math:`\\beta_1 = 0`; ========== =================== ====== =====. For the Wald and likelihood ratio tests, Hail fits the logistic model for each variant using Newton iteration and only emits the above annotations when the maximum likelihood estimate of the coefficients converges. The Firth test uses a modified form of Newton iteration. To help diagnose convergence issues, Hail also emits three variant annotations which summarize the iterative fitting process:. ================ =========================== ======= =====; Test Annotation Type Value; ================ =========================== ======= =====; Wald, LRT, Firth ``va.logreg.fit.nIter`` Int number of iterations until convergence, explosion, or reaching the max (25 for Wald, LRT; 100 for Firth); Wald, LRT, Firth ``va.logreg.fit.converged`` Boolean true if iteration converged; Wald, LRT, Firth ``va.logreg.fit.exploded`` Boolean true if itera",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:142251,Testability,log,logreg,142251,"Boolean covariate; :math:`\mathrm{isFemale}` is coded as 1 for true (female) and; 0 for false (male). The null model sets :math:`\\beta_1 = 0`. The resulting variant annotations depend on the test statistic; as shown in the tables below. ========== =================== ====== =====; Test Annotation Type Value; ========== =================== ====== =====; Wald ``va.logreg.beta`` Double fit genotype coefficient, :math:`\hat\\beta_1`; Wald ``va.logreg.se`` Double estimated standard error, :math:`\widehat{\mathrm{se}}`; Wald ``va.logreg.zstat`` Double Wald :math:`z`-statistic, equal to :math:`\hat\\beta_1 / \widehat{\mathrm{se}}`; Wald ``va.logreg.pval`` Double Wald p-value testing :math:`\\beta_1 = 0`; LRT, Firth ``va.logreg.beta`` Double fit genotype coefficient, :math:`\hat\\beta_1`; LRT, Firth ``va.logreg.chi2`` Double deviance statistic; LRT, Firth ``va.logreg.pval`` Double LRT / Firth p-value testing :math:`\\beta_1 = 0`; Score ``va.logreg.chi2`` Double score statistic; Score ``va.logreg.pval`` Double score p-value testing :math:`\\beta_1 = 0`; ========== =================== ====== =====. For the Wald and likelihood ratio tests, Hail fits the logistic model for each variant using Newton iteration and only emits the above annotations when the maximum likelihood estimate of the coefficients converges. The Firth test uses a modified form of Newton iteration. To help diagnose convergence issues, Hail also emits three variant annotations which summarize the iterative fitting process:. ================ =========================== ======= =====; Test Annotation Type Value; ================ =========================== ======= =====; Wald, LRT, Firth ``va.logreg.fit.nIter`` Int number of iterations until convergence, explosion, or reaching the max (25 for Wald, LRT; 100 for Firth); Wald, LRT, Firth ``va.logreg.fit.converged`` Boolean true if iteration converged; Wald, LRT, Firth ``va.logreg.fit.exploded`` Boolean true if iteration exploded; ================ =================",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:142286,Testability,test,testing,142286,"s 1 for true (female) and; 0 for false (male). The null model sets :math:`\\beta_1 = 0`. The resulting variant annotations depend on the test statistic; as shown in the tables below. ========== =================== ====== =====; Test Annotation Type Value; ========== =================== ====== =====; Wald ``va.logreg.beta`` Double fit genotype coefficient, :math:`\hat\\beta_1`; Wald ``va.logreg.se`` Double estimated standard error, :math:`\widehat{\mathrm{se}}`; Wald ``va.logreg.zstat`` Double Wald :math:`z`-statistic, equal to :math:`\hat\\beta_1 / \widehat{\mathrm{se}}`; Wald ``va.logreg.pval`` Double Wald p-value testing :math:`\\beta_1 = 0`; LRT, Firth ``va.logreg.beta`` Double fit genotype coefficient, :math:`\hat\\beta_1`; LRT, Firth ``va.logreg.chi2`` Double deviance statistic; LRT, Firth ``va.logreg.pval`` Double LRT / Firth p-value testing :math:`\\beta_1 = 0`; Score ``va.logreg.chi2`` Double score statistic; Score ``va.logreg.pval`` Double score p-value testing :math:`\\beta_1 = 0`; ========== =================== ====== =====. For the Wald and likelihood ratio tests, Hail fits the logistic model for each variant using Newton iteration and only emits the above annotations when the maximum likelihood estimate of the coefficients converges. The Firth test uses a modified form of Newton iteration. To help diagnose convergence issues, Hail also emits three variant annotations which summarize the iterative fitting process:. ================ =========================== ======= =====; Test Annotation Type Value; ================ =========================== ======= =====; Wald, LRT, Firth ``va.logreg.fit.nIter`` Int number of iterations until convergence, explosion, or reaching the max (25 for Wald, LRT; 100 for Firth); Wald, LRT, Firth ``va.logreg.fit.converged`` Boolean true if iteration converged; Wald, LRT, Firth ``va.logreg.fit.exploded`` Boolean true if iteration exploded; ================ =========================== ======= =====. We consider iteration to hav",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:142395,Testability,test,tests,142395,"own in the tables below. ========== =================== ====== =====; Test Annotation Type Value; ========== =================== ====== =====; Wald ``va.logreg.beta`` Double fit genotype coefficient, :math:`\hat\\beta_1`; Wald ``va.logreg.se`` Double estimated standard error, :math:`\widehat{\mathrm{se}}`; Wald ``va.logreg.zstat`` Double Wald :math:`z`-statistic, equal to :math:`\hat\\beta_1 / \widehat{\mathrm{se}}`; Wald ``va.logreg.pval`` Double Wald p-value testing :math:`\\beta_1 = 0`; LRT, Firth ``va.logreg.beta`` Double fit genotype coefficient, :math:`\hat\\beta_1`; LRT, Firth ``va.logreg.chi2`` Double deviance statistic; LRT, Firth ``va.logreg.pval`` Double LRT / Firth p-value testing :math:`\\beta_1 = 0`; Score ``va.logreg.chi2`` Double score statistic; Score ``va.logreg.pval`` Double score p-value testing :math:`\\beta_1 = 0`; ========== =================== ====== =====. For the Wald and likelihood ratio tests, Hail fits the logistic model for each variant using Newton iteration and only emits the above annotations when the maximum likelihood estimate of the coefficients converges. The Firth test uses a modified form of Newton iteration. To help diagnose convergence issues, Hail also emits three variant annotations which summarize the iterative fitting process:. ================ =========================== ======= =====; Test Annotation Type Value; ================ =========================== ======= =====; Wald, LRT, Firth ``va.logreg.fit.nIter`` Int number of iterations until convergence, explosion, or reaching the max (25 for Wald, LRT; 100 for Firth); Wald, LRT, Firth ``va.logreg.fit.converged`` Boolean true if iteration converged; Wald, LRT, Firth ``va.logreg.fit.exploded`` Boolean true if iteration exploded; ================ =========================== ======= =====. We consider iteration to have converged when every coordinate of :math:`\\beta` changes by less than :math:`10^{-6}`. For Wald and LRT, up to 25 iterations are attempted; in testing we fi",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:142416,Testability,log,logistic,142416,"own in the tables below. ========== =================== ====== =====; Test Annotation Type Value; ========== =================== ====== =====; Wald ``va.logreg.beta`` Double fit genotype coefficient, :math:`\hat\\beta_1`; Wald ``va.logreg.se`` Double estimated standard error, :math:`\widehat{\mathrm{se}}`; Wald ``va.logreg.zstat`` Double Wald :math:`z`-statistic, equal to :math:`\hat\\beta_1 / \widehat{\mathrm{se}}`; Wald ``va.logreg.pval`` Double Wald p-value testing :math:`\\beta_1 = 0`; LRT, Firth ``va.logreg.beta`` Double fit genotype coefficient, :math:`\hat\\beta_1`; LRT, Firth ``va.logreg.chi2`` Double deviance statistic; LRT, Firth ``va.logreg.pval`` Double LRT / Firth p-value testing :math:`\\beta_1 = 0`; Score ``va.logreg.chi2`` Double score statistic; Score ``va.logreg.pval`` Double score p-value testing :math:`\\beta_1 = 0`; ========== =================== ====== =====. For the Wald and likelihood ratio tests, Hail fits the logistic model for each variant using Newton iteration and only emits the above annotations when the maximum likelihood estimate of the coefficients converges. The Firth test uses a modified form of Newton iteration. To help diagnose convergence issues, Hail also emits three variant annotations which summarize the iterative fitting process:. ================ =========================== ======= =====; Test Annotation Type Value; ================ =========================== ======= =====; Wald, LRT, Firth ``va.logreg.fit.nIter`` Int number of iterations until convergence, explosion, or reaching the max (25 for Wald, LRT; 100 for Firth); Wald, LRT, Firth ``va.logreg.fit.converged`` Boolean true if iteration converged; Wald, LRT, Firth ``va.logreg.fit.exploded`` Boolean true if iteration exploded; ================ =========================== ======= =====. We consider iteration to have converged when every coordinate of :math:`\\beta` changes by less than :math:`10^{-6}`. For Wald and LRT, up to 25 iterations are attempted; in testing we fi",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:142586,Testability,test,test,142586,"=====; Wald ``va.logreg.beta`` Double fit genotype coefficient, :math:`\hat\\beta_1`; Wald ``va.logreg.se`` Double estimated standard error, :math:`\widehat{\mathrm{se}}`; Wald ``va.logreg.zstat`` Double Wald :math:`z`-statistic, equal to :math:`\hat\\beta_1 / \widehat{\mathrm{se}}`; Wald ``va.logreg.pval`` Double Wald p-value testing :math:`\\beta_1 = 0`; LRT, Firth ``va.logreg.beta`` Double fit genotype coefficient, :math:`\hat\\beta_1`; LRT, Firth ``va.logreg.chi2`` Double deviance statistic; LRT, Firth ``va.logreg.pval`` Double LRT / Firth p-value testing :math:`\\beta_1 = 0`; Score ``va.logreg.chi2`` Double score statistic; Score ``va.logreg.pval`` Double score p-value testing :math:`\\beta_1 = 0`; ========== =================== ====== =====. For the Wald and likelihood ratio tests, Hail fits the logistic model for each variant using Newton iteration and only emits the above annotations when the maximum likelihood estimate of the coefficients converges. The Firth test uses a modified form of Newton iteration. To help diagnose convergence issues, Hail also emits three variant annotations which summarize the iterative fitting process:. ================ =========================== ======= =====; Test Annotation Type Value; ================ =========================== ======= =====; Wald, LRT, Firth ``va.logreg.fit.nIter`` Int number of iterations until convergence, explosion, or reaching the max (25 for Wald, LRT; 100 for Firth); Wald, LRT, Firth ``va.logreg.fit.converged`` Boolean true if iteration converged; Wald, LRT, Firth ``va.logreg.fit.exploded`` Boolean true if iteration exploded; ================ =========================== ======= =====. We consider iteration to have converged when every coordinate of :math:`\\beta` changes by less than :math:`10^{-6}`. For Wald and LRT, up to 25 iterations are attempted; in testing we find 4 or 5 iterations nearly always suffice. Convergence may also fail due to explosion, which refers to low-level numerical linear algeb",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:142930,Testability,log,logreg,142930,"esting :math:`\\beta_1 = 0`; LRT, Firth ``va.logreg.beta`` Double fit genotype coefficient, :math:`\hat\\beta_1`; LRT, Firth ``va.logreg.chi2`` Double deviance statistic; LRT, Firth ``va.logreg.pval`` Double LRT / Firth p-value testing :math:`\\beta_1 = 0`; Score ``va.logreg.chi2`` Double score statistic; Score ``va.logreg.pval`` Double score p-value testing :math:`\\beta_1 = 0`; ========== =================== ====== =====. For the Wald and likelihood ratio tests, Hail fits the logistic model for each variant using Newton iteration and only emits the above annotations when the maximum likelihood estimate of the coefficients converges. The Firth test uses a modified form of Newton iteration. To help diagnose convergence issues, Hail also emits three variant annotations which summarize the iterative fitting process:. ================ =========================== ======= =====; Test Annotation Type Value; ================ =========================== ======= =====; Wald, LRT, Firth ``va.logreg.fit.nIter`` Int number of iterations until convergence, explosion, or reaching the max (25 for Wald, LRT; 100 for Firth); Wald, LRT, Firth ``va.logreg.fit.converged`` Boolean true if iteration converged; Wald, LRT, Firth ``va.logreg.fit.exploded`` Boolean true if iteration exploded; ================ =========================== ======= =====. We consider iteration to have converged when every coordinate of :math:`\\beta` changes by less than :math:`10^{-6}`. For Wald and LRT, up to 25 iterations are attempted; in testing we find 4 or 5 iterations nearly always suffice. Convergence may also fail due to explosion, which refers to low-level numerical linear algebra exceptions caused by manipulating ill-conditioned matrices. Explosion may result from (nearly) linearly dependent covariates or complete `separation <https://en.wikipedia.org/wiki/Separation_(statistics)>`__. A more common situation in genetics is quasi-complete seperation, e.g. variants that are observed only in cases (or co",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:143081,Testability,log,logreg,143081,"deviance statistic; LRT, Firth ``va.logreg.pval`` Double LRT / Firth p-value testing :math:`\\beta_1 = 0`; Score ``va.logreg.chi2`` Double score statistic; Score ``va.logreg.pval`` Double score p-value testing :math:`\\beta_1 = 0`; ========== =================== ====== =====. For the Wald and likelihood ratio tests, Hail fits the logistic model for each variant using Newton iteration and only emits the above annotations when the maximum likelihood estimate of the coefficients converges. The Firth test uses a modified form of Newton iteration. To help diagnose convergence issues, Hail also emits three variant annotations which summarize the iterative fitting process:. ================ =========================== ======= =====; Test Annotation Type Value; ================ =========================== ======= =====; Wald, LRT, Firth ``va.logreg.fit.nIter`` Int number of iterations until convergence, explosion, or reaching the max (25 for Wald, LRT; 100 for Firth); Wald, LRT, Firth ``va.logreg.fit.converged`` Boolean true if iteration converged; Wald, LRT, Firth ``va.logreg.fit.exploded`` Boolean true if iteration exploded; ================ =========================== ======= =====. We consider iteration to have converged when every coordinate of :math:`\\beta` changes by less than :math:`10^{-6}`. For Wald and LRT, up to 25 iterations are attempted; in testing we find 4 or 5 iterations nearly always suffice. Convergence may also fail due to explosion, which refers to low-level numerical linear algebra exceptions caused by manipulating ill-conditioned matrices. Explosion may result from (nearly) linearly dependent covariates or complete `separation <https://en.wikipedia.org/wiki/Separation_(statistics)>`__. A more common situation in genetics is quasi-complete seperation, e.g. variants that are observed only in cases (or controls). Such variants inevitably arise when testing millions of variants with very low minor allele count. The maximum likelihood estimate of :math:`\",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:143163,Testability,log,logreg,143163,"ng :math:`\\beta_1 = 0`; Score ``va.logreg.chi2`` Double score statistic; Score ``va.logreg.pval`` Double score p-value testing :math:`\\beta_1 = 0`; ========== =================== ====== =====. For the Wald and likelihood ratio tests, Hail fits the logistic model for each variant using Newton iteration and only emits the above annotations when the maximum likelihood estimate of the coefficients converges. The Firth test uses a modified form of Newton iteration. To help diagnose convergence issues, Hail also emits three variant annotations which summarize the iterative fitting process:. ================ =========================== ======= =====; Test Annotation Type Value; ================ =========================== ======= =====; Wald, LRT, Firth ``va.logreg.fit.nIter`` Int number of iterations until convergence, explosion, or reaching the max (25 for Wald, LRT; 100 for Firth); Wald, LRT, Firth ``va.logreg.fit.converged`` Boolean true if iteration converged; Wald, LRT, Firth ``va.logreg.fit.exploded`` Boolean true if iteration exploded; ================ =========================== ======= =====. We consider iteration to have converged when every coordinate of :math:`\\beta` changes by less than :math:`10^{-6}`. For Wald and LRT, up to 25 iterations are attempted; in testing we find 4 or 5 iterations nearly always suffice. Convergence may also fail due to explosion, which refers to low-level numerical linear algebra exceptions caused by manipulating ill-conditioned matrices. Explosion may result from (nearly) linearly dependent covariates or complete `separation <https://en.wikipedia.org/wiki/Separation_(statistics)>`__. A more common situation in genetics is quasi-complete seperation, e.g. variants that are observed only in cases (or controls). Such variants inevitably arise when testing millions of variants with very low minor allele count. The maximum likelihood estimate of :math:`\\beta` under logistic regression is then undefined but convergence may still occur",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:143455,Testability,test,testing,143455,"Newton iteration and only emits the above annotations when the maximum likelihood estimate of the coefficients converges. The Firth test uses a modified form of Newton iteration. To help diagnose convergence issues, Hail also emits three variant annotations which summarize the iterative fitting process:. ================ =========================== ======= =====; Test Annotation Type Value; ================ =========================== ======= =====; Wald, LRT, Firth ``va.logreg.fit.nIter`` Int number of iterations until convergence, explosion, or reaching the max (25 for Wald, LRT; 100 for Firth); Wald, LRT, Firth ``va.logreg.fit.converged`` Boolean true if iteration converged; Wald, LRT, Firth ``va.logreg.fit.exploded`` Boolean true if iteration exploded; ================ =========================== ======= =====. We consider iteration to have converged when every coordinate of :math:`\\beta` changes by less than :math:`10^{-6}`. For Wald and LRT, up to 25 iterations are attempted; in testing we find 4 or 5 iterations nearly always suffice. Convergence may also fail due to explosion, which refers to low-level numerical linear algebra exceptions caused by manipulating ill-conditioned matrices. Explosion may result from (nearly) linearly dependent covariates or complete `separation <https://en.wikipedia.org/wiki/Separation_(statistics)>`__. A more common situation in genetics is quasi-complete seperation, e.g. variants that are observed only in cases (or controls). Such variants inevitably arise when testing millions of variants with very low minor allele count. The maximum likelihood estimate of :math:`\\beta` under logistic regression is then undefined but convergence may still occur after a large number of iterations due to a very flat likelihood surface. In testing, we find that such variants produce a secondary bump from 10 to 15 iterations in the histogram of number of iterations per variant. We also find that this faux convergence produces large standard errors",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:143979,Testability,test,testing,143979,", explosion, or reaching the max (25 for Wald, LRT; 100 for Firth); Wald, LRT, Firth ``va.logreg.fit.converged`` Boolean true if iteration converged; Wald, LRT, Firth ``va.logreg.fit.exploded`` Boolean true if iteration exploded; ================ =========================== ======= =====. We consider iteration to have converged when every coordinate of :math:`\\beta` changes by less than :math:`10^{-6}`. For Wald and LRT, up to 25 iterations are attempted; in testing we find 4 or 5 iterations nearly always suffice. Convergence may also fail due to explosion, which refers to low-level numerical linear algebra exceptions caused by manipulating ill-conditioned matrices. Explosion may result from (nearly) linearly dependent covariates or complete `separation <https://en.wikipedia.org/wiki/Separation_(statistics)>`__. A more common situation in genetics is quasi-complete seperation, e.g. variants that are observed only in cases (or controls). Such variants inevitably arise when testing millions of variants with very low minor allele count. The maximum likelihood estimate of :math:`\\beta` under logistic regression is then undefined but convergence may still occur after a large number of iterations due to a very flat likelihood surface. In testing, we find that such variants produce a secondary bump from 10 to 15 iterations in the histogram of number of iterations per variant. We also find that this faux convergence produces large standard errors and large (insignificant) p-values. To not miss such variants, consider using Firth logistic regression, linear regression, or group-based tests. Here's a concrete illustration of quasi-complete seperation in R. Suppose we have 2010 samples distributed as follows for a particular variant:. ======= ====== === ======; Status HomRef Het HomVar; ======= ====== === ======; Case 1000 10 0; Control 1000 0 0; ======= ====== === ======. The following R code fits the (standard) logistic, Firth logistic, and linear regression models to this ",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:144098,Testability,log,logistic,144098,"Wald, LRT, Firth ``va.logreg.fit.exploded`` Boolean true if iteration exploded; ================ =========================== ======= =====. We consider iteration to have converged when every coordinate of :math:`\\beta` changes by less than :math:`10^{-6}`. For Wald and LRT, up to 25 iterations are attempted; in testing we find 4 or 5 iterations nearly always suffice. Convergence may also fail due to explosion, which refers to low-level numerical linear algebra exceptions caused by manipulating ill-conditioned matrices. Explosion may result from (nearly) linearly dependent covariates or complete `separation <https://en.wikipedia.org/wiki/Separation_(statistics)>`__. A more common situation in genetics is quasi-complete seperation, e.g. variants that are observed only in cases (or controls). Such variants inevitably arise when testing millions of variants with very low minor allele count. The maximum likelihood estimate of :math:`\\beta` under logistic regression is then undefined but convergence may still occur after a large number of iterations due to a very flat likelihood surface. In testing, we find that such variants produce a secondary bump from 10 to 15 iterations in the histogram of number of iterations per variant. We also find that this faux convergence produces large standard errors and large (insignificant) p-values. To not miss such variants, consider using Firth logistic regression, linear regression, or group-based tests. Here's a concrete illustration of quasi-complete seperation in R. Suppose we have 2010 samples distributed as follows for a particular variant:. ======= ====== === ======; Status HomRef Het HomVar; ======= ====== === ======; Case 1000 10 0; Control 1000 0 0; ======= ====== === ======. The following R code fits the (standard) logistic, Firth logistic, and linear regression models to this data, where ``x`` is genotype, ``y`` is phenotype, and ``logistf`` is from the logistf package:. .. code-block:: R. x <- c(rep(0,1000), rep(1,1000), ",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:144245,Testability,test,testing,144245,"onverged when every coordinate of :math:`\\beta` changes by less than :math:`10^{-6}`. For Wald and LRT, up to 25 iterations are attempted; in testing we find 4 or 5 iterations nearly always suffice. Convergence may also fail due to explosion, which refers to low-level numerical linear algebra exceptions caused by manipulating ill-conditioned matrices. Explosion may result from (nearly) linearly dependent covariates or complete `separation <https://en.wikipedia.org/wiki/Separation_(statistics)>`__. A more common situation in genetics is quasi-complete seperation, e.g. variants that are observed only in cases (or controls). Such variants inevitably arise when testing millions of variants with very low minor allele count. The maximum likelihood estimate of :math:`\\beta` under logistic regression is then undefined but convergence may still occur after a large number of iterations due to a very flat likelihood surface. In testing, we find that such variants produce a secondary bump from 10 to 15 iterations in the histogram of number of iterations per variant. We also find that this faux convergence produces large standard errors and large (insignificant) p-values. To not miss such variants, consider using Firth logistic regression, linear regression, or group-based tests. Here's a concrete illustration of quasi-complete seperation in R. Suppose we have 2010 samples distributed as follows for a particular variant:. ======= ====== === ======; Status HomRef Het HomVar; ======= ====== === ======; Case 1000 10 0; Control 1000 0 0; ======= ====== === ======. The following R code fits the (standard) logistic, Firth logistic, and linear regression models to this data, where ``x`` is genotype, ``y`` is phenotype, and ``logistf`` is from the logistf package:. .. code-block:: R. x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:144540,Testability,log,logistic,144540,"xplosion, which refers to low-level numerical linear algebra exceptions caused by manipulating ill-conditioned matrices. Explosion may result from (nearly) linearly dependent covariates or complete `separation <https://en.wikipedia.org/wiki/Separation_(statistics)>`__. A more common situation in genetics is quasi-complete seperation, e.g. variants that are observed only in cases (or controls). Such variants inevitably arise when testing millions of variants with very low minor allele count. The maximum likelihood estimate of :math:`\\beta` under logistic regression is then undefined but convergence may still occur after a large number of iterations due to a very flat likelihood surface. In testing, we find that such variants produce a secondary bump from 10 to 15 iterations in the histogram of number of iterations per variant. We also find that this faux convergence produces large standard errors and large (insignificant) p-values. To not miss such variants, consider using Firth logistic regression, linear regression, or group-based tests. Here's a concrete illustration of quasi-complete seperation in R. Suppose we have 2010 samples distributed as follows for a particular variant:. ======= ====== === ======; Status HomRef Het HomVar; ======= ====== === ======; Case 1000 10 0; Control 1000 0 0; ======= ====== === ======. The following R code fits the (standard) logistic, Firth logistic, and linear regression models to this data, where ``x`` is genotype, ``y`` is phenotype, and ``logistf`` is from the logistf package:. .. code-block:: R. x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085, and 0.0016, respectively. The erroneous value 0.991 is due to quasi-complete separation. Moving one of the 10 hets from case to control eliminates this quasi-complete separation; the p-va",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:144595,Testability,test,tests,144595,"xplosion, which refers to low-level numerical linear algebra exceptions caused by manipulating ill-conditioned matrices. Explosion may result from (nearly) linearly dependent covariates or complete `separation <https://en.wikipedia.org/wiki/Separation_(statistics)>`__. A more common situation in genetics is quasi-complete seperation, e.g. variants that are observed only in cases (or controls). Such variants inevitably arise when testing millions of variants with very low minor allele count. The maximum likelihood estimate of :math:`\\beta` under logistic regression is then undefined but convergence may still occur after a large number of iterations due to a very flat likelihood surface. In testing, we find that such variants produce a secondary bump from 10 to 15 iterations in the histogram of number of iterations per variant. We also find that this faux convergence produces large standard errors and large (insignificant) p-values. To not miss such variants, consider using Firth logistic regression, linear regression, or group-based tests. Here's a concrete illustration of quasi-complete seperation in R. Suppose we have 2010 samples distributed as follows for a particular variant:. ======= ====== === ======; Status HomRef Het HomVar; ======= ====== === ======; Case 1000 10 0; Control 1000 0 0; ======= ====== === ======. The following R code fits the (standard) logistic, Firth logistic, and linear regression models to this data, where ``x`` is genotype, ``y`` is phenotype, and ``logistf`` is from the logistf package:. .. code-block:: R. x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085, and 0.0016, respectively. The erroneous value 0.991 is due to quasi-complete separation. Moving one of the 10 hets from case to control eliminates this quasi-complete separation; the p-va",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:144929,Testability,log,logistic,144929,"millions of variants with very low minor allele count. The maximum likelihood estimate of :math:`\\beta` under logistic regression is then undefined but convergence may still occur after a large number of iterations due to a very flat likelihood surface. In testing, we find that such variants produce a secondary bump from 10 to 15 iterations in the histogram of number of iterations per variant. We also find that this faux convergence produces large standard errors and large (insignificant) p-values. To not miss such variants, consider using Firth logistic regression, linear regression, or group-based tests. Here's a concrete illustration of quasi-complete seperation in R. Suppose we have 2010 samples distributed as follows for a particular variant:. ======= ====== === ======; Status HomRef Het HomVar; ======= ====== === ======; Case 1000 10 0; Control 1000 0 0; ======= ====== === ======. The following R code fits the (standard) logistic, Firth logistic, and linear regression models to this data, where ``x`` is genotype, ``y`` is phenotype, and ``logistf`` is from the logistf package:. .. code-block:: R. x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085, and 0.0016, respectively. The erroneous value 0.991 is due to quasi-complete separation. Moving one of the 10 hets from case to control eliminates this quasi-complete separation; the p-values from R are then 0.0373, 0.0111, and 0.0116, respectively, as expected for a less significant association. The Firth test reduces bias from small counts and resolves the issue of separation by penalizing maximum likelihood estimation by the `Jeffrey's invariant prior <https://en.wikipedia.org/wiki/Jeffreys_prior>`__. This test is slower, as both the null and full model must be fit per variant, and convergence of the modified Newton me",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:144945,Testability,log,logistic,144945,"millions of variants with very low minor allele count. The maximum likelihood estimate of :math:`\\beta` under logistic regression is then undefined but convergence may still occur after a large number of iterations due to a very flat likelihood surface. In testing, we find that such variants produce a secondary bump from 10 to 15 iterations in the histogram of number of iterations per variant. We also find that this faux convergence produces large standard errors and large (insignificant) p-values. To not miss such variants, consider using Firth logistic regression, linear regression, or group-based tests. Here's a concrete illustration of quasi-complete seperation in R. Suppose we have 2010 samples distributed as follows for a particular variant:. ======= ====== === ======; Status HomRef Het HomVar; ======= ====== === ======; Case 1000 10 0; Control 1000 0 0; ======= ====== === ======. The following R code fits the (standard) logistic, Firth logistic, and linear regression models to this data, where ``x`` is genotype, ``y`` is phenotype, and ``logistf`` is from the logistf package:. .. code-block:: R. x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085, and 0.0016, respectively. The erroneous value 0.991 is due to quasi-complete separation. Moving one of the 10 hets from case to control eliminates this quasi-complete separation; the p-values from R are then 0.0373, 0.0111, and 0.0116, respectively, as expected for a less significant association. The Firth test reduces bias from small counts and resolves the issue of separation by penalizing maximum likelihood estimation by the `Jeffrey's invariant prior <https://en.wikipedia.org/wiki/Jeffreys_prior>`__. This test is slower, as both the null and full model must be fit per variant, and convergence of the modified Newton me",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:145049,Testability,log,logistf,145049,"millions of variants with very low minor allele count. The maximum likelihood estimate of :math:`\\beta` under logistic regression is then undefined but convergence may still occur after a large number of iterations due to a very flat likelihood surface. In testing, we find that such variants produce a secondary bump from 10 to 15 iterations in the histogram of number of iterations per variant. We also find that this faux convergence produces large standard errors and large (insignificant) p-values. To not miss such variants, consider using Firth logistic regression, linear regression, or group-based tests. Here's a concrete illustration of quasi-complete seperation in R. Suppose we have 2010 samples distributed as follows for a particular variant:. ======= ====== === ======; Status HomRef Het HomVar; ======= ====== === ======; Case 1000 10 0; Control 1000 0 0; ======= ====== === ======. The following R code fits the (standard) logistic, Firth logistic, and linear regression models to this data, where ``x`` is genotype, ``y`` is phenotype, and ``logistf`` is from the logistf package:. .. code-block:: R. x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085, and 0.0016, respectively. The erroneous value 0.991 is due to quasi-complete separation. Moving one of the 10 hets from case to control eliminates this quasi-complete separation; the p-values from R are then 0.0373, 0.0111, and 0.0116, respectively, as expected for a less significant association. The Firth test reduces bias from small counts and resolves the issue of separation by penalizing maximum likelihood estimation by the `Jeffrey's invariant prior <https://en.wikipedia.org/wiki/Jeffreys_prior>`__. This test is slower, as both the null and full model must be fit per variant, and convergence of the modified Newton me",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:145071,Testability,log,logistf,145071,"millions of variants with very low minor allele count. The maximum likelihood estimate of :math:`\\beta` under logistic regression is then undefined but convergence may still occur after a large number of iterations due to a very flat likelihood surface. In testing, we find that such variants produce a secondary bump from 10 to 15 iterations in the histogram of number of iterations per variant. We also find that this faux convergence produces large standard errors and large (insignificant) p-values. To not miss such variants, consider using Firth logistic regression, linear regression, or group-based tests. Here's a concrete illustration of quasi-complete seperation in R. Suppose we have 2010 samples distributed as follows for a particular variant:. ======= ====== === ======; Status HomRef Het HomVar; ======= ====== === ======; Case 1000 10 0; Control 1000 0 0; ======= ====== === ======. The following R code fits the (standard) logistic, Firth logistic, and linear regression models to this data, where ``x`` is genotype, ``y`` is phenotype, and ``logistf`` is from the logistf package:. .. code-block:: R. x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085, and 0.0016, respectively. The erroneous value 0.991 is due to quasi-complete separation. Moving one of the 10 hets from case to control eliminates this quasi-complete separation; the p-values from R are then 0.0373, 0.0111, and 0.0116, respectively, as expected for a less significant association. The Firth test reduces bias from small counts and resolves the issue of separation by penalizing maximum likelihood estimation by the `Jeffrey's invariant prior <https://en.wikipedia.org/wiki/Jeffreys_prior>`__. This test is slower, as both the null and full model must be fit per variant, and convergence of the modified Newton me",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:145197,Testability,log,logfit,145197,"ations due to a very flat likelihood surface. In testing, we find that such variants produce a secondary bump from 10 to 15 iterations in the histogram of number of iterations per variant. We also find that this faux convergence produces large standard errors and large (insignificant) p-values. To not miss such variants, consider using Firth logistic regression, linear regression, or group-based tests. Here's a concrete illustration of quasi-complete seperation in R. Suppose we have 2010 samples distributed as follows for a particular variant:. ======= ====== === ======; Status HomRef Het HomVar; ======= ====== === ======; Case 1000 10 0; Control 1000 0 0; ======= ====== === ======. The following R code fits the (standard) logistic, Firth logistic, and linear regression models to this data, where ``x`` is genotype, ``y`` is phenotype, and ``logistf`` is from the logistf package:. .. code-block:: R. x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085, and 0.0016, respectively. The erroneous value 0.991 is due to quasi-complete separation. Moving one of the 10 hets from case to control eliminates this quasi-complete separation; the p-values from R are then 0.0373, 0.0111, and 0.0116, respectively, as expected for a less significant association. The Firth test reduces bias from small counts and resolves the issue of separation by penalizing maximum likelihood estimation by the `Jeffrey's invariant prior <https://en.wikipedia.org/wiki/Jeffreys_prior>`__. This test is slower, as both the null and full model must be fit per variant, and convergence of the modified Newton method is linear rather than quadratic. For Firth, 100 iterations are attempted for the null model and, if that is successful, for the full model as well. In testing we find 20 iterations nearly always suffices.",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:145250,Testability,log,logistf,145250,"ations due to a very flat likelihood surface. In testing, we find that such variants produce a secondary bump from 10 to 15 iterations in the histogram of number of iterations per variant. We also find that this faux convergence produces large standard errors and large (insignificant) p-values. To not miss such variants, consider using Firth logistic regression, linear regression, or group-based tests. Here's a concrete illustration of quasi-complete seperation in R. Suppose we have 2010 samples distributed as follows for a particular variant:. ======= ====== === ======; Status HomRef Het HomVar; ======= ====== === ======; Case 1000 10 0; Control 1000 0 0; ======= ====== === ======. The following R code fits the (standard) logistic, Firth logistic, and linear regression models to this data, where ``x`` is genotype, ``y`` is phenotype, and ``logistf`` is from the logistf package:. .. code-block:: R. x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085, and 0.0016, respectively. The erroneous value 0.991 is due to quasi-complete separation. Moving one of the 10 hets from case to control eliminates this quasi-complete separation; the p-values from R are then 0.0373, 0.0111, and 0.0116, respectively, as expected for a less significant association. The Firth test reduces bias from small counts and resolves the issue of separation by penalizing maximum likelihood estimation by the `Jeffrey's invariant prior <https://en.wikipedia.org/wiki/Jeffreys_prior>`__. This test is slower, as both the null and full model must be fit per variant, and convergence of the modified Newton method is linear rather than quadratic. For Firth, 100 iterations are attempted for the null model and, if that is successful, for the full model as well. In testing we find 20 iterations nearly always suffices.",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:145667,Testability,test,test,145667,"t:. ======= ====== === ======; Status HomRef Het HomVar; ======= ====== === ======; Case 1000 10 0; Control 1000 0 0; ======= ====== === ======. The following R code fits the (standard) logistic, Firth logistic, and linear regression models to this data, where ``x`` is genotype, ``y`` is phenotype, and ``logistf`` is from the logistf package:. .. code-block:: R. x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085, and 0.0016, respectively. The erroneous value 0.991 is due to quasi-complete separation. Moving one of the 10 hets from case to control eliminates this quasi-complete separation; the p-values from R are then 0.0373, 0.0111, and 0.0116, respectively, as expected for a less significant association. The Firth test reduces bias from small counts and resolves the issue of separation by penalizing maximum likelihood estimation by the `Jeffrey's invariant prior <https://en.wikipedia.org/wiki/Jeffreys_prior>`__. This test is slower, as both the null and full model must be fit per variant, and convergence of the modified Newton method is linear rather than quadratic. For Firth, 100 iterations are attempted for the null model and, if that is successful, for the full model as well. In testing we find 20 iterations nearly always suffices. If the null model fails to converge, then the ``sa.lmmreg.fit`` annotations reflect the null model; otherwise, they reflect the full model. See `Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants <http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4049324/>`__ for an empirical comparison of the logistic Wald, LRT, score, and Firth tests. The theoretical foundations of the Wald, likelihood ratio, and score tests may be found in Chapter 3 of Gesine Reinert's notes `Statistical Theory <ht",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:145874,Testability,test,test,145874,"ogistic, and linear regression models to this data, where ``x`` is genotype, ``y`` is phenotype, and ``logistf`` is from the logistf package:. .. code-block:: R. x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085, and 0.0016, respectively. The erroneous value 0.991 is due to quasi-complete separation. Moving one of the 10 hets from case to control eliminates this quasi-complete separation; the p-values from R are then 0.0373, 0.0111, and 0.0116, respectively, as expected for a less significant association. The Firth test reduces bias from small counts and resolves the issue of separation by penalizing maximum likelihood estimation by the `Jeffrey's invariant prior <https://en.wikipedia.org/wiki/Jeffreys_prior>`__. This test is slower, as both the null and full model must be fit per variant, and convergence of the modified Newton method is linear rather than quadratic. For Firth, 100 iterations are attempted for the null model and, if that is successful, for the full model as well. In testing we find 20 iterations nearly always suffices. If the null model fails to converge, then the ``sa.lmmreg.fit`` annotations reflect the null model; otherwise, they reflect the full model. See `Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants <http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4049324/>`__ for an empirical comparison of the logistic Wald, LRT, score, and Firth tests. The theoretical foundations of the Wald, likelihood ratio, and score tests may be found in Chapter 3 of Gesine Reinert's notes `Statistical Theory <http://www.stats.ox.ac.uk/~reinert/stattheory/theoryshort09.pdf>`__. Firth introduced his approach in `Bias reduction of maximum likelihood estimates, 1993 <http://www2.stat.duke.edu/~scs/Courses/Stat376/Pa",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:146144,Testability,test,testing,146144,"0), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085, and 0.0016, respectively. The erroneous value 0.991 is due to quasi-complete separation. Moving one of the 10 hets from case to control eliminates this quasi-complete separation; the p-values from R are then 0.0373, 0.0111, and 0.0116, respectively, as expected for a less significant association. The Firth test reduces bias from small counts and resolves the issue of separation by penalizing maximum likelihood estimation by the `Jeffrey's invariant prior <https://en.wikipedia.org/wiki/Jeffreys_prior>`__. This test is slower, as both the null and full model must be fit per variant, and convergence of the modified Newton method is linear rather than quadratic. For Firth, 100 iterations are attempted for the null model and, if that is successful, for the full model as well. In testing we find 20 iterations nearly always suffices. If the null model fails to converge, then the ``sa.lmmreg.fit`` annotations reflect the null model; otherwise, they reflect the full model. See `Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants <http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4049324/>`__ for an empirical comparison of the logistic Wald, LRT, score, and Firth tests. The theoretical foundations of the Wald, likelihood ratio, and score tests may be found in Chapter 3 of Gesine Reinert's notes `Statistical Theory <http://www.stats.ox.ac.uk/~reinert/stattheory/theoryshort09.pdf>`__. Firth introduced his approach in `Bias reduction of maximum likelihood estimates, 1993 <http://www2.stat.duke.edu/~scs/Courses/Stat376/Papers/GibbsFieldEst/BiasReductionMLE.pdf>`__. Heinze and Schemper further analyze Firth's approach in `A solution to the problem of separation in logistic regression, 2002 <https://cemsiis.meduniwien.ac.at/fileadmin/msi_aki",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:146419,Testability,test,testing,146419,"ue 0.991 is due to quasi-complete separation. Moving one of the 10 hets from case to control eliminates this quasi-complete separation; the p-values from R are then 0.0373, 0.0111, and 0.0116, respectively, as expected for a less significant association. The Firth test reduces bias from small counts and resolves the issue of separation by penalizing maximum likelihood estimation by the `Jeffrey's invariant prior <https://en.wikipedia.org/wiki/Jeffreys_prior>`__. This test is slower, as both the null and full model must be fit per variant, and convergence of the modified Newton method is linear rather than quadratic. For Firth, 100 iterations are attempted for the null model and, if that is successful, for the full model as well. In testing we find 20 iterations nearly always suffices. If the null model fails to converge, then the ``sa.lmmreg.fit`` annotations reflect the null model; otherwise, they reflect the full model. See `Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants <http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4049324/>`__ for an empirical comparison of the logistic Wald, LRT, score, and Firth tests. The theoretical foundations of the Wald, likelihood ratio, and score tests may be found in Chapter 3 of Gesine Reinert's notes `Statistical Theory <http://www.stats.ox.ac.uk/~reinert/stattheory/theoryshort09.pdf>`__. Firth introduced his approach in `Bias reduction of maximum likelihood estimates, 1993 <http://www2.stat.duke.edu/~scs/Courses/Stat376/Papers/GibbsFieldEst/BiasReductionMLE.pdf>`__. Heinze and Schemper further analyze Firth's approach in `A solution to the problem of separation in logistic regression, 2002 <https://cemsiis.meduniwien.ac.at/fileadmin/msi_akim/CeMSIIS/KB/volltexte/Heinze_Schemper_2002_Statistics_in_Medicine.pdf>`__. Those variants that don't vary across the included samples (e.g., all genotypes; are HomRef) will have missing annotations. Phenotype and covariate sample ann",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:146549,Testability,log,logistic,146549,"; the p-values from R are then 0.0373, 0.0111, and 0.0116, respectively, as expected for a less significant association. The Firth test reduces bias from small counts and resolves the issue of separation by penalizing maximum likelihood estimation by the `Jeffrey's invariant prior <https://en.wikipedia.org/wiki/Jeffreys_prior>`__. This test is slower, as both the null and full model must be fit per variant, and convergence of the modified Newton method is linear rather than quadratic. For Firth, 100 iterations are attempted for the null model and, if that is successful, for the full model as well. In testing we find 20 iterations nearly always suffices. If the null model fails to converge, then the ``sa.lmmreg.fit`` annotations reflect the null model; otherwise, they reflect the full model. See `Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants <http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4049324/>`__ for an empirical comparison of the logistic Wald, LRT, score, and Firth tests. The theoretical foundations of the Wald, likelihood ratio, and score tests may be found in Chapter 3 of Gesine Reinert's notes `Statistical Theory <http://www.stats.ox.ac.uk/~reinert/stattheory/theoryshort09.pdf>`__. Firth introduced his approach in `Bias reduction of maximum likelihood estimates, 1993 <http://www2.stat.duke.edu/~scs/Courses/Stat376/Papers/GibbsFieldEst/BiasReductionMLE.pdf>`__. Heinze and Schemper further analyze Firth's approach in `A solution to the problem of separation in logistic regression, 2002 <https://cemsiis.meduniwien.ac.at/fileadmin/msi_akim/CeMSIIS/KB/volltexte/Heinze_Schemper_2002_Statistics_in_Medicine.pdf>`__. Those variants that don't vary across the included samples (e.g., all genotypes; are HomRef) will have missing annotations. Phenotype and covariate sample annotations may also be specified using `programmatic expressions <exprlang.html>`__ without identifiers, such as:. .. code-block:: text.",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:146586,Testability,test,tests,146586,"; the p-values from R are then 0.0373, 0.0111, and 0.0116, respectively, as expected for a less significant association. The Firth test reduces bias from small counts and resolves the issue of separation by penalizing maximum likelihood estimation by the `Jeffrey's invariant prior <https://en.wikipedia.org/wiki/Jeffreys_prior>`__. This test is slower, as both the null and full model must be fit per variant, and convergence of the modified Newton method is linear rather than quadratic. For Firth, 100 iterations are attempted for the null model and, if that is successful, for the full model as well. In testing we find 20 iterations nearly always suffices. If the null model fails to converge, then the ``sa.lmmreg.fit`` annotations reflect the null model; otherwise, they reflect the full model. See `Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants <http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4049324/>`__ for an empirical comparison of the logistic Wald, LRT, score, and Firth tests. The theoretical foundations of the Wald, likelihood ratio, and score tests may be found in Chapter 3 of Gesine Reinert's notes `Statistical Theory <http://www.stats.ox.ac.uk/~reinert/stattheory/theoryshort09.pdf>`__. Firth introduced his approach in `Bias reduction of maximum likelihood estimates, 1993 <http://www2.stat.duke.edu/~scs/Courses/Stat376/Papers/GibbsFieldEst/BiasReductionMLE.pdf>`__. Heinze and Schemper further analyze Firth's approach in `A solution to the problem of separation in logistic regression, 2002 <https://cemsiis.meduniwien.ac.at/fileadmin/msi_akim/CeMSIIS/KB/volltexte/Heinze_Schemper_2002_Statistics_in_Medicine.pdf>`__. Those variants that don't vary across the included samples (e.g., all genotypes; are HomRef) will have missing annotations. Phenotype and covariate sample annotations may also be specified using `programmatic expressions <exprlang.html>`__ without identifiers, such as:. .. code-block:: text.",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:146662,Testability,test,tests,146662,"reduces bias from small counts and resolves the issue of separation by penalizing maximum likelihood estimation by the `Jeffrey's invariant prior <https://en.wikipedia.org/wiki/Jeffreys_prior>`__. This test is slower, as both the null and full model must be fit per variant, and convergence of the modified Newton method is linear rather than quadratic. For Firth, 100 iterations are attempted for the null model and, if that is successful, for the full model as well. In testing we find 20 iterations nearly always suffices. If the null model fails to converge, then the ``sa.lmmreg.fit`` annotations reflect the null model; otherwise, they reflect the full model. See `Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants <http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4049324/>`__ for an empirical comparison of the logistic Wald, LRT, score, and Firth tests. The theoretical foundations of the Wald, likelihood ratio, and score tests may be found in Chapter 3 of Gesine Reinert's notes `Statistical Theory <http://www.stats.ox.ac.uk/~reinert/stattheory/theoryshort09.pdf>`__. Firth introduced his approach in `Bias reduction of maximum likelihood estimates, 1993 <http://www2.stat.duke.edu/~scs/Courses/Stat376/Papers/GibbsFieldEst/BiasReductionMLE.pdf>`__. Heinze and Schemper further analyze Firth's approach in `A solution to the problem of separation in logistic regression, 2002 <https://cemsiis.meduniwien.ac.at/fileadmin/msi_akim/CeMSIIS/KB/volltexte/Heinze_Schemper_2002_Statistics_in_Medicine.pdf>`__. Those variants that don't vary across the included samples (e.g., all genotypes; are HomRef) will have missing annotations. Phenotype and covariate sample annotations may also be specified using `programmatic expressions <exprlang.html>`__ without identifiers, such as:. .. code-block:: text. if (sa.isFemale) sa.cov.age else (2 * sa.cov.age + 10). For Boolean covariate types, true is coded as 1 and false as 0. In particular,",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:147092,Testability,log,logistic,147092,"ed for the null model and, if that is successful, for the full model as well. In testing we find 20 iterations nearly always suffices. If the null model fails to converge, then the ``sa.lmmreg.fit`` annotations reflect the null model; otherwise, they reflect the full model. See `Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants <http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4049324/>`__ for an empirical comparison of the logistic Wald, LRT, score, and Firth tests. The theoretical foundations of the Wald, likelihood ratio, and score tests may be found in Chapter 3 of Gesine Reinert's notes `Statistical Theory <http://www.stats.ox.ac.uk/~reinert/stattheory/theoryshort09.pdf>`__. Firth introduced his approach in `Bias reduction of maximum likelihood estimates, 1993 <http://www2.stat.duke.edu/~scs/Courses/Stat376/Papers/GibbsFieldEst/BiasReductionMLE.pdf>`__. Heinze and Schemper further analyze Firth's approach in `A solution to the problem of separation in logistic regression, 2002 <https://cemsiis.meduniwien.ac.at/fileadmin/msi_akim/CeMSIIS/KB/volltexte/Heinze_Schemper_2002_Statistics_in_Medicine.pdf>`__. Those variants that don't vary across the included samples (e.g., all genotypes; are HomRef) will have missing annotations. Phenotype and covariate sample annotations may also be specified using `programmatic expressions <exprlang.html>`__ without identifiers, such as:. .. code-block:: text. if (sa.isFemale) sa.cov.age else (2 * sa.cov.age + 10). For Boolean covariate types, true is coded as 1 and false as 0. In particular, for the sample annotation ``sa.fam.isCase`` added by importing a FAM file with case-control phenotype, case is 1 and control is 0. Hail's logistic regression tests correspond to the ``b.wald``, ``b.lrt``, and ``b.score`` tests in `EPACTS <http://genome.sph.umich.edu/wiki/EPACTS#Single_Variant_Tests>`__. For each variant, Hail imputes missing genotypes as the mean of called genotypes, whe",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:147811,Testability,log,logistic,147811,"approach in `Bias reduction of maximum likelihood estimates, 1993 <http://www2.stat.duke.edu/~scs/Courses/Stat376/Papers/GibbsFieldEst/BiasReductionMLE.pdf>`__. Heinze and Schemper further analyze Firth's approach in `A solution to the problem of separation in logistic regression, 2002 <https://cemsiis.meduniwien.ac.at/fileadmin/msi_akim/CeMSIIS/KB/volltexte/Heinze_Schemper_2002_Statistics_in_Medicine.pdf>`__. Those variants that don't vary across the included samples (e.g., all genotypes; are HomRef) will have missing annotations. Phenotype and covariate sample annotations may also be specified using `programmatic expressions <exprlang.html>`__ without identifiers, such as:. .. code-block:: text. if (sa.isFemale) sa.cov.age else (2 * sa.cov.age + 10). For Boolean covariate types, true is coded as 1 and false as 0. In particular, for the sample annotation ``sa.fam.isCase`` added by importing a FAM file with case-control phenotype, case is 1 and control is 0. Hail's logistic regression tests correspond to the ``b.wald``, ``b.lrt``, and ``b.score`` tests in `EPACTS <http://genome.sph.umich.edu/wiki/EPACTS#Single_Variant_Tests>`__. For each variant, Hail imputes missing genotypes as the mean of called genotypes, whereas EPACTS subsets to those samples with called genotypes. Hence, Hail and EPACTS results will currently only agree for variants with no missing genotypes. :param str test: Statistical test, one of: 'wald', 'lrt', 'score', or 'firth'. :param str y: Response expression. Must evaluate to Boolean or; numeric with all values 0 or 1. :param covariates: list of covariate expressions; :type covariates: list of str. :param str root: Variant annotation path to store result of logistic regression. :param bool use_dosages: If true, use genotype dosage rather than hard call. :return: Variant dataset with logistic regression variant annotations.; :rtype: :py:class:`.VariantDataset`; """""". jvds = self._jvdf.logreg(test, y, jarray(Env.jvm().java.lang.String, covariates), r",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:147831,Testability,test,tests,147831,"approach in `Bias reduction of maximum likelihood estimates, 1993 <http://www2.stat.duke.edu/~scs/Courses/Stat376/Papers/GibbsFieldEst/BiasReductionMLE.pdf>`__. Heinze and Schemper further analyze Firth's approach in `A solution to the problem of separation in logistic regression, 2002 <https://cemsiis.meduniwien.ac.at/fileadmin/msi_akim/CeMSIIS/KB/volltexte/Heinze_Schemper_2002_Statistics_in_Medicine.pdf>`__. Those variants that don't vary across the included samples (e.g., all genotypes; are HomRef) will have missing annotations. Phenotype and covariate sample annotations may also be specified using `programmatic expressions <exprlang.html>`__ without identifiers, such as:. .. code-block:: text. if (sa.isFemale) sa.cov.age else (2 * sa.cov.age + 10). For Boolean covariate types, true is coded as 1 and false as 0. In particular, for the sample annotation ``sa.fam.isCase`` added by importing a FAM file with case-control phenotype, case is 1 and control is 0. Hail's logistic regression tests correspond to the ``b.wald``, ``b.lrt``, and ``b.score`` tests in `EPACTS <http://genome.sph.umich.edu/wiki/EPACTS#Single_Variant_Tests>`__. For each variant, Hail imputes missing genotypes as the mean of called genotypes, whereas EPACTS subsets to those samples with called genotypes. Hence, Hail and EPACTS results will currently only agree for variants with no missing genotypes. :param str test: Statistical test, one of: 'wald', 'lrt', 'score', or 'firth'. :param str y: Response expression. Must evaluate to Boolean or; numeric with all values 0 or 1. :param covariates: list of covariate expressions; :type covariates: list of str. :param str root: Variant annotation path to store result of logistic regression. :param bool use_dosages: If true, use genotype dosage rather than hard call. :return: Variant dataset with logistic regression variant annotations.; :rtype: :py:class:`.VariantDataset`; """""". jvds = self._jvdf.logreg(test, y, jarray(Env.jvm().java.lang.String, covariates), r",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:147894,Testability,test,tests,147894,"ww2.stat.duke.edu/~scs/Courses/Stat376/Papers/GibbsFieldEst/BiasReductionMLE.pdf>`__. Heinze and Schemper further analyze Firth's approach in `A solution to the problem of separation in logistic regression, 2002 <https://cemsiis.meduniwien.ac.at/fileadmin/msi_akim/CeMSIIS/KB/volltexte/Heinze_Schemper_2002_Statistics_in_Medicine.pdf>`__. Those variants that don't vary across the included samples (e.g., all genotypes; are HomRef) will have missing annotations. Phenotype and covariate sample annotations may also be specified using `programmatic expressions <exprlang.html>`__ without identifiers, such as:. .. code-block:: text. if (sa.isFemale) sa.cov.age else (2 * sa.cov.age + 10). For Boolean covariate types, true is coded as 1 and false as 0. In particular, for the sample annotation ``sa.fam.isCase`` added by importing a FAM file with case-control phenotype, case is 1 and control is 0. Hail's logistic regression tests correspond to the ``b.wald``, ``b.lrt``, and ``b.score`` tests in `EPACTS <http://genome.sph.umich.edu/wiki/EPACTS#Single_Variant_Tests>`__. For each variant, Hail imputes missing genotypes as the mean of called genotypes, whereas EPACTS subsets to those samples with called genotypes. Hence, Hail and EPACTS results will currently only agree for variants with no missing genotypes. :param str test: Statistical test, one of: 'wald', 'lrt', 'score', or 'firth'. :param str y: Response expression. Must evaluate to Boolean or; numeric with all values 0 or 1. :param covariates: list of covariate expressions; :type covariates: list of str. :param str root: Variant annotation path to store result of logistic regression. :param bool use_dosages: If true, use genotype dosage rather than hard call. :return: Variant dataset with logistic regression variant annotations.; :rtype: :py:class:`.VariantDataset`; """""". jvds = self._jvdf.logreg(test, y, jarray(Env.jvm().java.lang.String, covariates), root, use_dosages); return VariantDataset(self.hc, jvds). [docs] @handle_py4",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:148231,Testability,test,test,148231,"s that don't vary across the included samples (e.g., all genotypes; are HomRef) will have missing annotations. Phenotype and covariate sample annotations may also be specified using `programmatic expressions <exprlang.html>`__ without identifiers, such as:. .. code-block:: text. if (sa.isFemale) sa.cov.age else (2 * sa.cov.age + 10). For Boolean covariate types, true is coded as 1 and false as 0. In particular, for the sample annotation ``sa.fam.isCase`` added by importing a FAM file with case-control phenotype, case is 1 and control is 0. Hail's logistic regression tests correspond to the ``b.wald``, ``b.lrt``, and ``b.score`` tests in `EPACTS <http://genome.sph.umich.edu/wiki/EPACTS#Single_Variant_Tests>`__. For each variant, Hail imputes missing genotypes as the mean of called genotypes, whereas EPACTS subsets to those samples with called genotypes. Hence, Hail and EPACTS results will currently only agree for variants with no missing genotypes. :param str test: Statistical test, one of: 'wald', 'lrt', 'score', or 'firth'. :param str y: Response expression. Must evaluate to Boolean or; numeric with all values 0 or 1. :param covariates: list of covariate expressions; :type covariates: list of str. :param str root: Variant annotation path to store result of logistic regression. :param bool use_dosages: If true, use genotype dosage rather than hard call. :return: Variant dataset with logistic regression variant annotations.; :rtype: :py:class:`.VariantDataset`; """""". jvds = self._jvdf.logreg(test, y, jarray(Env.jvm().java.lang.String, covariates), root, use_dosages); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(key_name=strlike,; variant_keys=strlike,; single_key=bool,; agg_expr=strlike,; test=strlike,; y=strlike,; covariates=listof(strlike)); def logreg_burden(self, key_name, variant_keys, single_key, agg_expr, test, y, covariates=[]):; r""""""Test each keyed group of variants for association by aggregating (collapsing) genotypes and apply",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:148249,Testability,test,test,148249,"s that don't vary across the included samples (e.g., all genotypes; are HomRef) will have missing annotations. Phenotype and covariate sample annotations may also be specified using `programmatic expressions <exprlang.html>`__ without identifiers, such as:. .. code-block:: text. if (sa.isFemale) sa.cov.age else (2 * sa.cov.age + 10). For Boolean covariate types, true is coded as 1 and false as 0. In particular, for the sample annotation ``sa.fam.isCase`` added by importing a FAM file with case-control phenotype, case is 1 and control is 0. Hail's logistic regression tests correspond to the ``b.wald``, ``b.lrt``, and ``b.score`` tests in `EPACTS <http://genome.sph.umich.edu/wiki/EPACTS#Single_Variant_Tests>`__. For each variant, Hail imputes missing genotypes as the mean of called genotypes, whereas EPACTS subsets to those samples with called genotypes. Hence, Hail and EPACTS results will currently only agree for variants with no missing genotypes. :param str test: Statistical test, one of: 'wald', 'lrt', 'score', or 'firth'. :param str y: Response expression. Must evaluate to Boolean or; numeric with all values 0 or 1. :param covariates: list of covariate expressions; :type covariates: list of str. :param str root: Variant annotation path to store result of logistic regression. :param bool use_dosages: If true, use genotype dosage rather than hard call. :return: Variant dataset with logistic regression variant annotations.; :rtype: :py:class:`.VariantDataset`; """""". jvds = self._jvdf.logreg(test, y, jarray(Env.jvm().java.lang.String, covariates), root, use_dosages); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(key_name=strlike,; variant_keys=strlike,; single_key=bool,; agg_expr=strlike,; test=strlike,; y=strlike,; covariates=listof(strlike)); def logreg_burden(self, key_name, variant_keys, single_key, agg_expr, test, y, covariates=[]):; r""""""Test each keyed group of variants for association by aggregating (collapsing) genotypes and apply",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:148536,Testability,log,logistic,148536," .. code-block:: text. if (sa.isFemale) sa.cov.age else (2 * sa.cov.age + 10). For Boolean covariate types, true is coded as 1 and false as 0. In particular, for the sample annotation ``sa.fam.isCase`` added by importing a FAM file with case-control phenotype, case is 1 and control is 0. Hail's logistic regression tests correspond to the ``b.wald``, ``b.lrt``, and ``b.score`` tests in `EPACTS <http://genome.sph.umich.edu/wiki/EPACTS#Single_Variant_Tests>`__. For each variant, Hail imputes missing genotypes as the mean of called genotypes, whereas EPACTS subsets to those samples with called genotypes. Hence, Hail and EPACTS results will currently only agree for variants with no missing genotypes. :param str test: Statistical test, one of: 'wald', 'lrt', 'score', or 'firth'. :param str y: Response expression. Must evaluate to Boolean or; numeric with all values 0 or 1. :param covariates: list of covariate expressions; :type covariates: list of str. :param str root: Variant annotation path to store result of logistic regression. :param bool use_dosages: If true, use genotype dosage rather than hard call. :return: Variant dataset with logistic regression variant annotations.; :rtype: :py:class:`.VariantDataset`; """""". jvds = self._jvdf.logreg(test, y, jarray(Env.jvm().java.lang.String, covariates), root, use_dosages); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(key_name=strlike,; variant_keys=strlike,; single_key=bool,; agg_expr=strlike,; test=strlike,; y=strlike,; covariates=listof(strlike)); def logreg_burden(self, key_name, variant_keys, single_key, agg_expr, test, y, covariates=[]):; r""""""Test each keyed group of variants for association by aggregating (collapsing) genotypes and applying the; logistic regression model. .. include:: requireTGenotype.rst. **Examples**. Run a gene burden test using the logistic Wald test on the maximum genotype per gene. Here ``va.genes`` is; a variant annotation of type Set[String] giving the set of genes",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:148664,Testability,log,logistic,148664,"lar, for the sample annotation ``sa.fam.isCase`` added by importing a FAM file with case-control phenotype, case is 1 and control is 0. Hail's logistic regression tests correspond to the ``b.wald``, ``b.lrt``, and ``b.score`` tests in `EPACTS <http://genome.sph.umich.edu/wiki/EPACTS#Single_Variant_Tests>`__. For each variant, Hail imputes missing genotypes as the mean of called genotypes, whereas EPACTS subsets to those samples with called genotypes. Hence, Hail and EPACTS results will currently only agree for variants with no missing genotypes. :param str test: Statistical test, one of: 'wald', 'lrt', 'score', or 'firth'. :param str y: Response expression. Must evaluate to Boolean or; numeric with all values 0 or 1. :param covariates: list of covariate expressions; :type covariates: list of str. :param str root: Variant annotation path to store result of logistic regression. :param bool use_dosages: If true, use genotype dosage rather than hard call. :return: Variant dataset with logistic regression variant annotations.; :rtype: :py:class:`.VariantDataset`; """""". jvds = self._jvdf.logreg(test, y, jarray(Env.jvm().java.lang.String, covariates), root, use_dosages); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(key_name=strlike,; variant_keys=strlike,; single_key=bool,; agg_expr=strlike,; test=strlike,; y=strlike,; covariates=listof(strlike)); def logreg_burden(self, key_name, variant_keys, single_key, agg_expr, test, y, covariates=[]):; r""""""Test each keyed group of variants for association by aggregating (collapsing) genotypes and applying the; logistic regression model. .. include:: requireTGenotype.rst. **Examples**. Run a gene burden test using the logistic Wald test on the maximum genotype per gene. Here ``va.genes`` is; a variant annotation of type Set[String] giving the set of genes containing the variant; (see **Extended example** in :py:meth:`.linreg_burden` for a deeper dive in the context of linear regression):. >>> logreg_kt, s",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:148766,Testability,log,logreg,148766,"e is 1 and control is 0. Hail's logistic regression tests correspond to the ``b.wald``, ``b.lrt``, and ``b.score`` tests in `EPACTS <http://genome.sph.umich.edu/wiki/EPACTS#Single_Variant_Tests>`__. For each variant, Hail imputes missing genotypes as the mean of called genotypes, whereas EPACTS subsets to those samples with called genotypes. Hence, Hail and EPACTS results will currently only agree for variants with no missing genotypes. :param str test: Statistical test, one of: 'wald', 'lrt', 'score', or 'firth'. :param str y: Response expression. Must evaluate to Boolean or; numeric with all values 0 or 1. :param covariates: list of covariate expressions; :type covariates: list of str. :param str root: Variant annotation path to store result of logistic regression. :param bool use_dosages: If true, use genotype dosage rather than hard call. :return: Variant dataset with logistic regression variant annotations.; :rtype: :py:class:`.VariantDataset`; """""". jvds = self._jvdf.logreg(test, y, jarray(Env.jvm().java.lang.String, covariates), root, use_dosages); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(key_name=strlike,; variant_keys=strlike,; single_key=bool,; agg_expr=strlike,; test=strlike,; y=strlike,; covariates=listof(strlike)); def logreg_burden(self, key_name, variant_keys, single_key, agg_expr, test, y, covariates=[]):; r""""""Test each keyed group of variants for association by aggregating (collapsing) genotypes and applying the; logistic regression model. .. include:: requireTGenotype.rst. **Examples**. Run a gene burden test using the logistic Wald test on the maximum genotype per gene. Here ``va.genes`` is; a variant annotation of type Set[String] giving the set of genes containing the variant; (see **Extended example** in :py:meth:`.linreg_burden` for a deeper dive in the context of linear regression):. >>> logreg_kt, sample_kt = (hc.read('data/example_burden.vds'); ... .logreg_burden(key_name='gene',; ... variant_keys='va.genes",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:148773,Testability,test,test,148773,"e is 1 and control is 0. Hail's logistic regression tests correspond to the ``b.wald``, ``b.lrt``, and ``b.score`` tests in `EPACTS <http://genome.sph.umich.edu/wiki/EPACTS#Single_Variant_Tests>`__. For each variant, Hail imputes missing genotypes as the mean of called genotypes, whereas EPACTS subsets to those samples with called genotypes. Hence, Hail and EPACTS results will currently only agree for variants with no missing genotypes. :param str test: Statistical test, one of: 'wald', 'lrt', 'score', or 'firth'. :param str y: Response expression. Must evaluate to Boolean or; numeric with all values 0 or 1. :param covariates: list of covariate expressions; :type covariates: list of str. :param str root: Variant annotation path to store result of logistic regression. :param bool use_dosages: If true, use genotype dosage rather than hard call. :return: Variant dataset with logistic regression variant annotations.; :rtype: :py:class:`.VariantDataset`; """""". jvds = self._jvdf.logreg(test, y, jarray(Env.jvm().java.lang.String, covariates), root, use_dosages); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(key_name=strlike,; variant_keys=strlike,; single_key=bool,; agg_expr=strlike,; test=strlike,; y=strlike,; covariates=listof(strlike)); def logreg_burden(self, key_name, variant_keys, single_key, agg_expr, test, y, covariates=[]):; r""""""Test each keyed group of variants for association by aggregating (collapsing) genotypes and applying the; logistic regression model. .. include:: requireTGenotype.rst. **Examples**. Run a gene burden test using the logistic Wald test on the maximum genotype per gene. Here ``va.genes`` is; a variant annotation of type Set[String] giving the set of genes containing the variant; (see **Extended example** in :py:meth:`.linreg_burden` for a deeper dive in the context of linear regression):. >>> logreg_kt, sample_kt = (hc.read('data/example_burden.vds'); ... .logreg_burden(key_name='gene',; ... variant_keys='va.genes",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:149006,Testability,test,test,149006,"e samples with called genotypes. Hence, Hail and EPACTS results will currently only agree for variants with no missing genotypes. :param str test: Statistical test, one of: 'wald', 'lrt', 'score', or 'firth'. :param str y: Response expression. Must evaluate to Boolean or; numeric with all values 0 or 1. :param covariates: list of covariate expressions; :type covariates: list of str. :param str root: Variant annotation path to store result of logistic regression. :param bool use_dosages: If true, use genotype dosage rather than hard call. :return: Variant dataset with logistic regression variant annotations.; :rtype: :py:class:`.VariantDataset`; """""". jvds = self._jvdf.logreg(test, y, jarray(Env.jvm().java.lang.String, covariates), root, use_dosages); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(key_name=strlike,; variant_keys=strlike,; single_key=bool,; agg_expr=strlike,; test=strlike,; y=strlike,; covariates=listof(strlike)); def logreg_burden(self, key_name, variant_keys, single_key, agg_expr, test, y, covariates=[]):; r""""""Test each keyed group of variants for association by aggregating (collapsing) genotypes and applying the; logistic regression model. .. include:: requireTGenotype.rst. **Examples**. Run a gene burden test using the logistic Wald test on the maximum genotype per gene. Here ``va.genes`` is; a variant annotation of type Set[String] giving the set of genes containing the variant; (see **Extended example** in :py:meth:`.linreg_burden` for a deeper dive in the context of linear regression):. >>> logreg_kt, sample_kt = (hc.read('data/example_burden.vds'); ... .logreg_burden(key_name='gene',; ... variant_keys='va.genes',; ... single_key=False,; ... agg_expr='gs.map(g => g.gt).max()',; ... test='wald',; ... y='sa.burden.pheno',; ... covariates=['sa.burden.cov1', 'sa.burden.cov2'])). Run a gene burden test using the logistic score test on the weighted sum of genotypes per gene.; Here ``va.gene`` is a variant annotation of typ",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:149132,Testability,test,test,149132,"e samples with called genotypes. Hence, Hail and EPACTS results will currently only agree for variants with no missing genotypes. :param str test: Statistical test, one of: 'wald', 'lrt', 'score', or 'firth'. :param str y: Response expression. Must evaluate to Boolean or; numeric with all values 0 or 1. :param covariates: list of covariate expressions; :type covariates: list of str. :param str root: Variant annotation path to store result of logistic regression. :param bool use_dosages: If true, use genotype dosage rather than hard call. :return: Variant dataset with logistic regression variant annotations.; :rtype: :py:class:`.VariantDataset`; """""". jvds = self._jvdf.logreg(test, y, jarray(Env.jvm().java.lang.String, covariates), root, use_dosages); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(key_name=strlike,; variant_keys=strlike,; single_key=bool,; agg_expr=strlike,; test=strlike,; y=strlike,; covariates=listof(strlike)); def logreg_burden(self, key_name, variant_keys, single_key, agg_expr, test, y, covariates=[]):; r""""""Test each keyed group of variants for association by aggregating (collapsing) genotypes and applying the; logistic regression model. .. include:: requireTGenotype.rst. **Examples**. Run a gene burden test using the logistic Wald test on the maximum genotype per gene. Here ``va.genes`` is; a variant annotation of type Set[String] giving the set of genes containing the variant; (see **Extended example** in :py:meth:`.linreg_burden` for a deeper dive in the context of linear regression):. >>> logreg_kt, sample_kt = (hc.read('data/example_burden.vds'); ... .logreg_burden(key_name='gene',; ... variant_keys='va.genes',; ... single_key=False,; ... agg_expr='gs.map(g => g.gt).max()',; ... test='wald',; ... y='sa.burden.pheno',; ... covariates=['sa.burden.cov1', 'sa.burden.cov2'])). Run a gene burden test using the logistic score test on the weighted sum of genotypes per gene.; Here ``va.gene`` is a variant annotation of typ",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:149268,Testability,log,logistic,149268,"e samples with called genotypes. Hence, Hail and EPACTS results will currently only agree for variants with no missing genotypes. :param str test: Statistical test, one of: 'wald', 'lrt', 'score', or 'firth'. :param str y: Response expression. Must evaluate to Boolean or; numeric with all values 0 or 1. :param covariates: list of covariate expressions; :type covariates: list of str. :param str root: Variant annotation path to store result of logistic regression. :param bool use_dosages: If true, use genotype dosage rather than hard call. :return: Variant dataset with logistic regression variant annotations.; :rtype: :py:class:`.VariantDataset`; """""". jvds = self._jvdf.logreg(test, y, jarray(Env.jvm().java.lang.String, covariates), root, use_dosages); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(key_name=strlike,; variant_keys=strlike,; single_key=bool,; agg_expr=strlike,; test=strlike,; y=strlike,; covariates=listof(strlike)); def logreg_burden(self, key_name, variant_keys, single_key, agg_expr, test, y, covariates=[]):; r""""""Test each keyed group of variants for association by aggregating (collapsing) genotypes and applying the; logistic regression model. .. include:: requireTGenotype.rst. **Examples**. Run a gene burden test using the logistic Wald test on the maximum genotype per gene. Here ``va.genes`` is; a variant annotation of type Set[String] giving the set of genes containing the variant; (see **Extended example** in :py:meth:`.linreg_burden` for a deeper dive in the context of linear regression):. >>> logreg_kt, sample_kt = (hc.read('data/example_burden.vds'); ... .logreg_burden(key_name='gene',; ... variant_keys='va.genes',; ... single_key=False,; ... agg_expr='gs.map(g => g.gt).max()',; ... test='wald',; ... y='sa.burden.pheno',; ... covariates=['sa.burden.cov1', 'sa.burden.cov2'])). Run a gene burden test using the logistic score test on the weighted sum of genotypes per gene.; Here ``va.gene`` is a variant annotation of typ",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:149362,Testability,test,test,149362," 0 or 1. :param covariates: list of covariate expressions; :type covariates: list of str. :param str root: Variant annotation path to store result of logistic regression. :param bool use_dosages: If true, use genotype dosage rather than hard call. :return: Variant dataset with logistic regression variant annotations.; :rtype: :py:class:`.VariantDataset`; """""". jvds = self._jvdf.logreg(test, y, jarray(Env.jvm().java.lang.String, covariates), root, use_dosages); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(key_name=strlike,; variant_keys=strlike,; single_key=bool,; agg_expr=strlike,; test=strlike,; y=strlike,; covariates=listof(strlike)); def logreg_burden(self, key_name, variant_keys, single_key, agg_expr, test, y, covariates=[]):; r""""""Test each keyed group of variants for association by aggregating (collapsing) genotypes and applying the; logistic regression model. .. include:: requireTGenotype.rst. **Examples**. Run a gene burden test using the logistic Wald test on the maximum genotype per gene. Here ``va.genes`` is; a variant annotation of type Set[String] giving the set of genes containing the variant; (see **Extended example** in :py:meth:`.linreg_burden` for a deeper dive in the context of linear regression):. >>> logreg_kt, sample_kt = (hc.read('data/example_burden.vds'); ... .logreg_burden(key_name='gene',; ... variant_keys='va.genes',; ... single_key=False,; ... agg_expr='gs.map(g => g.gt).max()',; ... test='wald',; ... y='sa.burden.pheno',; ... covariates=['sa.burden.cov1', 'sa.burden.cov2'])). Run a gene burden test using the logistic score test on the weighted sum of genotypes per gene.; Here ``va.gene`` is a variant annotation of type String giving a single gene per variant (or no gene if; missing), and ``va.weight`` is a numeric variant annotation:. >>> logreg_kt, sample_kt = (hc.read('data/example_burden.vds'); ... .logreg_burden(key_name='gene',; ... variant_keys='va.gene',; ... single_key=True,; ... agg_expr='gs.map(g ",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:149377,Testability,log,logistic,149377," 0 or 1. :param covariates: list of covariate expressions; :type covariates: list of str. :param str root: Variant annotation path to store result of logistic regression. :param bool use_dosages: If true, use genotype dosage rather than hard call. :return: Variant dataset with logistic regression variant annotations.; :rtype: :py:class:`.VariantDataset`; """""". jvds = self._jvdf.logreg(test, y, jarray(Env.jvm().java.lang.String, covariates), root, use_dosages); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(key_name=strlike,; variant_keys=strlike,; single_key=bool,; agg_expr=strlike,; test=strlike,; y=strlike,; covariates=listof(strlike)); def logreg_burden(self, key_name, variant_keys, single_key, agg_expr, test, y, covariates=[]):; r""""""Test each keyed group of variants for association by aggregating (collapsing) genotypes and applying the; logistic regression model. .. include:: requireTGenotype.rst. **Examples**. Run a gene burden test using the logistic Wald test on the maximum genotype per gene. Here ``va.genes`` is; a variant annotation of type Set[String] giving the set of genes containing the variant; (see **Extended example** in :py:meth:`.linreg_burden` for a deeper dive in the context of linear regression):. >>> logreg_kt, sample_kt = (hc.read('data/example_burden.vds'); ... .logreg_burden(key_name='gene',; ... variant_keys='va.genes',; ... single_key=False,; ... agg_expr='gs.map(g => g.gt).max()',; ... test='wald',; ... y='sa.burden.pheno',; ... covariates=['sa.burden.cov1', 'sa.burden.cov2'])). Run a gene burden test using the logistic score test on the weighted sum of genotypes per gene.; Here ``va.gene`` is a variant annotation of type String giving a single gene per variant (or no gene if; missing), and ``va.weight`` is a numeric variant annotation:. >>> logreg_kt, sample_kt = (hc.read('data/example_burden.vds'); ... .logreg_burden(key_name='gene',; ... variant_keys='va.gene',; ... single_key=True,; ... agg_expr='gs.map(g ",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:149391,Testability,test,test,149391," 0 or 1. :param covariates: list of covariate expressions; :type covariates: list of str. :param str root: Variant annotation path to store result of logistic regression. :param bool use_dosages: If true, use genotype dosage rather than hard call. :return: Variant dataset with logistic regression variant annotations.; :rtype: :py:class:`.VariantDataset`; """""". jvds = self._jvdf.logreg(test, y, jarray(Env.jvm().java.lang.String, covariates), root, use_dosages); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(key_name=strlike,; variant_keys=strlike,; single_key=bool,; agg_expr=strlike,; test=strlike,; y=strlike,; covariates=listof(strlike)); def logreg_burden(self, key_name, variant_keys, single_key, agg_expr, test, y, covariates=[]):; r""""""Test each keyed group of variants for association by aggregating (collapsing) genotypes and applying the; logistic regression model. .. include:: requireTGenotype.rst. **Examples**. Run a gene burden test using the logistic Wald test on the maximum genotype per gene. Here ``va.genes`` is; a variant annotation of type Set[String] giving the set of genes containing the variant; (see **Extended example** in :py:meth:`.linreg_burden` for a deeper dive in the context of linear regression):. >>> logreg_kt, sample_kt = (hc.read('data/example_burden.vds'); ... .logreg_burden(key_name='gene',; ... variant_keys='va.genes',; ... single_key=False,; ... agg_expr='gs.map(g => g.gt).max()',; ... test='wald',; ... y='sa.burden.pheno',; ... covariates=['sa.burden.cov1', 'sa.burden.cov2'])). Run a gene burden test using the logistic score test on the weighted sum of genotypes per gene.; Here ``va.gene`` is a variant annotation of type String giving a single gene per variant (or no gene if; missing), and ``va.weight`` is a numeric variant annotation:. >>> logreg_kt, sample_kt = (hc.read('data/example_burden.vds'); ... .logreg_burden(key_name='gene',; ... variant_keys='va.gene',; ... single_key=True,; ... agg_expr='gs.map(g ",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:149852,Testability,test,test,149852,"riantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(key_name=strlike,; variant_keys=strlike,; single_key=bool,; agg_expr=strlike,; test=strlike,; y=strlike,; covariates=listof(strlike)); def logreg_burden(self, key_name, variant_keys, single_key, agg_expr, test, y, covariates=[]):; r""""""Test each keyed group of variants for association by aggregating (collapsing) genotypes and applying the; logistic regression model. .. include:: requireTGenotype.rst. **Examples**. Run a gene burden test using the logistic Wald test on the maximum genotype per gene. Here ``va.genes`` is; a variant annotation of type Set[String] giving the set of genes containing the variant; (see **Extended example** in :py:meth:`.linreg_burden` for a deeper dive in the context of linear regression):. >>> logreg_kt, sample_kt = (hc.read('data/example_burden.vds'); ... .logreg_burden(key_name='gene',; ... variant_keys='va.genes',; ... single_key=False,; ... agg_expr='gs.map(g => g.gt).max()',; ... test='wald',; ... y='sa.burden.pheno',; ... covariates=['sa.burden.cov1', 'sa.burden.cov2'])). Run a gene burden test using the logistic score test on the weighted sum of genotypes per gene.; Here ``va.gene`` is a variant annotation of type String giving a single gene per variant (or no gene if; missing), and ``va.weight`` is a numeric variant annotation:. >>> logreg_kt, sample_kt = (hc.read('data/example_burden.vds'); ... .logreg_burden(key_name='gene',; ... variant_keys='va.gene',; ... single_key=True,; ... agg_expr='gs.map(g => va.weight * g.gt).sum()',; ... test='score',; ... y='sa.burden.pheno',; ... covariates=['sa.burden.cov1', 'sa.burden.cov2'])). To use a weighted sum of genotypes with missing genotypes mean-imputed rather than ignored, set; ``agg_expr='gs.map(g => va.weight * orElse(g.gt.toDouble, 2 * va.qc.AF)).sum()'`` where ``va.qc.AF``; is the allele frequency over those samples that have no missing phenotype or covariates. .. caution::. With ``single_key=False``, ``variant_keys",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:149965,Testability,test,test,149965,"r=strlike,; test=strlike,; y=strlike,; covariates=listof(strlike)); def logreg_burden(self, key_name, variant_keys, single_key, agg_expr, test, y, covariates=[]):; r""""""Test each keyed group of variants for association by aggregating (collapsing) genotypes and applying the; logistic regression model. .. include:: requireTGenotype.rst. **Examples**. Run a gene burden test using the logistic Wald test on the maximum genotype per gene. Here ``va.genes`` is; a variant annotation of type Set[String] giving the set of genes containing the variant; (see **Extended example** in :py:meth:`.linreg_burden` for a deeper dive in the context of linear regression):. >>> logreg_kt, sample_kt = (hc.read('data/example_burden.vds'); ... .logreg_burden(key_name='gene',; ... variant_keys='va.genes',; ... single_key=False,; ... agg_expr='gs.map(g => g.gt).max()',; ... test='wald',; ... y='sa.burden.pheno',; ... covariates=['sa.burden.cov1', 'sa.burden.cov2'])). Run a gene burden test using the logistic score test on the weighted sum of genotypes per gene.; Here ``va.gene`` is a variant annotation of type String giving a single gene per variant (or no gene if; missing), and ``va.weight`` is a numeric variant annotation:. >>> logreg_kt, sample_kt = (hc.read('data/example_burden.vds'); ... .logreg_burden(key_name='gene',; ... variant_keys='va.gene',; ... single_key=True,; ... agg_expr='gs.map(g => va.weight * g.gt).sum()',; ... test='score',; ... y='sa.burden.pheno',; ... covariates=['sa.burden.cov1', 'sa.burden.cov2'])). To use a weighted sum of genotypes with missing genotypes mean-imputed rather than ignored, set; ``agg_expr='gs.map(g => va.weight * orElse(g.gt.toDouble, 2 * va.qc.AF)).sum()'`` where ``va.qc.AF``; is the allele frequency over those samples that have no missing phenotype or covariates. .. caution::. With ``single_key=False``, ``variant_keys`` expects a variant annotation of Set or Array type, in order to; allow each variant to have zero, one, or more keys (for example, the",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:149980,Testability,log,logistic,149980,"r=strlike,; test=strlike,; y=strlike,; covariates=listof(strlike)); def logreg_burden(self, key_name, variant_keys, single_key, agg_expr, test, y, covariates=[]):; r""""""Test each keyed group of variants for association by aggregating (collapsing) genotypes and applying the; logistic regression model. .. include:: requireTGenotype.rst. **Examples**. Run a gene burden test using the logistic Wald test on the maximum genotype per gene. Here ``va.genes`` is; a variant annotation of type Set[String] giving the set of genes containing the variant; (see **Extended example** in :py:meth:`.linreg_burden` for a deeper dive in the context of linear regression):. >>> logreg_kt, sample_kt = (hc.read('data/example_burden.vds'); ... .logreg_burden(key_name='gene',; ... variant_keys='va.genes',; ... single_key=False,; ... agg_expr='gs.map(g => g.gt).max()',; ... test='wald',; ... y='sa.burden.pheno',; ... covariates=['sa.burden.cov1', 'sa.burden.cov2'])). Run a gene burden test using the logistic score test on the weighted sum of genotypes per gene.; Here ``va.gene`` is a variant annotation of type String giving a single gene per variant (or no gene if; missing), and ``va.weight`` is a numeric variant annotation:. >>> logreg_kt, sample_kt = (hc.read('data/example_burden.vds'); ... .logreg_burden(key_name='gene',; ... variant_keys='va.gene',; ... single_key=True,; ... agg_expr='gs.map(g => va.weight * g.gt).sum()',; ... test='score',; ... y='sa.burden.pheno',; ... covariates=['sa.burden.cov1', 'sa.burden.cov2'])). To use a weighted sum of genotypes with missing genotypes mean-imputed rather than ignored, set; ``agg_expr='gs.map(g => va.weight * orElse(g.gt.toDouble, 2 * va.qc.AF)).sum()'`` where ``va.qc.AF``; is the allele frequency over those samples that have no missing phenotype or covariates. .. caution::. With ``single_key=False``, ``variant_keys`` expects a variant annotation of Set or Array type, in order to; allow each variant to have zero, one, or more keys (for example, the",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:149995,Testability,test,test,149995,"r=strlike,; test=strlike,; y=strlike,; covariates=listof(strlike)); def logreg_burden(self, key_name, variant_keys, single_key, agg_expr, test, y, covariates=[]):; r""""""Test each keyed group of variants for association by aggregating (collapsing) genotypes and applying the; logistic regression model. .. include:: requireTGenotype.rst. **Examples**. Run a gene burden test using the logistic Wald test on the maximum genotype per gene. Here ``va.genes`` is; a variant annotation of type Set[String] giving the set of genes containing the variant; (see **Extended example** in :py:meth:`.linreg_burden` for a deeper dive in the context of linear regression):. >>> logreg_kt, sample_kt = (hc.read('data/example_burden.vds'); ... .logreg_burden(key_name='gene',; ... variant_keys='va.genes',; ... single_key=False,; ... agg_expr='gs.map(g => g.gt).max()',; ... test='wald',; ... y='sa.burden.pheno',; ... covariates=['sa.burden.cov1', 'sa.burden.cov2'])). Run a gene burden test using the logistic score test on the weighted sum of genotypes per gene.; Here ``va.gene`` is a variant annotation of type String giving a single gene per variant (or no gene if; missing), and ``va.weight`` is a numeric variant annotation:. >>> logreg_kt, sample_kt = (hc.read('data/example_burden.vds'); ... .logreg_burden(key_name='gene',; ... variant_keys='va.gene',; ... single_key=True,; ... agg_expr='gs.map(g => va.weight * g.gt).sum()',; ... test='score',; ... y='sa.burden.pheno',; ... covariates=['sa.burden.cov1', 'sa.burden.cov2'])). To use a weighted sum of genotypes with missing genotypes mean-imputed rather than ignored, set; ``agg_expr='gs.map(g => va.weight * orElse(g.gt.toDouble, 2 * va.qc.AF)).sum()'`` where ``va.qc.AF``; is the allele frequency over those samples that have no missing phenotype or covariates. .. caution::. With ``single_key=False``, ``variant_keys`` expects a variant annotation of Set or Array type, in order to; allow each variant to have zero, one, or more keys (for example, the",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:150420,Testability,test,test,150420,"e. Here ``va.genes`` is; a variant annotation of type Set[String] giving the set of genes containing the variant; (see **Extended example** in :py:meth:`.linreg_burden` for a deeper dive in the context of linear regression):. >>> logreg_kt, sample_kt = (hc.read('data/example_burden.vds'); ... .logreg_burden(key_name='gene',; ... variant_keys='va.genes',; ... single_key=False,; ... agg_expr='gs.map(g => g.gt).max()',; ... test='wald',; ... y='sa.burden.pheno',; ... covariates=['sa.burden.cov1', 'sa.burden.cov2'])). Run a gene burden test using the logistic score test on the weighted sum of genotypes per gene.; Here ``va.gene`` is a variant annotation of type String giving a single gene per variant (or no gene if; missing), and ``va.weight`` is a numeric variant annotation:. >>> logreg_kt, sample_kt = (hc.read('data/example_burden.vds'); ... .logreg_burden(key_name='gene',; ... variant_keys='va.gene',; ... single_key=True,; ... agg_expr='gs.map(g => va.weight * g.gt).sum()',; ... test='score',; ... y='sa.burden.pheno',; ... covariates=['sa.burden.cov1', 'sa.burden.cov2'])). To use a weighted sum of genotypes with missing genotypes mean-imputed rather than ignored, set; ``agg_expr='gs.map(g => va.weight * orElse(g.gt.toDouble, 2 * va.qc.AF)).sum()'`` where ``va.qc.AF``; is the allele frequency over those samples that have no missing phenotype or covariates. .. caution::. With ``single_key=False``, ``variant_keys`` expects a variant annotation of Set or Array type, in order to; allow each variant to have zero, one, or more keys (for example, the same variant may appear in multiple; genes). Unlike with type Set, if the same key appears twice in a variant annotation of type Array, then that; variant will be counted twice in that key's group. With ``single_key=True``, ``variant_keys`` expects a; variant annotation whose value is itself the key of interest. In bose cases, variants with missing keys are; ignored. **Notes**. This method modifies :py:meth:`.logreg` by replacing",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:151408,Testability,log,logreg,151408,"heno',; ... covariates=['sa.burden.cov1', 'sa.burden.cov2'])). To use a weighted sum of genotypes with missing genotypes mean-imputed rather than ignored, set; ``agg_expr='gs.map(g => va.weight * orElse(g.gt.toDouble, 2 * va.qc.AF)).sum()'`` where ``va.qc.AF``; is the allele frequency over those samples that have no missing phenotype or covariates. .. caution::. With ``single_key=False``, ``variant_keys`` expects a variant annotation of Set or Array type, in order to; allow each variant to have zero, one, or more keys (for example, the same variant may appear in multiple; genes). Unlike with type Set, if the same key appears twice in a variant annotation of type Array, then that; variant will be counted twice in that key's group. With ``single_key=True``, ``variant_keys`` expects a; variant annotation whose value is itself the key of interest. In bose cases, variants with missing keys are; ignored. **Notes**. This method modifies :py:meth:`.logreg` by replacing the genotype covariate per variant and sample with; an aggregated (i.e., collapsed) score per key and sample. This numeric score is computed from the sample's; genotypes and annotations over all variants with that key. The phenotype type must either be numeric; (with all present values 0 or 1) or Boolean, in which case true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'), Rao score test ('score'),; and Firth test ('firth') as the ``test`` parameter. Conceptually, the method proceeds as follows:. 1) Filter to the set of samples for which all phenotype and covariates are defined. 2) For each key and sample, aggregate genotypes across variants with that key to produce a numeric score.; ``agg_expr`` must be of numeric type and has the following symbols are in scope:. - ``s`` (*Sample*): sample; - ``sa``: sample annotations; - ``global``: global annotations; - ``gs`` (*Aggregable[Genotype]*): aggregable of :ref:`genotype` for sample ``s``. Note that",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:151824,Testability,test,test,151824,"riant annotation of Set or Array type, in order to; allow each variant to have zero, one, or more keys (for example, the same variant may appear in multiple; genes). Unlike with type Set, if the same key appears twice in a variant annotation of type Array, then that; variant will be counted twice in that key's group. With ``single_key=True``, ``variant_keys`` expects a; variant annotation whose value is itself the key of interest. In bose cases, variants with missing keys are; ignored. **Notes**. This method modifies :py:meth:`.logreg` by replacing the genotype covariate per variant and sample with; an aggregated (i.e., collapsed) score per key and sample. This numeric score is computed from the sample's; genotypes and annotations over all variants with that key. The phenotype type must either be numeric; (with all present values 0 or 1) or Boolean, in which case true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'), Rao score test ('score'),; and Firth test ('firth') as the ``test`` parameter. Conceptually, the method proceeds as follows:. 1) Filter to the set of samples for which all phenotype and covariates are defined. 2) For each key and sample, aggregate genotypes across variants with that key to produce a numeric score.; ``agg_expr`` must be of numeric type and has the following symbols are in scope:. - ``s`` (*Sample*): sample; - ``sa``: sample annotations; - ``global``: global annotations; - ``gs`` (*Aggregable[Genotype]*): aggregable of :ref:`genotype` for sample ``s``. Note that ``v``, ``va``, and ``g`` are accessible through; `Aggregable methods <https://hail.is/hail/types.html#aggregable>`_ on ``gs``. The resulting **sample key table** has key column ``key_name`` and a numeric column of scores for each sample; named by the sample ID. 3) For each key, fit the logistic regression model using the supplied phenotype, covariates, and test.; The model and tests are those of :py:meth:`.logreg` wi",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:151856,Testability,test,test,151856,"riant annotation of Set or Array type, in order to; allow each variant to have zero, one, or more keys (for example, the same variant may appear in multiple; genes). Unlike with type Set, if the same key appears twice in a variant annotation of type Array, then that; variant will be counted twice in that key's group. With ``single_key=True``, ``variant_keys`` expects a; variant annotation whose value is itself the key of interest. In bose cases, variants with missing keys are; ignored. **Notes**. This method modifies :py:meth:`.logreg` by replacing the genotype covariate per variant and sample with; an aggregated (i.e., collapsed) score per key and sample. This numeric score is computed from the sample's; genotypes and annotations over all variants with that key. The phenotype type must either be numeric; (with all present values 0 or 1) or Boolean, in which case true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'), Rao score test ('score'),; and Firth test ('firth') as the ``test`` parameter. Conceptually, the method proceeds as follows:. 1) Filter to the set of samples for which all phenotype and covariates are defined. 2) For each key and sample, aggregate genotypes across variants with that key to produce a numeric score.; ``agg_expr`` must be of numeric type and has the following symbols are in scope:. - ``s`` (*Sample*): sample; - ``sa``: sample annotations; - ``global``: global annotations; - ``gs`` (*Aggregable[Genotype]*): aggregable of :ref:`genotype` for sample ``s``. Note that ``v``, ``va``, and ``g`` are accessible through; `Aggregable methods <https://hail.is/hail/types.html#aggregable>`_ on ``gs``. The resulting **sample key table** has key column ``key_name`` and a numeric column of scores for each sample; named by the sample ID. 3) For each key, fit the logistic regression model using the supplied phenotype, covariates, and test.; The model and tests are those of :py:meth:`.logreg` wi",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:151880,Testability,test,test,151880,"riant annotation of Set or Array type, in order to; allow each variant to have zero, one, or more keys (for example, the same variant may appear in multiple; genes). Unlike with type Set, if the same key appears twice in a variant annotation of type Array, then that; variant will be counted twice in that key's group. With ``single_key=True``, ``variant_keys`` expects a; variant annotation whose value is itself the key of interest. In bose cases, variants with missing keys are; ignored. **Notes**. This method modifies :py:meth:`.logreg` by replacing the genotype covariate per variant and sample with; an aggregated (i.e., collapsed) score per key and sample. This numeric score is computed from the sample's; genotypes and annotations over all variants with that key. The phenotype type must either be numeric; (with all present values 0 or 1) or Boolean, in which case true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'), Rao score test ('score'),; and Firth test ('firth') as the ``test`` parameter. Conceptually, the method proceeds as follows:. 1) Filter to the set of samples for which all phenotype and covariates are defined. 2) For each key and sample, aggregate genotypes across variants with that key to produce a numeric score.; ``agg_expr`` must be of numeric type and has the following symbols are in scope:. - ``s`` (*Sample*): sample; - ``sa``: sample annotations; - ``global``: global annotations; - ``gs`` (*Aggregable[Genotype]*): aggregable of :ref:`genotype` for sample ``s``. Note that ``v``, ``va``, and ``g`` are accessible through; `Aggregable methods <https://hail.is/hail/types.html#aggregable>`_ on ``gs``. The resulting **sample key table** has key column ``key_name`` and a numeric column of scores for each sample; named by the sample ID. 3) For each key, fit the logistic regression model using the supplied phenotype, covariates, and test.; The model and tests are those of :py:meth:`.logreg` wi",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:151907,Testability,test,test,151907,"riant annotation of Set or Array type, in order to; allow each variant to have zero, one, or more keys (for example, the same variant may appear in multiple; genes). Unlike with type Set, if the same key appears twice in a variant annotation of type Array, then that; variant will be counted twice in that key's group. With ``single_key=True``, ``variant_keys`` expects a; variant annotation whose value is itself the key of interest. In bose cases, variants with missing keys are; ignored. **Notes**. This method modifies :py:meth:`.logreg` by replacing the genotype covariate per variant and sample with; an aggregated (i.e., collapsed) score per key and sample. This numeric score is computed from the sample's; genotypes and annotations over all variants with that key. The phenotype type must either be numeric; (with all present values 0 or 1) or Boolean, in which case true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'), Rao score test ('score'),; and Firth test ('firth') as the ``test`` parameter. Conceptually, the method proceeds as follows:. 1) Filter to the set of samples for which all phenotype and covariates are defined. 2) For each key and sample, aggregate genotypes across variants with that key to produce a numeric score.; ``agg_expr`` must be of numeric type and has the following symbols are in scope:. - ``s`` (*Sample*): sample; - ``sa``: sample annotations; - ``global``: global annotations; - ``gs`` (*Aggregable[Genotype]*): aggregable of :ref:`genotype` for sample ``s``. Note that ``v``, ``va``, and ``g`` are accessible through; `Aggregable methods <https://hail.is/hail/types.html#aggregable>`_ on ``gs``. The resulting **sample key table** has key column ``key_name`` and a numeric column of scores for each sample; named by the sample ID. 3) For each key, fit the logistic regression model using the supplied phenotype, covariates, and test.; The model and tests are those of :py:meth:`.logreg` wi",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:151931,Testability,test,test,151931,"riant annotation of Set or Array type, in order to; allow each variant to have zero, one, or more keys (for example, the same variant may appear in multiple; genes). Unlike with type Set, if the same key appears twice in a variant annotation of type Array, then that; variant will be counted twice in that key's group. With ``single_key=True``, ``variant_keys`` expects a; variant annotation whose value is itself the key of interest. In bose cases, variants with missing keys are; ignored. **Notes**. This method modifies :py:meth:`.logreg` by replacing the genotype covariate per variant and sample with; an aggregated (i.e., collapsed) score per key and sample. This numeric score is computed from the sample's; genotypes and annotations over all variants with that key. The phenotype type must either be numeric; (with all present values 0 or 1) or Boolean, in which case true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'), Rao score test ('score'),; and Firth test ('firth') as the ``test`` parameter. Conceptually, the method proceeds as follows:. 1) Filter to the set of samples for which all phenotype and covariates are defined. 2) For each key and sample, aggregate genotypes across variants with that key to produce a numeric score.; ``agg_expr`` must be of numeric type and has the following symbols are in scope:. - ``s`` (*Sample*): sample; - ``sa``: sample annotations; - ``global``: global annotations; - ``gs`` (*Aggregable[Genotype]*): aggregable of :ref:`genotype` for sample ``s``. Note that ``v``, ``va``, and ``g`` are accessible through; `Aggregable methods <https://hail.is/hail/types.html#aggregable>`_ on ``gs``. The resulting **sample key table** has key column ``key_name`` and a numeric column of scores for each sample; named by the sample ID. 3) For each key, fit the logistic regression model using the supplied phenotype, covariates, and test.; The model and tests are those of :py:meth:`.logreg` wi",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:152741,Testability,log,logistic,152741,"re coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'), Rao score test ('score'),; and Firth test ('firth') as the ``test`` parameter. Conceptually, the method proceeds as follows:. 1) Filter to the set of samples for which all phenotype and covariates are defined. 2) For each key and sample, aggregate genotypes across variants with that key to produce a numeric score.; ``agg_expr`` must be of numeric type and has the following symbols are in scope:. - ``s`` (*Sample*): sample; - ``sa``: sample annotations; - ``global``: global annotations; - ``gs`` (*Aggregable[Genotype]*): aggregable of :ref:`genotype` for sample ``s``. Note that ``v``, ``va``, and ``g`` are accessible through; `Aggregable methods <https://hail.is/hail/types.html#aggregable>`_ on ``gs``. The resulting **sample key table** has key column ``key_name`` and a numeric column of scores for each sample; named by the sample ID. 3) For each key, fit the logistic regression model using the supplied phenotype, covariates, and test.; The model and tests are those of :py:meth:`.logreg` with sample genotype ``gt`` replaced by the; score in the sample key table. For each key, missing scores are mean-imputed across all samples. The resulting **logistic regression key table** has key column of type String given by the ``key_name``; parameter and additional columns corresponding to the fields of the ``va.logreg`` schema given for ``test``; in :py:meth:`.logreg`. :py:meth:`.logreg_burden` returns both the logistic regression key table and the sample key table. :param str key_name: Name to assign to key column of returned key tables. :param str variant_keys: Variant annotation path for the TArray or TSet of keys associated to each variant. :param bool single_key: if true, ``variant_keys`` is interpreted as a single (or missing) key per variant,; rather than as a collection of keys. :param str agg_expr: Sample aggregation expression (per key). :param str test: Statistical",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:152813,Testability,test,test,152813,"re coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'), Rao score test ('score'),; and Firth test ('firth') as the ``test`` parameter. Conceptually, the method proceeds as follows:. 1) Filter to the set of samples for which all phenotype and covariates are defined. 2) For each key and sample, aggregate genotypes across variants with that key to produce a numeric score.; ``agg_expr`` must be of numeric type and has the following symbols are in scope:. - ``s`` (*Sample*): sample; - ``sa``: sample annotations; - ``global``: global annotations; - ``gs`` (*Aggregable[Genotype]*): aggregable of :ref:`genotype` for sample ``s``. Note that ``v``, ``va``, and ``g`` are accessible through; `Aggregable methods <https://hail.is/hail/types.html#aggregable>`_ on ``gs``. The resulting **sample key table** has key column ``key_name`` and a numeric column of scores for each sample; named by the sample ID. 3) For each key, fit the logistic regression model using the supplied phenotype, covariates, and test.; The model and tests are those of :py:meth:`.logreg` with sample genotype ``gt`` replaced by the; score in the sample key table. For each key, missing scores are mean-imputed across all samples. The resulting **logistic regression key table** has key column of type String given by the ``key_name``; parameter and additional columns corresponding to the fields of the ``va.logreg`` schema given for ``test``; in :py:meth:`.logreg`. :py:meth:`.logreg_burden` returns both the logistic regression key table and the sample key table. :param str key_name: Name to assign to key column of returned key tables. :param str variant_keys: Variant annotation path for the TArray or TSet of keys associated to each variant. :param bool single_key: if true, ``variant_keys`` is interpreted as a single (or missing) key per variant,; rather than as a collection of keys. :param str agg_expr: Sample aggregation expression (per key). :param str test: Statistical",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:152834,Testability,test,tests,152834,"kelihood ratio test ('lrt'), Rao score test ('score'),; and Firth test ('firth') as the ``test`` parameter. Conceptually, the method proceeds as follows:. 1) Filter to the set of samples for which all phenotype and covariates are defined. 2) For each key and sample, aggregate genotypes across variants with that key to produce a numeric score.; ``agg_expr`` must be of numeric type and has the following symbols are in scope:. - ``s`` (*Sample*): sample; - ``sa``: sample annotations; - ``global``: global annotations; - ``gs`` (*Aggregable[Genotype]*): aggregable of :ref:`genotype` for sample ``s``. Note that ``v``, ``va``, and ``g`` are accessible through; `Aggregable methods <https://hail.is/hail/types.html#aggregable>`_ on ``gs``. The resulting **sample key table** has key column ``key_name`` and a numeric column of scores for each sample; named by the sample ID. 3) For each key, fit the logistic regression model using the supplied phenotype, covariates, and test.; The model and tests are those of :py:meth:`.logreg` with sample genotype ``gt`` replaced by the; score in the sample key table. For each key, missing scores are mean-imputed across all samples. The resulting **logistic regression key table** has key column of type String given by the ``key_name``; parameter and additional columns corresponding to the fields of the ``va.logreg`` schema given for ``test``; in :py:meth:`.logreg`. :py:meth:`.logreg_burden` returns both the logistic regression key table and the sample key table. :param str key_name: Name to assign to key column of returned key tables. :param str variant_keys: Variant annotation path for the TArray or TSet of keys associated to each variant. :param bool single_key: if true, ``variant_keys`` is interpreted as a single (or missing) key per variant,; rather than as a collection of keys. :param str agg_expr: Sample aggregation expression (per key). :param str test: Statistical test, one of: 'wald', 'lrt', 'score', or 'firth'. :param str y: Response ",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:152864,Testability,log,logreg,152864,"h test ('firth') as the ``test`` parameter. Conceptually, the method proceeds as follows:. 1) Filter to the set of samples for which all phenotype and covariates are defined. 2) For each key and sample, aggregate genotypes across variants with that key to produce a numeric score.; ``agg_expr`` must be of numeric type and has the following symbols are in scope:. - ``s`` (*Sample*): sample; - ``sa``: sample annotations; - ``global``: global annotations; - ``gs`` (*Aggregable[Genotype]*): aggregable of :ref:`genotype` for sample ``s``. Note that ``v``, ``va``, and ``g`` are accessible through; `Aggregable methods <https://hail.is/hail/types.html#aggregable>`_ on ``gs``. The resulting **sample key table** has key column ``key_name`` and a numeric column of scores for each sample; named by the sample ID. 3) For each key, fit the logistic regression model using the supplied phenotype, covariates, and test.; The model and tests are those of :py:meth:`.logreg` with sample genotype ``gt`` replaced by the; score in the sample key table. For each key, missing scores are mean-imputed across all samples. The resulting **logistic regression key table** has key column of type String given by the ``key_name``; parameter and additional columns corresponding to the fields of the ``va.logreg`` schema given for ``test``; in :py:meth:`.logreg`. :py:meth:`.logreg_burden` returns both the logistic regression key table and the sample key table. :param str key_name: Name to assign to key column of returned key tables. :param str variant_keys: Variant annotation path for the TArray or TSet of keys associated to each variant. :param bool single_key: if true, ``variant_keys`` is interpreted as a single (or missing) key per variant,; rather than as a collection of keys. :param str agg_expr: Sample aggregation expression (per key). :param str test: Statistical test, one of: 'wald', 'lrt', 'score', or 'firth'. :param str y: Response expression. :param covariates: list of covariate expressions.; :t",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:153030,Testability,log,logistic,153030,"mple, aggregate genotypes across variants with that key to produce a numeric score.; ``agg_expr`` must be of numeric type and has the following symbols are in scope:. - ``s`` (*Sample*): sample; - ``sa``: sample annotations; - ``global``: global annotations; - ``gs`` (*Aggregable[Genotype]*): aggregable of :ref:`genotype` for sample ``s``. Note that ``v``, ``va``, and ``g`` are accessible through; `Aggregable methods <https://hail.is/hail/types.html#aggregable>`_ on ``gs``. The resulting **sample key table** has key column ``key_name`` and a numeric column of scores for each sample; named by the sample ID. 3) For each key, fit the logistic regression model using the supplied phenotype, covariates, and test.; The model and tests are those of :py:meth:`.logreg` with sample genotype ``gt`` replaced by the; score in the sample key table. For each key, missing scores are mean-imputed across all samples. The resulting **logistic regression key table** has key column of type String given by the ``key_name``; parameter and additional columns corresponding to the fields of the ``va.logreg`` schema given for ``test``; in :py:meth:`.logreg`. :py:meth:`.logreg_burden` returns both the logistic regression key table and the sample key table. :param str key_name: Name to assign to key column of returned key tables. :param str variant_keys: Variant annotation path for the TArray or TSet of keys associated to each variant. :param bool single_key: if true, ``variant_keys`` is interpreted as a single (or missing) key per variant,; rather than as a collection of keys. :param str agg_expr: Sample aggregation expression (per key). :param str test: Statistical test, one of: 'wald', 'lrt', 'score', or 'firth'. :param str y: Response expression. :param covariates: list of covariate expressions.; :type covariates: list of str. :return: Tuple of logistic regression key table and sample aggregation key table.; :rtype: (:py:class:`.KeyTable`, :py:class:`.KeyTable`); """""". r = self._jvdf.logregBur",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:153192,Testability,log,logreg,153192,"c type and has the following symbols are in scope:. - ``s`` (*Sample*): sample; - ``sa``: sample annotations; - ``global``: global annotations; - ``gs`` (*Aggregable[Genotype]*): aggregable of :ref:`genotype` for sample ``s``. Note that ``v``, ``va``, and ``g`` are accessible through; `Aggregable methods <https://hail.is/hail/types.html#aggregable>`_ on ``gs``. The resulting **sample key table** has key column ``key_name`` and a numeric column of scores for each sample; named by the sample ID. 3) For each key, fit the logistic regression model using the supplied phenotype, covariates, and test.; The model and tests are those of :py:meth:`.logreg` with sample genotype ``gt`` replaced by the; score in the sample key table. For each key, missing scores are mean-imputed across all samples. The resulting **logistic regression key table** has key column of type String given by the ``key_name``; parameter and additional columns corresponding to the fields of the ``va.logreg`` schema given for ``test``; in :py:meth:`.logreg`. :py:meth:`.logreg_burden` returns both the logistic regression key table and the sample key table. :param str key_name: Name to assign to key column of returned key tables. :param str variant_keys: Variant annotation path for the TArray or TSet of keys associated to each variant. :param bool single_key: if true, ``variant_keys`` is interpreted as a single (or missing) key per variant,; rather than as a collection of keys. :param str agg_expr: Sample aggregation expression (per key). :param str test: Statistical test, one of: 'wald', 'lrt', 'score', or 'firth'. :param str y: Response expression. :param covariates: list of covariate expressions.; :type covariates: list of str. :return: Tuple of logistic regression key table and sample aggregation key table.; :rtype: (:py:class:`.KeyTable`, :py:class:`.KeyTable`); """""". r = self._jvdf.logregBurden(key_name, variant_keys, single_key, agg_expr, test, y, jarray(Env.jvm().java.lang.String, covariates)); logreg",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:153220,Testability,test,test,153220,"c type and has the following symbols are in scope:. - ``s`` (*Sample*): sample; - ``sa``: sample annotations; - ``global``: global annotations; - ``gs`` (*Aggregable[Genotype]*): aggregable of :ref:`genotype` for sample ``s``. Note that ``v``, ``va``, and ``g`` are accessible through; `Aggregable methods <https://hail.is/hail/types.html#aggregable>`_ on ``gs``. The resulting **sample key table** has key column ``key_name`` and a numeric column of scores for each sample; named by the sample ID. 3) For each key, fit the logistic regression model using the supplied phenotype, covariates, and test.; The model and tests are those of :py:meth:`.logreg` with sample genotype ``gt`` replaced by the; score in the sample key table. For each key, missing scores are mean-imputed across all samples. The resulting **logistic regression key table** has key column of type String given by the ``key_name``; parameter and additional columns corresponding to the fields of the ``va.logreg`` schema given for ``test``; in :py:meth:`.logreg`. :py:meth:`.logreg_burden` returns both the logistic regression key table and the sample key table. :param str key_name: Name to assign to key column of returned key tables. :param str variant_keys: Variant annotation path for the TArray or TSet of keys associated to each variant. :param bool single_key: if true, ``variant_keys`` is interpreted as a single (or missing) key per variant,; rather than as a collection of keys. :param str agg_expr: Sample aggregation expression (per key). :param str test: Statistical test, one of: 'wald', 'lrt', 'score', or 'firth'. :param str y: Response expression. :param covariates: list of covariate expressions.; :type covariates: list of str. :return: Tuple of logistic regression key table and sample aggregation key table.; :rtype: (:py:class:`.KeyTable`, :py:class:`.KeyTable`); """""". r = self._jvdf.logregBurden(key_name, variant_keys, single_key, agg_expr, test, y, jarray(Env.jvm().java.lang.String, covariates)); logreg",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:153242,Testability,log,logreg,153242,"symbols are in scope:. - ``s`` (*Sample*): sample; - ``sa``: sample annotations; - ``global``: global annotations; - ``gs`` (*Aggregable[Genotype]*): aggregable of :ref:`genotype` for sample ``s``. Note that ``v``, ``va``, and ``g`` are accessible through; `Aggregable methods <https://hail.is/hail/types.html#aggregable>`_ on ``gs``. The resulting **sample key table** has key column ``key_name`` and a numeric column of scores for each sample; named by the sample ID. 3) For each key, fit the logistic regression model using the supplied phenotype, covariates, and test.; The model and tests are those of :py:meth:`.logreg` with sample genotype ``gt`` replaced by the; score in the sample key table. For each key, missing scores are mean-imputed across all samples. The resulting **logistic regression key table** has key column of type String given by the ``key_name``; parameter and additional columns corresponding to the fields of the ``va.logreg`` schema given for ``test``; in :py:meth:`.logreg`. :py:meth:`.logreg_burden` returns both the logistic regression key table and the sample key table. :param str key_name: Name to assign to key column of returned key tables. :param str variant_keys: Variant annotation path for the TArray or TSet of keys associated to each variant. :param bool single_key: if true, ``variant_keys`` is interpreted as a single (or missing) key per variant,; rather than as a collection of keys. :param str agg_expr: Sample aggregation expression (per key). :param str test: Statistical test, one of: 'wald', 'lrt', 'score', or 'firth'. :param str y: Response expression. :param covariates: list of covariate expressions.; :type covariates: list of str. :return: Tuple of logistic regression key table and sample aggregation key table.; :rtype: (:py:class:`.KeyTable`, :py:class:`.KeyTable`); """""". r = self._jvdf.logregBurden(key_name, variant_keys, single_key, agg_expr, test, y, jarray(Env.jvm().java.lang.String, covariates)); logreg_kt = KeyTable(self.hc, r._1(",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:153294,Testability,log,logistic,153294,": sample annotations; - ``global``: global annotations; - ``gs`` (*Aggregable[Genotype]*): aggregable of :ref:`genotype` for sample ``s``. Note that ``v``, ``va``, and ``g`` are accessible through; `Aggregable methods <https://hail.is/hail/types.html#aggregable>`_ on ``gs``. The resulting **sample key table** has key column ``key_name`` and a numeric column of scores for each sample; named by the sample ID. 3) For each key, fit the logistic regression model using the supplied phenotype, covariates, and test.; The model and tests are those of :py:meth:`.logreg` with sample genotype ``gt`` replaced by the; score in the sample key table. For each key, missing scores are mean-imputed across all samples. The resulting **logistic regression key table** has key column of type String given by the ``key_name``; parameter and additional columns corresponding to the fields of the ``va.logreg`` schema given for ``test``; in :py:meth:`.logreg`. :py:meth:`.logreg_burden` returns both the logistic regression key table and the sample key table. :param str key_name: Name to assign to key column of returned key tables. :param str variant_keys: Variant annotation path for the TArray or TSet of keys associated to each variant. :param bool single_key: if true, ``variant_keys`` is interpreted as a single (or missing) key per variant,; rather than as a collection of keys. :param str agg_expr: Sample aggregation expression (per key). :param str test: Statistical test, one of: 'wald', 'lrt', 'score', or 'firth'. :param str y: Response expression. :param covariates: list of covariate expressions.; :type covariates: list of str. :return: Tuple of logistic regression key table and sample aggregation key table.; :rtype: (:py:class:`.KeyTable`, :py:class:`.KeyTable`); """""". r = self._jvdf.logregBurden(key_name, variant_keys, single_key, agg_expr, test, y, jarray(Env.jvm().java.lang.String, covariates)); logreg_kt = KeyTable(self.hc, r._1()); sample_kt = KeyTable(self.hc, r._2()). return logreg_kt,",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:153750,Testability,test,test,153750,"supplied phenotype, covariates, and test.; The model and tests are those of :py:meth:`.logreg` with sample genotype ``gt`` replaced by the; score in the sample key table. For each key, missing scores are mean-imputed across all samples. The resulting **logistic regression key table** has key column of type String given by the ``key_name``; parameter and additional columns corresponding to the fields of the ``va.logreg`` schema given for ``test``; in :py:meth:`.logreg`. :py:meth:`.logreg_burden` returns both the logistic regression key table and the sample key table. :param str key_name: Name to assign to key column of returned key tables. :param str variant_keys: Variant annotation path for the TArray or TSet of keys associated to each variant. :param bool single_key: if true, ``variant_keys`` is interpreted as a single (or missing) key per variant,; rather than as a collection of keys. :param str agg_expr: Sample aggregation expression (per key). :param str test: Statistical test, one of: 'wald', 'lrt', 'score', or 'firth'. :param str y: Response expression. :param covariates: list of covariate expressions.; :type covariates: list of str. :return: Tuple of logistic regression key table and sample aggregation key table.; :rtype: (:py:class:`.KeyTable`, :py:class:`.KeyTable`); """""". r = self._jvdf.logregBurden(key_name, variant_keys, single_key, agg_expr, test, y, jarray(Env.jvm().java.lang.String, covariates)); logreg_kt = KeyTable(self.hc, r._1()); sample_kt = KeyTable(self.hc, r._2()). return logreg_kt, sample_kt. [docs] @handle_py4j; @requireTGenotype; @typecheck_method(pedigree=Pedigree); def mendel_errors(self, pedigree):; """"""Find Mendel errors; count per variant, individual and nuclear; family. .. include:: requireTGenotype.rst. **Examples**. Find all violations of Mendelian inheritance in each (dad,; mom, kid) trio in a pedigree and return four tables:. >>> ped = Pedigree.read('data/trios.fam'); >>> all, per_fam, per_sample, per_variant = vds.mendel_errors(ped)",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:153768,Testability,test,test,153768,"supplied phenotype, covariates, and test.; The model and tests are those of :py:meth:`.logreg` with sample genotype ``gt`` replaced by the; score in the sample key table. For each key, missing scores are mean-imputed across all samples. The resulting **logistic regression key table** has key column of type String given by the ``key_name``; parameter and additional columns corresponding to the fields of the ``va.logreg`` schema given for ``test``; in :py:meth:`.logreg`. :py:meth:`.logreg_burden` returns both the logistic regression key table and the sample key table. :param str key_name: Name to assign to key column of returned key tables. :param str variant_keys: Variant annotation path for the TArray or TSet of keys associated to each variant. :param bool single_key: if true, ``variant_keys`` is interpreted as a single (or missing) key per variant,; rather than as a collection of keys. :param str agg_expr: Sample aggregation expression (per key). :param str test: Statistical test, one of: 'wald', 'lrt', 'score', or 'firth'. :param str y: Response expression. :param covariates: list of covariate expressions.; :type covariates: list of str. :return: Tuple of logistic regression key table and sample aggregation key table.; :rtype: (:py:class:`.KeyTable`, :py:class:`.KeyTable`); """""". r = self._jvdf.logregBurden(key_name, variant_keys, single_key, agg_expr, test, y, jarray(Env.jvm().java.lang.String, covariates)); logreg_kt = KeyTable(self.hc, r._1()); sample_kt = KeyTable(self.hc, r._2()). return logreg_kt, sample_kt. [docs] @handle_py4j; @requireTGenotype; @typecheck_method(pedigree=Pedigree); def mendel_errors(self, pedigree):; """"""Find Mendel errors; count per variant, individual and nuclear; family. .. include:: requireTGenotype.rst. **Examples**. Find all violations of Mendelian inheritance in each (dad,; mom, kid) trio in a pedigree and return four tables:. >>> ped = Pedigree.read('data/trios.fam'); >>> all, per_fam, per_sample, per_variant = vds.mendel_errors(ped)",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:153953,Testability,log,logistic,153953,"s are mean-imputed across all samples. The resulting **logistic regression key table** has key column of type String given by the ``key_name``; parameter and additional columns corresponding to the fields of the ``va.logreg`` schema given for ``test``; in :py:meth:`.logreg`. :py:meth:`.logreg_burden` returns both the logistic regression key table and the sample key table. :param str key_name: Name to assign to key column of returned key tables. :param str variant_keys: Variant annotation path for the TArray or TSet of keys associated to each variant. :param bool single_key: if true, ``variant_keys`` is interpreted as a single (or missing) key per variant,; rather than as a collection of keys. :param str agg_expr: Sample aggregation expression (per key). :param str test: Statistical test, one of: 'wald', 'lrt', 'score', or 'firth'. :param str y: Response expression. :param covariates: list of covariate expressions.; :type covariates: list of str. :return: Tuple of logistic regression key table and sample aggregation key table.; :rtype: (:py:class:`.KeyTable`, :py:class:`.KeyTable`); """""". r = self._jvdf.logregBurden(key_name, variant_keys, single_key, agg_expr, test, y, jarray(Env.jvm().java.lang.String, covariates)); logreg_kt = KeyTable(self.hc, r._1()); sample_kt = KeyTable(self.hc, r._2()). return logreg_kt, sample_kt. [docs] @handle_py4j; @requireTGenotype; @typecheck_method(pedigree=Pedigree); def mendel_errors(self, pedigree):; """"""Find Mendel errors; count per variant, individual and nuclear; family. .. include:: requireTGenotype.rst. **Examples**. Find all violations of Mendelian inheritance in each (dad,; mom, kid) trio in a pedigree and return four tables:. >>> ped = Pedigree.read('data/trios.fam'); >>> all, per_fam, per_sample, per_variant = vds.mendel_errors(ped); ; Export all mendel errors to a text file:; ; >>> all.export('output/all_mendel_errors.tsv'). Annotate samples with the number of Mendel errors:; ; >>> annotated_vds = vds.annotate_samples_table(",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:154094,Testability,log,logregBurden,154094,"additional columns corresponding to the fields of the ``va.logreg`` schema given for ``test``; in :py:meth:`.logreg`. :py:meth:`.logreg_burden` returns both the logistic regression key table and the sample key table. :param str key_name: Name to assign to key column of returned key tables. :param str variant_keys: Variant annotation path for the TArray or TSet of keys associated to each variant. :param bool single_key: if true, ``variant_keys`` is interpreted as a single (or missing) key per variant,; rather than as a collection of keys. :param str agg_expr: Sample aggregation expression (per key). :param str test: Statistical test, one of: 'wald', 'lrt', 'score', or 'firth'. :param str y: Response expression. :param covariates: list of covariate expressions.; :type covariates: list of str. :return: Tuple of logistic regression key table and sample aggregation key table.; :rtype: (:py:class:`.KeyTable`, :py:class:`.KeyTable`); """""". r = self._jvdf.logregBurden(key_name, variant_keys, single_key, agg_expr, test, y, jarray(Env.jvm().java.lang.String, covariates)); logreg_kt = KeyTable(self.hc, r._1()); sample_kt = KeyTable(self.hc, r._2()). return logreg_kt, sample_kt. [docs] @handle_py4j; @requireTGenotype; @typecheck_method(pedigree=Pedigree); def mendel_errors(self, pedigree):; """"""Find Mendel errors; count per variant, individual and nuclear; family. .. include:: requireTGenotype.rst. **Examples**. Find all violations of Mendelian inheritance in each (dad,; mom, kid) trio in a pedigree and return four tables:. >>> ped = Pedigree.read('data/trios.fam'); >>> all, per_fam, per_sample, per_variant = vds.mendel_errors(ped); ; Export all mendel errors to a text file:; ; >>> all.export('output/all_mendel_errors.tsv'). Annotate samples with the number of Mendel errors:; ; >>> annotated_vds = vds.annotate_samples_table(per_sample, root=""sa.mendel""); ; Annotate variants with the number of Mendel errors:; ; >>> annotated_vds = vds.annotate_variants_table(per_variant, root=""va.m",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:154153,Testability,test,test,154153,"additional columns corresponding to the fields of the ``va.logreg`` schema given for ``test``; in :py:meth:`.logreg`. :py:meth:`.logreg_burden` returns both the logistic regression key table and the sample key table. :param str key_name: Name to assign to key column of returned key tables. :param str variant_keys: Variant annotation path for the TArray or TSet of keys associated to each variant. :param bool single_key: if true, ``variant_keys`` is interpreted as a single (or missing) key per variant,; rather than as a collection of keys. :param str agg_expr: Sample aggregation expression (per key). :param str test: Statistical test, one of: 'wald', 'lrt', 'score', or 'firth'. :param str y: Response expression. :param covariates: list of covariate expressions.; :type covariates: list of str. :return: Tuple of logistic regression key table and sample aggregation key table.; :rtype: (:py:class:`.KeyTable`, :py:class:`.KeyTable`); """""". r = self._jvdf.logregBurden(key_name, variant_keys, single_key, agg_expr, test, y, jarray(Env.jvm().java.lang.String, covariates)); logreg_kt = KeyTable(self.hc, r._1()); sample_kt = KeyTable(self.hc, r._2()). return logreg_kt, sample_kt. [docs] @handle_py4j; @requireTGenotype; @typecheck_method(pedigree=Pedigree); def mendel_errors(self, pedigree):; """"""Find Mendel errors; count per variant, individual and nuclear; family. .. include:: requireTGenotype.rst. **Examples**. Find all violations of Mendelian inheritance in each (dad,; mom, kid) trio in a pedigree and return four tables:. >>> ped = Pedigree.read('data/trios.fam'); >>> all, per_fam, per_sample, per_variant = vds.mendel_errors(ped); ; Export all mendel errors to a text file:; ; >>> all.export('output/all_mendel_errors.tsv'). Annotate samples with the number of Mendel errors:; ; >>> annotated_vds = vds.annotate_samples_table(per_sample, root=""sa.mendel""); ; Annotate variants with the number of Mendel errors:; ; >>> annotated_vds = vds.annotate_variants_table(per_variant, root=""va.m",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:206098,Testability,test,test,206098,"']). If we now export this VDS as VCF, it would produce the following header (for these new fields):. .. code-block:: text. ##INFO=<ID=AC_HC,Number=.,Type=String,Description="""". This header doesn't contain all information that should be present in an optimal VCF header:; 1) There is no FILTER entry for `HardFilter`; 2) Since `AC_HC` has one entry per non-reference allele, its `Number` should be `A`; 3) `AC_HC` should have a Description. We can fix this by setting the attributes of these fields:. >>> annotated_vds = (annotated_vds; ... .set_va_attributes(; ... 'va.info.AC_HC',; ... {'Description': 'Allele count for high quality genotypes (DP >= 10, GQ >= 20)',; ... 'Number': 'A'}); ... .set_va_attributes(; ... 'va.filters',; ... {'HardFilter': 'This site fails GATK suggested hard filters.'})). Exporting the VDS with the attributes now prints the following header lines:. .. code-block:: text. ##INFO=<ID=test,Number=A,Type=String,Description=""Allele count for high quality genotypes (DP >= 10, GQ >= 20)""; ##FILTER=<ID=HardFilter,Description=""This site fails GATK suggested hard filters."">. :param str ann_path: Path to variant annotation beginning with `va`. :param dict attributes: A str-str dict containing the attributes to set. :return: Annotated dataset with the attribute added to the variant annotation.; :rtype: :class:`.VariantDataset`. """""". return VariantDataset(self.hc, self._jvds.setVaAttributes(ann_path, Env.jutils().javaMapToMap(attributes))). [docs] @handle_py4j; @typecheck_method(ann_path=strlike,; attribute=strlike); def delete_va_attribute(self, ann_path, attribute):; """"""Removes an attribute from a variant annotation field.; Attributes are key/value pairs that can be attached to a variant annotation field. The following attributes are read from the VCF header when importing a VCF and written; to the VCF header when exporting a VCF:. - INFO fields attributes (attached to (`va.info.*`)):. - 'Number': The arity of the field. Can take values. - `0` (Boolean flag)",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:213214,Testability,test,test,213214,"es: Do not filter out * alleles.; :param int max_shift: maximum number of base pairs by which; a split variant can move. Affects memory usage, and will; cause Hail to throw an error if a variant that moves further; is encountered. :return: A biallelic variant dataset.; :rtype: :py:class:`.VariantDataset`; """""". jvds = self._jvdf.splitMulti(propagate_gq, keep_star_alleles, max_shift); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(pedigree=Pedigree,; root=strlike); def tdt(self, pedigree, root='va.tdt'):; """"""Find transmitted and untransmitted variants; count per variant and; nuclear family. .. include:: requireTGenotype.rst. **Examples**. Compute TDT association results:. >>> pedigree = Pedigree.read('data/trios.fam'); >>> (vds.tdt(pedigree); ... .export_variants(""output/tdt_results.tsv"", ""Variant = v, va.tdt.*"")). **Notes**. The transmission disequilibrium test tracks the number of times the alternate allele is transmitted (t) or not transmitted (u) from a heterozgyous parent to an affected child under the null that the rate of such transmissions is 0.5. For variants where transmission is guaranteed (i.e., the Y chromosome, mitochondria, and paternal chromosome X variants outside of the PAR), the test cannot be used. The TDT statistic is given by. .. math::. (t-u)^2 \over (t+u). and follows a 1 degree of freedom chi-squared distribution under the null hypothesis. The number of transmissions and untransmissions for each possible set of genotypes is determined from the table below. The copy state of a locus with respect to a trio is defined as follows, where PAR is the pseudoautosomal region (PAR). - HemiX -- in non-PAR of X and child is male; - Auto -- otherwise (in autosome or PAR, or child is female). +--------+--------+--------+------------+---+---+; | Kid | Dad | Mom | Copy State | T | U |; +========+========+========+============+===+===+; | HomRef | Het | Het | Auto | 0 | 2 |; +--------+--------+--------+---------",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:213561,Testability,test,test,213561,"ed. :return: A biallelic variant dataset.; :rtype: :py:class:`.VariantDataset`; """""". jvds = self._jvdf.splitMulti(propagate_gq, keep_star_alleles, max_shift); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(pedigree=Pedigree,; root=strlike); def tdt(self, pedigree, root='va.tdt'):; """"""Find transmitted and untransmitted variants; count per variant and; nuclear family. .. include:: requireTGenotype.rst. **Examples**. Compute TDT association results:. >>> pedigree = Pedigree.read('data/trios.fam'); >>> (vds.tdt(pedigree); ... .export_variants(""output/tdt_results.tsv"", ""Variant = v, va.tdt.*"")). **Notes**. The transmission disequilibrium test tracks the number of times the alternate allele is transmitted (t) or not transmitted (u) from a heterozgyous parent to an affected child under the null that the rate of such transmissions is 0.5. For variants where transmission is guaranteed (i.e., the Y chromosome, mitochondria, and paternal chromosome X variants outside of the PAR), the test cannot be used. The TDT statistic is given by. .. math::. (t-u)^2 \over (t+u). and follows a 1 degree of freedom chi-squared distribution under the null hypothesis. The number of transmissions and untransmissions for each possible set of genotypes is determined from the table below. The copy state of a locus with respect to a trio is defined as follows, where PAR is the pseudoautosomal region (PAR). - HemiX -- in non-PAR of X and child is male; - Auto -- otherwise (in autosome or PAR, or child is female). +--------+--------+--------+------------+---+---+; | Kid | Dad | Mom | Copy State | T | U |; +========+========+========+============+===+===+; | HomRef | Het | Het | Auto | 0 | 2 |; +--------+--------+--------+------------+---+---+; | HomRef | HomRef | Het | Auto | 0 | 1 |; +--------+--------+--------+------------+---+---+; | HomRef | Het | HomRef | Auto | 0 | 1 |; +--------+--------+--------+------------+---+---+; | Het | Het | Het | Auto | ",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:5001,Usability,guid,guide,5001,"mple_ids = jiterable_to_list(self._jvds.sampleIds()); return self._sample_ids. @property; @handle_py4j; def sample_annotations(self):; """"""Return a dict of sample annotations. The keys of this dictionary are the sample IDs (strings).; The values are sample annotations. :return: dict; """""". if self._sample_annotations is None:; zipped_annotations = Env.jutils().iterableToArrayList(; self._jvds.sampleIdsAndAnnotations(); ); r = {}; for element in zipped_annotations:; r[element._1()] = self.sample_schema._convert_to_py(element._2()); self._sample_annotations = r; return self._sample_annotations. [docs] @handle_py4j; def num_partitions(self):; """"""Number of partitions. **Notes**. The data in a variant dataset is divided into chunks called partitions, which may be stored together or across a network, so that each partition may be read and processed in parallel by available cores. Partitions are a core concept of distributed computation in Spark, see `here <http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds>`__ for details. :rtype: int; """""". return self._jvds.nPartitions(). @property; @handle_py4j; def num_samples(self):; """"""Number of samples. :rtype: int; """""". if self._num_samples is None:; self._num_samples = self._jvds.nSamples(); return self._num_samples. [docs] @handle_py4j; def count_variants(self):; """"""Count number of variants in variant dataset. :rtype: long; """""". return self._jvds.countVariants(). [docs] @handle_py4j; def was_split(self):; """"""True if multiallelic variants have been split into multiple biallelic variants. Result is True if :py:meth:`~hail.VariantDataset.split_multi` or :py:meth:`~hail.VariantDataset.filter_multi` has been called on this variant dataset,; or if the variant dataset was imported with :py:meth:`~hail.HailContext.import_plink`, :py:meth:`~hail.HailContext.import_gen`,; or :py:meth:`~hail.HailContext.import_bgen`, or if the variant dataset was simulated with :py:meth:`~hail.HailContext.balding_ni",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:91981,Usability,simpl,simply,91981,"taset(self.hc, self._jvds.join(right._jvds)). [docs] @handle_py4j; @typecheck(datasets=tupleof(vds_type)); def union(*datasets):; """"""Take the union of datasets vertically (include all variants). **Examples**. .. testsetup::. vds_autosomal = vds; vds_chromX = vds; vds_chromY = vds. Union two datasets:. >>> vds_union = vds_autosomal.union(vds_chromX). Given a list of datasets, union them all:. >>> all_vds = [vds_autosomal, vds_chromX, vds_chromY]. The following three syntaxes are equivalent:. >>> vds_union1 = vds_autosomal.union(vds_chromX, vds_chromY); >>> vds_union2 = all_vds[0].union(*all_vds[1:]); >>> vds_union3 = VariantDataset.union(*all_vds). **Notes**. In order to combine two datasets, these requirements must be met:; - the samples must match; - the variant annotation schemas must match (field order within structs matters).; - the cell (genotype) schemas must match (field order within structs matters). The column annotations in the resulting dataset are simply the column annotations; from the first dataset; the column annotation schemas do not need to match. This method can trigger a shuffle, if partitions from two datasets overlap. :param vds_type: Datasets to combine.; :type vds_type: tuple of :class:`.VariantDataset`. :return: Dataset with variants from all datasets.; :rtype: :class:`.VariantDataset`; """"""; if len(datasets) == 0:; raise ValueError('Expected at least one argument'); elif len(datasets) == 1:; return datasets[0]; else:; return VariantDataset(Env.hc(), Env.hail().variant.VariantSampleMatrix.union([d._jvds for d in datasets])). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(r2=numeric,; window=integral,; memory_per_core=integral,; num_cores=integral); def ld_prune(self, r2=0.2, window=1000000, memory_per_core=256, num_cores=1):; """"""Prune variants in linkage disequilibrium (LD). .. include:: requireTGenotype.rst. Requires :py:class:`~hail.VariantDataset.was_split` equals True. **Examples**. Export the set of common LD pruned variants to ",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:94479,Usability,guid,guide,94479,"lation(v1, v2) >= r2):; keep = False; if keep:; pruned_set.append(v1). The parameter ``window`` defines the maximum distance in base pairs between two variants to check whether; the variants are independent (:math:`R^2` < ``r2``) where ``r2`` is the maximum :math:`R^2` allowed.; :math:`R^2` is defined as the square of `Pearson's correlation coefficient <https://en.wikipedia.org/wiki/Pearson_correlation_coefficient>`__; :math:`{\\rho}_{x,y}` between the two genotype vectors :math:`{\\mathbf{x}}` and :math:`{\\mathbf{y}}`. .. math::. {\\rho}_{x,y} = \\frac{\\mathrm{Cov}(X,Y)}{\\sigma_X \\sigma_Y}. :py:meth:`.ld_prune` with default arguments is equivalent to ``plink --indep-pairwise 1000kb 1 0.2``.; The list of pruned variants returned by Hail and PLINK will differ because Hail mean-imputes missing values and tests pairs of variants in a different order than PLINK. Be sure to provide enough disk space per worker because :py:meth:`.ld_prune` `persists <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__ up to 3 copies of the data to both memory and disk.; The amount of disk space required will depend on the size and minor allele frequency of the input data and the prune parameters ``r2`` and ``window``. The number of bytes stored in memory per variant is about ``nSamples / 4 + 50``. .. warning::. The variants in the pruned set are not guaranteed to be identical each time :py:meth:`.ld_prune` is run. We recommend running :py:meth:`.ld_prune` once and exporting the list of LD pruned variants using; :py:meth:`.export_variants` for future use. :param float r2: Maximum :math:`R^2` threshold between two variants in the pruned set within a given window. :param int window: Width of window in base-pairs for computing pair-wise :math:`R^2` values. :param int memory_per_core: Total amount of memory available for each core in MB. If unsure, use the default value. :param int num_cores: The number of cores available. Equivalent to the total number of worker",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:123675,Usability,simpl,simplest,123675,"=====================================================================+; | ``va.lmmreg.beta`` | Double | fit genotype coefficient, :math:`\hat\\beta_0` |; +------------------------+--------+-------------------------------------------------------------------------+; | ``va.lmmreg.sigmaG2`` | Double | fit coefficient of genetic variance component, :math:`\hat{\sigma}_g^2` |; +------------------------+--------+-------------------------------------------------------------------------+; | ``va.lmmreg.chi2`` | Double | :math:`\chi^2` statistic of the likelihood ratio test |; +------------------------+--------+-------------------------------------------------------------------------+; | ``va.lmmreg.pval`` | Double | :math:`p`-value |; +------------------------+--------+-------------------------------------------------------------------------+. Those variants that don't vary across the included samples (e.g., all genotypes; are HomRef) will have missing annotations. The simplest way to export all resulting annotations is:. >>> lmm_vds.export_variants('output/lmmreg.tsv.bgz', 'variant = v, va.lmmreg.*'); >>> lmmreg_results = lmm_vds.globals['lmmreg']; ; By default, genotypes values are given by hard call genotypes (``g.gt``).; If ``use_dosages=True``, then genotype values for per-variant association are defined by the dosage; :math:`\mathrm{P}(\mathrm{Het}) + 2 \cdot \mathrm{P}(\mathrm{HomVar})`. For Phred-scaled values,; :math:`\mathrm{P}(\mathrm{Het})` and :math:`\mathrm{P}(\mathrm{HomVar})` are; calculated by normalizing the PL likelihoods (converted from the Phred-scale) to sum to 1. **Performance**. Hail's initial version of :py:meth:`.lmmreg` scales beyond 15k samples and to an essentially unbounded number of variants, making it particularly well-suited to modern sequencing studies and complementary to tools designed for SNP arrays. Analysts have used :py:meth:`.lmmreg` in research to compute kinship from 100k common variants and test 32 million non-rare variants on 8k ",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:126861,Usability,simpl,simply,126861,"ributed by variant with per-variant time complexity that is completely independent of the number of sample covariates and dominated by multiplication of the genotype vector :math:`v` by the matrix of eigenvectors :math:`U^T` as described below, which we accelerate with a sparse representation of :math:`v`. The matrix :math:`U^T` has size about :math:`8n^2` bytes and is currently broadcast to each Spark executor. For example, with 15k samples, storing :math:`U^T` consumes about 3.6GB of memory on a 16-core worker node with two 8-core executors. So for large :math:`n`, we recommend using a high-memory configuration such as ``highmem`` workers. **Linear mixed model**. :py:meth:`.lmmreg` estimates the genetic proportion of residual phenotypic variance (narrow-sense heritability) under a kinship-based linear mixed model, and then optionally tests each variant for association using the likelihood ratio test. Inference is exact. We first describe the sample-covariates-only model used to estimate heritability, which we simply refer to as the *global model*. With :math:`n` samples and :math:`c` sample covariates, we define:. - :math:`y = n \\times 1` vector of phenotypes; - :math:`X = n \\times c` matrix of sample covariates and intercept column of ones; - :math:`K = n \\times n` kinship matrix; - :math:`I = n \\times n` identity matrix; - :math:`\\beta = c \\times 1` vector of covariate coefficients; - :math:`\sigma_g^2 =` coefficient of genetic variance component :math:`K`; - :math:`\sigma_e^2 =` coefficient of environmental variance component :math:`I`; - :math:`\delta = \\frac{\sigma_e^2}{\sigma_g^2} =` ratio of environmental and genetic variance component coefficients; - :math:`h^2 = \\frac{\sigma_g^2}{\sigma_g^2 + \sigma_e^2} = \\frac{1}{1 + \delta} =` genetic proportion of residual phenotypic variance. Under a linear mixed model, :math:`y` is sampled from the :math:`n`-dimensional `multivariate normal distribution <https://en.wikipedia.org/wiki/Multivariate_normal_dis",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:135358,Usability,simpl,simply,135358,"enotypes; - :math:`X_v = \\left[v | X \\right] = n \\times (1 + c)` matrix concatenating :math:`v` and :math:`X`; - :math:`\\beta_v = (\\beta^0_v, \\beta^1_v, \\ldots, \\beta^c_v) = (1 + c) \\times 1` vector of covariate coefficients. Fixing :math:`\delta` at the global REML estimate :math:`\\hat{\delta}`, we find the REML estimate :math:`(\\hat{\\beta}_v, \\hat{\sigma}_{g,v}^2)` via rotation of the model. .. math::. y \\sim \\mathrm{N}\\left(X_v\\beta_v, \sigma_{g,v}^2 (K + \\hat{\delta} I)\\right). Note that the only new rotation to compute here is :math:`U^T v`. To test the null hypothesis that the genotype coefficient :math:`\\beta^0_v` is zero, we consider the restricted model with parameters :math:`((0, \\beta^1_v, \ldots, \\beta^c_v), \sigma_{g,v}^2)` within the full model with parameters :math:`(\\beta^0_v, \\beta^1_v, \\ldots, \\beta^c_v), \sigma_{g_v}^2)`, with :math:`\delta` fixed at :math:`\\hat\delta` in both. The latter fit is simply that of the global model, :math:`((0, \\hat{\\beta}^1, \\ldots, \\hat{\\beta}^c), \\hat{\sigma}_g^2)`. The likelihood ratio test statistic is given by. .. math::. \chi^2 = n \\, \\mathrm{ln}\left(\\frac{\hat{\sigma}^2_g}{\\hat{\sigma}_{g,v}^2}\\right). and follows a chi-squared distribution with one degree of freedom. Here the ratio :math:`\\hat{\sigma}^2_g / \\hat{\sigma}_{g,v}^2` captures the degree to which adding the variant :math:`v` to the global model reduces the residual phenotypic variance. **Kinship Matrix**. FastLMM uses the Realized Relationship Matrix (RRM) for kinship. This can be computed with :py:meth:`~hail.VariantDataset.rrm`. However, any instance of :py:class:`KinshipMatrix` may be used, so long as ``sample_list`` contains the complete samples of the caller variant dataset in the same order. **Low-rank approximation of kinship for improved performance**. :py:meth:`.lmmreg` can implicitly use a low-rank approximation of the kinship matrix to more rapidly fit delta and the statistics for each variant. The",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:164241,Usability,simpl,simply,164241,":math:`C_{ij}` is the number of alternate alleles of variant :math:`j` carried by sample :math:`i`, which can be 0, 1, 2, or missing. For each variant :math:`j`, the sample alternate allele frequency :math:`p_j` is computed as half the mean of the non-missing entries of column :math:`j`. Entries of :math:`M` are then mean-centered and variance-normalized as. .. math::. M_{ij} = \\frac{C_{ij}-2p_j}{\sqrt{2p_j(1-p_j)m}},. with :math:`M_{ij} = 0` for :math:`C_{ij}` missing (i.e. mean genotype imputation). This scaling normalizes genotype variances to a common value :math:`1/m` for variants in Hardy-Weinberg equilibrium and is further motivated in the paper cited above. (The resulting amplification of signal from the low end of the allele frequency spectrum will also introduce noise for rare variants; common practice is to filter out variants with minor allele frequency below some cutoff.) The factor :math:`1/m` gives each sample row approximately unit total variance (assuming linkage equilibrium) and yields the sample correlation or genetic relationship matrix (GRM) as simply :math:`MM^T`. PCA then computes the SVD. .. math::. M = USV^T. where columns of :math:`U` are left singular vectors (orthonormal in :math:`\mathbb{R}^n`), columns of :math:`V` are right singular vectors (orthonormal in :math:`\mathbb{R}^m`), and :math:`S=\mathrm{diag}(s_1, s_2, \ldots)` with ordered singular values :math:`s_1 \ge s_2 \ge \cdots \ge 0`. Typically one computes only the first :math:`k` singular vectors and values, yielding the best rank :math:`k` approximation :math:`U_k S_k V_k^T` of :math:`M`; the truncations :math:`U_k`, :math:`S_k` and :math:`V_k` are :math:`n \\times k`, :math:`k \\times k` and :math:`m \\times k` respectively. From the perspective of the samples or rows of :math:`M` as data, :math:`V_k` contains the variant loadings for the first :math:`k` PCs while :math:`MV_k = U_k S_k` contains the first :math:`k` PC scores of each sample. The loadings represent a new basis ",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:165668,Usability,simpl,simply,165668,"best rank :math:`k` approximation :math:`U_k S_k V_k^T` of :math:`M`; the truncations :math:`U_k`, :math:`S_k` and :math:`V_k` are :math:`n \\times k`, :math:`k \\times k` and :math:`m \\times k` respectively. From the perspective of the samples or rows of :math:`M` as data, :math:`V_k` contains the variant loadings for the first :math:`k` PCs while :math:`MV_k = U_k S_k` contains the first :math:`k` PC scores of each sample. The loadings represent a new basis of features while the scores represent the projected data on those features. The eigenvalues of the GRM :math:`MM^T` are the squares of the singular values :math:`s_1^2, s_2^2, \ldots`, which represent the variances carried by the respective PCs. By default, Hail only computes the loadings if the ``loadings`` parameter is specified. *Note:* In PLINK/GCTA the GRM is taken as the starting point and it is computed slightly differently with regard to missing data. Here the :math:`ij` entry of :math:`MM^T` is simply the dot product of rows :math:`i` and :math:`j` of :math:`M`; in terms of :math:`C` it is. .. math::. \\frac{1}{m}\sum_{l\in\mathcal{C}_i\cap\mathcal{C}_j}\\frac{(C_{il}-2p_l)(C_{jl} - 2p_l)}{2p_l(1-p_l)}. where :math:`\mathcal{C}_i = \{l \mid C_{il} \\text{ is non-missing}\}`. In PLINK/GCTA the denominator :math:`m` is replaced with the number of terms in the sum :math:`\\lvert\mathcal{C}_i\cap\\mathcal{C}_j\\rvert`, i.e. the number of variants where both samples have non-missing genotypes. While this is arguably a better estimator of the true GRM (trading shrinkage for noise), it has the drawback that one loses the clean interpretation of the loadings and scores as features and projections. Separately, for the PCs PLINK/GCTA output the eigenvectors of the GRM; even ignoring the above discrepancy that means the left singular vectors :math:`U_k` instead of the component scores :math:`U_k S_k`. While this is just a matter of the scale on each PC, the scores have the advantage of representing true projecti",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:170514,Usability,simpl,simply,170514,"le and; filtering using :py:meth:`~hail.KeyTable.filter`. >>> rel = vds.pc_relate(5, 0.01, min_kinship=0.1). **Method**. The traditional estimator for kinship between a pair of individuals; :math:`i` and :math:`j`, sharing the set :math:`S_{ij}` of; single-nucleotide variants, from a population with allele frequencies; :math:`p_s`, is given by:. .. math::. \\widehat{\phi_{ij}} := \\frac{1}{|S_{ij}|}\\sum_{s \in S_{ij}}\\frac{(g_{is} - 2 p_s) (g_{js} - 2 p_s)}{4 * \sum_{s \in S_{ij} p_s (1 - p_s)}}. This estimator is true under the model that the sharing of common; (relative to the population) alleles is not very informative to; relatedness (because they're common) and the sharing of rare alleles; suggests a recent common ancestor from which the allele was inherited by; descent. When multiple ancestry groups are mixed in a sample, this model breaks; down. Alleles that are rare in all but one ancestry group are treated as; very informative to relatedness. However, these alleles are simply; markers of the ancestry group. The PC-Relate method corrects for this; situation and the related situation of admixed individuals. PC-Relate slightly modifies the usual estimator for relatedness:; occurences of population allele frequency are replaced with an; ""individual-specific allele frequency"". This modification allows the; method to correctly weight an allele according to an individual's unique; ancestry profile. The ""individual-specific allele frequency"" at a given genetic locus is; modeled by PC-Relate as a linear function of their first ``k`` principal; component coordinates. As such, the efficacy of this method rests on two; assumptions:. - an individual's first ``k`` principal component coordinates fully; describe their allele-frequency-relevant ancestry, and. - the relationship between ancestry (as described by principal; component coordinates) and population allele frequency is linear. The estimators for kinship, and identity-by-descent zero, one, and two; follow. Let:.",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:178449,Usability,guid,guide,178449,"d identity-by-descent zero, one, and two.; :rtype: :py:class:`.KeyTable`. """""". intstatistics = { ""phi"" : 0, ""phik2"" : 1, ""phik2k0"" : 2, ""all"" : 3 }[statistics]. return KeyTable(self.hc, self._jvdf.pcRelate(k, maf, block_size, min_kinship, intstatistics)). [docs] @handle_py4j; @typecheck_method(storage_level=strlike); def persist(self, storage_level=""MEMORY_AND_DISK""):; """"""Persist this variant dataset to memory and/or disk. **Examples**. Persist the variant dataset to both memory and disk:. >>> vds_result = vds.persist(). **Notes**. The :py:meth:`~hail.VariantDataset.persist` and :py:meth:`~hail.VariantDataset.cache` methods ; allow you to store the current dataset on disk or in memory to avoid redundant computation and ; improve the performance of Hail pipelines. :py:meth:`~hail.VariantDataset.cache` is an alias for ; :func:`persist(""MEMORY_ONLY"") <hail.VariantDataset.persist>`. Most users will want ""MEMORY_AND_DISK"".; See the `Spark documentation <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__ ; for a more in-depth discussion of persisting data.; ; .. warning ::; ; Persist, like all other :class:`.VariantDataset` functions, is functional.; Its output must be captured. This is wrong:; ; >>> vds = vds.linreg('sa.phenotype') # doctest: +SKIP; >>> vds.persist() # doctest: +SKIP; ; The above code does NOT persist ``vds``. Instead, it copies ``vds`` and persists that result. ; The proper usage is this:; ; >>> vds = vds.pca().persist() # doctest: +SKIP. :param storage_level: Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP; ; :rtype: :class:`.VariantDataset`; """""". return VariantDataset(self.hc, self._jvdf.persist(storage_level)). [docs] def unpersist(self):; """"""; Unpersists this VDS from memory/disk.; ; **Notes**; This function will have no effect on a VDS that was not previously per",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:192404,Usability,guid,guide,192404,"ecrease the number of variant dataset partitions. **Examples**. Repartition the variant dataset to have 500 partitions:. >>> vds_result = vds.repartition(500). **Notes**. Check the current number of partitions with :py:meth:`.num_partitions`. The data in a variant dataset is divided into chunks called partitions, which may be stored together or across a network, so that each partition may be read and processed in parallel by available cores. When a variant dataset with :math:`M` variants is first imported, each of the :math:`k` partition will contain about :math:`M/k` of the variants. Since each partition has some computational overhead, decreasing the number of partitions can improve performance after significant filtering. Since it's recommended to have at least 2 - 4 partitions per core, increasing the number of partitions can allow one to take advantage of more cores. Partitions are a core concept of distributed computation in Spark, see `here <http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds>`__ for details. With ``shuffle=True``, Hail does a full shuffle of the data and creates equal sized partitions. With ``shuffle=False``, Hail combines existing partitions to avoid a full shuffle. These algorithms correspond to the ``repartition`` and ``coalesce`` commands in Spark, respectively. In particular, when ``shuffle=False``, ``num_partitions`` cannot exceed current number of partitions. :param int num_partitions: Desired number of partitions, must be less than the current number if ``shuffle=False``. :param bool shuffle: If true, use full shuffle to repartition. :return: Variant dataset with the number of partitions equal to at most ``num_partitions``; :rtype: :class:`.VariantDataset`; """""". jvds = self._jvdf.coalesce(num_partitions, shuffle); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(max_partitions=integral); def naive_coalesce(self, max_partitions):; """"""Naively descrease the number of",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:193468,Usability,simpl,simply,193468,"il does a full shuffle of the data and creates equal sized partitions. With ``shuffle=False``, Hail combines existing partitions to avoid a full shuffle. These algorithms correspond to the ``repartition`` and ``coalesce`` commands in Spark, respectively. In particular, when ``shuffle=False``, ``num_partitions`` cannot exceed current number of partitions. :param int num_partitions: Desired number of partitions, must be less than the current number if ``shuffle=False``. :param bool shuffle: If true, use full shuffle to repartition. :return: Variant dataset with the number of partitions equal to at most ``num_partitions``; :rtype: :class:`.VariantDataset`; """""". jvds = self._jvdf.coalesce(num_partitions, shuffle); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(max_partitions=integral); def naive_coalesce(self, max_partitions):; """"""Naively descrease the number of partitions. .. warning ::. :py:meth:`~hail.VariantDataset.naive_coalesce` simply combines adjacent partitions to achieve the desired number. It does not attempt to rebalance, unlike :py:meth:`~hail.VariantDataset.repartition`, so it can produce a heavily unbalanced dataset. An unbalanced dataset can be inefficient to operate on because the work is not evenly distributed across partitions. :param int max_partitions: Desired number of partitions. If the current number of partitions is less than ``max_partitions``, do nothing. :return: Variant dataset with the number of partitions equal to at most ``max_partitions``; :rtype: :class:`.VariantDataset`; """""". jvds = self._jvds.naiveCoalesce(max_partitions); return VariantDataset(self.hc, jvds); ; [docs] @handle_py4j; @typecheck_method(force_block=bool,; force_gramian=bool); def rrm(self, force_block=False, force_gramian=False):; """"""Computes the Realized Relationship Matrix (RRM). **Examples**. >>> kinship_matrix = vds.rrm(). **Notes**. The Realized Relationship Matrix is defined as follows. Consider the :math:`n \\times m` matrix :math:`C`",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:195440,Usability,simpl,simply,195440,"d Relationship Matrix (RRM). **Examples**. >>> kinship_matrix = vds.rrm(). **Notes**. The Realized Relationship Matrix is defined as follows. Consider the :math:`n \\times m` matrix :math:`C` of raw genotypes, with rows indexed by :math:`n` samples and; columns indexed by the :math:`m` bialellic autosomal variants; :math:`C_{ij}` is the number of alternate alleles of variant :math:`j` carried by sample :math:`i`, which; can be 0, 1, 2, or missing. For each variant :math:`j`, the sample alternate allele frequency :math:`p_j` is computed as half the mean of the non-missing entries of column; :math:`j`. Entries of :math:`M` are then mean-centered and variance-normalized as. .. math::. M_{ij} = \\frac{C_{ij}-2p_j}{\sqrt{\\frac{m}{n} \sum_{k=1}^n (C_{ij}-2p_j)^2}},. with :math:`M_{ij} = 0` for :math:`C_{ij}` missing (i.e. mean genotype imputation). This scaling normalizes each variant column to have empirical variance :math:`1/m`, which gives each sample row approximately unit total variance (assuming linkage equilibrium) and yields the :math:`n \\times n` sample correlation or realized relationship matrix (RRM) :math:`K` as simply. .. math::. K = MM^T. Note that the only difference between the Realized Relationship Matrix and the Genetic Relationship Matrix (GRM) used in :py:meth:`~hail.VariantDataset.grm` is the variant (column) normalization: where RRM uses empirical variance, GRM uses expected variance under Hardy-Weinberg Equilibrium. :param bool force_block: Force using Spark's BlockMatrix to compute kinship (advanced). :param bool force_gramian: Force using Spark's RowMatrix.computeGramian to compute kinship (advanced). :return: Realized Relationship Matrix for all samples.; :rtype: :py:class:`KinshipMatrix`; """"""; return KinshipMatrix(self._jvdf.rrm(force_block, force_gramian)). [docs] @handle_py4j; @typecheck_method(other=vds_type,; tolerance=numeric); def same(self, other, tolerance=1e-6):; """"""True if the two variant datasets have the same variants, samples, gen",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:201572,Usability,guid,guide,201572,"----------------+--------+----------------------------------------------------------+; | ``gqStDev`` | Double | Genotype quality standard deviation across all genotypes |; +---------------------------+--------+----------------------------------------------------------+. Missing values ``NA`` may result (for example, due to division by zero) and are handled properly in filtering and written as ""NA"" in export modules. The empirical standard deviation is computed with zero degrees of freedom. :param str root: Sample annotation root for the computed struct.; :param bool keep_star: Count star alleles as non-reference alleles; ; :return: Annotated variant dataset with new sample qc annotations.; :rtype: :class:`.VariantDataset`; """""". return VariantDataset(self.hc, self._jvdf.sampleQC(root, keep_star)). [docs] @handle_py4j; def storage_level(self):; """"""Returns the storage (persistence) level of the variant dataset. **Notes**. See the `Spark documentation <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__ for details on persistence levels. :rtype: str; """""". return self._jvds.storageLevel(). [docs] @handle_py4j; @requireTGenotype; def summarize(self):; """"""Returns a summary of useful information about the dataset.; ; .. include:: requireTGenotype.rst. ; **Examples**; ; >>> s = vds.summarize(); >>> print(s.contigs); >>> print('call rate is %.2f' % s.call_rate); >>> s.report(); ; The following information is contained in the summary:; ; - **samples** (*int*) - Number of samples.; - **variants** (*int*) - Number of variants.; - **call_rate** (*float*) - Fraction of all genotypes called.; - **contigs** (*list of str*) - List of all unique contigs found in the dataset.; - **multiallelics** (*int*) - Number of multiallelic variants.; - **snps** (*int*) - Number of SNP alternate alleles.; - **mnps** (*int*) - Number of MNP alternate alleles.; - **insertions** (*int*) - Number of insertion alternate alleles.; - **deletions** (*int*) - Number of deletions ",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:204222,Usability,simpl,simplicity,204222," that can be attached to a variant annotation field. The following attributes are read from the VCF header when importing a VCF and written; to the VCF header when exporting a VCF:. - INFO fields attributes (attached to (`va.info.*`)):. - 'Number': The arity of the field. Can take values. - `0` (Boolean flag),; - `1` (single value),; - `R` (one value per allele, including the reference),; - `A` (one value per non-reference allele),; - `G` (one value per genotype), and; - `.` (any number of values). - When importing: The value in read from the VCF INFO field definition; - When exporting: The default value is `0` for **Boolean**, `.` for **Arrays** and 1 for all other types. - 'Description' (default is ''). - FILTER entries in the VCF header are generated based on the attributes; of `va.filters`. Each key/value pair in the attributes will generate; a FILTER entry in the VCF with ID = key and Description = value. **Examples**. Consider the following command which adds a filter and an annotation to the VDS (we're assuming a split VDS for simplicity):. 1) an INFO field `AC_HC`, which stores the allele count of high; confidence genotypes (DP >= 10, GQ >= 20) for each non-reference allele,. 2) a filter `HardFilter` that filters all sites with the `GATK suggested hard filters <http://gatkforums.broadinstitute.org/gatk/discussion/2806/howto-apply-hard-filters-to-a-call-set>`__:. - For SNVs: QD < 2.0 || FS < 60 || MQ < 40 || MQRankSum < -12.5 || ReadPosRankSum < -8.0. - For Indels (and other complex): QD < 2.0 || FS < 200.0 || ReadPosRankSum < 20.0. >>> annotated_vds = vds.annotate_variants_expr([; ... 'va.info.AC_HC = gs.filter(g => g.dp >= 10 && g.gq >= 20).callStats(g => v).AC[1:]',; ... 'va.filters = if((v.altAllele.isSNP && (va.info.QD < 2.0 || va.info.FS < 60 || va.info.MQ < 40 || ' +; ... 'va.info.MQRankSum < -12.5 || va.info.ReadPosRankSum < -8.0)) || ' +; ... '(va.info.QD < 2.0 || va.info.FS < 200.0 || va.info.ReadPosRankSum < 20.0)) va.filters.add(""HardFilter"") else",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:210023,Usability,simpl,simply,210023,"ps to 0/0; and 0/1. The genotype 1/2 maps to 0/1 and 0/1. The biallelic alt AD entry is just the multiallelic AD entry; corresponding to the alternate allele. The ref AD entry is the; sum of the other multiallelic entries. The biallelic DP is the same as the multiallelic DP. The biallelic PL entry for for a genotype g is the minimum; over PL entries for multiallelic genotypes that downcode to; g. For example, the PL for (A, T) at 0/1 is the minimum of the; PLs for 0/1 (50) and 1/2 (45), and thus 45. Fixing an alternate allele and biallelic variant, downcoding; gives a map from multiallelic to biallelic alleles and; genotypes. The biallelic AD entry for an allele is just the; sum of the multiallelic AD entries for alleles that map to; that allele. Similarly, the biallelic PL entry for a genotype; is the minimum over multiallelic PL entries for genotypes that; map to that genotype. By default, GQ is recomputed from PL. If ``propagate_gq=True``; is passed, the biallelic GQ field is simply the multiallelic; GQ field, that is, genotype qualities are unchanged. Here is a second example for a het non-ref. .. code-block:: text. A C,T 1/2:2,8,6:16:45:99,50,99,45,0,99. splits as. .. code-block:: text. A C 0/1:8,8:16:45:45,0,99; A T 0/1:10,6:16:50:50,0,99. **VCF Info Fields**. Hail does not split annotations in the info field. This means; that if a multiallelic site with ``info.AC`` value ``[10, 2]`` is; split, each split site will contain the same array ``[10,; 2]``. The provided allele index annotation ``va.aIndex`` can be used; to select the value corresponding to the split allele's; position:. >>> vds_result = (vds.split_multi(); ... .filter_variants_expr('va.info.AC[va.aIndex - 1] < 10', keep = False)). VCFs split by Hail and exported to new VCFs may be; incompatible with other tools, if action is not taken; first. Since the ""Number"" of the arrays in split multiallelic; sites no longer matches the structure on import (""A"" for 1 per; allele, for example), Hail will export t",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/expr.html:20131,Deployability,patch,patch,20131,"; return annotation. def _typecheck(self, annotation):; if annotation and not isinstance(annotation, Locus):; raise TypeCheckError('TLocus expected type hail.representation.Locus, but found %s' %; type(annotation)). [docs]class TInterval(Type):; """"""; Hail type corresponding to :class:`hail.representation.Interval`. .. include:: hailType.rst. - `expression language documentation <types.html#interval>`__; - in Python, values are instances of :class:`hail.representation.Interval`. """"""; __metaclass__ = SingletonType. def __init__(self):; super(TInterval, self).__init__(scala_object(Env.hail().expr, 'TInterval')). def _convert_to_py(self, annotation):; if annotation:; return Interval._from_java(annotation); else:; return annotation. def _convert_to_j(self, annotation):; if annotation is not None:; return annotation._jrep; else:; return annotation. def _typecheck(self, annotation):; if annotation and not isinstance(annotation, Interval):; raise TypeCheckError('TInterval expected type hail.representation.Interval, but found %s' %; type(annotation)). __singletons__ = {'is.hail.expr.TInt$': TInt,; 'is.hail.expr.TLong$': TLong,; 'is.hail.expr.TFloat$': TFloat,; 'is.hail.expr.TDouble$': TDouble,; 'is.hail.expr.TBoolean$': TBoolean,; 'is.hail.expr.TString$': TString,; 'is.hail.expr.TVariant$': TVariant,; 'is.hail.expr.TAltAllele$': TAltAllele,; 'is.hail.expr.TLocus$': TLocus,; 'is.hail.expr.TGenotype$': TGenotype,; 'is.hail.expr.TCall$': TCall,; 'is.hail.expr.TInterval$': TInterval}. import pprint. _old_printer = pprint.PrettyPrinter. class TypePrettyPrinter(pprint.PrettyPrinter):; def _format(self, object, stream, indent, allowance, context, level):; if isinstance(object, Type):; stream.write(object.pretty(self._indent_per_level)); else:; return _old_printer._format(self, object, stream, indent, allowance, context, level). pprint.PrettyPrinter = TypePrettyPrinter # monkey-patch pprint. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,docs/0.1/_modules/hail/expr.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/expr.html
https://hail.is/docs/0.1/_modules/hail/expr.html:577,Integrability,message,message,577,"﻿. . hail.expr — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Module code »; hail.expr. Source code for hail.expr; import abc; from hail.java import scala_object, Env, jset; from hail.representation import Variant, AltAllele, Genotype, Locus, Interval, Struct, Call. class TypeCheckError(Exception):; """"""; Error thrown at mismatch between expected and supplied python types. :param str message: Error message; """""". def __init__(self, message):; self.msg = message; super(TypeCheckError).__init__(TypeCheckError). def __str__(self):; return self.msg. [docs]class Type(object):; """"""; Hail type superclass used for annotations and expression language.; """"""; __metaclass__ = abc.ABCMeta. def __init__(self, jtype):; self._jtype = jtype. def __repr__(self):; return str(self). def __str__(self):; return self._jtype.toPrettyString(0, True, False). def __eq__(self, other):; return self._jtype.equals(other._jtype). def __hash__(self):; return self._jtype.hashCode(). [docs] def pretty(self, indent=0, attrs=False):; """"""Returns a prettily formatted string representation of the type. :param int indent: Number of spaces to indent. :param bool attrs: Print struct field attributes. :rtype: str; """""". return self._jtype.toPrettyString(indent, False, attrs). @classmethod; def _from_java(cls, jtype):; # FIXME string matching is pretty hacky; class_name = jtype.getClass().getCanonicalName(). if class_name in __singletons__:; return __singletons__[class_name](); elif class_name == 'is.hail.expr.TArray':; return TArray._from_java(jtype); elif class_name == 'is.hail.expr.TSet':; return TSet._from_java(jtype); elif class_name == 'is.hail.expr.TDict':; return TDict._from_java(jtype); elif class_name == 'is.hail.expr.TStruct':; return TStruct._from_java(jtype); else:; raise TypeError(""unknown type class: '%s'"" % cla",MatchSource.WIKI,docs/0.1/_modules/hail/expr.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/expr.html
https://hail.is/docs/0.1/_modules/hail/expr.html:592,Integrability,message,message,592,"﻿. . hail.expr — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Module code »; hail.expr. Source code for hail.expr; import abc; from hail.java import scala_object, Env, jset; from hail.representation import Variant, AltAllele, Genotype, Locus, Interval, Struct, Call. class TypeCheckError(Exception):; """"""; Error thrown at mismatch between expected and supplied python types. :param str message: Error message; """""". def __init__(self, message):; self.msg = message; super(TypeCheckError).__init__(TypeCheckError). def __str__(self):; return self.msg. [docs]class Type(object):; """"""; Hail type superclass used for annotations and expression language.; """"""; __metaclass__ = abc.ABCMeta. def __init__(self, jtype):; self._jtype = jtype. def __repr__(self):; return str(self). def __str__(self):; return self._jtype.toPrettyString(0, True, False). def __eq__(self, other):; return self._jtype.equals(other._jtype). def __hash__(self):; return self._jtype.hashCode(). [docs] def pretty(self, indent=0, attrs=False):; """"""Returns a prettily formatted string representation of the type. :param int indent: Number of spaces to indent. :param bool attrs: Print struct field attributes. :rtype: str; """""". return self._jtype.toPrettyString(indent, False, attrs). @classmethod; def _from_java(cls, jtype):; # FIXME string matching is pretty hacky; class_name = jtype.getClass().getCanonicalName(). if class_name in __singletons__:; return __singletons__[class_name](); elif class_name == 'is.hail.expr.TArray':; return TArray._from_java(jtype); elif class_name == 'is.hail.expr.TSet':; return TSet._from_java(jtype); elif class_name == 'is.hail.expr.TDict':; return TDict._from_java(jtype); elif class_name == 'is.hail.expr.TStruct':; return TStruct._from_java(jtype); else:; raise TypeError(""unknown type class: '%s'"" % cla",MatchSource.WIKI,docs/0.1/_modules/hail/expr.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/expr.html
https://hail.is/docs/0.1/_modules/hail/expr.html:625,Integrability,message,message,625,"﻿. . hail.expr — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Module code »; hail.expr. Source code for hail.expr; import abc; from hail.java import scala_object, Env, jset; from hail.representation import Variant, AltAllele, Genotype, Locus, Interval, Struct, Call. class TypeCheckError(Exception):; """"""; Error thrown at mismatch between expected and supplied python types. :param str message: Error message; """""". def __init__(self, message):; self.msg = message; super(TypeCheckError).__init__(TypeCheckError). def __str__(self):; return self.msg. [docs]class Type(object):; """"""; Hail type superclass used for annotations and expression language.; """"""; __metaclass__ = abc.ABCMeta. def __init__(self, jtype):; self._jtype = jtype. def __repr__(self):; return str(self). def __str__(self):; return self._jtype.toPrettyString(0, True, False). def __eq__(self, other):; return self._jtype.equals(other._jtype). def __hash__(self):; return self._jtype.hashCode(). [docs] def pretty(self, indent=0, attrs=False):; """"""Returns a prettily formatted string representation of the type. :param int indent: Number of spaces to indent. :param bool attrs: Print struct field attributes. :rtype: str; """""". return self._jtype.toPrettyString(indent, False, attrs). @classmethod; def _from_java(cls, jtype):; # FIXME string matching is pretty hacky; class_name = jtype.getClass().getCanonicalName(). if class_name in __singletons__:; return __singletons__[class_name](); elif class_name == 'is.hail.expr.TArray':; return TArray._from_java(jtype); elif class_name == 'is.hail.expr.TSet':; return TSet._from_java(jtype); elif class_name == 'is.hail.expr.TDict':; return TDict._from_java(jtype); elif class_name == 'is.hail.expr.TStruct':; return TStruct._from_java(jtype); else:; raise TypeError(""unknown type class: '%s'"" % cla",MatchSource.WIKI,docs/0.1/_modules/hail/expr.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/expr.html
https://hail.is/docs/0.1/_modules/hail/expr.html:647,Integrability,message,message,647,"﻿. . hail.expr — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Module code »; hail.expr. Source code for hail.expr; import abc; from hail.java import scala_object, Env, jset; from hail.representation import Variant, AltAllele, Genotype, Locus, Interval, Struct, Call. class TypeCheckError(Exception):; """"""; Error thrown at mismatch between expected and supplied python types. :param str message: Error message; """""". def __init__(self, message):; self.msg = message; super(TypeCheckError).__init__(TypeCheckError). def __str__(self):; return self.msg. [docs]class Type(object):; """"""; Hail type superclass used for annotations and expression language.; """"""; __metaclass__ = abc.ABCMeta. def __init__(self, jtype):; self._jtype = jtype. def __repr__(self):; return str(self). def __str__(self):; return self._jtype.toPrettyString(0, True, False). def __eq__(self, other):; return self._jtype.equals(other._jtype). def __hash__(self):; return self._jtype.hashCode(). [docs] def pretty(self, indent=0, attrs=False):; """"""Returns a prettily formatted string representation of the type. :param int indent: Number of spaces to indent. :param bool attrs: Print struct field attributes. :rtype: str; """""". return self._jtype.toPrettyString(indent, False, attrs). @classmethod; def _from_java(cls, jtype):; # FIXME string matching is pretty hacky; class_name = jtype.getClass().getCanonicalName(). if class_name in __singletons__:; return __singletons__[class_name](); elif class_name == 'is.hail.expr.TArray':; return TArray._from_java(jtype); elif class_name == 'is.hail.expr.TSet':; return TSet._from_java(jtype); elif class_name == 'is.hail.expr.TDict':; return TDict._from_java(jtype); elif class_name == 'is.hail.expr.TStruct':; return TStruct._from_java(jtype); else:; raise TypeError(""unknown type class: '%s'"" % cla",MatchSource.WIKI,docs/0.1/_modules/hail/expr.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/expr.html
https://hail.is/docs/0.1/_modules/hail/expr.html:1141,Security,hash,hashCode,1141," Overview; Tutorials; Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Module code »; hail.expr. Source code for hail.expr; import abc; from hail.java import scala_object, Env, jset; from hail.representation import Variant, AltAllele, Genotype, Locus, Interval, Struct, Call. class TypeCheckError(Exception):; """"""; Error thrown at mismatch between expected and supplied python types. :param str message: Error message; """""". def __init__(self, message):; self.msg = message; super(TypeCheckError).__init__(TypeCheckError). def __str__(self):; return self.msg. [docs]class Type(object):; """"""; Hail type superclass used for annotations and expression language.; """"""; __metaclass__ = abc.ABCMeta. def __init__(self, jtype):; self._jtype = jtype. def __repr__(self):; return str(self). def __str__(self):; return self._jtype.toPrettyString(0, True, False). def __eq__(self, other):; return self._jtype.equals(other._jtype). def __hash__(self):; return self._jtype.hashCode(). [docs] def pretty(self, indent=0, attrs=False):; """"""Returns a prettily formatted string representation of the type. :param int indent: Number of spaces to indent. :param bool attrs: Print struct field attributes. :rtype: str; """""". return self._jtype.toPrettyString(indent, False, attrs). @classmethod; def _from_java(cls, jtype):; # FIXME string matching is pretty hacky; class_name = jtype.getClass().getCanonicalName(). if class_name in __singletons__:; return __singletons__[class_name](); elif class_name == 'is.hail.expr.TArray':; return TArray._from_java(jtype); elif class_name == 'is.hail.expr.TSet':; return TSet._from_java(jtype); elif class_name == 'is.hail.expr.TDict':; return TDict._from_java(jtype); elif class_name == 'is.hail.expr.TStruct':; return TStruct._from_java(jtype); else:; raise TypeError(""unknown type class: '%s'"" % class_name). @abc.abstractmethod; def _typecheck(self, annotation):; """"""; Raise an exception if the given annotation is not the appropriate type. :pa",MatchSource.WIKI,docs/0.1/_modules/hail/expr.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/expr.html
https://hail.is/docs/0.1/_modules/hail/keytable.html:23830,Availability,redundant,redundant,23830,"KT file. ***Examples***. >>> kt1.write('output/kt1.kt'). .. note:: The write path must end in "".kt"". . :param str output: Path of KT file to write. :param bool overwrite: If True, overwrite any existing KT file. Cannot be used ; to read from and write to the same path. """""". self._jkt.write(output, overwrite). [docs] @handle_py4j; def cache(self):; """"""Mark this key table to be cached in memory. :py:meth:`~hail.KeyTable.cache` is the same as :func:`persist(""MEMORY_ONLY"") <hail.KeyTable.persist>`. :rtype: :class:`.KeyTable`. """"""; return KeyTable(self.hc, self._jkt.cache()). [docs] @handle_py4j; @typecheck_method(storage_level=strlike); def persist(self, storage_level=""MEMORY_AND_DISK""):; """"""Persist this key table to memory and/or disk. **Examples**. Persist the key table to both memory and disk:. >>> kt = kt.persist() # doctest: +SKIP. **Notes**. The :py:meth:`~hail.KeyTable.persist` and :py:meth:`~hail.KeyTable.cache` methods ; allow you to store the current table on disk or in memory to avoid redundant computation and ; improve the performance of Hail pipelines. :py:meth:`~hail.KeyTable.cache` is an alias for ; :func:`persist(""MEMORY_ONLY"") <hail.KeyTable.persist>`. Most users will want ""MEMORY_AND_DISK"".; See the `Spark documentation <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__ ; for a more in-depth discussion of persisting data. :param storage_level: Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP; ; :rtype: :class:`.KeyTable`; """""". return KeyTable(self.hc, self._jkt.persist(storage_level)). [docs] @handle_py4j; def unpersist(self):; """"""; Unpersists this table from memory/disk.; ; **Notes**; This function will have no effect on a table that was not previously persisted.; ; There's nothing stopping you from continuing to use a table that has been unpersisted, but doing so w",MatchSource.WIKI,docs/0.1/_modules/hail/keytable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/keytable.html
https://hail.is/docs/0.1/_modules/hail/keytable.html:32218,Availability,error,error,32218,"avily unbalanced dataset. An unbalanced dataset can be inefficient to operate on because the work is not evenly distributed across partitions.; ; :param int n: Desired number of partitions. :param bool shuffle: Whether to shuffle or naively coalesce.; ; :rtype: :class:`.KeyTable` ; """""". return KeyTable(self.hc, self._jkt.repartition(n, shuffle)). [docs] @staticmethod; @handle_py4j; @typecheck(path=strlike,; quantitative=bool,; delimiter=strlike,; missing=strlike); def import_fam(path, quantitative=False, delimiter='\\\\s+', missing='NA'):; """"""Import PLINK .fam file into a key table. **Examples**. Import case-control phenotype data from a tab-separated `PLINK .fam; <https://www.cog-genomics.org/plink2/formats#fam>`_ file into sample; annotations:. >>> fam_kt = KeyTable.import_fam('data/myStudy.fam'). In Hail, unlike PLINK, the user must *explicitly* distinguish between; case-control and quantitative phenotypes. Importing a quantitative; phenotype without ``quantitative=True`` will return an error; (unless all values happen to be ``0``, ``1``, ``2``, and ``-9``):. >>> fam_kt = KeyTable.import_fam('data/myStudy.fam', quantitative=True). **Columns**. The column, types, and missing values are shown below. - **ID** (*String*) -- Sample ID (key column); - **famID** (*String*) -- Family ID (missing = ""0""); - **patID** (*String*) -- Paternal ID (missing = ""0""); - **matID** (*String*) -- Maternal ID (missing = ""0""); - **isFemale** (*Boolean*) -- Sex (missing = ""NA"", ""-9"", ""0""); ; One of:; ; - **isCase** (*Boolean*) -- Case-control phenotype (missing = ""0"", ""-9"", non-numeric or the ``missing`` argument, if given.; - **qPheno** (*Double*) -- Quantitative phenotype (missing = ""NA"" or the ``missing`` argument, if given. :param str path: Path to .fam file. :param bool quantitative: If True, .fam phenotype is interpreted as quantitative. :param str delimiter: .fam file field delimiter regex. :param str missing: The string used to denote missing values.; For case-control, 0, -9, and ",MatchSource.WIKI,docs/0.1/_modules/hail/keytable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/keytable.html
https://hail.is/docs/0.1/_modules/hail/keytable.html:23890,Deployability,pipeline,pipelines,23890,"KT file. ***Examples***. >>> kt1.write('output/kt1.kt'). .. note:: The write path must end in "".kt"". . :param str output: Path of KT file to write. :param bool overwrite: If True, overwrite any existing KT file. Cannot be used ; to read from and write to the same path. """""". self._jkt.write(output, overwrite). [docs] @handle_py4j; def cache(self):; """"""Mark this key table to be cached in memory. :py:meth:`~hail.KeyTable.cache` is the same as :func:`persist(""MEMORY_ONLY"") <hail.KeyTable.persist>`. :rtype: :class:`.KeyTable`. """"""; return KeyTable(self.hc, self._jkt.cache()). [docs] @handle_py4j; @typecheck_method(storage_level=strlike); def persist(self, storage_level=""MEMORY_AND_DISK""):; """"""Persist this key table to memory and/or disk. **Examples**. Persist the key table to both memory and disk:. >>> kt = kt.persist() # doctest: +SKIP. **Notes**. The :py:meth:`~hail.KeyTable.persist` and :py:meth:`~hail.KeyTable.cache` methods ; allow you to store the current table on disk or in memory to avoid redundant computation and ; improve the performance of Hail pipelines. :py:meth:`~hail.KeyTable.cache` is an alias for ; :func:`persist(""MEMORY_ONLY"") <hail.KeyTable.persist>`. Most users will want ""MEMORY_AND_DISK"".; See the `Spark documentation <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__ ; for a more in-depth discussion of persisting data. :param storage_level: Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP; ; :rtype: :class:`.KeyTable`; """""". return KeyTable(self.hc, self._jkt.persist(storage_level)). [docs] @handle_py4j; def unpersist(self):; """"""; Unpersists this table from memory/disk.; ; **Notes**; This function will have no effect on a table that was not previously persisted.; ; There's nothing stopping you from continuing to use a table that has been unpersisted, but doing so w",MatchSource.WIKI,docs/0.1/_modules/hail/keytable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/keytable.html
https://hail.is/docs/0.1/_modules/hail/keytable.html:17144,Modifiability,config,config,17144,"DB. .. warning::. :py:meth:`~.export_mongodb` is EXPERIMENTAL. """""". (scala_package_object(self.hc._hail.driver); .exportMongoDB(self.hc._jsql_context, self._jkt, mode)). [docs] @handle_py4j; @typecheck_method(zk_host=strlike,; collection=strlike,; block_size=integral); def export_solr(self, zk_host, collection, block_size=100):; """"""Export to Solr.; ; .. warning::. :py:meth:`~.export_solr` is EXPERIMENTAL. """""". self._jkt.exportSolr(zk_host, collection, block_size). [docs] @handle_py4j; @typecheck_method(address=strlike,; keyspace=strlike,; table=strlike,; block_size=integral,; rate=integral); def export_cassandra(self, address, keyspace, table, block_size=100, rate=1000):; """"""Export to Cassandra. .. warning::. :py:meth:`~.export_cassandra` is EXPERIMENTAL. """""". self._jkt.exportCassandra(address, keyspace, table, block_size, rate). [docs] @handle_py4j; @typecheck_method(host=strlike,; port=integral,; index=strlike,; index_type=strlike,; block_size=integral,; config=nullable(dictof(strlike, strlike)),; verbose=bool); def export_elasticsearch(self, host, port, index, index_type, block_size, config=None, verbose=True):; """"""Export to Elasticsearch. .. warning::. :py:meth:`~.export_elasticsearch` is EXPERIMENTAL. """""". self._jkt.exportElasticsearch(host, port, index, index_type, block_size, config, verbose). [docs] @handle_py4j; @typecheck_method(column_names=oneof(strlike, listof(strlike))); def explode(self, column_names):; """"""Explode columns of this key table. The explode operation unpacks the elements in a column of type ``Array`` or ``Set`` into its own row.; If an empty ``Array`` or ``Set`` is exploded, the entire row is removed from the :py:class:`.KeyTable`. **Examples**. Assume ``kt3`` is a :py:class:`.KeyTable` with three columns: c1, c2 and; c3. >>> kt3 = hc.import_table('data/kt_example3.tsv', impute=True,; ... types={'c1': TString(), 'c2': TArray(TInt()), 'c3': TArray(TArray(TInt()))}). The types of each column are ``String``, ``Array[Int]``, and ``Array[Array[I",MatchSource.WIKI,docs/0.1/_modules/hail/keytable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/keytable.html
https://hail.is/docs/0.1/_modules/hail/keytable.html:17277,Modifiability,config,config,17277,"DB. .. warning::. :py:meth:`~.export_mongodb` is EXPERIMENTAL. """""". (scala_package_object(self.hc._hail.driver); .exportMongoDB(self.hc._jsql_context, self._jkt, mode)). [docs] @handle_py4j; @typecheck_method(zk_host=strlike,; collection=strlike,; block_size=integral); def export_solr(self, zk_host, collection, block_size=100):; """"""Export to Solr.; ; .. warning::. :py:meth:`~.export_solr` is EXPERIMENTAL. """""". self._jkt.exportSolr(zk_host, collection, block_size). [docs] @handle_py4j; @typecheck_method(address=strlike,; keyspace=strlike,; table=strlike,; block_size=integral,; rate=integral); def export_cassandra(self, address, keyspace, table, block_size=100, rate=1000):; """"""Export to Cassandra. .. warning::. :py:meth:`~.export_cassandra` is EXPERIMENTAL. """""". self._jkt.exportCassandra(address, keyspace, table, block_size, rate). [docs] @handle_py4j; @typecheck_method(host=strlike,; port=integral,; index=strlike,; index_type=strlike,; block_size=integral,; config=nullable(dictof(strlike, strlike)),; verbose=bool); def export_elasticsearch(self, host, port, index, index_type, block_size, config=None, verbose=True):; """"""Export to Elasticsearch. .. warning::. :py:meth:`~.export_elasticsearch` is EXPERIMENTAL. """""". self._jkt.exportElasticsearch(host, port, index, index_type, block_size, config, verbose). [docs] @handle_py4j; @typecheck_method(column_names=oneof(strlike, listof(strlike))); def explode(self, column_names):; """"""Explode columns of this key table. The explode operation unpacks the elements in a column of type ``Array`` or ``Set`` into its own row.; If an empty ``Array`` or ``Set`` is exploded, the entire row is removed from the :py:class:`.KeyTable`. **Examples**. Assume ``kt3`` is a :py:class:`.KeyTable` with three columns: c1, c2 and; c3. >>> kt3 = hc.import_table('data/kt_example3.tsv', impute=True,; ... types={'c1': TString(), 'c2': TArray(TInt()), 'c3': TArray(TArray(TInt()))}). The types of each column are ``String``, ``Array[Int]``, and ``Array[Array[I",MatchSource.WIKI,docs/0.1/_modules/hail/keytable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/keytable.html
https://hail.is/docs/0.1/_modules/hail/keytable.html:17477,Modifiability,config,config,17477,"solr(self, zk_host, collection, block_size=100):; """"""Export to Solr.; ; .. warning::. :py:meth:`~.export_solr` is EXPERIMENTAL. """""". self._jkt.exportSolr(zk_host, collection, block_size). [docs] @handle_py4j; @typecheck_method(address=strlike,; keyspace=strlike,; table=strlike,; block_size=integral,; rate=integral); def export_cassandra(self, address, keyspace, table, block_size=100, rate=1000):; """"""Export to Cassandra. .. warning::. :py:meth:`~.export_cassandra` is EXPERIMENTAL. """""". self._jkt.exportCassandra(address, keyspace, table, block_size, rate). [docs] @handle_py4j; @typecheck_method(host=strlike,; port=integral,; index=strlike,; index_type=strlike,; block_size=integral,; config=nullable(dictof(strlike, strlike)),; verbose=bool); def export_elasticsearch(self, host, port, index, index_type, block_size, config=None, verbose=True):; """"""Export to Elasticsearch. .. warning::. :py:meth:`~.export_elasticsearch` is EXPERIMENTAL. """""". self._jkt.exportElasticsearch(host, port, index, index_type, block_size, config, verbose). [docs] @handle_py4j; @typecheck_method(column_names=oneof(strlike, listof(strlike))); def explode(self, column_names):; """"""Explode columns of this key table. The explode operation unpacks the elements in a column of type ``Array`` or ``Set`` into its own row.; If an empty ``Array`` or ``Set`` is exploded, the entire row is removed from the :py:class:`.KeyTable`. **Examples**. Assume ``kt3`` is a :py:class:`.KeyTable` with three columns: c1, c2 and; c3. >>> kt3 = hc.import_table('data/kt_example3.tsv', impute=True,; ... types={'c1': TString(), 'c2': TArray(TInt()), 'c3': TArray(TArray(TInt()))}). The types of each column are ``String``, ``Array[Int]``, and ``Array[Array[Int]]`` respectively.; c1 cannot be exploded because its type is not an ``Array`` or ``Set``.; c2 can only be exploded once because the type of c2 after the first explode operation is ``Int``. +----+----------+----------------+; | c1 | c2 | c3 |; +====+==========+================+",MatchSource.WIKI,docs/0.1/_modules/hail/keytable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/keytable.html
https://hail.is/docs/0.1/_modules/hail/keytable.html:23159,Performance,cache,cache,23159,"= {row.ID : row.SEX for row in kt1.collect()}. **Notes**. This method should be used on very small tables and as a last resort.; It is very slow to convert distributed Java objects to Python; (especially serially), and the resulting list may be too large; to fit in memory on one machine. :rtype: list of :py:class:`.hail.representation.Struct`; """""". return TArray(self.schema)._convert_to_py(self._jkt.collect()). @handle_py4j; def _typecheck(self):; """"""Check if all values with the schema."""""". self._jkt.typeCheck(). [docs] @handle_py4j; @typecheck_method(output=strlike,; overwrite=bool); def write(self, output, overwrite=False):; """"""Write as KT file. ***Examples***. >>> kt1.write('output/kt1.kt'). .. note:: The write path must end in "".kt"". . :param str output: Path of KT file to write. :param bool overwrite: If True, overwrite any existing KT file. Cannot be used ; to read from and write to the same path. """""". self._jkt.write(output, overwrite). [docs] @handle_py4j; def cache(self):; """"""Mark this key table to be cached in memory. :py:meth:`~hail.KeyTable.cache` is the same as :func:`persist(""MEMORY_ONLY"") <hail.KeyTable.persist>`. :rtype: :class:`.KeyTable`. """"""; return KeyTable(self.hc, self._jkt.cache()). [docs] @handle_py4j; @typecheck_method(storage_level=strlike); def persist(self, storage_level=""MEMORY_AND_DISK""):; """"""Persist this key table to memory and/or disk. **Examples**. Persist the key table to both memory and disk:. >>> kt = kt.persist() # doctest: +SKIP. **Notes**. The :py:meth:`~hail.KeyTable.persist` and :py:meth:`~hail.KeyTable.cache` methods ; allow you to store the current table on disk or in memory to avoid redundant computation and ; improve the performance of Hail pipelines. :py:meth:`~hail.KeyTable.cache` is an alias for ; :func:`persist(""MEMORY_ONLY"") <hail.KeyTable.persist>`. Most users will want ""MEMORY_AND_DISK"".; See the `Spark documentation <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__ ; for a more in-dep",MatchSource.WIKI,docs/0.1/_modules/hail/keytable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/keytable.html
https://hail.is/docs/0.1/_modules/hail/keytable.html:23202,Performance,cache,cached,23202,"= {row.ID : row.SEX for row in kt1.collect()}. **Notes**. This method should be used on very small tables and as a last resort.; It is very slow to convert distributed Java objects to Python; (especially serially), and the resulting list may be too large; to fit in memory on one machine. :rtype: list of :py:class:`.hail.representation.Struct`; """""". return TArray(self.schema)._convert_to_py(self._jkt.collect()). @handle_py4j; def _typecheck(self):; """"""Check if all values with the schema."""""". self._jkt.typeCheck(). [docs] @handle_py4j; @typecheck_method(output=strlike,; overwrite=bool); def write(self, output, overwrite=False):; """"""Write as KT file. ***Examples***. >>> kt1.write('output/kt1.kt'). .. note:: The write path must end in "".kt"". . :param str output: Path of KT file to write. :param bool overwrite: If True, overwrite any existing KT file. Cannot be used ; to read from and write to the same path. """""". self._jkt.write(output, overwrite). [docs] @handle_py4j; def cache(self):; """"""Mark this key table to be cached in memory. :py:meth:`~hail.KeyTable.cache` is the same as :func:`persist(""MEMORY_ONLY"") <hail.KeyTable.persist>`. :rtype: :class:`.KeyTable`. """"""; return KeyTable(self.hc, self._jkt.cache()). [docs] @handle_py4j; @typecheck_method(storage_level=strlike); def persist(self, storage_level=""MEMORY_AND_DISK""):; """"""Persist this key table to memory and/or disk. **Examples**. Persist the key table to both memory and disk:. >>> kt = kt.persist() # doctest: +SKIP. **Notes**. The :py:meth:`~hail.KeyTable.persist` and :py:meth:`~hail.KeyTable.cache` methods ; allow you to store the current table on disk or in memory to avoid redundant computation and ; improve the performance of Hail pipelines. :py:meth:`~hail.KeyTable.cache` is an alias for ; :func:`persist(""MEMORY_ONLY"") <hail.KeyTable.persist>`. Most users will want ""MEMORY_AND_DISK"".; See the `Spark documentation <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__ ; for a more in-dep",MatchSource.WIKI,docs/0.1/_modules/hail/keytable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/keytable.html
https://hail.is/docs/0.1/_modules/hail/keytable.html:23245,Performance,cache,cache,23245," tables and as a last resort.; It is very slow to convert distributed Java objects to Python; (especially serially), and the resulting list may be too large; to fit in memory on one machine. :rtype: list of :py:class:`.hail.representation.Struct`; """""". return TArray(self.schema)._convert_to_py(self._jkt.collect()). @handle_py4j; def _typecheck(self):; """"""Check if all values with the schema."""""". self._jkt.typeCheck(). [docs] @handle_py4j; @typecheck_method(output=strlike,; overwrite=bool); def write(self, output, overwrite=False):; """"""Write as KT file. ***Examples***. >>> kt1.write('output/kt1.kt'). .. note:: The write path must end in "".kt"". . :param str output: Path of KT file to write. :param bool overwrite: If True, overwrite any existing KT file. Cannot be used ; to read from and write to the same path. """""". self._jkt.write(output, overwrite). [docs] @handle_py4j; def cache(self):; """"""Mark this key table to be cached in memory. :py:meth:`~hail.KeyTable.cache` is the same as :func:`persist(""MEMORY_ONLY"") <hail.KeyTable.persist>`. :rtype: :class:`.KeyTable`. """"""; return KeyTable(self.hc, self._jkt.cache()). [docs] @handle_py4j; @typecheck_method(storage_level=strlike); def persist(self, storage_level=""MEMORY_AND_DISK""):; """"""Persist this key table to memory and/or disk. **Examples**. Persist the key table to both memory and disk:. >>> kt = kt.persist() # doctest: +SKIP. **Notes**. The :py:meth:`~hail.KeyTable.persist` and :py:meth:`~hail.KeyTable.cache` methods ; allow you to store the current table on disk or in memory to avoid redundant computation and ; improve the performance of Hail pipelines. :py:meth:`~hail.KeyTable.cache` is an alias for ; :func:`persist(""MEMORY_ONLY"") <hail.KeyTable.persist>`. Most users will want ""MEMORY_AND_DISK"".; See the `Spark documentation <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__ ; for a more in-depth discussion of persisting data. :param storage_level: Storage level. One of: NONE, DISK_ONLY,; D",MatchSource.WIKI,docs/0.1/_modules/hail/keytable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/keytable.html
https://hail.is/docs/0.1/_modules/hail/keytable.html:23391,Performance,cache,cache,23391,"the resulting list may be too large; to fit in memory on one machine. :rtype: list of :py:class:`.hail.representation.Struct`; """""". return TArray(self.schema)._convert_to_py(self._jkt.collect()). @handle_py4j; def _typecheck(self):; """"""Check if all values with the schema."""""". self._jkt.typeCheck(). [docs] @handle_py4j; @typecheck_method(output=strlike,; overwrite=bool); def write(self, output, overwrite=False):; """"""Write as KT file. ***Examples***. >>> kt1.write('output/kt1.kt'). .. note:: The write path must end in "".kt"". . :param str output: Path of KT file to write. :param bool overwrite: If True, overwrite any existing KT file. Cannot be used ; to read from and write to the same path. """""". self._jkt.write(output, overwrite). [docs] @handle_py4j; def cache(self):; """"""Mark this key table to be cached in memory. :py:meth:`~hail.KeyTable.cache` is the same as :func:`persist(""MEMORY_ONLY"") <hail.KeyTable.persist>`. :rtype: :class:`.KeyTable`. """"""; return KeyTable(self.hc, self._jkt.cache()). [docs] @handle_py4j; @typecheck_method(storage_level=strlike); def persist(self, storage_level=""MEMORY_AND_DISK""):; """"""Persist this key table to memory and/or disk. **Examples**. Persist the key table to both memory and disk:. >>> kt = kt.persist() # doctest: +SKIP. **Notes**. The :py:meth:`~hail.KeyTable.persist` and :py:meth:`~hail.KeyTable.cache` methods ; allow you to store the current table on disk or in memory to avoid redundant computation and ; improve the performance of Hail pipelines. :py:meth:`~hail.KeyTable.cache` is an alias for ; :func:`persist(""MEMORY_ONLY"") <hail.KeyTable.persist>`. Most users will want ""MEMORY_AND_DISK"".; See the `Spark documentation <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__ ; for a more in-depth discussion of persisting data. :param storage_level: Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_A",MatchSource.WIKI,docs/0.1/_modules/hail/keytable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/keytable.html
https://hail.is/docs/0.1/_modules/hail/keytable.html:23746,Performance,cache,cache,23746,"KT file. ***Examples***. >>> kt1.write('output/kt1.kt'). .. note:: The write path must end in "".kt"". . :param str output: Path of KT file to write. :param bool overwrite: If True, overwrite any existing KT file. Cannot be used ; to read from and write to the same path. """""". self._jkt.write(output, overwrite). [docs] @handle_py4j; def cache(self):; """"""Mark this key table to be cached in memory. :py:meth:`~hail.KeyTable.cache` is the same as :func:`persist(""MEMORY_ONLY"") <hail.KeyTable.persist>`. :rtype: :class:`.KeyTable`. """"""; return KeyTable(self.hc, self._jkt.cache()). [docs] @handle_py4j; @typecheck_method(storage_level=strlike); def persist(self, storage_level=""MEMORY_AND_DISK""):; """"""Persist this key table to memory and/or disk. **Examples**. Persist the key table to both memory and disk:. >>> kt = kt.persist() # doctest: +SKIP. **Notes**. The :py:meth:`~hail.KeyTable.persist` and :py:meth:`~hail.KeyTable.cache` methods ; allow you to store the current table on disk or in memory to avoid redundant computation and ; improve the performance of Hail pipelines. :py:meth:`~hail.KeyTable.cache` is an alias for ; :func:`persist(""MEMORY_ONLY"") <hail.KeyTable.persist>`. Most users will want ""MEMORY_AND_DISK"".; See the `Spark documentation <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__ ; for a more in-depth discussion of persisting data. :param storage_level: Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP; ; :rtype: :class:`.KeyTable`; """""". return KeyTable(self.hc, self._jkt.persist(storage_level)). [docs] @handle_py4j; def unpersist(self):; """"""; Unpersists this table from memory/disk.; ; **Notes**; This function will have no effect on a table that was not previously persisted.; ; There's nothing stopping you from continuing to use a table that has been unpersisted, but doing so w",MatchSource.WIKI,docs/0.1/_modules/hail/keytable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/keytable.html
https://hail.is/docs/0.1/_modules/hail/keytable.html:23870,Performance,perform,performance,23870,"KT file. ***Examples***. >>> kt1.write('output/kt1.kt'). .. note:: The write path must end in "".kt"". . :param str output: Path of KT file to write. :param bool overwrite: If True, overwrite any existing KT file. Cannot be used ; to read from and write to the same path. """""". self._jkt.write(output, overwrite). [docs] @handle_py4j; def cache(self):; """"""Mark this key table to be cached in memory. :py:meth:`~hail.KeyTable.cache` is the same as :func:`persist(""MEMORY_ONLY"") <hail.KeyTable.persist>`. :rtype: :class:`.KeyTable`. """"""; return KeyTable(self.hc, self._jkt.cache()). [docs] @handle_py4j; @typecheck_method(storage_level=strlike); def persist(self, storage_level=""MEMORY_AND_DISK""):; """"""Persist this key table to memory and/or disk. **Examples**. Persist the key table to both memory and disk:. >>> kt = kt.persist() # doctest: +SKIP. **Notes**. The :py:meth:`~hail.KeyTable.persist` and :py:meth:`~hail.KeyTable.cache` methods ; allow you to store the current table on disk or in memory to avoid redundant computation and ; improve the performance of Hail pipelines. :py:meth:`~hail.KeyTable.cache` is an alias for ; :func:`persist(""MEMORY_ONLY"") <hail.KeyTable.persist>`. Most users will want ""MEMORY_AND_DISK"".; See the `Spark documentation <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__ ; for a more in-depth discussion of persisting data. :param storage_level: Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP; ; :rtype: :class:`.KeyTable`; """""". return KeyTable(self.hc, self._jkt.persist(storage_level)). [docs] @handle_py4j; def unpersist(self):; """"""; Unpersists this table from memory/disk.; ; **Notes**; This function will have no effect on a table that was not previously persisted.; ; There's nothing stopping you from continuing to use a table that has been unpersisted, but doing so w",MatchSource.WIKI,docs/0.1/_modules/hail/keytable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/keytable.html
https://hail.is/docs/0.1/_modules/hail/keytable.html:23926,Performance,cache,cache,23926,"file to write. :param bool overwrite: If True, overwrite any existing KT file. Cannot be used ; to read from and write to the same path. """""". self._jkt.write(output, overwrite). [docs] @handle_py4j; def cache(self):; """"""Mark this key table to be cached in memory. :py:meth:`~hail.KeyTable.cache` is the same as :func:`persist(""MEMORY_ONLY"") <hail.KeyTable.persist>`. :rtype: :class:`.KeyTable`. """"""; return KeyTable(self.hc, self._jkt.cache()). [docs] @handle_py4j; @typecheck_method(storage_level=strlike); def persist(self, storage_level=""MEMORY_AND_DISK""):; """"""Persist this key table to memory and/or disk. **Examples**. Persist the key table to both memory and disk:. >>> kt = kt.persist() # doctest: +SKIP. **Notes**. The :py:meth:`~hail.KeyTable.persist` and :py:meth:`~hail.KeyTable.cache` methods ; allow you to store the current table on disk or in memory to avoid redundant computation and ; improve the performance of Hail pipelines. :py:meth:`~hail.KeyTable.cache` is an alias for ; :func:`persist(""MEMORY_ONLY"") <hail.KeyTable.persist>`. Most users will want ""MEMORY_AND_DISK"".; See the `Spark documentation <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__ ; for a more in-depth discussion of persisting data. :param storage_level: Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP; ; :rtype: :class:`.KeyTable`; """""". return KeyTable(self.hc, self._jkt.persist(storage_level)). [docs] @handle_py4j; def unpersist(self):; """"""; Unpersists this table from memory/disk.; ; **Notes**; This function will have no effect on a table that was not previously persisted.; ; There's nothing stopping you from continuing to use a table that has been unpersisted, but doing so will result in; all previous steps taken to compute the table being performed again since the table must be recomputed. Only unpersist;",MatchSource.WIKI,docs/0.1/_modules/hail/keytable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/keytable.html
https://hail.is/docs/0.1/_modules/hail/keytable.html:24890,Performance,perform,performed,24890," redundant computation and ; improve the performance of Hail pipelines. :py:meth:`~hail.KeyTable.cache` is an alias for ; :func:`persist(""MEMORY_ONLY"") <hail.KeyTable.persist>`. Most users will want ""MEMORY_AND_DISK"".; See the `Spark documentation <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__ ; for a more in-depth discussion of persisting data. :param storage_level: Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP; ; :rtype: :class:`.KeyTable`; """""". return KeyTable(self.hc, self._jkt.persist(storage_level)). [docs] @handle_py4j; def unpersist(self):; """"""; Unpersists this table from memory/disk.; ; **Notes**; This function will have no effect on a table that was not previously persisted.; ; There's nothing stopping you from continuing to use a table that has been unpersisted, but doing so will result in; all previous steps taken to compute the table being performed again since the table must be recomputed. Only unpersist; a table when you are done with it.; """"""; self._jkt.unpersist(). [docs] @handle_py4j; @typecheck_method(cols=tupleof(oneof(strlike, Ascending, Descending))); def order_by(self, *cols):; """"""Sort by the specified columns. Missing values are sorted after non-missing values. Sort by the first column, then the second, etc. :param cols: Columns to sort by.; :type: str or asc(str) or desc(str). :return: Key table sorted by ``cols``.; :rtype: :class:`.KeyTable`; """""". jsort_columns = [asc(col)._jrep if isinstance(col, str) else col._jrep for col in cols]; return KeyTable(self.hc,; self._jkt.orderBy(jarray(Env.hail().keytable.SortColumn, jsort_columns))). [docs] @handle_py4j; def num_partitions(self):; """"""Returns the number of partitions in the key table.; ; :rtype: int; """"""; return self._jkt.nPartitions(). [docs] @staticmethod; @handle_py4j; @typecheck(path=strlike); de",MatchSource.WIKI,docs/0.1/_modules/hail/keytable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/keytable.html
https://hail.is/docs/0.1/_modules/hail/keytable.html:23824,Safety,avoid,avoid,23824,"KT file. ***Examples***. >>> kt1.write('output/kt1.kt'). .. note:: The write path must end in "".kt"". . :param str output: Path of KT file to write. :param bool overwrite: If True, overwrite any existing KT file. Cannot be used ; to read from and write to the same path. """""". self._jkt.write(output, overwrite). [docs] @handle_py4j; def cache(self):; """"""Mark this key table to be cached in memory. :py:meth:`~hail.KeyTable.cache` is the same as :func:`persist(""MEMORY_ONLY"") <hail.KeyTable.persist>`. :rtype: :class:`.KeyTable`. """"""; return KeyTable(self.hc, self._jkt.cache()). [docs] @handle_py4j; @typecheck_method(storage_level=strlike); def persist(self, storage_level=""MEMORY_AND_DISK""):; """"""Persist this key table to memory and/or disk. **Examples**. Persist the key table to both memory and disk:. >>> kt = kt.persist() # doctest: +SKIP. **Notes**. The :py:meth:`~hail.KeyTable.persist` and :py:meth:`~hail.KeyTable.cache` methods ; allow you to store the current table on disk or in memory to avoid redundant computation and ; improve the performance of Hail pipelines. :py:meth:`~hail.KeyTable.cache` is an alias for ; :func:`persist(""MEMORY_ONLY"") <hail.KeyTable.persist>`. Most users will want ""MEMORY_AND_DISK"".; See the `Spark documentation <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__ ; for a more in-depth discussion of persisting data. :param storage_level: Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP; ; :rtype: :class:`.KeyTable`; """""". return KeyTable(self.hc, self._jkt.persist(storage_level)). [docs] @handle_py4j; def unpersist(self):; """"""; Unpersists this table from memory/disk.; ; **Notes**; This function will have no effect on a table that was not previously persisted.; ; There's nothing stopping you from continuing to use a table that has been unpersisted, but doing so w",MatchSource.WIKI,docs/0.1/_modules/hail/keytable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/keytable.html
https://hail.is/docs/0.1/_modules/hail/keytable.html:23830,Safety,redund,redundant,23830,"KT file. ***Examples***. >>> kt1.write('output/kt1.kt'). .. note:: The write path must end in "".kt"". . :param str output: Path of KT file to write. :param bool overwrite: If True, overwrite any existing KT file. Cannot be used ; to read from and write to the same path. """""". self._jkt.write(output, overwrite). [docs] @handle_py4j; def cache(self):; """"""Mark this key table to be cached in memory. :py:meth:`~hail.KeyTable.cache` is the same as :func:`persist(""MEMORY_ONLY"") <hail.KeyTable.persist>`. :rtype: :class:`.KeyTable`. """"""; return KeyTable(self.hc, self._jkt.cache()). [docs] @handle_py4j; @typecheck_method(storage_level=strlike); def persist(self, storage_level=""MEMORY_AND_DISK""):; """"""Persist this key table to memory and/or disk. **Examples**. Persist the key table to both memory and disk:. >>> kt = kt.persist() # doctest: +SKIP. **Notes**. The :py:meth:`~hail.KeyTable.persist` and :py:meth:`~hail.KeyTable.cache` methods ; allow you to store the current table on disk or in memory to avoid redundant computation and ; improve the performance of Hail pipelines. :py:meth:`~hail.KeyTable.cache` is an alias for ; :func:`persist(""MEMORY_ONLY"") <hail.KeyTable.persist>`. Most users will want ""MEMORY_AND_DISK"".; See the `Spark documentation <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__ ; for a more in-depth discussion of persisting data. :param storage_level: Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP; ; :rtype: :class:`.KeyTable`; """""". return KeyTable(self.hc, self._jkt.persist(storage_level)). [docs] @handle_py4j; def unpersist(self):; """"""; Unpersists this table from memory/disk.; ; **Notes**; This function will have no effect on a table that was not previously persisted.; ; There's nothing stopping you from continuing to use a table that has been unpersisted, but doing so w",MatchSource.WIKI,docs/0.1/_modules/hail/keytable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/keytable.html
https://hail.is/docs/0.1/_modules/hail/keytable.html:3546,Testability,assert,assert,3546,"; @handle_py4j; @typecheck(hc=anytype,; rows_py=oneof(listof(Struct), listof(dictof(strlike, anytype))),; schema=TStruct,; key_names=listof(strlike),; num_partitions=nullable(integral)); def from_py(hc, rows_py, schema, key_names=[], num_partitions=None):; return KeyTable(; hc,; Env.hail().keytable.KeyTable.parallelize(; hc._jhc, [schema._convert_to_j(r) for r in rows_py],; schema._jtype, key_names, joption(num_partitions))). @property; @handle_py4j; def num_columns(self):; """"""Number of columns. >>> kt1.num_columns; 8. :rtype: int; """""". if self._num_columns is None:; self._num_columns = self._jkt.nFields(); return self._num_columns. @property; @handle_py4j; def schema(self):; """"""Table schema. **Examples**. >>> print(kt1.schema). The ``pprint`` module can be used to print the schema in a more human-readable format:. >>> from pprint import pprint; >>> pprint(kt1.schema). :rtype: :class:`.TStruct`; """""". if self._schema is None:; self._schema = Type._from_java(self._jkt.signature()); assert (isinstance(self._schema, TStruct)); return self._schema. @property; @handle_py4j; def key(self):; """"""List of key columns. >>> kt1.key; [u'ID']. :rtype: list of str; """""". if self._key is None:; self._key = list(self._jkt.key()); return self._key. @property; @handle_py4j; def columns(self):; """"""Names of all columns. >>> kt1.columns; [u'ID', u'HT', u'SEX', u'X', u'Z', u'C1', u'C2', u'C3']. :rtype: list of str; """""". if self._column_names is None:; self._column_names = list(self._jkt.fieldNames()); return self._column_names. [docs] @handle_py4j; def count(self):; """"""Count the number of rows. **Examples**; ; >>> kt1.count(); ; :rtype: int; """""". return self._jkt.count(). [docs] @handle_py4j; @typecheck_method(other=kt_type); def same(self, other):; """"""Test whether two key tables are identical. **Examples**. >>> if kt1.same(kt2):; ... print(""KeyTables are the same!""). :param other: key table to compare against; :type other: :class:`.KeyTable` . :rtype: bool; """""". return self._jkt.same(other",MatchSource.WIKI,docs/0.1/_modules/hail/keytable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/keytable.html
https://hail.is/docs/0.1/_modules/hail/keytable.html:24126,Usability,guid,guide,24126,"utput, overwrite). [docs] @handle_py4j; def cache(self):; """"""Mark this key table to be cached in memory. :py:meth:`~hail.KeyTable.cache` is the same as :func:`persist(""MEMORY_ONLY"") <hail.KeyTable.persist>`. :rtype: :class:`.KeyTable`. """"""; return KeyTable(self.hc, self._jkt.cache()). [docs] @handle_py4j; @typecheck_method(storage_level=strlike); def persist(self, storage_level=""MEMORY_AND_DISK""):; """"""Persist this key table to memory and/or disk. **Examples**. Persist the key table to both memory and disk:. >>> kt = kt.persist() # doctest: +SKIP. **Notes**. The :py:meth:`~hail.KeyTable.persist` and :py:meth:`~hail.KeyTable.cache` methods ; allow you to store the current table on disk or in memory to avoid redundant computation and ; improve the performance of Hail pipelines. :py:meth:`~hail.KeyTable.cache` is an alias for ; :func:`persist(""MEMORY_ONLY"") <hail.KeyTable.persist>`. Most users will want ""MEMORY_AND_DISK"".; See the `Spark documentation <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__ ; for a more in-depth discussion of persisting data. :param storage_level: Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP; ; :rtype: :class:`.KeyTable`; """""". return KeyTable(self.hc, self._jkt.persist(storage_level)). [docs] @handle_py4j; def unpersist(self):; """"""; Unpersists this table from memory/disk.; ; **Notes**; This function will have no effect on a table that was not previously persisted.; ; There's nothing stopping you from continuing to use a table that has been unpersisted, but doing so will result in; all previous steps taken to compute the table being performed again since the table must be recomputed. Only unpersist; a table when you are done with it.; """"""; self._jkt.unpersist(). [docs] @handle_py4j; @typecheck_method(cols=tupleof(oneof(strlike, Ascending, Descending))); ",MatchSource.WIKI,docs/0.1/_modules/hail/keytable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/keytable.html
https://hail.is/docs/0.1/_modules/hail/keytable.html:31090,Usability,simpl,simply,31090,"me to key table. **Examples**. >>> kt = KeyTable.from_dataframe(df) # doctest: +SKIP. **Notes**. Spark SQL data types are converted to Hail types as follows:. .. code-block:: text. BooleanType => Boolean; IntegerType => Int; LongType => Long; FloatType => Float; DoubleType => Double; StringType => String; BinaryType => Binary; ArrayType => Array; StructType => Struct. Unlisted Spark SQL data types are currently unsupported.; ; :param df: PySpark DataFrame.; :type df: ``DataFrame``; ; :param key: Key column(s).; :type key: str or list of str. :return: Key table constructed from the Spark SQL DataFrame.; :rtype: :class:`.KeyTable`; """""". return KeyTable(Env.hc(), Env.hail().keytable.KeyTable.fromDF(Env.hc()._jhc, df._jdf, wrap_to_list(key))). [docs] @handle_py4j; @typecheck_method(n=integral,; shuffle=bool); def repartition(self, n, shuffle=True):; """"""Change the number of distributed partitions.; ; .. warning ::. When `shuffle` is `False`, `repartition` can only decrease the number of partitions and simply combines adjacent partitions to achieve the desired number. It does not attempt to rebalance and so can produce a heavily unbalanced dataset. An unbalanced dataset can be inefficient to operate on because the work is not evenly distributed across partitions.; ; :param int n: Desired number of partitions. :param bool shuffle: Whether to shuffle or naively coalesce.; ; :rtype: :class:`.KeyTable` ; """""". return KeyTable(self.hc, self._jkt.repartition(n, shuffle)). [docs] @staticmethod; @handle_py4j; @typecheck(path=strlike,; quantitative=bool,; delimiter=strlike,; missing=strlike); def import_fam(path, quantitative=False, delimiter='\\\\s+', missing='NA'):; """"""Import PLINK .fam file into a key table. **Examples**. Import case-control phenotype data from a tab-separated `PLINK .fam; <https://www.cog-genomics.org/plink2/formats#fam>`_ file into sample; annotations:. >>> fam_kt = KeyTable.import_fam('data/myStudy.fam'). In Hail, unlike PLINK, the user must *explicitly* disti",MatchSource.WIKI,docs/0.1/_modules/hail/keytable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/keytable.html
https://hail.is/docs/0.1/_modules/hail/keytable.html:35330,Usability,simpl,simple,35330,"le.; ; **Examples**; ; Take the first ten rows:; ; >>> first10 = kt1.take(10); ; **Notes**; ; This method does not need to look at all the data, and ; allows for fast queries of the start of the table.; ; :param int n: Number of rows to take.; ; :return: Rows from the start of the table.; :rtype: list of :class:`.~hail.representation.Struct`; """""". return [self.schema._convert_to_py(r) for r in self._jkt.take(n)]. [docs] @handle_py4j; @typecheck_method(name=strlike); def indexed(self, name='index'):; """"""Add the numerical index of each row as a new column. **Examples**. >>> ind_kt = kt1.indexed(). **Notes**. This method returns a table with a new column whose name is; given by the ``name`` parameter, with type ``Long``. The value; of this column is the numerical index of each row, starting; from 0. Methods that respect ordering (like :py:meth:`.KeyTable.take`; or :py:meth:`.KeyTable.export` will return rows in order. This method is helpful for creating a unique integer index for rows; of a table, so that more complex types can be encoded as a simple; number. :param str name: Name of index column. :return: Table with a new index column.; :rtype: :class:`.KeyTable`; """""". return KeyTable(self.hc, self._jkt.indexed(name)). [docs] @typecheck_method(n=integral,; truncate_to=nullable(integral),; print_types=bool); def show(self, n=10, truncate_to=None, print_types=True):; """"""Show the first few rows of the table in human-readable format. **Examples**. Show, with default parameters (10 rows, no truncation, and column types):. >>> kt1.show(). Truncate long columns to 25 characters and only write 5 rows:. >>> kt1.show(5, truncate_to=25). **Notes**. If the ``truncate_to`` argument is ``None``, then no truncation will; occur. This is the default behavior. An integer argument will truncate; each cell to the specified number of characters. :param int n: Number of rows to show. :param truncate_to: Truncate columns to the desired number of characters.; :type truncate_to: int or None. ",MatchSource.WIKI,docs/0.1/_modules/hail/keytable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/keytable.html
https://hail.is/docs/0.1/_modules/hail/kinshipMatrix.html:2821,Integrability,depend,depending,2821,"s()]. [docs] def matrix(self):; """"""; Gets the matrix backing this kinship matrix. :return: Matrix of kinship values.; :rtype: `IndexedRowMatrix <https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.distributed.IndexedRowMatrix>`__; """"""; from pyspark.mllib.linalg.distributed import IndexedRowMatrix. return IndexedRowMatrix(self._jkm.matrix()). [docs] @typecheck_method(output=strlike); def export_tsv(self, output):; """"""; Export kinship matrix to tab-delimited text file with sample list as header.; ; :param str output: File path for output. ; """"""; self._jkm.exportTSV(output). [docs] @typecheck_method(output=strlike); def export_rel(self, output):; """"""; Export kinship matrix as .rel file. See `PLINK formats <https://www.cog-genomics.org/plink2/formats>`_.; ; :param str output: File path for output. ; """"""; self._jkm.exportRel(output). [docs] @typecheck_method(output=strlike); def export_gcta_grm(self, output):; """"""; Export kinship matrix as .grm file. See `PLINK formats <https://www.cog-genomics.org/plink2/formats>`_.; ; :param str output: File path for output.; """"""; self._jkm.exportGctaGrm(output). [docs] @typecheck_method(output=strlike,; opt_n_file=nullable(strlike)); def export_gcta_grm_bin(self, output, opt_n_file=None):; """"""; Export kinship matrix as .grm.bin file or as .grm.N.bin file, depending on whether an N file is specified. See `PLINK formats <https://www.cog-genomics.org/plink2/formats>`_.; ; :param str output: File path for output. ; ; :param opt_n_file: The file path to the N file. ; :type opt_n_file: str or None; """"""; self._jkm.exportGctaGrmBin(output, joption(opt_n_file)). [docs] @typecheck_method(output=strlike); def export_id_file(self, output):; """"""; Export samples as .id file. See `PLINK formats <https://www.cog-genomics.org/plink2/formats>`_.; ; :param str output: File path for output.; """"""; self._jkm.exportIdFile(output). © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,docs/0.1/_modules/hail/kinshipMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/kinshipMatrix.html
https://hail.is/docs/0.1/_modules/hail/ldMatrix.html:1792,Testability,assert,assert,1792,"st of variants.; :rtype: list of Variant; """"""; jvars = self._jldm.variants(); return list(map(lambda jrep: Variant._from_java(jrep), jvars)). [docs] def matrix(self):; """"""; Gets the distributed matrix backing this LD matrix. :return: Matrix of Pearson correlation values.; :rtype: `IndexedRowMatrix <https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.distributed.IndexedRowMatrix>`__; """"""; from pyspark.mllib.linalg.distributed import IndexedRowMatrix. return IndexedRowMatrix(self._jldm.matrix()). [docs] def to_local_matrix(self):; """"""; Converts the LD matrix to a local Spark matrix.; ; .. caution::; ; Only call this method when the LD matrix is small enough to fit in local memory on the driver. ; ; :return: Matrix of Pearson correlation values.; :rtype: `Matrix <https://spark.apache.org/docs/2.1.0/api/python/pyspark.mllib.html#pyspark.mllib.linalg.Matrix>`__; """"""; from pyspark.mllib.linalg import DenseMatrix. j_local_mat = self._jldm.toLocalMatrix(); assert j_local_mat.majorStride() == j_local_mat.rows(); assert j_local_mat.offset() == 0; assert j_local_mat.isTranspose() == False; return DenseMatrix(j_local_mat.rows(), j_local_mat.cols(), list(j_local_mat.data()), False). [docs] def write(self, path):; """"""; Writes the LD matrix to a file. **Examples**. Write an LD matrix to a file. >>> vds.ld_matrix().write('output/ld_matrix'). :param path: the path to which to write the LD matrix; :type path: str; """""". self._jldm.write(path). [docs] @staticmethod; def read(path):; """"""; Reads the LD matrix from a file. **Examples**. Read an LD matrix from a file. >>> ld_matrix = LDMatrix.read('data/ld_matrix'). :param path: the path from which to read the LD matrix; :type path: str; """""". jldm = Env.hail().methods.LDMatrix.read(Env.hc()._jhc, path); return LDMatrix(jldm). [docs] @typecheck_method(path=strlike,; column_delimiter=strlike,; header=nullable(strlike),; parallel_write=bool,; entries=enumeration('full', 'lower', 'strict_lower', 'upper', 'st",MatchSource.WIKI,docs/0.1/_modules/hail/ldMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/ldMatrix.html
https://hail.is/docs/0.1/_modules/hail/ldMatrix.html:1848,Testability,assert,assert,1848,"_jldm.variants(); return list(map(lambda jrep: Variant._from_java(jrep), jvars)). [docs] def matrix(self):; """"""; Gets the distributed matrix backing this LD matrix. :return: Matrix of Pearson correlation values.; :rtype: `IndexedRowMatrix <https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.distributed.IndexedRowMatrix>`__; """"""; from pyspark.mllib.linalg.distributed import IndexedRowMatrix. return IndexedRowMatrix(self._jldm.matrix()). [docs] def to_local_matrix(self):; """"""; Converts the LD matrix to a local Spark matrix.; ; .. caution::; ; Only call this method when the LD matrix is small enough to fit in local memory on the driver. ; ; :return: Matrix of Pearson correlation values.; :rtype: `Matrix <https://spark.apache.org/docs/2.1.0/api/python/pyspark.mllib.html#pyspark.mllib.linalg.Matrix>`__; """"""; from pyspark.mllib.linalg import DenseMatrix. j_local_mat = self._jldm.toLocalMatrix(); assert j_local_mat.majorStride() == j_local_mat.rows(); assert j_local_mat.offset() == 0; assert j_local_mat.isTranspose() == False; return DenseMatrix(j_local_mat.rows(), j_local_mat.cols(), list(j_local_mat.data()), False). [docs] def write(self, path):; """"""; Writes the LD matrix to a file. **Examples**. Write an LD matrix to a file. >>> vds.ld_matrix().write('output/ld_matrix'). :param path: the path to which to write the LD matrix; :type path: str; """""". self._jldm.write(path). [docs] @staticmethod; def read(path):; """"""; Reads the LD matrix from a file. **Examples**. Read an LD matrix from a file. >>> ld_matrix = LDMatrix.read('data/ld_matrix'). :param path: the path from which to read the LD matrix; :type path: str; """""". jldm = Env.hail().methods.LDMatrix.read(Env.hc()._jhc, path); return LDMatrix(jldm). [docs] @typecheck_method(path=strlike,; column_delimiter=strlike,; header=nullable(strlike),; parallel_write=bool,; entries=enumeration('full', 'lower', 'strict_lower', 'upper', 'strict_upper')); def export(self, path, column_delimiter, heade",MatchSource.WIKI,docs/0.1/_modules/hail/ldMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/ldMatrix.html
https://hail.is/docs/0.1/_modules/hail/ldMatrix.html:1882,Testability,assert,assert,1882,"ap(lambda jrep: Variant._from_java(jrep), jvars)). [docs] def matrix(self):; """"""; Gets the distributed matrix backing this LD matrix. :return: Matrix of Pearson correlation values.; :rtype: `IndexedRowMatrix <https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.distributed.IndexedRowMatrix>`__; """"""; from pyspark.mllib.linalg.distributed import IndexedRowMatrix. return IndexedRowMatrix(self._jldm.matrix()). [docs] def to_local_matrix(self):; """"""; Converts the LD matrix to a local Spark matrix.; ; .. caution::; ; Only call this method when the LD matrix is small enough to fit in local memory on the driver. ; ; :return: Matrix of Pearson correlation values.; :rtype: `Matrix <https://spark.apache.org/docs/2.1.0/api/python/pyspark.mllib.html#pyspark.mllib.linalg.Matrix>`__; """"""; from pyspark.mllib.linalg import DenseMatrix. j_local_mat = self._jldm.toLocalMatrix(); assert j_local_mat.majorStride() == j_local_mat.rows(); assert j_local_mat.offset() == 0; assert j_local_mat.isTranspose() == False; return DenseMatrix(j_local_mat.rows(), j_local_mat.cols(), list(j_local_mat.data()), False). [docs] def write(self, path):; """"""; Writes the LD matrix to a file. **Examples**. Write an LD matrix to a file. >>> vds.ld_matrix().write('output/ld_matrix'). :param path: the path to which to write the LD matrix; :type path: str; """""". self._jldm.write(path). [docs] @staticmethod; def read(path):; """"""; Reads the LD matrix from a file. **Examples**. Read an LD matrix from a file. >>> ld_matrix = LDMatrix.read('data/ld_matrix'). :param path: the path from which to read the LD matrix; :type path: str; """""". jldm = Env.hail().methods.LDMatrix.read(Env.hc()._jhc, path); return LDMatrix(jldm). [docs] @typecheck_method(path=strlike,; column_delimiter=strlike,; header=nullable(strlike),; parallel_write=bool,; entries=enumeration('full', 'lower', 'strict_lower', 'upper', 'strict_upper')); def export(self, path, column_delimiter, header=None, parallel_write=False, ",MatchSource.WIKI,docs/0.1/_modules/hail/ldMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/ldMatrix.html
https://hail.is/docs/0.1/_modules/hail/utils.html:2317,Usability,clear,clear,2317,"iants(); summary.call_rate = jrep.callRate().get() if jrep.callRate().isDefined() else float('nan'); summary.contigs = [str(x) for x in jiterable_to_list(jrep.contigs())]; summary.multiallelics = jrep.multiallelics(); summary.snps = jrep.snps(); summary.mnps = jrep.mnps(); summary.insertions = jrep.insertions(); summary.deletions = jrep.deletions(); summary.complex = jrep.complex(); summary.star = jrep.star(); summary.max_alleles = jrep.maxAlleles(); return summary. def __repr__(self):; return 'Summary(samples=%d, variants=%d, call_rate=%f, contigs=%s, multiallelics=%d, snps=%d, ' \; 'mnps=%d, insertions=%d, deletions=%d, complex=%d, star=%d, max_alleles=%d)' % (; self.samples, self.variants, self.call_rate,; self.contigs, self.multiallelics, self.snps,; self.mnps, self.insertions, self.deletions,; self.complex, self.star, self.max_alleles). def __str__(self):; return repr(self). [docs] def report(self):; """"""Print the summary information.""""""; print('') # clear out pesky progress bar if necessary; print('%16s: %d' % ('Samples', self.samples)); print('%16s: %d' % ('Variants', self.variants)); print('%16s: %f' % ('Call Rate', self.call_rate)); print('%16s: %s' % ('Contigs', self.contigs)); print('%16s: %d' % ('Multiallelics', self.multiallelics)); print('%16s: %d' % ('SNPs', self.snps)); print('%16s: %d' % ('MNPs', self.mnps)); print('%16s: %d' % ('Insertions', self.insertions)); print('%16s: %d' % ('Deletions', self.deletions)); print('%16s: %d' % ('Complex Alleles', self.complex)); print('%16s: %d' % ('Star Alleles', self.star)); print('%16s: %d' % ('Max Alleles', self.max_alleles)). class FunctionDocumentation(object):; @handle_py4j; def types_rst(self, file_name):; Env.hail().utils.FunctionDocumentation.makeTypesDocs(file_name). @handle_py4j; def functions_rst(self, file_name):; Env.hail().utils.FunctionDocumentation.makeFunctionsDocs(file_name). [docs]@handle_py4j; @typecheck(path=strlike,; buffer_size=integral); def hadoop_read(path, buffer_size=8192):; """"""Open a",MatchSource.WIKI,docs/0.1/_modules/hail/utils.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/utils.html
https://hail.is/docs/0.1/_modules/hail/utils.html:2333,Usability,progress bar,progress bar,2333,"iants(); summary.call_rate = jrep.callRate().get() if jrep.callRate().isDefined() else float('nan'); summary.contigs = [str(x) for x in jiterable_to_list(jrep.contigs())]; summary.multiallelics = jrep.multiallelics(); summary.snps = jrep.snps(); summary.mnps = jrep.mnps(); summary.insertions = jrep.insertions(); summary.deletions = jrep.deletions(); summary.complex = jrep.complex(); summary.star = jrep.star(); summary.max_alleles = jrep.maxAlleles(); return summary. def __repr__(self):; return 'Summary(samples=%d, variants=%d, call_rate=%f, contigs=%s, multiallelics=%d, snps=%d, ' \; 'mnps=%d, insertions=%d, deletions=%d, complex=%d, star=%d, max_alleles=%d)' % (; self.samples, self.variants, self.call_rate,; self.contigs, self.multiallelics, self.snps,; self.mnps, self.insertions, self.deletions,; self.complex, self.star, self.max_alleles). def __str__(self):; return repr(self). [docs] def report(self):; """"""Print the summary information.""""""; print('') # clear out pesky progress bar if necessary; print('%16s: %d' % ('Samples', self.samples)); print('%16s: %d' % ('Variants', self.variants)); print('%16s: %f' % ('Call Rate', self.call_rate)); print('%16s: %s' % ('Contigs', self.contigs)); print('%16s: %d' % ('Multiallelics', self.multiallelics)); print('%16s: %d' % ('SNPs', self.snps)); print('%16s: %d' % ('MNPs', self.mnps)); print('%16s: %d' % ('Insertions', self.insertions)); print('%16s: %d' % ('Deletions', self.deletions)); print('%16s: %d' % ('Complex Alleles', self.complex)); print('%16s: %d' % ('Star Alleles', self.star)); print('%16s: %d' % ('Max Alleles', self.max_alleles)). class FunctionDocumentation(object):; @handle_py4j; def types_rst(self, file_name):; Env.hail().utils.FunctionDocumentation.makeTypesDocs(file_name). @handle_py4j; def functions_rst(self, file_name):; Env.hail().utils.FunctionDocumentation.makeFunctionsDocs(file_name). [docs]@handle_py4j; @typecheck(path=strlike,; buffer_size=integral); def hadoop_read(path, buffer_size=8192):; """"""Open a",MatchSource.WIKI,docs/0.1/_modules/hail/utils.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/utils.html
https://hail.is/docs/0.1/_modules/hail/representation/annotations.html:2737,Deployability,patch,patch,2737,"ll not parse to python because it; begins with an integer, which is not an acceptable leading character; for an identifier. There are two ways to access this field:. >>> getattr(bar, '1kg'); >>> bar['1kg']. The ``pprint`` module can be used to print nested Structs in a more; human-readable fashion:. >>> from pprint import pprint; >>> pprint(bar). :param dict attributes: struct members.; """""". def __init__(self, attributes):. self._attrs = attributes. def __getattr__(self, item):; assert (self._attrs); if item not in self._attrs:; raise AttributeError(""Struct instance has no attribute '%s'"" % item); return self._attrs[item]. def __contains__(self, item):; return item in self._attrs. def __getitem__(self, item):; return self.__getattr__(item). def __len__(self):; return len(self._attrs). def __repr__(self):; return str(self). def __str__(self):; return 'Struct' + str(self._attrs). def __eq__(self, other):; if isinstance(other, Struct):; return self._attrs == other._attrs; else:; return False. def __hash__(self):; return 37 + hash(tuple(sorted(self._attrs.items()))). [docs] @typecheck_method(item=strlike,; default=anytype); def get(self, item, default=None):; """"""Get an item, or return a default value if the item is not found.; ; :param str item: Name of attribute.; ; :param default: Default value.; ; :returns: Value of item if found, or default value if not.; """"""; return self._attrs.get(item, default). @typecheck(struct=Struct); def to_dict(struct):; d = {}; for k, v in struct._attrs.iteritems():; if isinstance(v, Struct):; d[k] = to_dict(v); else:; d[k] = v; return d. import pprint. _old_printer = pprint.PrettyPrinter. class StructPrettyPrinter(pprint.PrettyPrinter):; def _format(self, obj, *args, **kwargs):; if isinstance(obj, Struct):; obj = to_dict(obj); return _old_printer._format(self, obj, *args, **kwargs). pprint.PrettyPrinter = StructPrettyPrinter # monkey-patch pprint. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,docs/0.1/_modules/hail/representation/annotations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/annotations.html
https://hail.is/docs/0.1/_modules/hail/representation/annotations.html:600,Security,access,accessing,600,"﻿. . hail.representation.annotations — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Module code »; hail.representation.annotations. Source code for hail.representation.annotations; from hail.typecheck import *. [docs]class Struct(object):; """"""; Nested annotation structure. >>> bar = Struct({'foo': 5, '1kg': 10}). Struct elements are treated as both 'items' and 'attributes', which; allows either syntax for accessing the element ""foo"" of struct ""bar"":. >>> bar.foo; >>> bar['foo']. Note that it is possible to use Hail to define struct fields inside; of a key table or variant dataset that do not match python syntax.; The name ""1kg"", for example, will not parse to python because it; begins with an integer, which is not an acceptable leading character; for an identifier. There are two ways to access this field:. >>> getattr(bar, '1kg'); >>> bar['1kg']. The ``pprint`` module can be used to print nested Structs in a more; human-readable fashion:. >>> from pprint import pprint; >>> pprint(bar). :param dict attributes: struct members.; """""". def __init__(self, attributes):. self._attrs = attributes. def __getattr__(self, item):; assert (self._attrs); if item not in self._attrs:; raise AttributeError(""Struct instance has no attribute '%s'"" % item); return self._attrs[item]. def __contains__(self, item):; return item in self._attrs. def __getitem__(self, item):; return self.__getattr__(item). def __len__(self):; return len(self._attrs). def __repr__(self):; return str(self). def __str__(self):; return 'Struct' + str(self._attrs). def __eq__(self, other):; if isinstance(other, Struct):; return self._attrs == other._attrs; else:; return False. def __hash__(self):; return 37 + hash(tuple(sorted(self._attrs.items()))). [docs] @typecheck_method(item=strlike,; default=anytype); def get(self, item, ",MatchSource.WIKI,docs/0.1/_modules/hail/representation/annotations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/annotations.html
https://hail.is/docs/0.1/_modules/hail/representation/annotations.html:989,Security,access,access,989,"﻿. . hail.representation.annotations — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Module code »; hail.representation.annotations. Source code for hail.representation.annotations; from hail.typecheck import *. [docs]class Struct(object):; """"""; Nested annotation structure. >>> bar = Struct({'foo': 5, '1kg': 10}). Struct elements are treated as both 'items' and 'attributes', which; allows either syntax for accessing the element ""foo"" of struct ""bar"":. >>> bar.foo; >>> bar['foo']. Note that it is possible to use Hail to define struct fields inside; of a key table or variant dataset that do not match python syntax.; The name ""1kg"", for example, will not parse to python because it; begins with an integer, which is not an acceptable leading character; for an identifier. There are two ways to access this field:. >>> getattr(bar, '1kg'); >>> bar['1kg']. The ``pprint`` module can be used to print nested Structs in a more; human-readable fashion:. >>> from pprint import pprint; >>> pprint(bar). :param dict attributes: struct members.; """""". def __init__(self, attributes):. self._attrs = attributes. def __getattr__(self, item):; assert (self._attrs); if item not in self._attrs:; raise AttributeError(""Struct instance has no attribute '%s'"" % item); return self._attrs[item]. def __contains__(self, item):; return item in self._attrs. def __getitem__(self, item):; return self.__getattr__(item). def __len__(self):; return len(self._attrs). def __repr__(self):; return str(self). def __str__(self):; return 'Struct' + str(self._attrs). def __eq__(self, other):; if isinstance(other, Struct):; return self._attrs == other._attrs; else:; return False. def __hash__(self):; return 37 + hash(tuple(sorted(self._attrs.items()))). [docs] @typecheck_method(item=strlike,; default=anytype); def get(self, item, ",MatchSource.WIKI,docs/0.1/_modules/hail/representation/annotations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/annotations.html
https://hail.is/docs/0.1/_modules/hail/representation/annotations.html:1881,Security,hash,hash,1881,"ll not parse to python because it; begins with an integer, which is not an acceptable leading character; for an identifier. There are two ways to access this field:. >>> getattr(bar, '1kg'); >>> bar['1kg']. The ``pprint`` module can be used to print nested Structs in a more; human-readable fashion:. >>> from pprint import pprint; >>> pprint(bar). :param dict attributes: struct members.; """""". def __init__(self, attributes):. self._attrs = attributes. def __getattr__(self, item):; assert (self._attrs); if item not in self._attrs:; raise AttributeError(""Struct instance has no attribute '%s'"" % item); return self._attrs[item]. def __contains__(self, item):; return item in self._attrs. def __getitem__(self, item):; return self.__getattr__(item). def __len__(self):; return len(self._attrs). def __repr__(self):; return str(self). def __str__(self):; return 'Struct' + str(self._attrs). def __eq__(self, other):; if isinstance(other, Struct):; return self._attrs == other._attrs; else:; return False. def __hash__(self):; return 37 + hash(tuple(sorted(self._attrs.items()))). [docs] @typecheck_method(item=strlike,; default=anytype); def get(self, item, default=None):; """"""Get an item, or return a default value if the item is not found.; ; :param str item: Name of attribute.; ; :param default: Default value.; ; :returns: Value of item if found, or default value if not.; """"""; return self._attrs.get(item, default). @typecheck(struct=Struct); def to_dict(struct):; d = {}; for k, v in struct._attrs.iteritems():; if isinstance(v, Struct):; d[k] = to_dict(v); else:; d[k] = v; return d. import pprint. _old_printer = pprint.PrettyPrinter. class StructPrettyPrinter(pprint.PrettyPrinter):; def _format(self, obj, *args, **kwargs):; if isinstance(obj, Struct):; obj = to_dict(obj); return _old_printer._format(self, obj, *args, **kwargs). pprint.PrettyPrinter = StructPrettyPrinter # monkey-patch pprint. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,docs/0.1/_modules/hail/representation/annotations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/annotations.html
https://hail.is/docs/0.1/_modules/hail/representation/annotations.html:1327,Testability,assert,assert,1327,"ons. Source code for hail.representation.annotations; from hail.typecheck import *. [docs]class Struct(object):; """"""; Nested annotation structure. >>> bar = Struct({'foo': 5, '1kg': 10}). Struct elements are treated as both 'items' and 'attributes', which; allows either syntax for accessing the element ""foo"" of struct ""bar"":. >>> bar.foo; >>> bar['foo']. Note that it is possible to use Hail to define struct fields inside; of a key table or variant dataset that do not match python syntax.; The name ""1kg"", for example, will not parse to python because it; begins with an integer, which is not an acceptable leading character; for an identifier. There are two ways to access this field:. >>> getattr(bar, '1kg'); >>> bar['1kg']. The ``pprint`` module can be used to print nested Structs in a more; human-readable fashion:. >>> from pprint import pprint; >>> pprint(bar). :param dict attributes: struct members.; """""". def __init__(self, attributes):. self._attrs = attributes. def __getattr__(self, item):; assert (self._attrs); if item not in self._attrs:; raise AttributeError(""Struct instance has no attribute '%s'"" % item); return self._attrs[item]. def __contains__(self, item):; return item in self._attrs. def __getitem__(self, item):; return self.__getattr__(item). def __len__(self):; return len(self._attrs). def __repr__(self):; return str(self). def __str__(self):; return 'Struct' + str(self._attrs). def __eq__(self, other):; if isinstance(other, Struct):; return self._attrs == other._attrs; else:; return False. def __hash__(self):; return 37 + hash(tuple(sorted(self._attrs.items()))). [docs] @typecheck_method(item=strlike,; default=anytype); def get(self, item, default=None):; """"""Get an item, or return a default value if the item is not found.; ; :param str item: Name of attribute.; ; :param default: Default value.; ; :returns: Value of item if found, or default value if not.; """"""; return self._attrs.get(item, default). @typecheck(struct=Struct); def to_dict(struct):; d = ",MatchSource.WIKI,docs/0.1/_modules/hail/representation/annotations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/annotations.html
https://hail.is/docs/0.1/_modules/hail/representation/genotype.html:5360,Modifiability,variab,variables,5360," reference and one alternate allele. :rtype: bool; """""". return self._jrep.isHetRef(). [docs] def is_not_called(self):; """"""True if the genotype call is missing. :rtype: bool; """""". return self._jrep.isNotCalled(). [docs] def is_called(self):; """"""True if the genotype call is non-missing. :rtype: bool; """""". return self._jrep.isCalled(). [docs] def num_alt_alleles(self):; """"""Returns the count of non-reference alleles. This function returns None if the genotype call is missing. :rtype: int or None; """""". return from_option(self._jrep.nNonRefAlleles()). [docs] @handle_py4j; @typecheck_method(num_alleles=integral); def one_hot_alleles(self, num_alleles):; """"""Returns a list containing the one-hot encoded representation of the called alleles. This one-hot representation is the positional sum of the one-hot; encoding for each called allele. For a biallelic variant, the; one-hot encoding for a reference allele is [1, 0] and the one-hot; encoding for an alternate allele is [0, 1]. Thus, with the; following variables:. .. testcode::. num_alleles = 2; hom_ref = Genotype(0); het = Genotype(1); hom_var = Genotype(2). All the below statements are true:. .. testcode::. hom_ref.one_hot_alleles(num_alleles) == [2, 0]; het.one_hot_alleles(num_alleles) == [1, 1]; hom_var.one_hot_alleles(num_alleles) == [0, 2]. This function returns None if the genotype call is missing. :param int num_alleles: number of possible alternate alleles; :rtype: list of int or None; """"""; return jiterable_to_list(from_option(self._jrep.oneHotAlleles(num_alleles))). [docs] @handle_py4j; @typecheck_method(num_genotypes=integral); def one_hot_genotype(self, num_genotypes):; """"""Returns a list containing the one-hot encoded representation of the genotype call. A one-hot encoding is a vector with one '1' and many '0' values, like; [0, 0, 1, 0] or [1, 0, 0, 0]. This function is useful for transforming; the genotype call (gt) into a one-hot encoded array. With the following; variables:. .. testcode::. num_genotypes = 3; ho",MatchSource.WIKI,docs/0.1/_modules/hail/representation/genotype.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/genotype.html
https://hail.is/docs/0.1/_modules/hail/representation/genotype.html:6304,Modifiability,variab,variables,6304,"for an alternate allele is [0, 1]. Thus, with the; following variables:. .. testcode::. num_alleles = 2; hom_ref = Genotype(0); het = Genotype(1); hom_var = Genotype(2). All the below statements are true:. .. testcode::. hom_ref.one_hot_alleles(num_alleles) == [2, 0]; het.one_hot_alleles(num_alleles) == [1, 1]; hom_var.one_hot_alleles(num_alleles) == [0, 2]. This function returns None if the genotype call is missing. :param int num_alleles: number of possible alternate alleles; :rtype: list of int or None; """"""; return jiterable_to_list(from_option(self._jrep.oneHotAlleles(num_alleles))). [docs] @handle_py4j; @typecheck_method(num_genotypes=integral); def one_hot_genotype(self, num_genotypes):; """"""Returns a list containing the one-hot encoded representation of the genotype call. A one-hot encoding is a vector with one '1' and many '0' values, like; [0, 0, 1, 0] or [1, 0, 0, 0]. This function is useful for transforming; the genotype call (gt) into a one-hot encoded array. With the following; variables:. .. testcode::. num_genotypes = 3; hom_ref = Genotype(0); het = Genotype(1); hom_var = Genotype(2). All the below statements are true:. .. testcode::. hom_ref.one_hot_genotype(num_genotypes) == [1, 0, 0]; het.one_hot_genotype(num_genotypes) == [0, 1, 0]; hom_var.one_hot_genotype(num_genotypes) == [0, 0, 1]. This function returns None if the genotype call is missing. :param int num_genotypes: number of possible genotypes; :rtype: list of int or None; """""". return jiterable_to_list(from_option(self._jrep.oneHotGenotype(num_genotypes))). [docs] @handle_py4j; @typecheck_method(theta=numeric); def p_ab(self, theta=0.5):; """"""Returns the p-value associated with finding the given allele depth ratio. This function uses a one-tailed binomial test. This function returns None if the allelic depth (ad) is missing. :param float theta: null reference probability for binomial model; :rtype: float; """""". return from_option(self._jrep.pAB(theta)). [docs] def fraction_reads_ref(self):; """"""R",MatchSource.WIKI,docs/0.1/_modules/hail/representation/genotype.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/genotype.html
https://hail.is/docs/0.1/_modules/hail/representation/genotype.html:10202,Modifiability,variab,variables,10202,"d one alternate allele. :rtype: bool; """""". return self._jcall.isHetRef(self._jrep). [docs] def is_not_called(self):; """"""True if the call is missing. :rtype: bool; """""". return self._jcall.isNotCalled(self._jrep). [docs] def is_called(self):; """"""True if the call is non-missing. :rtype: bool; """""". return self._jcall.isCalled(self._jrep). [docs] def num_alt_alleles(self):; """"""Returns the count of non-reference alleles. This function returns None if the genotype call is missing. :rtype: int or None; """""". return self._jcall.nNonRefAlleles(self._jrep). [docs] @handle_py4j; @typecheck_method(num_alleles=integral); def one_hot_alleles(self, num_alleles):; """"""Returns a list containing the one-hot encoded representation of the called alleles. This one-hot representation is the positional sum of the one-hot; encoding for each called allele. For a biallelic variant, the; one-hot encoding for a reference allele is [1, 0] and the one-hot; encoding for an alternate allele is [0, 1]. Thus, with the; following variables:. .. testcode::. num_alleles = 2; hom_ref = Call(0); het = Call(1); hom_var = Call(2). All the below statements are true:. .. testcode::. hom_ref.one_hot_alleles(num_alleles) == [2, 0]; het.one_hot_alleles(num_alleles) == [1, 1]; hom_var.one_hot_alleles(num_alleles) == [0, 2]. This function returns None if the call is missing. :param int num_alleles: number of possible alternate alleles; :rtype: list of int or None; """"""; return jiterable_to_list(self._jcall.oneHotAlleles(self._jrep, num_alleles)). [docs] @handle_py4j; @typecheck_method(num_genotypes=integral); def one_hot_genotype(self, num_genotypes):; """"""Returns a list containing the one-hot encoded representation of the genotype call. A one-hot encoding is a vector with one '1' and many '0' values, like; [0, 0, 1, 0] or [1, 0, 0, 0]. This function is useful for transforming; the genotype call (gt) into a one-hot encoded array. With the following; variables:. .. testcode::. num_genotypes = 3; hom_ref = Call(0); het ",MatchSource.WIKI,docs/0.1/_modules/hail/representation/genotype.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/genotype.html
https://hail.is/docs/0.1/_modules/hail/representation/genotype.html:11125,Modifiability,variab,variables,11125,"cs] @handle_py4j; @typecheck_method(num_alleles=integral); def one_hot_alleles(self, num_alleles):; """"""Returns a list containing the one-hot encoded representation of the called alleles. This one-hot representation is the positional sum of the one-hot; encoding for each called allele. For a biallelic variant, the; one-hot encoding for a reference allele is [1, 0] and the one-hot; encoding for an alternate allele is [0, 1]. Thus, with the; following variables:. .. testcode::. num_alleles = 2; hom_ref = Call(0); het = Call(1); hom_var = Call(2). All the below statements are true:. .. testcode::. hom_ref.one_hot_alleles(num_alleles) == [2, 0]; het.one_hot_alleles(num_alleles) == [1, 1]; hom_var.one_hot_alleles(num_alleles) == [0, 2]. This function returns None if the call is missing. :param int num_alleles: number of possible alternate alleles; :rtype: list of int or None; """"""; return jiterable_to_list(self._jcall.oneHotAlleles(self._jrep, num_alleles)). [docs] @handle_py4j; @typecheck_method(num_genotypes=integral); def one_hot_genotype(self, num_genotypes):; """"""Returns a list containing the one-hot encoded representation of the genotype call. A one-hot encoding is a vector with one '1' and many '0' values, like; [0, 0, 1, 0] or [1, 0, 0, 0]. This function is useful for transforming; the genotype call (gt) into a one-hot encoded array. With the following; variables:. .. testcode::. num_genotypes = 3; hom_ref = Call(0); het = Call(1); hom_var = Call(2). All the below statements are true:. .. testcode::. hom_ref.one_hot_genotype(num_genotypes) == [1, 0, 0]; het.one_hot_genotype(num_genotypes) == [0, 1, 0]; hom_var.one_hot_genotype(num_genotypes) == [0, 0, 1]. This function returns None if the call is missing. :param int num_genotypes: number of possible genotypes; :rtype: list of int or None; """""". return jiterable_to_list(self._jcall.oneHotGenotype(self._jrep, num_genotypes)); . © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,docs/0.1/_modules/hail/representation/genotype.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/genotype.html
https://hail.is/docs/0.1/_modules/hail/representation/genotype.html:2001,Security,hash,hashCode,2001,"elf, gt, ad=None, dp=None, gq=None, pl=None):; """"""Initialize a Genotype object."""""". jvm = Env.jvm(); jgt = joption(gt); if ad:; jad = jsome(jarray(jvm.int, ad)); else:; jad = jnone(); jdp = joption(dp); jgq = joption(gq); if pl:; jpl = jsome(jarray(jvm.int, pl)); else:; jpl = jnone(). jrep = scala_object(Env.hail().variant, 'Genotype').apply(; jgt, jad, jdp, jgq, jpl, False, False); self._gt = gt; self._ad = ad; self._dp = dp; self._gq = gq; self._pl = pl; self._init_from_java(jrep). def __str__(self):; return self._jrep.toString(). def __repr__(self):; fake_ref = 'FakeRef=True' if self._jrep.fakeRef() else ''; if self._jrep.isLinearScale():; return 'Genotype(GT=%s, AD=%s, DP=%s, GQ=%s, GP=%s%s)' %\; (self.gt, self.ad, self.dp, self.gq, self.gp, fake_ref); else:; return 'Genotype(GT=%s, AD=%s, DP=%s, GQ=%s, PL=%s%s)' % \; (self.gt, self.ad, self.dp, self.gq, self.pl, fake_ref). def __eq__(self, other):; return self._jrep.equals(other._jrep). def __hash__(self):; return self._jrep.hashCode(). def _init_from_java(self, jrep):; self._jrep = jrep. @classmethod; def _from_java(cls, jrep):; g = Genotype.__new__(cls); g._init_from_java(jrep); g._gt = from_option(jrep.gt()); g._ad = jarray_to_list(from_option(jrep.ad())); g._dp = from_option(jrep.dp()); g._gq = from_option(jrep.gq()); g._pl = jarray_to_list(from_option(jrep.pl())); return g. @property; def gt(self):; """"""Returns the hard genotype call. :rtype: int or None; """""". return self._gt. @property; def ad(self):; """"""Returns the allelic depth. :rtype: list of int or None; """""". return self._ad. @property; def dp(self):; """"""Returns the total depth. :rtype: int or None; """""". return self._dp. @property; def gq(self):; """"""Returns the phred-scaled genotype quality. :return: int or None; """""". return self._gq. @property; def pl(self):; """"""Returns the phred-scaled genotype posterior likelihoods. :rtype: list of int or None; """""". return self._pl. [docs] def od(self):; """"""Returns the difference between the total depth and the alle",MatchSource.WIKI,docs/0.1/_modules/hail/representation/genotype.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/genotype.html
https://hail.is/docs/0.1/_modules/hail/representation/genotype.html:8100,Security,hash,hashCode,8100,"epth (ad) is missing. :param float theta: null reference probability for binomial model; :rtype: float; """""". return from_option(self._jrep.pAB(theta)). [docs] def fraction_reads_ref(self):; """"""Returns the fraction of reads that are reference reads. Equivalent to:. >>> g.ad[0] / sum(g.ad). :rtype: float or None; """""". return from_option(self._jrep.fractionReadsRef()). [docs]class Call(object):; """"""; An object that represents an individual's call at a genomic locus. :param call: Genotype hard call; :type call: int or None; """""". _call_jobject = None. @handle_py4j; def __init__(self, call):; """"""Initialize a Call object."""""". if not Call._call_jobject:; Call._call_jobject = scala_object(Env.hail().variant, 'Call'). jrep = Call._call_jobject.apply(call); self._init_from_java(jrep). def __str__(self):; return self._jrep.toString(). def __repr__(self):; return 'Call(gt=%s)' % self._jrep. def __eq__(self, other):; return self._jrep.equals(other._jrep). def __hash__(self):; return self._jrep.hashCode(). def _init_from_java(self, jrep):; self._jcall = Call._call_jobject; self._jrep = jrep. @classmethod; def _from_java(cls, jrep):; c = Call.__new__(cls); c._init_from_java(jrep); return c. @property; def gt(self):; """"""Returns the hard call. :rtype: int or None; """""". return self._jrep. [docs] def is_hom_ref(self):; """"""True if the call is 0/0. :rtype: bool; """""". return self._jcall.isHomRef(self._jrep). [docs] def is_het(self):; """"""True if the call contains two different alleles. :rtype: bool; """""". return self._jcall.isHet(self._jrep). [docs] def is_hom_var(self):; """"""True if the call contains two identical alternate alleles. :rtype: bool; """""". return self._jcall.isHomVar(self._jrep). [docs] def is_called_non_ref(self):; """"""True if the call contains any non-reference alleles. :rtype: bool; """""". return self._jcall.isCalledNonRef(self._jrep). [docs] def is_het_non_ref(self):; """"""True if the call contains two different alternate alleles. :rtype: bool; """""". return self._jcall.isHetNonRef(",MatchSource.WIKI,docs/0.1/_modules/hail/representation/genotype.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/genotype.html
https://hail.is/docs/0.1/_modules/hail/representation/genotype.html:528,Testability,test,testsetup,528,"﻿. . hail.representation.genotype — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Module code »; hail.representation.genotype. Source code for hail.representation.genotype; from hail.java import *; from hail.typecheck import *. [docs]class Genotype(object):; """"""; An object that represents an individual's genotype at a genomic locus. .. testsetup::. g = Genotype(0, ad=[9,1], dp=11, gq=20, pl=[0,100,1000]). :param gt: Genotype hard call; :type gt: int or None; :param ad: allelic depth (1 element per allele including reference); :type ad: list of int or None; :param dp: total depth; :type dp: int or None; :param gq: genotype quality; :type gq: int or None; :param pl: phred-scaled posterior genotype likelihoods (1 element per possible genotype); :type pl: list of int or None; """""". @handle_py4j; def __init__(self, gt, ad=None, dp=None, gq=None, pl=None):; """"""Initialize a Genotype object."""""". jvm = Env.jvm(); jgt = joption(gt); if ad:; jad = jsome(jarray(jvm.int, ad)); else:; jad = jnone(); jdp = joption(dp); jgq = joption(gq); if pl:; jpl = jsome(jarray(jvm.int, pl)); else:; jpl = jnone(). jrep = scala_object(Env.hail().variant, 'Genotype').apply(; jgt, jad, jdp, jgq, jpl, False, False); self._gt = gt; self._ad = ad; self._dp = dp; self._gq = gq; self._pl = pl; self._init_from_java(jrep). def __str__(self):; return self._jrep.toString(). def __repr__(self):; fake_ref = 'FakeRef=True' if self._jrep.fakeRef() else ''; if self._jrep.isLinearScale():; return 'Genotype(GT=%s, AD=%s, DP=%s, GQ=%s, GP=%s%s)' %\; (self.gt, self.ad, self.dp, self.gq, self.gp, fake_ref); else:; return 'Genotype(GT=%s, AD=%s, DP=%s, GQ=%s, PL=%s%s)' % \; (self.gt, self.ad, self.dp, self.gq, self.pl, fake_ref). def __eq__(self, other):; return self._jrep.equals(other._jrep). def __hash__(self):; return self._jrep.",MatchSource.WIKI,docs/0.1/_modules/hail/representation/genotype.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/genotype.html
https://hail.is/docs/0.1/_modules/hail/representation/genotype.html:3041,Testability,test,testcode,3041," self._jrep = jrep. @classmethod; def _from_java(cls, jrep):; g = Genotype.__new__(cls); g._init_from_java(jrep); g._gt = from_option(jrep.gt()); g._ad = jarray_to_list(from_option(jrep.ad())); g._dp = from_option(jrep.dp()); g._gq = from_option(jrep.gq()); g._pl = jarray_to_list(from_option(jrep.pl())); return g. @property; def gt(self):; """"""Returns the hard genotype call. :rtype: int or None; """""". return self._gt. @property; def ad(self):; """"""Returns the allelic depth. :rtype: list of int or None; """""". return self._ad. @property; def dp(self):; """"""Returns the total depth. :rtype: int or None; """""". return self._dp. @property; def gq(self):; """"""Returns the phred-scaled genotype quality. :return: int or None; """""". return self._gq. @property; def pl(self):; """"""Returns the phred-scaled genotype posterior likelihoods. :rtype: list of int or None; """""". return self._pl. [docs] def od(self):; """"""Returns the difference between the total depth and the allelic depth sum. Equivalent to:. .. testcode::. g.dp - sum(g.ad). :rtype: int or None; """""". return from_option(self._jrep.od()). @property; def gp(self):; """"""Returns the linear-scaled genotype probabilities. :rtype: list of float of None; """""". return jarray_to_list(from_option(self._jrep.gp())). [docs] def dosage(self):; """"""Returns the expected value of the genotype based on genotype probabilities,; :math:`\\mathrm{P}(\\mathrm{Het}) + 2 \\mathrm{P}(\\mathrm{HomVar})`. Genotype must be bi-allelic. :rtype: float; """""". return from_option(self._jrep.dosage()). [docs] def is_hom_ref(self):; """"""True if the genotype call is 0/0. :rtype: bool; """""". return self._jrep.isHomRef(). [docs] def is_het(self):; """"""True if the genotype call contains two different alleles. :rtype: bool; """""". return self._jrep.isHet(). [docs] def is_hom_var(self):; """"""True if the genotype call contains two identical alternate alleles. :rtype: bool; """""". return self._jrep.isHomVar(). [docs] def is_called_non_ref(self):; """"""True if the genotype call contains any ",MatchSource.WIKI,docs/0.1/_modules/hail/representation/genotype.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/genotype.html
https://hail.is/docs/0.1/_modules/hail/representation/genotype.html:5375,Testability,test,testcode,5375," allele. :rtype: bool; """""". return self._jrep.isHetRef(). [docs] def is_not_called(self):; """"""True if the genotype call is missing. :rtype: bool; """""". return self._jrep.isNotCalled(). [docs] def is_called(self):; """"""True if the genotype call is non-missing. :rtype: bool; """""". return self._jrep.isCalled(). [docs] def num_alt_alleles(self):; """"""Returns the count of non-reference alleles. This function returns None if the genotype call is missing. :rtype: int or None; """""". return from_option(self._jrep.nNonRefAlleles()). [docs] @handle_py4j; @typecheck_method(num_alleles=integral); def one_hot_alleles(self, num_alleles):; """"""Returns a list containing the one-hot encoded representation of the called alleles. This one-hot representation is the positional sum of the one-hot; encoding for each called allele. For a biallelic variant, the; one-hot encoding for a reference allele is [1, 0] and the one-hot; encoding for an alternate allele is [0, 1]. Thus, with the; following variables:. .. testcode::. num_alleles = 2; hom_ref = Genotype(0); het = Genotype(1); hom_var = Genotype(2). All the below statements are true:. .. testcode::. hom_ref.one_hot_alleles(num_alleles) == [2, 0]; het.one_hot_alleles(num_alleles) == [1, 1]; hom_var.one_hot_alleles(num_alleles) == [0, 2]. This function returns None if the genotype call is missing. :param int num_alleles: number of possible alternate alleles; :rtype: list of int or None; """"""; return jiterable_to_list(from_option(self._jrep.oneHotAlleles(num_alleles))). [docs] @handle_py4j; @typecheck_method(num_genotypes=integral); def one_hot_genotype(self, num_genotypes):; """"""Returns a list containing the one-hot encoded representation of the genotype call. A one-hot encoding is a vector with one '1' and many '0' values, like; [0, 0, 1, 0] or [1, 0, 0, 0]. This function is useful for transforming; the genotype call (gt) into a one-hot encoded array. With the following; variables:. .. testcode::. num_genotypes = 3; hom_ref = Genotype(0); het = G",MatchSource.WIKI,docs/0.1/_modules/hail/representation/genotype.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/genotype.html
https://hail.is/docs/0.1/_modules/hail/representation/genotype.html:5508,Testability,test,testcode,5508,"rtype: bool; """""". return self._jrep.isNotCalled(). [docs] def is_called(self):; """"""True if the genotype call is non-missing. :rtype: bool; """""". return self._jrep.isCalled(). [docs] def num_alt_alleles(self):; """"""Returns the count of non-reference alleles. This function returns None if the genotype call is missing. :rtype: int or None; """""". return from_option(self._jrep.nNonRefAlleles()). [docs] @handle_py4j; @typecheck_method(num_alleles=integral); def one_hot_alleles(self, num_alleles):; """"""Returns a list containing the one-hot encoded representation of the called alleles. This one-hot representation is the positional sum of the one-hot; encoding for each called allele. For a biallelic variant, the; one-hot encoding for a reference allele is [1, 0] and the one-hot; encoding for an alternate allele is [0, 1]. Thus, with the; following variables:. .. testcode::. num_alleles = 2; hom_ref = Genotype(0); het = Genotype(1); hom_var = Genotype(2). All the below statements are true:. .. testcode::. hom_ref.one_hot_alleles(num_alleles) == [2, 0]; het.one_hot_alleles(num_alleles) == [1, 1]; hom_var.one_hot_alleles(num_alleles) == [0, 2]. This function returns None if the genotype call is missing. :param int num_alleles: number of possible alternate alleles; :rtype: list of int or None; """"""; return jiterable_to_list(from_option(self._jrep.oneHotAlleles(num_alleles))). [docs] @handle_py4j; @typecheck_method(num_genotypes=integral); def one_hot_genotype(self, num_genotypes):; """"""Returns a list containing the one-hot encoded representation of the genotype call. A one-hot encoding is a vector with one '1' and many '0' values, like; [0, 0, 1, 0] or [1, 0, 0, 0]. This function is useful for transforming; the genotype call (gt) into a one-hot encoded array. With the following; variables:. .. testcode::. num_genotypes = 3; hom_ref = Genotype(0); het = Genotype(1); hom_var = Genotype(2). All the below statements are true:. .. testcode::. hom_ref.one_hot_genotype(num_genotypes) == [1, ",MatchSource.WIKI,docs/0.1/_modules/hail/representation/genotype.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/genotype.html
https://hail.is/docs/0.1/_modules/hail/representation/genotype.html:6319,Testability,test,testcode,6319,"s [0, 1]. Thus, with the; following variables:. .. testcode::. num_alleles = 2; hom_ref = Genotype(0); het = Genotype(1); hom_var = Genotype(2). All the below statements are true:. .. testcode::. hom_ref.one_hot_alleles(num_alleles) == [2, 0]; het.one_hot_alleles(num_alleles) == [1, 1]; hom_var.one_hot_alleles(num_alleles) == [0, 2]. This function returns None if the genotype call is missing. :param int num_alleles: number of possible alternate alleles; :rtype: list of int or None; """"""; return jiterable_to_list(from_option(self._jrep.oneHotAlleles(num_alleles))). [docs] @handle_py4j; @typecheck_method(num_genotypes=integral); def one_hot_genotype(self, num_genotypes):; """"""Returns a list containing the one-hot encoded representation of the genotype call. A one-hot encoding is a vector with one '1' and many '0' values, like; [0, 0, 1, 0] or [1, 0, 0, 0]. This function is useful for transforming; the genotype call (gt) into a one-hot encoded array. With the following; variables:. .. testcode::. num_genotypes = 3; hom_ref = Genotype(0); het = Genotype(1); hom_var = Genotype(2). All the below statements are true:. .. testcode::. hom_ref.one_hot_genotype(num_genotypes) == [1, 0, 0]; het.one_hot_genotype(num_genotypes) == [0, 1, 0]; hom_var.one_hot_genotype(num_genotypes) == [0, 0, 1]. This function returns None if the genotype call is missing. :param int num_genotypes: number of possible genotypes; :rtype: list of int or None; """""". return jiterable_to_list(from_option(self._jrep.oneHotGenotype(num_genotypes))). [docs] @handle_py4j; @typecheck_method(theta=numeric); def p_ab(self, theta=0.5):; """"""Returns the p-value associated with finding the given allele depth ratio. This function uses a one-tailed binomial test. This function returns None if the allelic depth (ad) is missing. :param float theta: null reference probability for binomial model; :rtype: float; """""". return from_option(self._jrep.pAB(theta)). [docs] def fraction_reads_ref(self):; """"""Returns the fraction of re",MatchSource.WIKI,docs/0.1/_modules/hail/representation/genotype.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/genotype.html
https://hail.is/docs/0.1/_modules/hail/representation/genotype.html:6454,Testability,test,testcode,6454,"otype(2). All the below statements are true:. .. testcode::. hom_ref.one_hot_alleles(num_alleles) == [2, 0]; het.one_hot_alleles(num_alleles) == [1, 1]; hom_var.one_hot_alleles(num_alleles) == [0, 2]. This function returns None if the genotype call is missing. :param int num_alleles: number of possible alternate alleles; :rtype: list of int or None; """"""; return jiterable_to_list(from_option(self._jrep.oneHotAlleles(num_alleles))). [docs] @handle_py4j; @typecheck_method(num_genotypes=integral); def one_hot_genotype(self, num_genotypes):; """"""Returns a list containing the one-hot encoded representation of the genotype call. A one-hot encoding is a vector with one '1' and many '0' values, like; [0, 0, 1, 0] or [1, 0, 0, 0]. This function is useful for transforming; the genotype call (gt) into a one-hot encoded array. With the following; variables:. .. testcode::. num_genotypes = 3; hom_ref = Genotype(0); het = Genotype(1); hom_var = Genotype(2). All the below statements are true:. .. testcode::. hom_ref.one_hot_genotype(num_genotypes) == [1, 0, 0]; het.one_hot_genotype(num_genotypes) == [0, 1, 0]; hom_var.one_hot_genotype(num_genotypes) == [0, 0, 1]. This function returns None if the genotype call is missing. :param int num_genotypes: number of possible genotypes; :rtype: list of int or None; """""". return jiterable_to_list(from_option(self._jrep.oneHotGenotype(num_genotypes))). [docs] @handle_py4j; @typecheck_method(theta=numeric); def p_ab(self, theta=0.5):; """"""Returns the p-value associated with finding the given allele depth ratio. This function uses a one-tailed binomial test. This function returns None if the allelic depth (ad) is missing. :param float theta: null reference probability for binomial model; :rtype: float; """""". return from_option(self._jrep.pAB(theta)). [docs] def fraction_reads_ref(self):; """"""Returns the fraction of reads that are reference reads. Equivalent to:. >>> g.ad[0] / sum(g.ad). :rtype: float or None; """""". return from_option(self._jrep.fracti",MatchSource.WIKI,docs/0.1/_modules/hail/representation/genotype.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/genotype.html
https://hail.is/docs/0.1/_modules/hail/representation/genotype.html:7056,Testability,test,test,7056,"e-hot encoded representation of the genotype call. A one-hot encoding is a vector with one '1' and many '0' values, like; [0, 0, 1, 0] or [1, 0, 0, 0]. This function is useful for transforming; the genotype call (gt) into a one-hot encoded array. With the following; variables:. .. testcode::. num_genotypes = 3; hom_ref = Genotype(0); het = Genotype(1); hom_var = Genotype(2). All the below statements are true:. .. testcode::. hom_ref.one_hot_genotype(num_genotypes) == [1, 0, 0]; het.one_hot_genotype(num_genotypes) == [0, 1, 0]; hom_var.one_hot_genotype(num_genotypes) == [0, 0, 1]. This function returns None if the genotype call is missing. :param int num_genotypes: number of possible genotypes; :rtype: list of int or None; """""". return jiterable_to_list(from_option(self._jrep.oneHotGenotype(num_genotypes))). [docs] @handle_py4j; @typecheck_method(theta=numeric); def p_ab(self, theta=0.5):; """"""Returns the p-value associated with finding the given allele depth ratio. This function uses a one-tailed binomial test. This function returns None if the allelic depth (ad) is missing. :param float theta: null reference probability for binomial model; :rtype: float; """""". return from_option(self._jrep.pAB(theta)). [docs] def fraction_reads_ref(self):; """"""Returns the fraction of reads that are reference reads. Equivalent to:. >>> g.ad[0] / sum(g.ad). :rtype: float or None; """""". return from_option(self._jrep.fractionReadsRef()). [docs]class Call(object):; """"""; An object that represents an individual's call at a genomic locus. :param call: Genotype hard call; :type call: int or None; """""". _call_jobject = None. @handle_py4j; def __init__(self, call):; """"""Initialize a Call object."""""". if not Call._call_jobject:; Call._call_jobject = scala_object(Env.hail().variant, 'Call'). jrep = Call._call_jobject.apply(call); self._init_from_java(jrep). def __str__(self):; return self._jrep.toString(). def __repr__(self):; return 'Call(gt=%s)' % self._jrep. def __eq__(self, other):; return self._jre",MatchSource.WIKI,docs/0.1/_modules/hail/representation/genotype.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/genotype.html
https://hail.is/docs/0.1/_modules/hail/representation/genotype.html:10217,Testability,test,testcode,10217,"pe: bool; """""". return self._jcall.isHetRef(self._jrep). [docs] def is_not_called(self):; """"""True if the call is missing. :rtype: bool; """""". return self._jcall.isNotCalled(self._jrep). [docs] def is_called(self):; """"""True if the call is non-missing. :rtype: bool; """""". return self._jcall.isCalled(self._jrep). [docs] def num_alt_alleles(self):; """"""Returns the count of non-reference alleles. This function returns None if the genotype call is missing. :rtype: int or None; """""". return self._jcall.nNonRefAlleles(self._jrep). [docs] @handle_py4j; @typecheck_method(num_alleles=integral); def one_hot_alleles(self, num_alleles):; """"""Returns a list containing the one-hot encoded representation of the called alleles. This one-hot representation is the positional sum of the one-hot; encoding for each called allele. For a biallelic variant, the; one-hot encoding for a reference allele is [1, 0] and the one-hot; encoding for an alternate allele is [0, 1]. Thus, with the; following variables:. .. testcode::. num_alleles = 2; hom_ref = Call(0); het = Call(1); hom_var = Call(2). All the below statements are true:. .. testcode::. hom_ref.one_hot_alleles(num_alleles) == [2, 0]; het.one_hot_alleles(num_alleles) == [1, 1]; hom_var.one_hot_alleles(num_alleles) == [0, 2]. This function returns None if the call is missing. :param int num_alleles: number of possible alternate alleles; :rtype: list of int or None; """"""; return jiterable_to_list(self._jcall.oneHotAlleles(self._jrep, num_alleles)). [docs] @handle_py4j; @typecheck_method(num_genotypes=integral); def one_hot_genotype(self, num_genotypes):; """"""Returns a list containing the one-hot encoded representation of the genotype call. A one-hot encoding is a vector with one '1' and many '0' values, like; [0, 0, 1, 0] or [1, 0, 0, 0]. This function is useful for transforming; the genotype call (gt) into a one-hot encoded array. With the following; variables:. .. testcode::. num_genotypes = 3; hom_ref = Call(0); het = Call(1); hom_var = Call(2)",MatchSource.WIKI,docs/0.1/_modules/hail/representation/genotype.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/genotype.html
https://hail.is/docs/0.1/_modules/hail/representation/genotype.html:10338,Testability,test,testcode,10338,":rtype: bool; """""". return self._jcall.isNotCalled(self._jrep). [docs] def is_called(self):; """"""True if the call is non-missing. :rtype: bool; """""". return self._jcall.isCalled(self._jrep). [docs] def num_alt_alleles(self):; """"""Returns the count of non-reference alleles. This function returns None if the genotype call is missing. :rtype: int or None; """""". return self._jcall.nNonRefAlleles(self._jrep). [docs] @handle_py4j; @typecheck_method(num_alleles=integral); def one_hot_alleles(self, num_alleles):; """"""Returns a list containing the one-hot encoded representation of the called alleles. This one-hot representation is the positional sum of the one-hot; encoding for each called allele. For a biallelic variant, the; one-hot encoding for a reference allele is [1, 0] and the one-hot; encoding for an alternate allele is [0, 1]. Thus, with the; following variables:. .. testcode::. num_alleles = 2; hom_ref = Call(0); het = Call(1); hom_var = Call(2). All the below statements are true:. .. testcode::. hom_ref.one_hot_alleles(num_alleles) == [2, 0]; het.one_hot_alleles(num_alleles) == [1, 1]; hom_var.one_hot_alleles(num_alleles) == [0, 2]. This function returns None if the call is missing. :param int num_alleles: number of possible alternate alleles; :rtype: list of int or None; """"""; return jiterable_to_list(self._jcall.oneHotAlleles(self._jrep, num_alleles)). [docs] @handle_py4j; @typecheck_method(num_genotypes=integral); def one_hot_genotype(self, num_genotypes):; """"""Returns a list containing the one-hot encoded representation of the genotype call. A one-hot encoding is a vector with one '1' and many '0' values, like; [0, 0, 1, 0] or [1, 0, 0, 0]. This function is useful for transforming; the genotype call (gt) into a one-hot encoded array. With the following; variables:. .. testcode::. num_genotypes = 3; hom_ref = Call(0); het = Call(1); hom_var = Call(2). All the below statements are true:. .. testcode::. hom_ref.one_hot_genotype(num_genotypes) == [1, 0, 0]; het.one_hot_ge",MatchSource.WIKI,docs/0.1/_modules/hail/representation/genotype.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/genotype.html
https://hail.is/docs/0.1/_modules/hail/representation/genotype.html:11140,Testability,test,testcode,11140,"cs] @handle_py4j; @typecheck_method(num_alleles=integral); def one_hot_alleles(self, num_alleles):; """"""Returns a list containing the one-hot encoded representation of the called alleles. This one-hot representation is the positional sum of the one-hot; encoding for each called allele. For a biallelic variant, the; one-hot encoding for a reference allele is [1, 0] and the one-hot; encoding for an alternate allele is [0, 1]. Thus, with the; following variables:. .. testcode::. num_alleles = 2; hom_ref = Call(0); het = Call(1); hom_var = Call(2). All the below statements are true:. .. testcode::. hom_ref.one_hot_alleles(num_alleles) == [2, 0]; het.one_hot_alleles(num_alleles) == [1, 1]; hom_var.one_hot_alleles(num_alleles) == [0, 2]. This function returns None if the call is missing. :param int num_alleles: number of possible alternate alleles; :rtype: list of int or None; """"""; return jiterable_to_list(self._jcall.oneHotAlleles(self._jrep, num_alleles)). [docs] @handle_py4j; @typecheck_method(num_genotypes=integral); def one_hot_genotype(self, num_genotypes):; """"""Returns a list containing the one-hot encoded representation of the genotype call. A one-hot encoding is a vector with one '1' and many '0' values, like; [0, 0, 1, 0] or [1, 0, 0, 0]. This function is useful for transforming; the genotype call (gt) into a one-hot encoded array. With the following; variables:. .. testcode::. num_genotypes = 3; hom_ref = Call(0); het = Call(1); hom_var = Call(2). All the below statements are true:. .. testcode::. hom_ref.one_hot_genotype(num_genotypes) == [1, 0, 0]; het.one_hot_genotype(num_genotypes) == [0, 1, 0]; hom_var.one_hot_genotype(num_genotypes) == [0, 0, 1]. This function returns None if the call is missing. :param int num_genotypes: number of possible genotypes; :rtype: list of int or None; """""". return jiterable_to_list(self._jcall.oneHotGenotype(self._jrep, num_genotypes)); . © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,docs/0.1/_modules/hail/representation/genotype.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/genotype.html
https://hail.is/docs/0.1/_modules/hail/representation/genotype.html:11263,Testability,test,testcode,11263,"cs] @handle_py4j; @typecheck_method(num_alleles=integral); def one_hot_alleles(self, num_alleles):; """"""Returns a list containing the one-hot encoded representation of the called alleles. This one-hot representation is the positional sum of the one-hot; encoding for each called allele. For a biallelic variant, the; one-hot encoding for a reference allele is [1, 0] and the one-hot; encoding for an alternate allele is [0, 1]. Thus, with the; following variables:. .. testcode::. num_alleles = 2; hom_ref = Call(0); het = Call(1); hom_var = Call(2). All the below statements are true:. .. testcode::. hom_ref.one_hot_alleles(num_alleles) == [2, 0]; het.one_hot_alleles(num_alleles) == [1, 1]; hom_var.one_hot_alleles(num_alleles) == [0, 2]. This function returns None if the call is missing. :param int num_alleles: number of possible alternate alleles; :rtype: list of int or None; """"""; return jiterable_to_list(self._jcall.oneHotAlleles(self._jrep, num_alleles)). [docs] @handle_py4j; @typecheck_method(num_genotypes=integral); def one_hot_genotype(self, num_genotypes):; """"""Returns a list containing the one-hot encoded representation of the genotype call. A one-hot encoding is a vector with one '1' and many '0' values, like; [0, 0, 1, 0] or [1, 0, 0, 0]. This function is useful for transforming; the genotype call (gt) into a one-hot encoded array. With the following; variables:. .. testcode::. num_genotypes = 3; hom_ref = Call(0); het = Call(1); hom_var = Call(2). All the below statements are true:. .. testcode::. hom_ref.one_hot_genotype(num_genotypes) == [1, 0, 0]; het.one_hot_genotype(num_genotypes) == [0, 1, 0]; hom_var.one_hot_genotype(num_genotypes) == [0, 0, 1]. This function returns None if the call is missing. :param int num_genotypes: number of possible genotypes; :rtype: list of int or None; """""". return jiterable_to_list(self._jcall.oneHotGenotype(self._jrep, num_genotypes)); . © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,docs/0.1/_modules/hail/representation/genotype.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/genotype.html
https://hail.is/docs/0.1/_modules/hail/representation/interval.html:1415,Security,hash,hashCode,1415," import Locus; from hail.typecheck import *. interval_type = lazy(). [docs]class Interval(object):; """"""; A genomic interval marked by start and end loci. .. testsetup::. interval1 = Interval.parse('X:100005-X:150020'); interval2 = Interval.parse('16:29500000-30200000'). :param start: inclusive start locus; :type start: :class:`.Locus`; :param end: exclusive end locus; :type end: :class:`.Locus`; """""". @handle_py4j; def __init__(self, start, end):; if not (isinstance(start, Locus) and isinstance(end, Locus)):; raise TypeError('expect arguments of type (Locus, Locus) but found (%s, %s)' %; (str(type(start)), str(type(end)))); jrep = scala_object(Env.hail().variant, 'Locus').makeInterval(start._jrep, end._jrep); self._init_from_java(jrep). def __str__(self):; return self._jrep.toString(). def __repr__(self):; return 'Interval(start=%s, end=%s)' % (repr(self.start), repr(self.end)). def __eq__(self, other):; return self._jrep.equals(other._jrep). def __hash__(self):; return self._jrep.hashCode(). def _init_from_java(self, jrep):; self._jrep = jrep; self._start = Locus._from_java(self._jrep.start()). @classmethod; def _from_java(cls, jrep):; interval = Interval.__new__(cls); interval._init_from_java(jrep); return interval. [docs] @staticmethod; @handle_py4j; @typecheck(string=strlike); def parse(string):; """"""Parses a genomic interval from string representation. **Examples**:. >>> interval_1 = Interval.parse('X:100005-X:150020'); >>> interval_2 = Interval.parse('16:29500000-30200000'); >>> interval_3 = Interval.parse('16:29.5M-30.2M') # same as interval_2; >>> interval_4 = Interval.parse('16:30000000-END'); >>> interval_5 = Interval.parse('16:30M-END') # same as interval_4; >>> interval_6 = Interval.parse('1-22') # autosomes; >>> interval_7 = Interval.parse('X') # all of chromosome X. There are several acceptable representations. ``CHR1:POS1-CHR2:POS2`` is the fully specified representation, and; we use this to define the various shortcut representations. In a ``POS`` field",MatchSource.WIKI,docs/0.1/_modules/hail/representation/interval.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/interval.html
https://hail.is/docs/0.1/_modules/hail/representation/interval.html:577,Testability,test,testsetup,577,"﻿. . hail.representation.interval — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Module code »; hail.representation.interval. Source code for hail.representation.interval; from hail.java import *; from hail.representation.variant import Locus; from hail.typecheck import *. interval_type = lazy(). [docs]class Interval(object):; """"""; A genomic interval marked by start and end loci. .. testsetup::. interval1 = Interval.parse('X:100005-X:150020'); interval2 = Interval.parse('16:29500000-30200000'). :param start: inclusive start locus; :type start: :class:`.Locus`; :param end: exclusive end locus; :type end: :class:`.Locus`; """""". @handle_py4j; def __init__(self, start, end):; if not (isinstance(start, Locus) and isinstance(end, Locus)):; raise TypeError('expect arguments of type (Locus, Locus) but found (%s, %s)' %; (str(type(start)), str(type(end)))); jrep = scala_object(Env.hail().variant, 'Locus').makeInterval(start._jrep, end._jrep); self._init_from_java(jrep). def __str__(self):; return self._jrep.toString(). def __repr__(self):; return 'Interval(start=%s, end=%s)' % (repr(self.start), repr(self.end)). def __eq__(self, other):; return self._jrep.equals(other._jrep). def __hash__(self):; return self._jrep.hashCode(). def _init_from_java(self, jrep):; self._jrep = jrep; self._start = Locus._from_java(self._jrep.start()). @classmethod; def _from_java(cls, jrep):; interval = Interval.__new__(cls); interval._init_from_java(jrep); return interval. [docs] @staticmethod; @handle_py4j; @typecheck(string=strlike); def parse(string):; """"""Parses a genomic interval from string representation. **Examples**:. >>> interval_1 = Interval.parse('X:100005-X:150020'); >>> interval_2 = Interval.parse('16:29500000-30200000'); >>> interval_3 = Interval.parse('16:29.5M-30.2M') # same as interval_2; >>> i",MatchSource.WIKI,docs/0.1/_modules/hail/representation/interval.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/interval.html
https://hail.is/docs/0.1/_modules/hail/representation/pedigree.html:2072,Security,hash,hashCode,2072,"; if is_female is not None:; jsex = jsome(jobject.Female()) if is_female else jsome(jobject.Male()); else:; jsex = jnone(). self._jrep = Env.hail().methods.BaseTrio(proband, joption(fam), joption(father), joption(mother), jsex); self._fam = fam; self._proband = proband; self._father = father; self._mother = mother; self._is_female = is_female. @classmethod; def _from_java(cls, jrep):; trio = Trio.__new__(cls); trio._jrep = jrep; return trio. def __repr__(self):; return 'Trio(proband=%s, fam=%s, father=%s, mother=%s, is_female=%s)' % (; repr(self.proband), repr(self.fam), repr(self.father),; repr(self.mother), repr(self.is_female)). def __str__(self):; return 'Trio(proband=%s, fam=%s, father=%s, mother=%s, is_female=%s)' % (; str(self.proband), str(self.fam), str(self.father),; str(self.mother), str(self.is_female)). def __eq__(self, other):; if not isinstance(other, Trio):; return False; else:; return self._jrep == other._jrep. @handle_py4j; def __hash__(self):; return self._jrep.hashCode(). @property; @handle_py4j; def proband(self):; """"""ID of proband in trio, never missing. :rtype: str; """"""; if not hasattr(self, '_proband'):; self._proband = self._jrep.kid(); return self._proband. @property; @handle_py4j; def father(self):; """"""ID of father in trio, may be missing. :rtype: str or None; """""". if not hasattr(self, '_father'):; self._father = from_option(self._jrep.dad()); return self._father. @property; @handle_py4j; def mother(self):; """"""ID of mother in trio, may be missing. :rtype: str or None; """""". if not hasattr(self, '_mother'):; self._mother = from_option(self._jrep.mom()); return self._mother. @property; @handle_py4j; def fam(self):; """"""Family ID. :rtype: str or None; """""". if not hasattr(self, '_fam'):; self._fam = from_option(self._jrep.fam()); return self._fam. @property; @handle_py4j; def is_male(self):; """"""Returns True if the proband is a reported male, False if reported female, and None if no sex is defined. :rtype: bool or None; """"""; if not hasattr(self, '",MatchSource.WIKI,docs/0.1/_modules/hail/representation/pedigree.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/pedigree.html
https://hail.is/docs/0.1/_modules/hail/representation/pedigree.html:4720,Security,hash,hashCode,4720,"lete(self):; """"""Returns True if the trio has a defined mother, father, and sex. The considered fields are ``mother``, ``father``, and ``sex``.; Recall that ``proband`` may never be missing. The ``fam`` field; may be missing in a complete trio. :rtype: bool; """""". if not hasattr(self, '_complete'):; self._complete = self._jrep.isComplete(); return self._complete. [docs]class Pedigree(object):; """"""Class containing a list of trios, with extra functionality. :param trios: list of trio objects to include in pedigree; :type trios: list of :class:`.Trio`; """""". @handle_py4j; def __init__(self, trios):. self._jrep = Env.hail().methods.Pedigree(jindexed_seq([t._jrep for t in trios])); self._trios = trios. @classmethod; def _from_java(cls, jrep):; ped = Pedigree.__new__(cls); ped._jrep = jrep; ped._trios = None; return ped. def __eq__(self, other):; if not isinstance(other, Pedigree):; return False; else:; return self._jrep == other._jrep. @handle_py4j; def __hash__(self):; return self._jrep.hashCode(). [docs] @staticmethod; @handle_py4j; @typecheck(fam_path=strlike,; delimiter=strlike); def read(fam_path, delimiter='\\s+'):; """"""Read a .fam file and return a pedigree object. **Examples**. >>> ped = Pedigree.read('data/test.fam'). **Notes**. This method reads a `PLINK .fam file <https://www.cog-genomics.org/plink2/formats#fam>`_. Hail expects a file in the same spec as PLINK outlines. :param str fam_path: path to .fam file. :param str delimiter: Field delimiter. :rtype: :class:`.Pedigree`; """""". jrep = Env.hail().methods.Pedigree.read(fam_path, Env.hc()._jhc.hadoopConf(), delimiter); return Pedigree._from_java(jrep). @property; @handle_py4j; def trios(self):; """"""List of trio objects in this pedigree. :rtype: list of :class:`.Trio`; """""". if not self._trios:; self._trios = [Trio._from_java(t) for t in jiterable_to_list(self._jrep.trios())]; return self._trios. [docs] def complete_trios(self):; """"""List of trio objects that have a defined father, mother, and sex. :rtype: list of :clas",MatchSource.WIKI,docs/0.1/_modules/hail/representation/pedigree.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/pedigree.html
https://hail.is/docs/0.1/_modules/hail/representation/pedigree.html:4951,Testability,test,test,4951," in a complete trio. :rtype: bool; """""". if not hasattr(self, '_complete'):; self._complete = self._jrep.isComplete(); return self._complete. [docs]class Pedigree(object):; """"""Class containing a list of trios, with extra functionality. :param trios: list of trio objects to include in pedigree; :type trios: list of :class:`.Trio`; """""". @handle_py4j; def __init__(self, trios):. self._jrep = Env.hail().methods.Pedigree(jindexed_seq([t._jrep for t in trios])); self._trios = trios. @classmethod; def _from_java(cls, jrep):; ped = Pedigree.__new__(cls); ped._jrep = jrep; ped._trios = None; return ped. def __eq__(self, other):; if not isinstance(other, Pedigree):; return False; else:; return self._jrep == other._jrep. @handle_py4j; def __hash__(self):; return self._jrep.hashCode(). [docs] @staticmethod; @handle_py4j; @typecheck(fam_path=strlike,; delimiter=strlike); def read(fam_path, delimiter='\\s+'):; """"""Read a .fam file and return a pedigree object. **Examples**. >>> ped = Pedigree.read('data/test.fam'). **Notes**. This method reads a `PLINK .fam file <https://www.cog-genomics.org/plink2/formats#fam>`_. Hail expects a file in the same spec as PLINK outlines. :param str fam_path: path to .fam file. :param str delimiter: Field delimiter. :rtype: :class:`.Pedigree`; """""". jrep = Env.hail().methods.Pedigree.read(fam_path, Env.hc()._jhc.hadoopConf(), delimiter); return Pedigree._from_java(jrep). @property; @handle_py4j; def trios(self):; """"""List of trio objects in this pedigree. :rtype: list of :class:`.Trio`; """""". if not self._trios:; self._trios = [Trio._from_java(t) for t in jiterable_to_list(self._jrep.trios())]; return self._trios. [docs] def complete_trios(self):; """"""List of trio objects that have a defined father, mother, and sex. :rtype: list of :class:`.Trio`; """"""; return filter(lambda t: t.is_complete(), self.trios). [docs] @handle_py4j; @typecheck_method(samples=listof(strlike)); def filter_to(self, samples):; """"""Filter the pedigree to a given list of sample IDs. **",MatchSource.WIKI,docs/0.1/_modules/hail/representation/pedigree.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/pedigree.html
https://hail.is/docs/0.1/_modules/hail/representation/pedigree.html:6594,Testability,test,test,6594,"er. :rtype: :class:`.Pedigree`; """""". jrep = Env.hail().methods.Pedigree.read(fam_path, Env.hc()._jhc.hadoopConf(), delimiter); return Pedigree._from_java(jrep). @property; @handle_py4j; def trios(self):; """"""List of trio objects in this pedigree. :rtype: list of :class:`.Trio`; """""". if not self._trios:; self._trios = [Trio._from_java(t) for t in jiterable_to_list(self._jrep.trios())]; return self._trios. [docs] def complete_trios(self):; """"""List of trio objects that have a defined father, mother, and sex. :rtype: list of :class:`.Trio`; """"""; return filter(lambda t: t.is_complete(), self.trios). [docs] @handle_py4j; @typecheck_method(samples=listof(strlike)); def filter_to(self, samples):; """"""Filter the pedigree to a given list of sample IDs. **Notes**. For any trio, the following steps will be applied:. - If the proband is not in the list of samples provided, the trio is removed.; - If the father is not in the list of samples provided, the father is set to ``None``.; - If the mother is not in the list of samples provided, the mother is set to ``None``. :param samples: list of sample IDs to keep; :type samples: list of str. :rtype: :class:`.Pedigree`; """""". return Pedigree._from_java(self._jrep.filterTo(jset(samples))). [docs] @handle_py4j; @typecheck_method(path=strlike); def write(self, path):; """"""Write a .fam file to the given path. **Examples**. >>> ped = Pedigree.read('data/test.fam'); >>> ped.write('out.fam'). **Notes**. This method writes a `PLINK .fam file <https://www.cog-genomics.org/plink2/formats#fam>`_. .. caution::. Phenotype information is not preserved in the Pedigree data structure in Hail.; Reading and writing a PLINK .fam file will result in loss of this information.; Use the key table method :py:meth:`~hail.KeyTable.import_fam` to manipulate this; information. :param path: output path; :type path: str; """""". self._jrep.write(path, Env.hc()._jhc.hadoopConf()). © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,docs/0.1/_modules/hail/representation/pedigree.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/pedigree.html
https://hail.is/docs/0.1/_modules/hail/representation/variant.html:7140,Energy Efficiency,reduce,reduced,7140," alt):; jaa = scala_object(Env.hail().variant, 'AltAllele').apply(ref, alt); self._init_from_java(jaa); self._ref = ref; self._alt = alt. def __str__(self):; return self._jrep.toString(). def __repr__(self):; return 'AltAllele(ref=%s, alt=%s)' % (self.ref, self.alt). def __eq__(self, other):; return self._jrep.equals(other._jrep). def __hash__(self):; return self._jrep.hashCode(). def _init_from_java(self, jrep):; self._jrep = jrep. @classmethod; def _from_java(cls, jaa):; aa = AltAllele.__new__(cls); aa._init_from_java(jaa); aa._ref = jaa.ref(); aa._alt = jaa.alt(); return aa. @property; def ref(self):; """"""; Reference allele. :rtype: str; """"""; return self._ref. @property; def alt(self):; """"""; Alternate allele. :rtype: str; """"""; return self._alt. [docs] def num_mismatch(self):; """"""Returns the number of mismatched bases in this alternate allele. Fails if the ref and alt alleles are not the same length. :rtype: int; """""". return self._jrep.nMismatch(). [docs] def stripped_snp(self):; """"""Returns the one-character reduced SNP. Fails if called on an alternate allele that is not a SNP. :rtype: str, str; """""". r = self._jrep.strippedSNP(); return r._1(), r._2(). [docs] def is_SNP(self):; """"""True if this alternate allele is a single nucleotide polymorphism (SNP). :rtype: bool; """""". return self._jrep.isSNP(). [docs] def is_MNP(self):; """"""True if this alternate allele is a multiple nucleotide polymorphism (MNP). :rtype: bool; """""". return self._jrep.isMNP(). [docs] def is_insertion(self):; """"""True if this alternate allele is an insertion of one or more bases. :rtype: bool; """""". return self._jrep.isInsertion(). [docs] def is_deletion(self):; """"""True if this alternate allele is a deletion of one or more bases. :rtype: bool; """""". return self._jrep.isDeletion(). [docs] def is_indel(self):; """"""True if this alternate allele is either an insertion or deletion of one or more bases. :rtype: bool; """""". return self._jrep.isIndel(). [docs] def is_complex(self):; """"""True if this alternate al",MatchSource.WIKI,docs/0.1/_modules/hail/representation/variant.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/variant.html
https://hail.is/docs/0.1/_modules/hail/representation/variant.html:515,Modifiability,polymorphi,polymorphism,515,"﻿. . hail.representation.variant — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Module code »; hail.representation.variant. Source code for hail.representation.variant; from hail.java import scala_object, Env, handle_py4j; from hail.typecheck import *. [docs]class Variant(object):; """"""; An object that represents a genomic polymorphism. .. testsetup::. v_biallelic = Variant.parse('16:20012:A:TT'); v_multiallelic = Variant.parse('16:12311:T:C,TTT,A'). :param contig: chromosome identifier; :type contig: str or int; :param int start: chromosomal position (1-based); :param str ref: reference allele; :param alts: single alternate allele, or list of alternate alleles; :type alts: str or list of str; """""". @handle_py4j; def __init__(self, contig, start, ref, alts):; if isinstance(contig, int):; contig = str(contig); jrep = scala_object(Env.hail().variant, 'Variant').apply(contig, start, ref, alts); self._init_from_java(jrep); self._contig = contig; self._start = start; self._ref = ref. def __str__(self):; return self._jrep.toString(). def __repr__(self):; return 'Variant(contig=%s, start=%s, ref=%s, alts=%s)' % (self.contig, self.start, self.ref, self._alt_alleles). def __eq__(self, other):; return self._jrep.equals(other._jrep). def __hash__(self):; return self._jrep.hashCode(). def _init_from_java(self, jrep):; self._jrep = jrep; self._alt_alleles = map(AltAllele._from_java, [jrep.altAlleles().apply(i) for i in xrange(jrep.nAltAlleles())]). @classmethod; def _from_java(cls, jrep):; v = Variant.__new__(cls); v._init_from_java(jrep); v._contig = jrep.contig(); v._start = jrep.start(); v._ref = jrep.ref(); return v. [docs] @staticmethod; @handle_py4j; @typecheck(string=strlike); def parse(string):; """"""Parses a variant object from a string. There are two acceptable formats: CHR:POS:REF:ALT,",MatchSource.WIKI,docs/0.1/_modules/hail/representation/variant.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/variant.html
https://hail.is/docs/0.1/_modules/hail/representation/variant.html:2690,Modifiability,polymorphi,polymorphism,2690,"method; def _from_java(cls, jrep):; v = Variant.__new__(cls); v._init_from_java(jrep); v._contig = jrep.contig(); v._start = jrep.start(); v._ref = jrep.ref(); return v. [docs] @staticmethod; @handle_py4j; @typecheck(string=strlike); def parse(string):; """"""Parses a variant object from a string. There are two acceptable formats: CHR:POS:REF:ALT, and; CHR:POS:REF:ALT1,ALT2,...ALTN. Below is an example of; each:. >>> v_biallelic = Variant.parse('16:20012:A:TT'); >>> v_multiallelic = Variant.parse('16:12311:T:C,TTT,A'). :rtype: :class:`.Variant`; """"""; jrep = scala_object(Env.hail().variant, 'Variant').parse(string); return Variant._from_java(jrep). @property; def contig(self):; """"""; Chromosome identifier. :rtype: str; """"""; return self._contig. @property; def start(self):; """"""; Chromosomal position (1-based). :rtype: int; """"""; return self._start. @property; def ref(self):; """"""; Reference allele at this locus. :rtype: str; """""". return self._ref. @property; def alt_alleles(self):; """"""; List of alternate allele objects in this polymorphism. :rtype: list of :class:`.AltAllele`; """"""; return self._alt_alleles. [docs] def num_alt_alleles(self):; """"""Returns the number of alternate alleles in this polymorphism. :rtype: int; """""". return self._jrep.nAltAlleles(). [docs] def is_biallelic(self):; """"""True if there is only one alternate allele in this polymorphism. :rtype: bool; """""". return self._jrep.isBiallelic(). [docs] def alt_allele(self):; """"""Returns the alternate allele object, assumes biallelic. Fails if called on a multiallelic variant. :rtype: :class:`.AltAllele`; """""". return AltAllele._from_java(self._jrep.altAllele()). [docs] def alt(self):; """"""Returns the alternate allele string, assumes biallelic. Fails if called on a multiallelic variant. :rtype: str; """""". return self._jrep.alt(). [docs] def num_alleles(self):; """"""Returns the number of total alleles in this polymorphism, including the reference. :rtype: int; """""". return self._jrep.nAlleles(). [docs] @handle_py4j; @typeche",MatchSource.WIKI,docs/0.1/_modules/hail/representation/variant.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/variant.html
https://hail.is/docs/0.1/_modules/hail/representation/variant.html:2858,Modifiability,polymorphi,polymorphism,2858," v. [docs] @staticmethod; @handle_py4j; @typecheck(string=strlike); def parse(string):; """"""Parses a variant object from a string. There are two acceptable formats: CHR:POS:REF:ALT, and; CHR:POS:REF:ALT1,ALT2,...ALTN. Below is an example of; each:. >>> v_biallelic = Variant.parse('16:20012:A:TT'); >>> v_multiallelic = Variant.parse('16:12311:T:C,TTT,A'). :rtype: :class:`.Variant`; """"""; jrep = scala_object(Env.hail().variant, 'Variant').parse(string); return Variant._from_java(jrep). @property; def contig(self):; """"""; Chromosome identifier. :rtype: str; """"""; return self._contig. @property; def start(self):; """"""; Chromosomal position (1-based). :rtype: int; """"""; return self._start. @property; def ref(self):; """"""; Reference allele at this locus. :rtype: str; """""". return self._ref. @property; def alt_alleles(self):; """"""; List of alternate allele objects in this polymorphism. :rtype: list of :class:`.AltAllele`; """"""; return self._alt_alleles. [docs] def num_alt_alleles(self):; """"""Returns the number of alternate alleles in this polymorphism. :rtype: int; """""". return self._jrep.nAltAlleles(). [docs] def is_biallelic(self):; """"""True if there is only one alternate allele in this polymorphism. :rtype: bool; """""". return self._jrep.isBiallelic(). [docs] def alt_allele(self):; """"""Returns the alternate allele object, assumes biallelic. Fails if called on a multiallelic variant. :rtype: :class:`.AltAllele`; """""". return AltAllele._from_java(self._jrep.altAllele()). [docs] def alt(self):; """"""Returns the alternate allele string, assumes biallelic. Fails if called on a multiallelic variant. :rtype: str; """""". return self._jrep.alt(). [docs] def num_alleles(self):; """"""Returns the number of total alleles in this polymorphism, including the reference. :rtype: int; """""". return self._jrep.nAlleles(). [docs] @handle_py4j; @typecheck_method(i=integral); def allele(self, i):; """"""Returns the string allele representation for the ith allele. The reference is included in the allele index. The index",MatchSource.WIKI,docs/0.1/_modules/hail/representation/variant.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/variant.html
https://hail.is/docs/0.1/_modules/hail/representation/variant.html:3009,Modifiability,polymorphi,polymorphism,3009,"ble formats: CHR:POS:REF:ALT, and; CHR:POS:REF:ALT1,ALT2,...ALTN. Below is an example of; each:. >>> v_biallelic = Variant.parse('16:20012:A:TT'); >>> v_multiallelic = Variant.parse('16:12311:T:C,TTT,A'). :rtype: :class:`.Variant`; """"""; jrep = scala_object(Env.hail().variant, 'Variant').parse(string); return Variant._from_java(jrep). @property; def contig(self):; """"""; Chromosome identifier. :rtype: str; """"""; return self._contig. @property; def start(self):; """"""; Chromosomal position (1-based). :rtype: int; """"""; return self._start. @property; def ref(self):; """"""; Reference allele at this locus. :rtype: str; """""". return self._ref. @property; def alt_alleles(self):; """"""; List of alternate allele objects in this polymorphism. :rtype: list of :class:`.AltAllele`; """"""; return self._alt_alleles. [docs] def num_alt_alleles(self):; """"""Returns the number of alternate alleles in this polymorphism. :rtype: int; """""". return self._jrep.nAltAlleles(). [docs] def is_biallelic(self):; """"""True if there is only one alternate allele in this polymorphism. :rtype: bool; """""". return self._jrep.isBiallelic(). [docs] def alt_allele(self):; """"""Returns the alternate allele object, assumes biallelic. Fails if called on a multiallelic variant. :rtype: :class:`.AltAllele`; """""". return AltAllele._from_java(self._jrep.altAllele()). [docs] def alt(self):; """"""Returns the alternate allele string, assumes biallelic. Fails if called on a multiallelic variant. :rtype: str; """""". return self._jrep.alt(). [docs] def num_alleles(self):; """"""Returns the number of total alleles in this polymorphism, including the reference. :rtype: int; """""". return self._jrep.nAlleles(). [docs] @handle_py4j; @typecheck_method(i=integral); def allele(self, i):; """"""Returns the string allele representation for the ith allele. The reference is included in the allele index. The index of; the first alternate allele is 1. The following is true for all; variants:. >>> v_multiallelic.ref == v_multiallelic.allele(0). Additionally, the f",MatchSource.WIKI,docs/0.1/_modules/hail/representation/variant.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/variant.html
https://hail.is/docs/0.1/_modules/hail/representation/variant.html:3540,Modifiability,polymorphi,polymorphism,3540," def ref(self):; """"""; Reference allele at this locus. :rtype: str; """""". return self._ref. @property; def alt_alleles(self):; """"""; List of alternate allele objects in this polymorphism. :rtype: list of :class:`.AltAllele`; """"""; return self._alt_alleles. [docs] def num_alt_alleles(self):; """"""Returns the number of alternate alleles in this polymorphism. :rtype: int; """""". return self._jrep.nAltAlleles(). [docs] def is_biallelic(self):; """"""True if there is only one alternate allele in this polymorphism. :rtype: bool; """""". return self._jrep.isBiallelic(). [docs] def alt_allele(self):; """"""Returns the alternate allele object, assumes biallelic. Fails if called on a multiallelic variant. :rtype: :class:`.AltAllele`; """""". return AltAllele._from_java(self._jrep.altAllele()). [docs] def alt(self):; """"""Returns the alternate allele string, assumes biallelic. Fails if called on a multiallelic variant. :rtype: str; """""". return self._jrep.alt(). [docs] def num_alleles(self):; """"""Returns the number of total alleles in this polymorphism, including the reference. :rtype: int; """""". return self._jrep.nAlleles(). [docs] @handle_py4j; @typecheck_method(i=integral); def allele(self, i):; """"""Returns the string allele representation for the ith allele. The reference is included in the allele index. The index of; the first alternate allele is 1. The following is true for all; variants:. >>> v_multiallelic.ref == v_multiallelic.allele(0). Additionally, the following is true for all biallelic variants:. >>> v_biallelic.alt == v_biallelic.allele(1). :param int i: integer index of desired allele. :return: string representation of ith allele; :rtype: str; """""". return self._jrep.allele(i). [docs] def num_genotypes(self):; """"""Returns the total number of unique genotypes possible for this variant. For a biallelic variant, this value is 3: 0/0, 0/1, and 1/1. For a triallelic variant, this value is 6: 0/0, 0/1, 1/1, 0/2, 1/2, 2/2. For a variant with N alleles, this value is:. .. math::. \\frac{N * (N + 1",MatchSource.WIKI,docs/0.1/_modules/hail/representation/variant.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/variant.html
https://hail.is/docs/0.1/_modules/hail/representation/variant.html:4637,Modifiability,polymorphi,polymorphism,4637,"ep.nAlleles(). [docs] @handle_py4j; @typecheck_method(i=integral); def allele(self, i):; """"""Returns the string allele representation for the ith allele. The reference is included in the allele index. The index of; the first alternate allele is 1. The following is true for all; variants:. >>> v_multiallelic.ref == v_multiallelic.allele(0). Additionally, the following is true for all biallelic variants:. >>> v_biallelic.alt == v_biallelic.allele(1). :param int i: integer index of desired allele. :return: string representation of ith allele; :rtype: str; """""". return self._jrep.allele(i). [docs] def num_genotypes(self):; """"""Returns the total number of unique genotypes possible for this variant. For a biallelic variant, this value is 3: 0/0, 0/1, and 1/1. For a triallelic variant, this value is 6: 0/0, 0/1, 1/1, 0/2, 1/2, 2/2. For a variant with N alleles, this value is:. .. math::. \\frac{N * (N + 1)}{2}. :rtype: int"""""". return self._jrep.nGenotypes(). [docs] def locus(self):; """"""Returns the locus object for this polymorphism. :rtype: :class:`.Locus`; """"""; return Locus._from_java(self._jrep.locus()). [docs] def is_autosomal_or_pseudoautosomal(self):; """"""True if this polymorphism is found on an autosome, or the PAR on X or Y. :rtype: bool; """"""; return self._jrep.isAutosomalOrPseudoAutosomal(). [docs] def is_autosomal(self):; """"""True if this polymorphism is located on an autosome. :rtype: bool; """"""; return self._jrep.isAutosomal(). [docs] def is_mitochondrial(self):; """"""True if this polymorphism is mapped to mitochondrial DNA. :rtype: bool; """""". return self._jrep.isMitochondrial(). [docs] def in_X_PAR(self):; """"""True of this polymorphism is found on the pseudoautosomal region of chromosome X. :rtype: bool; """""". return self._jrep.inXPar(). [docs] def in_Y_PAR(self):; """"""True of this polymorphism is found on the pseudoautosomal region of chromosome Y. :rtype: bool; """""". return self._jrep.inYPar(). [docs] def in_X_non_PAR(self):; """"""True of this polymorphism is found on the ",MatchSource.WIKI,docs/0.1/_modules/hail/representation/variant.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/variant.html
https://hail.is/docs/0.1/_modules/hail/representation/variant.html:4793,Modifiability,polymorphi,polymorphism,4793,"ed in the allele index. The index of; the first alternate allele is 1. The following is true for all; variants:. >>> v_multiallelic.ref == v_multiallelic.allele(0). Additionally, the following is true for all biallelic variants:. >>> v_biallelic.alt == v_biallelic.allele(1). :param int i: integer index of desired allele. :return: string representation of ith allele; :rtype: str; """""". return self._jrep.allele(i). [docs] def num_genotypes(self):; """"""Returns the total number of unique genotypes possible for this variant. For a biallelic variant, this value is 3: 0/0, 0/1, and 1/1. For a triallelic variant, this value is 6: 0/0, 0/1, 1/1, 0/2, 1/2, 2/2. For a variant with N alleles, this value is:. .. math::. \\frac{N * (N + 1)}{2}. :rtype: int"""""". return self._jrep.nGenotypes(). [docs] def locus(self):; """"""Returns the locus object for this polymorphism. :rtype: :class:`.Locus`; """"""; return Locus._from_java(self._jrep.locus()). [docs] def is_autosomal_or_pseudoautosomal(self):; """"""True if this polymorphism is found on an autosome, or the PAR on X or Y. :rtype: bool; """"""; return self._jrep.isAutosomalOrPseudoAutosomal(). [docs] def is_autosomal(self):; """"""True if this polymorphism is located on an autosome. :rtype: bool; """"""; return self._jrep.isAutosomal(). [docs] def is_mitochondrial(self):; """"""True if this polymorphism is mapped to mitochondrial DNA. :rtype: bool; """""". return self._jrep.isMitochondrial(). [docs] def in_X_PAR(self):; """"""True of this polymorphism is found on the pseudoautosomal region of chromosome X. :rtype: bool; """""". return self._jrep.inXPar(). [docs] def in_Y_PAR(self):; """"""True of this polymorphism is found on the pseudoautosomal region of chromosome Y. :rtype: bool; """""". return self._jrep.inYPar(). [docs] def in_X_non_PAR(self):; """"""True of this polymorphism is found on the non-pseudoautosomal region of chromosome X. :rtype: bool; """""". return self._jrep.inXNonPar(). [docs] def in_Y_non_PAR(self):; """"""True of this polymorphism is found on the non-ps",MatchSource.WIKI,docs/0.1/_modules/hail/representation/variant.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/variant.html
https://hail.is/docs/0.1/_modules/hail/representation/variant.html:4970,Modifiability,polymorphi,polymorphism,4970,", the following is true for all biallelic variants:. >>> v_biallelic.alt == v_biallelic.allele(1). :param int i: integer index of desired allele. :return: string representation of ith allele; :rtype: str; """""". return self._jrep.allele(i). [docs] def num_genotypes(self):; """"""Returns the total number of unique genotypes possible for this variant. For a biallelic variant, this value is 3: 0/0, 0/1, and 1/1. For a triallelic variant, this value is 6: 0/0, 0/1, 1/1, 0/2, 1/2, 2/2. For a variant with N alleles, this value is:. .. math::. \\frac{N * (N + 1)}{2}. :rtype: int"""""". return self._jrep.nGenotypes(). [docs] def locus(self):; """"""Returns the locus object for this polymorphism. :rtype: :class:`.Locus`; """"""; return Locus._from_java(self._jrep.locus()). [docs] def is_autosomal_or_pseudoautosomal(self):; """"""True if this polymorphism is found on an autosome, or the PAR on X or Y. :rtype: bool; """"""; return self._jrep.isAutosomalOrPseudoAutosomal(). [docs] def is_autosomal(self):; """"""True if this polymorphism is located on an autosome. :rtype: bool; """"""; return self._jrep.isAutosomal(). [docs] def is_mitochondrial(self):; """"""True if this polymorphism is mapped to mitochondrial DNA. :rtype: bool; """""". return self._jrep.isMitochondrial(). [docs] def in_X_PAR(self):; """"""True of this polymorphism is found on the pseudoautosomal region of chromosome X. :rtype: bool; """""". return self._jrep.inXPar(). [docs] def in_Y_PAR(self):; """"""True of this polymorphism is found on the pseudoautosomal region of chromosome Y. :rtype: bool; """""". return self._jrep.inYPar(). [docs] def in_X_non_PAR(self):; """"""True of this polymorphism is found on the non-pseudoautosomal region of chromosome X. :rtype: bool; """""". return self._jrep.inXNonPar(). [docs] def in_Y_non_PAR(self):; """"""True of this polymorphism is found on the non-pseudoautosomal region of chromosome Y. :rtype: bool; """""". return self._jrep.inYNonPar(). [docs]class AltAllele(object):; """"""; An object that represents an allele in a polymorphi",MatchSource.WIKI,docs/0.1/_modules/hail/representation/variant.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/variant.html
https://hail.is/docs/0.1/_modules/hail/representation/variant.html:5114,Modifiability,polymorphi,polymorphism,5114,". :return: string representation of ith allele; :rtype: str; """""". return self._jrep.allele(i). [docs] def num_genotypes(self):; """"""Returns the total number of unique genotypes possible for this variant. For a biallelic variant, this value is 3: 0/0, 0/1, and 1/1. For a triallelic variant, this value is 6: 0/0, 0/1, 1/1, 0/2, 1/2, 2/2. For a variant with N alleles, this value is:. .. math::. \\frac{N * (N + 1)}{2}. :rtype: int"""""". return self._jrep.nGenotypes(). [docs] def locus(self):; """"""Returns the locus object for this polymorphism. :rtype: :class:`.Locus`; """"""; return Locus._from_java(self._jrep.locus()). [docs] def is_autosomal_or_pseudoautosomal(self):; """"""True if this polymorphism is found on an autosome, or the PAR on X or Y. :rtype: bool; """"""; return self._jrep.isAutosomalOrPseudoAutosomal(). [docs] def is_autosomal(self):; """"""True if this polymorphism is located on an autosome. :rtype: bool; """"""; return self._jrep.isAutosomal(). [docs] def is_mitochondrial(self):; """"""True if this polymorphism is mapped to mitochondrial DNA. :rtype: bool; """""". return self._jrep.isMitochondrial(). [docs] def in_X_PAR(self):; """"""True of this polymorphism is found on the pseudoautosomal region of chromosome X. :rtype: bool; """""". return self._jrep.inXPar(). [docs] def in_Y_PAR(self):; """"""True of this polymorphism is found on the pseudoautosomal region of chromosome Y. :rtype: bool; """""". return self._jrep.inYPar(). [docs] def in_X_non_PAR(self):; """"""True of this polymorphism is found on the non-pseudoautosomal region of chromosome X. :rtype: bool; """""". return self._jrep.inXNonPar(). [docs] def in_Y_non_PAR(self):; """"""True of this polymorphism is found on the non-pseudoautosomal region of chromosome Y. :rtype: bool; """""". return self._jrep.inYNonPar(). [docs]class AltAllele(object):; """"""; An object that represents an allele in a polymorphism deviating from the reference allele. :param str ref: reference allele; :param str alt: alternate allele; """""". @handle_py4j; def __init__(self,",MatchSource.WIKI,docs/0.1/_modules/hail/representation/variant.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/variant.html
https://hail.is/docs/0.1/_modules/hail/representation/variant.html:5259,Modifiability,polymorphi,polymorphism,5259,"ique genotypes possible for this variant. For a biallelic variant, this value is 3: 0/0, 0/1, and 1/1. For a triallelic variant, this value is 6: 0/0, 0/1, 1/1, 0/2, 1/2, 2/2. For a variant with N alleles, this value is:. .. math::. \\frac{N * (N + 1)}{2}. :rtype: int"""""". return self._jrep.nGenotypes(). [docs] def locus(self):; """"""Returns the locus object for this polymorphism. :rtype: :class:`.Locus`; """"""; return Locus._from_java(self._jrep.locus()). [docs] def is_autosomal_or_pseudoautosomal(self):; """"""True if this polymorphism is found on an autosome, or the PAR on X or Y. :rtype: bool; """"""; return self._jrep.isAutosomalOrPseudoAutosomal(). [docs] def is_autosomal(self):; """"""True if this polymorphism is located on an autosome. :rtype: bool; """"""; return self._jrep.isAutosomal(). [docs] def is_mitochondrial(self):; """"""True if this polymorphism is mapped to mitochondrial DNA. :rtype: bool; """""". return self._jrep.isMitochondrial(). [docs] def in_X_PAR(self):; """"""True of this polymorphism is found on the pseudoautosomal region of chromosome X. :rtype: bool; """""". return self._jrep.inXPar(). [docs] def in_Y_PAR(self):; """"""True of this polymorphism is found on the pseudoautosomal region of chromosome Y. :rtype: bool; """""". return self._jrep.inYPar(). [docs] def in_X_non_PAR(self):; """"""True of this polymorphism is found on the non-pseudoautosomal region of chromosome X. :rtype: bool; """""". return self._jrep.inXNonPar(). [docs] def in_Y_non_PAR(self):; """"""True of this polymorphism is found on the non-pseudoautosomal region of chromosome Y. :rtype: bool; """""". return self._jrep.inYNonPar(). [docs]class AltAllele(object):; """"""; An object that represents an allele in a polymorphism deviating from the reference allele. :param str ref: reference allele; :param str alt: alternate allele; """""". @handle_py4j; def __init__(self, ref, alt):; jaa = scala_object(Env.hail().variant, 'AltAllele').apply(ref, alt); self._init_from_java(jaa); self._ref = ref; self._alt = alt. def __str__(self):",MatchSource.WIKI,docs/0.1/_modules/hail/representation/variant.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/variant.html
https://hail.is/docs/0.1/_modules/hail/representation/variant.html:5419,Modifiability,polymorphi,polymorphism,5419," 0/2, 1/2, 2/2. For a variant with N alleles, this value is:. .. math::. \\frac{N * (N + 1)}{2}. :rtype: int"""""". return self._jrep.nGenotypes(). [docs] def locus(self):; """"""Returns the locus object for this polymorphism. :rtype: :class:`.Locus`; """"""; return Locus._from_java(self._jrep.locus()). [docs] def is_autosomal_or_pseudoautosomal(self):; """"""True if this polymorphism is found on an autosome, or the PAR on X or Y. :rtype: bool; """"""; return self._jrep.isAutosomalOrPseudoAutosomal(). [docs] def is_autosomal(self):; """"""True if this polymorphism is located on an autosome. :rtype: bool; """"""; return self._jrep.isAutosomal(). [docs] def is_mitochondrial(self):; """"""True if this polymorphism is mapped to mitochondrial DNA. :rtype: bool; """""". return self._jrep.isMitochondrial(). [docs] def in_X_PAR(self):; """"""True of this polymorphism is found on the pseudoautosomal region of chromosome X. :rtype: bool; """""". return self._jrep.inXPar(). [docs] def in_Y_PAR(self):; """"""True of this polymorphism is found on the pseudoautosomal region of chromosome Y. :rtype: bool; """""". return self._jrep.inYPar(). [docs] def in_X_non_PAR(self):; """"""True of this polymorphism is found on the non-pseudoautosomal region of chromosome X. :rtype: bool; """""". return self._jrep.inXNonPar(). [docs] def in_Y_non_PAR(self):; """"""True of this polymorphism is found on the non-pseudoautosomal region of chromosome Y. :rtype: bool; """""". return self._jrep.inYNonPar(). [docs]class AltAllele(object):; """"""; An object that represents an allele in a polymorphism deviating from the reference allele. :param str ref: reference allele; :param str alt: alternate allele; """""". @handle_py4j; def __init__(self, ref, alt):; jaa = scala_object(Env.hail().variant, 'AltAllele').apply(ref, alt); self._init_from_java(jaa); self._ref = ref; self._alt = alt. def __str__(self):; return self._jrep.toString(). def __repr__(self):; return 'AltAllele(ref=%s, alt=%s)' % (self.ref, self.alt). def __eq__(self, other):; return self._jrep.equa",MatchSource.WIKI,docs/0.1/_modules/hail/representation/variant.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/variant.html
https://hail.is/docs/0.1/_modules/hail/representation/variant.html:5583,Modifiability,polymorphi,polymorphism,5583,"lf):; """"""Returns the locus object for this polymorphism. :rtype: :class:`.Locus`; """"""; return Locus._from_java(self._jrep.locus()). [docs] def is_autosomal_or_pseudoautosomal(self):; """"""True if this polymorphism is found on an autosome, or the PAR on X or Y. :rtype: bool; """"""; return self._jrep.isAutosomalOrPseudoAutosomal(). [docs] def is_autosomal(self):; """"""True if this polymorphism is located on an autosome. :rtype: bool; """"""; return self._jrep.isAutosomal(). [docs] def is_mitochondrial(self):; """"""True if this polymorphism is mapped to mitochondrial DNA. :rtype: bool; """""". return self._jrep.isMitochondrial(). [docs] def in_X_PAR(self):; """"""True of this polymorphism is found on the pseudoautosomal region of chromosome X. :rtype: bool; """""". return self._jrep.inXPar(). [docs] def in_Y_PAR(self):; """"""True of this polymorphism is found on the pseudoautosomal region of chromosome Y. :rtype: bool; """""". return self._jrep.inYPar(). [docs] def in_X_non_PAR(self):; """"""True of this polymorphism is found on the non-pseudoautosomal region of chromosome X. :rtype: bool; """""". return self._jrep.inXNonPar(). [docs] def in_Y_non_PAR(self):; """"""True of this polymorphism is found on the non-pseudoautosomal region of chromosome Y. :rtype: bool; """""". return self._jrep.inYNonPar(). [docs]class AltAllele(object):; """"""; An object that represents an allele in a polymorphism deviating from the reference allele. :param str ref: reference allele; :param str alt: alternate allele; """""". @handle_py4j; def __init__(self, ref, alt):; jaa = scala_object(Env.hail().variant, 'AltAllele').apply(ref, alt); self._init_from_java(jaa); self._ref = ref; self._alt = alt. def __str__(self):; return self._jrep.toString(). def __repr__(self):; return 'AltAllele(ref=%s, alt=%s)' % (self.ref, self.alt). def __eq__(self, other):; return self._jrep.equals(other._jrep). def __hash__(self):; return self._jrep.hashCode(). def _init_from_java(self, jrep):; self._jrep = jrep. @classmethod; def _from_java(cls, jaa):; aa",MatchSource.WIKI,docs/0.1/_modules/hail/representation/variant.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/variant.html
https://hail.is/docs/0.1/_modules/hail/representation/variant.html:5754,Modifiability,polymorphi,polymorphism,5754,"mal(self):; """"""True if this polymorphism is found on an autosome, or the PAR on X or Y. :rtype: bool; """"""; return self._jrep.isAutosomalOrPseudoAutosomal(). [docs] def is_autosomal(self):; """"""True if this polymorphism is located on an autosome. :rtype: bool; """"""; return self._jrep.isAutosomal(). [docs] def is_mitochondrial(self):; """"""True if this polymorphism is mapped to mitochondrial DNA. :rtype: bool; """""". return self._jrep.isMitochondrial(). [docs] def in_X_PAR(self):; """"""True of this polymorphism is found on the pseudoautosomal region of chromosome X. :rtype: bool; """""". return self._jrep.inXPar(). [docs] def in_Y_PAR(self):; """"""True of this polymorphism is found on the pseudoautosomal region of chromosome Y. :rtype: bool; """""". return self._jrep.inYPar(). [docs] def in_X_non_PAR(self):; """"""True of this polymorphism is found on the non-pseudoautosomal region of chromosome X. :rtype: bool; """""". return self._jrep.inXNonPar(). [docs] def in_Y_non_PAR(self):; """"""True of this polymorphism is found on the non-pseudoautosomal region of chromosome Y. :rtype: bool; """""". return self._jrep.inYNonPar(). [docs]class AltAllele(object):; """"""; An object that represents an allele in a polymorphism deviating from the reference allele. :param str ref: reference allele; :param str alt: alternate allele; """""". @handle_py4j; def __init__(self, ref, alt):; jaa = scala_object(Env.hail().variant, 'AltAllele').apply(ref, alt); self._init_from_java(jaa); self._ref = ref; self._alt = alt. def __str__(self):; return self._jrep.toString(). def __repr__(self):; return 'AltAllele(ref=%s, alt=%s)' % (self.ref, self.alt). def __eq__(self, other):; return self._jrep.equals(other._jrep). def __hash__(self):; return self._jrep.hashCode(). def _init_from_java(self, jrep):; self._jrep = jrep. @classmethod; def _from_java(cls, jaa):; aa = AltAllele.__new__(cls); aa._init_from_java(jaa); aa._ref = jaa.ref(); aa._alt = jaa.alt(); return aa. @property; def ref(self):; """"""; Reference allele. :rtype: str; """"""",MatchSource.WIKI,docs/0.1/_modules/hail/representation/variant.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/variant.html
https://hail.is/docs/0.1/_modules/hail/representation/variant.html:5955,Modifiability,polymorphi,polymorphism,5955,"somal(self):; """"""True if this polymorphism is located on an autosome. :rtype: bool; """"""; return self._jrep.isAutosomal(). [docs] def is_mitochondrial(self):; """"""True if this polymorphism is mapped to mitochondrial DNA. :rtype: bool; """""". return self._jrep.isMitochondrial(). [docs] def in_X_PAR(self):; """"""True of this polymorphism is found on the pseudoautosomal region of chromosome X. :rtype: bool; """""". return self._jrep.inXPar(). [docs] def in_Y_PAR(self):; """"""True of this polymorphism is found on the pseudoautosomal region of chromosome Y. :rtype: bool; """""". return self._jrep.inYPar(). [docs] def in_X_non_PAR(self):; """"""True of this polymorphism is found on the non-pseudoautosomal region of chromosome X. :rtype: bool; """""". return self._jrep.inXNonPar(). [docs] def in_Y_non_PAR(self):; """"""True of this polymorphism is found on the non-pseudoautosomal region of chromosome Y. :rtype: bool; """""". return self._jrep.inYNonPar(). [docs]class AltAllele(object):; """"""; An object that represents an allele in a polymorphism deviating from the reference allele. :param str ref: reference allele; :param str alt: alternate allele; """""". @handle_py4j; def __init__(self, ref, alt):; jaa = scala_object(Env.hail().variant, 'AltAllele').apply(ref, alt); self._init_from_java(jaa); self._ref = ref; self._alt = alt. def __str__(self):; return self._jrep.toString(). def __repr__(self):; return 'AltAllele(ref=%s, alt=%s)' % (self.ref, self.alt). def __eq__(self, other):; return self._jrep.equals(other._jrep). def __hash__(self):; return self._jrep.hashCode(). def _init_from_java(self, jrep):; self._jrep = jrep. @classmethod; def _from_java(cls, jaa):; aa = AltAllele.__new__(cls); aa._init_from_java(jaa); aa._ref = jaa.ref(); aa._alt = jaa.alt(); return aa. @property; def ref(self):; """"""; Reference allele. :rtype: str; """"""; return self._ref. @property; def alt(self):; """"""; Alternate allele. :rtype: str; """"""; return self._alt. [docs] def num_mismatch(self):; """"""Returns the number of mismatched ",MatchSource.WIKI,docs/0.1/_modules/hail/representation/variant.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/variant.html
https://hail.is/docs/0.1/_modules/hail/representation/variant.html:7369,Modifiability,polymorphi,polymorphism,7369,"lele(ref=%s, alt=%s)' % (self.ref, self.alt). def __eq__(self, other):; return self._jrep.equals(other._jrep). def __hash__(self):; return self._jrep.hashCode(). def _init_from_java(self, jrep):; self._jrep = jrep. @classmethod; def _from_java(cls, jaa):; aa = AltAllele.__new__(cls); aa._init_from_java(jaa); aa._ref = jaa.ref(); aa._alt = jaa.alt(); return aa. @property; def ref(self):; """"""; Reference allele. :rtype: str; """"""; return self._ref. @property; def alt(self):; """"""; Alternate allele. :rtype: str; """"""; return self._alt. [docs] def num_mismatch(self):; """"""Returns the number of mismatched bases in this alternate allele. Fails if the ref and alt alleles are not the same length. :rtype: int; """""". return self._jrep.nMismatch(). [docs] def stripped_snp(self):; """"""Returns the one-character reduced SNP. Fails if called on an alternate allele that is not a SNP. :rtype: str, str; """""". r = self._jrep.strippedSNP(); return r._1(), r._2(). [docs] def is_SNP(self):; """"""True if this alternate allele is a single nucleotide polymorphism (SNP). :rtype: bool; """""". return self._jrep.isSNP(). [docs] def is_MNP(self):; """"""True if this alternate allele is a multiple nucleotide polymorphism (MNP). :rtype: bool; """""". return self._jrep.isMNP(). [docs] def is_insertion(self):; """"""True if this alternate allele is an insertion of one or more bases. :rtype: bool; """""". return self._jrep.isInsertion(). [docs] def is_deletion(self):; """"""True if this alternate allele is a deletion of one or more bases. :rtype: bool; """""". return self._jrep.isDeletion(). [docs] def is_indel(self):; """"""True if this alternate allele is either an insertion or deletion of one or more bases. :rtype: bool; """""". return self._jrep.isIndel(). [docs] def is_complex(self):; """"""True if this alternate allele does not fit into the categories of SNP, MNP, Insertion, or Deletion. :rtype: bool; """""". return self._jrep.isComplex(). [docs] def is_transition(self):; """"""True if this alternate allele is a transition SNP. This is tr",MatchSource.WIKI,docs/0.1/_modules/hail/representation/variant.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/variant.html
https://hail.is/docs/0.1/_modules/hail/representation/variant.html:7519,Modifiability,polymorphi,polymorphism,7519,".hashCode(). def _init_from_java(self, jrep):; self._jrep = jrep. @classmethod; def _from_java(cls, jaa):; aa = AltAllele.__new__(cls); aa._init_from_java(jaa); aa._ref = jaa.ref(); aa._alt = jaa.alt(); return aa. @property; def ref(self):; """"""; Reference allele. :rtype: str; """"""; return self._ref. @property; def alt(self):; """"""; Alternate allele. :rtype: str; """"""; return self._alt. [docs] def num_mismatch(self):; """"""Returns the number of mismatched bases in this alternate allele. Fails if the ref and alt alleles are not the same length. :rtype: int; """""". return self._jrep.nMismatch(). [docs] def stripped_snp(self):; """"""Returns the one-character reduced SNP. Fails if called on an alternate allele that is not a SNP. :rtype: str, str; """""". r = self._jrep.strippedSNP(); return r._1(), r._2(). [docs] def is_SNP(self):; """"""True if this alternate allele is a single nucleotide polymorphism (SNP). :rtype: bool; """""". return self._jrep.isSNP(). [docs] def is_MNP(self):; """"""True if this alternate allele is a multiple nucleotide polymorphism (MNP). :rtype: bool; """""". return self._jrep.isMNP(). [docs] def is_insertion(self):; """"""True if this alternate allele is an insertion of one or more bases. :rtype: bool; """""". return self._jrep.isInsertion(). [docs] def is_deletion(self):; """"""True if this alternate allele is a deletion of one or more bases. :rtype: bool; """""". return self._jrep.isDeletion(). [docs] def is_indel(self):; """"""True if this alternate allele is either an insertion or deletion of one or more bases. :rtype: bool; """""". return self._jrep.isIndel(). [docs] def is_complex(self):; """"""True if this alternate allele does not fit into the categories of SNP, MNP, Insertion, or Deletion. :rtype: bool; """""". return self._jrep.isComplex(). [docs] def is_transition(self):; """"""True if this alternate allele is a transition SNP. This is true if the reference and alternate bases are; both purine (A/G) or both pyrimidine (C/T). This method; raises an exception if the polymorphism is not ",MatchSource.WIKI,docs/0.1/_modules/hail/representation/variant.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/variant.html
https://hail.is/docs/0.1/_modules/hail/representation/variant.html:8466,Modifiability,polymorphi,polymorphism,8466,":; """"""True if this alternate allele is a multiple nucleotide polymorphism (MNP). :rtype: bool; """""". return self._jrep.isMNP(). [docs] def is_insertion(self):; """"""True if this alternate allele is an insertion of one or more bases. :rtype: bool; """""". return self._jrep.isInsertion(). [docs] def is_deletion(self):; """"""True if this alternate allele is a deletion of one or more bases. :rtype: bool; """""". return self._jrep.isDeletion(). [docs] def is_indel(self):; """"""True if this alternate allele is either an insertion or deletion of one or more bases. :rtype: bool; """""". return self._jrep.isIndel(). [docs] def is_complex(self):; """"""True if this alternate allele does not fit into the categories of SNP, MNP, Insertion, or Deletion. :rtype: bool; """""". return self._jrep.isComplex(). [docs] def is_transition(self):; """"""True if this alternate allele is a transition SNP. This is true if the reference and alternate bases are; both purine (A/G) or both pyrimidine (C/T). This method; raises an exception if the polymorphism is not a SNP. :rtype: bool; """""". return self._jrep.isTransition(). [docs] def is_transversion(self):; """"""True if this alternate allele is a transversion SNP. This is true if the reference and alternate bases contain; one purine (A/G) and one pyrimidine (C/T). This method; raises an exception if the polymorphism is not a SNP. :rtype: bool; """""". return self._jrep.isTransversion(). [docs] @handle_py4j; def category(self):; """"""Returns the type of alt, i.e one of; SNP,; Insertion,; Deletion,; Star,; MNP,; Complex. :rtype: str; """"""; return self._jrep.altAlleleType(). [docs]class Locus(object):; """"""; An object that represents a location in the genome. :param contig: chromosome identifier; :type contig: str or int; :param int position: chromosomal position (1-indexed); """""". @handle_py4j; def __init__(self, contig, position):; if isinstance(contig, int):; contig = str(contig); jrep = scala_object(Env.hail().variant, 'Locus').apply(contig, position); self._init_from_java(jrep",MatchSource.WIKI,docs/0.1/_modules/hail/representation/variant.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/variant.html
https://hail.is/docs/0.1/_modules/hail/representation/variant.html:8779,Modifiability,polymorphi,polymorphism,8779,"""""""True if this alternate allele is a deletion of one or more bases. :rtype: bool; """""". return self._jrep.isDeletion(). [docs] def is_indel(self):; """"""True if this alternate allele is either an insertion or deletion of one or more bases. :rtype: bool; """""". return self._jrep.isIndel(). [docs] def is_complex(self):; """"""True if this alternate allele does not fit into the categories of SNP, MNP, Insertion, or Deletion. :rtype: bool; """""". return self._jrep.isComplex(). [docs] def is_transition(self):; """"""True if this alternate allele is a transition SNP. This is true if the reference and alternate bases are; both purine (A/G) or both pyrimidine (C/T). This method; raises an exception if the polymorphism is not a SNP. :rtype: bool; """""". return self._jrep.isTransition(). [docs] def is_transversion(self):; """"""True if this alternate allele is a transversion SNP. This is true if the reference and alternate bases contain; one purine (A/G) and one pyrimidine (C/T). This method; raises an exception if the polymorphism is not a SNP. :rtype: bool; """""". return self._jrep.isTransversion(). [docs] @handle_py4j; def category(self):; """"""Returns the type of alt, i.e one of; SNP,; Insertion,; Deletion,; Star,; MNP,; Complex. :rtype: str; """"""; return self._jrep.altAlleleType(). [docs]class Locus(object):; """"""; An object that represents a location in the genome. :param contig: chromosome identifier; :type contig: str or int; :param int position: chromosomal position (1-indexed); """""". @handle_py4j; def __init__(self, contig, position):; if isinstance(contig, int):; contig = str(contig); jrep = scala_object(Env.hail().variant, 'Locus').apply(contig, position); self._init_from_java(jrep); self._contig = contig; self._position = position. def __str__(self):; return self._jrep.toString(). def __repr__(self):; return 'Locus(contig=%s, position=%s)' % (self.contig, self.position). def __eq__(self, other):; return self._jrep.equals(other._jrep). def __hash__(self):; return self._jrep.hashCode(). de",MatchSource.WIKI,docs/0.1/_modules/hail/representation/variant.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/variant.html
https://hail.is/docs/0.1/_modules/hail/representation/variant.html:1471,Security,hash,hashCode,1471,"""; An object that represents a genomic polymorphism. .. testsetup::. v_biallelic = Variant.parse('16:20012:A:TT'); v_multiallelic = Variant.parse('16:12311:T:C,TTT,A'). :param contig: chromosome identifier; :type contig: str or int; :param int start: chromosomal position (1-based); :param str ref: reference allele; :param alts: single alternate allele, or list of alternate alleles; :type alts: str or list of str; """""". @handle_py4j; def __init__(self, contig, start, ref, alts):; if isinstance(contig, int):; contig = str(contig); jrep = scala_object(Env.hail().variant, 'Variant').apply(contig, start, ref, alts); self._init_from_java(jrep); self._contig = contig; self._start = start; self._ref = ref. def __str__(self):; return self._jrep.toString(). def __repr__(self):; return 'Variant(contig=%s, start=%s, ref=%s, alts=%s)' % (self.contig, self.start, self.ref, self._alt_alleles). def __eq__(self, other):; return self._jrep.equals(other._jrep). def __hash__(self):; return self._jrep.hashCode(). def _init_from_java(self, jrep):; self._jrep = jrep; self._alt_alleles = map(AltAllele._from_java, [jrep.altAlleles().apply(i) for i in xrange(jrep.nAltAlleles())]). @classmethod; def _from_java(cls, jrep):; v = Variant.__new__(cls); v._init_from_java(jrep); v._contig = jrep.contig(); v._start = jrep.start(); v._ref = jrep.ref(); return v. [docs] @staticmethod; @handle_py4j; @typecheck(string=strlike); def parse(string):; """"""Parses a variant object from a string. There are two acceptable formats: CHR:POS:REF:ALT, and; CHR:POS:REF:ALT1,ALT2,...ALTN. Below is an example of; each:. >>> v_biallelic = Variant.parse('16:20012:A:TT'); >>> v_multiallelic = Variant.parse('16:12311:T:C,TTT,A'). :rtype: :class:`.Variant`; """"""; jrep = scala_object(Env.hail().variant, 'Variant').parse(string); return Variant._from_java(jrep). @property; def contig(self):; """"""; Chromosome identifier. :rtype: str; """"""; return self._contig. @property; def start(self):; """"""; Chromosomal position (1-based). :rtype",MatchSource.WIKI,docs/0.1/_modules/hail/representation/variant.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/variant.html
https://hail.is/docs/0.1/_modules/hail/representation/variant.html:6487,Security,hash,hashCode,6487,"pe: bool; """""". return self._jrep.inYPar(). [docs] def in_X_non_PAR(self):; """"""True of this polymorphism is found on the non-pseudoautosomal region of chromosome X. :rtype: bool; """""". return self._jrep.inXNonPar(). [docs] def in_Y_non_PAR(self):; """"""True of this polymorphism is found on the non-pseudoautosomal region of chromosome Y. :rtype: bool; """""". return self._jrep.inYNonPar(). [docs]class AltAllele(object):; """"""; An object that represents an allele in a polymorphism deviating from the reference allele. :param str ref: reference allele; :param str alt: alternate allele; """""". @handle_py4j; def __init__(self, ref, alt):; jaa = scala_object(Env.hail().variant, 'AltAllele').apply(ref, alt); self._init_from_java(jaa); self._ref = ref; self._alt = alt. def __str__(self):; return self._jrep.toString(). def __repr__(self):; return 'AltAllele(ref=%s, alt=%s)' % (self.ref, self.alt). def __eq__(self, other):; return self._jrep.equals(other._jrep). def __hash__(self):; return self._jrep.hashCode(). def _init_from_java(self, jrep):; self._jrep = jrep. @classmethod; def _from_java(cls, jaa):; aa = AltAllele.__new__(cls); aa._init_from_java(jaa); aa._ref = jaa.ref(); aa._alt = jaa.alt(); return aa. @property; def ref(self):; """"""; Reference allele. :rtype: str; """"""; return self._ref. @property; def alt(self):; """"""; Alternate allele. :rtype: str; """"""; return self._alt. [docs] def num_mismatch(self):; """"""Returns the number of mismatched bases in this alternate allele. Fails if the ref and alt alleles are not the same length. :rtype: int; """""". return self._jrep.nMismatch(). [docs] def stripped_snp(self):; """"""Returns the one-character reduced SNP. Fails if called on an alternate allele that is not a SNP. :rtype: str, str; """""". r = self._jrep.strippedSNP(); return r._1(), r._2(). [docs] def is_SNP(self):; """"""True if this alternate allele is a single nucleotide polymorphism (SNP). :rtype: bool; """""". return self._jrep.isSNP(). [docs] def is_MNP(self):; """"""True if this alternate allele",MatchSource.WIKI,docs/0.1/_modules/hail/representation/variant.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/variant.html
https://hail.is/docs/0.1/_modules/hail/representation/variant.html:9758,Security,hash,hashCode,9758,"ele is a transversion SNP. This is true if the reference and alternate bases contain; one purine (A/G) and one pyrimidine (C/T). This method; raises an exception if the polymorphism is not a SNP. :rtype: bool; """""". return self._jrep.isTransversion(). [docs] @handle_py4j; def category(self):; """"""Returns the type of alt, i.e one of; SNP,; Insertion,; Deletion,; Star,; MNP,; Complex. :rtype: str; """"""; return self._jrep.altAlleleType(). [docs]class Locus(object):; """"""; An object that represents a location in the genome. :param contig: chromosome identifier; :type contig: str or int; :param int position: chromosomal position (1-indexed); """""". @handle_py4j; def __init__(self, contig, position):; if isinstance(contig, int):; contig = str(contig); jrep = scala_object(Env.hail().variant, 'Locus').apply(contig, position); self._init_from_java(jrep); self._contig = contig; self._position = position. def __str__(self):; return self._jrep.toString(). def __repr__(self):; return 'Locus(contig=%s, position=%s)' % (self.contig, self.position). def __eq__(self, other):; return self._jrep.equals(other._jrep). def __hash__(self):; return self._jrep.hashCode(). def _init_from_java(self, jrep):; self._jrep = jrep. @classmethod; def _from_java(cls, jrep):; l = Locus.__new__(cls); l._init_from_java(jrep); l._contig = jrep.contig(); l._position = jrep.position(); return l. [docs] @staticmethod; @handle_py4j; @typecheck(string=strlike); def parse(string):; """"""Parses a locus object from a CHR:POS string. **Examples**. >>> l1 = Locus.parse('1:101230'); >>> l2 = Locus.parse('X:4201230'). :rtype: :class:`.Locus`; """""". return Locus._from_java(scala_object(Env.hail().variant, 'Locus').parse(string)). @property; def contig(self):; """"""; Chromosome identifier.; :rtype: str; """"""; return self._contig. @property; def position(self):; """"""; Chromosomal position (1-based).; :rtype: int; """"""; return self._position. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,docs/0.1/_modules/hail/representation/variant.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/variant.html
https://hail.is/docs/0.1/_modules/hail/representation/variant.html:532,Testability,test,testsetup,532,"﻿. . hail.representation.variant — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Module code »; hail.representation.variant. Source code for hail.representation.variant; from hail.java import scala_object, Env, handle_py4j; from hail.typecheck import *. [docs]class Variant(object):; """"""; An object that represents a genomic polymorphism. .. testsetup::. v_biallelic = Variant.parse('16:20012:A:TT'); v_multiallelic = Variant.parse('16:12311:T:C,TTT,A'). :param contig: chromosome identifier; :type contig: str or int; :param int start: chromosomal position (1-based); :param str ref: reference allele; :param alts: single alternate allele, or list of alternate alleles; :type alts: str or list of str; """""". @handle_py4j; def __init__(self, contig, start, ref, alts):; if isinstance(contig, int):; contig = str(contig); jrep = scala_object(Env.hail().variant, 'Variant').apply(contig, start, ref, alts); self._init_from_java(jrep); self._contig = contig; self._start = start; self._ref = ref. def __str__(self):; return self._jrep.toString(). def __repr__(self):; return 'Variant(contig=%s, start=%s, ref=%s, alts=%s)' % (self.contig, self.start, self.ref, self._alt_alleles). def __eq__(self, other):; return self._jrep.equals(other._jrep). def __hash__(self):; return self._jrep.hashCode(). def _init_from_java(self, jrep):; self._jrep = jrep; self._alt_alleles = map(AltAllele._from_java, [jrep.altAlleles().apply(i) for i in xrange(jrep.nAltAlleles())]). @classmethod; def _from_java(cls, jrep):; v = Variant.__new__(cls); v._init_from_java(jrep); v._contig = jrep.contig(); v._start = jrep.start(); v._ref = jrep.ref(); return v. [docs] @staticmethod; @handle_py4j; @typecheck(string=strlike); def parse(string):; """"""Parses a variant object from a string. There are two acceptable formats: CHR:POS:REF:ALT,",MatchSource.WIKI,docs/0.1/_modules/hail/representation/variant.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/variant.html
https://hail.is/docs/0.2/cloud/amazon_web_services.html:891,Deployability,update,updated,891,"﻿. Hail | ; Amazon Web Services. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; General Advice; Query-on-Batch; Google Cloud; Microsoft Azure; Amazon Web Services; Databricks. Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail on the Cloud; Amazon Web Services. View page source. Amazon Web Services; While Hail does not have any built-in tools for working with Amazon EMR, there are two approaches maintained by third parties:. AWS maintains a Hail on AWS quickstart.; The Avillach Lab at Harvard Medical School maintains an open-source tool. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/cloud/amazon_web_services.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/cloud/amazon_web_services.html
https://hail.is/docs/0.2/cloud/azure.html:2062,Availability,down,down,2062,"ht.net/jupyter . The Jupyter server is protected by a; username-password combination. The username and password are printed to the terminal after the; cluster is created.; Every HDInsight cluster is associated with one storage account which your Jupyter notebooks may; access. In addition, HDInsight will create a container within this storage account (sharing a name; with the cluster) for its own purposes. When a cluster is stopped using hailctl hdinsight stop,; this container will be deleted.; To start a cluster, you must specify the cluster name, a storage account, and a resource group. The; storage account must be in the given resource group.; hailctl hdinsight start CLUSTER_NAME STORAGE_ACCOUNT RESOURCE_GROUP. To submit a Python job to that cluster, use:; hailctl hdinsight submit CLUSTER_NAME STORAGE_ACCOUNT HTTP_PASSWORD SCRIPT [optional args to your python script...]. To list running clusters:; hailctl hdinsight list. Importantly, to shut down a cluster when done with it, use:; hailctl hdinsight stop CLUSTER_NAME STORAGE_ACCOUNT RESOURCE_GROUP. Variant Effect Predictor (VEP); The following cluster configuration enables Hail to run VEP in parallel on every; variant in a dataset containing GRCh37 variants:; hailctl hdinsight start CLUSTER_NAME STORAGE_ACCOUNT RESOURCE_GROUP \; --vep GRCh37 \; --vep-loftee-uri https://STORAGE_ACCOUNT.blob.core.windows.net/CONTAINER/loftee-GRCh37 \; --vep-homo-sapiens-uri https://STORAGE_ACCOUNT.blob.core.windows.net/CONTAINER/homo-sapiens-GRCh37. Those two URIs must point at directories containing the VEP data files. You can populate them by; downloading the two tar files using gcloud storage cp,; gs://hail-us-central1-vep/loftee-beta/GRCh37.tar and gs://hail-us-central1-vep/homo-sapiens/85_GRCh37.tar,; extracting them into a local folder, and uploading that folder to your storage account using az; storage copy. The hail-us-central1-vep Google Cloud Storage bucket is a requester pays bucket which means; you must pay the cost of tr",MatchSource.WIKI,docs/0.2/cloud/azure.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/cloud/azure.html
https://hail.is/docs/0.2/cloud/azure.html:2710,Availability,down,downloading,2710,"ge account, and a resource group. The; storage account must be in the given resource group.; hailctl hdinsight start CLUSTER_NAME STORAGE_ACCOUNT RESOURCE_GROUP. To submit a Python job to that cluster, use:; hailctl hdinsight submit CLUSTER_NAME STORAGE_ACCOUNT HTTP_PASSWORD SCRIPT [optional args to your python script...]. To list running clusters:; hailctl hdinsight list. Importantly, to shut down a cluster when done with it, use:; hailctl hdinsight stop CLUSTER_NAME STORAGE_ACCOUNT RESOURCE_GROUP. Variant Effect Predictor (VEP); The following cluster configuration enables Hail to run VEP in parallel on every; variant in a dataset containing GRCh37 variants:; hailctl hdinsight start CLUSTER_NAME STORAGE_ACCOUNT RESOURCE_GROUP \; --vep GRCh37 \; --vep-loftee-uri https://STORAGE_ACCOUNT.blob.core.windows.net/CONTAINER/loftee-GRCh37 \; --vep-homo-sapiens-uri https://STORAGE_ACCOUNT.blob.core.windows.net/CONTAINER/homo-sapiens-GRCh37. Those two URIs must point at directories containing the VEP data files. You can populate them by; downloading the two tar files using gcloud storage cp,; gs://hail-us-central1-vep/loftee-beta/GRCh37.tar and gs://hail-us-central1-vep/homo-sapiens/85_GRCh37.tar,; extracting them into a local folder, and uploading that folder to your storage account using az; storage copy. The hail-us-central1-vep Google Cloud Storage bucket is a requester pays bucket which means; you must pay the cost of transferring them out of Google Cloud. We do not provide these files in; Azure because Azure Blob Storage lacks an equivalent cost control mechanism.; Hail also supports VEP for GRCh38 variants. The required tar files are located at; gs://hail-REGION-vep/loftee-beta/GRCh38.tar and; gs://hail-REGION-vep/homo-sapiens/95_GRCh38.tar.; A cluster started without the --vep argument is unable to run VEP and cannot be modified to run; VEP. You must start a new cluster using --vep. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/cloud/azure.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/cloud/azure.html
https://hail.is/docs/0.2/cloud/azure.html:676,Deployability,install,installations,676,"﻿. Hail | ; Microsoft Azure. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; General Advice; Query-on-Batch; Google Cloud; Microsoft Azure; hailctl hdinsight; Variant Effect Predictor (VEP). Amazon Web Services; Databricks. Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail on the Cloud; Microsoft Azure. View page source. Microsoft Azure. hailctl hdinsight; As of version 0.2.82, pip installations of Hail come bundled with a command-line tool, hailctl; hdinsight for working with Microsoft Azure HDInsight Spark clusters configured for; Hail.; This tool requires the Azure CLI.; An HDInsight cluster always consists of two “head” nodes, two or more “worker” nodes, and an Azure; Blob Storage container. The head nodes are automatically configured to serve Jupyter Notebooks at; https://CLUSTER_NAME.azurehdinsight.net/jupyter . The Jupyter server is protected by a; username-password combination. The username and password are printed to the terminal after the; cluster is created.; Every HDInsight cluster is associated with one storage account which your Jupyter notebooks may; access. In addition, HDInsight will create a container within this storage account (sharing a name; with the cluster) for its own purposes. When a cluster is stopped using hailctl hdinsight stop,; this container will be deleted.; To start a cluster, you must specify the cluster name, a storage account, and a resource group. The; storage account must be in the given resource group.; hailctl hdinsight start CLUSTER_NAME STORAGE_ACCOUNT RESOURCE_GROUP. To submit a Python job to that cluster, use:; hailctl hdinsight submit CLUSTER_NAME STORAGE_ACCOUNT HTTP_PASSWORD SCRIPT [optional args to your python script...]. To list run",MatchSource.WIKI,docs/0.2/cloud/azure.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/cloud/azure.html
https://hail.is/docs/0.2/cloud/azure.html:2225,Deployability,configurat,configuration,2225,"th one storage account which your Jupyter notebooks may; access. In addition, HDInsight will create a container within this storage account (sharing a name; with the cluster) for its own purposes. When a cluster is stopped using hailctl hdinsight stop,; this container will be deleted.; To start a cluster, you must specify the cluster name, a storage account, and a resource group. The; storage account must be in the given resource group.; hailctl hdinsight start CLUSTER_NAME STORAGE_ACCOUNT RESOURCE_GROUP. To submit a Python job to that cluster, use:; hailctl hdinsight submit CLUSTER_NAME STORAGE_ACCOUNT HTTP_PASSWORD SCRIPT [optional args to your python script...]. To list running clusters:; hailctl hdinsight list. Importantly, to shut down a cluster when done with it, use:; hailctl hdinsight stop CLUSTER_NAME STORAGE_ACCOUNT RESOURCE_GROUP. Variant Effect Predictor (VEP); The following cluster configuration enables Hail to run VEP in parallel on every; variant in a dataset containing GRCh37 variants:; hailctl hdinsight start CLUSTER_NAME STORAGE_ACCOUNT RESOURCE_GROUP \; --vep GRCh37 \; --vep-loftee-uri https://STORAGE_ACCOUNT.blob.core.windows.net/CONTAINER/loftee-GRCh37 \; --vep-homo-sapiens-uri https://STORAGE_ACCOUNT.blob.core.windows.net/CONTAINER/homo-sapiens-GRCh37. Those two URIs must point at directories containing the VEP data files. You can populate them by; downloading the two tar files using gcloud storage cp,; gs://hail-us-central1-vep/loftee-beta/GRCh37.tar and gs://hail-us-central1-vep/homo-sapiens/85_GRCh37.tar,; extracting them into a local folder, and uploading that folder to your storage account using az; storage copy. The hail-us-central1-vep Google Cloud Storage bucket is a requester pays bucket which means; you must pay the cost of transferring them out of Google Cloud. We do not provide these files in; Azure because Azure Blob Storage lacks an equivalent cost control mechanism.; Hail also supports VEP for GRCh38 variants. The required tar f",MatchSource.WIKI,docs/0.2/cloud/azure.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/cloud/azure.html
https://hail.is/docs/0.2/cloud/azure.html:3637,Deployability,update,updated,3637,"ge account, and a resource group. The; storage account must be in the given resource group.; hailctl hdinsight start CLUSTER_NAME STORAGE_ACCOUNT RESOURCE_GROUP. To submit a Python job to that cluster, use:; hailctl hdinsight submit CLUSTER_NAME STORAGE_ACCOUNT HTTP_PASSWORD SCRIPT [optional args to your python script...]. To list running clusters:; hailctl hdinsight list. Importantly, to shut down a cluster when done with it, use:; hailctl hdinsight stop CLUSTER_NAME STORAGE_ACCOUNT RESOURCE_GROUP. Variant Effect Predictor (VEP); The following cluster configuration enables Hail to run VEP in parallel on every; variant in a dataset containing GRCh37 variants:; hailctl hdinsight start CLUSTER_NAME STORAGE_ACCOUNT RESOURCE_GROUP \; --vep GRCh37 \; --vep-loftee-uri https://STORAGE_ACCOUNT.blob.core.windows.net/CONTAINER/loftee-GRCh37 \; --vep-homo-sapiens-uri https://STORAGE_ACCOUNT.blob.core.windows.net/CONTAINER/homo-sapiens-GRCh37. Those two URIs must point at directories containing the VEP data files. You can populate them by; downloading the two tar files using gcloud storage cp,; gs://hail-us-central1-vep/loftee-beta/GRCh37.tar and gs://hail-us-central1-vep/homo-sapiens/85_GRCh37.tar,; extracting them into a local folder, and uploading that folder to your storage account using az; storage copy. The hail-us-central1-vep Google Cloud Storage bucket is a requester pays bucket which means; you must pay the cost of transferring them out of Google Cloud. We do not provide these files in; Azure because Azure Blob Storage lacks an equivalent cost control mechanism.; Hail also supports VEP for GRCh38 variants. The required tar files are located at; gs://hail-REGION-vep/loftee-beta/GRCh38.tar and; gs://hail-REGION-vep/homo-sapiens/95_GRCh38.tar.; A cluster started without the --vep argument is unable to run VEP and cannot be modified to run; VEP. You must start a new cluster using --vep. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/cloud/azure.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/cloud/azure.html
https://hail.is/docs/0.2/cloud/azure.html:814,Modifiability,config,configured,814,"﻿. Hail | ; Microsoft Azure. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; General Advice; Query-on-Batch; Google Cloud; Microsoft Azure; hailctl hdinsight; Variant Effect Predictor (VEP). Amazon Web Services; Databricks. Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail on the Cloud; Microsoft Azure. View page source. Microsoft Azure. hailctl hdinsight; As of version 0.2.82, pip installations of Hail come bundled with a command-line tool, hailctl; hdinsight for working with Microsoft Azure HDInsight Spark clusters configured for; Hail.; This tool requires the Azure CLI.; An HDInsight cluster always consists of two “head” nodes, two or more “worker” nodes, and an Azure; Blob Storage container. The head nodes are automatically configured to serve Jupyter Notebooks at; https://CLUSTER_NAME.azurehdinsight.net/jupyter . The Jupyter server is protected by a; username-password combination. The username and password are printed to the terminal after the; cluster is created.; Every HDInsight cluster is associated with one storage account which your Jupyter notebooks may; access. In addition, HDInsight will create a container within this storage account (sharing a name; with the cluster) for its own purposes. When a cluster is stopped using hailctl hdinsight stop,; this container will be deleted.; To start a cluster, you must specify the cluster name, a storage account, and a resource group. The; storage account must be in the given resource group.; hailctl hdinsight start CLUSTER_NAME STORAGE_ACCOUNT RESOURCE_GROUP. To submit a Python job to that cluster, use:; hailctl hdinsight submit CLUSTER_NAME STORAGE_ACCOUNT HTTP_PASSWORD SCRIPT [optional args to your python script...]. To list run",MatchSource.WIKI,docs/0.2/cloud/azure.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/cloud/azure.html
https://hail.is/docs/0.2/cloud/azure.html:1029,Modifiability,config,configured,1029,"Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; General Advice; Query-on-Batch; Google Cloud; Microsoft Azure; hailctl hdinsight; Variant Effect Predictor (VEP). Amazon Web Services; Databricks. Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail on the Cloud; Microsoft Azure. View page source. Microsoft Azure. hailctl hdinsight; As of version 0.2.82, pip installations of Hail come bundled with a command-line tool, hailctl; hdinsight for working with Microsoft Azure HDInsight Spark clusters configured for; Hail.; This tool requires the Azure CLI.; An HDInsight cluster always consists of two “head” nodes, two or more “worker” nodes, and an Azure; Blob Storage container. The head nodes are automatically configured to serve Jupyter Notebooks at; https://CLUSTER_NAME.azurehdinsight.net/jupyter . The Jupyter server is protected by a; username-password combination. The username and password are printed to the terminal after the; cluster is created.; Every HDInsight cluster is associated with one storage account which your Jupyter notebooks may; access. In addition, HDInsight will create a container within this storage account (sharing a name; with the cluster) for its own purposes. When a cluster is stopped using hailctl hdinsight stop,; this container will be deleted.; To start a cluster, you must specify the cluster name, a storage account, and a resource group. The; storage account must be in the given resource group.; hailctl hdinsight start CLUSTER_NAME STORAGE_ACCOUNT RESOURCE_GROUP. To submit a Python job to that cluster, use:; hailctl hdinsight submit CLUSTER_NAME STORAGE_ACCOUNT HTTP_PASSWORD SCRIPT [optional args to your python script...]. To list running clusters:; hailctl hdinsight list. Imp",MatchSource.WIKI,docs/0.2/cloud/azure.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/cloud/azure.html
https://hail.is/docs/0.2/cloud/azure.html:2225,Modifiability,config,configuration,2225,"th one storage account which your Jupyter notebooks may; access. In addition, HDInsight will create a container within this storage account (sharing a name; with the cluster) for its own purposes. When a cluster is stopped using hailctl hdinsight stop,; this container will be deleted.; To start a cluster, you must specify the cluster name, a storage account, and a resource group. The; storage account must be in the given resource group.; hailctl hdinsight start CLUSTER_NAME STORAGE_ACCOUNT RESOURCE_GROUP. To submit a Python job to that cluster, use:; hailctl hdinsight submit CLUSTER_NAME STORAGE_ACCOUNT HTTP_PASSWORD SCRIPT [optional args to your python script...]. To list running clusters:; hailctl hdinsight list. Importantly, to shut down a cluster when done with it, use:; hailctl hdinsight stop CLUSTER_NAME STORAGE_ACCOUNT RESOURCE_GROUP. Variant Effect Predictor (VEP); The following cluster configuration enables Hail to run VEP in parallel on every; variant in a dataset containing GRCh37 variants:; hailctl hdinsight start CLUSTER_NAME STORAGE_ACCOUNT RESOURCE_GROUP \; --vep GRCh37 \; --vep-loftee-uri https://STORAGE_ACCOUNT.blob.core.windows.net/CONTAINER/loftee-GRCh37 \; --vep-homo-sapiens-uri https://STORAGE_ACCOUNT.blob.core.windows.net/CONTAINER/homo-sapiens-GRCh37. Those two URIs must point at directories containing the VEP data files. You can populate them by; downloading the two tar files using gcloud storage cp,; gs://hail-us-central1-vep/loftee-beta/GRCh37.tar and gs://hail-us-central1-vep/homo-sapiens/85_GRCh37.tar,; extracting them into a local folder, and uploading that folder to your storage account using az; storage copy. The hail-us-central1-vep Google Cloud Storage bucket is a requester pays bucket which means; you must pay the cost of transferring them out of Google Cloud. We do not provide these files in; Azure because Azure Blob Storage lacks an equivalent cost control mechanism.; Hail also supports VEP for GRCh38 variants. The required tar f",MatchSource.WIKI,docs/0.2/cloud/azure.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/cloud/azure.html
https://hail.is/docs/0.2/cloud/azure.html:1168,Security,password,password,1168,"nstallation; Hail on the Cloud; General Advice; Query-on-Batch; Google Cloud; Microsoft Azure; hailctl hdinsight; Variant Effect Predictor (VEP). Amazon Web Services; Databricks. Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail on the Cloud; Microsoft Azure. View page source. Microsoft Azure. hailctl hdinsight; As of version 0.2.82, pip installations of Hail come bundled with a command-line tool, hailctl; hdinsight for working with Microsoft Azure HDInsight Spark clusters configured for; Hail.; This tool requires the Azure CLI.; An HDInsight cluster always consists of two “head” nodes, two or more “worker” nodes, and an Azure; Blob Storage container. The head nodes are automatically configured to serve Jupyter Notebooks at; https://CLUSTER_NAME.azurehdinsight.net/jupyter . The Jupyter server is protected by a; username-password combination. The username and password are printed to the terminal after the; cluster is created.; Every HDInsight cluster is associated with one storage account which your Jupyter notebooks may; access. In addition, HDInsight will create a container within this storage account (sharing a name; with the cluster) for its own purposes. When a cluster is stopped using hailctl hdinsight stop,; this container will be deleted.; To start a cluster, you must specify the cluster name, a storage account, and a resource group. The; storage account must be in the given resource group.; hailctl hdinsight start CLUSTER_NAME STORAGE_ACCOUNT RESOURCE_GROUP. To submit a Python job to that cluster, use:; hailctl hdinsight submit CLUSTER_NAME STORAGE_ACCOUNT HTTP_PASSWORD SCRIPT [optional args to your python script...]. To list running clusters:; hailctl hdinsight list. Importantly, to shut down a cluster when done with it, use:; hailctl hdinsight stop CLUSTER_NAME STORAGE_ACCOUNT R",MatchSource.WIKI,docs/0.2/cloud/azure.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/cloud/azure.html
https://hail.is/docs/0.2/cloud/azure.html:1207,Security,password,password,1207," Microsoft Azure; hailctl hdinsight; Variant Effect Predictor (VEP). Amazon Web Services; Databricks. Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail on the Cloud; Microsoft Azure. View page source. Microsoft Azure. hailctl hdinsight; As of version 0.2.82, pip installations of Hail come bundled with a command-line tool, hailctl; hdinsight for working with Microsoft Azure HDInsight Spark clusters configured for; Hail.; This tool requires the Azure CLI.; An HDInsight cluster always consists of two “head” nodes, two or more “worker” nodes, and an Azure; Blob Storage container. The head nodes are automatically configured to serve Jupyter Notebooks at; https://CLUSTER_NAME.azurehdinsight.net/jupyter . The Jupyter server is protected by a; username-password combination. The username and password are printed to the terminal after the; cluster is created.; Every HDInsight cluster is associated with one storage account which your Jupyter notebooks may; access. In addition, HDInsight will create a container within this storage account (sharing a name; with the cluster) for its own purposes. When a cluster is stopped using hailctl hdinsight stop,; this container will be deleted.; To start a cluster, you must specify the cluster name, a storage account, and a resource group. The; storage account must be in the given resource group.; hailctl hdinsight start CLUSTER_NAME STORAGE_ACCOUNT RESOURCE_GROUP. To submit a Python job to that cluster, use:; hailctl hdinsight submit CLUSTER_NAME STORAGE_ACCOUNT HTTP_PASSWORD SCRIPT [optional args to your python script...]. To list running clusters:; hailctl hdinsight list. Importantly, to shut down a cluster when done with it, use:; hailctl hdinsight stop CLUSTER_NAME STORAGE_ACCOUNT RESOURCE_GROUP. Variant Effect Predictor (VEP); The following cluster configu",MatchSource.WIKI,docs/0.2/cloud/azure.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/cloud/azure.html
https://hail.is/docs/0.2/cloud/azure.html:1373,Security,access,access,1373,"icks. Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail on the Cloud; Microsoft Azure. View page source. Microsoft Azure. hailctl hdinsight; As of version 0.2.82, pip installations of Hail come bundled with a command-line tool, hailctl; hdinsight for working with Microsoft Azure HDInsight Spark clusters configured for; Hail.; This tool requires the Azure CLI.; An HDInsight cluster always consists of two “head” nodes, two or more “worker” nodes, and an Azure; Blob Storage container. The head nodes are automatically configured to serve Jupyter Notebooks at; https://CLUSTER_NAME.azurehdinsight.net/jupyter . The Jupyter server is protected by a; username-password combination. The username and password are printed to the terminal after the; cluster is created.; Every HDInsight cluster is associated with one storage account which your Jupyter notebooks may; access. In addition, HDInsight will create a container within this storage account (sharing a name; with the cluster) for its own purposes. When a cluster is stopped using hailctl hdinsight stop,; this container will be deleted.; To start a cluster, you must specify the cluster name, a storage account, and a resource group. The; storage account must be in the given resource group.; hailctl hdinsight start CLUSTER_NAME STORAGE_ACCOUNT RESOURCE_GROUP. To submit a Python job to that cluster, use:; hailctl hdinsight submit CLUSTER_NAME STORAGE_ACCOUNT HTTP_PASSWORD SCRIPT [optional args to your python script...]. To list running clusters:; hailctl hdinsight list. Importantly, to shut down a cluster when done with it, use:; hailctl hdinsight stop CLUSTER_NAME STORAGE_ACCOUNT RESOURCE_GROUP. Variant Effect Predictor (VEP); The following cluster configuration enables Hail to run VEP in parallel on every; variant in a dataset containing GRCh37 var",MatchSource.WIKI,docs/0.2/cloud/azure.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/cloud/azure.html
https://hail.is/docs/0.2/cloud/databricks.html:1200,Availability,avail,available,1200,"Hail on the Cloud; General Advice; Query-on-Batch; Google Cloud; Microsoft Azure; Amazon Web Services; Databricks; Use Hail in a notebook; Initialize Hail; Display Bokeh plots. Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail on the Cloud; Databricks. View page source. Databricks; The docker images described below are maintained by Databricks. Please direct questions about them; to Databricks.; Hail can be installed on a Databricks Spark cluster on Microsoft Azure, Amazon Web Services, or; Google Cloud Platform via an open source Docker container located in the Project Glow Dockerhub. Docker; files to build your own Hail container on Databricks can be found in the Glow Github repository.; Install Hail via Docker with Databricks Container Services.; Use the Docker Image URL, projectglow/databricks-hail:<hail_version>, replacing the tag with an; available Hail version. Please match the Databricks Runtime Spark version to the Spark version Hail; is built with. Use Hail in a notebook; For the most part, Hail in Databricks works identically to the Hail documentation. However, there; are a few modifications that are necessary for the Databricks environment. Initialize Hail; When initializing Hail, pass in the pre-created SparkContext and mark the initialization as; idempotent. This setting enables multiple Databricks notebooks to use the same Hail context. note:. Enable skip_logging_configuration to save logs to the rolling driver log4j output. This; setting is supported only in Hail 0.2.39 and above.; Hail is not supported with Credential passthrough. code:; >>> import hail as hl; >>> hl.init(sc, idempotent=True, quiet=True, skip_logging_configuration=True) . Display Bokeh plots; Hail uses the Bokeh library to create plots. The show; function built into Bokeh does not work in Databricks. T",MatchSource.WIKI,docs/0.2/cloud/databricks.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/cloud/databricks.html
https://hail.is/docs/0.2/cloud/databricks.html:754,Deployability,install,installed,754,"﻿. Hail | ; Databricks. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; General Advice; Query-on-Batch; Google Cloud; Microsoft Azure; Amazon Web Services; Databricks; Use Hail in a notebook; Initialize Hail; Display Bokeh plots. Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail on the Cloud; Databricks. View page source. Databricks; The docker images described below are maintained by Databricks. Please direct questions about them; to Databricks.; Hail can be installed on a Databricks Spark cluster on Microsoft Azure, Amazon Web Services, or; Google Cloud Platform via an open source Docker container located in the Project Glow Dockerhub. Docker; files to build your own Hail container on Databricks can be found in the Glow Github repository.; Install Hail via Docker with Databricks Container Services.; Use the Docker Image URL, projectglow/databricks-hail:<hail_version>, replacing the tag with an; available Hail version. Please match the Databricks Runtime Spark version to the Spark version Hail; is built with. Use Hail in a notebook; For the most part, Hail in Databricks works identically to the Hail documentation. However, there; are a few modifications that are necessary for the Databricks environment. Initialize Hail; When initializing Hail, pass in the pre-created SparkContext and mark the initialization as; idempotent. This setting enables multiple Databricks notebooks to use the same Hail context. note:. Enable skip_logging_configuration to save logs to the rolling driver log4j output. This; setting is supported only in Hail 0.2.39 and above.; Hail is not supported with Credential passthrough. code:; >>> import hail as hl; >>> hl.init(sc, idempotent=True, quiet=True, skip_lo",MatchSource.WIKI,docs/0.2/cloud/databricks.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/cloud/databricks.html
https://hail.is/docs/0.2/cloud/databricks.html:1780,Deployability,rolling,rolling,1780,"ed by Databricks. Please direct questions about them; to Databricks.; Hail can be installed on a Databricks Spark cluster on Microsoft Azure, Amazon Web Services, or; Google Cloud Platform via an open source Docker container located in the Project Glow Dockerhub. Docker; files to build your own Hail container on Databricks can be found in the Glow Github repository.; Install Hail via Docker with Databricks Container Services.; Use the Docker Image URL, projectglow/databricks-hail:<hail_version>, replacing the tag with an; available Hail version. Please match the Databricks Runtime Spark version to the Spark version Hail; is built with. Use Hail in a notebook; For the most part, Hail in Databricks works identically to the Hail documentation. However, there; are a few modifications that are necessary for the Databricks environment. Initialize Hail; When initializing Hail, pass in the pre-created SparkContext and mark the initialization as; idempotent. This setting enables multiple Databricks notebooks to use the same Hail context. note:. Enable skip_logging_configuration to save logs to the rolling driver log4j output. This; setting is supported only in Hail 0.2.39 and above.; Hail is not supported with Credential passthrough. code:; >>> import hail as hl; >>> hl.init(sc, idempotent=True, quiet=True, skip_logging_configuration=True) . Display Bokeh plots; Hail uses the Bokeh library to create plots. The show; function built into Bokeh does not work in Databricks. To display a Bokeh plot generated by Hail,; you can run a command like:; >>> from bokeh.embed import components, file_html; >>> from bokeh.resources import CDN; >>> plot = hl.plot.histogram(mt.DP, range=(0,30), bins=30, title='DP Histogram', legend='DP'); >>> html = file_html(plot, CDN, ""Chart""). And then call the Databricks function displayHTML with html as its argument.; See Databricks’ Bokeh docs for; more information. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/cloud/databricks.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/cloud/databricks.html
https://hail.is/docs/0.2/cloud/databricks.html:2644,Deployability,update,updated,2644,"ed by Databricks. Please direct questions about them; to Databricks.; Hail can be installed on a Databricks Spark cluster on Microsoft Azure, Amazon Web Services, or; Google Cloud Platform via an open source Docker container located in the Project Glow Dockerhub. Docker; files to build your own Hail container on Databricks can be found in the Glow Github repository.; Install Hail via Docker with Databricks Container Services.; Use the Docker Image URL, projectglow/databricks-hail:<hail_version>, replacing the tag with an; available Hail version. Please match the Databricks Runtime Spark version to the Spark version Hail; is built with. Use Hail in a notebook; For the most part, Hail in Databricks works identically to the Hail documentation. However, there; are a few modifications that are necessary for the Databricks environment. Initialize Hail; When initializing Hail, pass in the pre-created SparkContext and mark the initialization as; idempotent. This setting enables multiple Databricks notebooks to use the same Hail context. note:. Enable skip_logging_configuration to save logs to the rolling driver log4j output. This; setting is supported only in Hail 0.2.39 and above.; Hail is not supported with Credential passthrough. code:; >>> import hail as hl; >>> hl.init(sc, idempotent=True, quiet=True, skip_logging_configuration=True) . Display Bokeh plots; Hail uses the Bokeh library to create plots. The show; function built into Bokeh does not work in Databricks. To display a Bokeh plot generated by Hail,; you can run a command like:; >>> from bokeh.embed import components, file_html; >>> from bokeh.resources import CDN; >>> plot = hl.plot.histogram(mt.DP, range=(0,30), bins=30, title='DP Histogram', legend='DP'); >>> html = file_html(plot, CDN, ""Chart""). And then call the Databricks function displayHTML with html as its argument.; See Databricks’ Bokeh docs for; more information. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/cloud/databricks.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/cloud/databricks.html
https://hail.is/docs/0.2/cloud/databricks.html:1768,Testability,log,logs,1768,"ed by Databricks. Please direct questions about them; to Databricks.; Hail can be installed on a Databricks Spark cluster on Microsoft Azure, Amazon Web Services, or; Google Cloud Platform via an open source Docker container located in the Project Glow Dockerhub. Docker; files to build your own Hail container on Databricks can be found in the Glow Github repository.; Install Hail via Docker with Databricks Container Services.; Use the Docker Image URL, projectglow/databricks-hail:<hail_version>, replacing the tag with an; available Hail version. Please match the Databricks Runtime Spark version to the Spark version Hail; is built with. Use Hail in a notebook; For the most part, Hail in Databricks works identically to the Hail documentation. However, there; are a few modifications that are necessary for the Databricks environment. Initialize Hail; When initializing Hail, pass in the pre-created SparkContext and mark the initialization as; idempotent. This setting enables multiple Databricks notebooks to use the same Hail context. note:. Enable skip_logging_configuration to save logs to the rolling driver log4j output. This; setting is supported only in Hail 0.2.39 and above.; Hail is not supported with Credential passthrough. code:; >>> import hail as hl; >>> hl.init(sc, idempotent=True, quiet=True, skip_logging_configuration=True) . Display Bokeh plots; Hail uses the Bokeh library to create plots. The show; function built into Bokeh does not work in Databricks. To display a Bokeh plot generated by Hail,; you can run a command like:; >>> from bokeh.embed import components, file_html; >>> from bokeh.resources import CDN; >>> plot = hl.plot.histogram(mt.DP, range=(0,30), bins=30, title='DP Histogram', legend='DP'); >>> html = file_html(plot, CDN, ""Chart""). And then call the Databricks function displayHTML with html as its argument.; See Databricks’ Bokeh docs for; more information. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/cloud/databricks.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/cloud/databricks.html
https://hail.is/docs/0.2/cloud/general_advice.html:4067,Deployability,update,updated,4067," This yields a reasonable expectation of the time; to compute results on the full dataset using a cluster of the same size. However, not all operations will scale this way. Certain complicated operations; like pca or BlockMatrix multiplies do not scale linearly. When doing small time estimates, it can sometimes be helpful to get a few datapoints as; you gradually increase the size of your small dataset to see if it’s scaling linearly. Estimating cost; Costs vary between cloud providers. This cost estimate is based on Google Cloud, but the same principles often apply to other providers.; Google charges by the core-hour, so we can convert so-called “wall clock time” (time elapsed from starting the cluster to stopping the cluster); to dollars-spent by multiplying it by the number of cores of each type and the price per core per hour of each type. At time of writing,; preemptible cores are 0.01 dollars per core hour and non-preemptible cores are 0.0475 dollars per core hour. Moreover, each core has an; additional 0.01 dollar “dataproc premium” fee. The cost of CPU cores for a cluster with an 8-core leader node; two non-preemptible, 8-core workers;; and 10 preemptible, 8-core workers running for 2 hours is:; 2 * (2 * 8 * 0.0575 + # non-preemptible workers; 10 * 8 * 0.02 + # preemptible workers; 1 * 8 * 0.0575) # leader (master) node. 2.98 USD.; There are additional charges for persistent disk and SSDs. If your leader node has 100 GB and your worker nodes have 40 GB each you can expect; a modest increase in cost, slightly less than a dollar. The cost per disk is prorated from a per-month rate; at time of writing it is 0.04 USD; per GB per month. SSDs are more than four times as expensive.; In general, once you know the wall clock time of your job, you can enter your cluster parameters into the; Google Cloud Pricing Calculator. and get a precise estimate; of cost using the latest prices. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/cloud/general_advice.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/cloud/general_advice.html
https://hail.is/docs/0.2/cloud/general_advice.html:2697,Energy Efficiency,charge,charges,2697," simple. Start a small cluster and use filter_rows to read a small fraction of the data:; test_mt = mt.filter_rows(mt.locus.contig == '22'); print(mt.count_rows() / test_mt.count_rows()). Multiply the time spent computing results on this smaller dataset by the number printed. This yields a reasonable expectation of the time; to compute results on the full dataset using a cluster of the same size. However, not all operations will scale this way. Certain complicated operations; like pca or BlockMatrix multiplies do not scale linearly. When doing small time estimates, it can sometimes be helpful to get a few datapoints as; you gradually increase the size of your small dataset to see if it’s scaling linearly. Estimating cost; Costs vary between cloud providers. This cost estimate is based on Google Cloud, but the same principles often apply to other providers.; Google charges by the core-hour, so we can convert so-called “wall clock time” (time elapsed from starting the cluster to stopping the cluster); to dollars-spent by multiplying it by the number of cores of each type and the price per core per hour of each type. At time of writing,; preemptible cores are 0.01 dollars per core hour and non-preemptible cores are 0.0475 dollars per core hour. Moreover, each core has an; additional 0.01 dollar “dataproc premium” fee. The cost of CPU cores for a cluster with an 8-core leader node; two non-preemptible, 8-core workers;; and 10 preemptible, 8-core workers running for 2 hours is:; 2 * (2 * 8 * 0.0575 + # non-preemptible workers; 10 * 8 * 0.02 + # preemptible workers; 1 * 8 * 0.0575) # leader (master) node. 2.98 USD.; There are additional charges for persistent disk and SSDs. If your leader node has 100 GB and your worker nodes have 40 GB each you can expect; a modest increase in cost, slightly less than a dollar. The cost per disk is prorated from a per-month rate; at time of writing it is 0.04 USD; per GB per month. SSDs are more than four times as expensive.; In general,",MatchSource.WIKI,docs/0.2/cloud/general_advice.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/cloud/general_advice.html
https://hail.is/docs/0.2/cloud/general_advice.html:3479,Energy Efficiency,charge,charges,3479," This yields a reasonable expectation of the time; to compute results on the full dataset using a cluster of the same size. However, not all operations will scale this way. Certain complicated operations; like pca or BlockMatrix multiplies do not scale linearly. When doing small time estimates, it can sometimes be helpful to get a few datapoints as; you gradually increase the size of your small dataset to see if it’s scaling linearly. Estimating cost; Costs vary between cloud providers. This cost estimate is based on Google Cloud, but the same principles often apply to other providers.; Google charges by the core-hour, so we can convert so-called “wall clock time” (time elapsed from starting the cluster to stopping the cluster); to dollars-spent by multiplying it by the number of cores of each type and the price per core per hour of each type. At time of writing,; preemptible cores are 0.01 dollars per core hour and non-preemptible cores are 0.0475 dollars per core hour. Moreover, each core has an; additional 0.01 dollar “dataproc premium” fee. The cost of CPU cores for a cluster with an 8-core leader node; two non-preemptible, 8-core workers;; and 10 preemptible, 8-core workers running for 2 hours is:; 2 * (2 * 8 * 0.0575 + # non-preemptible workers; 10 * 8 * 0.02 + # preemptible workers; 1 * 8 * 0.0575) # leader (master) node. 2.98 USD.; There are additional charges for persistent disk and SSDs. If your leader node has 100 GB and your worker nodes have 40 GB each you can expect; a modest increase in cost, slightly less than a dollar. The cost per disk is prorated from a per-month rate; at time of writing it is 0.04 USD; per GB per month. SSDs are more than four times as expensive.; In general, once you know the wall clock time of your job, you can enter your cluster parameters into the; Google Cloud Pricing Calculator. and get a precise estimate; of cost using the latest prices. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/cloud/general_advice.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/cloud/general_advice.html
https://hail.is/docs/0.2/cloud/general_advice.html:1431,Modifiability,config,configurable,1431,"ets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail on the Cloud; General Advice. View page source. General Advice. Start Small; The cloud has a reputation for easily burning lots of money. You don’t want to be the person who; spent ten thousand dollars one night without thinking about it. Luckily, it’s easy to not be that person!; Always start small. For Hail, this means using a two worker Spark cluster and experimenting on a small; fraction of the data. For genetic data, make sure your scripts work on chromosome 22 (the 2nd smallest autosomal chromosome) before; you try running on the entire genome! If you have a matrix table you can limit to chromosome 22 with filter_rows.; Hail will make sure not to load data for other chromosomes.; import hail as hl. mt = hl.read_matrix_table('gs://....'); mt = mt.filter_rows(mt.locus.contig == '22'). Hail’s hl.balding_nichols_model creates a random genotype dataset with configurable numbers of rows and columns.; You can use these datasets for experimentation.; As you’ll see later, the smallest Hail cluster (on GCP) costs about 3 dollars per hour. Each time you think you need to double; the size of your cluster ask yourself: am I prepared to spend twice as much money per hour?. Estimating time; Estimating the time and cost of a Hail operation is often simple. Start a small cluster and use filter_rows to read a small fraction of the data:; test_mt = mt.filter_rows(mt.locus.contig == '22'); print(mt.count_rows() / test_mt.count_rows()). Multiply the time spent computing results on this smaller dataset by the number printed. This yields a reasonable expectation of the time; to compute results on the full dataset using a cluster of the same size. However, not all operations will scale this way. Certain complicated operations; like pca or BlockMatrix multiplies do not scale linearly. When doing small time estimates, it can sometimes be helpful to",MatchSource.WIKI,docs/0.2/cloud/general_advice.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/cloud/general_advice.html
https://hail.is/docs/0.2/cloud/general_advice.html:1221,Performance,load,load,1221,"ng time; Estimating cost. Query-on-Batch; Google Cloud; Microsoft Azure; Amazon Web Services; Databricks. Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail on the Cloud; General Advice. View page source. General Advice. Start Small; The cloud has a reputation for easily burning lots of money. You don’t want to be the person who; spent ten thousand dollars one night without thinking about it. Luckily, it’s easy to not be that person!; Always start small. For Hail, this means using a two worker Spark cluster and experimenting on a small; fraction of the data. For genetic data, make sure your scripts work on chromosome 22 (the 2nd smallest autosomal chromosome) before; you try running on the entire genome! If you have a matrix table you can limit to chromosome 22 with filter_rows.; Hail will make sure not to load data for other chromosomes.; import hail as hl. mt = hl.read_matrix_table('gs://....'); mt = mt.filter_rows(mt.locus.contig == '22'). Hail’s hl.balding_nichols_model creates a random genotype dataset with configurable numbers of rows and columns.; You can use these datasets for experimentation.; As you’ll see later, the smallest Hail cluster (on GCP) costs about 3 dollars per hour. Each time you think you need to double; the size of your cluster ask yourself: am I prepared to spend twice as much money per hour?. Estimating time; Estimating the time and cost of a Hail operation is often simple. Start a small cluster and use filter_rows to read a small fraction of the data:; test_mt = mt.filter_rows(mt.locus.contig == '22'); print(mt.count_rows() / test_mt.count_rows()). Multiply the time spent computing results on this smaller dataset by the number printed. This yields a reasonable expectation of the time; to compute results on the full dataset using a cluster of the same size. Howe",MatchSource.WIKI,docs/0.2/cloud/general_advice.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/cloud/general_advice.html
https://hail.is/docs/0.2/cloud/general_advice.html:1820,Usability,simpl,simple,1820,"ing about it. Luckily, it’s easy to not be that person!; Always start small. For Hail, this means using a two worker Spark cluster and experimenting on a small; fraction of the data. For genetic data, make sure your scripts work on chromosome 22 (the 2nd smallest autosomal chromosome) before; you try running on the entire genome! If you have a matrix table you can limit to chromosome 22 with filter_rows.; Hail will make sure not to load data for other chromosomes.; import hail as hl. mt = hl.read_matrix_table('gs://....'); mt = mt.filter_rows(mt.locus.contig == '22'). Hail’s hl.balding_nichols_model creates a random genotype dataset with configurable numbers of rows and columns.; You can use these datasets for experimentation.; As you’ll see later, the smallest Hail cluster (on GCP) costs about 3 dollars per hour. Each time you think you need to double; the size of your cluster ask yourself: am I prepared to spend twice as much money per hour?. Estimating time; Estimating the time and cost of a Hail operation is often simple. Start a small cluster and use filter_rows to read a small fraction of the data:; test_mt = mt.filter_rows(mt.locus.contig == '22'); print(mt.count_rows() / test_mt.count_rows()). Multiply the time spent computing results on this smaller dataset by the number printed. This yields a reasonable expectation of the time; to compute results on the full dataset using a cluster of the same size. However, not all operations will scale this way. Certain complicated operations; like pca or BlockMatrix multiplies do not scale linearly. When doing small time estimates, it can sometimes be helpful to get a few datapoints as; you gradually increase the size of your small dataset to see if it’s scaling linearly. Estimating cost; Costs vary between cloud providers. This cost estimate is based on Google Cloud, but the same principles often apply to other providers.; Google charges by the core-hour, so we can convert so-called “wall clock time” (time elapsed fr",MatchSource.WIKI,docs/0.2/cloud/general_advice.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/cloud/general_advice.html
https://hail.is/docs/0.2/cloud/google_cloud.html:1813,Availability,down,down,1813,"b to cloud computing. hailctl dataproc; As of version 0.2.15, pip installations of Hail come bundled with a command-line; tool, hailctl. This tool has a submodule called dataproc for working with; Google Dataproc clusters configured for Hail.; This tool requires the Google Cloud SDK.; Until full documentation for the command-line interface is written, we encourage; you to run the following command to see the list of modules:; hailctl dataproc. It is possible to print help for a specific command using the help flag:; hailctl dataproc start --help. To start a cluster, use:; hailctl dataproc start CLUSTER_NAME [optional args...]. To submit a Python job to that cluster, use:; hailctl dataproc submit CLUSTER_NAME SCRIPT [optional args to your python script...]. To connect to a Jupyter notebook running on that cluster, use:; hailctl dataproc connect CLUSTER_NAME notebook [optional args...]. To list active clusters, use:; hailctl dataproc list. Importantly, to shut down a cluster when done with it, use:; hailctl dataproc stop CLUSTER_NAME. Reading from Google Cloud Storage; A dataproc cluster created through hailctl dataproc will automatically be configured to allow hail to read files from; Google Cloud Storage (GCS). To allow hail to read from GCS when running locally, you need to install the; Cloud Storage Connector. The easiest way to do that is to; run the following script from your command line:; curl -sSL https://broad.io/install-gcs-connector | python3. After this is installed, you’ll be able to read from paths beginning with gs directly from you laptop. Requester Pays; Some google cloud buckets are Requester Pays, meaning; that accessing them will incur charges on the requester. Google breaks down the charges in the linked document,; but the most important class of charges to be aware of are Network Charges.; Specifically, the egress charges. You should always be careful reading data from a bucket in a different region; then your own project, as it is easy to rac",MatchSource.WIKI,docs/0.2/cloud/google_cloud.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/cloud/google_cloud.html
https://hail.is/docs/0.2/cloud/google_cloud.html:2565,Availability,down,down,2565,"t to a Jupyter notebook running on that cluster, use:; hailctl dataproc connect CLUSTER_NAME notebook [optional args...]. To list active clusters, use:; hailctl dataproc list. Importantly, to shut down a cluster when done with it, use:; hailctl dataproc stop CLUSTER_NAME. Reading from Google Cloud Storage; A dataproc cluster created through hailctl dataproc will automatically be configured to allow hail to read files from; Google Cloud Storage (GCS). To allow hail to read from GCS when running locally, you need to install the; Cloud Storage Connector. The easiest way to do that is to; run the following script from your command line:; curl -sSL https://broad.io/install-gcs-connector | python3. After this is installed, you’ll be able to read from paths beginning with gs directly from you laptop. Requester Pays; Some google cloud buckets are Requester Pays, meaning; that accessing them will incur charges on the requester. Google breaks down the charges in the linked document,; but the most important class of charges to be aware of are Network Charges.; Specifically, the egress charges. You should always be careful reading data from a bucket in a different region; then your own project, as it is easy to rack up a large bill. For this reason, you must specifically enable; requester pays on your hailctl dataproc cluster if you’d like to use it.; To allow your cluster to read from any requester pays bucket, use:; hailctl dataproc start CLUSTER_NAME --requester-pays-allow-all. To make it easier to avoid accidentally reading from a requester pays bucket, we also have; --requester-pays-allow-buckets. If you’d like to enable only reading from buckets named; hail-bucket and big-data, you can specify the following:; hailctl dataproc start my-cluster --requester-pays-allow-buckets hail-bucket,big-data. Users of the Annotation Database will find that many of the files are stored in requester pays buckets.; In order to allow the dataproc cluster to read from them, you can either ",MatchSource.WIKI,docs/0.2/cloud/google_cloud.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/cloud/google_cloud.html
https://hail.is/docs/0.2/cloud/google_cloud.html:906,Deployability,install,installations,906,"﻿. Hail | ; Google Cloud Platform. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; General Advice; Query-on-Batch; Google Cloud; hailctl dataproc; Reading from Google Cloud Storage; Requester Pays; Variant Effect Predictor (VEP). Microsoft Azure; Amazon Web Services; Databricks. Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail on the Cloud; Google Cloud Platform. View page source. Google Cloud Platform; If you’re new to Google Cloud in general, and would like an overview, linked; here.; is a document written to onboard new users within our lab to cloud computing. hailctl dataproc; As of version 0.2.15, pip installations of Hail come bundled with a command-line; tool, hailctl. This tool has a submodule called dataproc for working with; Google Dataproc clusters configured for Hail.; This tool requires the Google Cloud SDK.; Until full documentation for the command-line interface is written, we encourage; you to run the following command to see the list of modules:; hailctl dataproc. It is possible to print help for a specific command using the help flag:; hailctl dataproc start --help. To start a cluster, use:; hailctl dataproc start CLUSTER_NAME [optional args...]. To submit a Python job to that cluster, use:; hailctl dataproc submit CLUSTER_NAME SCRIPT [optional args to your python script...]. To connect to a Jupyter notebook running on that cluster, use:; hailctl dataproc connect CLUSTER_NAME notebook [optional args...]. To list active clusters, use:; hailctl dataproc list. Importantly, to shut down a cluster when done with it, use:; hailctl dataproc stop CLUSTER_NAME. Reading from Google Cloud Storage; A dataproc cluster created through hailctl dataproc will automatically be co",MatchSource.WIKI,docs/0.2/cloud/google_cloud.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/cloud/google_cloud.html
https://hail.is/docs/0.2/cloud/google_cloud.html:2137,Deployability,install,install,2137,"K.; Until full documentation for the command-line interface is written, we encourage; you to run the following command to see the list of modules:; hailctl dataproc. It is possible to print help for a specific command using the help flag:; hailctl dataproc start --help. To start a cluster, use:; hailctl dataproc start CLUSTER_NAME [optional args...]. To submit a Python job to that cluster, use:; hailctl dataproc submit CLUSTER_NAME SCRIPT [optional args to your python script...]. To connect to a Jupyter notebook running on that cluster, use:; hailctl dataproc connect CLUSTER_NAME notebook [optional args...]. To list active clusters, use:; hailctl dataproc list. Importantly, to shut down a cluster when done with it, use:; hailctl dataproc stop CLUSTER_NAME. Reading from Google Cloud Storage; A dataproc cluster created through hailctl dataproc will automatically be configured to allow hail to read files from; Google Cloud Storage (GCS). To allow hail to read from GCS when running locally, you need to install the; Cloud Storage Connector. The easiest way to do that is to; run the following script from your command line:; curl -sSL https://broad.io/install-gcs-connector | python3. After this is installed, you’ll be able to read from paths beginning with gs directly from you laptop. Requester Pays; Some google cloud buckets are Requester Pays, meaning; that accessing them will incur charges on the requester. Google breaks down the charges in the linked document,; but the most important class of charges to be aware of are Network Charges.; Specifically, the egress charges. You should always be careful reading data from a bucket in a different region; then your own project, as it is easy to rack up a large bill. For this reason, you must specifically enable; requester pays on your hailctl dataproc cluster if you’d like to use it.; To allow your cluster to read from any requester pays bucket, use:; hailctl dataproc start CLUSTER_NAME --requester-pays-allow-all. To make it ",MatchSource.WIKI,docs/0.2/cloud/google_cloud.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/cloud/google_cloud.html
https://hail.is/docs/0.2/cloud/google_cloud.html:2286,Deployability,install,install-gcs-connector,2286,"le to print help for a specific command using the help flag:; hailctl dataproc start --help. To start a cluster, use:; hailctl dataproc start CLUSTER_NAME [optional args...]. To submit a Python job to that cluster, use:; hailctl dataproc submit CLUSTER_NAME SCRIPT [optional args to your python script...]. To connect to a Jupyter notebook running on that cluster, use:; hailctl dataproc connect CLUSTER_NAME notebook [optional args...]. To list active clusters, use:; hailctl dataproc list. Importantly, to shut down a cluster when done with it, use:; hailctl dataproc stop CLUSTER_NAME. Reading from Google Cloud Storage; A dataproc cluster created through hailctl dataproc will automatically be configured to allow hail to read files from; Google Cloud Storage (GCS). To allow hail to read from GCS when running locally, you need to install the; Cloud Storage Connector. The easiest way to do that is to; run the following script from your command line:; curl -sSL https://broad.io/install-gcs-connector | python3. After this is installed, you’ll be able to read from paths beginning with gs directly from you laptop. Requester Pays; Some google cloud buckets are Requester Pays, meaning; that accessing them will incur charges on the requester. Google breaks down the charges in the linked document,; but the most important class of charges to be aware of are Network Charges.; Specifically, the egress charges. You should always be careful reading data from a bucket in a different region; then your own project, as it is easy to rack up a large bill. For this reason, you must specifically enable; requester pays on your hailctl dataproc cluster if you’d like to use it.; To allow your cluster to read from any requester pays bucket, use:; hailctl dataproc start CLUSTER_NAME --requester-pays-allow-all. To make it easier to avoid accidentally reading from a requester pays bucket, we also have; --requester-pays-allow-buckets. If you’d like to enable only reading from buckets named; hail-buc",MatchSource.WIKI,docs/0.2/cloud/google_cloud.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/cloud/google_cloud.html
https://hail.is/docs/0.2/cloud/google_cloud.html:2333,Deployability,install,installed,2333," dataproc start --help. To start a cluster, use:; hailctl dataproc start CLUSTER_NAME [optional args...]. To submit a Python job to that cluster, use:; hailctl dataproc submit CLUSTER_NAME SCRIPT [optional args to your python script...]. To connect to a Jupyter notebook running on that cluster, use:; hailctl dataproc connect CLUSTER_NAME notebook [optional args...]. To list active clusters, use:; hailctl dataproc list. Importantly, to shut down a cluster when done with it, use:; hailctl dataproc stop CLUSTER_NAME. Reading from Google Cloud Storage; A dataproc cluster created through hailctl dataproc will automatically be configured to allow hail to read files from; Google Cloud Storage (GCS). To allow hail to read from GCS when running locally, you need to install the; Cloud Storage Connector. The easiest way to do that is to; run the following script from your command line:; curl -sSL https://broad.io/install-gcs-connector | python3. After this is installed, you’ll be able to read from paths beginning with gs directly from you laptop. Requester Pays; Some google cloud buckets are Requester Pays, meaning; that accessing them will incur charges on the requester. Google breaks down the charges in the linked document,; but the most important class of charges to be aware of are Network Charges.; Specifically, the egress charges. You should always be careful reading data from a bucket in a different region; then your own project, as it is easy to rack up a large bill. For this reason, you must specifically enable; requester pays on your hailctl dataproc cluster if you’d like to use it.; To allow your cluster to read from any requester pays bucket, use:; hailctl dataproc start CLUSTER_NAME --requester-pays-allow-all. To make it easier to avoid accidentally reading from a requester pays bucket, we also have; --requester-pays-allow-buckets. If you’d like to enable only reading from buckets named; hail-bucket and big-data, you can specify the following:; hailctl dataproc st",MatchSource.WIKI,docs/0.2/cloud/google_cloud.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/cloud/google_cloud.html
https://hail.is/docs/0.2/cloud/google_cloud.html:3850,Deployability,configurat,configuration,3850,"lled, you’ll be able to read from paths beginning with gs directly from you laptop. Requester Pays; Some google cloud buckets are Requester Pays, meaning; that accessing them will incur charges on the requester. Google breaks down the charges in the linked document,; but the most important class of charges to be aware of are Network Charges.; Specifically, the egress charges. You should always be careful reading data from a bucket in a different region; then your own project, as it is easy to rack up a large bill. For this reason, you must specifically enable; requester pays on your hailctl dataproc cluster if you’d like to use it.; To allow your cluster to read from any requester pays bucket, use:; hailctl dataproc start CLUSTER_NAME --requester-pays-allow-all. To make it easier to avoid accidentally reading from a requester pays bucket, we also have; --requester-pays-allow-buckets. If you’d like to enable only reading from buckets named; hail-bucket and big-data, you can specify the following:; hailctl dataproc start my-cluster --requester-pays-allow-buckets hail-bucket,big-data. Users of the Annotation Database will find that many of the files are stored in requester pays buckets.; In order to allow the dataproc cluster to read from them, you can either use --requester-pays-allow-all from above; or use the special --requester-pays-allow-annotation-db to enable the specific list of buckets that the annotation database; relies on. Variant Effect Predictor (VEP); The following cluster configuration enables Hail to run VEP in parallel on every; variant in a dataset containing GRCh37 variants:; hailctl dataproc start NAME --vep GRCh37. Hail also supports VEP for GRCh38 variants, but you must start a cluster with; the argument --vep GRCh38. A cluster started without the --vep argument is; unable to run VEP and cannot be modified to run VEP. You must start a new; cluster using --vep. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/cloud/google_cloud.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/cloud/google_cloud.html
https://hail.is/docs/0.2/cloud/google_cloud.html:4310,Deployability,update,updated,4310,"lled, you’ll be able to read from paths beginning with gs directly from you laptop. Requester Pays; Some google cloud buckets are Requester Pays, meaning; that accessing them will incur charges on the requester. Google breaks down the charges in the linked document,; but the most important class of charges to be aware of are Network Charges.; Specifically, the egress charges. You should always be careful reading data from a bucket in a different region; then your own project, as it is easy to rack up a large bill. For this reason, you must specifically enable; requester pays on your hailctl dataproc cluster if you’d like to use it.; To allow your cluster to read from any requester pays bucket, use:; hailctl dataproc start CLUSTER_NAME --requester-pays-allow-all. To make it easier to avoid accidentally reading from a requester pays bucket, we also have; --requester-pays-allow-buckets. If you’d like to enable only reading from buckets named; hail-bucket and big-data, you can specify the following:; hailctl dataproc start my-cluster --requester-pays-allow-buckets hail-bucket,big-data. Users of the Annotation Database will find that many of the files are stored in requester pays buckets.; In order to allow the dataproc cluster to read from them, you can either use --requester-pays-allow-all from above; or use the special --requester-pays-allow-annotation-db to enable the specific list of buckets that the annotation database; relies on. Variant Effect Predictor (VEP); The following cluster configuration enables Hail to run VEP in parallel on every; variant in a dataset containing GRCh37 variants:; hailctl dataproc start NAME --vep GRCh37. Hail also supports VEP for GRCh38 variants, but you must start a cluster with; the argument --vep GRCh38. A cluster started without the --vep argument is; unable to run VEP and cannot be modified to run VEP. You must start a new; cluster using --vep. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/cloud/google_cloud.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/cloud/google_cloud.html
https://hail.is/docs/0.2/cloud/google_cloud.html:2525,Energy Efficiency,charge,charges,2525,"a Python job to that cluster, use:; hailctl dataproc submit CLUSTER_NAME SCRIPT [optional args to your python script...]. To connect to a Jupyter notebook running on that cluster, use:; hailctl dataproc connect CLUSTER_NAME notebook [optional args...]. To list active clusters, use:; hailctl dataproc list. Importantly, to shut down a cluster when done with it, use:; hailctl dataproc stop CLUSTER_NAME. Reading from Google Cloud Storage; A dataproc cluster created through hailctl dataproc will automatically be configured to allow hail to read files from; Google Cloud Storage (GCS). To allow hail to read from GCS when running locally, you need to install the; Cloud Storage Connector. The easiest way to do that is to; run the following script from your command line:; curl -sSL https://broad.io/install-gcs-connector | python3. After this is installed, you’ll be able to read from paths beginning with gs directly from you laptop. Requester Pays; Some google cloud buckets are Requester Pays, meaning; that accessing them will incur charges on the requester. Google breaks down the charges in the linked document,; but the most important class of charges to be aware of are Network Charges.; Specifically, the egress charges. You should always be careful reading data from a bucket in a different region; then your own project, as it is easy to rack up a large bill. For this reason, you must specifically enable; requester pays on your hailctl dataproc cluster if you’d like to use it.; To allow your cluster to read from any requester pays bucket, use:; hailctl dataproc start CLUSTER_NAME --requester-pays-allow-all. To make it easier to avoid accidentally reading from a requester pays bucket, we also have; --requester-pays-allow-buckets. If you’d like to enable only reading from buckets named; hail-bucket and big-data, you can specify the following:; hailctl dataproc start my-cluster --requester-pays-allow-buckets hail-bucket,big-data. Users of the Annotation Database will find that ",MatchSource.WIKI,docs/0.2/cloud/google_cloud.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/cloud/google_cloud.html
https://hail.is/docs/0.2/cloud/google_cloud.html:2574,Energy Efficiency,charge,charges,2574,"t to a Jupyter notebook running on that cluster, use:; hailctl dataproc connect CLUSTER_NAME notebook [optional args...]. To list active clusters, use:; hailctl dataproc list. Importantly, to shut down a cluster when done with it, use:; hailctl dataproc stop CLUSTER_NAME. Reading from Google Cloud Storage; A dataproc cluster created through hailctl dataproc will automatically be configured to allow hail to read files from; Google Cloud Storage (GCS). To allow hail to read from GCS when running locally, you need to install the; Cloud Storage Connector. The easiest way to do that is to; run the following script from your command line:; curl -sSL https://broad.io/install-gcs-connector | python3. After this is installed, you’ll be able to read from paths beginning with gs directly from you laptop. Requester Pays; Some google cloud buckets are Requester Pays, meaning; that accessing them will incur charges on the requester. Google breaks down the charges in the linked document,; but the most important class of charges to be aware of are Network Charges.; Specifically, the egress charges. You should always be careful reading data from a bucket in a different region; then your own project, as it is easy to rack up a large bill. For this reason, you must specifically enable; requester pays on your hailctl dataproc cluster if you’d like to use it.; To allow your cluster to read from any requester pays bucket, use:; hailctl dataproc start CLUSTER_NAME --requester-pays-allow-all. To make it easier to avoid accidentally reading from a requester pays bucket, we also have; --requester-pays-allow-buckets. If you’d like to enable only reading from buckets named; hail-bucket and big-data, you can specify the following:; hailctl dataproc start my-cluster --requester-pays-allow-buckets hail-bucket,big-data. Users of the Annotation Database will find that many of the files are stored in requester pays buckets.; In order to allow the dataproc cluster to read from them, you can either ",MatchSource.WIKI,docs/0.2/cloud/google_cloud.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/cloud/google_cloud.html
https://hail.is/docs/0.2/cloud/google_cloud.html:2639,Energy Efficiency,charge,charges,2639,"t to a Jupyter notebook running on that cluster, use:; hailctl dataproc connect CLUSTER_NAME notebook [optional args...]. To list active clusters, use:; hailctl dataproc list. Importantly, to shut down a cluster when done with it, use:; hailctl dataproc stop CLUSTER_NAME. Reading from Google Cloud Storage; A dataproc cluster created through hailctl dataproc will automatically be configured to allow hail to read files from; Google Cloud Storage (GCS). To allow hail to read from GCS when running locally, you need to install the; Cloud Storage Connector. The easiest way to do that is to; run the following script from your command line:; curl -sSL https://broad.io/install-gcs-connector | python3. After this is installed, you’ll be able to read from paths beginning with gs directly from you laptop. Requester Pays; Some google cloud buckets are Requester Pays, meaning; that accessing them will incur charges on the requester. Google breaks down the charges in the linked document,; but the most important class of charges to be aware of are Network Charges.; Specifically, the egress charges. You should always be careful reading data from a bucket in a different region; then your own project, as it is easy to rack up a large bill. For this reason, you must specifically enable; requester pays on your hailctl dataproc cluster if you’d like to use it.; To allow your cluster to read from any requester pays bucket, use:; hailctl dataproc start CLUSTER_NAME --requester-pays-allow-all. To make it easier to avoid accidentally reading from a requester pays bucket, we also have; --requester-pays-allow-buckets. If you’d like to enable only reading from buckets named; hail-bucket and big-data, you can specify the following:; hailctl dataproc start my-cluster --requester-pays-allow-buckets hail-bucket,big-data. Users of the Annotation Database will find that many of the files are stored in requester pays buckets.; In order to allow the dataproc cluster to read from them, you can either ",MatchSource.WIKI,docs/0.2/cloud/google_cloud.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/cloud/google_cloud.html
https://hail.is/docs/0.2/cloud/google_cloud.html:2709,Energy Efficiency,charge,charges,2709,"STER_NAME notebook [optional args...]. To list active clusters, use:; hailctl dataproc list. Importantly, to shut down a cluster when done with it, use:; hailctl dataproc stop CLUSTER_NAME. Reading from Google Cloud Storage; A dataproc cluster created through hailctl dataproc will automatically be configured to allow hail to read files from; Google Cloud Storage (GCS). To allow hail to read from GCS when running locally, you need to install the; Cloud Storage Connector. The easiest way to do that is to; run the following script from your command line:; curl -sSL https://broad.io/install-gcs-connector | python3. After this is installed, you’ll be able to read from paths beginning with gs directly from you laptop. Requester Pays; Some google cloud buckets are Requester Pays, meaning; that accessing them will incur charges on the requester. Google breaks down the charges in the linked document,; but the most important class of charges to be aware of are Network Charges.; Specifically, the egress charges. You should always be careful reading data from a bucket in a different region; then your own project, as it is easy to rack up a large bill. For this reason, you must specifically enable; requester pays on your hailctl dataproc cluster if you’d like to use it.; To allow your cluster to read from any requester pays bucket, use:; hailctl dataproc start CLUSTER_NAME --requester-pays-allow-all. To make it easier to avoid accidentally reading from a requester pays bucket, we also have; --requester-pays-allow-buckets. If you’d like to enable only reading from buckets named; hail-bucket and big-data, you can specify the following:; hailctl dataproc start my-cluster --requester-pays-allow-buckets hail-bucket,big-data. Users of the Annotation Database will find that many of the files are stored in requester pays buckets.; In order to allow the dataproc cluster to read from them, you can either use --requester-pays-allow-all from above; or use the special --requester-pays-allow",MatchSource.WIKI,docs/0.2/cloud/google_cloud.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/cloud/google_cloud.html
https://hail.is/docs/0.2/cloud/google_cloud.html:1172,Integrability,interface,interface,1172,"e; Query-on-Batch; Google Cloud; hailctl dataproc; Reading from Google Cloud Storage; Requester Pays; Variant Effect Predictor (VEP). Microsoft Azure; Amazon Web Services; Databricks. Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail on the Cloud; Google Cloud Platform. View page source. Google Cloud Platform; If you’re new to Google Cloud in general, and would like an overview, linked; here.; is a document written to onboard new users within our lab to cloud computing. hailctl dataproc; As of version 0.2.15, pip installations of Hail come bundled with a command-line; tool, hailctl. This tool has a submodule called dataproc for working with; Google Dataproc clusters configured for Hail.; This tool requires the Google Cloud SDK.; Until full documentation for the command-line interface is written, we encourage; you to run the following command to see the list of modules:; hailctl dataproc. It is possible to print help for a specific command using the help flag:; hailctl dataproc start --help. To start a cluster, use:; hailctl dataproc start CLUSTER_NAME [optional args...]. To submit a Python job to that cluster, use:; hailctl dataproc submit CLUSTER_NAME SCRIPT [optional args to your python script...]. To connect to a Jupyter notebook running on that cluster, use:; hailctl dataproc connect CLUSTER_NAME notebook [optional args...]. To list active clusters, use:; hailctl dataproc list. Importantly, to shut down a cluster when done with it, use:; hailctl dataproc stop CLUSTER_NAME. Reading from Google Cloud Storage; A dataproc cluster created through hailctl dataproc will automatically be configured to allow hail to read files from; Google Cloud Storage (GCS). To allow hail to read from GCS when running locally, you need to install the; Cloud Storage Connector. The easiest way to do that is t",MatchSource.WIKI,docs/0.2/cloud/google_cloud.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/cloud/google_cloud.html
https://hail.is/docs/0.2/cloud/google_cloud.html:1062,Modifiability,config,configured,1062,"form. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; General Advice; Query-on-Batch; Google Cloud; hailctl dataproc; Reading from Google Cloud Storage; Requester Pays; Variant Effect Predictor (VEP). Microsoft Azure; Amazon Web Services; Databricks. Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail on the Cloud; Google Cloud Platform. View page source. Google Cloud Platform; If you’re new to Google Cloud in general, and would like an overview, linked; here.; is a document written to onboard new users within our lab to cloud computing. hailctl dataproc; As of version 0.2.15, pip installations of Hail come bundled with a command-line; tool, hailctl. This tool has a submodule called dataproc for working with; Google Dataproc clusters configured for Hail.; This tool requires the Google Cloud SDK.; Until full documentation for the command-line interface is written, we encourage; you to run the following command to see the list of modules:; hailctl dataproc. It is possible to print help for a specific command using the help flag:; hailctl dataproc start --help. To start a cluster, use:; hailctl dataproc start CLUSTER_NAME [optional args...]. To submit a Python job to that cluster, use:; hailctl dataproc submit CLUSTER_NAME SCRIPT [optional args to your python script...]. To connect to a Jupyter notebook running on that cluster, use:; hailctl dataproc connect CLUSTER_NAME notebook [optional args...]. To list active clusters, use:; hailctl dataproc list. Importantly, to shut down a cluster when done with it, use:; hailctl dataproc stop CLUSTER_NAME. Reading from Google Cloud Storage; A dataproc cluster created through hailctl dataproc will automatically be configured to allow hail to re",MatchSource.WIKI,docs/0.2/cloud/google_cloud.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/cloud/google_cloud.html
https://hail.is/docs/0.2/cloud/google_cloud.html:1999,Modifiability,config,configured,1999,"is tool has a submodule called dataproc for working with; Google Dataproc clusters configured for Hail.; This tool requires the Google Cloud SDK.; Until full documentation for the command-line interface is written, we encourage; you to run the following command to see the list of modules:; hailctl dataproc. It is possible to print help for a specific command using the help flag:; hailctl dataproc start --help. To start a cluster, use:; hailctl dataproc start CLUSTER_NAME [optional args...]. To submit a Python job to that cluster, use:; hailctl dataproc submit CLUSTER_NAME SCRIPT [optional args to your python script...]. To connect to a Jupyter notebook running on that cluster, use:; hailctl dataproc connect CLUSTER_NAME notebook [optional args...]. To list active clusters, use:; hailctl dataproc list. Importantly, to shut down a cluster when done with it, use:; hailctl dataproc stop CLUSTER_NAME. Reading from Google Cloud Storage; A dataproc cluster created through hailctl dataproc will automatically be configured to allow hail to read files from; Google Cloud Storage (GCS). To allow hail to read from GCS when running locally, you need to install the; Cloud Storage Connector. The easiest way to do that is to; run the following script from your command line:; curl -sSL https://broad.io/install-gcs-connector | python3. After this is installed, you’ll be able to read from paths beginning with gs directly from you laptop. Requester Pays; Some google cloud buckets are Requester Pays, meaning; that accessing them will incur charges on the requester. Google breaks down the charges in the linked document,; but the most important class of charges to be aware of are Network Charges.; Specifically, the egress charges. You should always be careful reading data from a bucket in a different region; then your own project, as it is easy to rack up a large bill. For this reason, you must specifically enable; requester pays on your hailctl dataproc cluster if you’d like to use it.; ",MatchSource.WIKI,docs/0.2/cloud/google_cloud.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/cloud/google_cloud.html
https://hail.is/docs/0.2/cloud/google_cloud.html:3850,Modifiability,config,configuration,3850,"lled, you’ll be able to read from paths beginning with gs directly from you laptop. Requester Pays; Some google cloud buckets are Requester Pays, meaning; that accessing them will incur charges on the requester. Google breaks down the charges in the linked document,; but the most important class of charges to be aware of are Network Charges.; Specifically, the egress charges. You should always be careful reading data from a bucket in a different region; then your own project, as it is easy to rack up a large bill. For this reason, you must specifically enable; requester pays on your hailctl dataproc cluster if you’d like to use it.; To allow your cluster to read from any requester pays bucket, use:; hailctl dataproc start CLUSTER_NAME --requester-pays-allow-all. To make it easier to avoid accidentally reading from a requester pays bucket, we also have; --requester-pays-allow-buckets. If you’d like to enable only reading from buckets named; hail-bucket and big-data, you can specify the following:; hailctl dataproc start my-cluster --requester-pays-allow-buckets hail-bucket,big-data. Users of the Annotation Database will find that many of the files are stored in requester pays buckets.; In order to allow the dataproc cluster to read from them, you can either use --requester-pays-allow-all from above; or use the special --requester-pays-allow-annotation-db to enable the specific list of buckets that the annotation database; relies on. Variant Effect Predictor (VEP); The following cluster configuration enables Hail to run VEP in parallel on every; variant in a dataset containing GRCh37 variants:; hailctl dataproc start NAME --vep GRCh37. Hail also supports VEP for GRCh38 variants, but you must start a cluster with; the argument --vep GRCh38. A cluster started without the --vep argument is; unable to run VEP and cannot be modified to run VEP. You must start a new; cluster using --vep. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/cloud/google_cloud.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/cloud/google_cloud.html
https://hail.is/docs/0.2/cloud/google_cloud.html:3133,Safety,avoid,avoid,3133,". The easiest way to do that is to; run the following script from your command line:; curl -sSL https://broad.io/install-gcs-connector | python3. After this is installed, you’ll be able to read from paths beginning with gs directly from you laptop. Requester Pays; Some google cloud buckets are Requester Pays, meaning; that accessing them will incur charges on the requester. Google breaks down the charges in the linked document,; but the most important class of charges to be aware of are Network Charges.; Specifically, the egress charges. You should always be careful reading data from a bucket in a different region; then your own project, as it is easy to rack up a large bill. For this reason, you must specifically enable; requester pays on your hailctl dataproc cluster if you’d like to use it.; To allow your cluster to read from any requester pays bucket, use:; hailctl dataproc start CLUSTER_NAME --requester-pays-allow-all. To make it easier to avoid accidentally reading from a requester pays bucket, we also have; --requester-pays-allow-buckets. If you’d like to enable only reading from buckets named; hail-bucket and big-data, you can specify the following:; hailctl dataproc start my-cluster --requester-pays-allow-buckets hail-bucket,big-data. Users of the Annotation Database will find that many of the files are stored in requester pays buckets.; In order to allow the dataproc cluster to read from them, you can either use --requester-pays-allow-all from above; or use the special --requester-pays-allow-annotation-db to enable the specific list of buckets that the annotation database; relies on. Variant Effect Predictor (VEP); The following cluster configuration enables Hail to run VEP in parallel on every; variant in a dataset containing GRCh37 variants:; hailctl dataproc start NAME --vep GRCh37. Hail also supports VEP for GRCh38 variants, but you must start a cluster with; the argument --vep GRCh38. A cluster started without the --vep argument is; unable to run VE",MatchSource.WIKI,docs/0.2/cloud/google_cloud.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/cloud/google_cloud.html
https://hail.is/docs/0.2/cloud/google_cloud.html:2499,Security,access,accessing,2499,"a Python job to that cluster, use:; hailctl dataproc submit CLUSTER_NAME SCRIPT [optional args to your python script...]. To connect to a Jupyter notebook running on that cluster, use:; hailctl dataproc connect CLUSTER_NAME notebook [optional args...]. To list active clusters, use:; hailctl dataproc list. Importantly, to shut down a cluster when done with it, use:; hailctl dataproc stop CLUSTER_NAME. Reading from Google Cloud Storage; A dataproc cluster created through hailctl dataproc will automatically be configured to allow hail to read files from; Google Cloud Storage (GCS). To allow hail to read from GCS when running locally, you need to install the; Cloud Storage Connector. The easiest way to do that is to; run the following script from your command line:; curl -sSL https://broad.io/install-gcs-connector | python3. After this is installed, you’ll be able to read from paths beginning with gs directly from you laptop. Requester Pays; Some google cloud buckets are Requester Pays, meaning; that accessing them will incur charges on the requester. Google breaks down the charges in the linked document,; but the most important class of charges to be aware of are Network Charges.; Specifically, the egress charges. You should always be careful reading data from a bucket in a different region; then your own project, as it is easy to rack up a large bill. For this reason, you must specifically enable; requester pays on your hailctl dataproc cluster if you’d like to use it.; To allow your cluster to read from any requester pays bucket, use:; hailctl dataproc start CLUSTER_NAME --requester-pays-allow-all. To make it easier to avoid accidentally reading from a requester pays bucket, we also have; --requester-pays-allow-buckets. If you’d like to enable only reading from buckets named; hail-bucket and big-data, you can specify the following:; hailctl dataproc start my-cluster --requester-pays-allow-buckets hail-bucket,big-data. Users of the Annotation Database will find that ",MatchSource.WIKI,docs/0.2/cloud/google_cloud.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/cloud/google_cloud.html
https://hail.is/docs/0.2/cloud/query_on_batch.html:1306,Availability,avail,available,1306,"ure; Amazon Web Services; Databricks. Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail on the Cloud; Hail Query-on-Batch. View page source. Hail Query-on-Batch. Warning; Hail Query-on-Batch (the Batch backend) is currently in beta. This means some functionality is; not yet working. Please contact us if you would like to use missing; functionality on Query-on-Batch!. Hail Query-on-Batch uses Hail Batch instead of Apache Spark to execute jobs. Instead of a Dataproc; cluster, you will need a Hail Batch cluster. For more information on using Hail Batch, see the Hail; Batch docs. For more information on deploying a Hail Batch cluster,; please contact the Hail Team at our discussion forum. Getting Started. Install Hail version 0.2.93 or later:. pip install 'hail>=0.2.93'. Sign up for a Hail Batch account (currently only available to; Broad affiliates).; Authenticate with Hail Batch. hailctl auth login. Specify a bucket for Hail to use for temporary intermediate files. In Google Cloud, we recommend; using a bucket with automatic deletion after a set period of time. hailctl config set batch/remote_tmpdir gs://my-auto-delete-bucket/hail-query-temporaries. Specify a Hail Batch billing project (these are different from Google Cloud projects). Every new; user has a trial billing project loaded with 10 USD. The name is available on the Hail User; account page. hailctl config set batch/billing_project my-billing-project. Set the default Hail Query backend to batch:. hailctl config set query/backend batch. Now you are ready to try Hail! If you want to switch back to; Query-on-Spark, run the previous command again with “spark” in place of “batch”. Variant Effect Predictor (VEP); More information coming very soon. If you want to use VEP with Hail Query-on-Batch, please contact; the Hail Team at our dis",MatchSource.WIKI,docs/0.2/cloud/query_on_batch.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/cloud/query_on_batch.html
https://hail.is/docs/0.2/cloud/query_on_batch.html:1808,Availability,avail,available,1808,"erview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail on the Cloud; Hail Query-on-Batch. View page source. Hail Query-on-Batch. Warning; Hail Query-on-Batch (the Batch backend) is currently in beta. This means some functionality is; not yet working. Please contact us if you would like to use missing; functionality on Query-on-Batch!. Hail Query-on-Batch uses Hail Batch instead of Apache Spark to execute jobs. Instead of a Dataproc; cluster, you will need a Hail Batch cluster. For more information on using Hail Batch, see the Hail; Batch docs. For more information on deploying a Hail Batch cluster,; please contact the Hail Team at our discussion forum. Getting Started. Install Hail version 0.2.93 or later:. pip install 'hail>=0.2.93'. Sign up for a Hail Batch account (currently only available to; Broad affiliates).; Authenticate with Hail Batch. hailctl auth login. Specify a bucket for Hail to use for temporary intermediate files. In Google Cloud, we recommend; using a bucket with automatic deletion after a set period of time. hailctl config set batch/remote_tmpdir gs://my-auto-delete-bucket/hail-query-temporaries. Specify a Hail Batch billing project (these are different from Google Cloud projects). Every new; user has a trial billing project loaded with 10 USD. The name is available on the Hail User; account page. hailctl config set batch/billing_project my-billing-project. Set the default Hail Query backend to batch:. hailctl config set query/backend batch. Now you are ready to try Hail! If you want to switch back to; Query-on-Spark, run the previous command again with “spark” in place of “batch”. Variant Effect Predictor (VEP); More information coming very soon. If you want to use VEP with Hail Query-on-Batch, please contact; the Hail Team at our discussion forum. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/cloud/query_on_batch.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/cloud/query_on_batch.html
https://hail.is/docs/0.2/cloud/query_on_batch.html:1085,Deployability,deploy,deploying,1085,". Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; General Advice; Query-on-Batch; Getting Started; Variant Effect Predictor (VEP). Google Cloud; Microsoft Azure; Amazon Web Services; Databricks. Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail on the Cloud; Hail Query-on-Batch. View page source. Hail Query-on-Batch. Warning; Hail Query-on-Batch (the Batch backend) is currently in beta. This means some functionality is; not yet working. Please contact us if you would like to use missing; functionality on Query-on-Batch!. Hail Query-on-Batch uses Hail Batch instead of Apache Spark to execute jobs. Instead of a Dataproc; cluster, you will need a Hail Batch cluster. For more information on using Hail Batch, see the Hail; Batch docs. For more information on deploying a Hail Batch cluster,; please contact the Hail Team at our discussion forum. Getting Started. Install Hail version 0.2.93 or later:. pip install 'hail>=0.2.93'. Sign up for a Hail Batch account (currently only available to; Broad affiliates).; Authenticate with Hail Batch. hailctl auth login. Specify a bucket for Hail to use for temporary intermediate files. In Google Cloud, we recommend; using a bucket with automatic deletion after a set period of time. hailctl config set batch/remote_tmpdir gs://my-auto-delete-bucket/hail-query-temporaries. Specify a Hail Batch billing project (these are different from Google Cloud projects). Every new; user has a trial billing project loaded with 10 USD. The name is available on the Hail User; account page. hailctl config set batch/billing_project my-billing-project. Set the default Hail Query backend to batch:. hailctl config set query/backend batch. Now you are ready to try Hail! If you want to switch back to; Query-on-Spark, run the previous command again with “spar",MatchSource.WIKI,docs/0.2/cloud/query_on_batch.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/cloud/query_on_batch.html
https://hail.is/docs/0.2/cloud/query_on_batch.html:1233,Deployability,install,install,1233,"Variant Effect Predictor (VEP). Google Cloud; Microsoft Azure; Amazon Web Services; Databricks. Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail on the Cloud; Hail Query-on-Batch. View page source. Hail Query-on-Batch. Warning; Hail Query-on-Batch (the Batch backend) is currently in beta. This means some functionality is; not yet working. Please contact us if you would like to use missing; functionality on Query-on-Batch!. Hail Query-on-Batch uses Hail Batch instead of Apache Spark to execute jobs. Instead of a Dataproc; cluster, you will need a Hail Batch cluster. For more information on using Hail Batch, see the Hail; Batch docs. For more information on deploying a Hail Batch cluster,; please contact the Hail Team at our discussion forum. Getting Started. Install Hail version 0.2.93 or later:. pip install 'hail>=0.2.93'. Sign up for a Hail Batch account (currently only available to; Broad affiliates).; Authenticate with Hail Batch. hailctl auth login. Specify a bucket for Hail to use for temporary intermediate files. In Google Cloud, we recommend; using a bucket with automatic deletion after a set period of time. hailctl config set batch/remote_tmpdir gs://my-auto-delete-bucket/hail-query-temporaries. Specify a Hail Batch billing project (these are different from Google Cloud projects). Every new; user has a trial billing project loaded with 10 USD. The name is available on the Hail User; account page. hailctl config set batch/billing_project my-billing-project. Set the default Hail Query backend to batch:. hailctl config set query/backend batch. Now you are ready to try Hail! If you want to switch back to; Query-on-Spark, run the previous command again with “spark” in place of “batch”. Variant Effect Predictor (VEP); More information coming very soon. If you want to use VEP with Hai",MatchSource.WIKI,docs/0.2/cloud/query_on_batch.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/cloud/query_on_batch.html
https://hail.is/docs/0.2/cloud/query_on_batch.html:2369,Deployability,update,updated,2369,"erview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail on the Cloud; Hail Query-on-Batch. View page source. Hail Query-on-Batch. Warning; Hail Query-on-Batch (the Batch backend) is currently in beta. This means some functionality is; not yet working. Please contact us if you would like to use missing; functionality on Query-on-Batch!. Hail Query-on-Batch uses Hail Batch instead of Apache Spark to execute jobs. Instead of a Dataproc; cluster, you will need a Hail Batch cluster. For more information on using Hail Batch, see the Hail; Batch docs. For more information on deploying a Hail Batch cluster,; please contact the Hail Team at our discussion forum. Getting Started. Install Hail version 0.2.93 or later:. pip install 'hail>=0.2.93'. Sign up for a Hail Batch account (currently only available to; Broad affiliates).; Authenticate with Hail Batch. hailctl auth login. Specify a bucket for Hail to use for temporary intermediate files. In Google Cloud, we recommend; using a bucket with automatic deletion after a set period of time. hailctl config set batch/remote_tmpdir gs://my-auto-delete-bucket/hail-query-temporaries. Specify a Hail Batch billing project (these are different from Google Cloud projects). Every new; user has a trial billing project loaded with 10 USD. The name is available on the Hail User; account page. hailctl config set batch/billing_project my-billing-project. Set the default Hail Query backend to batch:. hailctl config set query/backend batch. Now you are ready to try Hail! If you want to switch back to; Query-on-Spark, run the previous command again with “spark” in place of “batch”. Variant Effect Predictor (VEP); More information coming very soon. If you want to use VEP with Hail Query-on-Batch, please contact; the Hail Team at our discussion forum. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/cloud/query_on_batch.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/cloud/query_on_batch.html
https://hail.is/docs/0.2/cloud/query_on_batch.html:1563,Modifiability,config,config,1563,"erview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail on the Cloud; Hail Query-on-Batch. View page source. Hail Query-on-Batch. Warning; Hail Query-on-Batch (the Batch backend) is currently in beta. This means some functionality is; not yet working. Please contact us if you would like to use missing; functionality on Query-on-Batch!. Hail Query-on-Batch uses Hail Batch instead of Apache Spark to execute jobs. Instead of a Dataproc; cluster, you will need a Hail Batch cluster. For more information on using Hail Batch, see the Hail; Batch docs. For more information on deploying a Hail Batch cluster,; please contact the Hail Team at our discussion forum. Getting Started. Install Hail version 0.2.93 or later:. pip install 'hail>=0.2.93'. Sign up for a Hail Batch account (currently only available to; Broad affiliates).; Authenticate with Hail Batch. hailctl auth login. Specify a bucket for Hail to use for temporary intermediate files. In Google Cloud, we recommend; using a bucket with automatic deletion after a set period of time. hailctl config set batch/remote_tmpdir gs://my-auto-delete-bucket/hail-query-temporaries. Specify a Hail Batch billing project (these are different from Google Cloud projects). Every new; user has a trial billing project loaded with 10 USD. The name is available on the Hail User; account page. hailctl config set batch/billing_project my-billing-project. Set the default Hail Query backend to batch:. hailctl config set query/backend batch. Now you are ready to try Hail! If you want to switch back to; Query-on-Spark, run the previous command again with “spark” in place of “batch”. Variant Effect Predictor (VEP); More information coming very soon. If you want to use VEP with Hail Query-on-Batch, please contact; the Hail Team at our discussion forum. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/cloud/query_on_batch.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/cloud/query_on_batch.html
https://hail.is/docs/0.2/cloud/query_on_batch.html:1858,Modifiability,config,config,1858,"erview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail on the Cloud; Hail Query-on-Batch. View page source. Hail Query-on-Batch. Warning; Hail Query-on-Batch (the Batch backend) is currently in beta. This means some functionality is; not yet working. Please contact us if you would like to use missing; functionality on Query-on-Batch!. Hail Query-on-Batch uses Hail Batch instead of Apache Spark to execute jobs. Instead of a Dataproc; cluster, you will need a Hail Batch cluster. For more information on using Hail Batch, see the Hail; Batch docs. For more information on deploying a Hail Batch cluster,; please contact the Hail Team at our discussion forum. Getting Started. Install Hail version 0.2.93 or later:. pip install 'hail>=0.2.93'. Sign up for a Hail Batch account (currently only available to; Broad affiliates).; Authenticate with Hail Batch. hailctl auth login. Specify a bucket for Hail to use for temporary intermediate files. In Google Cloud, we recommend; using a bucket with automatic deletion after a set period of time. hailctl config set batch/remote_tmpdir gs://my-auto-delete-bucket/hail-query-temporaries. Specify a Hail Batch billing project (these are different from Google Cloud projects). Every new; user has a trial billing project loaded with 10 USD. The name is available on the Hail User; account page. hailctl config set batch/billing_project my-billing-project. Set the default Hail Query backend to batch:. hailctl config set query/backend batch. Now you are ready to try Hail! If you want to switch back to; Query-on-Spark, run the previous command again with “spark” in place of “batch”. Variant Effect Predictor (VEP); More information coming very soon. If you want to use VEP with Hail Query-on-Batch, please contact; the Hail Team at our discussion forum. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/cloud/query_on_batch.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/cloud/query_on_batch.html
https://hail.is/docs/0.2/cloud/query_on_batch.html:1965,Modifiability,config,config,1965,"erview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail on the Cloud; Hail Query-on-Batch. View page source. Hail Query-on-Batch. Warning; Hail Query-on-Batch (the Batch backend) is currently in beta. This means some functionality is; not yet working. Please contact us if you would like to use missing; functionality on Query-on-Batch!. Hail Query-on-Batch uses Hail Batch instead of Apache Spark to execute jobs. Instead of a Dataproc; cluster, you will need a Hail Batch cluster. For more information on using Hail Batch, see the Hail; Batch docs. For more information on deploying a Hail Batch cluster,; please contact the Hail Team at our discussion forum. Getting Started. Install Hail version 0.2.93 or later:. pip install 'hail>=0.2.93'. Sign up for a Hail Batch account (currently only available to; Broad affiliates).; Authenticate with Hail Batch. hailctl auth login. Specify a bucket for Hail to use for temporary intermediate files. In Google Cloud, we recommend; using a bucket with automatic deletion after a set period of time. hailctl config set batch/remote_tmpdir gs://my-auto-delete-bucket/hail-query-temporaries. Specify a Hail Batch billing project (these are different from Google Cloud projects). Every new; user has a trial billing project loaded with 10 USD. The name is available on the Hail User; account page. hailctl config set batch/billing_project my-billing-project. Set the default Hail Query backend to batch:. hailctl config set query/backend batch. Now you are ready to try Hail! If you want to switch back to; Query-on-Spark, run the previous command again with “spark” in place of “batch”. Variant Effect Predictor (VEP); More information coming very soon. If you want to use VEP with Hail Query-on-Batch, please contact; the Hail Team at our discussion forum. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/cloud/query_on_batch.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/cloud/query_on_batch.html
https://hail.is/docs/0.2/cloud/query_on_batch.html:1776,Performance,load,loaded,1776,"erview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail on the Cloud; Hail Query-on-Batch. View page source. Hail Query-on-Batch. Warning; Hail Query-on-Batch (the Batch backend) is currently in beta. This means some functionality is; not yet working. Please contact us if you would like to use missing; functionality on Query-on-Batch!. Hail Query-on-Batch uses Hail Batch instead of Apache Spark to execute jobs. Instead of a Dataproc; cluster, you will need a Hail Batch cluster. For more information on using Hail Batch, see the Hail; Batch docs. For more information on deploying a Hail Batch cluster,; please contact the Hail Team at our discussion forum. Getting Started. Install Hail version 0.2.93 or later:. pip install 'hail>=0.2.93'. Sign up for a Hail Batch account (currently only available to; Broad affiliates).; Authenticate with Hail Batch. hailctl auth login. Specify a bucket for Hail to use for temporary intermediate files. In Google Cloud, we recommend; using a bucket with automatic deletion after a set period of time. hailctl config set batch/remote_tmpdir gs://my-auto-delete-bucket/hail-query-temporaries. Specify a Hail Batch billing project (these are different from Google Cloud projects). Every new; user has a trial billing project loaded with 10 USD. The name is available on the Hail User; account page. hailctl config set batch/billing_project my-billing-project. Set the default Hail Query backend to batch:. hailctl config set query/backend batch. Now you are ready to try Hail! If you want to switch back to; Query-on-Spark, run the previous command again with “spark” in place of “batch”. Variant Effect Predictor (VEP); More information coming very soon. If you want to use VEP with Hail Query-on-Batch, please contact; the Hail Team at our discussion forum. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/cloud/query_on_batch.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/cloud/query_on_batch.html
https://hail.is/docs/0.2/cloud/query_on_batch.html:1383,Testability,log,login,1383,"tion Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail on the Cloud; Hail Query-on-Batch. View page source. Hail Query-on-Batch. Warning; Hail Query-on-Batch (the Batch backend) is currently in beta. This means some functionality is; not yet working. Please contact us if you would like to use missing; functionality on Query-on-Batch!. Hail Query-on-Batch uses Hail Batch instead of Apache Spark to execute jobs. Instead of a Dataproc; cluster, you will need a Hail Batch cluster. For more information on using Hail Batch, see the Hail; Batch docs. For more information on deploying a Hail Batch cluster,; please contact the Hail Team at our discussion forum. Getting Started. Install Hail version 0.2.93 or later:. pip install 'hail>=0.2.93'. Sign up for a Hail Batch account (currently only available to; Broad affiliates).; Authenticate with Hail Batch. hailctl auth login. Specify a bucket for Hail to use for temporary intermediate files. In Google Cloud, we recommend; using a bucket with automatic deletion after a set period of time. hailctl config set batch/remote_tmpdir gs://my-auto-delete-bucket/hail-query-temporaries. Specify a Hail Batch billing project (these are different from Google Cloud projects). Every new; user has a trial billing project loaded with 10 USD. The name is available on the Hail User; account page. hailctl config set batch/billing_project my-billing-project. Set the default Hail Query backend to batch:. hailctl config set query/backend batch. Now you are ready to try Hail! If you want to switch back to; Query-on-Spark, run the previous command again with “spark” in place of “batch”. Variant Effect Predictor (VEP); More information coming very soon. If you want to use VEP with Hail Query-on-Batch, please contact; the Hail Team at our discussion forum. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on",MatchSource.WIKI,docs/0.2/cloud/query_on_batch.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/cloud/query_on_batch.html
https://hail.is/docs/0.2/datasets/schemas.html:17268,Deployability,update,updated,17268,"_exome_C_ALL_Rec; giant_whr_exome_C_EUR_Add; giant_whr_exome_C_EUR_Rec; giant_whr_exome_M_ALL_Add; giant_whr_exome_M_ALL_Rec; giant_whr_exome_M_EUR_Add; giant_whr_exome_M_EUR_Rec; giant_whr_exome_W_ALL_Add; giant_whr_exome_W_ALL_Rec; giant_whr_exome_W_EUR_Add; giant_whr_exome_W_EUR_Rec; gnomad_annotation_pext; gnomad_base_pext; gnomad_chrM_coverage; gnomad_chrM_sites; gnomad_exome_coverage; gnomad_exome_sites; gnomad_genome_coverage; gnomad_genome_sites; gnomad_hgdp_1kg_subset_dense; gnomad_hgdp_1kg_subset_sample_metadata; gnomad_hgdp_1kg_subset_sparse; gnomad_hgdp_1kg_subset_variant_annotations; gnomad_ld_scores_afr; gnomad_ld_scores_amr; gnomad_ld_scores_asj; gnomad_ld_scores_eas; gnomad_ld_scores_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas.html
https://hail.is/docs/0.2/experimental/hail.experimental.DB.html:1708,Availability,avail,available,1708,".DB[source]; An annotation database instance.; This class facilitates the annotation of genetic datasets with variant annotations. It accepts; either an HTTP(S) URL to an Annotation DB configuration or a Python dict describing an; Annotation DB configuration. User must specify the region (aws: 'us', gcp:; 'us-central1' or 'europe-west1') in which the cluster is running if connecting to the; default Hail Annotation DB. User must also specify the cloud platform that they are using; ('gcp' or 'aws'). Parameters:. region (str) – Region cluster is running in, either 'us', 'us-central1', or 'europe-west1'; (default is 'us-central1').; cloud (str) – Cloud platform, either 'gcp' or 'aws' (default is 'gcp').; url (str, optional) – Optional URL to annotation DB configuration, if using custom configuration; (default is None).; config (str, optional) – Optional dict describing an annotation DB configuration, if using; custom configuration (default is None). Note; The 'aws' cloud platform is currently only available for the 'us'; region. Examples; Create an annotation database connecting to the default Hail Annotation DB:; >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'). Attributes. available_datasets; List of names of available annotation datasets. Methods. annotate_rows_db; Add annotations from datasets specified by name to a relational object. annotate_rows_db(rel, *names)[source]; Add annotations from datasets specified by name to a relational; object.; List datasets with available_datasets.; An interactive query builder is available in the; Hail Annotation Database documentation.; Examples; Annotate a MatrixTable with gnomad_lof_metrics:; >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'); >>> mt = db.annotate_rows_db(mt, 'gnomad_lof_metrics') . Annotate a Table with clinvar_gene_summary, CADD,; and DANN:; >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'); >>> ht = db.annotate_rows_db(ht, 'clinvar_gene_summary', 'CADD', 'DANN') . Not",MatchSource.WIKI,docs/0.2/experimental/hail.experimental.DB.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/experimental/hail.experimental.DB.html
https://hail.is/docs/0.2/experimental/hail.experimental.DB.html:1940,Availability,avail,available,1940,"tion DB configuration. User must specify the region (aws: 'us', gcp:; 'us-central1' or 'europe-west1') in which the cluster is running if connecting to the; default Hail Annotation DB. User must also specify the cloud platform that they are using; ('gcp' or 'aws'). Parameters:. region (str) – Region cluster is running in, either 'us', 'us-central1', or 'europe-west1'; (default is 'us-central1').; cloud (str) – Cloud platform, either 'gcp' or 'aws' (default is 'gcp').; url (str, optional) – Optional URL to annotation DB configuration, if using custom configuration; (default is None).; config (str, optional) – Optional dict describing an annotation DB configuration, if using; custom configuration (default is None). Note; The 'aws' cloud platform is currently only available for the 'us'; region. Examples; Create an annotation database connecting to the default Hail Annotation DB:; >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'). Attributes. available_datasets; List of names of available annotation datasets. Methods. annotate_rows_db; Add annotations from datasets specified by name to a relational object. annotate_rows_db(rel, *names)[source]; Add annotations from datasets specified by name to a relational; object.; List datasets with available_datasets.; An interactive query builder is available in the; Hail Annotation Database documentation.; Examples; Annotate a MatrixTable with gnomad_lof_metrics:; >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'); >>> mt = db.annotate_rows_db(mt, 'gnomad_lof_metrics') . Annotate a Table with clinvar_gene_summary, CADD,; and DANN:; >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'); >>> ht = db.annotate_rows_db(ht, 'clinvar_gene_summary', 'CADD', 'DANN') . Notes; If a dataset is gene-keyed, the annotation will be a dictionary mapping; from gene name to the annotation value. There will be one entry for each; gene overlapping the given locus.; If a dataset does not have unique rows for each key",MatchSource.WIKI,docs/0.2/experimental/hail.experimental.DB.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/experimental/hail.experimental.DB.html
https://hail.is/docs/0.2/experimental/hail.experimental.DB.html:2256,Availability,avail,available,2256,"'us', 'us-central1', or 'europe-west1'; (default is 'us-central1').; cloud (str) – Cloud platform, either 'gcp' or 'aws' (default is 'gcp').; url (str, optional) – Optional URL to annotation DB configuration, if using custom configuration; (default is None).; config (str, optional) – Optional dict describing an annotation DB configuration, if using; custom configuration (default is None). Note; The 'aws' cloud platform is currently only available for the 'us'; region. Examples; Create an annotation database connecting to the default Hail Annotation DB:; >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'). Attributes. available_datasets; List of names of available annotation datasets. Methods. annotate_rows_db; Add annotations from datasets specified by name to a relational object. annotate_rows_db(rel, *names)[source]; Add annotations from datasets specified by name to a relational; object.; List datasets with available_datasets.; An interactive query builder is available in the; Hail Annotation Database documentation.; Examples; Annotate a MatrixTable with gnomad_lof_metrics:; >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'); >>> mt = db.annotate_rows_db(mt, 'gnomad_lof_metrics') . Annotate a Table with clinvar_gene_summary, CADD,; and DANN:; >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'); >>> ht = db.annotate_rows_db(ht, 'clinvar_gene_summary', 'CADD', 'DANN') . Notes; If a dataset is gene-keyed, the annotation will be a dictionary mapping; from gene name to the annotation value. There will be one entry for each; gene overlapping the given locus.; If a dataset does not have unique rows for each key (consider the; gencode genes, which may overlap; and clinvar_variant_summary,; which contains many overlapping multiple nucleotide variants), then the; result will be an array of annotation values, one for each row. Parameters:. rel (MatrixTable or Table) – The relational object to which to add annotations.; names (varargs of str",MatchSource.WIKI,docs/0.2/experimental/hail.experimental.DB.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/experimental/hail.experimental.DB.html
https://hail.is/docs/0.2/experimental/hail.experimental.DB.html:3473,Availability,avail,available,3473,"ult is None). Note; The 'aws' cloud platform is currently only available for the 'us'; region. Examples; Create an annotation database connecting to the default Hail Annotation DB:; >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'). Attributes. available_datasets; List of names of available annotation datasets. Methods. annotate_rows_db; Add annotations from datasets specified by name to a relational object. annotate_rows_db(rel, *names)[source]; Add annotations from datasets specified by name to a relational; object.; List datasets with available_datasets.; An interactive query builder is available in the; Hail Annotation Database documentation.; Examples; Annotate a MatrixTable with gnomad_lof_metrics:; >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'); >>> mt = db.annotate_rows_db(mt, 'gnomad_lof_metrics') . Annotate a Table with clinvar_gene_summary, CADD,; and DANN:; >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'); >>> ht = db.annotate_rows_db(ht, 'clinvar_gene_summary', 'CADD', 'DANN') . Notes; If a dataset is gene-keyed, the annotation will be a dictionary mapping; from gene name to the annotation value. There will be one entry for each; gene overlapping the given locus.; If a dataset does not have unique rows for each key (consider the; gencode genes, which may overlap; and clinvar_variant_summary,; which contains many overlapping multiple nucleotide variants), then the; result will be an array of annotation values, one for each row. Parameters:. rel (MatrixTable or Table) – The relational object to which to add annotations.; names (varargs of str) – The names of the datasets with which to annotate rel. Returns:; MatrixTable or Table – The relational object rel, with the annotations from names; added. property available_datasets; List of names of available annotation datasets. Returns:; list – List of available annotation datasets. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/experimental/hail.experimental.DB.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/experimental/hail.experimental.DB.html
https://hail.is/docs/0.2/experimental/hail.experimental.DB.html:3529,Availability,avail,available,3529,"ult is None). Note; The 'aws' cloud platform is currently only available for the 'us'; region. Examples; Create an annotation database connecting to the default Hail Annotation DB:; >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'). Attributes. available_datasets; List of names of available annotation datasets. Methods. annotate_rows_db; Add annotations from datasets specified by name to a relational object. annotate_rows_db(rel, *names)[source]; Add annotations from datasets specified by name to a relational; object.; List datasets with available_datasets.; An interactive query builder is available in the; Hail Annotation Database documentation.; Examples; Annotate a MatrixTable with gnomad_lof_metrics:; >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'); >>> mt = db.annotate_rows_db(mt, 'gnomad_lof_metrics') . Annotate a Table with clinvar_gene_summary, CADD,; and DANN:; >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'); >>> ht = db.annotate_rows_db(ht, 'clinvar_gene_summary', 'CADD', 'DANN') . Notes; If a dataset is gene-keyed, the annotation will be a dictionary mapping; from gene name to the annotation value. There will be one entry for each; gene overlapping the given locus.; If a dataset does not have unique rows for each key (consider the; gencode genes, which may overlap; and clinvar_variant_summary,; which contains many overlapping multiple nucleotide variants), then the; result will be an array of annotation values, one for each row. Parameters:. rel (MatrixTable or Table) – The relational object to which to add annotations.; names (varargs of str) – The names of the datasets with which to annotate rel. Returns:; MatrixTable or Table – The relational object rel, with the annotations from names; added. property available_datasets; List of names of available annotation datasets. Returns:; list – List of available annotation datasets. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/experimental/hail.experimental.DB.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/experimental/hail.experimental.DB.html
https://hail.is/docs/0.2/experimental/hail.experimental.DB.html:884,Deployability,configurat,configuration,884,"﻿. Hail | ; DB. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Experimental; DB. View page source. DB. class hail.experimental.DB[source]; An annotation database instance.; This class facilitates the annotation of genetic datasets with variant annotations. It accepts; either an HTTP(S) URL to an Annotation DB configuration or a Python dict describing an; Annotation DB configuration. User must specify the region (aws: 'us', gcp:; 'us-central1' or 'europe-west1') in which the cluster is running if connecting to the; default Hail Annotation DB. User must also specify the cloud platform that they are using; ('gcp' or 'aws'). Parameters:. region (str) – Region cluster is running in, either 'us', 'us-central1', or 'europe-west1'; (default is 'us-central1').; cloud (str) – Cloud platform, either 'gcp' or 'aws' (default is 'gcp').; url (str, optional) – Optional URL to annotation DB configuration, if using custom configuration; (default is None).; config (str, optional) – Optional dict describing an annotation DB configuration, if using; custom configuration (default is None). Note; The 'aws' cloud platform is currently only available for the 'us'; region. Examples; Create an annotation database connecting to the default Hail Annotation DB:; >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'). Attributes. available_datasets; List of names of available annotation datasets. Methods. annotate_rows_db; Add",MatchSource.WIKI,docs/0.2/experimental/hail.experimental.DB.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/experimental/hail.experimental.DB.html
https://hail.is/docs/0.2/experimental/hail.experimental.DB.html:944,Deployability,configurat,configuration,944,"﻿. Hail | ; DB. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Experimental; DB. View page source. DB. class hail.experimental.DB[source]; An annotation database instance.; This class facilitates the annotation of genetic datasets with variant annotations. It accepts; either an HTTP(S) URL to an Annotation DB configuration or a Python dict describing an; Annotation DB configuration. User must specify the region (aws: 'us', gcp:; 'us-central1' or 'europe-west1') in which the cluster is running if connecting to the; default Hail Annotation DB. User must also specify the cloud platform that they are using; ('gcp' or 'aws'). Parameters:. region (str) – Region cluster is running in, either 'us', 'us-central1', or 'europe-west1'; (default is 'us-central1').; cloud (str) – Cloud platform, either 'gcp' or 'aws' (default is 'gcp').; url (str, optional) – Optional URL to annotation DB configuration, if using custom configuration; (default is None).; config (str, optional) – Optional dict describing an annotation DB configuration, if using; custom configuration (default is None). Note; The 'aws' cloud platform is currently only available for the 'us'; region. Examples; Create an annotation database connecting to the default Hail Annotation DB:; >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'). Attributes. available_datasets; List of names of available annotation datasets. Methods. annotate_rows_db; Add",MatchSource.WIKI,docs/0.2/experimental/hail.experimental.DB.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/experimental/hail.experimental.DB.html
https://hail.is/docs/0.2/experimental/hail.experimental.DB.html:1461,Deployability,configurat,configuration,1461,"eets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Experimental; DB. View page source. DB. class hail.experimental.DB[source]; An annotation database instance.; This class facilitates the annotation of genetic datasets with variant annotations. It accepts; either an HTTP(S) URL to an Annotation DB configuration or a Python dict describing an; Annotation DB configuration. User must specify the region (aws: 'us', gcp:; 'us-central1' or 'europe-west1') in which the cluster is running if connecting to the; default Hail Annotation DB. User must also specify the cloud platform that they are using; ('gcp' or 'aws'). Parameters:. region (str) – Region cluster is running in, either 'us', 'us-central1', or 'europe-west1'; (default is 'us-central1').; cloud (str) – Cloud platform, either 'gcp' or 'aws' (default is 'gcp').; url (str, optional) – Optional URL to annotation DB configuration, if using custom configuration; (default is None).; config (str, optional) – Optional dict describing an annotation DB configuration, if using; custom configuration (default is None). Note; The 'aws' cloud platform is currently only available for the 'us'; region. Examples; Create an annotation database connecting to the default Hail Annotation DB:; >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'). Attributes. available_datasets; List of names of available annotation datasets. Methods. annotate_rows_db; Add annotations from datasets specified by name to a relational object. annotate_rows_db(rel, *names)[source]; Add annotations from datasets specified by name to a relational; object.; List datasets with available_datasets.; An interactive query builder is available in the; Hail Annotation Database documentation.; Examples; Annotate a MatrixTable with gnomad_lof_metrics:; >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'); >>> mt = db.annotate_rows_db",MatchSource.WIKI,docs/0.2/experimental/hail.experimental.DB.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/experimental/hail.experimental.DB.html
https://hail.is/docs/0.2/experimental/hail.experimental.DB.html:1492,Deployability,configurat,configuration,1492,"eets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Experimental; DB. View page source. DB. class hail.experimental.DB[source]; An annotation database instance.; This class facilitates the annotation of genetic datasets with variant annotations. It accepts; either an HTTP(S) URL to an Annotation DB configuration or a Python dict describing an; Annotation DB configuration. User must specify the region (aws: 'us', gcp:; 'us-central1' or 'europe-west1') in which the cluster is running if connecting to the; default Hail Annotation DB. User must also specify the cloud platform that they are using; ('gcp' or 'aws'). Parameters:. region (str) – Region cluster is running in, either 'us', 'us-central1', or 'europe-west1'; (default is 'us-central1').; cloud (str) – Cloud platform, either 'gcp' or 'aws' (default is 'gcp').; url (str, optional) – Optional URL to annotation DB configuration, if using custom configuration; (default is None).; config (str, optional) – Optional dict describing an annotation DB configuration, if using; custom configuration (default is None). Note; The 'aws' cloud platform is currently only available for the 'us'; region. Examples; Create an annotation database connecting to the default Hail Annotation DB:; >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'). Attributes. available_datasets; List of names of available annotation datasets. Methods. annotate_rows_db; Add annotations from datasets specified by name to a relational object. annotate_rows_db(rel, *names)[source]; Add annotations from datasets specified by name to a relational; object.; List datasets with available_datasets.; An interactive query builder is available in the; Hail Annotation Database documentation.; Examples; Annotate a MatrixTable with gnomad_lof_metrics:; >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'); >>> mt = db.annotate_rows_db",MatchSource.WIKI,docs/0.2/experimental/hail.experimental.DB.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/experimental/hail.experimental.DB.html
https://hail.is/docs/0.2/experimental/hail.experimental.DB.html:1594,Deployability,configurat,configuration,1594,"; Hail. Python API; Hail Query Python API; Experimental; DB. View page source. DB. class hail.experimental.DB[source]; An annotation database instance.; This class facilitates the annotation of genetic datasets with variant annotations. It accepts; either an HTTP(S) URL to an Annotation DB configuration or a Python dict describing an; Annotation DB configuration. User must specify the region (aws: 'us', gcp:; 'us-central1' or 'europe-west1') in which the cluster is running if connecting to the; default Hail Annotation DB. User must also specify the cloud platform that they are using; ('gcp' or 'aws'). Parameters:. region (str) – Region cluster is running in, either 'us', 'us-central1', or 'europe-west1'; (default is 'us-central1').; cloud (str) – Cloud platform, either 'gcp' or 'aws' (default is 'gcp').; url (str, optional) – Optional URL to annotation DB configuration, if using custom configuration; (default is None).; config (str, optional) – Optional dict describing an annotation DB configuration, if using; custom configuration (default is None). Note; The 'aws' cloud platform is currently only available for the 'us'; region. Examples; Create an annotation database connecting to the default Hail Annotation DB:; >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'). Attributes. available_datasets; List of names of available annotation datasets. Methods. annotate_rows_db; Add annotations from datasets specified by name to a relational object. annotate_rows_db(rel, *names)[source]; Add annotations from datasets specified by name to a relational; object.; List datasets with available_datasets.; An interactive query builder is available in the; Hail Annotation Database documentation.; Examples; Annotate a MatrixTable with gnomad_lof_metrics:; >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'); >>> mt = db.annotate_rows_db(mt, 'gnomad_lof_metrics') . Annotate a Table with clinvar_gene_summary, CADD,; and DANN:; >>> db = hl.experimental.DB(region=",MatchSource.WIKI,docs/0.2/experimental/hail.experimental.DB.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/experimental/hail.experimental.DB.html
https://hail.is/docs/0.2/experimental/hail.experimental.DB.html:1626,Deployability,configurat,configuration,1626,"; Hail. Python API; Hail Query Python API; Experimental; DB. View page source. DB. class hail.experimental.DB[source]; An annotation database instance.; This class facilitates the annotation of genetic datasets with variant annotations. It accepts; either an HTTP(S) URL to an Annotation DB configuration or a Python dict describing an; Annotation DB configuration. User must specify the region (aws: 'us', gcp:; 'us-central1' or 'europe-west1') in which the cluster is running if connecting to the; default Hail Annotation DB. User must also specify the cloud platform that they are using; ('gcp' or 'aws'). Parameters:. region (str) – Region cluster is running in, either 'us', 'us-central1', or 'europe-west1'; (default is 'us-central1').; cloud (str) – Cloud platform, either 'gcp' or 'aws' (default is 'gcp').; url (str, optional) – Optional URL to annotation DB configuration, if using custom configuration; (default is None).; config (str, optional) – Optional dict describing an annotation DB configuration, if using; custom configuration (default is None). Note; The 'aws' cloud platform is currently only available for the 'us'; region. Examples; Create an annotation database connecting to the default Hail Annotation DB:; >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'). Attributes. available_datasets; List of names of available annotation datasets. Methods. annotate_rows_db; Add annotations from datasets specified by name to a relational object. annotate_rows_db(rel, *names)[source]; Add annotations from datasets specified by name to a relational; object.; List datasets with available_datasets.; An interactive query builder is available in the; Hail Annotation Database documentation.; Examples; Annotate a MatrixTable with gnomad_lof_metrics:; >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'); >>> mt = db.annotate_rows_db(mt, 'gnomad_lof_metrics') . Annotate a Table with clinvar_gene_summary, CADD,; and DANN:; >>> db = hl.experimental.DB(region=",MatchSource.WIKI,docs/0.2/experimental/hail.experimental.DB.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/experimental/hail.experimental.DB.html
https://hail.is/docs/0.2/experimental/hail.experimental.DB.html:3617,Deployability,update,updated,3617,"ult is None). Note; The 'aws' cloud platform is currently only available for the 'us'; region. Examples; Create an annotation database connecting to the default Hail Annotation DB:; >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'). Attributes. available_datasets; List of names of available annotation datasets. Methods. annotate_rows_db; Add annotations from datasets specified by name to a relational object. annotate_rows_db(rel, *names)[source]; Add annotations from datasets specified by name to a relational; object.; List datasets with available_datasets.; An interactive query builder is available in the; Hail Annotation Database documentation.; Examples; Annotate a MatrixTable with gnomad_lof_metrics:; >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'); >>> mt = db.annotate_rows_db(mt, 'gnomad_lof_metrics') . Annotate a Table with clinvar_gene_summary, CADD,; and DANN:; >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'); >>> ht = db.annotate_rows_db(ht, 'clinvar_gene_summary', 'CADD', 'DANN') . Notes; If a dataset is gene-keyed, the annotation will be a dictionary mapping; from gene name to the annotation value. There will be one entry for each; gene overlapping the given locus.; If a dataset does not have unique rows for each key (consider the; gencode genes, which may overlap; and clinvar_variant_summary,; which contains many overlapping multiple nucleotide variants), then the; result will be an array of annotation values, one for each row. Parameters:. rel (MatrixTable or Table) – The relational object to which to add annotations.; names (varargs of str) – The names of the datasets with which to annotate rel. Returns:; MatrixTable or Table – The relational object rel, with the annotations from names; added. property available_datasets; List of names of available annotation datasets. Returns:; list – List of available annotation datasets. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/experimental/hail.experimental.DB.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/experimental/hail.experimental.DB.html
https://hail.is/docs/0.2/experimental/hail.experimental.DB.html:884,Modifiability,config,configuration,884,"﻿. Hail | ; DB. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Experimental; DB. View page source. DB. class hail.experimental.DB[source]; An annotation database instance.; This class facilitates the annotation of genetic datasets with variant annotations. It accepts; either an HTTP(S) URL to an Annotation DB configuration or a Python dict describing an; Annotation DB configuration. User must specify the region (aws: 'us', gcp:; 'us-central1' or 'europe-west1') in which the cluster is running if connecting to the; default Hail Annotation DB. User must also specify the cloud platform that they are using; ('gcp' or 'aws'). Parameters:. region (str) – Region cluster is running in, either 'us', 'us-central1', or 'europe-west1'; (default is 'us-central1').; cloud (str) – Cloud platform, either 'gcp' or 'aws' (default is 'gcp').; url (str, optional) – Optional URL to annotation DB configuration, if using custom configuration; (default is None).; config (str, optional) – Optional dict describing an annotation DB configuration, if using; custom configuration (default is None). Note; The 'aws' cloud platform is currently only available for the 'us'; region. Examples; Create an annotation database connecting to the default Hail Annotation DB:; >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'). Attributes. available_datasets; List of names of available annotation datasets. Methods. annotate_rows_db; Add",MatchSource.WIKI,docs/0.2/experimental/hail.experimental.DB.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/experimental/hail.experimental.DB.html
https://hail.is/docs/0.2/experimental/hail.experimental.DB.html:944,Modifiability,config,configuration,944,"﻿. Hail | ; DB. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Experimental; DB. View page source. DB. class hail.experimental.DB[source]; An annotation database instance.; This class facilitates the annotation of genetic datasets with variant annotations. It accepts; either an HTTP(S) URL to an Annotation DB configuration or a Python dict describing an; Annotation DB configuration. User must specify the region (aws: 'us', gcp:; 'us-central1' or 'europe-west1') in which the cluster is running if connecting to the; default Hail Annotation DB. User must also specify the cloud platform that they are using; ('gcp' or 'aws'). Parameters:. region (str) – Region cluster is running in, either 'us', 'us-central1', or 'europe-west1'; (default is 'us-central1').; cloud (str) – Cloud platform, either 'gcp' or 'aws' (default is 'gcp').; url (str, optional) – Optional URL to annotation DB configuration, if using custom configuration; (default is None).; config (str, optional) – Optional dict describing an annotation DB configuration, if using; custom configuration (default is None). Note; The 'aws' cloud platform is currently only available for the 'us'; region. Examples; Create an annotation database connecting to the default Hail Annotation DB:; >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'). Attributes. available_datasets; List of names of available annotation datasets. Methods. annotate_rows_db; Add",MatchSource.WIKI,docs/0.2/experimental/hail.experimental.DB.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/experimental/hail.experimental.DB.html
https://hail.is/docs/0.2/experimental/hail.experimental.DB.html:1461,Modifiability,config,configuration,1461,"eets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Experimental; DB. View page source. DB. class hail.experimental.DB[source]; An annotation database instance.; This class facilitates the annotation of genetic datasets with variant annotations. It accepts; either an HTTP(S) URL to an Annotation DB configuration or a Python dict describing an; Annotation DB configuration. User must specify the region (aws: 'us', gcp:; 'us-central1' or 'europe-west1') in which the cluster is running if connecting to the; default Hail Annotation DB. User must also specify the cloud platform that they are using; ('gcp' or 'aws'). Parameters:. region (str) – Region cluster is running in, either 'us', 'us-central1', or 'europe-west1'; (default is 'us-central1').; cloud (str) – Cloud platform, either 'gcp' or 'aws' (default is 'gcp').; url (str, optional) – Optional URL to annotation DB configuration, if using custom configuration; (default is None).; config (str, optional) – Optional dict describing an annotation DB configuration, if using; custom configuration (default is None). Note; The 'aws' cloud platform is currently only available for the 'us'; region. Examples; Create an annotation database connecting to the default Hail Annotation DB:; >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'). Attributes. available_datasets; List of names of available annotation datasets. Methods. annotate_rows_db; Add annotations from datasets specified by name to a relational object. annotate_rows_db(rel, *names)[source]; Add annotations from datasets specified by name to a relational; object.; List datasets with available_datasets.; An interactive query builder is available in the; Hail Annotation Database documentation.; Examples; Annotate a MatrixTable with gnomad_lof_metrics:; >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'); >>> mt = db.annotate_rows_db",MatchSource.WIKI,docs/0.2/experimental/hail.experimental.DB.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/experimental/hail.experimental.DB.html
https://hail.is/docs/0.2/experimental/hail.experimental.DB.html:1492,Modifiability,config,configuration,1492,"eets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Experimental; DB. View page source. DB. class hail.experimental.DB[source]; An annotation database instance.; This class facilitates the annotation of genetic datasets with variant annotations. It accepts; either an HTTP(S) URL to an Annotation DB configuration or a Python dict describing an; Annotation DB configuration. User must specify the region (aws: 'us', gcp:; 'us-central1' or 'europe-west1') in which the cluster is running if connecting to the; default Hail Annotation DB. User must also specify the cloud platform that they are using; ('gcp' or 'aws'). Parameters:. region (str) – Region cluster is running in, either 'us', 'us-central1', or 'europe-west1'; (default is 'us-central1').; cloud (str) – Cloud platform, either 'gcp' or 'aws' (default is 'gcp').; url (str, optional) – Optional URL to annotation DB configuration, if using custom configuration; (default is None).; config (str, optional) – Optional dict describing an annotation DB configuration, if using; custom configuration (default is None). Note; The 'aws' cloud platform is currently only available for the 'us'; region. Examples; Create an annotation database connecting to the default Hail Annotation DB:; >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'). Attributes. available_datasets; List of names of available annotation datasets. Methods. annotate_rows_db; Add annotations from datasets specified by name to a relational object. annotate_rows_db(rel, *names)[source]; Add annotations from datasets specified by name to a relational; object.; List datasets with available_datasets.; An interactive query builder is available in the; Hail Annotation Database documentation.; Examples; Annotate a MatrixTable with gnomad_lof_metrics:; >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'); >>> mt = db.annotate_rows_db",MatchSource.WIKI,docs/0.2/experimental/hail.experimental.DB.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/experimental/hail.experimental.DB.html
https://hail.is/docs/0.2/experimental/hail.experimental.DB.html:1527,Modifiability,config,config,1527,"; Hail. Python API; Hail Query Python API; Experimental; DB. View page source. DB. class hail.experimental.DB[source]; An annotation database instance.; This class facilitates the annotation of genetic datasets with variant annotations. It accepts; either an HTTP(S) URL to an Annotation DB configuration or a Python dict describing an; Annotation DB configuration. User must specify the region (aws: 'us', gcp:; 'us-central1' or 'europe-west1') in which the cluster is running if connecting to the; default Hail Annotation DB. User must also specify the cloud platform that they are using; ('gcp' or 'aws'). Parameters:. region (str) – Region cluster is running in, either 'us', 'us-central1', or 'europe-west1'; (default is 'us-central1').; cloud (str) – Cloud platform, either 'gcp' or 'aws' (default is 'gcp').; url (str, optional) – Optional URL to annotation DB configuration, if using custom configuration; (default is None).; config (str, optional) – Optional dict describing an annotation DB configuration, if using; custom configuration (default is None). Note; The 'aws' cloud platform is currently only available for the 'us'; region. Examples; Create an annotation database connecting to the default Hail Annotation DB:; >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'). Attributes. available_datasets; List of names of available annotation datasets. Methods. annotate_rows_db; Add annotations from datasets specified by name to a relational object. annotate_rows_db(rel, *names)[source]; Add annotations from datasets specified by name to a relational; object.; List datasets with available_datasets.; An interactive query builder is available in the; Hail Annotation Database documentation.; Examples; Annotate a MatrixTable with gnomad_lof_metrics:; >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'); >>> mt = db.annotate_rows_db(mt, 'gnomad_lof_metrics') . Annotate a Table with clinvar_gene_summary, CADD,; and DANN:; >>> db = hl.experimental.DB(region=",MatchSource.WIKI,docs/0.2/experimental/hail.experimental.DB.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/experimental/hail.experimental.DB.html
https://hail.is/docs/0.2/experimental/hail.experimental.DB.html:1594,Modifiability,config,configuration,1594,"; Hail. Python API; Hail Query Python API; Experimental; DB. View page source. DB. class hail.experimental.DB[source]; An annotation database instance.; This class facilitates the annotation of genetic datasets with variant annotations. It accepts; either an HTTP(S) URL to an Annotation DB configuration or a Python dict describing an; Annotation DB configuration. User must specify the region (aws: 'us', gcp:; 'us-central1' or 'europe-west1') in which the cluster is running if connecting to the; default Hail Annotation DB. User must also specify the cloud platform that they are using; ('gcp' or 'aws'). Parameters:. region (str) – Region cluster is running in, either 'us', 'us-central1', or 'europe-west1'; (default is 'us-central1').; cloud (str) – Cloud platform, either 'gcp' or 'aws' (default is 'gcp').; url (str, optional) – Optional URL to annotation DB configuration, if using custom configuration; (default is None).; config (str, optional) – Optional dict describing an annotation DB configuration, if using; custom configuration (default is None). Note; The 'aws' cloud platform is currently only available for the 'us'; region. Examples; Create an annotation database connecting to the default Hail Annotation DB:; >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'). Attributes. available_datasets; List of names of available annotation datasets. Methods. annotate_rows_db; Add annotations from datasets specified by name to a relational object. annotate_rows_db(rel, *names)[source]; Add annotations from datasets specified by name to a relational; object.; List datasets with available_datasets.; An interactive query builder is available in the; Hail Annotation Database documentation.; Examples; Annotate a MatrixTable with gnomad_lof_metrics:; >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'); >>> mt = db.annotate_rows_db(mt, 'gnomad_lof_metrics') . Annotate a Table with clinvar_gene_summary, CADD,; and DANN:; >>> db = hl.experimental.DB(region=",MatchSource.WIKI,docs/0.2/experimental/hail.experimental.DB.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/experimental/hail.experimental.DB.html
https://hail.is/docs/0.2/experimental/hail.experimental.DB.html:1626,Modifiability,config,configuration,1626,"; Hail. Python API; Hail Query Python API; Experimental; DB. View page source. DB. class hail.experimental.DB[source]; An annotation database instance.; This class facilitates the annotation of genetic datasets with variant annotations. It accepts; either an HTTP(S) URL to an Annotation DB configuration or a Python dict describing an; Annotation DB configuration. User must specify the region (aws: 'us', gcp:; 'us-central1' or 'europe-west1') in which the cluster is running if connecting to the; default Hail Annotation DB. User must also specify the cloud platform that they are using; ('gcp' or 'aws'). Parameters:. region (str) – Region cluster is running in, either 'us', 'us-central1', or 'europe-west1'; (default is 'us-central1').; cloud (str) – Cloud platform, either 'gcp' or 'aws' (default is 'gcp').; url (str, optional) – Optional URL to annotation DB configuration, if using custom configuration; (default is None).; config (str, optional) – Optional dict describing an annotation DB configuration, if using; custom configuration (default is None). Note; The 'aws' cloud platform is currently only available for the 'us'; region. Examples; Create an annotation database connecting to the default Hail Annotation DB:; >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'). Attributes. available_datasets; List of names of available annotation datasets. Methods. annotate_rows_db; Add annotations from datasets specified by name to a relational object. annotate_rows_db(rel, *names)[source]; Add annotations from datasets specified by name to a relational; object.; List datasets with available_datasets.; An interactive query builder is available in the; Hail Annotation Database documentation.; Examples; Annotate a MatrixTable with gnomad_lof_metrics:; >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'); >>> mt = db.annotate_rows_db(mt, 'gnomad_lof_metrics') . Annotate a Table with clinvar_gene_summary, CADD,; and DANN:; >>> db = hl.experimental.DB(region=",MatchSource.WIKI,docs/0.2/experimental/hail.experimental.DB.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/experimental/hail.experimental.DB.html
https://hail.is/docs/0.2/experimental/index.html:4345,Availability,avail,available,4345,"entries of the mt by column as separate text files. pc_project(call_expr, loadings_expr, af_expr); Projects genotypes onto pre-computed PCs. dplyr-inspired Methods. gather(ht, key, value, *fields); Collapse fields into key-value pairs. separate(ht, field, into, delim); Separate a field into multiple fields by splitting on a delimiter character or position. spread(ht, field, value[, key]); Spread a key-value pair of fields across multiple fields. Functions. hail.experimental.load_dataset(name, version, reference_genome, region='us-central1', cloud='gcp')[source]; Load a genetic dataset from Hail’s repository.; Example; >>> # Load the gnomAD ""HGDP + 1000 Genomes"" dense MatrixTable with GRCh38 coordinates.; >>> mt = hl.experimental.load_dataset(name='gnomad_hgdp_1kg_subset_dense',; ... version='3.1.2',; ... reference_genome='GRCh38',; ... region='us-central1',; ... cloud='gcp'). Parameters:. name (str) – Name of the dataset to load.; version (str, optional) – Version of the named dataset to load (see available versions in; documentation). Possibly None for some datasets.; reference_genome (str, optional) – Reference genome build, 'GRCh37' or 'GRCh38'. Possibly None; for some datasets.; region (str) – Specify region for bucket, 'us', 'us-central1', or 'europe-west1', (default is; 'us-central1').; cloud (str) – Specify if using Google Cloud Platform or Amazon Web Services,; 'gcp' or 'aws' (default is 'gcp'). Note; The 'aws' cloud platform is currently only available for the 'us'; region. Returns:; Table, MatrixTable, or BlockMatrix. hail.experimental.ld_score(entry_expr, locus_expr, radius, coord_expr=None, annotation_exprs=None, block_size=None)[source]; Calculate LD scores.; Example; >>> # Load genetic data into MatrixTable; >>> mt = hl.import_plink(bed='data/ldsc.bed',; ... bim='data/ldsc.bim',; ... fam='data/ldsc.fam'). >>> # Create locus-keyed Table with numeric variant annotations; >>> ht = hl.import_table('data/ldsc.annot',; ... types={'BP': hl.tint,; ... 'bin",MatchSource.WIKI,docs/0.2/experimental/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/experimental/index.html
https://hail.is/docs/0.2/experimental/index.html:4808,Availability,avail,available,4808,"xperimental.load_dataset(name, version, reference_genome, region='us-central1', cloud='gcp')[source]; Load a genetic dataset from Hail’s repository.; Example; >>> # Load the gnomAD ""HGDP + 1000 Genomes"" dense MatrixTable with GRCh38 coordinates.; >>> mt = hl.experimental.load_dataset(name='gnomad_hgdp_1kg_subset_dense',; ... version='3.1.2',; ... reference_genome='GRCh38',; ... region='us-central1',; ... cloud='gcp'). Parameters:. name (str) – Name of the dataset to load.; version (str, optional) – Version of the named dataset to load (see available versions in; documentation). Possibly None for some datasets.; reference_genome (str, optional) – Reference genome build, 'GRCh37' or 'GRCh38'. Possibly None; for some datasets.; region (str) – Specify region for bucket, 'us', 'us-central1', or 'europe-west1', (default is; 'us-central1').; cloud (str) – Specify if using Google Cloud Platform or Amazon Web Services,; 'gcp' or 'aws' (default is 'gcp'). Note; The 'aws' cloud platform is currently only available for the 'us'; region. Returns:; Table, MatrixTable, or BlockMatrix. hail.experimental.ld_score(entry_expr, locus_expr, radius, coord_expr=None, annotation_exprs=None, block_size=None)[source]; Calculate LD scores.; Example; >>> # Load genetic data into MatrixTable; >>> mt = hl.import_plink(bed='data/ldsc.bed',; ... bim='data/ldsc.bim',; ... fam='data/ldsc.fam'). >>> # Create locus-keyed Table with numeric variant annotations; >>> ht = hl.import_table('data/ldsc.annot',; ... types={'BP': hl.tint,; ... 'binary': hl.tfloat,; ... 'continuous': hl.tfloat}); >>> ht = ht.annotate(locus=hl.locus(ht.CHR, ht.BP)); >>> ht = ht.key_by('locus'). >>> # Annotate MatrixTable with external annotations; >>> mt = mt.annotate_rows(binary_annotation=ht[mt.locus].binary,; ... continuous_annotation=ht[mt.locus].continuous). >>> # Calculate LD scores using centimorgan coordinates; >>> ht_scores = hl.experimental.ld_score(entry_expr=mt.GT.n_alt_alleles(),; ... locus_expr=mt.locus,; ... radi",MatchSource.WIKI,docs/0.2/experimental/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/experimental/index.html
https://hail.is/docs/0.2/experimental/index.html:13281,Availability,error,error,13281,be a single; expression; compound keys are not accepted.; weight_expr and ld_score_expr must be row-indexed; fields.; chi_sq_exprs must be a single entry-indexed field; (not a list of fields).; n_samples_exprs must be a single entry-indexed field; (not a list of fields).; The phenotype field that keys the table returned by; ld_score_regression() will have values corresponding to the; column keys of the input matrix table. This function returns a Table with one row per set of summary; statistics passed to the chi_sq_exprs argument. The following; row-indexed fields are included in the table:. phenotype (tstr) – The name of the phenotype. The; returned table is keyed by this field. See the notes below for; details on the possible values of this field.; mean_chi_sq (tfloat64) – The mean chi-squared; test statistic for the given phenotype.; intercept (Struct) – Contains fields:. estimate (tfloat64) – A point estimate of the; intercept \(1 + Na\).; standard_error (tfloat64) – An estimate of; the standard error of this point estimate. snp_heritability (Struct) – Contains fields:. estimate (tfloat64) – A point estimate of the; SNP-heritability \(h_g^2\).; standard_error (tfloat64) – An estimate of; the standard error of this point estimate. Warning; ld_score_regression() considers only the rows for which both row; fields weight_expr and ld_score_expr are defined. Rows with missing; values in either field are removed prior to fitting the LD score; regression model. Parameters:. weight_expr (Float64Expression) – Row-indexed expression for the LD scores used to derive; variant weights in the model.; ld_score_expr (Float64Expression) – Row-indexed expression for the LD scores used as covariates; in the model.; chi_sq_exprs (Float64Expression or list of) – Float64Expression; One or more row-indexed (if table) or entry-indexed; (if matrix table) expressions for chi-squared; statistics resulting from genome-wide association; studies (GWAS).; n_samples_exprs (NumericExpression or ,MatchSource.WIKI,docs/0.2/experimental/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/experimental/index.html
https://hail.is/docs/0.2/experimental/index.html:13490,Availability,error,error,13490, must be a single entry-indexed field; (not a list of fields).; The phenotype field that keys the table returned by; ld_score_regression() will have values corresponding to the; column keys of the input matrix table. This function returns a Table with one row per set of summary; statistics passed to the chi_sq_exprs argument. The following; row-indexed fields are included in the table:. phenotype (tstr) – The name of the phenotype. The; returned table is keyed by this field. See the notes below for; details on the possible values of this field.; mean_chi_sq (tfloat64) – The mean chi-squared; test statistic for the given phenotype.; intercept (Struct) – Contains fields:. estimate (tfloat64) – A point estimate of the; intercept \(1 + Na\).; standard_error (tfloat64) – An estimate of; the standard error of this point estimate. snp_heritability (Struct) – Contains fields:. estimate (tfloat64) – A point estimate of the; SNP-heritability \(h_g^2\).; standard_error (tfloat64) – An estimate of; the standard error of this point estimate. Warning; ld_score_regression() considers only the rows for which both row; fields weight_expr and ld_score_expr are defined. Rows with missing; values in either field are removed prior to fitting the LD score; regression model. Parameters:. weight_expr (Float64Expression) – Row-indexed expression for the LD scores used to derive; variant weights in the model.; ld_score_expr (Float64Expression) – Row-indexed expression for the LD scores used as covariates; in the model.; chi_sq_exprs (Float64Expression or list of) – Float64Expression; One or more row-indexed (if table) or entry-indexed; (if matrix table) expressions for chi-squared; statistics resulting from genome-wide association; studies (GWAS).; n_samples_exprs (NumericExpression or list of) – NumericExpression; One or more row-indexed (if table) or entry-indexed; (if matrix table) expressions indicating the number of; samples used in the studies that generated the test; statistics suppli,MatchSource.WIKI,docs/0.2/experimental/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/experimental/index.html
https://hail.is/docs/0.2/experimental/index.html:14589,Availability,error,errors,14589,"ion() considers only the rows for which both row; fields weight_expr and ld_score_expr are defined. Rows with missing; values in either field are removed prior to fitting the LD score; regression model. Parameters:. weight_expr (Float64Expression) – Row-indexed expression for the LD scores used to derive; variant weights in the model.; ld_score_expr (Float64Expression) – Row-indexed expression for the LD scores used as covariates; in the model.; chi_sq_exprs (Float64Expression or list of) – Float64Expression; One or more row-indexed (if table) or entry-indexed; (if matrix table) expressions for chi-squared; statistics resulting from genome-wide association; studies (GWAS).; n_samples_exprs (NumericExpression or list of) – NumericExpression; One or more row-indexed (if table) or entry-indexed; (if matrix table) expressions indicating the number of; samples used in the studies that generated the test; statistics supplied to chi_sq_exprs.; n_blocks (int) – The number of blocks used in the jackknife approach to; estimating standard errors.; two_step_threshold (int) – Variants with chi-squared statistics greater than this; value are excluded in the first step of the two-step; procedure used to fit the model.; n_reference_panel_variants (int, optional) – Number of variants used to estimate the; SNP-heritability \(h_g^2\). Returns:; Table – Table keyed by phenotype with intercept and heritability estimates; for each phenotype passed to the function. hail.experimental.write_expression(expr, path, overwrite=False)[source]; Write an Expression.; In the same vein as Python’s pickle, write out an expression; that does not have a source (such as one that comes from; Table.aggregate with _localize=False).; Example; >>> ht = hl.utils.range_table(100).annotate(x=hl.rand_norm()); >>> mean_norm = ht.aggregate(hl.agg.mean(ht.x), _localize=False); >>> mean_norm; >>> hl.eval(mean_norm); >>> hl.experimental.write_expression(mean_norm, 'output/expression.he'). Parameters:. expr (Expressi",MatchSource.WIKI,docs/0.2/experimental/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/experimental/index.html
https://hail.is/docs/0.2/experimental/index.html:27672,Availability,avail,available,27672,"ines where; seqname is not consistent with the reference genome.; min_partitions (int or None) – Minimum number of partitions (passed to import_table).; force_bgz (bool) – If True, load files as blocked gzip files, assuming; that they were actually compressed using the BGZ codec. This option is; useful when the file extension is not '.bgz', but the file is; blocked gzip, so that the file can be read in parallel and not on a; single node.; force (bool) – If True, load gzipped files serially on one core. This should; be used only when absolutely necessary, as processing time will be; increased due to lack of parallelism. Returns:; Table. hail.experimental.get_gene_intervals(gene_symbols=None, gene_ids=None, transcript_ids=None, verbose=True, reference_genome=None, gtf_file=None)[source]; Get intervals of genes or transcripts.; Get the boundaries of genes or transcripts from a GTF file, for quick filtering of a Table or MatrixTable.; On Google Cloud platform:; Gencode v19 (GRCh37) GTF available at: gs://hail-common/references/gencode/gencode.v19.annotation.gtf.bgz; Gencode v29 (GRCh38) GTF available at: gs://hail-common/references/gencode/gencode.v29.annotation.gtf.bgz; Example; >>> hl.filter_intervals(ht, get_gene_intervals(gene_symbols=['PCSK9'], reference_genome='GRCh37')) . Parameters:. gene_symbols (list of str, optional) – Gene symbols (e.g. PCSK9).; gene_ids (list of str, optional) – Gene IDs (e.g. ENSG00000223972).; transcript_ids (list of str, optional) – Transcript IDs (e.g. ENSG00000223972).; verbose (bool) – If True, print which genes and transcripts were matched in the GTF file.; reference_genome (str or ReferenceGenome, optional) – Reference genome to use (passed along to import_gtf).; gtf_file (str) – GTF file to load. If none is provided, but reference_genome is one of; GRCh37 or GRCh38, a default will be used (on Google Cloud Platform). Returns:; list of Interval. hail.experimental.export_entries_by_col(mt, path, batch_size=256, bgzip=True, header_jso",MatchSource.WIKI,docs/0.2/experimental/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/experimental/index.html
https://hail.is/docs/0.2/experimental/index.html:27779,Availability,avail,available,27779,"itions (passed to import_table).; force_bgz (bool) – If True, load files as blocked gzip files, assuming; that they were actually compressed using the BGZ codec. This option is; useful when the file extension is not '.bgz', but the file is; blocked gzip, so that the file can be read in parallel and not on a; single node.; force (bool) – If True, load gzipped files serially on one core. This should; be used only when absolutely necessary, as processing time will be; increased due to lack of parallelism. Returns:; Table. hail.experimental.get_gene_intervals(gene_symbols=None, gene_ids=None, transcript_ids=None, verbose=True, reference_genome=None, gtf_file=None)[source]; Get intervals of genes or transcripts.; Get the boundaries of genes or transcripts from a GTF file, for quick filtering of a Table or MatrixTable.; On Google Cloud platform:; Gencode v19 (GRCh37) GTF available at: gs://hail-common/references/gencode/gencode.v19.annotation.gtf.bgz; Gencode v29 (GRCh38) GTF available at: gs://hail-common/references/gencode/gencode.v29.annotation.gtf.bgz; Example; >>> hl.filter_intervals(ht, get_gene_intervals(gene_symbols=['PCSK9'], reference_genome='GRCh37')) . Parameters:. gene_symbols (list of str, optional) – Gene symbols (e.g. PCSK9).; gene_ids (list of str, optional) – Gene IDs (e.g. ENSG00000223972).; transcript_ids (list of str, optional) – Transcript IDs (e.g. ENSG00000223972).; verbose (bool) – If True, print which genes and transcripts were matched in the GTF file.; reference_genome (str or ReferenceGenome, optional) – Reference genome to use (passed along to import_gtf).; gtf_file (str) – GTF file to load. If none is provided, but reference_genome is one of; GRCh37 or GRCh38, a default will be used (on Google Cloud Platform). Returns:; list of Interval. hail.experimental.export_entries_by_col(mt, path, batch_size=256, bgzip=True, header_json_in_file=True, use_string_key_as_file_name=False)[source]; Export entries of the mt by column as separate text files.",MatchSource.WIKI,docs/0.2/experimental/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/experimental/index.html
https://hail.is/docs/0.2/experimental/index.html:40922,Availability,error,error,40922,"enotypes onto loadings; >>> ht = pc_project(mt_to_project.GT, loadings_ht.loadings, loadings_ht.af) . Parameters:. call_expr (CallExpression) – Entry-indexed call expression for genotypes; to project onto loadings.; loadings_expr (ArrayNumericExpression) – Location of expression for loadings; af_expr (Float64Expression) – Location of expression for allele frequency. Returns:; Table – Table with scores calculated from loadings in column scores. hail.experimental.loop(f, typ, *args)[source]; Define and call a tail-recursive function with given arguments.; Notes; The argument f must be a function where the first argument defines the; recursive call, and the remaining arguments are the arguments to the; recursive function, e.g. to define the recursive function. \[f(x, y) = \begin{cases}; y & \textrm{if } x \equiv 0 \\; f(x - 1, y + x) & \textrm{otherwise}; \end{cases}\]; we would write:; >>> f = lambda recur, x, y: hl.if_else(x == 0, y, recur(x - 1, y + x)); Full recursion is not supported, and any non-tail-recursive methods will; throw an error when called.; This means that the result of any recursive call within the function must; also be the result of the entire function, without modification. Let’s; consider two different recursive definitions for the triangle function; \(f(x) = 0 + 1 + \dots + x\):; >>> def triangle1(x):; ... if x == 1:; ... return x; ... return x + triangle1(x - 1). >>> def triangle2(x, total):; ... if x == 0:; ... return total; ... return triangle2(x - 1, total + x). The first function definition, triangle1, will call itself and then add x.; This is an example of a non-tail recursive function, since triangle1(9); needs to modify the result of the inner recursive call to triangle1(8) by; adding 9 to the result.; The second function is tail recursive: the result of triangle2(9, 0) is; the same as the result of the inner recursive call, triangle2(8, 9).; Example; To find the sum of all the numbers from n=1…10:; >>> triangle_f = lambda f, x, total: ",MatchSource.WIKI,docs/0.2/experimental/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/experimental/index.html
https://hail.is/docs/0.2/experimental/index.html:42538,Availability,error,error,42538,"(9); needs to modify the result of the inner recursive call to triangle1(8) by; adding 9 to the result.; The second function is tail recursive: the result of triangle2(9, 0) is; the same as the result of the inner recursive call, triangle2(8, 9).; Example; To find the sum of all the numbers from n=1…10:; >>> triangle_f = lambda f, x, total: hl.if_else(x == 0, total, f(x - 1, total + x)); >>> x = hl.experimental.loop(triangle_f, hl.tint32, 10, 0); >>> hl.eval(x); 55; Let’s say we want to find the root of a polynomial equation:; >>> def polynomial(x):; … return 5 * x**3 - 2 * x - 1; We’ll use Newton’s method<https://en.wikipedia.org/wiki/Newton%27s_method>; to find it, so we’ll also define the derivative:; >>> def derivative(x):; ... return 15 * x**2 - 2. and starting at \(x_0 = 0\), we’ll compute the next step \(x_{i+1} = x_i - \frac{f(x_i)}{f'(x_i)}\); until the difference between \(x_{i}\) and \(x_{i+1}\) falls below; our convergence threshold:; >>> threshold = 0.005; >>> def find_root(f, guess, error):; ... converged = hl.is_defined(error) & (error < threshold); ... new_guess = guess - (polynomial(guess) / derivative(guess)); ... new_error = hl.abs(new_guess - guess); ... return hl.if_else(converged, guess, f(new_guess, new_error)); >>> x = hl.experimental.loop(find_root, hl.tfloat, 0.0, hl.missing(hl.tfloat)); >>> hl.eval(x); 0.8052291984599675. Warning; Using arguments of a type other than numeric types and booleans can cause; memory issues if if you expect the recursive call to happen many times. Parameters:. f (function ( (marker, *args) -> Expression) – Function of one callable marker, denoting where the recursive call (or calls) is located,; and many args, the loop variables.; typ (str or HailType) – Type the loop returns.; args (variable-length args of Expression) – Expressions to initialize the loop values. Returns:; Expression – Result of the loop with args as initial loop values. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, ",MatchSource.WIKI,docs/0.2/experimental/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/experimental/index.html
https://hail.is/docs/0.2/experimental/index.html:42577,Availability,error,error,42577," to modify the result of the inner recursive call to triangle1(8) by; adding 9 to the result.; The second function is tail recursive: the result of triangle2(9, 0) is; the same as the result of the inner recursive call, triangle2(8, 9).; Example; To find the sum of all the numbers from n=1…10:; >>> triangle_f = lambda f, x, total: hl.if_else(x == 0, total, f(x - 1, total + x)); >>> x = hl.experimental.loop(triangle_f, hl.tint32, 10, 0); >>> hl.eval(x); 55; Let’s say we want to find the root of a polynomial equation:; >>> def polynomial(x):; … return 5 * x**3 - 2 * x - 1; We’ll use Newton’s method<https://en.wikipedia.org/wiki/Newton%27s_method>; to find it, so we’ll also define the derivative:; >>> def derivative(x):; ... return 15 * x**2 - 2. and starting at \(x_0 = 0\), we’ll compute the next step \(x_{i+1} = x_i - \frac{f(x_i)}{f'(x_i)}\); until the difference between \(x_{i}\) and \(x_{i+1}\) falls below; our convergence threshold:; >>> threshold = 0.005; >>> def find_root(f, guess, error):; ... converged = hl.is_defined(error) & (error < threshold); ... new_guess = guess - (polynomial(guess) / derivative(guess)); ... new_error = hl.abs(new_guess - guess); ... return hl.if_else(converged, guess, f(new_guess, new_error)); >>> x = hl.experimental.loop(find_root, hl.tfloat, 0.0, hl.missing(hl.tfloat)); >>> hl.eval(x); 0.8052291984599675. Warning; Using arguments of a type other than numeric types and booleans can cause; memory issues if if you expect the recursive call to happen many times. Parameters:. f (function ( (marker, *args) -> Expression) – Function of one callable marker, denoting where the recursive call (or calls) is located,; and many args, the loop variables.; typ (str or HailType) – Type the loop returns.; args (variable-length args of Expression) – Expressions to initialize the loop values. Returns:; Expression – Result of the loop with args as initial loop values. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/experimental/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/experimental/index.html
https://hail.is/docs/0.2/experimental/index.html:42587,Availability,error,error,42587," to modify the result of the inner recursive call to triangle1(8) by; adding 9 to the result.; The second function is tail recursive: the result of triangle2(9, 0) is; the same as the result of the inner recursive call, triangle2(8, 9).; Example; To find the sum of all the numbers from n=1…10:; >>> triangle_f = lambda f, x, total: hl.if_else(x == 0, total, f(x - 1, total + x)); >>> x = hl.experimental.loop(triangle_f, hl.tint32, 10, 0); >>> hl.eval(x); 55; Let’s say we want to find the root of a polynomial equation:; >>> def polynomial(x):; … return 5 * x**3 - 2 * x - 1; We’ll use Newton’s method<https://en.wikipedia.org/wiki/Newton%27s_method>; to find it, so we’ll also define the derivative:; >>> def derivative(x):; ... return 15 * x**2 - 2. and starting at \(x_0 = 0\), we’ll compute the next step \(x_{i+1} = x_i - \frac{f(x_i)}{f'(x_i)}\); until the difference between \(x_{i}\) and \(x_{i+1}\) falls below; our convergence threshold:; >>> threshold = 0.005; >>> def find_root(f, guess, error):; ... converged = hl.is_defined(error) & (error < threshold); ... new_guess = guess - (polynomial(guess) / derivative(guess)); ... new_error = hl.abs(new_guess - guess); ... return hl.if_else(converged, guess, f(new_guess, new_error)); >>> x = hl.experimental.loop(find_root, hl.tfloat, 0.0, hl.missing(hl.tfloat)); >>> hl.eval(x); 0.8052291984599675. Warning; Using arguments of a type other than numeric types and booleans can cause; memory issues if if you expect the recursive call to happen many times. Parameters:. f (function ( (marker, *args) -> Expression) – Function of one callable marker, denoting where the recursive call (or calls) is located,; and many args, the loop variables.; typ (str or HailType) – Type the loop returns.; args (variable-length args of Expression) – Expressions to initialize the loop values. Returns:; Expression – Result of the loop with args as initial loop values. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/experimental/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/experimental/index.html
https://hail.is/docs/0.2/experimental/index.html:5352,Deployability,continuous,continuous,5352,"ions in; documentation). Possibly None for some datasets.; reference_genome (str, optional) – Reference genome build, 'GRCh37' or 'GRCh38'. Possibly None; for some datasets.; region (str) – Specify region for bucket, 'us', 'us-central1', or 'europe-west1', (default is; 'us-central1').; cloud (str) – Specify if using Google Cloud Platform or Amazon Web Services,; 'gcp' or 'aws' (default is 'gcp'). Note; The 'aws' cloud platform is currently only available for the 'us'; region. Returns:; Table, MatrixTable, or BlockMatrix. hail.experimental.ld_score(entry_expr, locus_expr, radius, coord_expr=None, annotation_exprs=None, block_size=None)[source]; Calculate LD scores.; Example; >>> # Load genetic data into MatrixTable; >>> mt = hl.import_plink(bed='data/ldsc.bed',; ... bim='data/ldsc.bim',; ... fam='data/ldsc.fam'). >>> # Create locus-keyed Table with numeric variant annotations; >>> ht = hl.import_table('data/ldsc.annot',; ... types={'BP': hl.tint,; ... 'binary': hl.tfloat,; ... 'continuous': hl.tfloat}); >>> ht = ht.annotate(locus=hl.locus(ht.CHR, ht.BP)); >>> ht = ht.key_by('locus'). >>> # Annotate MatrixTable with external annotations; >>> mt = mt.annotate_rows(binary_annotation=ht[mt.locus].binary,; ... continuous_annotation=ht[mt.locus].continuous). >>> # Calculate LD scores using centimorgan coordinates; >>> ht_scores = hl.experimental.ld_score(entry_expr=mt.GT.n_alt_alleles(),; ... locus_expr=mt.locus,; ... radius=1.0,; ... coord_expr=mt.cm_position,; ... annotation_exprs=[mt.binary_annotation,; ... mt.continuous_annotation]). >>> # Show results; >>> ht_scores.show(3). +---------------+-------------------+-----------------------+-------------+; | locus | binary_annotation | continuous_annotation | univariate |; +---------------+-------------------+-----------------------+-------------+; | locus<GRCh37> | float64 | float64 | float64 |; +---------------+-------------------+-----------------------+-------------+; | 20:82079 | 1.15183e+00 | 7.30145e+01 | 1.60117e+0",MatchSource.WIKI,docs/0.2/experimental/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/experimental/index.html
https://hail.is/docs/0.2/experimental/index.html:5619,Deployability,continuous,continuous,5619,"is; 'us-central1').; cloud (str) – Specify if using Google Cloud Platform or Amazon Web Services,; 'gcp' or 'aws' (default is 'gcp'). Note; The 'aws' cloud platform is currently only available for the 'us'; region. Returns:; Table, MatrixTable, or BlockMatrix. hail.experimental.ld_score(entry_expr, locus_expr, radius, coord_expr=None, annotation_exprs=None, block_size=None)[source]; Calculate LD scores.; Example; >>> # Load genetic data into MatrixTable; >>> mt = hl.import_plink(bed='data/ldsc.bed',; ... bim='data/ldsc.bim',; ... fam='data/ldsc.fam'). >>> # Create locus-keyed Table with numeric variant annotations; >>> ht = hl.import_table('data/ldsc.annot',; ... types={'BP': hl.tint,; ... 'binary': hl.tfloat,; ... 'continuous': hl.tfloat}); >>> ht = ht.annotate(locus=hl.locus(ht.CHR, ht.BP)); >>> ht = ht.key_by('locus'). >>> # Annotate MatrixTable with external annotations; >>> mt = mt.annotate_rows(binary_annotation=ht[mt.locus].binary,; ... continuous_annotation=ht[mt.locus].continuous). >>> # Calculate LD scores using centimorgan coordinates; >>> ht_scores = hl.experimental.ld_score(entry_expr=mt.GT.n_alt_alleles(),; ... locus_expr=mt.locus,; ... radius=1.0,; ... coord_expr=mt.cm_position,; ... annotation_exprs=[mt.binary_annotation,; ... mt.continuous_annotation]). >>> # Show results; >>> ht_scores.show(3). +---------------+-------------------+-----------------------+-------------+; | locus | binary_annotation | continuous_annotation | univariate |; +---------------+-------------------+-----------------------+-------------+; | locus<GRCh37> | float64 | float64 | float64 |; +---------------+-------------------+-----------------------+-------------+; | 20:82079 | 1.15183e+00 | 7.30145e+01 | 1.60117e+00 |; | 20:103517 | 2.04604e+00 | 2.75392e+02 | 4.69239e+00 |; | 20:108286 | 2.06585e+00 | 2.86453e+02 | 5.00124e+00 |; +---------------+-------------------+-----------------------+-------------+. Warning; ld_score() will fail if entry_expr results in any missing; va",MatchSource.WIKI,docs/0.2/experimental/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/experimental/index.html
https://hail.is/docs/0.2/experimental/index.html:43508,Deployability,update,updated,43508," to modify the result of the inner recursive call to triangle1(8) by; adding 9 to the result.; The second function is tail recursive: the result of triangle2(9, 0) is; the same as the result of the inner recursive call, triangle2(8, 9).; Example; To find the sum of all the numbers from n=1…10:; >>> triangle_f = lambda f, x, total: hl.if_else(x == 0, total, f(x - 1, total + x)); >>> x = hl.experimental.loop(triangle_f, hl.tint32, 10, 0); >>> hl.eval(x); 55; Let’s say we want to find the root of a polynomial equation:; >>> def polynomial(x):; … return 5 * x**3 - 2 * x - 1; We’ll use Newton’s method<https://en.wikipedia.org/wiki/Newton%27s_method>; to find it, so we’ll also define the derivative:; >>> def derivative(x):; ... return 15 * x**2 - 2. and starting at \(x_0 = 0\), we’ll compute the next step \(x_{i+1} = x_i - \frac{f(x_i)}{f'(x_i)}\); until the difference between \(x_{i}\) and \(x_{i+1}\) falls below; our convergence threshold:; >>> threshold = 0.005; >>> def find_root(f, guess, error):; ... converged = hl.is_defined(error) & (error < threshold); ... new_guess = guess - (polynomial(guess) / derivative(guess)); ... new_error = hl.abs(new_guess - guess); ... return hl.if_else(converged, guess, f(new_guess, new_error)); >>> x = hl.experimental.loop(find_root, hl.tfloat, 0.0, hl.missing(hl.tfloat)); >>> hl.eval(x); 0.8052291984599675. Warning; Using arguments of a type other than numeric types and booleans can cause; memory issues if if you expect the recursive call to happen many times. Parameters:. f (function ( (marker, *args) -> Expression) – Function of one callable marker, denoting where the recursive call (or calls) is located,; and many args, the loop variables.; typ (str or HailType) – Type the loop returns.; args (variable-length args of Expression) – Expressions to initialize the loop values. Returns:; Expression – Result of the loop with args as initial loop values. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/experimental/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/experimental/index.html
https://hail.is/docs/0.2/experimental/index.html:1172,Integrability,depend,depend,1172,"asses; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Experimental. View page source. Experimental; This module serves two functions: as a staging area for extensions of Hail; not ready for inclusion in the main package, and as a library of lightly reviewed; community submissions.; At present, the experimental module is organized into a few freestanding; modules, linked immediately below, and many freestanding functions, documented; on this page. Warning; The functionality in this module may change or disappear entirely between different versions of; Hail. If you critically depend on functionality in this module, please create an issue to request; promotion of that functionality to non-experimental. Otherwise, that functionality may disappear!. ldscsim. Contribution Guidelines; Submissions from the community are welcome! The criteria for inclusion in the; experimental module are loose and subject to change:. Function docstrings are required. Hail uses; NumPy style docstrings.; Tests are not required, but are encouraged. If you do include tests, they must; run in no more than a few seconds. Place tests as a class method on Tests in; python/tests/experimental/test_experimental.py; Code style is not strictly enforced, aside from egregious violations. We do; recommend using autopep8 though!. Annotation Database; Classes. hail.experimental.DB; An annotation database instance. Genetics Methods. load_dataset(name, version, reference_genome); Load a genetic dataset from Hail's repository. ld_score(entry_expr, locus_expr, radius[, ...]); Calculate LD scores. ld_score_regression(weight_expr, ...[, ...]); Estimate S",MatchSource.WIKI,docs/0.2/experimental/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/experimental/index.html
https://hail.is/docs/0.2/experimental/index.html:19870,Integrability,wrap,wrapper,19870,"f sex chromosomes:; - Sex chromosomes of male individuals should be haploid to be phased correctly.; - If proband_call is diploid on non-par regions of the sex chromosomes, it is assumed to be female.; Returns NA when genotype calls cannot be phased.; The following genotype calls combinations cannot be phased by transmission:; 1. One of the calls in the trio is missing; 2. The proband genotype cannot be obtained from the parents alleles (Mendelian violation); 3. All individuals of the trio are heterozygous for the same two alleles; 4. Father is diploid on non-PAR region of X or Y; 5. Proband is diploid on non-PAR region of Y; In addition, individual phased genotype calls are returned as missing in the following situations:; 1. All mother genotype calls non-PAR region of Y; 2. Diploid father genotype calls on non-PAR region of X for a male proband (proband and mother are still phased as father doesn’t participate in allele transmission). Note; phase_trio_matrix_by_transmission() provides a convenience wrapper for phasing a trio matrix. Parameters:. locus (LocusExpression) – Expression for the locus in the trio matrix; alleles (ArrayExpression) – Expression for the alleles in the trio matrix; proband_call (CallExpression) – Expression for the proband call in the trio matrix; father_call (CallExpression) – Expression for the father call in the trio matrix; mother_call (CallExpression) – Expression for the mother call in the trio matrix. Returns:; ArrayExpression – Array containing: [phased proband call, phased father call, phased mother call]. hail.experimental.phase_trio_matrix_by_transmission(tm, call_field='GT', phased_call_field='PBT_GT')[source]; Adds a phased genoype entry to a trio MatrixTable based allele transmission in the trio.; Example; >>> # Create a trio matrix; >>> pedigree = hl.Pedigree.read('data/case_control_study.fam'); >>> trio_dataset = hl.trio_matrix(dataset, pedigree, complete_trios=True). >>> # Phase trios by transmission; >>> phased_trio_datas",MatchSource.WIKI,docs/0.2/experimental/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/experimental/index.html
https://hail.is/docs/0.2/experimental/index.html:31194,Modifiability,variab,variable-length,31194,"20e-01; 7 9.7540e-01; 8 8.4848e-01; 9 3.7423e-01. Due to overhead and file system limits related to having large numbers; of open files, this function will iteratively export groups of columns.; The batch_size parameter can control the size of these groups. Parameters:. mt (MatrixTable); path (int) – Path (directory to write to.; batch_size (int) – Number of columns to write per iteration.; bgzip (bool) – BGZip output files.; header_json_in_file (bool) – Include JSON header in each component file (if False, only written to index.tsv). hail.experimental.gather(ht, key, value, *fields)[source]; Collapse fields into key-value pairs.; gather() mimics the functionality of the gather() function found in R’s; tidyr package. This is a way to turn “wide” format data into “long”; format data. Parameters:. ht (Table) – A Hail table.; key (str) – The name of the key field in the gathered table.; value (str) – The name of the value field in the gathered table.; fields (variable-length args of obj:str) – Names of fields to gather in ht. Returns:; Table – Table with original fields gathered into key and value fields. hail.experimental.separate(ht, field, into, delim)[source]; Separate a field into multiple fields by splitting on a delimiter; character or position.; separate() mimics the functionality of the separate() function in R’s; tidyr package.; This function will create a new table where field has been split into; multiple new fields, whose names are given by into.; If delim is a str (including regular expression strings), field; will be separated into columns by that string. In this case, the length; of into must match the number of resulting fields.; If delim is an int, field will be separated into two row fields,; where the first field contains the first delim characters of field; and the second field contains the remaining characters. Parameters:. ht (Table) – A Hail table.; field (str) – The name of the field to separate in ht.; into (list of str) – The names of the fi",MatchSource.WIKI,docs/0.2/experimental/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/experimental/index.html
https://hail.is/docs/0.2/experimental/index.html:43228,Modifiability,variab,variables,43228," to modify the result of the inner recursive call to triangle1(8) by; adding 9 to the result.; The second function is tail recursive: the result of triangle2(9, 0) is; the same as the result of the inner recursive call, triangle2(8, 9).; Example; To find the sum of all the numbers from n=1…10:; >>> triangle_f = lambda f, x, total: hl.if_else(x == 0, total, f(x - 1, total + x)); >>> x = hl.experimental.loop(triangle_f, hl.tint32, 10, 0); >>> hl.eval(x); 55; Let’s say we want to find the root of a polynomial equation:; >>> def polynomial(x):; … return 5 * x**3 - 2 * x - 1; We’ll use Newton’s method<https://en.wikipedia.org/wiki/Newton%27s_method>; to find it, so we’ll also define the derivative:; >>> def derivative(x):; ... return 15 * x**2 - 2. and starting at \(x_0 = 0\), we’ll compute the next step \(x_{i+1} = x_i - \frac{f(x_i)}{f'(x_i)}\); until the difference between \(x_{i}\) and \(x_{i+1}\) falls below; our convergence threshold:; >>> threshold = 0.005; >>> def find_root(f, guess, error):; ... converged = hl.is_defined(error) & (error < threshold); ... new_guess = guess - (polynomial(guess) / derivative(guess)); ... new_error = hl.abs(new_guess - guess); ... return hl.if_else(converged, guess, f(new_guess, new_error)); >>> x = hl.experimental.loop(find_root, hl.tfloat, 0.0, hl.missing(hl.tfloat)); >>> hl.eval(x); 0.8052291984599675. Warning; Using arguments of a type other than numeric types and booleans can cause; memory issues if if you expect the recursive call to happen many times. Parameters:. f (function ( (marker, *args) -> Expression) – Function of one callable marker, denoting where the recursive call (or calls) is located,; and many args, the loop variables.; typ (str or HailType) – Type the loop returns.; args (variable-length args of Expression) – Expressions to initialize the loop values. Returns:; Expression – Result of the loop with args as initial loop values. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/experimental/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/experimental/index.html
https://hail.is/docs/0.2/experimental/index.html:43294,Modifiability,variab,variable-length,43294," to modify the result of the inner recursive call to triangle1(8) by; adding 9 to the result.; The second function is tail recursive: the result of triangle2(9, 0) is; the same as the result of the inner recursive call, triangle2(8, 9).; Example; To find the sum of all the numbers from n=1…10:; >>> triangle_f = lambda f, x, total: hl.if_else(x == 0, total, f(x - 1, total + x)); >>> x = hl.experimental.loop(triangle_f, hl.tint32, 10, 0); >>> hl.eval(x); 55; Let’s say we want to find the root of a polynomial equation:; >>> def polynomial(x):; … return 5 * x**3 - 2 * x - 1; We’ll use Newton’s method<https://en.wikipedia.org/wiki/Newton%27s_method>; to find it, so we’ll also define the derivative:; >>> def derivative(x):; ... return 15 * x**2 - 2. and starting at \(x_0 = 0\), we’ll compute the next step \(x_{i+1} = x_i - \frac{f(x_i)}{f'(x_i)}\); until the difference between \(x_{i}\) and \(x_{i+1}\) falls below; our convergence threshold:; >>> threshold = 0.005; >>> def find_root(f, guess, error):; ... converged = hl.is_defined(error) & (error < threshold); ... new_guess = guess - (polynomial(guess) / derivative(guess)); ... new_error = hl.abs(new_guess - guess); ... return hl.if_else(converged, guess, f(new_guess, new_error)); >>> x = hl.experimental.loop(find_root, hl.tfloat, 0.0, hl.missing(hl.tfloat)); >>> hl.eval(x); 0.8052291984599675. Warning; Using arguments of a type other than numeric types and booleans can cause; memory issues if if you expect the recursive call to happen many times. Parameters:. f (function ( (marker, *args) -> Expression) – Function of one callable marker, denoting where the recursive call (or calls) is located,; and many args, the loop variables.; typ (str or HailType) – Type the loop returns.; args (variable-length args of Expression) – Expressions to initialize the loop values. Returns:; Expression – Result of the loop with args as initial loop values. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/experimental/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/experimental/index.html
https://hail.is/docs/0.2/experimental/index.html:4270,Performance,load,load,4270,"ls of genes or transcripts. export_entries_by_col(mt, path[, ...]); Export entries of the mt by column as separate text files. pc_project(call_expr, loadings_expr, af_expr); Projects genotypes onto pre-computed PCs. dplyr-inspired Methods. gather(ht, key, value, *fields); Collapse fields into key-value pairs. separate(ht, field, into, delim); Separate a field into multiple fields by splitting on a delimiter character or position. spread(ht, field, value[, key]); Spread a key-value pair of fields across multiple fields. Functions. hail.experimental.load_dataset(name, version, reference_genome, region='us-central1', cloud='gcp')[source]; Load a genetic dataset from Hail’s repository.; Example; >>> # Load the gnomAD ""HGDP + 1000 Genomes"" dense MatrixTable with GRCh38 coordinates.; >>> mt = hl.experimental.load_dataset(name='gnomad_hgdp_1kg_subset_dense',; ... version='3.1.2',; ... reference_genome='GRCh38',; ... region='us-central1',; ... cloud='gcp'). Parameters:. name (str) – Name of the dataset to load.; version (str, optional) – Version of the named dataset to load (see available versions in; documentation). Possibly None for some datasets.; reference_genome (str, optional) – Reference genome build, 'GRCh37' or 'GRCh38'. Possibly None; for some datasets.; region (str) – Specify region for bucket, 'us', 'us-central1', or 'europe-west1', (default is; 'us-central1').; cloud (str) – Specify if using Google Cloud Platform or Amazon Web Services,; 'gcp' or 'aws' (default is 'gcp'). Note; The 'aws' cloud platform is currently only available for the 'us'; region. Returns:; Table, MatrixTable, or BlockMatrix. hail.experimental.ld_score(entry_expr, locus_expr, radius, coord_expr=None, annotation_exprs=None, block_size=None)[source]; Calculate LD scores.; Example; >>> # Load genetic data into MatrixTable; >>> mt = hl.import_plink(bed='data/ldsc.bed',; ... bim='data/ldsc.bim',; ... fam='data/ldsc.fam'). >>> # Create locus-keyed Table with numeric variant annotations; >>> h",MatchSource.WIKI,docs/0.2/experimental/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/experimental/index.html
https://hail.is/docs/0.2/experimental/index.html:4335,Performance,load,load,4335,"entries of the mt by column as separate text files. pc_project(call_expr, loadings_expr, af_expr); Projects genotypes onto pre-computed PCs. dplyr-inspired Methods. gather(ht, key, value, *fields); Collapse fields into key-value pairs. separate(ht, field, into, delim); Separate a field into multiple fields by splitting on a delimiter character or position. spread(ht, field, value[, key]); Spread a key-value pair of fields across multiple fields. Functions. hail.experimental.load_dataset(name, version, reference_genome, region='us-central1', cloud='gcp')[source]; Load a genetic dataset from Hail’s repository.; Example; >>> # Load the gnomAD ""HGDP + 1000 Genomes"" dense MatrixTable with GRCh38 coordinates.; >>> mt = hl.experimental.load_dataset(name='gnomad_hgdp_1kg_subset_dense',; ... version='3.1.2',; ... reference_genome='GRCh38',; ... region='us-central1',; ... cloud='gcp'). Parameters:. name (str) – Name of the dataset to load.; version (str, optional) – Version of the named dataset to load (see available versions in; documentation). Possibly None for some datasets.; reference_genome (str, optional) – Reference genome build, 'GRCh37' or 'GRCh38'. Possibly None; for some datasets.; region (str) – Specify region for bucket, 'us', 'us-central1', or 'europe-west1', (default is; 'us-central1').; cloud (str) – Specify if using Google Cloud Platform or Amazon Web Services,; 'gcp' or 'aws' (default is 'gcp'). Note; The 'aws' cloud platform is currently only available for the 'us'; region. Returns:; Table, MatrixTable, or BlockMatrix. hail.experimental.ld_score(entry_expr, locus_expr, radius, coord_expr=None, annotation_exprs=None, block_size=None)[source]; Calculate LD scores.; Example; >>> # Load genetic data into MatrixTable; >>> mt = hl.import_plink(bed='data/ldsc.bed',; ... bim='data/ldsc.bim',; ... fam='data/ldsc.fam'). >>> # Create locus-keyed Table with numeric variant annotations; >>> ht = hl.import_table('data/ldsc.annot',; ... types={'BP': hl.tint,; ... 'bin",MatchSource.WIKI,docs/0.2/experimental/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/experimental/index.html
https://hail.is/docs/0.2/experimental/index.html:26855,Performance,load,load,26855,"---------; Global fields:; None; ----------------------------------------; Row fields:; 'source': str; 'feature': str; 'score': float64; 'strand': str; 'frame': int32; 'gene_type': str; 'exon_id': str; 'havana_transcript': str; 'level': str; 'transcript_name': str; 'gene_status': str; 'gene_id': str; 'transcript_type': str; 'tag': str; 'transcript_status': str; 'gene_name': str; 'transcript_id': str; 'exon_number': str; 'havana_gene': str; 'interval': interval<locus<GRCh37>>; ----------------------------------------; Key: ['interval']; ----------------------------------------. Parameters:. path (str) – File to import.; reference_genome (str or ReferenceGenome, optional) – Reference genome to use.; skip_invalid_contigs (bool) – If True and reference_genome is not None, skip lines where; seqname is not consistent with the reference genome.; min_partitions (int or None) – Minimum number of partitions (passed to import_table).; force_bgz (bool) – If True, load files as blocked gzip files, assuming; that they were actually compressed using the BGZ codec. This option is; useful when the file extension is not '.bgz', but the file is; blocked gzip, so that the file can be read in parallel and not on a; single node.; force (bool) – If True, load gzipped files serially on one core. This should; be used only when absolutely necessary, as processing time will be; increased due to lack of parallelism. Returns:; Table. hail.experimental.get_gene_intervals(gene_symbols=None, gene_ids=None, transcript_ids=None, verbose=True, reference_genome=None, gtf_file=None)[source]; Get intervals of genes or transcripts.; Get the boundaries of genes or transcripts from a GTF file, for quick filtering of a Table or MatrixTable.; On Google Cloud platform:; Gencode v19 (GRCh37) GTF available at: gs://hail-common/references/gencode/gencode.v19.annotation.gtf.bgz; Gencode v29 (GRCh38) GTF available at: gs://hail-common/references/gencode/gencode.v29.annotation.gtf.bgz; Example; >>> hl.filter_interv",MatchSource.WIKI,docs/0.2/experimental/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/experimental/index.html
https://hail.is/docs/0.2/experimental/index.html:27141,Performance,load,load,27141,": str; 'gene_status': str; 'gene_id': str; 'transcript_type': str; 'tag': str; 'transcript_status': str; 'gene_name': str; 'transcript_id': str; 'exon_number': str; 'havana_gene': str; 'interval': interval<locus<GRCh37>>; ----------------------------------------; Key: ['interval']; ----------------------------------------. Parameters:. path (str) – File to import.; reference_genome (str or ReferenceGenome, optional) – Reference genome to use.; skip_invalid_contigs (bool) – If True and reference_genome is not None, skip lines where; seqname is not consistent with the reference genome.; min_partitions (int or None) – Minimum number of partitions (passed to import_table).; force_bgz (bool) – If True, load files as blocked gzip files, assuming; that they were actually compressed using the BGZ codec. This option is; useful when the file extension is not '.bgz', but the file is; blocked gzip, so that the file can be read in parallel and not on a; single node.; force (bool) – If True, load gzipped files serially on one core. This should; be used only when absolutely necessary, as processing time will be; increased due to lack of parallelism. Returns:; Table. hail.experimental.get_gene_intervals(gene_symbols=None, gene_ids=None, transcript_ids=None, verbose=True, reference_genome=None, gtf_file=None)[source]; Get intervals of genes or transcripts.; Get the boundaries of genes or transcripts from a GTF file, for quick filtering of a Table or MatrixTable.; On Google Cloud platform:; Gencode v19 (GRCh37) GTF available at: gs://hail-common/references/gencode/gencode.v19.annotation.gtf.bgz; Gencode v29 (GRCh38) GTF available at: gs://hail-common/references/gencode/gencode.v29.annotation.gtf.bgz; Example; >>> hl.filter_intervals(ht, get_gene_intervals(gene_symbols=['PCSK9'], reference_genome='GRCh37')) . Parameters:. gene_symbols (list of str, optional) – Gene symbols (e.g. PCSK9).; gene_ids (list of str, optional) – Gene IDs (e.g. ENSG00000223972).; transcript_ids (list of str,",MatchSource.WIKI,docs/0.2/experimental/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/experimental/index.html
https://hail.is/docs/0.2/experimental/index.html:28430,Performance,load,load,28430,"=True, reference_genome=None, gtf_file=None)[source]; Get intervals of genes or transcripts.; Get the boundaries of genes or transcripts from a GTF file, for quick filtering of a Table or MatrixTable.; On Google Cloud platform:; Gencode v19 (GRCh37) GTF available at: gs://hail-common/references/gencode/gencode.v19.annotation.gtf.bgz; Gencode v29 (GRCh38) GTF available at: gs://hail-common/references/gencode/gencode.v29.annotation.gtf.bgz; Example; >>> hl.filter_intervals(ht, get_gene_intervals(gene_symbols=['PCSK9'], reference_genome='GRCh37')) . Parameters:. gene_symbols (list of str, optional) – Gene symbols (e.g. PCSK9).; gene_ids (list of str, optional) – Gene IDs (e.g. ENSG00000223972).; transcript_ids (list of str, optional) – Transcript IDs (e.g. ENSG00000223972).; verbose (bool) – If True, print which genes and transcripts were matched in the GTF file.; reference_genome (str or ReferenceGenome, optional) – Reference genome to use (passed along to import_gtf).; gtf_file (str) – GTF file to load. If none is provided, but reference_genome is one of; GRCh37 or GRCh38, a default will be used (on Google Cloud Platform). Returns:; list of Interval. hail.experimental.export_entries_by_col(mt, path, batch_size=256, bgzip=True, header_json_in_file=True, use_string_key_as_file_name=False)[source]; Export entries of the mt by column as separate text files.; Examples; >>> range_mt = hl.utils.range_matrix_table(10, 10); >>> range_mt = range_mt.annotate_entries(x = hl.rand_unif(0, 1)); >>> hl.experimental.export_entries_by_col(range_mt, 'output/cols_files'). Notes; This function writes a directory with one file per column in mt. The; files contain one tab-separated field (with header) for each row field; and entry field in mt. The column fields of mt are written as JSON; in the first line of each file, prefixed with a #.; The above will produce a directory at output/cols_files with the; following files:; $ ls -l output/cols_files; total 80; -rw-r--r-- 1 hail-dev wheel 71",MatchSource.WIKI,docs/0.2/experimental/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/experimental/index.html
https://hail.is/docs/0.2/experimental/index.html:39341,Performance,load,loadings,39341,"dia.org/wiki/List_of_tz_database_time_zones; Currently, the parser implicitly uses the “en_US” locale.; This function will fail if there is not enough information in the string to determine a particular timestamp.; For example, if you have the string “07/08/09” and the format string “%Y.%m.%d”, this method will fail, since that’s not specific; enough to determine seconds from. You can fix this by adding “00:00:00” to your date string and “%H:%M:%S” to your format string. Parameters:. time (str or Expression of type tstr) – The string from which to parse the time.; format (str or Expression of type tstr) – The format string describing how to parse the time.; zone_id (str or Expression of type tstr) – An id representing the timezone. See notes above. Returns:; Int64Expression – The Unix timestamp associated with the given time string. hail.experimental.pc_project(call_expr, loadings_expr, af_expr)[source]; Projects genotypes onto pre-computed PCs. Requires loadings and; allele-frequency from a reference dataset (see example). Note that; loadings_expr must have no missing data and reflect the rows; from the original PCA run for this method to be accurate.; Example; >>> # Compute loadings and allele frequency for reference dataset; >>> _, _, loadings_ht = hl.hwe_normalized_pca(mt.GT, k=10, compute_loadings=True) ; >>> mt = mt.annotate_rows(af=hl.agg.mean(mt.GT.n_alt_alleles()) / 2) ; >>> loadings_ht = loadings_ht.annotate(af=mt.rows()[loadings_ht.key].af) ; >>> # Project new genotypes onto loadings; >>> ht = pc_project(mt_to_project.GT, loadings_ht.loadings, loadings_ht.af) . Parameters:. call_expr (CallExpression) – Entry-indexed call expression for genotypes; to project onto loadings.; loadings_expr (ArrayNumericExpression) – Location of expression for loadings; af_expr (Float64Expression) – Location of expression for allele frequency. Returns:; Table – Table with scores calculated from loadings in column scores. hail.experimental.loop(f, typ, *args)[source]; Define",MatchSource.WIKI,docs/0.2/experimental/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/experimental/index.html
https://hail.is/docs/0.2/experimental/index.html:39567,Performance,load,loadings,39567,"ple, if you have the string “07/08/09” and the format string “%Y.%m.%d”, this method will fail, since that’s not specific; enough to determine seconds from. You can fix this by adding “00:00:00” to your date string and “%H:%M:%S” to your format string. Parameters:. time (str or Expression of type tstr) – The string from which to parse the time.; format (str or Expression of type tstr) – The format string describing how to parse the time.; zone_id (str or Expression of type tstr) – An id representing the timezone. See notes above. Returns:; Int64Expression – The Unix timestamp associated with the given time string. hail.experimental.pc_project(call_expr, loadings_expr, af_expr)[source]; Projects genotypes onto pre-computed PCs. Requires loadings and; allele-frequency from a reference dataset (see example). Note that; loadings_expr must have no missing data and reflect the rows; from the original PCA run for this method to be accurate.; Example; >>> # Compute loadings and allele frequency for reference dataset; >>> _, _, loadings_ht = hl.hwe_normalized_pca(mt.GT, k=10, compute_loadings=True) ; >>> mt = mt.annotate_rows(af=hl.agg.mean(mt.GT.n_alt_alleles()) / 2) ; >>> loadings_ht = loadings_ht.annotate(af=mt.rows()[loadings_ht.key].af) ; >>> # Project new genotypes onto loadings; >>> ht = pc_project(mt_to_project.GT, loadings_ht.loadings, loadings_ht.af) . Parameters:. call_expr (CallExpression) – Entry-indexed call expression for genotypes; to project onto loadings.; loadings_expr (ArrayNumericExpression) – Location of expression for loadings; af_expr (Float64Expression) – Location of expression for allele frequency. Returns:; Table – Table with scores calculated from loadings in column scores. hail.experimental.loop(f, typ, *args)[source]; Define and call a tail-recursive function with given arguments.; Notes; The argument f must be a function where the first argument defines the; recursive call, and the remaining arguments are the arguments to the; recursive functi",MatchSource.WIKI,docs/0.2/experimental/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/experimental/index.html
https://hail.is/docs/0.2/experimental/index.html:39883,Performance,load,loadings,39883,"f type tstr) – The string from which to parse the time.; format (str or Expression of type tstr) – The format string describing how to parse the time.; zone_id (str or Expression of type tstr) – An id representing the timezone. See notes above. Returns:; Int64Expression – The Unix timestamp associated with the given time string. hail.experimental.pc_project(call_expr, loadings_expr, af_expr)[source]; Projects genotypes onto pre-computed PCs. Requires loadings and; allele-frequency from a reference dataset (see example). Note that; loadings_expr must have no missing data and reflect the rows; from the original PCA run for this method to be accurate.; Example; >>> # Compute loadings and allele frequency for reference dataset; >>> _, _, loadings_ht = hl.hwe_normalized_pca(mt.GT, k=10, compute_loadings=True) ; >>> mt = mt.annotate_rows(af=hl.agg.mean(mt.GT.n_alt_alleles()) / 2) ; >>> loadings_ht = loadings_ht.annotate(af=mt.rows()[loadings_ht.key].af) ; >>> # Project new genotypes onto loadings; >>> ht = pc_project(mt_to_project.GT, loadings_ht.loadings, loadings_ht.af) . Parameters:. call_expr (CallExpression) – Entry-indexed call expression for genotypes; to project onto loadings.; loadings_expr (ArrayNumericExpression) – Location of expression for loadings; af_expr (Float64Expression) – Location of expression for allele frequency. Returns:; Table – Table with scores calculated from loadings in column scores. hail.experimental.loop(f, typ, *args)[source]; Define and call a tail-recursive function with given arguments.; Notes; The argument f must be a function where the first argument defines the; recursive call, and the remaining arguments are the arguments to the; recursive function, e.g. to define the recursive function. \[f(x, y) = \begin{cases}; y & \textrm{if } x \equiv 0 \\; f(x - 1, y + x) & \textrm{otherwise}; \end{cases}\]; we would write:; >>> f = lambda recur, x, y: hl.if_else(x == 0, y, recur(x - 1, y + x)); Full recursion is not supported, and any non-ta",MatchSource.WIKI,docs/0.2/experimental/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/experimental/index.html
https://hail.is/docs/0.2/experimental/index.html:39943,Performance,load,loadings,39943,"or Expression of type tstr) – The format string describing how to parse the time.; zone_id (str or Expression of type tstr) – An id representing the timezone. See notes above. Returns:; Int64Expression – The Unix timestamp associated with the given time string. hail.experimental.pc_project(call_expr, loadings_expr, af_expr)[source]; Projects genotypes onto pre-computed PCs. Requires loadings and; allele-frequency from a reference dataset (see example). Note that; loadings_expr must have no missing data and reflect the rows; from the original PCA run for this method to be accurate.; Example; >>> # Compute loadings and allele frequency for reference dataset; >>> _, _, loadings_ht = hl.hwe_normalized_pca(mt.GT, k=10, compute_loadings=True) ; >>> mt = mt.annotate_rows(af=hl.agg.mean(mt.GT.n_alt_alleles()) / 2) ; >>> loadings_ht = loadings_ht.annotate(af=mt.rows()[loadings_ht.key].af) ; >>> # Project new genotypes onto loadings; >>> ht = pc_project(mt_to_project.GT, loadings_ht.loadings, loadings_ht.af) . Parameters:. call_expr (CallExpression) – Entry-indexed call expression for genotypes; to project onto loadings.; loadings_expr (ArrayNumericExpression) – Location of expression for loadings; af_expr (Float64Expression) – Location of expression for allele frequency. Returns:; Table – Table with scores calculated from loadings in column scores. hail.experimental.loop(f, typ, *args)[source]; Define and call a tail-recursive function with given arguments.; Notes; The argument f must be a function where the first argument defines the; recursive call, and the remaining arguments are the arguments to the; recursive function, e.g. to define the recursive function. \[f(x, y) = \begin{cases}; y & \textrm{if } x \equiv 0 \\; f(x - 1, y + x) & \textrm{otherwise}; \end{cases}\]; we would write:; >>> f = lambda recur, x, y: hl.if_else(x == 0, y, recur(x - 1, y + x)); Full recursion is not supported, and any non-tail-recursive methods will; throw an error when called.; This means t",MatchSource.WIKI,docs/0.2/experimental/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/experimental/index.html
https://hail.is/docs/0.2/experimental/index.html:40074,Performance,load,loadings,40074,"e.; zone_id (str or Expression of type tstr) – An id representing the timezone. See notes above. Returns:; Int64Expression – The Unix timestamp associated with the given time string. hail.experimental.pc_project(call_expr, loadings_expr, af_expr)[source]; Projects genotypes onto pre-computed PCs. Requires loadings and; allele-frequency from a reference dataset (see example). Note that; loadings_expr must have no missing data and reflect the rows; from the original PCA run for this method to be accurate.; Example; >>> # Compute loadings and allele frequency for reference dataset; >>> _, _, loadings_ht = hl.hwe_normalized_pca(mt.GT, k=10, compute_loadings=True) ; >>> mt = mt.annotate_rows(af=hl.agg.mean(mt.GT.n_alt_alleles()) / 2) ; >>> loadings_ht = loadings_ht.annotate(af=mt.rows()[loadings_ht.key].af) ; >>> # Project new genotypes onto loadings; >>> ht = pc_project(mt_to_project.GT, loadings_ht.loadings, loadings_ht.af) . Parameters:. call_expr (CallExpression) – Entry-indexed call expression for genotypes; to project onto loadings.; loadings_expr (ArrayNumericExpression) – Location of expression for loadings; af_expr (Float64Expression) – Location of expression for allele frequency. Returns:; Table – Table with scores calculated from loadings in column scores. hail.experimental.loop(f, typ, *args)[source]; Define and call a tail-recursive function with given arguments.; Notes; The argument f must be a function where the first argument defines the; recursive call, and the remaining arguments are the arguments to the; recursive function, e.g. to define the recursive function. \[f(x, y) = \begin{cases}; y & \textrm{if } x \equiv 0 \\; f(x - 1, y + x) & \textrm{otherwise}; \end{cases}\]; we would write:; >>> f = lambda recur, x, y: hl.if_else(x == 0, y, recur(x - 1, y + x)); Full recursion is not supported, and any non-tail-recursive methods will; throw an error when called.; This means that the result of any recursive call within the function must; also be the resu",MatchSource.WIKI,docs/0.2/experimental/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/experimental/index.html
https://hail.is/docs/0.2/experimental/index.html:40153,Performance,load,loadings,40153,"e Unix timestamp associated with the given time string. hail.experimental.pc_project(call_expr, loadings_expr, af_expr)[source]; Projects genotypes onto pre-computed PCs. Requires loadings and; allele-frequency from a reference dataset (see example). Note that; loadings_expr must have no missing data and reflect the rows; from the original PCA run for this method to be accurate.; Example; >>> # Compute loadings and allele frequency for reference dataset; >>> _, _, loadings_ht = hl.hwe_normalized_pca(mt.GT, k=10, compute_loadings=True) ; >>> mt = mt.annotate_rows(af=hl.agg.mean(mt.GT.n_alt_alleles()) / 2) ; >>> loadings_ht = loadings_ht.annotate(af=mt.rows()[loadings_ht.key].af) ; >>> # Project new genotypes onto loadings; >>> ht = pc_project(mt_to_project.GT, loadings_ht.loadings, loadings_ht.af) . Parameters:. call_expr (CallExpression) – Entry-indexed call expression for genotypes; to project onto loadings.; loadings_expr (ArrayNumericExpression) – Location of expression for loadings; af_expr (Float64Expression) – Location of expression for allele frequency. Returns:; Table – Table with scores calculated from loadings in column scores. hail.experimental.loop(f, typ, *args)[source]; Define and call a tail-recursive function with given arguments.; Notes; The argument f must be a function where the first argument defines the; recursive call, and the remaining arguments are the arguments to the; recursive function, e.g. to define the recursive function. \[f(x, y) = \begin{cases}; y & \textrm{if } x \equiv 0 \\; f(x - 1, y + x) & \textrm{otherwise}; \end{cases}\]; we would write:; >>> f = lambda recur, x, y: hl.if_else(x == 0, y, recur(x - 1, y + x)); Full recursion is not supported, and any non-tail-recursive methods will; throw an error when called.; This means that the result of any recursive call within the function must; also be the result of the entire function, without modification. Let’s; consider two different recursive definitions for the triangle function;",MatchSource.WIKI,docs/0.2/experimental/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/experimental/index.html
https://hail.is/docs/0.2/experimental/index.html:40290,Performance,load,loadings,40290,"pr)[source]; Projects genotypes onto pre-computed PCs. Requires loadings and; allele-frequency from a reference dataset (see example). Note that; loadings_expr must have no missing data and reflect the rows; from the original PCA run for this method to be accurate.; Example; >>> # Compute loadings and allele frequency for reference dataset; >>> _, _, loadings_ht = hl.hwe_normalized_pca(mt.GT, k=10, compute_loadings=True) ; >>> mt = mt.annotate_rows(af=hl.agg.mean(mt.GT.n_alt_alleles()) / 2) ; >>> loadings_ht = loadings_ht.annotate(af=mt.rows()[loadings_ht.key].af) ; >>> # Project new genotypes onto loadings; >>> ht = pc_project(mt_to_project.GT, loadings_ht.loadings, loadings_ht.af) . Parameters:. call_expr (CallExpression) – Entry-indexed call expression for genotypes; to project onto loadings.; loadings_expr (ArrayNumericExpression) – Location of expression for loadings; af_expr (Float64Expression) – Location of expression for allele frequency. Returns:; Table – Table with scores calculated from loadings in column scores. hail.experimental.loop(f, typ, *args)[source]; Define and call a tail-recursive function with given arguments.; Notes; The argument f must be a function where the first argument defines the; recursive call, and the remaining arguments are the arguments to the; recursive function, e.g. to define the recursive function. \[f(x, y) = \begin{cases}; y & \textrm{if } x \equiv 0 \\; f(x - 1, y + x) & \textrm{otherwise}; \end{cases}\]; we would write:; >>> f = lambda recur, x, y: hl.if_else(x == 0, y, recur(x - 1, y + x)); Full recursion is not supported, and any non-tail-recursive methods will; throw an error when called.; This means that the result of any recursive call within the function must; also be the result of the entire function, without modification. Let’s; consider two different recursive definitions for the triangle function; \(f(x) = 0 + 1 + \dots + x\):; >>> def triangle1(x):; ... if x == 1:; ... return x; ... return x + triangle1(x - 1).",MatchSource.WIKI,docs/0.2/experimental/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/experimental/index.html
https://hail.is/docs/0.2/experimental/index.html:1646,Testability,test,tests,1646," page source. Experimental; This module serves two functions: as a staging area for extensions of Hail; not ready for inclusion in the main package, and as a library of lightly reviewed; community submissions.; At present, the experimental module is organized into a few freestanding; modules, linked immediately below, and many freestanding functions, documented; on this page. Warning; The functionality in this module may change or disappear entirely between different versions of; Hail. If you critically depend on functionality in this module, please create an issue to request; promotion of that functionality to non-experimental. Otherwise, that functionality may disappear!. ldscsim. Contribution Guidelines; Submissions from the community are welcome! The criteria for inclusion in the; experimental module are loose and subject to change:. Function docstrings are required. Hail uses; NumPy style docstrings.; Tests are not required, but are encouraged. If you do include tests, they must; run in no more than a few seconds. Place tests as a class method on Tests in; python/tests/experimental/test_experimental.py; Code style is not strictly enforced, aside from egregious violations. We do; recommend using autopep8 though!. Annotation Database; Classes. hail.experimental.DB; An annotation database instance. Genetics Methods. load_dataset(name, version, reference_genome); Load a genetic dataset from Hail's repository. ld_score(entry_expr, locus_expr, radius[, ...]); Calculate LD scores. ld_score_regression(weight_expr, ...[, ...]); Estimate SNP-heritability and level of confounding biases from genome-wide association study (GWAS) summary statistics. write_expression(expr, path[, overwrite]); Write an Expression. read_expression(path[, _assert_type]); Read an Expression written with experimental.write_expression(). filtering_allele_frequency(ac, an, ci); Computes a filtering allele frequency (described below) for ac and an with confidence ci. hail_metadata(t_path); Create",MatchSource.WIKI,docs/0.2/experimental/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/experimental/index.html
https://hail.is/docs/0.2/experimental/index.html:1705,Testability,test,tests,1705," for extensions of Hail; not ready for inclusion in the main package, and as a library of lightly reviewed; community submissions.; At present, the experimental module is organized into a few freestanding; modules, linked immediately below, and many freestanding functions, documented; on this page. Warning; The functionality in this module may change or disappear entirely between different versions of; Hail. If you critically depend on functionality in this module, please create an issue to request; promotion of that functionality to non-experimental. Otherwise, that functionality may disappear!. ldscsim. Contribution Guidelines; Submissions from the community are welcome! The criteria for inclusion in the; experimental module are loose and subject to change:. Function docstrings are required. Hail uses; NumPy style docstrings.; Tests are not required, but are encouraged. If you do include tests, they must; run in no more than a few seconds. Place tests as a class method on Tests in; python/tests/experimental/test_experimental.py; Code style is not strictly enforced, aside from egregious violations. We do; recommend using autopep8 though!. Annotation Database; Classes. hail.experimental.DB; An annotation database instance. Genetics Methods. load_dataset(name, version, reference_genome); Load a genetic dataset from Hail's repository. ld_score(entry_expr, locus_expr, radius[, ...]); Calculate LD scores. ld_score_regression(weight_expr, ...[, ...]); Estimate SNP-heritability and level of confounding biases from genome-wide association study (GWAS) summary statistics. write_expression(expr, path[, overwrite]); Write an Expression. read_expression(path[, _assert_type]); Read an Expression written with experimental.write_expression(). filtering_allele_frequency(ac, an, ci); Computes a filtering allele frequency (described below) for ac and an with confidence ci. hail_metadata(t_path); Create a metadata plot for a Hail Table or MatrixTable. plot_roc_curve(ht, scores[, t",MatchSource.WIKI,docs/0.2/experimental/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/experimental/index.html
https://hail.is/docs/0.2/experimental/index.html:1749,Testability,test,tests,1749," for extensions of Hail; not ready for inclusion in the main package, and as a library of lightly reviewed; community submissions.; At present, the experimental module is organized into a few freestanding; modules, linked immediately below, and many freestanding functions, documented; on this page. Warning; The functionality in this module may change or disappear entirely between different versions of; Hail. If you critically depend on functionality in this module, please create an issue to request; promotion of that functionality to non-experimental. Otherwise, that functionality may disappear!. ldscsim. Contribution Guidelines; Submissions from the community are welcome! The criteria for inclusion in the; experimental module are loose and subject to change:. Function docstrings are required. Hail uses; NumPy style docstrings.; Tests are not required, but are encouraged. If you do include tests, they must; run in no more than a few seconds. Place tests as a class method on Tests in; python/tests/experimental/test_experimental.py; Code style is not strictly enforced, aside from egregious violations. We do; recommend using autopep8 though!. Annotation Database; Classes. hail.experimental.DB; An annotation database instance. Genetics Methods. load_dataset(name, version, reference_genome); Load a genetic dataset from Hail's repository. ld_score(entry_expr, locus_expr, radius[, ...]); Calculate LD scores. ld_score_regression(weight_expr, ...[, ...]); Estimate SNP-heritability and level of confounding biases from genome-wide association study (GWAS) summary statistics. write_expression(expr, path[, overwrite]); Write an Expression. read_expression(path[, _assert_type]); Read an Expression written with experimental.write_expression(). filtering_allele_frequency(ac, an, ci); Computes a filtering allele frequency (described below) for ac and an with confidence ci. hail_metadata(t_path); Create a metadata plot for a Hail Table or MatrixTable. plot_roc_curve(ht, scores[, t",MatchSource.WIKI,docs/0.2/experimental/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/experimental/index.html
https://hail.is/docs/0.2/experimental/index.html:8854,Testability,test,test,8854,"specified.; block_size (int, optional) – Block size. Default given by BlockMatrix.default_block_size(). Returns:; Table – Table keyed by locus_expr with LD scores for each variant and; annotation_expr. The function will always return LD scores for; the univariate (all SNPs) annotation. hail.experimental.ld_score_regression(weight_expr, ld_score_expr, chi_sq_exprs, n_samples_exprs, n_blocks=200, two_step_threshold=30, n_reference_panel_variants=None)[source]; Estimate SNP-heritability and level of confounding biases from genome-wide association study; (GWAS) summary statistics.; Given a set or multiple sets of GWAS summary statistics, ld_score_regression() estimates the heritability; of a trait or set of traits and the level of confounding biases present in; the underlying studies by regressing chi-squared statistics on LD scores,; leveraging the model:. \[\mathrm{E}[\chi_j^2] = 1 + Na + \frac{Nh_g^2}{M}l_j\]. \(\mathrm{E}[\chi_j^2]\) is the expected chi-squared statistic; for variant \(j\) resulting from a test of association between; variant \(j\) and a trait.; \(l_j = \sum_{k} r_{jk}^2\) is the LD score of variant; \(j\), calculated as the sum of squared correlation coefficients; between variant \(j\) and nearby variants. See ld_score(); for further details.; \(a\) captures the contribution of confounding biases, such as; cryptic relatedness and uncontrolled population structure, to the; association test statistic.; \(h_g^2\) is the SNP-heritability, or the proportion of variation; in the trait explained by the effects of variants included in the; regression model above.; \(M\) is the number of variants used to estimate \(h_g^2\).; \(N\) is the number of samples in the underlying association study. For more details on the method implemented in this function, see:. LD Score regression distinguishes confounding from polygenicity in genome-wide association studies (Bulik-Sullivan et al, 2015). Examples; Run the method on a matrix table of summary statistics, where th",MatchSource.WIKI,docs/0.2/experimental/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/experimental/index.html
https://hail.is/docs/0.2/experimental/index.html:9257,Testability,test,test,9257,"xprs, n_samples_exprs, n_blocks=200, two_step_threshold=30, n_reference_panel_variants=None)[source]; Estimate SNP-heritability and level of confounding biases from genome-wide association study; (GWAS) summary statistics.; Given a set or multiple sets of GWAS summary statistics, ld_score_regression() estimates the heritability; of a trait or set of traits and the level of confounding biases present in; the underlying studies by regressing chi-squared statistics on LD scores,; leveraging the model:. \[\mathrm{E}[\chi_j^2] = 1 + Na + \frac{Nh_g^2}{M}l_j\]. \(\mathrm{E}[\chi_j^2]\) is the expected chi-squared statistic; for variant \(j\) resulting from a test of association between; variant \(j\) and a trait.; \(l_j = \sum_{k} r_{jk}^2\) is the LD score of variant; \(j\), calculated as the sum of squared correlation coefficients; between variant \(j\) and nearby variants. See ld_score(); for further details.; \(a\) captures the contribution of confounding biases, such as; cryptic relatedness and uncontrolled population structure, to the; association test statistic.; \(h_g^2\) is the SNP-heritability, or the proportion of variation; in the trait explained by the effects of variants included in the; regression model above.; \(M\) is the number of variants used to estimate \(h_g^2\).; \(N\) is the number of samples in the underlying association study. For more details on the method implemented in this function, see:. LD Score regression distinguishes confounding from polygenicity in genome-wide association studies (Bulik-Sullivan et al, 2015). Examples; Run the method on a matrix table of summary statistics, where the rows; are variants and the columns are different phenotypes:; >>> mt_gwas = ld_score_all_phenos_sumstats; >>> ht_results = hl.experimental.ld_score_regression(; ... weight_expr=mt_gwas['ld_score'],; ... ld_score_expr=mt_gwas['ld_score'],; ... chi_sq_exprs=mt_gwas['chi_squared'],; ... n_samples_exprs=mt_gwas['n']). Run the method on a table with summary sta",MatchSource.WIKI,docs/0.2/experimental/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/experimental/index.html
https://hail.is/docs/0.2/experimental/index.html:13074,Testability,test,test,13074,"leles,; a tarray of tstr elements.; The columns of the matrix table must be keyed by a field; of type tstr that uniquely identifies phenotypes; represented in the matrix table. The column key must be a single; expression; compound keys are not accepted.; weight_expr and ld_score_expr must be row-indexed; fields.; chi_sq_exprs must be a single entry-indexed field; (not a list of fields).; n_samples_exprs must be a single entry-indexed field; (not a list of fields).; The phenotype field that keys the table returned by; ld_score_regression() will have values corresponding to the; column keys of the input matrix table. This function returns a Table with one row per set of summary; statistics passed to the chi_sq_exprs argument. The following; row-indexed fields are included in the table:. phenotype (tstr) – The name of the phenotype. The; returned table is keyed by this field. See the notes below for; details on the possible values of this field.; mean_chi_sq (tfloat64) – The mean chi-squared; test statistic for the given phenotype.; intercept (Struct) – Contains fields:. estimate (tfloat64) – A point estimate of the; intercept \(1 + Na\).; standard_error (tfloat64) – An estimate of; the standard error of this point estimate. snp_heritability (Struct) – Contains fields:. estimate (tfloat64) – A point estimate of the; SNP-heritability \(h_g^2\).; standard_error (tfloat64) – An estimate of; the standard error of this point estimate. Warning; ld_score_regression() considers only the rows for which both row; fields weight_expr and ld_score_expr are defined. Rows with missing; values in either field are removed prior to fitting the LD score; regression model. Parameters:. weight_expr (Float64Expression) – Row-indexed expression for the LD scores used to derive; variant weights in the model.; ld_score_expr (Float64Expression) – Row-indexed expression for the LD scores used as covariates; in the model.; chi_sq_exprs (Float64Expression or list of) – Float64Expression; One or mo",MatchSource.WIKI,docs/0.2/experimental/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/experimental/index.html
https://hail.is/docs/0.2/experimental/index.html:14452,Testability,test,test,14452,"imate (tfloat64) – A point estimate of the; SNP-heritability \(h_g^2\).; standard_error (tfloat64) – An estimate of; the standard error of this point estimate. Warning; ld_score_regression() considers only the rows for which both row; fields weight_expr and ld_score_expr are defined. Rows with missing; values in either field are removed prior to fitting the LD score; regression model. Parameters:. weight_expr (Float64Expression) – Row-indexed expression for the LD scores used to derive; variant weights in the model.; ld_score_expr (Float64Expression) – Row-indexed expression for the LD scores used as covariates; in the model.; chi_sq_exprs (Float64Expression or list of) – Float64Expression; One or more row-indexed (if table) or entry-indexed; (if matrix table) expressions for chi-squared; statistics resulting from genome-wide association; studies (GWAS).; n_samples_exprs (NumericExpression or list of) – NumericExpression; One or more row-indexed (if table) or entry-indexed; (if matrix table) expressions indicating the number of; samples used in the studies that generated the test; statistics supplied to chi_sq_exprs.; n_blocks (int) – The number of blocks used in the jackknife approach to; estimating standard errors.; two_step_threshold (int) – Variants with chi-squared statistics greater than this; value are excluded in the first step of the two-step; procedure used to fit the model.; n_reference_panel_variants (int, optional) – Number of variants used to estimate the; SNP-heritability \(h_g^2\). Returns:; Table – Table keyed by phenotype with intercept and heritability estimates; for each phenotype passed to the function. hail.experimental.write_expression(expr, path, overwrite=False)[source]; Write an Expression.; In the same vein as Python’s pickle, write out an expression; that does not have a source (such as one that comes from; Table.aggregate with _localize=False).; Example; >>> ht = hl.utils.range_table(100).annotate(x=hl.rand_norm()); >>> mean_norm = ht.a",MatchSource.WIKI,docs/0.2/experimental/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/experimental/index.html
https://hail.is/docs/0.2/experimental/index.html:25762,Testability,test,test,25762,"field and will include the following row fields:; 'source': str; 'feature': str; 'score': float64; 'strand': str; 'frame': int32; 'interval': interval<>. There will also be corresponding fields for every tag found in the; attribute field of the GTF file. Note; This function will return an interval field of type tinterval; constructed from the seqname, start, and end fields in the; GTF file. This interval is inclusive of both the start and end positions; in the GTF file.; If the reference_genome parameter is specified, the start and end; points of the interval field will be of type tlocus.; Otherwise, the start and end points of the interval field will be of; type tstruct with fields seqname (type str) and; position (type tint32).; Furthermore, if the reference_genome parameter is specified and; skip_invalid_contigs is True, this import function will skip; lines in the GTF where seqname is not consistent with the reference; genome specified. Example; >>> ht = hl.experimental.import_gtf('data/test.gtf',; ... reference_genome='GRCh37',; ... skip_invalid_contigs=True). >>> ht.describe() ; ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'source': str; 'feature': str; 'score': float64; 'strand': str; 'frame': int32; 'gene_type': str; 'exon_id': str; 'havana_transcript': str; 'level': str; 'transcript_name': str; 'gene_status': str; 'gene_id': str; 'transcript_type': str; 'tag': str; 'transcript_status': str; 'gene_name': str; 'transcript_id': str; 'exon_number': str; 'havana_gene': str; 'interval': interval<locus<GRCh37>>; ----------------------------------------; Key: ['interval']; ----------------------------------------. Parameters:. path (str) – File to import.; reference_genome (str or ReferenceGenome, optional) – Reference genome to use.; skip_invalid_contigs (bool) – If True and reference_genome is not None, skip lines where; seqname is not consistent with the reference genome.; min_partitions (",MatchSource.WIKI,docs/0.2/experimental/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/experimental/index.html
https://hail.is/docs/0.2/experimental/ldscsim.html:17602,Deployability,update,updated,17602,"tb, coef_dict=None, str_expr=None, axis='rows')[source]; Aggregates by linear combination fields matching either keys in coef_dict; or str_expr. Outputs the aggregation in a MatrixTable or Table; as a new row field “agg_annot” or a new column field “agg_cov”. Parameters:. tb (MatrixTable or Table) – MatrixTable or Table containing fields to be aggregated.; coef_dict (dict, optional) – Coefficients to multiply each field. The coefficients are specified by; coef_dict value, the row (or col) field name is specified by coef_dict key.; If not included, coefficients are assumed to be 1.; str_expr (str, optional) – String expression to match against row (or col) field names.; axis (str) – Either ‘rows’ or ‘cols’. If ‘rows’, this aggregates across row fields.; If ‘cols’, this aggregates across col fields. If tb is a Table, axis = ‘rows’. Returns:; MatrixTable or Table – MatrixTable or Table containing aggregation field. hail.experimental.ldscsim.get_coef_dict(tb, str_expr=None, ref_coef_dict=None, axis='rows')[source]; Gets either col or row fields matching str_expr and take intersection; with keys in coefficient reference dict. Parameters:. tb (MatrixTable or Table) – MatrixTable or Table containing row (or col) for coef_dict.; str_expr (str, optional) – String expression pattern to match against row (or col) fields. If left; unspecified, the intersection of field names is only between existing; row (or col) fields in mt and keys of ref_coef_dict.; ref_coef_dict (dict, optional) – Reference coefficient dictionary with keys that are row (or col) field; names from which to subset. If not included, coefficients are assumed to be 1.; axis (str) – Field type in which to search for field names. Options: ‘rows’, ‘cols’. Returns:; coef_dict (dict) – Coefficients to multiply each field. The coefficients are specified by; coef_dict value, the row (or col) field name is specified by coef_dict key. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/experimental/ldscsim.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/experimental/ldscsim.html
https://hail.is/docs/0.2/experimental/ldscsim.html:3040,Integrability,depend,depending,3040,"alent of _annotate_all, but checks source MatrixTable of exprs. ascertainment_bias(mt, y, P); Adds ascertainment bias to a binary phenotype to give it a sample prevalence of P = cases/(cases+controls). binarize(mt, y, K[, exact]); Binarize phenotype y such that it has prevalence K = cases/(cases+controls) Uses inverse CDF of Gaussian to set binarization threshold when exact = False, otherwise uses ranking to determine threshold. agg_fields(tb[, coef_dict, str_expr, axis]); Aggregates by linear combination fields matching either keys in coef_dict or str_expr. get_coef_dict(tb[, str_expr, ref_coef_dict, ...]); Gets either col or row fields matching str_expr and take intersection with keys in coefficient reference dict. hail.experimental.ldscsim.simulate_phenotypes(mt, genotype, h2, pi=None, rg=None, annot=None, popstrat=None, popstrat_var=None, exact_h2=False)[source]; Simulate phenotypes for testing LD score regression.; Simulates betas (SNP effects) under the infinitesimal, spike & slab, or; annotation-informed models, depending on parameters passed. Optionally adds; population stratification. Parameters:. mt (MatrixTable) – MatrixTable containing genotypes to be used. Also should contain; variant annotations as row fields if running the annotation-informed; model or covariates as column fields if adding population stratification.; genotype (Expression or CallExpression) – Entry field containing genotypes of individuals to be used for the; simulation.; h2 (float or int or list or numpy.ndarray) – SNP-based heritability of simulated trait.; pi (float or int or list or numpy.ndarray, optional) – Probability of SNP being causal when simulating under the spike & slab; model.; rg (float or int or list or numpy.ndarray, optional) – Genetic correlation between traits.; annot (Expression, optional) – Row field to use as our aggregated annotations.; popstrat (Expression, optional) – Column field to use as our aggregated covariates for adding population; stratification.; exa",MatchSource.WIKI,docs/0.2/experimental/ldscsim.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/experimental/ldscsim.html
https://hail.is/docs/0.2/experimental/ldscsim.html:4619,Integrability,depend,depending,4619," int or list or numpy.ndarray, optional) – Probability of SNP being causal when simulating under the spike & slab; model.; rg (float or int or list or numpy.ndarray, optional) – Genetic correlation between traits.; annot (Expression, optional) – Row field to use as our aggregated annotations.; popstrat (Expression, optional) – Column field to use as our aggregated covariates for adding population; stratification.; exact_h2 (bool, optional) – Whether to exactly simulate ratio of variance of genetic component of; phenotype to variance of phenotype to be h2. If False, ratio will be; h2 in expectation. Observed h2 in the simulation will be close to; expected h2 for large-scale simulations. Returns:; MatrixTable – MatrixTable with simulated betas and phenotypes, simulated according; to specified model. hail.experimental.ldscsim.make_betas(mt, h2, pi=None, annot=None, rg=None)[source]; Generates betas under different models.; Simulates betas (SNP effects) under the infinitesimal, spike & slab, or; annotation-informed models, depending on parameters passed. Parameters:. mt (MatrixTable) – MatrixTable containing genotypes to be used. Also should contain; variant annotations as row fields if running the annotation-informed; model or covariates as column fields if adding population stratification.; h2 (float or int or list or numpy.ndarray) – SNP-based heritability of simulated trait(s).; pi (float or int or list or numpy.ndarray, optional) – Probability of SNP being causal when simulating under the spike & slab; model. If doing two-trait spike & slab pi is a list of probabilities for; overlapping causal SNPs (see docstring of multitrait_ss()); annot (Expression, optional) – Row field of aggregated annotations for annotation-informed model.; rg (float or int or list or numpy.ndarray, optional) – Genetic correlation between traits. Returns:. mt (MatrixTable) – MatrixTable with betas as a row field, simulated according to specified model.; pi (list) – Probability of a SNP bein",MatchSource.WIKI,docs/0.2/experimental/ldscsim.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/experimental/ldscsim.html
https://hail.is/docs/0.2/experimental/ldscsim.html:1288,Testability,test,testing,1288,"methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Experimental; ldscsim. View page source. ldscsim. Models for SNP effects:; Infinitesimal (can simulate n correlated traits); Spike & slab (can simulate 2 correlated traits); Annotation-informed. Features:; Field aggregation tools for annotation-informed model and; population stratification with many covariates.; Automatic adjustment of genetic correlation parameters; to allow for the joint simulation of up to 100 randomly; correlated phenotypes.; Methods for binarizing phenotypes to have a certain prevalence; and for adding ascertainment bias to binarized phenotypes. simulate_phenotypes(mt, genotype, h2[, pi, ...]); Simulate phenotypes for testing LD score regression. make_betas(mt, h2[, pi, annot, rg]); Generates betas under different models. multitrait_inf(mt[, h2, rg, cov_matrix, seed]); Generates correlated betas for multi-trait infinitesimal simulations for any number of phenotypes. multitrait_ss(mt, h2, pi[, rg, seed]); Generates spike & slab betas for simulation of two correlated phenotypes. get_cov_matrix(h2, rg[, psd_rg]); Creates covariance matrix for simulating correlated SNP effects. calculate_phenotypes(mt, genotype, beta, h2); Calculates phenotypes by multiplying genotypes and betas. normalize_genotypes(genotype); Normalizes genotypes to have mean 0 and variance 1 at each SNP. annotate_all(mt[, row_exprs, col_exprs, ...]); Equivalent of _annotate_all, but checks source MatrixTable of exprs. ascertainment_bias(mt, y, P); Adds ascertainment bias to a binary phenotype to give it a sample prevalence of P = cases/(cases+controls). binarize(mt, y, K[, exact]); Binarize phenotype y such that it has prevalence K = ",MatchSource.WIKI,docs/0.2/experimental/ldscsim.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/experimental/ldscsim.html
https://hail.is/docs/0.2/experimental/ldscsim.html:2909,Testability,test,testing,2909,"nd betas. normalize_genotypes(genotype); Normalizes genotypes to have mean 0 and variance 1 at each SNP. annotate_all(mt[, row_exprs, col_exprs, ...]); Equivalent of _annotate_all, but checks source MatrixTable of exprs. ascertainment_bias(mt, y, P); Adds ascertainment bias to a binary phenotype to give it a sample prevalence of P = cases/(cases+controls). binarize(mt, y, K[, exact]); Binarize phenotype y such that it has prevalence K = cases/(cases+controls) Uses inverse CDF of Gaussian to set binarization threshold when exact = False, otherwise uses ranking to determine threshold. agg_fields(tb[, coef_dict, str_expr, axis]); Aggregates by linear combination fields matching either keys in coef_dict or str_expr. get_coef_dict(tb[, str_expr, ref_coef_dict, ...]); Gets either col or row fields matching str_expr and take intersection with keys in coefficient reference dict. hail.experimental.ldscsim.simulate_phenotypes(mt, genotype, h2, pi=None, rg=None, annot=None, popstrat=None, popstrat_var=None, exact_h2=False)[source]; Simulate phenotypes for testing LD score regression.; Simulates betas (SNP effects) under the infinitesimal, spike & slab, or; annotation-informed models, depending on parameters passed. Optionally adds; population stratification. Parameters:. mt (MatrixTable) – MatrixTable containing genotypes to be used. Also should contain; variant annotations as row fields if running the annotation-informed; model or covariates as column fields if adding population stratification.; genotype (Expression or CallExpression) – Entry field containing genotypes of individuals to be used for the; simulation.; h2 (float or int or list or numpy.ndarray) – SNP-based heritability of simulated trait.; pi (float or int or list or numpy.ndarray, optional) – Probability of SNP being causal when simulating under the spike & slab; model.; rg (float or int or list or numpy.ndarray, optional) – Genetic correlation between traits.; annot (Expression, optional) – Row field to use a",MatchSource.WIKI,docs/0.2/experimental/ldscsim.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/experimental/ldscsim.html
https://hail.is/docs/0.2/functions/collections.html:13586,Availability,down,downstream,13586,"onary keyed by results of f. hail.expr.functions.fold(f, zero, collection)[source]; Reduces a collection with the given function f, provided the initial value zero.; Examples; >>> a = [0, 1, 2]. >>> hl.eval(hl.fold(lambda i, j: i + j, 0, a)); 3. Parameters:. f (function ( (Expression, Expression) -> Expression)) – Function which takes the cumulative value and the next element, and; returns a new value.; zero (Expression) – Initial value to pass in as left argument of f.; collection (ArrayExpression or SetExpression). Returns:; Expression. hail.expr.functions.array_scan(f, zero, a)[source]; Map each element of a to cumulative value of function f, with initial value zero.; Examples; >>> a = [0, 1, 2]. >>> hl.eval(hl.array_scan(lambda i, j: i + j, 0, a)); [0, 0, 1, 3]. Parameters:. f (function ( (Expression, Expression) -> Expression)) – Function which takes the cumulative value and the next element, and; returns a new value.; zero (Expression) – Initial value to pass in as left argument of f.; a (ArrayExpression). Returns:; ArrayExpression. hail.expr.functions.reversed(x)[source]; Reverses the elements of a collection.; Examples; >>> a = ['The', 'quick', 'brown', 'fox']; >>> hl.eval(hl.reversed(a)); ['fox', 'brown', 'quick', 'The']. Parameters:; x (ArrayExpression or StringExpression) – Array or string expression. Returns:; Expression. hail.expr.functions.keyed_intersection(*arrays, key)[source]; Compute the intersection of sorted arrays on a given key.; Requires sorted arrays with distinct keys. Warning; Experimental. Does not support downstream randomness. Parameters:. arrays; key. Returns:; ArrayExpression. hail.expr.functions.keyed_union(*arrays, key)[source]; Compute the distinct union of sorted arrays on a given key.; Requires sorted arrays with distinct keys. Warning; Experimental. Does not support downstream randomness. Parameters:. exprs; key. Returns:; ArrayExpression. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/functions/collections.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/collections.html
https://hail.is/docs/0.2/functions/collections.html:13862,Availability,down,downstream,13862,"onary keyed by results of f. hail.expr.functions.fold(f, zero, collection)[source]; Reduces a collection with the given function f, provided the initial value zero.; Examples; >>> a = [0, 1, 2]. >>> hl.eval(hl.fold(lambda i, j: i + j, 0, a)); 3. Parameters:. f (function ( (Expression, Expression) -> Expression)) – Function which takes the cumulative value and the next element, and; returns a new value.; zero (Expression) – Initial value to pass in as left argument of f.; collection (ArrayExpression or SetExpression). Returns:; Expression. hail.expr.functions.array_scan(f, zero, a)[source]; Map each element of a to cumulative value of function f, with initial value zero.; Examples; >>> a = [0, 1, 2]. >>> hl.eval(hl.array_scan(lambda i, j: i + j, 0, a)); [0, 0, 1, 3]. Parameters:. f (function ( (Expression, Expression) -> Expression)) – Function which takes the cumulative value and the next element, and; returns a new value.; zero (Expression) – Initial value to pass in as left argument of f.; a (ArrayExpression). Returns:; ArrayExpression. hail.expr.functions.reversed(x)[source]; Reverses the elements of a collection.; Examples; >>> a = ['The', 'quick', 'brown', 'fox']; >>> hl.eval(hl.reversed(a)); ['fox', 'brown', 'quick', 'The']. Parameters:; x (ArrayExpression or StringExpression) – Array or string expression. Returns:; Expression. hail.expr.functions.keyed_intersection(*arrays, key)[source]; Compute the intersection of sorted arrays on a given key.; Requires sorted arrays with distinct keys. Warning; Experimental. Does not support downstream randomness. Parameters:. arrays; key. Returns:; ArrayExpression. hail.expr.functions.keyed_union(*arrays, key)[source]; Compute the distinct union of sorted arrays on a given key.; Requires sorted arrays with distinct keys. Warning; Experimental. Does not support downstream randomness. Parameters:. exprs; key. Returns:; ArrayExpression. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/functions/collections.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/collections.html
https://hail.is/docs/0.2/functions/collections.html:13994,Deployability,update,updated,13994,"onary keyed by results of f. hail.expr.functions.fold(f, zero, collection)[source]; Reduces a collection with the given function f, provided the initial value zero.; Examples; >>> a = [0, 1, 2]. >>> hl.eval(hl.fold(lambda i, j: i + j, 0, a)); 3. Parameters:. f (function ( (Expression, Expression) -> Expression)) – Function which takes the cumulative value and the next element, and; returns a new value.; zero (Expression) – Initial value to pass in as left argument of f.; collection (ArrayExpression or SetExpression). Returns:; Expression. hail.expr.functions.array_scan(f, zero, a)[source]; Map each element of a to cumulative value of function f, with initial value zero.; Examples; >>> a = [0, 1, 2]. >>> hl.eval(hl.array_scan(lambda i, j: i + j, 0, a)); [0, 0, 1, 3]. Parameters:. f (function ( (Expression, Expression) -> Expression)) – Function which takes the cumulative value and the next element, and; returns a new value.; zero (Expression) – Initial value to pass in as left argument of f.; a (ArrayExpression). Returns:; ArrayExpression. hail.expr.functions.reversed(x)[source]; Reverses the elements of a collection.; Examples; >>> a = ['The', 'quick', 'brown', 'fox']; >>> hl.eval(hl.reversed(a)); ['fox', 'brown', 'quick', 'The']. Parameters:; x (ArrayExpression or StringExpression) – Array or string expression. Returns:; Expression. hail.expr.functions.keyed_intersection(*arrays, key)[source]; Compute the intersection of sorted arrays on a given key.; Requires sorted arrays with distinct keys. Warning; Experimental. Does not support downstream randomness. Parameters:. arrays; key. Returns:; ArrayExpression. hail.expr.functions.keyed_union(*arrays, key)[source]; Compute the distinct union of sorted arrays on a given key.; Requires sorted arrays with distinct keys. Warning; Experimental. Does not support downstream randomness. Parameters:. exprs; key. Returns:; ArrayExpression. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/functions/collections.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/collections.html
https://hail.is/docs/0.2/functions/collections.html:5357,Modifiability,variab,variable-length,5357,">>> a = [(1, 5), (3, 2), (7, 8)]. >>> hl.eval(hl.starmap(lambda x, y: hl.if_else(x < y, x, y), a)); [1, 2, 7]. Parameters:. f (function ( (*args) -> Expression)) – Function to transform each element of the collection.; collection (ArrayExpression or SetExpression) – Collection expression. Returns:; ArrayExpression or SetExpression. – Collection where each element has been transformed by f. hail.expr.functions.zip(*arrays, fill_missing=False)[source]; Zip together arrays into a single array.; Examples; >>> hl.eval(hl.zip([1, 2, 3], [4, 5, 6])); [(1, 4), (2, 5), (3, 6)]. If the arrays are different lengths, the behavior is decided by the fill_missing parameter.; >>> hl.eval(hl.zip([1], [10, 20], [100, 200, 300])); [(1, 10, 100)]. >>> hl.eval(hl.zip([1], [10, 20], [100, 200, 300], fill_missing=True)); [(1, 10, 100), (None, 20, 200), (None, None, 300)]. Notes; The element type of the resulting array is a ttuple with a field; for each array. Parameters:. arrays (: variable-length args of ArrayExpression) – Array expressions.; fill_missing (bool) – If False, return an array with length equal to the shortest length; of the arrays. If True, return an array equal to the longest; length of the arrays, by extending the shorter arrays with missing; values. Returns:; ArrayExpression. hail.expr.functions.enumerate(a, start=0, *, index_first=True)[source]; Returns an array of (index, element) tuples.; Examples; >>> hl.eval(hl.enumerate(['A', 'B', 'C'])); [(0, 'A'), (1, 'B'), (2, 'C')]. >>> hl.eval(hl.enumerate(['A', 'B', 'C'], start=3)); [(3, 'A'), (4, 'B'), (5, 'C')]. >>> hl.eval(hl.enumerate(['A', 'B', 'C'], index_first=False)); [('A', 0), ('B', 1), ('C', 2)]. Parameters:. a (ArrayExpression); start (Int32Expression) – The index value from which the counter is started, 0 by default.; index_first (bool) – If True, the index is the first value of the element tuples. If; False, the index is the second value. Returns:; ArrayExpression – Array of (index, element) or (element, index",MatchSource.WIKI,docs/0.2/functions/collections.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/collections.html
https://hail.is/docs/0.2/functions/collections.html:5597,Modifiability,extend,extending,5597,"he collection.; collection (ArrayExpression or SetExpression) – Collection expression. Returns:; ArrayExpression or SetExpression. – Collection where each element has been transformed by f. hail.expr.functions.zip(*arrays, fill_missing=False)[source]; Zip together arrays into a single array.; Examples; >>> hl.eval(hl.zip([1, 2, 3], [4, 5, 6])); [(1, 4), (2, 5), (3, 6)]. If the arrays are different lengths, the behavior is decided by the fill_missing parameter.; >>> hl.eval(hl.zip([1], [10, 20], [100, 200, 300])); [(1, 10, 100)]. >>> hl.eval(hl.zip([1], [10, 20], [100, 200, 300], fill_missing=True)); [(1, 10, 100), (None, 20, 200), (None, None, 300)]. Notes; The element type of the resulting array is a ttuple with a field; for each array. Parameters:. arrays (: variable-length args of ArrayExpression) – Array expressions.; fill_missing (bool) – If False, return an array with length equal to the shortest length; of the arrays. If True, return an array equal to the longest; length of the arrays, by extending the shorter arrays with missing; values. Returns:; ArrayExpression. hail.expr.functions.enumerate(a, start=0, *, index_first=True)[source]; Returns an array of (index, element) tuples.; Examples; >>> hl.eval(hl.enumerate(['A', 'B', 'C'])); [(0, 'A'), (1, 'B'), (2, 'C')]. >>> hl.eval(hl.enumerate(['A', 'B', 'C'], start=3)); [(3, 'A'), (4, 'B'), (5, 'C')]. >>> hl.eval(hl.enumerate(['A', 'B', 'C'], index_first=False)); [('A', 0), ('B', 1), ('C', 2)]. Parameters:. a (ArrayExpression); start (Int32Expression) – The index value from which the counter is started, 0 by default.; index_first (bool) – If True, the index is the first value of the element tuples. If; False, the index is the second value. Returns:; ArrayExpression – Array of (index, element) or (element, index) tuples. hail.expr.functions.zip_with_index(a, index_first=True)[source]; Deprecated in favor of enumerate().; Returns an array of (index, element) tuples.; Examples; >>> hl.eval(hl.zip_with_index(['A',",MatchSource.WIKI,docs/0.2/functions/collections.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/collections.html
https://hail.is/docs/0.2/functions/constructors.html:7165,Deployability,update,updated,7165," '3'). >>> hl.eval(t[2]); '3'. Parameters:; iterable (an iterable of Expression) – Tuple elements. Returns:; TupleExpression. hail.expr.functions.array(collection)[source]; Construct an array expression.; Examples; >>> s = {'Bob', 'Charlie', 'Alice'}. >>> hl.eval(hl.array(s)); ['Alice', 'Bob', 'Charlie']. Parameters:; collection (ArrayExpression or SetExpression or DictExpression). Returns:; ArrayExpression. hail.expr.functions.empty_array(t)[source]; Returns an empty array of elements of a type t.; Examples; >>> hl.eval(hl.empty_array(hl.tint32)); []. Parameters:; t (str or HailType) – Type of the array elements. Returns:; ArrayExpression. hail.expr.functions.set(collection)[source]; Convert a set expression.; Examples; >>> s = hl.set(['Bob', 'Charlie', 'Alice', 'Bob', 'Bob']); >>> hl.eval(s) ; {'Alice', 'Bob', 'Charlie'}. Returns:; SetExpression – Set of all unique elements. hail.expr.functions.empty_set(t)[source]; Returns an empty set of elements of a type t.; Examples; >>> hl.eval(hl.empty_set(hl.tstr)); set(). Parameters:; t (str or HailType) – Type of the set elements. Returns:; SetExpression. hail.expr.functions.dict(collection)[source]; Creates a dictionary.; Examples; >>> hl.eval(hl.dict([('foo', 1), ('bar', 2), ('baz', 3)])); {'bar': 2, 'baz': 3, 'foo': 1}. Notes; This method expects arrays or sets with elements of type ttuple; with 2 fields. The first field of the tuple becomes the key, and the second; field becomes the value. Parameters:; collection (DictExpression or ArrayExpression or SetExpression). Returns:; DictExpression. hail.expr.functions.empty_dict(key_type, value_type)[source]; Returns an empty dictionary with key type key_type and value type; value_type.; Examples; >>> hl.eval(hl.empty_dict(hl.tstr, hl.tint32)); {}. Parameters:. key_type (str or HailType) – Type of the keys.; value_type (str or HailType) – Type of the values. Returns:; DictExpression. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/functions/constructors.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/constructors.html
https://hail.is/docs/0.2/functions/core.html:11338,Deployability,update,updated,11338,".or_else(5, 7)); 5. >>> hl.eval(hl.or_else(hl.missing(hl.tint32), 7)); 7. See also; coalesce(). Parameters:. a (Expression); b (Expression). Returns:; Expression. hail.expr.functions.or_missing(predicate, value)[source]; Returns value if predicate is True, otherwise returns missing.; Examples; >>> hl.eval(hl.or_missing(True, 5)); 5. >>> hl.eval(hl.or_missing(False, 5)); None. Parameters:. predicate (BooleanExpression); value (Expression) – Value to return if predicate is True. Returns:; Expression – This expression has the same type as b. hail.expr.functions.range(start, stop=None, step=1)[source]; Returns an array of integers from start to stop by step.; Examples; >>> hl.eval(hl.range(10)); [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]. >>> hl.eval(hl.range(3, 10)); [3, 4, 5, 6, 7, 8, 9]. >>> hl.eval(hl.range(0, 10, step=3)); [0, 3, 6, 9]. Notes; The range includes start, but excludes stop.; If provided exactly one argument, the argument is interpreted as stop and; start is set to zero. This matches the behavior of Python’s range. Parameters:. start (int or Expression of type tint32) – Start of range.; stop (int or Expression of type tint32) – End of range.; step (int or Expression of type tint32) – Step of range. Returns:; ArrayNumericExpression. hail.expr.functions.query_table(path, point_or_interval)[source]; Query records from a table corresponding to a given point or range of keys.; Notes; This function does not dispatch to a distributed runtime; it can be used inside; already-distributed queries such as in Table.annotate(). Warning; This function contains no safeguards against reading large amounts of data; using a single thread. Parameters:. path (str) – Table path.; point_or_interval – Point or interval to query. Returns:; ArrayExpression. CaseBuilder; Class for chaining multiple if-else statements. SwitchBuilder; Class for generating conditional trees based on value of an expression. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/functions/core.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/core.html
https://hail.is/docs/0.2/functions/core.html:789,Modifiability,variab,variable,789,"﻿. Hail | ; Core language functions. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Functions; Core language functions. View page source. Core language functions. literal(x[, dtype]); Captures and broadcasts a Python variable or object as an expression. cond(condition, consequent, alternate[, ...]); Deprecated in favor of if_else(). if_else(condition, consequent, alternate[, ...]); Expression for an if/else statement; tests a condition and returns one of two options based on the result. switch(expr); Build a conditional tree on the value of an expression. case([missing_false]); Chain multiple if-else statements with a CaseBuilder. bind(f, *exprs[, _ctx]); Bind a temporary variable and use it in a function. rbind(*exprs[, _ctx]); Bind a temporary variable and use it in a function. missing(t); Creates an expression representing a missing value of a specified type. null(t); Deprecated in favor of missing(). str(x); Returns the string representation of x. is_missing(expression); Returns True if the argument is missing. is_defined(expression); Returns True if the argument is not missing. coalesce(*args); Returns the first non-missing value of args. or_else(a, b); If a is missing, return b. or_missing(predicate, value); Returns value if predicate is True, otherwise returns missing. range(start[, stop, step]); Returns an array of integers from start to stop by step. query_table(path, point_or_interval); Query rec",MatchSource.WIKI,docs/0.2/functions/core.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/core.html
https://hail.is/docs/0.2/functions/core.html:1253,Modifiability,variab,variable,1253,"les; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Functions; Core language functions. View page source. Core language functions. literal(x[, dtype]); Captures and broadcasts a Python variable or object as an expression. cond(condition, consequent, alternate[, ...]); Deprecated in favor of if_else(). if_else(condition, consequent, alternate[, ...]); Expression for an if/else statement; tests a condition and returns one of two options based on the result. switch(expr); Build a conditional tree on the value of an expression. case([missing_false]); Chain multiple if-else statements with a CaseBuilder. bind(f, *exprs[, _ctx]); Bind a temporary variable and use it in a function. rbind(*exprs[, _ctx]); Bind a temporary variable and use it in a function. missing(t); Creates an expression representing a missing value of a specified type. null(t); Deprecated in favor of missing(). str(x); Returns the string representation of x. is_missing(expression); Returns True if the argument is missing. is_defined(expression); Returns True if the argument is not missing. coalesce(*args); Returns the first non-missing value of args. or_else(a, b); If a is missing, return b. or_missing(predicate, value); Returns value if predicate is True, otherwise returns missing. range(start[, stop, step]); Returns an array of integers from start to stop by step. query_table(path, point_or_interval); Query records from a table corresponding to a given point or range of keys. hail.expr.functions.literal(x, dtype=None)[source]; Captures and broadcasts a Python variable or object as an expression.; Examples; >>> table = hl.utils.range_table(8); >>> greeti",MatchSource.WIKI,docs/0.2/functions/core.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/core.html
https://hail.is/docs/0.2/functions/core.html:1328,Modifiability,variab,variable,1328,"linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Functions; Core language functions. View page source. Core language functions. literal(x[, dtype]); Captures and broadcasts a Python variable or object as an expression. cond(condition, consequent, alternate[, ...]); Deprecated in favor of if_else(). if_else(condition, consequent, alternate[, ...]); Expression for an if/else statement; tests a condition and returns one of two options based on the result. switch(expr); Build a conditional tree on the value of an expression. case([missing_false]); Chain multiple if-else statements with a CaseBuilder. bind(f, *exprs[, _ctx]); Bind a temporary variable and use it in a function. rbind(*exprs[, _ctx]); Bind a temporary variable and use it in a function. missing(t); Creates an expression representing a missing value of a specified type. null(t); Deprecated in favor of missing(). str(x); Returns the string representation of x. is_missing(expression); Returns True if the argument is missing. is_defined(expression); Returns True if the argument is not missing. coalesce(*args); Returns the first non-missing value of args. or_else(a, b); If a is missing, return b. or_missing(predicate, value); Returns value if predicate is True, otherwise returns missing. range(start[, stop, step]); Returns an array of integers from start to stop by step. query_table(path, point_or_interval); Query records from a table corresponding to a given point or range of keys. hail.expr.functions.literal(x, dtype=None)[source]; Captures and broadcasts a Python variable or object as an expression.; Examples; >>> table = hl.utils.range_table(8); >>> greetings = hl.literal({1: 'Good morning', 4: 'Good afternoon', 6 : 'Good evening'",MatchSource.WIKI,docs/0.2/functions/core.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/core.html
https://hail.is/docs/0.2/functions/core.html:2154,Modifiability,variab,variable,2154,"[missing_false]); Chain multiple if-else statements with a CaseBuilder. bind(f, *exprs[, _ctx]); Bind a temporary variable and use it in a function. rbind(*exprs[, _ctx]); Bind a temporary variable and use it in a function. missing(t); Creates an expression representing a missing value of a specified type. null(t); Deprecated in favor of missing(). str(x); Returns the string representation of x. is_missing(expression); Returns True if the argument is missing. is_defined(expression); Returns True if the argument is not missing. coalesce(*args); Returns the first non-missing value of args. or_else(a, b); If a is missing, return b. or_missing(predicate, value); Returns value if predicate is True, otherwise returns missing. range(start[, stop, step]); Returns an array of integers from start to stop by step. query_table(path, point_or_interval); Query records from a table corresponding to a given point or range of keys. hail.expr.functions.literal(x, dtype=None)[source]; Captures and broadcasts a Python variable or object as an expression.; Examples; >>> table = hl.utils.range_table(8); >>> greetings = hl.literal({1: 'Good morning', 4: 'Good afternoon', 6 : 'Good evening'}); >>> table.annotate(greeting = greetings.get(table.idx)).show(); +-------+------------------+; | idx | greeting |; +-------+------------------+; | int32 | str |; +-------+------------------+; | 0 | NA |; | 1 | ""Good morning"" |; | 2 | NA |; | 3 | NA |; | 4 | ""Good afternoon"" |; | 5 | NA |; | 6 | ""Good evening"" |; | 7 | NA |; +-------+------------------+. Notes; Use this function to capture large Python objects for use in expressions. This; function provides an alternative to adding an object as a global annotation on a; Table or MatrixTable. Parameters:; x – Object to capture and broadcast as an expression. Returns:; Expression. hail.expr.functions.cond(condition, consequent, alternate, missing_false=False)[source]; Deprecated in favor of if_else().; Expression for an if/else statement; tests a condi",MatchSource.WIKI,docs/0.2/functions/core.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/core.html
https://hail.is/docs/0.2/functions/core.html:5998,Modifiability,variab,variable,5998,"unctions.switch(expr)[source]; Build a conditional tree on the value of an expression.; Examples; >>> csq = hl.literal('loss of function'); >>> expr = (hl.switch(csq); ... .when('synonymous', 1); ... .when('SYN', 1); ... .when('missense', 2); ... .when('MIS', 2); ... .when('loss of function', 3); ... .when('LOF', 3); ... .or_missing()); >>> hl.eval(expr); 3. See also; SwitchBuilder, case(), cond(). Parameters:; expr (Expression) – Value to match against. Returns:; SwitchBuilder. hail.expr.functions.case(missing_false=False)[source]; Chain multiple if-else statements with a CaseBuilder.; Examples; >>> x = hl.literal('foo bar baz'); >>> expr = (hl.case(); ... .when(x[:3] == 'FOO', 1); ... .when(hl.len(x) == 11, 2); ... .when(x == 'secret phrase', 3); ... .default(0)); >>> hl.eval(expr); 2. Parameters:; missing_false (bool) – Treat missing predicates as False. See also; CaseBuilder, switch(), cond(). Returns:; CaseBuilder. hail.expr.functions.bind(f, *exprs, _ctx=None)[source]; Bind a temporary variable and use it in a function.; Examples; >>> hl.eval(hl.bind(lambda x: x + 1, 1)); 2. bind() also can take multiple arguments:; >>> hl.eval(hl.bind(lambda x, y: x / y, x, x)); 1.0. Parameters:. f (function ( (args) -> Expression)) – Function of exprs.; exprs (variable-length args of Expression) – Expressions to bind. Returns:; Expression – Result of evaluating f with exprs as arguments. hail.expr.functions.rbind(*exprs, _ctx=None)[source]; Bind a temporary variable and use it in a function.; This is bind() with flipped argument order.; Examples; >>> hl.eval(hl.rbind(1, lambda x: x + 1)); 2. rbind() also can take multiple arguments:; >>> hl.eval(hl.rbind(4.0, 2.0, lambda x, y: x / y)); 2.0. Parameters:. exprs (variable-length args of Expression) – Expressions to bind.; f (function ( (args) -> Expression)) – Function of exprs. Returns:; Expression – Result of evaluating f with exprs as arguments. hail.expr.functions.missing(t)[source]; Creates an expression representing ",MatchSource.WIKI,docs/0.2/functions/core.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/core.html
https://hail.is/docs/0.2/functions/core.html:6263,Modifiability,variab,variable-length,6263,"... .when('LOF', 3); ... .or_missing()); >>> hl.eval(expr); 3. See also; SwitchBuilder, case(), cond(). Parameters:; expr (Expression) – Value to match against. Returns:; SwitchBuilder. hail.expr.functions.case(missing_false=False)[source]; Chain multiple if-else statements with a CaseBuilder.; Examples; >>> x = hl.literal('foo bar baz'); >>> expr = (hl.case(); ... .when(x[:3] == 'FOO', 1); ... .when(hl.len(x) == 11, 2); ... .when(x == 'secret phrase', 3); ... .default(0)); >>> hl.eval(expr); 2. Parameters:; missing_false (bool) – Treat missing predicates as False. See also; CaseBuilder, switch(), cond(). Returns:; CaseBuilder. hail.expr.functions.bind(f, *exprs, _ctx=None)[source]; Bind a temporary variable and use it in a function.; Examples; >>> hl.eval(hl.bind(lambda x: x + 1, 1)); 2. bind() also can take multiple arguments:; >>> hl.eval(hl.bind(lambda x, y: x / y, x, x)); 1.0. Parameters:. f (function ( (args) -> Expression)) – Function of exprs.; exprs (variable-length args of Expression) – Expressions to bind. Returns:; Expression – Result of evaluating f with exprs as arguments. hail.expr.functions.rbind(*exprs, _ctx=None)[source]; Bind a temporary variable and use it in a function.; This is bind() with flipped argument order.; Examples; >>> hl.eval(hl.rbind(1, lambda x: x + 1)); 2. rbind() also can take multiple arguments:; >>> hl.eval(hl.rbind(4.0, 2.0, lambda x, y: x / y)); 2.0. Parameters:. exprs (variable-length args of Expression) – Expressions to bind.; f (function ( (args) -> Expression)) – Function of exprs. Returns:; Expression – Result of evaluating f with exprs as arguments. hail.expr.functions.missing(t)[source]; Creates an expression representing a missing value of a specified type.; Examples; >>> hl.eval(hl.missing(hl.tarray(hl.tstr))); None. >>> hl.eval(hl.missing('array<str>')); None. Notes; This method is useful for constructing an expression that includes missing; values, since None cannot be interpreted as an expression. Parameters:; t",MatchSource.WIKI,docs/0.2/functions/core.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/core.html
https://hail.is/docs/0.2/functions/core.html:6465,Modifiability,variab,variable,6465,"; SwitchBuilder. hail.expr.functions.case(missing_false=False)[source]; Chain multiple if-else statements with a CaseBuilder.; Examples; >>> x = hl.literal('foo bar baz'); >>> expr = (hl.case(); ... .when(x[:3] == 'FOO', 1); ... .when(hl.len(x) == 11, 2); ... .when(x == 'secret phrase', 3); ... .default(0)); >>> hl.eval(expr); 2. Parameters:; missing_false (bool) – Treat missing predicates as False. See also; CaseBuilder, switch(), cond(). Returns:; CaseBuilder. hail.expr.functions.bind(f, *exprs, _ctx=None)[source]; Bind a temporary variable and use it in a function.; Examples; >>> hl.eval(hl.bind(lambda x: x + 1, 1)); 2. bind() also can take multiple arguments:; >>> hl.eval(hl.bind(lambda x, y: x / y, x, x)); 1.0. Parameters:. f (function ( (args) -> Expression)) – Function of exprs.; exprs (variable-length args of Expression) – Expressions to bind. Returns:; Expression – Result of evaluating f with exprs as arguments. hail.expr.functions.rbind(*exprs, _ctx=None)[source]; Bind a temporary variable and use it in a function.; This is bind() with flipped argument order.; Examples; >>> hl.eval(hl.rbind(1, lambda x: x + 1)); 2. rbind() also can take multiple arguments:; >>> hl.eval(hl.rbind(4.0, 2.0, lambda x, y: x / y)); 2.0. Parameters:. exprs (variable-length args of Expression) – Expressions to bind.; f (function ( (args) -> Expression)) – Function of exprs. Returns:; Expression – Result of evaluating f with exprs as arguments. hail.expr.functions.missing(t)[source]; Creates an expression representing a missing value of a specified type.; Examples; >>> hl.eval(hl.missing(hl.tarray(hl.tstr))); None. >>> hl.eval(hl.missing('array<str>')); None. Notes; This method is useful for constructing an expression that includes missing; values, since None cannot be interpreted as an expression. Parameters:; t (str or HailType) – Type of the missing expression. Returns:; Expression – A missing expression of type t. hail.expr.functions.null(t)[source]; Deprecated in favor of",MatchSource.WIKI,docs/0.2/functions/core.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/core.html
https://hail.is/docs/0.2/functions/core.html:6723,Modifiability,variab,variable-length,6723," ... .default(0)); >>> hl.eval(expr); 2. Parameters:; missing_false (bool) – Treat missing predicates as False. See also; CaseBuilder, switch(), cond(). Returns:; CaseBuilder. hail.expr.functions.bind(f, *exprs, _ctx=None)[source]; Bind a temporary variable and use it in a function.; Examples; >>> hl.eval(hl.bind(lambda x: x + 1, 1)); 2. bind() also can take multiple arguments:; >>> hl.eval(hl.bind(lambda x, y: x / y, x, x)); 1.0. Parameters:. f (function ( (args) -> Expression)) – Function of exprs.; exprs (variable-length args of Expression) – Expressions to bind. Returns:; Expression – Result of evaluating f with exprs as arguments. hail.expr.functions.rbind(*exprs, _ctx=None)[source]; Bind a temporary variable and use it in a function.; This is bind() with flipped argument order.; Examples; >>> hl.eval(hl.rbind(1, lambda x: x + 1)); 2. rbind() also can take multiple arguments:; >>> hl.eval(hl.rbind(4.0, 2.0, lambda x, y: x / y)); 2.0. Parameters:. exprs (variable-length args of Expression) – Expressions to bind.; f (function ( (args) -> Expression)) – Function of exprs. Returns:; Expression – Result of evaluating f with exprs as arguments. hail.expr.functions.missing(t)[source]; Creates an expression representing a missing value of a specified type.; Examples; >>> hl.eval(hl.missing(hl.tarray(hl.tstr))); None. >>> hl.eval(hl.missing('array<str>')); None. Notes; This method is useful for constructing an expression that includes missing; values, since None cannot be interpreted as an expression. Parameters:; t (str or HailType) – Type of the missing expression. Returns:; Expression – A missing expression of type t. hail.expr.functions.null(t)[source]; Deprecated in favor of missing().; Creates an expression representing a missing value of a specified type.; Examples; >>> hl.eval(hl.null(hl.tarray(hl.tstr))); None. >>> hl.eval(hl.null('array<str>')); None. Notes; This method is useful for constructing an expression that includes missing; values, since None cann",MatchSource.WIKI,docs/0.2/functions/core.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/core.html
https://hail.is/docs/0.2/functions/core.html:9211,Modifiability,variab,variable-length,9211,"_missing(5)); False. >>> hl.eval(hl.is_missing(hl.missing(hl.tstr))); True. >>> hl.eval(hl.is_missing(hl.missing(hl.tbool) & True)); True. Parameters:; expression – Expression to test. Returns:; BooleanExpression – True if expression is missing, False otherwise. hail.expr.functions.is_defined(expression)[source]; Returns True if the argument is not missing.; Examples; >>> hl.eval(hl.is_defined(5)); True. >>> hl.eval(hl.is_defined(hl.missing(hl.tstr))); False. >>> hl.eval(hl.is_defined(hl.missing(hl.tbool) & True)); False. Parameters:; expression – Expression to test. Returns:; BooleanExpression – True if expression is not missing, False otherwise. hail.expr.functions.coalesce(*args)[source]; Returns the first non-missing value of args.; Examples; >>> x1 = hl.missing('int'); >>> x2 = 2; >>> hl.eval(hl.coalesce(x1, x2)); 2. Notes; All arguments must have the same type, or must be convertible to a common; type (all numeric, for instance). See also; or_else(). Parameters:; args (variable-length args of Expression). Returns:; Expression. hail.expr.functions.or_else(a, b)[source]; If a is missing, return b.; Examples; >>> hl.eval(hl.or_else(5, 7)); 5. >>> hl.eval(hl.or_else(hl.missing(hl.tint32), 7)); 7. See also; coalesce(). Parameters:. a (Expression); b (Expression). Returns:; Expression. hail.expr.functions.or_missing(predicate, value)[source]; Returns value if predicate is True, otherwise returns missing.; Examples; >>> hl.eval(hl.or_missing(True, 5)); 5. >>> hl.eval(hl.or_missing(False, 5)); None. Parameters:. predicate (BooleanExpression); value (Expression) – Value to return if predicate is True. Returns:; Expression – This expression has the same type as b. hail.expr.functions.range(start, stop=None, step=1)[source]; Returns an array of integers from start to stop by step.; Examples; >>> hl.eval(hl.range(10)); [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]. >>> hl.eval(hl.range(3, 10)); [3, 4, 5, 6, 7, 8, 9]. >>> hl.eval(hl.range(0, 10, step=3)); [0, 3, 6, 9]. Notes; The ra",MatchSource.WIKI,docs/0.2/functions/core.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/core.html
https://hail.is/docs/0.2/functions/core.html:10946,Safety,safe,safeguards,10946,".or_else(5, 7)); 5. >>> hl.eval(hl.or_else(hl.missing(hl.tint32), 7)); 7. See also; coalesce(). Parameters:. a (Expression); b (Expression). Returns:; Expression. hail.expr.functions.or_missing(predicate, value)[source]; Returns value if predicate is True, otherwise returns missing.; Examples; >>> hl.eval(hl.or_missing(True, 5)); 5. >>> hl.eval(hl.or_missing(False, 5)); None. Parameters:. predicate (BooleanExpression); value (Expression) – Value to return if predicate is True. Returns:; Expression – This expression has the same type as b. hail.expr.functions.range(start, stop=None, step=1)[source]; Returns an array of integers from start to stop by step.; Examples; >>> hl.eval(hl.range(10)); [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]. >>> hl.eval(hl.range(3, 10)); [3, 4, 5, 6, 7, 8, 9]. >>> hl.eval(hl.range(0, 10, step=3)); [0, 3, 6, 9]. Notes; The range includes start, but excludes stop.; If provided exactly one argument, the argument is interpreted as stop and; start is set to zero. This matches the behavior of Python’s range. Parameters:. start (int or Expression of type tint32) – Start of range.; stop (int or Expression of type tint32) – End of range.; step (int or Expression of type tint32) – Step of range. Returns:; ArrayNumericExpression. hail.expr.functions.query_table(path, point_or_interval)[source]; Query records from a table corresponding to a given point or range of keys.; Notes; This function does not dispatch to a distributed runtime; it can be used inside; already-distributed queries such as in Table.annotate(). Warning; This function contains no safeguards against reading large amounts of data; using a single thread. Parameters:. path (str) – Table path.; point_or_interval – Point or interval to query. Returns:; ArrayExpression. CaseBuilder; Class for chaining multiple if-else statements. SwitchBuilder; Class for generating conditional trees based on value of an expression. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/functions/core.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/core.html
https://hail.is/docs/0.2/functions/core.html:994,Testability,test,tests,994,"| ; Core language functions. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Functions; Core language functions. View page source. Core language functions. literal(x[, dtype]); Captures and broadcasts a Python variable or object as an expression. cond(condition, consequent, alternate[, ...]); Deprecated in favor of if_else(). if_else(condition, consequent, alternate[, ...]); Expression for an if/else statement; tests a condition and returns one of two options based on the result. switch(expr); Build a conditional tree on the value of an expression. case([missing_false]); Chain multiple if-else statements with a CaseBuilder. bind(f, *exprs[, _ctx]); Bind a temporary variable and use it in a function. rbind(*exprs[, _ctx]); Bind a temporary variable and use it in a function. missing(t); Creates an expression representing a missing value of a specified type. null(t); Deprecated in favor of missing(). str(x); Returns the string representation of x. is_missing(expression); Returns True if the argument is missing. is_defined(expression); Returns True if the argument is not missing. coalesce(*args); Returns the first non-missing value of args. or_else(a, b); If a is missing, return b. or_missing(predicate, value); Returns value if predicate is True, otherwise returns missing. range(start[, stop, step]); Returns an array of integers from start to stop by step. query_table(path, point_or_interval); Query records fr",MatchSource.WIKI,docs/0.2/functions/core.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/core.html
https://hail.is/docs/0.2/functions/core.html:3126,Testability,test,tests,3126,"sts a Python variable or object as an expression.; Examples; >>> table = hl.utils.range_table(8); >>> greetings = hl.literal({1: 'Good morning', 4: 'Good afternoon', 6 : 'Good evening'}); >>> table.annotate(greeting = greetings.get(table.idx)).show(); +-------+------------------+; | idx | greeting |; +-------+------------------+; | int32 | str |; +-------+------------------+; | 0 | NA |; | 1 | ""Good morning"" |; | 2 | NA |; | 3 | NA |; | 4 | ""Good afternoon"" |; | 5 | NA |; | 6 | ""Good evening"" |; | 7 | NA |; +-------+------------------+. Notes; Use this function to capture large Python objects for use in expressions. This; function provides an alternative to adding an object as a global annotation on a; Table or MatrixTable. Parameters:; x – Object to capture and broadcast as an expression. Returns:; Expression. hail.expr.functions.cond(condition, consequent, alternate, missing_false=False)[source]; Deprecated in favor of if_else().; Expression for an if/else statement; tests a condition and returns one of two options based on the result.; Examples; >>> x = 5; >>> hl.eval(hl.cond(x < 2, 'Hi', 'Bye')); 'Bye'. >>> a = hl.literal([1, 2, 3, 4]); >>> hl.eval(hl.cond(hl.len(a) > 0, 2.0 * a, a / 2.0)); [2.0, 4.0, 6.0, 8.0]. Notes; If condition evaluates to True, returns consequent. If condition; evaluates to False, returns alternate. If predicate is missing, returns; missing. Note; The type of consequent and alternate must be the same. Parameters:. condition (BooleanExpression) – Condition to test.; consequent (Expression) – Branch to return if the condition is True.; alternate (Expression) – Branch to return if the condition is False.; missing_false (bool) – If True, treat missing condition as False. See also; case(), switch(), if_else(). Returns:; Expression – One of consequent, alternate, or missing, based on condition. hail.expr.functions.if_else(condition, consequent, alternate, missing_false=False)[source]; Expression for an if/else statement; tests a condition and ",MatchSource.WIKI,docs/0.2/functions/core.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/core.html
https://hail.is/docs/0.2/functions/core.html:3652,Testability,test,test,3652,"vening"" |; | 7 | NA |; +-------+------------------+. Notes; Use this function to capture large Python objects for use in expressions. This; function provides an alternative to adding an object as a global annotation on a; Table or MatrixTable. Parameters:; x – Object to capture and broadcast as an expression. Returns:; Expression. hail.expr.functions.cond(condition, consequent, alternate, missing_false=False)[source]; Deprecated in favor of if_else().; Expression for an if/else statement; tests a condition and returns one of two options based on the result.; Examples; >>> x = 5; >>> hl.eval(hl.cond(x < 2, 'Hi', 'Bye')); 'Bye'. >>> a = hl.literal([1, 2, 3, 4]); >>> hl.eval(hl.cond(hl.len(a) > 0, 2.0 * a, a / 2.0)); [2.0, 4.0, 6.0, 8.0]. Notes; If condition evaluates to True, returns consequent. If condition; evaluates to False, returns alternate. If predicate is missing, returns; missing. Note; The type of consequent and alternate must be the same. Parameters:. condition (BooleanExpression) – Condition to test.; consequent (Expression) – Branch to return if the condition is True.; alternate (Expression) – Branch to return if the condition is False.; missing_false (bool) – If True, treat missing condition as False. See also; case(), switch(), if_else(). Returns:; Expression – One of consequent, alternate, or missing, based on condition. hail.expr.functions.if_else(condition, consequent, alternate, missing_false=False)[source]; Expression for an if/else statement; tests a condition and returns one of two options based on the result.; Examples; >>> x = 5; >>> hl.eval(hl.if_else(x < 2, 'Hi', 'Bye')); 'Bye'. >>> a = hl.literal([1, 2, 3, 4]); >>> hl.eval(hl.if_else(hl.len(a) > 0, 2.0 * a, a / 2.0)); [2.0, 4.0, 6.0, 8.0]. Notes; If condition evaluates to True, returns consequent. If condition; evaluates to False, returns alternate. If predicate is missing, returns; missing. Note; The type of consequent and alternate must be the same. Parameters:. condition (BooleanExpressi",MatchSource.WIKI,docs/0.2/functions/core.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/core.html
https://hail.is/docs/0.2/functions/core.html:4119,Testability,test,tests,4119,"n for an if/else statement; tests a condition and returns one of two options based on the result.; Examples; >>> x = 5; >>> hl.eval(hl.cond(x < 2, 'Hi', 'Bye')); 'Bye'. >>> a = hl.literal([1, 2, 3, 4]); >>> hl.eval(hl.cond(hl.len(a) > 0, 2.0 * a, a / 2.0)); [2.0, 4.0, 6.0, 8.0]. Notes; If condition evaluates to True, returns consequent. If condition; evaluates to False, returns alternate. If predicate is missing, returns; missing. Note; The type of consequent and alternate must be the same. Parameters:. condition (BooleanExpression) – Condition to test.; consequent (Expression) – Branch to return if the condition is True.; alternate (Expression) – Branch to return if the condition is False.; missing_false (bool) – If True, treat missing condition as False. See also; case(), switch(), if_else(). Returns:; Expression – One of consequent, alternate, or missing, based on condition. hail.expr.functions.if_else(condition, consequent, alternate, missing_false=False)[source]; Expression for an if/else statement; tests a condition and returns one of two options based on the result.; Examples; >>> x = 5; >>> hl.eval(hl.if_else(x < 2, 'Hi', 'Bye')); 'Bye'. >>> a = hl.literal([1, 2, 3, 4]); >>> hl.eval(hl.if_else(hl.len(a) > 0, 2.0 * a, a / 2.0)); [2.0, 4.0, 6.0, 8.0]. Notes; If condition evaluates to True, returns consequent. If condition; evaluates to False, returns alternate. If predicate is missing, returns; missing. Note; The type of consequent and alternate must be the same. Parameters:. condition (BooleanExpression) – Condition to test.; consequent (Expression) – Branch to return if the condition is True.; alternate (Expression) – Branch to return if the condition is False.; missing_false (bool) – If True, treat missing condition as False. See also; case(), switch(). Returns:; Expression – One of consequent, alternate, or missing, based on condition. hail.expr.functions.switch(expr)[source]; Build a conditional tree on the value of an expression.; Examples; >>> csq = hl",MatchSource.WIKI,docs/0.2/functions/core.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/core.html
https://hail.is/docs/0.2/functions/core.html:4651,Testability,test,test,4651,"ssion) – Condition to test.; consequent (Expression) – Branch to return if the condition is True.; alternate (Expression) – Branch to return if the condition is False.; missing_false (bool) – If True, treat missing condition as False. See also; case(), switch(), if_else(). Returns:; Expression – One of consequent, alternate, or missing, based on condition. hail.expr.functions.if_else(condition, consequent, alternate, missing_false=False)[source]; Expression for an if/else statement; tests a condition and returns one of two options based on the result.; Examples; >>> x = 5; >>> hl.eval(hl.if_else(x < 2, 'Hi', 'Bye')); 'Bye'. >>> a = hl.literal([1, 2, 3, 4]); >>> hl.eval(hl.if_else(hl.len(a) > 0, 2.0 * a, a / 2.0)); [2.0, 4.0, 6.0, 8.0]. Notes; If condition evaluates to True, returns consequent. If condition; evaluates to False, returns alternate. If predicate is missing, returns; missing. Note; The type of consequent and alternate must be the same. Parameters:. condition (BooleanExpression) – Condition to test.; consequent (Expression) – Branch to return if the condition is True.; alternate (Expression) – Branch to return if the condition is False.; missing_false (bool) – If True, treat missing condition as False. See also; case(), switch(). Returns:; Expression – One of consequent, alternate, or missing, based on condition. hail.expr.functions.switch(expr)[source]; Build a conditional tree on the value of an expression.; Examples; >>> csq = hl.literal('loss of function'); >>> expr = (hl.switch(csq); ... .when('synonymous', 1); ... .when('SYN', 1); ... .when('missense', 2); ... .when('MIS', 2); ... .when('loss of function', 3); ... .when('LOF', 3); ... .or_missing()); >>> hl.eval(expr); 3. See also; SwitchBuilder, case(), cond(). Parameters:; expr (Expression) – Value to match against. Returns:; SwitchBuilder. hail.expr.functions.case(missing_false=False)[source]; Chain multiple if-else statements with a CaseBuilder.; Examples; >>> x = hl.literal('foo bar baz'); >>",MatchSource.WIKI,docs/0.2/functions/core.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/core.html
https://hail.is/docs/0.2/functions/core.html:8398,Testability,test,test,8398,"sion of type t. hail.expr.functions.null(t)[source]; Deprecated in favor of missing().; Creates an expression representing a missing value of a specified type.; Examples; >>> hl.eval(hl.null(hl.tarray(hl.tstr))); None. >>> hl.eval(hl.null('array<str>')); None. Notes; This method is useful for constructing an expression that includes missing; values, since None cannot be interpreted as an expression. Parameters:; t (str or HailType) – Type of the missing expression. Returns:; Expression – A missing expression of type t. hail.expr.functions.str(x)[source]; Returns the string representation of x.; Examples; >>> hl.eval(hl.str(hl.struct(a=5, b=7))); '{""a"":5,""b"":7}'. Parameters:; x. Returns:; StringExpression. hail.expr.functions.is_missing(expression)[source]; Returns True if the argument is missing.; Examples; >>> hl.eval(hl.is_missing(5)); False. >>> hl.eval(hl.is_missing(hl.missing(hl.tstr))); True. >>> hl.eval(hl.is_missing(hl.missing(hl.tbool) & True)); True. Parameters:; expression – Expression to test. Returns:; BooleanExpression – True if expression is missing, False otherwise. hail.expr.functions.is_defined(expression)[source]; Returns True if the argument is not missing.; Examples; >>> hl.eval(hl.is_defined(5)); True. >>> hl.eval(hl.is_defined(hl.missing(hl.tstr))); False. >>> hl.eval(hl.is_defined(hl.missing(hl.tbool) & True)); False. Parameters:; expression – Expression to test. Returns:; BooleanExpression – True if expression is not missing, False otherwise. hail.expr.functions.coalesce(*args)[source]; Returns the first non-missing value of args.; Examples; >>> x1 = hl.missing('int'); >>> x2 = 2; >>> hl.eval(hl.coalesce(x1, x2)); 2. Notes; All arguments must have the same type, or must be convertible to a common; type (all numeric, for instance). See also; or_else(). Parameters:; args (variable-length args of Expression). Returns:; Expression. hail.expr.functions.or_else(a, b)[source]; If a is missing, return b.; Examples; >>> hl.eval(hl.or_else(5, 7)",MatchSource.WIKI,docs/0.2/functions/core.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/core.html
https://hail.is/docs/0.2/functions/core.html:8788,Testability,test,test,8788,"n expression. Parameters:; t (str or HailType) – Type of the missing expression. Returns:; Expression – A missing expression of type t. hail.expr.functions.str(x)[source]; Returns the string representation of x.; Examples; >>> hl.eval(hl.str(hl.struct(a=5, b=7))); '{""a"":5,""b"":7}'. Parameters:; x. Returns:; StringExpression. hail.expr.functions.is_missing(expression)[source]; Returns True if the argument is missing.; Examples; >>> hl.eval(hl.is_missing(5)); False. >>> hl.eval(hl.is_missing(hl.missing(hl.tstr))); True. >>> hl.eval(hl.is_missing(hl.missing(hl.tbool) & True)); True. Parameters:; expression – Expression to test. Returns:; BooleanExpression – True if expression is missing, False otherwise. hail.expr.functions.is_defined(expression)[source]; Returns True if the argument is not missing.; Examples; >>> hl.eval(hl.is_defined(5)); True. >>> hl.eval(hl.is_defined(hl.missing(hl.tstr))); False. >>> hl.eval(hl.is_defined(hl.missing(hl.tbool) & True)); False. Parameters:; expression – Expression to test. Returns:; BooleanExpression – True if expression is not missing, False otherwise. hail.expr.functions.coalesce(*args)[source]; Returns the first non-missing value of args.; Examples; >>> x1 = hl.missing('int'); >>> x2 = 2; >>> hl.eval(hl.coalesce(x1, x2)); 2. Notes; All arguments must have the same type, or must be convertible to a common; type (all numeric, for instance). See also; or_else(). Parameters:; args (variable-length args of Expression). Returns:; Expression. hail.expr.functions.or_else(a, b)[source]; If a is missing, return b.; Examples; >>> hl.eval(hl.or_else(5, 7)); 5. >>> hl.eval(hl.or_else(hl.missing(hl.tint32), 7)); 7. See also; coalesce(). Parameters:. a (Expression); b (Expression). Returns:; Expression. hail.expr.functions.or_missing(predicate, value)[source]; Returns value if predicate is True, otherwise returns missing.; Examples; >>> hl.eval(hl.or_missing(True, 5)); 5. >>> hl.eval(hl.or_missing(False, 5)); None. Parameters:. predicate (",MatchSource.WIKI,docs/0.2/functions/core.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/core.html
https://hail.is/docs/0.2/functions/genetics.html:1669,Availability,down,downcode,1669," functions. locus(contig, pos[, reference_genome]); Construct a locus expression from a chromosome and position. locus_from_global_position(global_pos[, ...]); Constructs a locus expression from a global position and a reference genome. locus_interval(contig, start, end[, ...]); Construct a locus interval expression. parse_locus(s[, reference_genome]); Construct a locus expression by parsing a string or string expression. parse_variant(s[, reference_genome]); Construct a struct with a locus and alleles by parsing a string. parse_locus_interval(s[, reference_genome, ...]); Construct a locus interval expression by parsing a string or string expression. variant_str(*args); Create a variant colon-delimited string. call(*alleles[, phased]); Construct a call expression. unphased_diploid_gt_index_call(gt_index); Construct an unphased, diploid call from a genotype index. parse_call(s); Construct a call expression by parsing a string or string expression. downcode(c, i); Create a new call by setting all alleles other than i to ref. triangle(n); Returns the triangle number of n. is_snp(ref, alt); Returns True if the alleles constitute a single nucleotide polymorphism. is_mnp(ref, alt); Returns True if the alleles constitute a multiple nucleotide polymorphism. is_transition(ref, alt); Returns True if the alleles constitute a transition. is_transversion(ref, alt); Returns True if the alleles constitute a transversion. is_insertion(ref, alt); Returns True if the alleles constitute an insertion. is_deletion(ref, alt); Returns True if the alleles constitute a deletion. is_indel(ref, alt); Returns True if the alleles constitute an insertion or deletion. is_star(ref, alt); Returns True if the alleles constitute an upstream deletion. is_complex(ref, alt); Returns True if the alleles constitute a complex polymorphism. is_strand_ambiguous(ref, alt); Returns True if the alleles are strand ambiguous. is_valid_contig(contig[, reference_genome]); Returns True if contig is a valid contig n",MatchSource.WIKI,docs/0.2/functions/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/genetics.html
https://hail.is/docs/0.2/functions/genetics.html:11351,Availability,down,downcode,11351," of allele indices.; phased (bool) – If True, preserve the order of alleles. Returns:; CallExpression. hail.expr.functions.unphased_diploid_gt_index_call(gt_index)[source]; Construct an unphased, diploid call from a genotype index.; Examples; >>> hl.eval(hl.unphased_diploid_gt_index_call(4)); Call(alleles=[1, 2], phased=False). Parameters:; gt_index (int or Expression of type tint32) – Unphased, diploid genotype index. Returns:; CallExpression. hail.expr.functions.parse_call(s)[source]; Construct a call expression by parsing a string or string expression.; Examples; >>> hl.eval(hl.parse_call('0|2')); Call(alleles=[0, 2], phased=True). Notes; This method expects strings in the following format:. ploidy; Phased; Unphased. 0; |-; -. 1; |i; i. 2; i|j; i/j. 3; i|j|k; i/j/k. N; i|j|k|...|N; i/j/k/.../N. Parameters:; s (str or StringExpression) – String to parse. Returns:; CallExpression. hail.expr.functions.downcode(c, i)[source]; Create a new call by setting all alleles other than i to ref; Examples; Preserve the third allele and downcode all other alleles to reference.; >>> hl.eval(hl.downcode(hl.call(1, 2), 2)); Call(alleles=[0, 1], phased=False). >>> hl.eval(hl.downcode(hl.call(2, 2), 2)); Call(alleles=[1, 1], phased=False). >>> hl.eval(hl.downcode(hl.call(0, 1), 2)); Call(alleles=[0, 0], phased=False). Parameters:. c (CallExpression) – A call.; i (Expression of type tint32) – The index of the allele that will be sent to the alternate allele. All; other alleles will be downcoded to reference. Returns:; CallExpression. hail.expr.functions.triangle(n)[source]; Returns the triangle number of n.; Examples; >>> hl.eval(hl.triangle(3)); 6. Notes; The calculation is n * (n + 1) / 2. Parameters:; n (Expression of type tint32). Returns:; Expression of type tint32. hail.expr.functions.is_snp(ref, alt)[source]; Returns True if the alleles constitute a single nucleotide polymorphism.; Examples; >>> hl.eval(hl.is_snp('A', 'T')); True. Parameters:. ref (StringExpression) – Refe",MatchSource.WIKI,docs/0.2/functions/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/genetics.html
https://hail.is/docs/0.2/functions/genetics.html:11478,Availability,down,downcode,11478," of allele indices.; phased (bool) – If True, preserve the order of alleles. Returns:; CallExpression. hail.expr.functions.unphased_diploid_gt_index_call(gt_index)[source]; Construct an unphased, diploid call from a genotype index.; Examples; >>> hl.eval(hl.unphased_diploid_gt_index_call(4)); Call(alleles=[1, 2], phased=False). Parameters:; gt_index (int or Expression of type tint32) – Unphased, diploid genotype index. Returns:; CallExpression. hail.expr.functions.parse_call(s)[source]; Construct a call expression by parsing a string or string expression.; Examples; >>> hl.eval(hl.parse_call('0|2')); Call(alleles=[0, 2], phased=True). Notes; This method expects strings in the following format:. ploidy; Phased; Unphased. 0; |-; -. 1; |i; i. 2; i|j; i/j. 3; i|j|k; i/j/k. N; i|j|k|...|N; i/j/k/.../N. Parameters:; s (str or StringExpression) – String to parse. Returns:; CallExpression. hail.expr.functions.downcode(c, i)[source]; Create a new call by setting all alleles other than i to ref; Examples; Preserve the third allele and downcode all other alleles to reference.; >>> hl.eval(hl.downcode(hl.call(1, 2), 2)); Call(alleles=[0, 1], phased=False). >>> hl.eval(hl.downcode(hl.call(2, 2), 2)); Call(alleles=[1, 1], phased=False). >>> hl.eval(hl.downcode(hl.call(0, 1), 2)); Call(alleles=[0, 0], phased=False). Parameters:. c (CallExpression) – A call.; i (Expression of type tint32) – The index of the allele that will be sent to the alternate allele. All; other alleles will be downcoded to reference. Returns:; CallExpression. hail.expr.functions.triangle(n)[source]; Returns the triangle number of n.; Examples; >>> hl.eval(hl.triangle(3)); 6. Notes; The calculation is n * (n + 1) / 2. Parameters:; n (Expression of type tint32). Returns:; Expression of type tint32. hail.expr.functions.is_snp(ref, alt)[source]; Returns True if the alleles constitute a single nucleotide polymorphism.; Examples; >>> hl.eval(hl.is_snp('A', 'T')); True. Parameters:. ref (StringExpression) – Refe",MatchSource.WIKI,docs/0.2/functions/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/genetics.html
https://hail.is/docs/0.2/functions/genetics.html:11535,Availability,down,downcode,11535,".expr.functions.unphased_diploid_gt_index_call(gt_index)[source]; Construct an unphased, diploid call from a genotype index.; Examples; >>> hl.eval(hl.unphased_diploid_gt_index_call(4)); Call(alleles=[1, 2], phased=False). Parameters:; gt_index (int or Expression of type tint32) – Unphased, diploid genotype index. Returns:; CallExpression. hail.expr.functions.parse_call(s)[source]; Construct a call expression by parsing a string or string expression.; Examples; >>> hl.eval(hl.parse_call('0|2')); Call(alleles=[0, 2], phased=True). Notes; This method expects strings in the following format:. ploidy; Phased; Unphased. 0; |-; -. 1; |i; i. 2; i|j; i/j. 3; i|j|k; i/j/k. N; i|j|k|...|N; i/j/k/.../N. Parameters:; s (str or StringExpression) – String to parse. Returns:; CallExpression. hail.expr.functions.downcode(c, i)[source]; Create a new call by setting all alleles other than i to ref; Examples; Preserve the third allele and downcode all other alleles to reference.; >>> hl.eval(hl.downcode(hl.call(1, 2), 2)); Call(alleles=[0, 1], phased=False). >>> hl.eval(hl.downcode(hl.call(2, 2), 2)); Call(alleles=[1, 1], phased=False). >>> hl.eval(hl.downcode(hl.call(0, 1), 2)); Call(alleles=[0, 0], phased=False). Parameters:. c (CallExpression) – A call.; i (Expression of type tint32) – The index of the allele that will be sent to the alternate allele. All; other alleles will be downcoded to reference. Returns:; CallExpression. hail.expr.functions.triangle(n)[source]; Returns the triangle number of n.; Examples; >>> hl.eval(hl.triangle(3)); 6. Notes; The calculation is n * (n + 1) / 2. Parameters:; n (Expression of type tint32). Returns:; Expression of type tint32. hail.expr.functions.is_snp(ref, alt)[source]; Returns True if the alleles constitute a single nucleotide polymorphism.; Examples; >>> hl.eval(hl.is_snp('A', 'T')); True. Parameters:. ref (StringExpression) – Reference allele.; alt (StringExpression) – Alternate allele. Returns:; BooleanExpression. hail.expr.functions",MatchSource.WIKI,docs/0.2/functions/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/genetics.html
https://hail.is/docs/0.2/functions/genetics.html:11615,Availability,down,downcode,11615,"unphased, diploid call from a genotype index.; Examples; >>> hl.eval(hl.unphased_diploid_gt_index_call(4)); Call(alleles=[1, 2], phased=False). Parameters:; gt_index (int or Expression of type tint32) – Unphased, diploid genotype index. Returns:; CallExpression. hail.expr.functions.parse_call(s)[source]; Construct a call expression by parsing a string or string expression.; Examples; >>> hl.eval(hl.parse_call('0|2')); Call(alleles=[0, 2], phased=True). Notes; This method expects strings in the following format:. ploidy; Phased; Unphased. 0; |-; -. 1; |i; i. 2; i|j; i/j. 3; i|j|k; i/j/k. N; i|j|k|...|N; i/j/k/.../N. Parameters:; s (str or StringExpression) – String to parse. Returns:; CallExpression. hail.expr.functions.downcode(c, i)[source]; Create a new call by setting all alleles other than i to ref; Examples; Preserve the third allele and downcode all other alleles to reference.; >>> hl.eval(hl.downcode(hl.call(1, 2), 2)); Call(alleles=[0, 1], phased=False). >>> hl.eval(hl.downcode(hl.call(2, 2), 2)); Call(alleles=[1, 1], phased=False). >>> hl.eval(hl.downcode(hl.call(0, 1), 2)); Call(alleles=[0, 0], phased=False). Parameters:. c (CallExpression) – A call.; i (Expression of type tint32) – The index of the allele that will be sent to the alternate allele. All; other alleles will be downcoded to reference. Returns:; CallExpression. hail.expr.functions.triangle(n)[source]; Returns the triangle number of n.; Examples; >>> hl.eval(hl.triangle(3)); 6. Notes; The calculation is n * (n + 1) / 2. Parameters:; n (Expression of type tint32). Returns:; Expression of type tint32. hail.expr.functions.is_snp(ref, alt)[source]; Returns True if the alleles constitute a single nucleotide polymorphism.; Examples; >>> hl.eval(hl.is_snp('A', 'T')); True. Parameters:. ref (StringExpression) – Reference allele.; alt (StringExpression) – Alternate allele. Returns:; BooleanExpression. hail.expr.functions.is_mnp(ref, alt)[source]; Returns True if the alleles constitute a multiple nu",MatchSource.WIKI,docs/0.2/functions/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/genetics.html
https://hail.is/docs/0.2/functions/genetics.html:11695,Availability,down,downcode,11695,"_diploid_gt_index_call(4)); Call(alleles=[1, 2], phased=False). Parameters:; gt_index (int or Expression of type tint32) – Unphased, diploid genotype index. Returns:; CallExpression. hail.expr.functions.parse_call(s)[source]; Construct a call expression by parsing a string or string expression.; Examples; >>> hl.eval(hl.parse_call('0|2')); Call(alleles=[0, 2], phased=True). Notes; This method expects strings in the following format:. ploidy; Phased; Unphased. 0; |-; -. 1; |i; i. 2; i|j; i/j. 3; i|j|k; i/j/k. N; i|j|k|...|N; i/j/k/.../N. Parameters:; s (str or StringExpression) – String to parse. Returns:; CallExpression. hail.expr.functions.downcode(c, i)[source]; Create a new call by setting all alleles other than i to ref; Examples; Preserve the third allele and downcode all other alleles to reference.; >>> hl.eval(hl.downcode(hl.call(1, 2), 2)); Call(alleles=[0, 1], phased=False). >>> hl.eval(hl.downcode(hl.call(2, 2), 2)); Call(alleles=[1, 1], phased=False). >>> hl.eval(hl.downcode(hl.call(0, 1), 2)); Call(alleles=[0, 0], phased=False). Parameters:. c (CallExpression) – A call.; i (Expression of type tint32) – The index of the allele that will be sent to the alternate allele. All; other alleles will be downcoded to reference. Returns:; CallExpression. hail.expr.functions.triangle(n)[source]; Returns the triangle number of n.; Examples; >>> hl.eval(hl.triangle(3)); 6. Notes; The calculation is n * (n + 1) / 2. Parameters:; n (Expression of type tint32). Returns:; Expression of type tint32. hail.expr.functions.is_snp(ref, alt)[source]; Returns True if the alleles constitute a single nucleotide polymorphism.; Examples; >>> hl.eval(hl.is_snp('A', 'T')); True. Parameters:. ref (StringExpression) – Reference allele.; alt (StringExpression) – Alternate allele. Returns:; BooleanExpression. hail.expr.functions.is_mnp(ref, alt)[source]; Returns True if the alleles constitute a multiple nucleotide polymorphism.; Examples; >>> hl.eval(hl.is_mnp('AA', 'GT')); True. Para",MatchSource.WIKI,docs/0.2/functions/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/genetics.html
https://hail.is/docs/0.2/functions/genetics.html:11929,Availability,down,downcoded,11929,"; Construct a call expression by parsing a string or string expression.; Examples; >>> hl.eval(hl.parse_call('0|2')); Call(alleles=[0, 2], phased=True). Notes; This method expects strings in the following format:. ploidy; Phased; Unphased. 0; |-; -. 1; |i; i. 2; i|j; i/j. 3; i|j|k; i/j/k. N; i|j|k|...|N; i/j/k/.../N. Parameters:; s (str or StringExpression) – String to parse. Returns:; CallExpression. hail.expr.functions.downcode(c, i)[source]; Create a new call by setting all alleles other than i to ref; Examples; Preserve the third allele and downcode all other alleles to reference.; >>> hl.eval(hl.downcode(hl.call(1, 2), 2)); Call(alleles=[0, 1], phased=False). >>> hl.eval(hl.downcode(hl.call(2, 2), 2)); Call(alleles=[1, 1], phased=False). >>> hl.eval(hl.downcode(hl.call(0, 1), 2)); Call(alleles=[0, 0], phased=False). Parameters:. c (CallExpression) – A call.; i (Expression of type tint32) – The index of the allele that will be sent to the alternate allele. All; other alleles will be downcoded to reference. Returns:; CallExpression. hail.expr.functions.triangle(n)[source]; Returns the triangle number of n.; Examples; >>> hl.eval(hl.triangle(3)); 6. Notes; The calculation is n * (n + 1) / 2. Parameters:; n (Expression of type tint32). Returns:; Expression of type tint32. hail.expr.functions.is_snp(ref, alt)[source]; Returns True if the alleles constitute a single nucleotide polymorphism.; Examples; >>> hl.eval(hl.is_snp('A', 'T')); True. Parameters:. ref (StringExpression) – Reference allele.; alt (StringExpression) – Alternate allele. Returns:; BooleanExpression. hail.expr.functions.is_mnp(ref, alt)[source]; Returns True if the alleles constitute a multiple nucleotide polymorphism.; Examples; >>> hl.eval(hl.is_mnp('AA', 'GT')); True. Parameters:. ref (StringExpression) – Reference allele.; alt (StringExpression) – Alternate allele. Returns:; BooleanExpression. hail.expr.functions.is_transition(ref, alt)[source]; Returns True if the alleles constitute a transi",MatchSource.WIKI,docs/0.2/functions/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/genetics.html
https://hail.is/docs/0.2/functions/genetics.html:19679,Availability,avail,available,19679,"ore=0, after=0, reference_genome='default')[source]; Return the reference sequence at a given locus.; Examples; Return the reference allele for 'GRCh37' at the locus '1:45323':; >>> hl.eval(hl.get_sequence('1', 45323, reference_genome='GRCh37')) ; ""T"". Notes; This function requires reference genome has an attached; reference sequence. Use ReferenceGenome.add_sequence() to; load and attach a reference sequence to a reference genome.; Returns None if contig and position are not valid coordinates in; reference_genome. Parameters:. contig (Expression of type tstr) – Locus contig.; position (Expression of type tint32) – Locus position.; before (Expression of type tint32, optional) – Number of bases to include before the locus of interest. Truncates at; contig boundary.; after (Expression of type tint32, optional) – Number of bases to include after the locus of interest. Truncates at; contig boundary.; reference_genome (str or ReferenceGenome) – Reference genome to use. Must have a reference sequence available. Returns:; StringExpression. hail.expr.functions.mendel_error_code(locus, is_female, father, mother, child)[source]; Compute a Mendelian violation code for genotypes.; >>> father = hl.call(0, 0); >>> mother = hl.call(1, 1); >>> child1 = hl.call(0, 1) # consistent; >>> child2 = hl.call(0, 0) # Mendel error; >>> locus = hl.locus('2', 2000000). >>> hl.eval(hl.mendel_error_code(locus, True, father, mother, child1)); None. >>> hl.eval(hl.mendel_error_code(locus, True, father, mother, child2)); 7. Note; Ignores call phasing, and assumes diploid and biallelic. Haploid calls for; hemiploid samples on sex chromosomes also are acceptable input. Notes; In the table below, the copy state of a locus with respect to a trio is; defined as follows, where PAR is the pseudoautosomal region (PAR) of X and Y; defined by the reference genome and the autosome is defined by; LocusExpression.in_autosome():. Auto – in autosome or in PAR, or in non-PAR of X and female child; HemiX – in non",MatchSource.WIKI,docs/0.2/functions/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/genetics.html
https://hail.is/docs/0.2/functions/genetics.html:19991,Availability,error,error,19991,"nce sequence. Use ReferenceGenome.add_sequence() to; load and attach a reference sequence to a reference genome.; Returns None if contig and position are not valid coordinates in; reference_genome. Parameters:. contig (Expression of type tstr) – Locus contig.; position (Expression of type tint32) – Locus position.; before (Expression of type tint32, optional) – Number of bases to include before the locus of interest. Truncates at; contig boundary.; after (Expression of type tint32, optional) – Number of bases to include after the locus of interest. Truncates at; contig boundary.; reference_genome (str or ReferenceGenome) – Reference genome to use. Must have a reference sequence available. Returns:; StringExpression. hail.expr.functions.mendel_error_code(locus, is_female, father, mother, child)[source]; Compute a Mendelian violation code for genotypes.; >>> father = hl.call(0, 0); >>> mother = hl.call(1, 1); >>> child1 = hl.call(0, 1) # consistent; >>> child2 = hl.call(0, 0) # Mendel error; >>> locus = hl.locus('2', 2000000). >>> hl.eval(hl.mendel_error_code(locus, True, father, mother, child1)); None. >>> hl.eval(hl.mendel_error_code(locus, True, father, mother, child2)); 7. Note; Ignores call phasing, and assumes diploid and biallelic. Haploid calls for; hemiploid samples on sex chromosomes also are acceptable input. Notes; In the table below, the copy state of a locus with respect to a trio is; defined as follows, where PAR is the pseudoautosomal region (PAR) of X and Y; defined by the reference genome and the autosome is defined by; LocusExpression.in_autosome():. Auto – in autosome or in PAR, or in non-PAR of X and female child; HemiX – in non-PAR of X and male child; HemiY – in non-PAR of Y and male child. Any refers to the set { HomRef, Het, HomVar, NoCall } and ~; denotes complement in this set. Code; Dad; Mom; Kid; Copy State; Implicated. 1; HomVar; HomVar; Het; Auto; Dad, Mom, Kid. 2; HomRef; HomRef; Het; Auto; Dad, Mom, Kid. 3; HomRef; ~HomRef; HomVar; Au",MatchSource.WIKI,docs/0.2/functions/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/genetics.html
https://hail.is/docs/0.2/functions/genetics.html:24707,Deployability,update,updated,24707," filter out missing values. Parameters:. x (Expression of type tlocus or tinterval of tlocus) – Locus or locus interval to lift over.; dest_reference_genome (str or ReferenceGenome) – Reference genome to convert to.; min_match (float) – Minimum ratio of bases that must remap.; include_strand (bool) – If True, output the result as a StructExpression with the first field result being; the locus or locus interval and the second field is_negative_strand is a boolean indicating; whether the locus or locus interval has been mapped to the negative strand of the destination; reference genome. Otherwise, output the converted locus or locus interval. Returns:; Expression – A locus or locus interval converted to dest_reference_genome. hail.expr.functions.min_rep(locus, alleles)[source]; Computes the minimal representation of a (locus, alleles) polymorphism.; Examples; >>> hl.eval(hl.min_rep(hl.locus('1', 100000), ['TAA', 'TA'])); Struct(locus=Locus(contig=1, position=100000, reference_genome=GRCh37), alleles=['TA', 'T']). >>> hl.eval(hl.min_rep(hl.locus('1', 100000), ['AATAA', 'AACAA'])); Struct(locus=Locus(contig=1, position=100002, reference_genome=GRCh37), alleles=['T', 'C']). Notes; Computing the minimal representation can cause the locus shift right (the; position can increase). Parameters:. locus (LocusExpression); alleles (ArrayExpression of type tstr). Returns:; StructExpression – A tstruct expression with two fields, locus; (LocusExpression) and alleles; (ArrayExpression of type tstr). hail.expr.functions.reverse_complement(s, rna=False)[source]; Reverses the string and translates base pairs into their complements; .. rubric:: Examples; >>> bases = hl.literal('NNGATTACA'); >>> hl.eval(hl.reverse_complement(bases)); 'TGTAATCNN'. Parameters:. s (StringExpression) – Base string.; rna (bool) – If True, pair adenine (A) with uracil (U) instead of thymine (T). Returns:; StringExpression. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/functions/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/genetics.html
https://hail.is/docs/0.2/functions/genetics.html:1871,Modifiability,polymorphi,polymorphism,1871,"_position(global_pos[, ...]); Constructs a locus expression from a global position and a reference genome. locus_interval(contig, start, end[, ...]); Construct a locus interval expression. parse_locus(s[, reference_genome]); Construct a locus expression by parsing a string or string expression. parse_variant(s[, reference_genome]); Construct a struct with a locus and alleles by parsing a string. parse_locus_interval(s[, reference_genome, ...]); Construct a locus interval expression by parsing a string or string expression. variant_str(*args); Create a variant colon-delimited string. call(*alleles[, phased]); Construct a call expression. unphased_diploid_gt_index_call(gt_index); Construct an unphased, diploid call from a genotype index. parse_call(s); Construct a call expression by parsing a string or string expression. downcode(c, i); Create a new call by setting all alleles other than i to ref. triangle(n); Returns the triangle number of n. is_snp(ref, alt); Returns True if the alleles constitute a single nucleotide polymorphism. is_mnp(ref, alt); Returns True if the alleles constitute a multiple nucleotide polymorphism. is_transition(ref, alt); Returns True if the alleles constitute a transition. is_transversion(ref, alt); Returns True if the alleles constitute a transversion. is_insertion(ref, alt); Returns True if the alleles constitute an insertion. is_deletion(ref, alt); Returns True if the alleles constitute a deletion. is_indel(ref, alt); Returns True if the alleles constitute an insertion or deletion. is_star(ref, alt); Returns True if the alleles constitute an upstream deletion. is_complex(ref, alt); Returns True if the alleles constitute a complex polymorphism. is_strand_ambiguous(ref, alt); Returns True if the alleles are strand ambiguous. is_valid_contig(contig[, reference_genome]); Returns True if contig is a valid contig name in reference_genome. is_valid_locus(contig, position[, ...]); Returns True if contig and position is a valid site in reference_g",MatchSource.WIKI,docs/0.2/functions/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/genetics.html
https://hail.is/docs/0.2/functions/genetics.html:1964,Modifiability,polymorphi,polymorphism,1964,"erence genome. locus_interval(contig, start, end[, ...]); Construct a locus interval expression. parse_locus(s[, reference_genome]); Construct a locus expression by parsing a string or string expression. parse_variant(s[, reference_genome]); Construct a struct with a locus and alleles by parsing a string. parse_locus_interval(s[, reference_genome, ...]); Construct a locus interval expression by parsing a string or string expression. variant_str(*args); Create a variant colon-delimited string. call(*alleles[, phased]); Construct a call expression. unphased_diploid_gt_index_call(gt_index); Construct an unphased, diploid call from a genotype index. parse_call(s); Construct a call expression by parsing a string or string expression. downcode(c, i); Create a new call by setting all alleles other than i to ref. triangle(n); Returns the triangle number of n. is_snp(ref, alt); Returns True if the alleles constitute a single nucleotide polymorphism. is_mnp(ref, alt); Returns True if the alleles constitute a multiple nucleotide polymorphism. is_transition(ref, alt); Returns True if the alleles constitute a transition. is_transversion(ref, alt); Returns True if the alleles constitute a transversion. is_insertion(ref, alt); Returns True if the alleles constitute an insertion. is_deletion(ref, alt); Returns True if the alleles constitute a deletion. is_indel(ref, alt); Returns True if the alleles constitute an insertion or deletion. is_star(ref, alt); Returns True if the alleles constitute an upstream deletion. is_complex(ref, alt); Returns True if the alleles constitute a complex polymorphism. is_strand_ambiguous(ref, alt); Returns True if the alleles are strand ambiguous. is_valid_contig(contig[, reference_genome]); Returns True if contig is a valid contig name in reference_genome. is_valid_locus(contig, position[, ...]); Returns True if contig and position is a valid site in reference_genome. contig_length(contig[, reference_genome]); Returns the length of contig in reference_",MatchSource.WIKI,docs/0.2/functions/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/genetics.html
https://hail.is/docs/0.2/functions/genetics.html:2525,Modifiability,polymorphi,polymorphism,2525,"loid_gt_index_call(gt_index); Construct an unphased, diploid call from a genotype index. parse_call(s); Construct a call expression by parsing a string or string expression. downcode(c, i); Create a new call by setting all alleles other than i to ref. triangle(n); Returns the triangle number of n. is_snp(ref, alt); Returns True if the alleles constitute a single nucleotide polymorphism. is_mnp(ref, alt); Returns True if the alleles constitute a multiple nucleotide polymorphism. is_transition(ref, alt); Returns True if the alleles constitute a transition. is_transversion(ref, alt); Returns True if the alleles constitute a transversion. is_insertion(ref, alt); Returns True if the alleles constitute an insertion. is_deletion(ref, alt); Returns True if the alleles constitute a deletion. is_indel(ref, alt); Returns True if the alleles constitute an insertion or deletion. is_star(ref, alt); Returns True if the alleles constitute an upstream deletion. is_complex(ref, alt); Returns True if the alleles constitute a complex polymorphism. is_strand_ambiguous(ref, alt); Returns True if the alleles are strand ambiguous. is_valid_contig(contig[, reference_genome]); Returns True if contig is a valid contig name in reference_genome. is_valid_locus(contig, position[, ...]); Returns True if contig and position is a valid site in reference_genome. contig_length(contig[, reference_genome]); Returns the length of contig in reference_genome. allele_type(ref, alt); Returns the type of the polymorphism as a string. numeric_allele_type(ref, alt); Returns the type of the polymorphism as an integer. pl_dosage(pl); Return expected genotype dosage from array of Phred-scaled genotype likelihoods with uniform prior. gp_dosage(gp); Return expected genotype dosage from array of genotype probabilities. get_sequence(contig, position[, before, ...]); Return the reference sequence at a given locus. mendel_error_code(locus, is_female, father, ...); Compute a Mendelian violation code for genotypes. liftov",MatchSource.WIKI,docs/0.2/functions/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/genetics.html
https://hail.is/docs/0.2/functions/genetics.html:2986,Modifiability,polymorphi,polymorphism,2986,"sm. is_transition(ref, alt); Returns True if the alleles constitute a transition. is_transversion(ref, alt); Returns True if the alleles constitute a transversion. is_insertion(ref, alt); Returns True if the alleles constitute an insertion. is_deletion(ref, alt); Returns True if the alleles constitute a deletion. is_indel(ref, alt); Returns True if the alleles constitute an insertion or deletion. is_star(ref, alt); Returns True if the alleles constitute an upstream deletion. is_complex(ref, alt); Returns True if the alleles constitute a complex polymorphism. is_strand_ambiguous(ref, alt); Returns True if the alleles are strand ambiguous. is_valid_contig(contig[, reference_genome]); Returns True if contig is a valid contig name in reference_genome. is_valid_locus(contig, position[, ...]); Returns True if contig and position is a valid site in reference_genome. contig_length(contig[, reference_genome]); Returns the length of contig in reference_genome. allele_type(ref, alt); Returns the type of the polymorphism as a string. numeric_allele_type(ref, alt); Returns the type of the polymorphism as an integer. pl_dosage(pl); Return expected genotype dosage from array of Phred-scaled genotype likelihoods with uniform prior. gp_dosage(gp); Return expected genotype dosage from array of genotype probabilities. get_sequence(contig, position[, before, ...]); Return the reference sequence at a given locus. mendel_error_code(locus, is_female, father, ...); Compute a Mendelian violation code for genotypes. liftover(x, dest_reference_genome[, ...]); Lift over coordinates to a different reference genome. min_rep(locus, alleles); Computes the minimal representation of a (locus, alleles) polymorphism. reverse_complement(s[, rna]); Reverses the string and translates base pairs into their complements . hail.expr.functions.locus(contig, pos, reference_genome='default')[source]; Construct a locus expression from a chromosome and position.; Examples; >>> hl.eval(hl.locus(""1"", 10000, referen",MatchSource.WIKI,docs/0.2/functions/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/genetics.html
https://hail.is/docs/0.2/functions/genetics.html:3067,Modifiability,polymorphi,polymorphism,3067,"on. is_transversion(ref, alt); Returns True if the alleles constitute a transversion. is_insertion(ref, alt); Returns True if the alleles constitute an insertion. is_deletion(ref, alt); Returns True if the alleles constitute a deletion. is_indel(ref, alt); Returns True if the alleles constitute an insertion or deletion. is_star(ref, alt); Returns True if the alleles constitute an upstream deletion. is_complex(ref, alt); Returns True if the alleles constitute a complex polymorphism. is_strand_ambiguous(ref, alt); Returns True if the alleles are strand ambiguous. is_valid_contig(contig[, reference_genome]); Returns True if contig is a valid contig name in reference_genome. is_valid_locus(contig, position[, ...]); Returns True if contig and position is a valid site in reference_genome. contig_length(contig[, reference_genome]); Returns the length of contig in reference_genome. allele_type(ref, alt); Returns the type of the polymorphism as a string. numeric_allele_type(ref, alt); Returns the type of the polymorphism as an integer. pl_dosage(pl); Return expected genotype dosage from array of Phred-scaled genotype likelihoods with uniform prior. gp_dosage(gp); Return expected genotype dosage from array of genotype probabilities. get_sequence(contig, position[, before, ...]); Return the reference sequence at a given locus. mendel_error_code(locus, is_female, father, ...); Compute a Mendelian violation code for genotypes. liftover(x, dest_reference_genome[, ...]); Lift over coordinates to a different reference genome. min_rep(locus, alleles); Computes the minimal representation of a (locus, alleles) polymorphism. reverse_complement(s[, rna]); Reverses the string and translates base pairs into their complements . hail.expr.functions.locus(contig, pos, reference_genome='default')[source]; Construct a locus expression from a chromosome and position.; Examples; >>> hl.eval(hl.locus(""1"", 10000, reference_genome='GRCh37')); Locus(contig=1, position=10000, reference_genome=GRCh37)",MatchSource.WIKI,docs/0.2/functions/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/genetics.html
https://hail.is/docs/0.2/functions/genetics.html:3671,Modifiability,polymorphi,polymorphism,3671,"(contig[, reference_genome]); Returns True if contig is a valid contig name in reference_genome. is_valid_locus(contig, position[, ...]); Returns True if contig and position is a valid site in reference_genome. contig_length(contig[, reference_genome]); Returns the length of contig in reference_genome. allele_type(ref, alt); Returns the type of the polymorphism as a string. numeric_allele_type(ref, alt); Returns the type of the polymorphism as an integer. pl_dosage(pl); Return expected genotype dosage from array of Phred-scaled genotype likelihoods with uniform prior. gp_dosage(gp); Return expected genotype dosage from array of genotype probabilities. get_sequence(contig, position[, before, ...]); Return the reference sequence at a given locus. mendel_error_code(locus, is_female, father, ...); Compute a Mendelian violation code for genotypes. liftover(x, dest_reference_genome[, ...]); Lift over coordinates to a different reference genome. min_rep(locus, alleles); Computes the minimal representation of a (locus, alleles) polymorphism. reverse_complement(s[, rna]); Reverses the string and translates base pairs into their complements . hail.expr.functions.locus(contig, pos, reference_genome='default')[source]; Construct a locus expression from a chromosome and position.; Examples; >>> hl.eval(hl.locus(""1"", 10000, reference_genome='GRCh37')); Locus(contig=1, position=10000, reference_genome=GRCh37). Parameters:. contig (str or StringExpression) – Chromosome.; pos (int or Expression of type tint32) – Base position along the chromosome.; reference_genome (str or ReferenceGenome) – Reference genome to use. Returns:; LocusExpression. hail.expr.functions.locus_from_global_position(global_pos, reference_genome='default')[source]; Constructs a locus expression from a global position and a reference genome.; The inverse of LocusExpression.global_position().; Examples; >>> hl.eval(hl.locus_from_global_position(0)); Locus(contig=1, position=1, reference_genome=GRCh37). >>> hl.ev",MatchSource.WIKI,docs/0.2/functions/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/genetics.html
https://hail.is/docs/0.2/functions/genetics.html:10370,Modifiability,variab,variable-length,10370,"tr or StringExpression) – String to parse.; reference_genome (str or hail.genetics.ReferenceGenome) – Reference genome to use.; invalid_missing (BooleanExpression) – If True, invalid intervals are set to NA rather than causing an exception. Returns:; IntervalExpression. hail.expr.functions.variant_str(*args)[source]; Create a variant colon-delimited string. Parameters:; args – Arguments (see notes). Returns:; StringExpression. Notes; Expects either one argument of type; struct{locus: locus<RG>, alleles: array<str>, or two arguments of type; locus<RG> and array<str>. The function returns a string of the form; CHR:POS:REF:ALT1,ALT2,...ALTN; e.g.; 1:1:A:T; 16:250125:AAA:A,CAA. Examples; >>> hl.eval(hl.variant_str(hl.locus('1', 10000), ['A', 'T', 'C'])); '1:10000:A:T,C'. hail.expr.functions.call(*alleles, phased=False)[source]; Construct a call expression.; Examples; >>> hl.eval(hl.call(1, 0)); Call(alleles=[0, 1], phased=False). Parameters:. alleles (variable-length args of int or Expression of type tint32) – List of allele indices.; phased (bool) – If True, preserve the order of alleles. Returns:; CallExpression. hail.expr.functions.unphased_diploid_gt_index_call(gt_index)[source]; Construct an unphased, diploid call from a genotype index.; Examples; >>> hl.eval(hl.unphased_diploid_gt_index_call(4)); Call(alleles=[1, 2], phased=False). Parameters:; gt_index (int or Expression of type tint32) – Unphased, diploid genotype index. Returns:; CallExpression. hail.expr.functions.parse_call(s)[source]; Construct a call expression by parsing a string or string expression.; Examples; >>> hl.eval(hl.parse_call('0|2')); Call(alleles=[0, 2], phased=True). Notes; This method expects strings in the following format:. ploidy; Phased; Unphased. 0; |-; -. 1; |i; i. 2; i|j; i/j. 3; i|j|k; i/j/k. N; i|j|k|...|N; i/j/k/.../N. Parameters:; s (str or StringExpression) – String to parse. Returns:; CallExpression. hail.expr.functions.downcode(c, i)[source]; Create a new call by setting al",MatchSource.WIKI,docs/0.2/functions/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/genetics.html
https://hail.is/docs/0.2/functions/genetics.html:12328,Modifiability,polymorphi,polymorphism,12328,"ing to parse. Returns:; CallExpression. hail.expr.functions.downcode(c, i)[source]; Create a new call by setting all alleles other than i to ref; Examples; Preserve the third allele and downcode all other alleles to reference.; >>> hl.eval(hl.downcode(hl.call(1, 2), 2)); Call(alleles=[0, 1], phased=False). >>> hl.eval(hl.downcode(hl.call(2, 2), 2)); Call(alleles=[1, 1], phased=False). >>> hl.eval(hl.downcode(hl.call(0, 1), 2)); Call(alleles=[0, 0], phased=False). Parameters:. c (CallExpression) – A call.; i (Expression of type tint32) – The index of the allele that will be sent to the alternate allele. All; other alleles will be downcoded to reference. Returns:; CallExpression. hail.expr.functions.triangle(n)[source]; Returns the triangle number of n.; Examples; >>> hl.eval(hl.triangle(3)); 6. Notes; The calculation is n * (n + 1) / 2. Parameters:; n (Expression of type tint32). Returns:; Expression of type tint32. hail.expr.functions.is_snp(ref, alt)[source]; Returns True if the alleles constitute a single nucleotide polymorphism.; Examples; >>> hl.eval(hl.is_snp('A', 'T')); True. Parameters:. ref (StringExpression) – Reference allele.; alt (StringExpression) – Alternate allele. Returns:; BooleanExpression. hail.expr.functions.is_mnp(ref, alt)[source]; Returns True if the alleles constitute a multiple nucleotide polymorphism.; Examples; >>> hl.eval(hl.is_mnp('AA', 'GT')); True. Parameters:. ref (StringExpression) – Reference allele.; alt (StringExpression) – Alternate allele. Returns:; BooleanExpression. hail.expr.functions.is_transition(ref, alt)[source]; Returns True if the alleles constitute a transition.; Examples; >>> hl.eval(hl.is_transition('A', 'T')); False. >>> hl.eval(hl.is_transition('AAA', 'AGA')); True. Parameters:. ref (StringExpression) – Reference allele.; alt (StringExpression) – Alternate allele. Returns:; BooleanExpression. hail.expr.functions.is_transversion(ref, alt)[source]; Returns True if the alleles constitute a transversion.; Examples",MatchSource.WIKI,docs/0.2/functions/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/genetics.html
https://hail.is/docs/0.2/functions/genetics.html:12630,Modifiability,polymorphi,polymorphism,12630,"False). >>> hl.eval(hl.downcode(hl.call(2, 2), 2)); Call(alleles=[1, 1], phased=False). >>> hl.eval(hl.downcode(hl.call(0, 1), 2)); Call(alleles=[0, 0], phased=False). Parameters:. c (CallExpression) – A call.; i (Expression of type tint32) – The index of the allele that will be sent to the alternate allele. All; other alleles will be downcoded to reference. Returns:; CallExpression. hail.expr.functions.triangle(n)[source]; Returns the triangle number of n.; Examples; >>> hl.eval(hl.triangle(3)); 6. Notes; The calculation is n * (n + 1) / 2. Parameters:; n (Expression of type tint32). Returns:; Expression of type tint32. hail.expr.functions.is_snp(ref, alt)[source]; Returns True if the alleles constitute a single nucleotide polymorphism.; Examples; >>> hl.eval(hl.is_snp('A', 'T')); True. Parameters:. ref (StringExpression) – Reference allele.; alt (StringExpression) – Alternate allele. Returns:; BooleanExpression. hail.expr.functions.is_mnp(ref, alt)[source]; Returns True if the alleles constitute a multiple nucleotide polymorphism.; Examples; >>> hl.eval(hl.is_mnp('AA', 'GT')); True. Parameters:. ref (StringExpression) – Reference allele.; alt (StringExpression) – Alternate allele. Returns:; BooleanExpression. hail.expr.functions.is_transition(ref, alt)[source]; Returns True if the alleles constitute a transition.; Examples; >>> hl.eval(hl.is_transition('A', 'T')); False. >>> hl.eval(hl.is_transition('AAA', 'AGA')); True. Parameters:. ref (StringExpression) – Reference allele.; alt (StringExpression) – Alternate allele. Returns:; BooleanExpression. hail.expr.functions.is_transversion(ref, alt)[source]; Returns True if the alleles constitute a transversion.; Examples; >>> hl.eval(hl.is_transversion('A', 'T')); True. >>> hl.eval(hl.is_transversion('AAA', 'AGA')); False. Parameters:. ref (StringExpression) – Reference allele.; alt (StringExpression) – Alternate allele. Returns:; BooleanExpression. hail.expr.functions.is_insertion(ref, alt)[source]; Returns True i",MatchSource.WIKI,docs/0.2/functions/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/genetics.html
https://hail.is/docs/0.2/functions/genetics.html:14798,Modifiability,polymorphi,polymorphism,14798,"ssion) – Alternate allele. Returns:; BooleanExpression. hail.expr.functions.is_deletion(ref, alt)[source]; Returns True if the alleles constitute a deletion.; Examples; >>> hl.eval(hl.is_deletion('ATT', 'A')); True. Parameters:. ref (StringExpression) – Reference allele.; alt (StringExpression) – Alternate allele. Returns:; BooleanExpression. hail.expr.functions.is_indel(ref, alt)[source]; Returns True if the alleles constitute an insertion or deletion.; Examples; >>> hl.eval(hl.is_indel('ATT', 'A')); True. Parameters:. ref (StringExpression) – Reference allele.; alt (StringExpression) – Alternate allele. Returns:; BooleanExpression. hail.expr.functions.is_star(ref, alt)[source]; Returns True if the alleles constitute an upstream deletion.; Examples; >>> hl.eval(hl.is_star('A', '*')); True. Parameters:. ref (StringExpression) – Reference allele.; alt (StringExpression) – Alternate allele. Returns:; BooleanExpression. hail.expr.functions.is_complex(ref, alt)[source]; Returns True if the alleles constitute a complex polymorphism.; Examples; >>> hl.eval(hl.is_complex('ATT', 'GCAC')); True. Parameters:. ref (StringExpression) – Reference allele.; alt (StringExpression) – Alternate allele. Returns:; BooleanExpression. hail.expr.functions.is_strand_ambiguous(ref, alt)[source]; Returns True if the alleles are strand ambiguous.; Strand ambiguous allele pairs are A/T, T/A,; C/G, and G/C where the first allele is ref; and the second allele is alt.; Examples; >>> hl.eval(hl.is_strand_ambiguous('A', 'T')); True. Parameters:. ref (StringExpression) – Reference allele.; alt (StringExpression) – Alternate allele. Returns:; BooleanExpression. hail.expr.functions.is_valid_contig(contig, reference_genome='default')[source]; Returns True if contig is a valid contig name in reference_genome.; Examples; >>> hl.eval(hl.is_valid_contig('1', reference_genome='GRCh37')); True. >>> hl.eval(hl.is_valid_contig('chr1', reference_genome='GRCh37')); False. Parameters:. contig (Expression of ",MatchSource.WIKI,docs/0.2/functions/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/genetics.html
https://hail.is/docs/0.2/functions/genetics.html:16713,Modifiability,polymorphi,polymorphism,16713,"erence_genome='GRCh37')); False. Parameters:. contig (Expression of type tstr); reference_genome (str or ReferenceGenome). Returns:; BooleanExpression. hail.expr.functions.is_valid_locus(contig, position, reference_genome='default')[source]; Returns True if contig and position is a valid site in reference_genome.; Examples; >>> hl.eval(hl.is_valid_locus('1', 324254, 'GRCh37')); True. >>> hl.eval(hl.is_valid_locus('chr1', 324254, 'GRCh37')); False. Parameters:. contig (Expression of type tstr); position (Expression of type tint); reference_genome (str or ReferenceGenome). Returns:; BooleanExpression. hail.expr.functions.contig_length(contig, reference_genome='default')[source]; Returns the length of contig in reference_genome.; Examples; >>> hl.eval(hl.contig_length('5', reference_genome='GRCh37')); 180915260. Parameters:. contig (Expression of type tstr); reference_genome (str or ReferenceGenome). Returns:; Int32Expression. hail.expr.functions.allele_type(ref, alt)[source]; Returns the type of the polymorphism as a string.; Examples; >>> hl.eval(hl.allele_type('A', 'T')); 'SNP'. >>> hl.eval(hl.allele_type('ATT', 'A')); 'Deletion'. Notes. The possible return values are:; ""SNP""; ""MNP""; ""Insertion""; ""Deletion""; ""Complex""; ""Star""; ""Symbolic""; ""Unknown"". Parameters:. ref (StringExpression) – Reference allele.; alt (StringExpression) – Alternate allele. Returns:; StringExpression. hail.expr.functions.numeric_allele_type(ref, alt)[source]; Returns the type of the polymorphism as an integer. The value returned; is the integer value of AlleleType representing that kind of; polymorphism.; Examples; >>> hl.eval(hl.numeric_allele_type('A', 'T')) == AlleleType.SNP; True. Notes; The values of AlleleType are not stable and thus should not be; relied upon across hail versions. hail.expr.functions.pl_dosage(pl)[source]; Return expected genotype dosage from array of Phred-scaled genotype; likelihoods with uniform prior. Only defined for bi-allelic variants. The; pl argument must ",MatchSource.WIKI,docs/0.2/functions/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/genetics.html
https://hail.is/docs/0.2/functions/genetics.html:17182,Modifiability,polymorphi,polymorphism,17182,"contig (Expression of type tstr); position (Expression of type tint); reference_genome (str or ReferenceGenome). Returns:; BooleanExpression. hail.expr.functions.contig_length(contig, reference_genome='default')[source]; Returns the length of contig in reference_genome.; Examples; >>> hl.eval(hl.contig_length('5', reference_genome='GRCh37')); 180915260. Parameters:. contig (Expression of type tstr); reference_genome (str or ReferenceGenome). Returns:; Int32Expression. hail.expr.functions.allele_type(ref, alt)[source]; Returns the type of the polymorphism as a string.; Examples; >>> hl.eval(hl.allele_type('A', 'T')); 'SNP'. >>> hl.eval(hl.allele_type('ATT', 'A')); 'Deletion'. Notes. The possible return values are:; ""SNP""; ""MNP""; ""Insertion""; ""Deletion""; ""Complex""; ""Star""; ""Symbolic""; ""Unknown"". Parameters:. ref (StringExpression) – Reference allele.; alt (StringExpression) – Alternate allele. Returns:; StringExpression. hail.expr.functions.numeric_allele_type(ref, alt)[source]; Returns the type of the polymorphism as an integer. The value returned; is the integer value of AlleleType representing that kind of; polymorphism.; Examples; >>> hl.eval(hl.numeric_allele_type('A', 'T')) == AlleleType.SNP; True. Notes; The values of AlleleType are not stable and thus should not be; relied upon across hail versions. hail.expr.functions.pl_dosage(pl)[source]; Return expected genotype dosage from array of Phred-scaled genotype; likelihoods with uniform prior. Only defined for bi-allelic variants. The; pl argument must be length 3.; For a PL array [a, b, c], let:. \[a^\prime = 10^{-a/10} \\; b^\prime = 10^{-b/10} \\; c^\prime = 10^{-c/10} \\\]; The genotype dosage is given by:. \[\frac{b^\prime + 2 c^\prime}; {a^\prime + b^\prime +c ^\prime}\]; Examples; >>> hl.eval(hl.pl_dosage([5, 10, 100])); 0.24025307377482674. Parameters:; pl (ArrayNumericExpression of type tint32) – Length 3 array of bi-allelic Phred-scaled genotype likelihoods. Returns:; Expression of type tfloat64. hai",MatchSource.WIKI,docs/0.2/functions/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/genetics.html
https://hail.is/docs/0.2/functions/genetics.html:17292,Modifiability,polymorphi,polymorphism,17292," ReferenceGenome). Returns:; BooleanExpression. hail.expr.functions.contig_length(contig, reference_genome='default')[source]; Returns the length of contig in reference_genome.; Examples; >>> hl.eval(hl.contig_length('5', reference_genome='GRCh37')); 180915260. Parameters:. contig (Expression of type tstr); reference_genome (str or ReferenceGenome). Returns:; Int32Expression. hail.expr.functions.allele_type(ref, alt)[source]; Returns the type of the polymorphism as a string.; Examples; >>> hl.eval(hl.allele_type('A', 'T')); 'SNP'. >>> hl.eval(hl.allele_type('ATT', 'A')); 'Deletion'. Notes. The possible return values are:; ""SNP""; ""MNP""; ""Insertion""; ""Deletion""; ""Complex""; ""Star""; ""Symbolic""; ""Unknown"". Parameters:. ref (StringExpression) – Reference allele.; alt (StringExpression) – Alternate allele. Returns:; StringExpression. hail.expr.functions.numeric_allele_type(ref, alt)[source]; Returns the type of the polymorphism as an integer. The value returned; is the integer value of AlleleType representing that kind of; polymorphism.; Examples; >>> hl.eval(hl.numeric_allele_type('A', 'T')) == AlleleType.SNP; True. Notes; The values of AlleleType are not stable and thus should not be; relied upon across hail versions. hail.expr.functions.pl_dosage(pl)[source]; Return expected genotype dosage from array of Phred-scaled genotype; likelihoods with uniform prior. Only defined for bi-allelic variants. The; pl argument must be length 3.; For a PL array [a, b, c], let:. \[a^\prime = 10^{-a/10} \\; b^\prime = 10^{-b/10} \\; c^\prime = 10^{-c/10} \\\]; The genotype dosage is given by:. \[\frac{b^\prime + 2 c^\prime}; {a^\prime + b^\prime +c ^\prime}\]; Examples; >>> hl.eval(hl.pl_dosage([5, 10, 100])); 0.24025307377482674. Parameters:; pl (ArrayNumericExpression of type tint32) – Length 3 array of bi-allelic Phred-scaled genotype likelihoods. Returns:; Expression of type tfloat64. hail.expr.functions.gp_dosage(gp)[source]; Return expected genotype dosage from array of genoty",MatchSource.WIKI,docs/0.2/functions/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/genetics.html
https://hail.is/docs/0.2/functions/genetics.html:23581,Modifiability,polymorphi,polymorphism,23581,"er() to; load and attach a chain file to a reference genome.; Returns None if x could not be converted. Warning; Before using the result of liftover() as a new row key or column; key, be sure to filter out missing values. Parameters:. x (Expression of type tlocus or tinterval of tlocus) – Locus or locus interval to lift over.; dest_reference_genome (str or ReferenceGenome) – Reference genome to convert to.; min_match (float) – Minimum ratio of bases that must remap.; include_strand (bool) – If True, output the result as a StructExpression with the first field result being; the locus or locus interval and the second field is_negative_strand is a boolean indicating; whether the locus or locus interval has been mapped to the negative strand of the destination; reference genome. Otherwise, output the converted locus or locus interval. Returns:; Expression – A locus or locus interval converted to dest_reference_genome. hail.expr.functions.min_rep(locus, alleles)[source]; Computes the minimal representation of a (locus, alleles) polymorphism.; Examples; >>> hl.eval(hl.min_rep(hl.locus('1', 100000), ['TAA', 'TA'])); Struct(locus=Locus(contig=1, position=100000, reference_genome=GRCh37), alleles=['TA', 'T']). >>> hl.eval(hl.min_rep(hl.locus('1', 100000), ['AATAA', 'AACAA'])); Struct(locus=Locus(contig=1, position=100002, reference_genome=GRCh37), alleles=['T', 'C']). Notes; Computing the minimal representation can cause the locus shift right (the; position can increase). Parameters:. locus (LocusExpression); alleles (ArrayExpression of type tstr). Returns:; StructExpression – A tstruct expression with two fields, locus; (LocusExpression) and alleles; (ArrayExpression of type tstr). hail.expr.functions.reverse_complement(s, rna=False)[source]; Reverses the string and translates base pairs into their complements; .. rubric:: Examples; >>> bases = hl.literal('NNGATTACA'); >>> hl.eval(hl.reverse_complement(bases)); 'TGTAATCNN'. Parameters:. s (StringExpression) – Base string.;",MatchSource.WIKI,docs/0.2/functions/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/genetics.html
https://hail.is/docs/0.2/functions/genetics.html:19045,Performance,load,load,19045,"3 array of bi-allelic Phred-scaled genotype likelihoods. Returns:; Expression of type tfloat64. hail.expr.functions.gp_dosage(gp)[source]; Return expected genotype dosage from array of genotype probabilities.; Examples; >>> hl.eval(hl.gp_dosage([0.0, 0.5, 0.5])); 1.5. Notes; This function is only defined for bi-allelic variants. The gp argument; must be length 3. The value is gp[1] + 2 * gp[2]. Parameters:; gp (Expression of type tarray of tfloat64) – Length 3 array of bi-allelic genotype probabilities. Returns:; Expression of type tfloat64. hail.expr.functions.get_sequence(contig, position, before=0, after=0, reference_genome='default')[source]; Return the reference sequence at a given locus.; Examples; Return the reference allele for 'GRCh37' at the locus '1:45323':; >>> hl.eval(hl.get_sequence('1', 45323, reference_genome='GRCh37')) ; ""T"". Notes; This function requires reference genome has an attached; reference sequence. Use ReferenceGenome.add_sequence() to; load and attach a reference sequence to a reference genome.; Returns None if contig and position are not valid coordinates in; reference_genome. Parameters:. contig (Expression of type tstr) – Locus contig.; position (Expression of type tint32) – Locus position.; before (Expression of type tint32, optional) – Number of bases to include before the locus of interest. Truncates at; contig boundary.; after (Expression of type tint32, optional) – Number of bases to include after the locus of interest. Truncates at; contig boundary.; reference_genome (str or ReferenceGenome) – Reference genome to use. Must have a reference sequence available. Returns:; StringExpression. hail.expr.functions.mendel_error_code(locus, is_female, father, mother, child)[source]; Compute a Mendelian violation code for genotypes.; >>> father = hl.call(0, 0); >>> mother = hl.call(1, 1); >>> child1 = hl.call(0, 1) # consistent; >>> child2 = hl.call(0, 0) # Mendel error; >>> locus = hl.locus('2', 2000000). >>> hl.eval(hl.mendel_error_cod",MatchSource.WIKI,docs/0.2/functions/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/genetics.html
https://hail.is/docs/0.2/functions/genetics.html:22476,Performance,load,loaded,22476,"r (CallExpression); mother (CallExpression); child (CallExpression). Returns:; Int32Expression. hail.expr.functions.liftover(x, dest_reference_genome, min_match=0.95, include_strand=False)[source]; Lift over coordinates to a different reference genome.; Examples; Lift over the locus coordinates from reference genome 'GRCh37' to; 'GRCh38':; >>> hl.eval(hl.liftover(hl.locus('1', 1034245, 'GRCh37'), 'GRCh38')) ; Locus(contig='chr1', position=1098865, reference_genome='GRCh38'). Lift over the locus interval coordinates from reference genome 'GRCh37'; to 'GRCh38':; >>> hl.eval(hl.liftover(hl.locus_interval('20', 60001, 82456, True, True, 'GRCh37'), 'GRCh38')) ; Interval(Locus(contig='chr20', position=79360, reference_genome='GRCh38'),; Locus(contig='chr20', position=101815, reference_genome='GRCh38'),; True,; True). See Liftover variants from one coordinate system to another for more instructions on lifting over a Table; or MatrixTable.; Notes; This function requires the reference genome of x has a chain file loaded; for dest_reference_genome. Use ReferenceGenome.add_liftover() to; load and attach a chain file to a reference genome.; Returns None if x could not be converted. Warning; Before using the result of liftover() as a new row key or column; key, be sure to filter out missing values. Parameters:. x (Expression of type tlocus or tinterval of tlocus) – Locus or locus interval to lift over.; dest_reference_genome (str or ReferenceGenome) – Reference genome to convert to.; min_match (float) – Minimum ratio of bases that must remap.; include_strand (bool) – If True, output the result as a StructExpression with the first field result being; the locus or locus interval and the second field is_negative_strand is a boolean indicating; whether the locus or locus interval has been mapped to the negative strand of the destination; reference genome. Otherwise, output the converted locus or locus interval. Returns:; Expression – A locus or locus interval converted to dest_refer",MatchSource.WIKI,docs/0.2/functions/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/genetics.html
https://hail.is/docs/0.2/functions/genetics.html:22550,Performance,load,load,22550,"ions.liftover(x, dest_reference_genome, min_match=0.95, include_strand=False)[source]; Lift over coordinates to a different reference genome.; Examples; Lift over the locus coordinates from reference genome 'GRCh37' to; 'GRCh38':; >>> hl.eval(hl.liftover(hl.locus('1', 1034245, 'GRCh37'), 'GRCh38')) ; Locus(contig='chr1', position=1098865, reference_genome='GRCh38'). Lift over the locus interval coordinates from reference genome 'GRCh37'; to 'GRCh38':; >>> hl.eval(hl.liftover(hl.locus_interval('20', 60001, 82456, True, True, 'GRCh37'), 'GRCh38')) ; Interval(Locus(contig='chr20', position=79360, reference_genome='GRCh38'),; Locus(contig='chr20', position=101815, reference_genome='GRCh38'),; True,; True). See Liftover variants from one coordinate system to another for more instructions on lifting over a Table; or MatrixTable.; Notes; This function requires the reference genome of x has a chain file loaded; for dest_reference_genome. Use ReferenceGenome.add_liftover() to; load and attach a chain file to a reference genome.; Returns None if x could not be converted. Warning; Before using the result of liftover() as a new row key or column; key, be sure to filter out missing values. Parameters:. x (Expression of type tlocus or tinterval of tlocus) – Locus or locus interval to lift over.; dest_reference_genome (str or ReferenceGenome) – Reference genome to convert to.; min_match (float) – Minimum ratio of bases that must remap.; include_strand (bool) – If True, output the result as a StructExpression with the first field result being; the locus or locus interval and the second field is_negative_strand is a boolean indicating; whether the locus or locus interval has been mapped to the negative strand of the destination; reference genome. Otherwise, output the converted locus or locus interval. Returns:; Expression – A locus or locus interval converted to dest_reference_genome. hail.expr.functions.min_rep(locus, alleles)[source]; Computes the minimal representation of a (l",MatchSource.WIKI,docs/0.2/functions/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/genetics.html
https://hail.is/docs/0.2/functions/hail.expr.builders.CaseBuilder.html:1384,Availability,error,error,1384,"p-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Functions; Core language functions; CaseBuilder. View page source. CaseBuilder. class hail.expr.builders.CaseBuilder[source]; Class for chaining multiple if-else statements.; Examples; >>> x = hl.literal('foo bar baz'); >>> expr = (hl.case(); ... .when(x[:3] == 'FOO', 1); ... .when(x.length() == 11, 2); ... .when(x == 'secret phrase', 3); ... .default(0)); >>> hl.eval(expr); 2. Notes; All expressions appearing as the then parameters to; when() or; default() method calls must be the; same type. Parameters:; missing_false (bool) – Treat missing predicates as False. See also; case(), cond(), switch(). Attributes. Methods. default; Finish the case statement by adding a default case. or_error; Finish the case statement by throwing an error with the given message. or_missing; Finish the case statement by returning missing. when; Add a branch. default(then)[source]; Finish the case statement by adding a default case.; Notes; If no condition from a when() call is True,; then then is returned. Parameters:; then (Expression). Returns:; Expression. or_error(message)[source]; Finish the case statement by throwing an error with the given message.; Notes; If no condition from a CaseBuilder.when() call is True, then; an error is thrown. Parameters:; message (Expression of type tstr). Returns:; Expression. or_missing()[source]; Finish the case statement by returning missing.; Notes; If no condition from a CaseBuilder.when() call is True, then; the result is missing. Parameters:; then (Expression). Returns:; Expression. when(condition, then)[source]; Add a branch. If condition is True, then returns then. Warning; Missingness is treated similarly to cond(). Missingness is; not treated as False. A condition that e",MatchSource.WIKI,docs/0.2/functions/hail.expr.builders.CaseBuilder.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/hail.expr.builders.CaseBuilder.html
https://hail.is/docs/0.2/functions/hail.expr.builders.CaseBuilder.html:1769,Availability,error,error,1769," source. CaseBuilder. class hail.expr.builders.CaseBuilder[source]; Class for chaining multiple if-else statements.; Examples; >>> x = hl.literal('foo bar baz'); >>> expr = (hl.case(); ... .when(x[:3] == 'FOO', 1); ... .when(x.length() == 11, 2); ... .when(x == 'secret phrase', 3); ... .default(0)); >>> hl.eval(expr); 2. Notes; All expressions appearing as the then parameters to; when() or; default() method calls must be the; same type. Parameters:; missing_false (bool) – Treat missing predicates as False. See also; case(), cond(), switch(). Attributes. Methods. default; Finish the case statement by adding a default case. or_error; Finish the case statement by throwing an error with the given message. or_missing; Finish the case statement by returning missing. when; Add a branch. default(then)[source]; Finish the case statement by adding a default case.; Notes; If no condition from a when() call is True,; then then is returned. Parameters:; then (Expression). Returns:; Expression. or_error(message)[source]; Finish the case statement by throwing an error with the given message.; Notes; If no condition from a CaseBuilder.when() call is True, then; an error is thrown. Parameters:; message (Expression of type tstr). Returns:; Expression. or_missing()[source]; Finish the case statement by returning missing.; Notes; If no condition from a CaseBuilder.when() call is True, then; the result is missing. Parameters:; then (Expression). Returns:; Expression. when(condition, then)[source]; Add a branch. If condition is True, then returns then. Warning; Missingness is treated similarly to cond(). Missingness is; not treated as False. A condition that evaluates to missing; will return a missing result, not proceed to the next case. Always; test missingness first in a CaseBuilder. Parameters:. condition (BooleanExpression); then (Expression). Returns:; CaseBuilder – Mutates and returns self. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/functions/hail.expr.builders.CaseBuilder.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/hail.expr.builders.CaseBuilder.html
https://hail.is/docs/0.2/functions/hail.expr.builders.CaseBuilder.html:1872,Availability,error,error,1872," source. CaseBuilder. class hail.expr.builders.CaseBuilder[source]; Class for chaining multiple if-else statements.; Examples; >>> x = hl.literal('foo bar baz'); >>> expr = (hl.case(); ... .when(x[:3] == 'FOO', 1); ... .when(x.length() == 11, 2); ... .when(x == 'secret phrase', 3); ... .default(0)); >>> hl.eval(expr); 2. Notes; All expressions appearing as the then parameters to; when() or; default() method calls must be the; same type. Parameters:; missing_false (bool) – Treat missing predicates as False. See also; case(), cond(), switch(). Attributes. Methods. default; Finish the case statement by adding a default case. or_error; Finish the case statement by throwing an error with the given message. or_missing; Finish the case statement by returning missing. when; Add a branch. default(then)[source]; Finish the case statement by adding a default case.; Notes; If no condition from a when() call is True,; then then is returned. Parameters:; then (Expression). Returns:; Expression. or_error(message)[source]; Finish the case statement by throwing an error with the given message.; Notes; If no condition from a CaseBuilder.when() call is True, then; an error is thrown. Parameters:; message (Expression of type tstr). Returns:; Expression. or_missing()[source]; Finish the case statement by returning missing.; Notes; If no condition from a CaseBuilder.when() call is True, then; the result is missing. Parameters:; then (Expression). Returns:; Expression. when(condition, then)[source]; Add a branch. If condition is True, then returns then. Warning; Missingness is treated similarly to cond(). Missingness is; not treated as False. A condition that evaluates to missing; will return a missing result, not proceed to the next case. Always; test missingness first in a CaseBuilder. Parameters:. condition (BooleanExpression); then (Expression). Returns:; CaseBuilder – Mutates and returns self. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/functions/hail.expr.builders.CaseBuilder.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/hail.expr.builders.CaseBuilder.html
https://hail.is/docs/0.2/functions/hail.expr.builders.CaseBuilder.html:2673,Deployability,update,updated,2673," source. CaseBuilder. class hail.expr.builders.CaseBuilder[source]; Class for chaining multiple if-else statements.; Examples; >>> x = hl.literal('foo bar baz'); >>> expr = (hl.case(); ... .when(x[:3] == 'FOO', 1); ... .when(x.length() == 11, 2); ... .when(x == 'secret phrase', 3); ... .default(0)); >>> hl.eval(expr); 2. Notes; All expressions appearing as the then parameters to; when() or; default() method calls must be the; same type. Parameters:; missing_false (bool) – Treat missing predicates as False. See also; case(), cond(), switch(). Attributes. Methods. default; Finish the case statement by adding a default case. or_error; Finish the case statement by throwing an error with the given message. or_missing; Finish the case statement by returning missing. when; Add a branch. default(then)[source]; Finish the case statement by adding a default case.; Notes; If no condition from a when() call is True,; then then is returned. Parameters:; then (Expression). Returns:; Expression. or_error(message)[source]; Finish the case statement by throwing an error with the given message.; Notes; If no condition from a CaseBuilder.when() call is True, then; an error is thrown. Parameters:; message (Expression of type tstr). Returns:; Expression. or_missing()[source]; Finish the case statement by returning missing.; Notes; If no condition from a CaseBuilder.when() call is True, then; the result is missing. Parameters:; then (Expression). Returns:; Expression. when(condition, then)[source]; Add a branch. If condition is True, then returns then. Warning; Missingness is treated similarly to cond(). Missingness is; not treated as False. A condition that evaluates to missing; will return a missing result, not proceed to the next case. Always; test missingness first in a CaseBuilder. Parameters:. condition (BooleanExpression); then (Expression). Returns:; CaseBuilder – Mutates and returns self. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/functions/hail.expr.builders.CaseBuilder.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/hail.expr.builders.CaseBuilder.html
https://hail.is/docs/0.2/functions/hail.expr.builders.CaseBuilder.html:1405,Integrability,message,message,1405,"p-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Functions; Core language functions; CaseBuilder. View page source. CaseBuilder. class hail.expr.builders.CaseBuilder[source]; Class for chaining multiple if-else statements.; Examples; >>> x = hl.literal('foo bar baz'); >>> expr = (hl.case(); ... .when(x[:3] == 'FOO', 1); ... .when(x.length() == 11, 2); ... .when(x == 'secret phrase', 3); ... .default(0)); >>> hl.eval(expr); 2. Notes; All expressions appearing as the then parameters to; when() or; default() method calls must be the; same type. Parameters:; missing_false (bool) – Treat missing predicates as False. See also; case(), cond(), switch(). Attributes. Methods. default; Finish the case statement by adding a default case. or_error; Finish the case statement by throwing an error with the given message. or_missing; Finish the case statement by returning missing. when; Add a branch. default(then)[source]; Finish the case statement by adding a default case.; Notes; If no condition from a when() call is True,; then then is returned. Parameters:; then (Expression). Returns:; Expression. or_error(message)[source]; Finish the case statement by throwing an error with the given message.; Notes; If no condition from a CaseBuilder.when() call is True, then; an error is thrown. Parameters:; message (Expression of type tstr). Returns:; Expression. or_missing()[source]; Finish the case statement by returning missing.; Notes; If no condition from a CaseBuilder.when() call is True, then; the result is missing. Parameters:; then (Expression). Returns:; Expression. when(condition, then)[source]; Add a branch. If condition is True, then returns then. Warning; Missingness is treated similarly to cond(). Missingness is; not treated as False. A condition that e",MatchSource.WIKI,docs/0.2/functions/hail.expr.builders.CaseBuilder.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/hail.expr.builders.CaseBuilder.html
https://hail.is/docs/0.2/functions/hail.expr.builders.CaseBuilder.html:1709,Integrability,message,message,1709," source. CaseBuilder. class hail.expr.builders.CaseBuilder[source]; Class for chaining multiple if-else statements.; Examples; >>> x = hl.literal('foo bar baz'); >>> expr = (hl.case(); ... .when(x[:3] == 'FOO', 1); ... .when(x.length() == 11, 2); ... .when(x == 'secret phrase', 3); ... .default(0)); >>> hl.eval(expr); 2. Notes; All expressions appearing as the then parameters to; when() or; default() method calls must be the; same type. Parameters:; missing_false (bool) – Treat missing predicates as False. See also; case(), cond(), switch(). Attributes. Methods. default; Finish the case statement by adding a default case. or_error; Finish the case statement by throwing an error with the given message. or_missing; Finish the case statement by returning missing. when; Add a branch. default(then)[source]; Finish the case statement by adding a default case.; Notes; If no condition from a when() call is True,; then then is returned. Parameters:; then (Expression). Returns:; Expression. or_error(message)[source]; Finish the case statement by throwing an error with the given message.; Notes; If no condition from a CaseBuilder.when() call is True, then; an error is thrown. Parameters:; message (Expression of type tstr). Returns:; Expression. or_missing()[source]; Finish the case statement by returning missing.; Notes; If no condition from a CaseBuilder.when() call is True, then; the result is missing. Parameters:; then (Expression). Returns:; Expression. when(condition, then)[source]; Add a branch. If condition is True, then returns then. Warning; Missingness is treated similarly to cond(). Missingness is; not treated as False. A condition that evaluates to missing; will return a missing result, not proceed to the next case. Always; test missingness first in a CaseBuilder. Parameters:. condition (BooleanExpression); then (Expression). Returns:; CaseBuilder – Mutates and returns self. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/functions/hail.expr.builders.CaseBuilder.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/hail.expr.builders.CaseBuilder.html
https://hail.is/docs/0.2/functions/hail.expr.builders.CaseBuilder.html:1790,Integrability,message,message,1790," source. CaseBuilder. class hail.expr.builders.CaseBuilder[source]; Class for chaining multiple if-else statements.; Examples; >>> x = hl.literal('foo bar baz'); >>> expr = (hl.case(); ... .when(x[:3] == 'FOO', 1); ... .when(x.length() == 11, 2); ... .when(x == 'secret phrase', 3); ... .default(0)); >>> hl.eval(expr); 2. Notes; All expressions appearing as the then parameters to; when() or; default() method calls must be the; same type. Parameters:; missing_false (bool) – Treat missing predicates as False. See also; case(), cond(), switch(). Attributes. Methods. default; Finish the case statement by adding a default case. or_error; Finish the case statement by throwing an error with the given message. or_missing; Finish the case statement by returning missing. when; Add a branch. default(then)[source]; Finish the case statement by adding a default case.; Notes; If no condition from a when() call is True,; then then is returned. Parameters:; then (Expression). Returns:; Expression. or_error(message)[source]; Finish the case statement by throwing an error with the given message.; Notes; If no condition from a CaseBuilder.when() call is True, then; an error is thrown. Parameters:; message (Expression of type tstr). Returns:; Expression. or_missing()[source]; Finish the case statement by returning missing.; Notes; If no condition from a CaseBuilder.when() call is True, then; the result is missing. Parameters:; then (Expression). Returns:; Expression. when(condition, then)[source]; Add a branch. If condition is True, then returns then. Warning; Missingness is treated similarly to cond(). Missingness is; not treated as False. A condition that evaluates to missing; will return a missing result, not proceed to the next case. Always; test missingness first in a CaseBuilder. Parameters:. condition (BooleanExpression); then (Expression). Returns:; CaseBuilder – Mutates and returns self. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/functions/hail.expr.builders.CaseBuilder.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/hail.expr.builders.CaseBuilder.html
https://hail.is/docs/0.2/functions/hail.expr.builders.CaseBuilder.html:1902,Integrability,message,message,1902," source. CaseBuilder. class hail.expr.builders.CaseBuilder[source]; Class for chaining multiple if-else statements.; Examples; >>> x = hl.literal('foo bar baz'); >>> expr = (hl.case(); ... .when(x[:3] == 'FOO', 1); ... .when(x.length() == 11, 2); ... .when(x == 'secret phrase', 3); ... .default(0)); >>> hl.eval(expr); 2. Notes; All expressions appearing as the then parameters to; when() or; default() method calls must be the; same type. Parameters:; missing_false (bool) – Treat missing predicates as False. See also; case(), cond(), switch(). Attributes. Methods. default; Finish the case statement by adding a default case. or_error; Finish the case statement by throwing an error with the given message. or_missing; Finish the case statement by returning missing. when; Add a branch. default(then)[source]; Finish the case statement by adding a default case.; Notes; If no condition from a when() call is True,; then then is returned. Parameters:; then (Expression). Returns:; Expression. or_error(message)[source]; Finish the case statement by throwing an error with the given message.; Notes; If no condition from a CaseBuilder.when() call is True, then; an error is thrown. Parameters:; message (Expression of type tstr). Returns:; Expression. or_missing()[source]; Finish the case statement by returning missing.; Notes; If no condition from a CaseBuilder.when() call is True, then; the result is missing. Parameters:; then (Expression). Returns:; Expression. when(condition, then)[source]; Add a branch. If condition is True, then returns then. Warning; Missingness is treated similarly to cond(). Missingness is; not treated as False. A condition that evaluates to missing; will return a missing result, not proceed to the next case. Always; test missingness first in a CaseBuilder. Parameters:. condition (BooleanExpression); then (Expression). Returns:; CaseBuilder – Mutates and returns self. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/functions/hail.expr.builders.CaseBuilder.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/hail.expr.builders.CaseBuilder.html
https://hail.is/docs/0.2/functions/hail.expr.builders.CaseBuilder.html:2462,Testability,test,test,2462," source. CaseBuilder. class hail.expr.builders.CaseBuilder[source]; Class for chaining multiple if-else statements.; Examples; >>> x = hl.literal('foo bar baz'); >>> expr = (hl.case(); ... .when(x[:3] == 'FOO', 1); ... .when(x.length() == 11, 2); ... .when(x == 'secret phrase', 3); ... .default(0)); >>> hl.eval(expr); 2. Notes; All expressions appearing as the then parameters to; when() or; default() method calls must be the; same type. Parameters:; missing_false (bool) – Treat missing predicates as False. See also; case(), cond(), switch(). Attributes. Methods. default; Finish the case statement by adding a default case. or_error; Finish the case statement by throwing an error with the given message. or_missing; Finish the case statement by returning missing. when; Add a branch. default(then)[source]; Finish the case statement by adding a default case.; Notes; If no condition from a when() call is True,; then then is returned. Parameters:; then (Expression). Returns:; Expression. or_error(message)[source]; Finish the case statement by throwing an error with the given message.; Notes; If no condition from a CaseBuilder.when() call is True, then; an error is thrown. Parameters:; message (Expression of type tstr). Returns:; Expression. or_missing()[source]; Finish the case statement by returning missing.; Notes; If no condition from a CaseBuilder.when() call is True, then; the result is missing. Parameters:; then (Expression). Returns:; Expression. when(condition, then)[source]; Add a branch. If condition is True, then returns then. Warning; Missingness is treated similarly to cond(). Missingness is; not treated as False. A condition that evaluates to missing; will return a missing result, not proceed to the next case. Always; test missingness first in a CaseBuilder. Parameters:. condition (BooleanExpression); then (Expression). Returns:; CaseBuilder – Mutates and returns self. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/functions/hail.expr.builders.CaseBuilder.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/hail.expr.builders.CaseBuilder.html
https://hail.is/docs/0.2/functions/hail.expr.builders.SwitchBuilder.html:1473,Availability,error,error,1473,"o Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Functions; Core language functions; SwitchBuilder. View page source. SwitchBuilder. class hail.expr.builders.SwitchBuilder[source]; Class for generating conditional trees based on value of an expression.; Examples; >>> csq = hl.literal('loss of function'); >>> expr = (hl.switch(csq); ... .when('synonymous', 1); ... .when('SYN', 1); ... .when('missense', 2); ... .when('MIS', 2); ... .when('loss of function', 3); ... .when('LOF', 3); ... .or_missing()); >>> hl.eval(expr); 3. Notes; All expressions appearing as the then parameters to; when() or; default() method calls must be the; same type. See also; case(), cond(), switch(). Parameters:; expr (Expression) – Value to match against. Attributes. Methods. default; Finish the switch statement by adding a default case. or_error; Finish the switch statement by throwing an error with the given message. or_missing; Finish the switch statement by returning missing. when; Add a value test. when_missing; Add a test for missingness. default(then)[source]; Finish the switch statement by adding a default case.; Notes; If no value from a when() call is matched, then; then is returned. Parameters:; then (Expression). Returns:; Expression. or_error(message)[source]; Finish the switch statement by throwing an error with the given message.; Notes; If no value from a SwitchBuilder.when() call is matched, then an; error is thrown. Parameters:; message (Expression of type tstr). Returns:; Expression. or_missing()[source]; Finish the switch statement by returning missing.; Notes; If no value from a when() call is matched, then; the result is missing. Parameters:; then (Expression). Returns:; Expression. when(value, then)[source]; Add a value test. If the base expression is equal to value, then; returns then. Warning; Missingness always compares to missin",MatchSource.WIKI,docs/0.2/functions/hail.expr.builders.SwitchBuilder.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/hail.expr.builders.SwitchBuilder.html
https://hail.is/docs/0.2/functions/hail.expr.builders.SwitchBuilder.html:1909,Availability,error,error,1909,"s of function'); >>> expr = (hl.switch(csq); ... .when('synonymous', 1); ... .when('SYN', 1); ... .when('missense', 2); ... .when('MIS', 2); ... .when('loss of function', 3); ... .when('LOF', 3); ... .or_missing()); >>> hl.eval(expr); 3. Notes; All expressions appearing as the then parameters to; when() or; default() method calls must be the; same type. See also; case(), cond(), switch(). Parameters:; expr (Expression) – Value to match against. Attributes. Methods. default; Finish the switch statement by adding a default case. or_error; Finish the switch statement by throwing an error with the given message. or_missing; Finish the switch statement by returning missing. when; Add a value test. when_missing; Add a test for missingness. default(then)[source]; Finish the switch statement by adding a default case.; Notes; If no value from a when() call is matched, then; then is returned. Parameters:; then (Expression). Returns:; Expression. or_error(message)[source]; Finish the switch statement by throwing an error with the given message.; Notes; If no value from a SwitchBuilder.when() call is matched, then an; error is thrown. Parameters:; message (Expression of type tstr). Returns:; Expression. or_missing()[source]; Finish the switch statement by returning missing.; Notes; If no value from a when() call is matched, then; the result is missing. Parameters:; then (Expression). Returns:; Expression. when(value, then)[source]; Add a value test. If the base expression is equal to value, then; returns then. Warning; Missingness always compares to missing. Both NA == NA and; NA != NA return NA. Use when_missing(); to test missingness. Parameters:. value (Expression); then (Expression). Returns:; SwitchBuilder – Mutates and returns self. when_missing(then)[source]; Add a test for missingness. If the base expression is missing,; returns then. Parameters:; then (Expression). Returns:; SwitchBuilder – Mutates and returns self. Previous; Next . © Copyright 2015-2024, Hail Team",MatchSource.WIKI,docs/0.2/functions/hail.expr.builders.SwitchBuilder.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/hail.expr.builders.SwitchBuilder.html
https://hail.is/docs/0.2/functions/hail.expr.builders.SwitchBuilder.html:2013,Availability,error,error,2013,"h(csq); ... .when('synonymous', 1); ... .when('SYN', 1); ... .when('missense', 2); ... .when('MIS', 2); ... .when('loss of function', 3); ... .when('LOF', 3); ... .or_missing()); >>> hl.eval(expr); 3. Notes; All expressions appearing as the then parameters to; when() or; default() method calls must be the; same type. See also; case(), cond(), switch(). Parameters:; expr (Expression) – Value to match against. Attributes. Methods. default; Finish the switch statement by adding a default case. or_error; Finish the switch statement by throwing an error with the given message. or_missing; Finish the switch statement by returning missing. when; Add a value test. when_missing; Add a test for missingness. default(then)[source]; Finish the switch statement by adding a default case.; Notes; If no value from a when() call is matched, then; then is returned. Parameters:; then (Expression). Returns:; Expression. or_error(message)[source]; Finish the switch statement by throwing an error with the given message.; Notes; If no value from a SwitchBuilder.when() call is matched, then an; error is thrown. Parameters:; message (Expression of type tstr). Returns:; Expression. or_missing()[source]; Finish the switch statement by returning missing.; Notes; If no value from a when() call is matched, then; the result is missing. Parameters:; then (Expression). Returns:; Expression. when(value, then)[source]; Add a value test. If the base expression is equal to value, then; returns then. Warning; Missingness always compares to missing. Both NA == NA and; NA != NA return NA. Use when_missing(); to test missingness. Parameters:. value (Expression); then (Expression). Returns:; SwitchBuilder – Mutates and returns self. when_missing(then)[source]; Add a test for missingness. If the base expression is missing,; returns then. Parameters:; then (Expression). Returns:; SwitchBuilder – Mutates and returns self. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/functions/hail.expr.builders.SwitchBuilder.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/hail.expr.builders.SwitchBuilder.html
https://hail.is/docs/0.2/functions/hail.expr.builders.SwitchBuilder.html:2896,Deployability,update,updated,2896,"h(csq); ... .when('synonymous', 1); ... .when('SYN', 1); ... .when('missense', 2); ... .when('MIS', 2); ... .when('loss of function', 3); ... .when('LOF', 3); ... .or_missing()); >>> hl.eval(expr); 3. Notes; All expressions appearing as the then parameters to; when() or; default() method calls must be the; same type. See also; case(), cond(), switch(). Parameters:; expr (Expression) – Value to match against. Attributes. Methods. default; Finish the switch statement by adding a default case. or_error; Finish the switch statement by throwing an error with the given message. or_missing; Finish the switch statement by returning missing. when; Add a value test. when_missing; Add a test for missingness. default(then)[source]; Finish the switch statement by adding a default case.; Notes; If no value from a when() call is matched, then; then is returned. Parameters:; then (Expression). Returns:; Expression. or_error(message)[source]; Finish the switch statement by throwing an error with the given message.; Notes; If no value from a SwitchBuilder.when() call is matched, then an; error is thrown. Parameters:; message (Expression of type tstr). Returns:; Expression. or_missing()[source]; Finish the switch statement by returning missing.; Notes; If no value from a when() call is matched, then; the result is missing. Parameters:; then (Expression). Returns:; Expression. when(value, then)[source]; Add a value test. If the base expression is equal to value, then; returns then. Warning; Missingness always compares to missing. Both NA == NA and; NA != NA return NA. Use when_missing(); to test missingness. Parameters:. value (Expression); then (Expression). Returns:; SwitchBuilder – Mutates and returns self. when_missing(then)[source]; Add a test for missingness. If the base expression is missing,; returns then. Parameters:; then (Expression). Returns:; SwitchBuilder – Mutates and returns self. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/functions/hail.expr.builders.SwitchBuilder.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/hail.expr.builders.SwitchBuilder.html
https://hail.is/docs/0.2/functions/hail.expr.builders.SwitchBuilder.html:1494,Integrability,message,message,1494,"o Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Functions; Core language functions; SwitchBuilder. View page source. SwitchBuilder. class hail.expr.builders.SwitchBuilder[source]; Class for generating conditional trees based on value of an expression.; Examples; >>> csq = hl.literal('loss of function'); >>> expr = (hl.switch(csq); ... .when('synonymous', 1); ... .when('SYN', 1); ... .when('missense', 2); ... .when('MIS', 2); ... .when('loss of function', 3); ... .when('LOF', 3); ... .or_missing()); >>> hl.eval(expr); 3. Notes; All expressions appearing as the then parameters to; when() or; default() method calls must be the; same type. See also; case(), cond(), switch(). Parameters:; expr (Expression) – Value to match against. Attributes. Methods. default; Finish the switch statement by adding a default case. or_error; Finish the switch statement by throwing an error with the given message. or_missing; Finish the switch statement by returning missing. when; Add a value test. when_missing; Add a test for missingness. default(then)[source]; Finish the switch statement by adding a default case.; Notes; If no value from a when() call is matched, then; then is returned. Parameters:; then (Expression). Returns:; Expression. or_error(message)[source]; Finish the switch statement by throwing an error with the given message.; Notes; If no value from a SwitchBuilder.when() call is matched, then an; error is thrown. Parameters:; message (Expression of type tstr). Returns:; Expression. or_missing()[source]; Finish the switch statement by returning missing.; Notes; If no value from a when() call is matched, then; the result is missing. Parameters:; then (Expression). Returns:; Expression. when(value, then)[source]; Add a value test. If the base expression is equal to value, then; returns then. Warning; Missingness always compares to missin",MatchSource.WIKI,docs/0.2/functions/hail.expr.builders.SwitchBuilder.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/hail.expr.builders.SwitchBuilder.html
https://hail.is/docs/0.2/functions/hail.expr.builders.SwitchBuilder.html:1847,Integrability,message,message,1847,"s of function'); >>> expr = (hl.switch(csq); ... .when('synonymous', 1); ... .when('SYN', 1); ... .when('missense', 2); ... .when('MIS', 2); ... .when('loss of function', 3); ... .when('LOF', 3); ... .or_missing()); >>> hl.eval(expr); 3. Notes; All expressions appearing as the then parameters to; when() or; default() method calls must be the; same type. See also; case(), cond(), switch(). Parameters:; expr (Expression) – Value to match against. Attributes. Methods. default; Finish the switch statement by adding a default case. or_error; Finish the switch statement by throwing an error with the given message. or_missing; Finish the switch statement by returning missing. when; Add a value test. when_missing; Add a test for missingness. default(then)[source]; Finish the switch statement by adding a default case.; Notes; If no value from a when() call is matched, then; then is returned. Parameters:; then (Expression). Returns:; Expression. or_error(message)[source]; Finish the switch statement by throwing an error with the given message.; Notes; If no value from a SwitchBuilder.when() call is matched, then an; error is thrown. Parameters:; message (Expression of type tstr). Returns:; Expression. or_missing()[source]; Finish the switch statement by returning missing.; Notes; If no value from a when() call is matched, then; the result is missing. Parameters:; then (Expression). Returns:; Expression. when(value, then)[source]; Add a value test. If the base expression is equal to value, then; returns then. Warning; Missingness always compares to missing. Both NA == NA and; NA != NA return NA. Use when_missing(); to test missingness. Parameters:. value (Expression); then (Expression). Returns:; SwitchBuilder – Mutates and returns self. when_missing(then)[source]; Add a test for missingness. If the base expression is missing,; returns then. Parameters:; then (Expression). Returns:; SwitchBuilder – Mutates and returns self. Previous; Next . © Copyright 2015-2024, Hail Team",MatchSource.WIKI,docs/0.2/functions/hail.expr.builders.SwitchBuilder.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/hail.expr.builders.SwitchBuilder.html
https://hail.is/docs/0.2/functions/hail.expr.builders.SwitchBuilder.html:1930,Integrability,message,message,1930,"s of function'); >>> expr = (hl.switch(csq); ... .when('synonymous', 1); ... .when('SYN', 1); ... .when('missense', 2); ... .when('MIS', 2); ... .when('loss of function', 3); ... .when('LOF', 3); ... .or_missing()); >>> hl.eval(expr); 3. Notes; All expressions appearing as the then parameters to; when() or; default() method calls must be the; same type. See also; case(), cond(), switch(). Parameters:; expr (Expression) – Value to match against. Attributes. Methods. default; Finish the switch statement by adding a default case. or_error; Finish the switch statement by throwing an error with the given message. or_missing; Finish the switch statement by returning missing. when; Add a value test. when_missing; Add a test for missingness. default(then)[source]; Finish the switch statement by adding a default case.; Notes; If no value from a when() call is matched, then; then is returned. Parameters:; then (Expression). Returns:; Expression. or_error(message)[source]; Finish the switch statement by throwing an error with the given message.; Notes; If no value from a SwitchBuilder.when() call is matched, then an; error is thrown. Parameters:; message (Expression of type tstr). Returns:; Expression. or_missing()[source]; Finish the switch statement by returning missing.; Notes; If no value from a when() call is matched, then; the result is missing. Parameters:; then (Expression). Returns:; Expression. when(value, then)[source]; Add a value test. If the base expression is equal to value, then; returns then. Warning; Missingness always compares to missing. Both NA == NA and; NA != NA return NA. Use when_missing(); to test missingness. Parameters:. value (Expression); then (Expression). Returns:; SwitchBuilder – Mutates and returns self. when_missing(then)[source]; Add a test for missingness. If the base expression is missing,; returns then. Parameters:; then (Expression). Returns:; SwitchBuilder – Mutates and returns self. Previous; Next . © Copyright 2015-2024, Hail Team",MatchSource.WIKI,docs/0.2/functions/hail.expr.builders.SwitchBuilder.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/hail.expr.builders.SwitchBuilder.html
https://hail.is/docs/0.2/functions/hail.expr.builders.SwitchBuilder.html:2043,Integrability,message,message,2043,"h(csq); ... .when('synonymous', 1); ... .when('SYN', 1); ... .when('missense', 2); ... .when('MIS', 2); ... .when('loss of function', 3); ... .when('LOF', 3); ... .or_missing()); >>> hl.eval(expr); 3. Notes; All expressions appearing as the then parameters to; when() or; default() method calls must be the; same type. See also; case(), cond(), switch(). Parameters:; expr (Expression) – Value to match against. Attributes. Methods. default; Finish the switch statement by adding a default case. or_error; Finish the switch statement by throwing an error with the given message. or_missing; Finish the switch statement by returning missing. when; Add a value test. when_missing; Add a test for missingness. default(then)[source]; Finish the switch statement by adding a default case.; Notes; If no value from a when() call is matched, then; then is returned. Parameters:; then (Expression). Returns:; Expression. or_error(message)[source]; Finish the switch statement by throwing an error with the given message.; Notes; If no value from a SwitchBuilder.when() call is matched, then an; error is thrown. Parameters:; message (Expression of type tstr). Returns:; Expression. or_missing()[source]; Finish the switch statement by returning missing.; Notes; If no value from a when() call is matched, then; the result is missing. Parameters:; then (Expression). Returns:; Expression. when(value, then)[source]; Add a value test. If the base expression is equal to value, then; returns then. Warning; Missingness always compares to missing. Both NA == NA and; NA != NA return NA. Use when_missing(); to test missingness. Parameters:. value (Expression); then (Expression). Returns:; SwitchBuilder – Mutates and returns self. when_missing(then)[source]; Add a test for missingness. If the base expression is missing,; returns then. Parameters:; then (Expression). Returns:; SwitchBuilder – Mutates and returns self. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/functions/hail.expr.builders.SwitchBuilder.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/hail.expr.builders.SwitchBuilder.html
https://hail.is/docs/0.2/functions/hail.expr.builders.SwitchBuilder.html:1583,Testability,test,test,1583,"g And Version Policy. menu; Hail. Python API; Hail Query Python API; Functions; Core language functions; SwitchBuilder. View page source. SwitchBuilder. class hail.expr.builders.SwitchBuilder[source]; Class for generating conditional trees based on value of an expression.; Examples; >>> csq = hl.literal('loss of function'); >>> expr = (hl.switch(csq); ... .when('synonymous', 1); ... .when('SYN', 1); ... .when('missense', 2); ... .when('MIS', 2); ... .when('loss of function', 3); ... .when('LOF', 3); ... .or_missing()); >>> hl.eval(expr); 3. Notes; All expressions appearing as the then parameters to; when() or; default() method calls must be the; same type. See also; case(), cond(), switch(). Parameters:; expr (Expression) – Value to match against. Attributes. Methods. default; Finish the switch statement by adding a default case. or_error; Finish the switch statement by throwing an error with the given message. or_missing; Finish the switch statement by returning missing. when; Add a value test. when_missing; Add a test for missingness. default(then)[source]; Finish the switch statement by adding a default case.; Notes; If no value from a when() call is matched, then; then is returned. Parameters:; then (Expression). Returns:; Expression. or_error(message)[source]; Finish the switch statement by throwing an error with the given message.; Notes; If no value from a SwitchBuilder.when() call is matched, then an; error is thrown. Parameters:; message (Expression of type tstr). Returns:; Expression. or_missing()[source]; Finish the switch statement by returning missing.; Notes; If no value from a when() call is matched, then; the result is missing. Parameters:; then (Expression). Returns:; Expression. when(value, then)[source]; Add a value test. If the base expression is equal to value, then; returns then. Warning; Missingness always compares to missing. Both NA == NA and; NA != NA return NA. Use when_missing(); to test missingness. Parameters:. value (Expression);",MatchSource.WIKI,docs/0.2/functions/hail.expr.builders.SwitchBuilder.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/hail.expr.builders.SwitchBuilder.html
https://hail.is/docs/0.2/functions/hail.expr.builders.SwitchBuilder.html:1609,Testability,test,test,1609," Python API; Hail Query Python API; Functions; Core language functions; SwitchBuilder. View page source. SwitchBuilder. class hail.expr.builders.SwitchBuilder[source]; Class for generating conditional trees based on value of an expression.; Examples; >>> csq = hl.literal('loss of function'); >>> expr = (hl.switch(csq); ... .when('synonymous', 1); ... .when('SYN', 1); ... .when('missense', 2); ... .when('MIS', 2); ... .when('loss of function', 3); ... .when('LOF', 3); ... .or_missing()); >>> hl.eval(expr); 3. Notes; All expressions appearing as the then parameters to; when() or; default() method calls must be the; same type. See also; case(), cond(), switch(). Parameters:; expr (Expression) – Value to match against. Attributes. Methods. default; Finish the switch statement by adding a default case. or_error; Finish the switch statement by throwing an error with the given message. or_missing; Finish the switch statement by returning missing. when; Add a value test. when_missing; Add a test for missingness. default(then)[source]; Finish the switch statement by adding a default case.; Notes; If no value from a when() call is matched, then; then is returned. Parameters:; then (Expression). Returns:; Expression. or_error(message)[source]; Finish the switch statement by throwing an error with the given message.; Notes; If no value from a SwitchBuilder.when() call is matched, then an; error is thrown. Parameters:; message (Expression of type tstr). Returns:; Expression. or_missing()[source]; Finish the switch statement by returning missing.; Notes; If no value from a when() call is matched, then; the result is missing. Parameters:; then (Expression). Returns:; Expression. when(value, then)[source]; Add a value test. If the base expression is equal to value, then; returns then. Warning; Missingness always compares to missing. Both NA == NA and; NA != NA return NA. Use when_missing(); to test missingness. Parameters:. value (Expression); then (Expression). Returns:; Swi",MatchSource.WIKI,docs/0.2/functions/hail.expr.builders.SwitchBuilder.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/hail.expr.builders.SwitchBuilder.html
https://hail.is/docs/0.2/functions/hail.expr.builders.SwitchBuilder.html:2347,Testability,test,test,2347,"h(csq); ... .when('synonymous', 1); ... .when('SYN', 1); ... .when('missense', 2); ... .when('MIS', 2); ... .when('loss of function', 3); ... .when('LOF', 3); ... .or_missing()); >>> hl.eval(expr); 3. Notes; All expressions appearing as the then parameters to; when() or; default() method calls must be the; same type. See also; case(), cond(), switch(). Parameters:; expr (Expression) – Value to match against. Attributes. Methods. default; Finish the switch statement by adding a default case. or_error; Finish the switch statement by throwing an error with the given message. or_missing; Finish the switch statement by returning missing. when; Add a value test. when_missing; Add a test for missingness. default(then)[source]; Finish the switch statement by adding a default case.; Notes; If no value from a when() call is matched, then; then is returned. Parameters:; then (Expression). Returns:; Expression. or_error(message)[source]; Finish the switch statement by throwing an error with the given message.; Notes; If no value from a SwitchBuilder.when() call is matched, then an; error is thrown. Parameters:; message (Expression of type tstr). Returns:; Expression. or_missing()[source]; Finish the switch statement by returning missing.; Notes; If no value from a when() call is matched, then; the result is missing. Parameters:; then (Expression). Returns:; Expression. when(value, then)[source]; Add a value test. If the base expression is equal to value, then; returns then. Warning; Missingness always compares to missing. Both NA == NA and; NA != NA return NA. Use when_missing(); to test missingness. Parameters:. value (Expression); then (Expression). Returns:; SwitchBuilder – Mutates and returns self. when_missing(then)[source]; Add a test for missingness. If the base expression is missing,; returns then. Parameters:; then (Expression). Returns:; SwitchBuilder – Mutates and returns self. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/functions/hail.expr.builders.SwitchBuilder.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/hail.expr.builders.SwitchBuilder.html
https://hail.is/docs/0.2/functions/hail.expr.builders.SwitchBuilder.html:2526,Testability,test,test,2526,"h(csq); ... .when('synonymous', 1); ... .when('SYN', 1); ... .when('missense', 2); ... .when('MIS', 2); ... .when('loss of function', 3); ... .when('LOF', 3); ... .or_missing()); >>> hl.eval(expr); 3. Notes; All expressions appearing as the then parameters to; when() or; default() method calls must be the; same type. See also; case(), cond(), switch(). Parameters:; expr (Expression) – Value to match against. Attributes. Methods. default; Finish the switch statement by adding a default case. or_error; Finish the switch statement by throwing an error with the given message. or_missing; Finish the switch statement by returning missing. when; Add a value test. when_missing; Add a test for missingness. default(then)[source]; Finish the switch statement by adding a default case.; Notes; If no value from a when() call is matched, then; then is returned. Parameters:; then (Expression). Returns:; Expression. or_error(message)[source]; Finish the switch statement by throwing an error with the given message.; Notes; If no value from a SwitchBuilder.when() call is matched, then an; error is thrown. Parameters:; message (Expression of type tstr). Returns:; Expression. or_missing()[source]; Finish the switch statement by returning missing.; Notes; If no value from a when() call is matched, then; the result is missing. Parameters:; then (Expression). Returns:; Expression. when(value, then)[source]; Add a value test. If the base expression is equal to value, then; returns then. Warning; Missingness always compares to missing. Both NA == NA and; NA != NA return NA. Use when_missing(); to test missingness. Parameters:. value (Expression); then (Expression). Returns:; SwitchBuilder – Mutates and returns self. when_missing(then)[source]; Add a test for missingness. If the base expression is missing,; returns then. Parameters:; then (Expression). Returns:; SwitchBuilder – Mutates and returns self. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/functions/hail.expr.builders.SwitchBuilder.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/hail.expr.builders.SwitchBuilder.html
https://hail.is/docs/0.2/functions/hail.expr.builders.SwitchBuilder.html:2683,Testability,test,test,2683,"h(csq); ... .when('synonymous', 1); ... .when('SYN', 1); ... .when('missense', 2); ... .when('MIS', 2); ... .when('loss of function', 3); ... .when('LOF', 3); ... .or_missing()); >>> hl.eval(expr); 3. Notes; All expressions appearing as the then parameters to; when() or; default() method calls must be the; same type. See also; case(), cond(), switch(). Parameters:; expr (Expression) – Value to match against. Attributes. Methods. default; Finish the switch statement by adding a default case. or_error; Finish the switch statement by throwing an error with the given message. or_missing; Finish the switch statement by returning missing. when; Add a value test. when_missing; Add a test for missingness. default(then)[source]; Finish the switch statement by adding a default case.; Notes; If no value from a when() call is matched, then; then is returned. Parameters:; then (Expression). Returns:; Expression. or_error(message)[source]; Finish the switch statement by throwing an error with the given message.; Notes; If no value from a SwitchBuilder.when() call is matched, then an; error is thrown. Parameters:; message (Expression of type tstr). Returns:; Expression. or_missing()[source]; Finish the switch statement by returning missing.; Notes; If no value from a when() call is matched, then; the result is missing. Parameters:; then (Expression). Returns:; Expression. when(value, then)[source]; Add a value test. If the base expression is equal to value, then; returns then. Warning; Missingness always compares to missing. Both NA == NA and; NA != NA return NA. Use when_missing(); to test missingness. Parameters:. value (Expression); then (Expression). Returns:; SwitchBuilder – Mutates and returns self. when_missing(then)[source]; Add a test for missingness. If the base expression is missing,; returns then. Parameters:; then (Expression). Returns:; SwitchBuilder – Mutates and returns self. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/functions/hail.expr.builders.SwitchBuilder.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/hail.expr.builders.SwitchBuilder.html
https://hail.is/docs/0.2/functions/index.html:2518,Availability,down,downcode,2518," logit(); floor(); ceil(); sqrt(); sign(); min(); nanmin(); max(); nanmax(); mean(); median(); product(); sum(); cumulative_sum(); argmin(); argmax(); corr(); uniroot(); binary_search(). String functions; format(); json(); parse_json(); hamming(); delimit(); entropy(); parse_int(); parse_int32(); parse_int64(); parse_float(); parse_float32(); parse_float64(). Statistical functions; chi_squared_test(); fisher_exact_test(); contingency_table_test(); cochran_mantel_haenszel_test(); dbeta(); dchisq(); dnorm(); dpois(); hardy_weinberg_test(); binom_test(); pchisqtail(); pgenchisq(); pnorm(); pT(); pF(); ppois(); qchisqtail(); qnorm(); qpois(). Random functions; Setting a seed; Reproducibility across sessions. Genetics functions; locus(); locus_from_global_position(); locus_interval(); parse_locus(); parse_variant(); parse_locus_interval(); variant_str(); call(); unphased_diploid_gt_index_call(); parse_call(); downcode(); triangle(); is_snp(); is_mnp(); is_transition(); is_transversion(); is_insertion(); is_deletion(); is_indel(); is_star(); is_complex(); is_strand_ambiguous(); is_valid_contig(); is_valid_locus(); contig_length(); allele_type(); numeric_allele_type(); pl_dosage(); gp_dosage(); get_sequence(); mendel_error_code(); liftover(); min_rep(); reverse_complement(). Core language functions. literal(x[, dtype]); Captures and broadcasts a Python variable or object as an expression. cond(condition, consequent, alternate[, ...]); Deprecated in favor of if_else(). if_else(condition, consequent, alternate[, ...]); Expression for an if/else statement; tests a condition and returns one of two options based on the result. switch(expr); Build a conditional tree on the value of an expression. case([missing_false]); Chain multiple if-else statements with a CaseBuilder. bind(f, *exprs[, _ctx]); Bind a temporary variable and use it in a function. rbind(*exprs[, _ctx]); Bind a temporary variable and use it in a function. null(t); Deprecated in favor of missing(). is_missing(expre",MatchSource.WIKI,docs/0.2/functions/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/index.html
https://hail.is/docs/0.2/functions/index.html:6689,Availability,toler,tolerance,6689,"n boolean expressions or collections of booleans. all(*args); Check for all True in boolean expressions or collections of booleans. filter(f, collection); Returns a new collection containing elements where f returns True. sorted(collection[, key, reverse]); Returns a sorted array. find(f, collection); Returns the first element where f returns True. group_by(f, collection); Group collection elements into a dict according to a lambda function. fold(f, zero, collection); Reduces a collection with the given function f, provided the initial value zero. array_scan(f, zero, a); Map each element of a to cumulative value of function f, with initial value zero. reversed(x); Reverses the elements of a collection. keyed_intersection(*arrays, key); Compute the intersection of sorted arrays on a given key. keyed_union(*arrays, key); Compute the distinct union of sorted arrays on a given key. Numeric functions. abs(x); Take the absolute value of a numeric value, array or ndarray. approx_equal(x, y[, tolerance, absolute, ...]); Tests whether two numbers are approximately equal. bit_and(x, y); Bitwise and x and y. bit_or(x, y); Bitwise or x and y. bit_xor(x, y); Bitwise exclusive-or x and y. bit_lshift(x, y); Bitwise left-shift x by y. bit_rshift(x, y[, logical]); Bitwise right-shift x by y. bit_not(x); Bitwise invert x. bit_count(x); Count the number of 1s in the in the two's complement binary representation of x. exp(x). expit(x). is_nan(x). is_finite(x). is_infinite(x). log(x[, base]); Take the logarithm of the x with base base. log10(x). logit(x). sign(x); Returns the sign of a numeric value, array or ndarray. sqrt(x). int(x); Convert to a 32-bit integer expression. int32(x); Convert to a 32-bit integer expression. int64(x); Convert to a 64-bit integer expression. float(x); Convert to a 64-bit floating point expression. float32(x); Convert to a 32-bit floating point expression. float64(x); Convert to a 64-bit floating point expression. floor(x). ceil(x). uniroot(f, min, max, *[, ",MatchSource.WIKI,docs/0.2/functions/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/index.html
https://hail.is/docs/0.2/functions/index.html:13414,Availability,down,downcode,13414,"s functions. locus(contig, pos[, reference_genome]); Construct a locus expression from a chromosome and position. locus_from_global_position(global_pos[, ...]); Constructs a locus expression from a global position and a reference genome. locus_interval(contig, start, end[, ...]); Construct a locus interval expression. parse_locus(s[, reference_genome]); Construct a locus expression by parsing a string or string expression. parse_variant(s[, reference_genome]); Construct a struct with a locus and alleles by parsing a string. parse_locus_interval(s[, reference_genome, ...]); Construct a locus interval expression by parsing a string or string expression. variant_str(*args); Create a variant colon-delimited string. call(*alleles[, phased]); Construct a call expression. unphased_diploid_gt_index_call(gt_index); Construct an unphased, diploid call from a genotype index. parse_call(s); Construct a call expression by parsing a string or string expression. downcode(c, i); Create a new call by setting all alleles other than i to ref. triangle(n); Returns the triangle number of n. is_snp(ref, alt); Returns True if the alleles constitute a single nucleotide polymorphism. is_mnp(ref, alt); Returns True if the alleles constitute a multiple nucleotide polymorphism. is_transition(ref, alt); Returns True if the alleles constitute a transition. is_transversion(ref, alt); Returns True if the alleles constitute a transversion. is_insertion(ref, alt); Returns True if the alleles constitute an insertion. is_deletion(ref, alt); Returns True if the alleles constitute a deletion. is_indel(ref, alt); Returns True if the alleles constitute an insertion or deletion. is_star(ref, alt); Returns True if the alleles constitute an upstream deletion. is_complex(ref, alt); Returns True if the alleles constitute a complex polymorphism. is_valid_contig(contig[, reference_genome]); Returns True if contig is a valid contig name in reference_genome. is_valid_locus(contig, position[, ...]); Returns True if",MatchSource.WIKI,docs/0.2/functions/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/index.html
https://hail.is/docs/0.2/functions/index.html:15424,Deployability,update,updated,15424,"etting all alleles other than i to ref. triangle(n); Returns the triangle number of n. is_snp(ref, alt); Returns True if the alleles constitute a single nucleotide polymorphism. is_mnp(ref, alt); Returns True if the alleles constitute a multiple nucleotide polymorphism. is_transition(ref, alt); Returns True if the alleles constitute a transition. is_transversion(ref, alt); Returns True if the alleles constitute a transversion. is_insertion(ref, alt); Returns True if the alleles constitute an insertion. is_deletion(ref, alt); Returns True if the alleles constitute a deletion. is_indel(ref, alt); Returns True if the alleles constitute an insertion or deletion. is_star(ref, alt); Returns True if the alleles constitute an upstream deletion. is_complex(ref, alt); Returns True if the alleles constitute a complex polymorphism. is_valid_contig(contig[, reference_genome]); Returns True if contig is a valid contig name in reference_genome. is_valid_locus(contig, position[, ...]); Returns True if contig and position is a valid site in reference_genome. contig_length(contig[, reference_genome]); Returns the length of contig in reference_genome. allele_type(ref, alt); Returns the type of the polymorphism as a string. pl_dosage(pl); Return expected genotype dosage from array of Phred-scaled genotype likelihoods with uniform prior. gp_dosage(gp); Return expected genotype dosage from array of genotype probabilities. get_sequence(contig, position[, before, ...]); Return the reference sequence at a given locus. mendel_error_code(locus, is_female, father, ...); Compute a Mendelian violation code for genotypes. liftover(x, dest_reference_genome[, ...]); Lift over coordinates to a different reference genome. min_rep(locus, alleles); Computes the minimal representation of a (locus, alleles) polymorphism. reverse_complement(s[, rna]); Reverses the string and translates base pairs into their complements . Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/functions/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/index.html
https://hail.is/docs/0.2/functions/index.html:2968,Modifiability,variab,variable,2968,"(). Statistical functions; chi_squared_test(); fisher_exact_test(); contingency_table_test(); cochran_mantel_haenszel_test(); dbeta(); dchisq(); dnorm(); dpois(); hardy_weinberg_test(); binom_test(); pchisqtail(); pgenchisq(); pnorm(); pT(); pF(); ppois(); qchisqtail(); qnorm(); qpois(). Random functions; Setting a seed; Reproducibility across sessions. Genetics functions; locus(); locus_from_global_position(); locus_interval(); parse_locus(); parse_variant(); parse_locus_interval(); variant_str(); call(); unphased_diploid_gt_index_call(); parse_call(); downcode(); triangle(); is_snp(); is_mnp(); is_transition(); is_transversion(); is_insertion(); is_deletion(); is_indel(); is_star(); is_complex(); is_strand_ambiguous(); is_valid_contig(); is_valid_locus(); contig_length(); allele_type(); numeric_allele_type(); pl_dosage(); gp_dosage(); get_sequence(); mendel_error_code(); liftover(); min_rep(); reverse_complement(). Core language functions. literal(x[, dtype]); Captures and broadcasts a Python variable or object as an expression. cond(condition, consequent, alternate[, ...]); Deprecated in favor of if_else(). if_else(condition, consequent, alternate[, ...]); Expression for an if/else statement; tests a condition and returns one of two options based on the result. switch(expr); Build a conditional tree on the value of an expression. case([missing_false]); Chain multiple if-else statements with a CaseBuilder. bind(f, *exprs[, _ctx]); Bind a temporary variable and use it in a function. rbind(*exprs[, _ctx]); Bind a temporary variable and use it in a function. null(t); Deprecated in favor of missing(). is_missing(expression); Returns True if the argument is missing. is_defined(expression); Returns True if the argument is not missing. coalesce(*args); Returns the first non-missing value of args. or_else(a, b); If a is missing, return b. or_missing(predicate, value); Returns value if predicate is True, otherwise returns missing. range(start[, stop, step]); Returns an arra",MatchSource.WIKI,docs/0.2/functions/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/index.html
https://hail.is/docs/0.2/functions/index.html:3432,Modifiability,variab,variable,3432,"e_locus_interval(); variant_str(); call(); unphased_diploid_gt_index_call(); parse_call(); downcode(); triangle(); is_snp(); is_mnp(); is_transition(); is_transversion(); is_insertion(); is_deletion(); is_indel(); is_star(); is_complex(); is_strand_ambiguous(); is_valid_contig(); is_valid_locus(); contig_length(); allele_type(); numeric_allele_type(); pl_dosage(); gp_dosage(); get_sequence(); mendel_error_code(); liftover(); min_rep(); reverse_complement(). Core language functions. literal(x[, dtype]); Captures and broadcasts a Python variable or object as an expression. cond(condition, consequent, alternate[, ...]); Deprecated in favor of if_else(). if_else(condition, consequent, alternate[, ...]); Expression for an if/else statement; tests a condition and returns one of two options based on the result. switch(expr); Build a conditional tree on the value of an expression. case([missing_false]); Chain multiple if-else statements with a CaseBuilder. bind(f, *exprs[, _ctx]); Bind a temporary variable and use it in a function. rbind(*exprs[, _ctx]); Bind a temporary variable and use it in a function. null(t); Deprecated in favor of missing(). is_missing(expression); Returns True if the argument is missing. is_defined(expression); Returns True if the argument is not missing. coalesce(*args); Returns the first non-missing value of args. or_else(a, b); If a is missing, return b. or_missing(predicate, value); Returns value if predicate is True, otherwise returns missing. range(start[, stop, step]); Returns an array of integers from start to stop by step. query_table(path, point_or_interval); Query records from a table corresponding to a given point or range of keys. Constructors. bool(x); Convert to a Boolean expression. float(x); Convert to a 64-bit floating point expression. float32(x); Convert to a 32-bit floating point expression. float64(x); Convert to a 64-bit floating point expression. int(x); Convert to a 32-bit integer expression. int32(x); Convert to a 32-bit inte",MatchSource.WIKI,docs/0.2/functions/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/index.html
https://hail.is/docs/0.2/functions/index.html:3507,Modifiability,variab,variable,3507," parse_call(); downcode(); triangle(); is_snp(); is_mnp(); is_transition(); is_transversion(); is_insertion(); is_deletion(); is_indel(); is_star(); is_complex(); is_strand_ambiguous(); is_valid_contig(); is_valid_locus(); contig_length(); allele_type(); numeric_allele_type(); pl_dosage(); gp_dosage(); get_sequence(); mendel_error_code(); liftover(); min_rep(); reverse_complement(). Core language functions. literal(x[, dtype]); Captures and broadcasts a Python variable or object as an expression. cond(condition, consequent, alternate[, ...]); Deprecated in favor of if_else(). if_else(condition, consequent, alternate[, ...]); Expression for an if/else statement; tests a condition and returns one of two options based on the result. switch(expr); Build a conditional tree on the value of an expression. case([missing_false]); Chain multiple if-else statements with a CaseBuilder. bind(f, *exprs[, _ctx]); Bind a temporary variable and use it in a function. rbind(*exprs[, _ctx]); Bind a temporary variable and use it in a function. null(t); Deprecated in favor of missing(). is_missing(expression); Returns True if the argument is missing. is_defined(expression); Returns True if the argument is not missing. coalesce(*args); Returns the first non-missing value of args. or_else(a, b); If a is missing, return b. or_missing(predicate, value); Returns value if predicate is True, otherwise returns missing. range(start[, stop, step]); Returns an array of integers from start to stop by step. query_table(path, point_or_interval); Query records from a table corresponding to a given point or range of keys. Constructors. bool(x); Convert to a Boolean expression. float(x); Convert to a 64-bit floating point expression. float32(x); Convert to a 32-bit floating point expression. float64(x); Convert to a 64-bit floating point expression. int(x); Convert to a 32-bit integer expression. int32(x); Convert to a 32-bit integer expression. int64(x); Convert to a 64-bit integer expression. interval(s",MatchSource.WIKI,docs/0.2/functions/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/index.html
https://hail.is/docs/0.2/functions/index.html:13616,Modifiability,polymorphi,polymorphism,13616,"_position(global_pos[, ...]); Constructs a locus expression from a global position and a reference genome. locus_interval(contig, start, end[, ...]); Construct a locus interval expression. parse_locus(s[, reference_genome]); Construct a locus expression by parsing a string or string expression. parse_variant(s[, reference_genome]); Construct a struct with a locus and alleles by parsing a string. parse_locus_interval(s[, reference_genome, ...]); Construct a locus interval expression by parsing a string or string expression. variant_str(*args); Create a variant colon-delimited string. call(*alleles[, phased]); Construct a call expression. unphased_diploid_gt_index_call(gt_index); Construct an unphased, diploid call from a genotype index. parse_call(s); Construct a call expression by parsing a string or string expression. downcode(c, i); Create a new call by setting all alleles other than i to ref. triangle(n); Returns the triangle number of n. is_snp(ref, alt); Returns True if the alleles constitute a single nucleotide polymorphism. is_mnp(ref, alt); Returns True if the alleles constitute a multiple nucleotide polymorphism. is_transition(ref, alt); Returns True if the alleles constitute a transition. is_transversion(ref, alt); Returns True if the alleles constitute a transversion. is_insertion(ref, alt); Returns True if the alleles constitute an insertion. is_deletion(ref, alt); Returns True if the alleles constitute a deletion. is_indel(ref, alt); Returns True if the alleles constitute an insertion or deletion. is_star(ref, alt); Returns True if the alleles constitute an upstream deletion. is_complex(ref, alt); Returns True if the alleles constitute a complex polymorphism. is_valid_contig(contig[, reference_genome]); Returns True if contig is a valid contig name in reference_genome. is_valid_locus(contig, position[, ...]); Returns True if contig and position is a valid site in reference_genome. contig_length(contig[, reference_genome]); Returns the length of contig in",MatchSource.WIKI,docs/0.2/functions/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/index.html
https://hail.is/docs/0.2/functions/index.html:13709,Modifiability,polymorphi,polymorphism,13709,"erence genome. locus_interval(contig, start, end[, ...]); Construct a locus interval expression. parse_locus(s[, reference_genome]); Construct a locus expression by parsing a string or string expression. parse_variant(s[, reference_genome]); Construct a struct with a locus and alleles by parsing a string. parse_locus_interval(s[, reference_genome, ...]); Construct a locus interval expression by parsing a string or string expression. variant_str(*args); Create a variant colon-delimited string. call(*alleles[, phased]); Construct a call expression. unphased_diploid_gt_index_call(gt_index); Construct an unphased, diploid call from a genotype index. parse_call(s); Construct a call expression by parsing a string or string expression. downcode(c, i); Create a new call by setting all alleles other than i to ref. triangle(n); Returns the triangle number of n. is_snp(ref, alt); Returns True if the alleles constitute a single nucleotide polymorphism. is_mnp(ref, alt); Returns True if the alleles constitute a multiple nucleotide polymorphism. is_transition(ref, alt); Returns True if the alleles constitute a transition. is_transversion(ref, alt); Returns True if the alleles constitute a transversion. is_insertion(ref, alt); Returns True if the alleles constitute an insertion. is_deletion(ref, alt); Returns True if the alleles constitute a deletion. is_indel(ref, alt); Returns True if the alleles constitute an insertion or deletion. is_star(ref, alt); Returns True if the alleles constitute an upstream deletion. is_complex(ref, alt); Returns True if the alleles constitute a complex polymorphism. is_valid_contig(contig[, reference_genome]); Returns True if contig is a valid contig name in reference_genome. is_valid_locus(contig, position[, ...]); Returns True if contig and position is a valid site in reference_genome. contig_length(contig[, reference_genome]); Returns the length of contig in reference_genome. allele_type(ref, alt); Returns the type of the polymorphism as a string. ",MatchSource.WIKI,docs/0.2/functions/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/index.html
https://hail.is/docs/0.2/functions/index.html:14270,Modifiability,polymorphi,polymorphism,14270,"loid_gt_index_call(gt_index); Construct an unphased, diploid call from a genotype index. parse_call(s); Construct a call expression by parsing a string or string expression. downcode(c, i); Create a new call by setting all alleles other than i to ref. triangle(n); Returns the triangle number of n. is_snp(ref, alt); Returns True if the alleles constitute a single nucleotide polymorphism. is_mnp(ref, alt); Returns True if the alleles constitute a multiple nucleotide polymorphism. is_transition(ref, alt); Returns True if the alleles constitute a transition. is_transversion(ref, alt); Returns True if the alleles constitute a transversion. is_insertion(ref, alt); Returns True if the alleles constitute an insertion. is_deletion(ref, alt); Returns True if the alleles constitute a deletion. is_indel(ref, alt); Returns True if the alleles constitute an insertion or deletion. is_star(ref, alt); Returns True if the alleles constitute an upstream deletion. is_complex(ref, alt); Returns True if the alleles constitute a complex polymorphism. is_valid_contig(contig[, reference_genome]); Returns True if contig is a valid contig name in reference_genome. is_valid_locus(contig, position[, ...]); Returns True if contig and position is a valid site in reference_genome. contig_length(contig[, reference_genome]); Returns the length of contig in reference_genome. allele_type(ref, alt); Returns the type of the polymorphism as a string. pl_dosage(pl); Return expected genotype dosage from array of Phred-scaled genotype likelihoods with uniform prior. gp_dosage(gp); Return expected genotype dosage from array of genotype probabilities. get_sequence(contig, position[, before, ...]); Return the reference sequence at a given locus. mendel_error_code(locus, is_female, father, ...); Compute a Mendelian violation code for genotypes. liftover(x, dest_reference_genome[, ...]); Lift over coordinates to a different reference genome. min_rep(locus, alleles); Computes the minimal representation of a (locus",MatchSource.WIKI,docs/0.2/functions/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/index.html
https://hail.is/docs/0.2/functions/index.html:14650,Modifiability,polymorphi,polymorphism,14650,"etting all alleles other than i to ref. triangle(n); Returns the triangle number of n. is_snp(ref, alt); Returns True if the alleles constitute a single nucleotide polymorphism. is_mnp(ref, alt); Returns True if the alleles constitute a multiple nucleotide polymorphism. is_transition(ref, alt); Returns True if the alleles constitute a transition. is_transversion(ref, alt); Returns True if the alleles constitute a transversion. is_insertion(ref, alt); Returns True if the alleles constitute an insertion. is_deletion(ref, alt); Returns True if the alleles constitute a deletion. is_indel(ref, alt); Returns True if the alleles constitute an insertion or deletion. is_star(ref, alt); Returns True if the alleles constitute an upstream deletion. is_complex(ref, alt); Returns True if the alleles constitute a complex polymorphism. is_valid_contig(contig[, reference_genome]); Returns True if contig is a valid contig name in reference_genome. is_valid_locus(contig, position[, ...]); Returns True if contig and position is a valid site in reference_genome. contig_length(contig[, reference_genome]); Returns the length of contig in reference_genome. allele_type(ref, alt); Returns the type of the polymorphism as a string. pl_dosage(pl); Return expected genotype dosage from array of Phred-scaled genotype likelihoods with uniform prior. gp_dosage(gp); Return expected genotype dosage from array of genotype probabilities. get_sequence(contig, position[, before, ...]); Return the reference sequence at a given locus. mendel_error_code(locus, is_female, father, ...); Compute a Mendelian violation code for genotypes. liftover(x, dest_reference_genome[, ...]); Lift over coordinates to a different reference genome. min_rep(locus, alleles); Computes the minimal representation of a (locus, alleles) polymorphism. reverse_complement(s[, rna]); Reverses the string and translates base pairs into their complements . Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/functions/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/index.html
https://hail.is/docs/0.2/functions/index.html:15252,Modifiability,polymorphi,polymorphism,15252,"etting all alleles other than i to ref. triangle(n); Returns the triangle number of n. is_snp(ref, alt); Returns True if the alleles constitute a single nucleotide polymorphism. is_mnp(ref, alt); Returns True if the alleles constitute a multiple nucleotide polymorphism. is_transition(ref, alt); Returns True if the alleles constitute a transition. is_transversion(ref, alt); Returns True if the alleles constitute a transversion. is_insertion(ref, alt); Returns True if the alleles constitute an insertion. is_deletion(ref, alt); Returns True if the alleles constitute a deletion. is_indel(ref, alt); Returns True if the alleles constitute an insertion or deletion. is_star(ref, alt); Returns True if the alleles constitute an upstream deletion. is_complex(ref, alt); Returns True if the alleles constitute a complex polymorphism. is_valid_contig(contig[, reference_genome]); Returns True if contig is a valid contig name in reference_genome. is_valid_locus(contig, position[, ...]); Returns True if contig and position is a valid site in reference_genome. contig_length(contig[, reference_genome]); Returns the length of contig in reference_genome. allele_type(ref, alt); Returns the type of the polymorphism as a string. pl_dosage(pl); Return expected genotype dosage from array of Phred-scaled genotype likelihoods with uniform prior. gp_dosage(gp); Return expected genotype dosage from array of genotype probabilities. get_sequence(contig, position[, before, ...]); Return the reference sequence at a given locus. mendel_error_code(locus, is_female, father, ...); Compute a Mendelian violation code for genotypes. liftover(x, dest_reference_genome[, ...]); Lift over coordinates to a different reference genome. min_rep(locus, alleles); Computes the minimal representation of a (locus, alleles) polymorphism. reverse_complement(s[, rna]); Reverses the string and translates base pairs into their complements . Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/functions/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/index.html
https://hail.is/docs/0.2/functions/index.html:702,Security,expose,exposed,702,"﻿. Hail | ; Functions. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Functions. View page source. Functions; These functions are exposed at the top level of the module, e.g. hl.case. Core language functions; literal(); cond(); if_else(); switch(); case(); bind(); rbind(); missing(); null(); str(); is_missing(); is_defined(); coalesce(); or_else(); or_missing(); range(); query_table(); CaseBuilder; SwitchBuilder. Constructor functions; bool(); float(); float32(); float64(); int(); int32(); int64(); interval(); struct(); tuple(); array(); empty_array(); set(); empty_set(); dict(); empty_dict(). Collection functions; len(); map(); flatmap(); starmap(); zip(); enumerate(); zip_with_index(); flatten(); any(); all(); filter(); sorted(); find(); group_by(); fold(); array_scan(); reversed(); keyed_intersection(); keyed_union(). Numeric functions; abs(); approx_equal(); bit_and(); bit_or(); bit_xor(); bit_lshift(); bit_rshift(); bit_not(); bit_count(); exp(); expit(); is_nan(); is_finite(); is_infinite(); log(); log10(); logit(); floor(); ceil(); sqrt(); sign(); min(); nanmin(); max(); nanmax(); mean(); median(); product(); sum(); cumulative_sum(); argmin(); argmax(); corr(); uniroot(); binary_search(). String functions; format(); json(); parse_json(); hamming(); delimit(); entropy(); parse_int(); parse_int32(); parse_int64(); parse_float(); parse_float32(); parse_float64(). Statistical functions; chi_squared_test",MatchSource.WIKI,docs/0.2/functions/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/index.html
https://hail.is/docs/0.2/functions/index.html:1585,Testability,log,log,1585,"enu; Hail. Python API; Hail Query Python API; Functions. View page source. Functions; These functions are exposed at the top level of the module, e.g. hl.case. Core language functions; literal(); cond(); if_else(); switch(); case(); bind(); rbind(); missing(); null(); str(); is_missing(); is_defined(); coalesce(); or_else(); or_missing(); range(); query_table(); CaseBuilder; SwitchBuilder. Constructor functions; bool(); float(); float32(); float64(); int(); int32(); int64(); interval(); struct(); tuple(); array(); empty_array(); set(); empty_set(); dict(); empty_dict(). Collection functions; len(); map(); flatmap(); starmap(); zip(); enumerate(); zip_with_index(); flatten(); any(); all(); filter(); sorted(); find(); group_by(); fold(); array_scan(); reversed(); keyed_intersection(); keyed_union(). Numeric functions; abs(); approx_equal(); bit_and(); bit_or(); bit_xor(); bit_lshift(); bit_rshift(); bit_not(); bit_count(); exp(); expit(); is_nan(); is_finite(); is_infinite(); log(); log10(); logit(); floor(); ceil(); sqrt(); sign(); min(); nanmin(); max(); nanmax(); mean(); median(); product(); sum(); cumulative_sum(); argmin(); argmax(); corr(); uniroot(); binary_search(). String functions; format(); json(); parse_json(); hamming(); delimit(); entropy(); parse_int(); parse_int32(); parse_int64(); parse_float(); parse_float32(); parse_float64(). Statistical functions; chi_squared_test(); fisher_exact_test(); contingency_table_test(); cochran_mantel_haenszel_test(); dbeta(); dchisq(); dnorm(); dpois(); hardy_weinberg_test(); binom_test(); pchisqtail(); pgenchisq(); pnorm(); pT(); pF(); ppois(); qchisqtail(); qnorm(); qpois(). Random functions; Setting a seed; Reproducibility across sessions. Genetics functions; locus(); locus_from_global_position(); locus_interval(); parse_locus(); parse_variant(); parse_locus_interval(); variant_str(); call(); unphased_diploid_gt_index_call(); parse_call(); downcode(); triangle(); is_snp(); is_mnp(); is_transition(); is_transversion(",MatchSource.WIKI,docs/0.2/functions/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/index.html
https://hail.is/docs/0.2/functions/index.html:1601,Testability,log,logit,1601,"enu; Hail. Python API; Hail Query Python API; Functions. View page source. Functions; These functions are exposed at the top level of the module, e.g. hl.case. Core language functions; literal(); cond(); if_else(); switch(); case(); bind(); rbind(); missing(); null(); str(); is_missing(); is_defined(); coalesce(); or_else(); or_missing(); range(); query_table(); CaseBuilder; SwitchBuilder. Constructor functions; bool(); float(); float32(); float64(); int(); int32(); int64(); interval(); struct(); tuple(); array(); empty_array(); set(); empty_set(); dict(); empty_dict(). Collection functions; len(); map(); flatmap(); starmap(); zip(); enumerate(); zip_with_index(); flatten(); any(); all(); filter(); sorted(); find(); group_by(); fold(); array_scan(); reversed(); keyed_intersection(); keyed_union(). Numeric functions; abs(); approx_equal(); bit_and(); bit_or(); bit_xor(); bit_lshift(); bit_rshift(); bit_not(); bit_count(); exp(); expit(); is_nan(); is_finite(); is_infinite(); log(); log10(); logit(); floor(); ceil(); sqrt(); sign(); min(); nanmin(); max(); nanmax(); mean(); median(); product(); sum(); cumulative_sum(); argmin(); argmax(); corr(); uniroot(); binary_search(). String functions; format(); json(); parse_json(); hamming(); delimit(); entropy(); parse_int(); parse_int32(); parse_int64(); parse_float(); parse_float32(); parse_float64(). Statistical functions; chi_squared_test(); fisher_exact_test(); contingency_table_test(); cochran_mantel_haenszel_test(); dbeta(); dchisq(); dnorm(); dpois(); hardy_weinberg_test(); binom_test(); pchisqtail(); pgenchisq(); pnorm(); pT(); pF(); ppois(); qchisqtail(); qnorm(); qpois(). Random functions; Setting a seed; Reproducibility across sessions. Genetics functions; locus(); locus_from_global_position(); locus_interval(); parse_locus(); parse_variant(); parse_locus_interval(); variant_str(); call(); unphased_diploid_gt_index_call(); parse_call(); downcode(); triangle(); is_snp(); is_mnp(); is_transition(); is_transversion(",MatchSource.WIKI,docs/0.2/functions/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/index.html
https://hail.is/docs/0.2/functions/index.html:3173,Testability,test,tests,3173,"orm(); pT(); pF(); ppois(); qchisqtail(); qnorm(); qpois(). Random functions; Setting a seed; Reproducibility across sessions. Genetics functions; locus(); locus_from_global_position(); locus_interval(); parse_locus(); parse_variant(); parse_locus_interval(); variant_str(); call(); unphased_diploid_gt_index_call(); parse_call(); downcode(); triangle(); is_snp(); is_mnp(); is_transition(); is_transversion(); is_insertion(); is_deletion(); is_indel(); is_star(); is_complex(); is_strand_ambiguous(); is_valid_contig(); is_valid_locus(); contig_length(); allele_type(); numeric_allele_type(); pl_dosage(); gp_dosage(); get_sequence(); mendel_error_code(); liftover(); min_rep(); reverse_complement(). Core language functions. literal(x[, dtype]); Captures and broadcasts a Python variable or object as an expression. cond(condition, consequent, alternate[, ...]); Deprecated in favor of if_else(). if_else(condition, consequent, alternate[, ...]); Expression for an if/else statement; tests a condition and returns one of two options based on the result. switch(expr); Build a conditional tree on the value of an expression. case([missing_false]); Chain multiple if-else statements with a CaseBuilder. bind(f, *exprs[, _ctx]); Bind a temporary variable and use it in a function. rbind(*exprs[, _ctx]); Bind a temporary variable and use it in a function. null(t); Deprecated in favor of missing(). is_missing(expression); Returns True if the argument is missing. is_defined(expression); Returns True if the argument is not missing. coalesce(*args); Returns the first non-missing value of args. or_else(a, b); If a is missing, return b. or_missing(predicate, value); Returns value if predicate is True, otherwise returns missing. range(start[, stop, step]); Returns an array of integers from start to stop by step. query_table(path, point_or_interval); Query records from a table corresponding to a given point or range of keys. Constructors. bool(x); Convert to a Boolean expression. float(x); Conver",MatchSource.WIKI,docs/0.2/functions/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/index.html
https://hail.is/docs/0.2/functions/index.html:6946,Testability,log,logical,6946,"a sorted array. find(f, collection); Returns the first element where f returns True. group_by(f, collection); Group collection elements into a dict according to a lambda function. fold(f, zero, collection); Reduces a collection with the given function f, provided the initial value zero. array_scan(f, zero, a); Map each element of a to cumulative value of function f, with initial value zero. reversed(x); Reverses the elements of a collection. keyed_intersection(*arrays, key); Compute the intersection of sorted arrays on a given key. keyed_union(*arrays, key); Compute the distinct union of sorted arrays on a given key. Numeric functions. abs(x); Take the absolute value of a numeric value, array or ndarray. approx_equal(x, y[, tolerance, absolute, ...]); Tests whether two numbers are approximately equal. bit_and(x, y); Bitwise and x and y. bit_or(x, y); Bitwise or x and y. bit_xor(x, y); Bitwise exclusive-or x and y. bit_lshift(x, y); Bitwise left-shift x by y. bit_rshift(x, y[, logical]); Bitwise right-shift x by y. bit_not(x); Bitwise invert x. bit_count(x); Count the number of 1s in the in the two's complement binary representation of x. exp(x). expit(x). is_nan(x). is_finite(x). is_infinite(x). log(x[, base]); Take the logarithm of the x with base base. log10(x). logit(x). sign(x); Returns the sign of a numeric value, array or ndarray. sqrt(x). int(x); Convert to a 32-bit integer expression. int32(x); Convert to a 32-bit integer expression. int64(x); Convert to a 64-bit integer expression. float(x); Convert to a 64-bit floating point expression. float32(x); Convert to a 32-bit floating point expression. float64(x); Convert to a 64-bit floating point expression. floor(x). ceil(x). uniroot(f, min, max, *[, max_iter, epsilon, ...]); Finds a root of the function f within the interval [min, max]. Numeric collection functions. min(*exprs[, filter_missing]); Returns the minimum element of a collection or of given numeric expressions. nanmin(*exprs[, filter_missing]); Retur",MatchSource.WIKI,docs/0.2/functions/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/index.html
https://hail.is/docs/0.2/functions/index.html:7170,Testability,log,log,7170,"unction f, provided the initial value zero. array_scan(f, zero, a); Map each element of a to cumulative value of function f, with initial value zero. reversed(x); Reverses the elements of a collection. keyed_intersection(*arrays, key); Compute the intersection of sorted arrays on a given key. keyed_union(*arrays, key); Compute the distinct union of sorted arrays on a given key. Numeric functions. abs(x); Take the absolute value of a numeric value, array or ndarray. approx_equal(x, y[, tolerance, absolute, ...]); Tests whether two numbers are approximately equal. bit_and(x, y); Bitwise and x and y. bit_or(x, y); Bitwise or x and y. bit_xor(x, y); Bitwise exclusive-or x and y. bit_lshift(x, y); Bitwise left-shift x by y. bit_rshift(x, y[, logical]); Bitwise right-shift x by y. bit_not(x); Bitwise invert x. bit_count(x); Count the number of 1s in the in the two's complement binary representation of x. exp(x). expit(x). is_nan(x). is_finite(x). is_infinite(x). log(x[, base]); Take the logarithm of the x with base base. log10(x). logit(x). sign(x); Returns the sign of a numeric value, array or ndarray. sqrt(x). int(x); Convert to a 32-bit integer expression. int32(x); Convert to a 32-bit integer expression. int64(x); Convert to a 64-bit integer expression. float(x); Convert to a 64-bit floating point expression. float32(x); Convert to a 32-bit floating point expression. float64(x); Convert to a 64-bit floating point expression. floor(x). ceil(x). uniroot(f, min, max, *[, max_iter, epsilon, ...]); Finds a root of the function f within the interval [min, max]. Numeric collection functions. min(*exprs[, filter_missing]); Returns the minimum element of a collection or of given numeric expressions. nanmin(*exprs[, filter_missing]); Returns the minimum value of a collection or of given arguments, excluding NaN. max(*exprs[, filter_missing]); Returns the maximum element of a collection or of given numeric expressions. nanmax(*exprs[, filter_missing]); Returns the maximum value ",MatchSource.WIKI,docs/0.2/functions/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/index.html
https://hail.is/docs/0.2/functions/index.html:7195,Testability,log,logarithm,7195,"unction f, provided the initial value zero. array_scan(f, zero, a); Map each element of a to cumulative value of function f, with initial value zero. reversed(x); Reverses the elements of a collection. keyed_intersection(*arrays, key); Compute the intersection of sorted arrays on a given key. keyed_union(*arrays, key); Compute the distinct union of sorted arrays on a given key. Numeric functions. abs(x); Take the absolute value of a numeric value, array or ndarray. approx_equal(x, y[, tolerance, absolute, ...]); Tests whether two numbers are approximately equal. bit_and(x, y); Bitwise and x and y. bit_or(x, y); Bitwise or x and y. bit_xor(x, y); Bitwise exclusive-or x and y. bit_lshift(x, y); Bitwise left-shift x by y. bit_rshift(x, y[, logical]); Bitwise right-shift x by y. bit_not(x); Bitwise invert x. bit_count(x); Count the number of 1s in the in the two's complement binary representation of x. exp(x). expit(x). is_nan(x). is_finite(x). is_infinite(x). log(x[, base]); Take the logarithm of the x with base base. log10(x). logit(x). sign(x); Returns the sign of a numeric value, array or ndarray. sqrt(x). int(x); Convert to a 32-bit integer expression. int32(x); Convert to a 32-bit integer expression. int64(x); Convert to a 64-bit integer expression. float(x); Convert to a 64-bit floating point expression. float32(x); Convert to a 32-bit floating point expression. float64(x); Convert to a 64-bit floating point expression. floor(x). ceil(x). uniroot(f, min, max, *[, max_iter, epsilon, ...]); Finds a root of the function f within the interval [min, max]. Numeric collection functions. min(*exprs[, filter_missing]); Returns the minimum element of a collection or of given numeric expressions. nanmin(*exprs[, filter_missing]); Returns the minimum value of a collection or of given arguments, excluding NaN. max(*exprs[, filter_missing]); Returns the maximum element of a collection or of given numeric expressions. nanmax(*exprs[, filter_missing]); Returns the maximum value ",MatchSource.WIKI,docs/0.2/functions/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/index.html
https://hail.is/docs/0.2/functions/index.html:7240,Testability,log,logit,7240,"rray_scan(f, zero, a); Map each element of a to cumulative value of function f, with initial value zero. reversed(x); Reverses the elements of a collection. keyed_intersection(*arrays, key); Compute the intersection of sorted arrays on a given key. keyed_union(*arrays, key); Compute the distinct union of sorted arrays on a given key. Numeric functions. abs(x); Take the absolute value of a numeric value, array or ndarray. approx_equal(x, y[, tolerance, absolute, ...]); Tests whether two numbers are approximately equal. bit_and(x, y); Bitwise and x and y. bit_or(x, y); Bitwise or x and y. bit_xor(x, y); Bitwise exclusive-or x and y. bit_lshift(x, y); Bitwise left-shift x by y. bit_rshift(x, y[, logical]); Bitwise right-shift x by y. bit_not(x); Bitwise invert x. bit_count(x); Count the number of 1s in the in the two's complement binary representation of x. exp(x). expit(x). is_nan(x). is_finite(x). is_infinite(x). log(x[, base]); Take the logarithm of the x with base base. log10(x). logit(x). sign(x); Returns the sign of a numeric value, array or ndarray. sqrt(x). int(x); Convert to a 32-bit integer expression. int32(x); Convert to a 32-bit integer expression. int64(x); Convert to a 64-bit integer expression. float(x); Convert to a 64-bit floating point expression. float32(x); Convert to a 32-bit floating point expression. float64(x); Convert to a 64-bit floating point expression. floor(x). ceil(x). uniroot(f, min, max, *[, max_iter, epsilon, ...]); Finds a root of the function f within the interval [min, max]. Numeric collection functions. min(*exprs[, filter_missing]); Returns the minimum element of a collection or of given numeric expressions. nanmin(*exprs[, filter_missing]); Returns the minimum value of a collection or of given arguments, excluding NaN. max(*exprs[, filter_missing]); Returns the maximum element of a collection or of given numeric expressions. nanmax(*exprs[, filter_missing]); Returns the maximum value of a collection or of given arguments, exclud",MatchSource.WIKI,docs/0.2/functions/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/index.html
https://hail.is/docs/0.2/functions/index.html:9925,Testability,test,test,9925,"em); Binary search array for the insertion point of elem. String functions. format(f, *args); Returns a formatted string using a specified format string and arguments. json(x); Convert an expression to a JSON string expression. parse_json(x, dtype); Convert a JSON string to a structured expression. hamming(s1, s2); Returns the Hamming distance between the two strings. delimit(collection[, delimiter]); Joins elements of collection into single string delimited by delimiter. entropy(s); Returns the Shannon entropy of the character distribution defined by the string. parse_int(x); Parse a string as a 32-bit integer. parse_int32(x); Parse a string as a 32-bit integer. parse_int64(x); Parse a string as a 64-bit integer. parse_float(x); Parse a string as a 64-bit floating point number. parse_float32(x); Parse a string as a 32-bit floating point number. parse_float64(x); Parse a string as a 64-bit floating point number. Statistical functions. chi_squared_test(c1, c2, c3, c4); Performs chi-squared test of independence on a 2x2 contingency table. fisher_exact_test(c1, c2, c3, c4); Calculates the p-value, odds ratio, and 95% confidence interval using Fisher's exact test for a 2x2 table. contingency_table_test(c1, c2, c3, c4, ...); Performs chi-squared or Fisher's exact test of independence on a 2x2 contingency table. cochran_mantel_haenszel_test(a, b, c, d); Perform the Cochran-Mantel-Haenszel test for association. dbeta(x, a, b); Returns the probability density at x of a beta distribution with parameters a (alpha) and b (beta). dpois(x, lamb[, log_p]); Compute the (log) probability density at x of a Poisson distribution with rate parameter lamb. hardy_weinberg_test(n_hom_ref, n_het, n_hom_var); Performs test of Hardy-Weinberg equilibrium. pchisqtail(x, df[, ncp, lower_tail, log_p]); Returns the probability under the right-tail starting at x for a chi-squared distribution with df degrees of freedom. pnorm(x[, mu, sigma, lower_tail, log_p]); The cumulative probability function ",MatchSource.WIKI,docs/0.2/functions/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/index.html
https://hail.is/docs/0.2/functions/index.html:10094,Testability,test,test,10094,"ing a specified format string and arguments. json(x); Convert an expression to a JSON string expression. parse_json(x, dtype); Convert a JSON string to a structured expression. hamming(s1, s2); Returns the Hamming distance between the two strings. delimit(collection[, delimiter]); Joins elements of collection into single string delimited by delimiter. entropy(s); Returns the Shannon entropy of the character distribution defined by the string. parse_int(x); Parse a string as a 32-bit integer. parse_int32(x); Parse a string as a 32-bit integer. parse_int64(x); Parse a string as a 64-bit integer. parse_float(x); Parse a string as a 64-bit floating point number. parse_float32(x); Parse a string as a 32-bit floating point number. parse_float64(x); Parse a string as a 64-bit floating point number. Statistical functions. chi_squared_test(c1, c2, c3, c4); Performs chi-squared test of independence on a 2x2 contingency table. fisher_exact_test(c1, c2, c3, c4); Calculates the p-value, odds ratio, and 95% confidence interval using Fisher's exact test for a 2x2 table. contingency_table_test(c1, c2, c3, c4, ...); Performs chi-squared or Fisher's exact test of independence on a 2x2 contingency table. cochran_mantel_haenszel_test(a, b, c, d); Perform the Cochran-Mantel-Haenszel test for association. dbeta(x, a, b); Returns the probability density at x of a beta distribution with parameters a (alpha) and b (beta). dpois(x, lamb[, log_p]); Compute the (log) probability density at x of a Poisson distribution with rate parameter lamb. hardy_weinberg_test(n_hom_ref, n_het, n_hom_var); Performs test of Hardy-Weinberg equilibrium. pchisqtail(x, df[, ncp, lower_tail, log_p]); Returns the probability under the right-tail starting at x for a chi-squared distribution with df degrees of freedom. pnorm(x[, mu, sigma, lower_tail, log_p]); The cumulative probability function of a normal distribution with mean mu and standard deviation sigma. ppois(x, lamb[, lower_tail, log_p]); The cumulative pro",MatchSource.WIKI,docs/0.2/functions/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/index.html
https://hail.is/docs/0.2/functions/index.html:10200,Testability,test,test,10200,"tured expression. hamming(s1, s2); Returns the Hamming distance between the two strings. delimit(collection[, delimiter]); Joins elements of collection into single string delimited by delimiter. entropy(s); Returns the Shannon entropy of the character distribution defined by the string. parse_int(x); Parse a string as a 32-bit integer. parse_int32(x); Parse a string as a 32-bit integer. parse_int64(x); Parse a string as a 64-bit integer. parse_float(x); Parse a string as a 64-bit floating point number. parse_float32(x); Parse a string as a 32-bit floating point number. parse_float64(x); Parse a string as a 64-bit floating point number. Statistical functions. chi_squared_test(c1, c2, c3, c4); Performs chi-squared test of independence on a 2x2 contingency table. fisher_exact_test(c1, c2, c3, c4); Calculates the p-value, odds ratio, and 95% confidence interval using Fisher's exact test for a 2x2 table. contingency_table_test(c1, c2, c3, c4, ...); Performs chi-squared or Fisher's exact test of independence on a 2x2 contingency table. cochran_mantel_haenszel_test(a, b, c, d); Perform the Cochran-Mantel-Haenszel test for association. dbeta(x, a, b); Returns the probability density at x of a beta distribution with parameters a (alpha) and b (beta). dpois(x, lamb[, log_p]); Compute the (log) probability density at x of a Poisson distribution with rate parameter lamb. hardy_weinberg_test(n_hom_ref, n_het, n_hom_var); Performs test of Hardy-Weinberg equilibrium. pchisqtail(x, df[, ncp, lower_tail, log_p]); Returns the probability under the right-tail starting at x for a chi-squared distribution with df degrees of freedom. pnorm(x[, mu, sigma, lower_tail, log_p]); The cumulative probability function of a normal distribution with mean mu and standard deviation sigma. ppois(x, lamb[, lower_tail, log_p]); The cumulative probability function of a Poisson distribution. qchisqtail(p, df[, ncp, lower_tail, log_p]); The quantile function of a chi-squared distribution with df degrees o",MatchSource.WIKI,docs/0.2/functions/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/index.html
https://hail.is/docs/0.2/functions/index.html:10327,Testability,test,test,10327,"t(collection[, delimiter]); Joins elements of collection into single string delimited by delimiter. entropy(s); Returns the Shannon entropy of the character distribution defined by the string. parse_int(x); Parse a string as a 32-bit integer. parse_int32(x); Parse a string as a 32-bit integer. parse_int64(x); Parse a string as a 64-bit integer. parse_float(x); Parse a string as a 64-bit floating point number. parse_float32(x); Parse a string as a 32-bit floating point number. parse_float64(x); Parse a string as a 64-bit floating point number. Statistical functions. chi_squared_test(c1, c2, c3, c4); Performs chi-squared test of independence on a 2x2 contingency table. fisher_exact_test(c1, c2, c3, c4); Calculates the p-value, odds ratio, and 95% confidence interval using Fisher's exact test for a 2x2 table. contingency_table_test(c1, c2, c3, c4, ...); Performs chi-squared or Fisher's exact test of independence on a 2x2 contingency table. cochran_mantel_haenszel_test(a, b, c, d); Perform the Cochran-Mantel-Haenszel test for association. dbeta(x, a, b); Returns the probability density at x of a beta distribution with parameters a (alpha) and b (beta). dpois(x, lamb[, log_p]); Compute the (log) probability density at x of a Poisson distribution with rate parameter lamb. hardy_weinberg_test(n_hom_ref, n_het, n_hom_var); Performs test of Hardy-Weinberg equilibrium. pchisqtail(x, df[, ncp, lower_tail, log_p]); Returns the probability under the right-tail starting at x for a chi-squared distribution with df degrees of freedom. pnorm(x[, mu, sigma, lower_tail, log_p]); The cumulative probability function of a normal distribution with mean mu and standard deviation sigma. ppois(x, lamb[, lower_tail, log_p]); The cumulative probability function of a Poisson distribution. qchisqtail(p, df[, ncp, lower_tail, log_p]); The quantile function of a chi-squared distribution with df degrees of freedom, inverts pchisqtail(). qnorm(p[, mu, sigma, lower_tail, log_p]); The quantile functio",MatchSource.WIKI,docs/0.2/functions/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/index.html
https://hail.is/docs/0.2/functions/index.html:10503,Testability,log,log,10503," 32-bit integer. parse_int32(x); Parse a string as a 32-bit integer. parse_int64(x); Parse a string as a 64-bit integer. parse_float(x); Parse a string as a 64-bit floating point number. parse_float32(x); Parse a string as a 32-bit floating point number. parse_float64(x); Parse a string as a 64-bit floating point number. Statistical functions. chi_squared_test(c1, c2, c3, c4); Performs chi-squared test of independence on a 2x2 contingency table. fisher_exact_test(c1, c2, c3, c4); Calculates the p-value, odds ratio, and 95% confidence interval using Fisher's exact test for a 2x2 table. contingency_table_test(c1, c2, c3, c4, ...); Performs chi-squared or Fisher's exact test of independence on a 2x2 contingency table. cochran_mantel_haenszel_test(a, b, c, d); Perform the Cochran-Mantel-Haenszel test for association. dbeta(x, a, b); Returns the probability density at x of a beta distribution with parameters a (alpha) and b (beta). dpois(x, lamb[, log_p]); Compute the (log) probability density at x of a Poisson distribution with rate parameter lamb. hardy_weinberg_test(n_hom_ref, n_het, n_hom_var); Performs test of Hardy-Weinberg equilibrium. pchisqtail(x, df[, ncp, lower_tail, log_p]); Returns the probability under the right-tail starting at x for a chi-squared distribution with df degrees of freedom. pnorm(x[, mu, sigma, lower_tail, log_p]); The cumulative probability function of a normal distribution with mean mu and standard deviation sigma. ppois(x, lamb[, lower_tail, log_p]); The cumulative probability function of a Poisson distribution. qchisqtail(p, df[, ncp, lower_tail, log_p]); The quantile function of a chi-squared distribution with df degrees of freedom, inverts pchisqtail(). qnorm(p[, mu, sigma, lower_tail, log_p]); The quantile function of a normal distribution with mean mu and standard deviation sigma, inverts pnorm(). qpois(p, lamb[, lower_tail, log_p]); The quantile function of a Poisson distribution with rate parameter lamb, inverts ppois(). Randomness.",MatchSource.WIKI,docs/0.2/functions/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/index.html
https://hail.is/docs/0.2/functions/index.html:10644,Testability,test,test,10644,"-bit integer. parse_float(x); Parse a string as a 64-bit floating point number. parse_float32(x); Parse a string as a 32-bit floating point number. parse_float64(x); Parse a string as a 64-bit floating point number. Statistical functions. chi_squared_test(c1, c2, c3, c4); Performs chi-squared test of independence on a 2x2 contingency table. fisher_exact_test(c1, c2, c3, c4); Calculates the p-value, odds ratio, and 95% confidence interval using Fisher's exact test for a 2x2 table. contingency_table_test(c1, c2, c3, c4, ...); Performs chi-squared or Fisher's exact test of independence on a 2x2 contingency table. cochran_mantel_haenszel_test(a, b, c, d); Perform the Cochran-Mantel-Haenszel test for association. dbeta(x, a, b); Returns the probability density at x of a beta distribution with parameters a (alpha) and b (beta). dpois(x, lamb[, log_p]); Compute the (log) probability density at x of a Poisson distribution with rate parameter lamb. hardy_weinberg_test(n_hom_ref, n_het, n_hom_var); Performs test of Hardy-Weinberg equilibrium. pchisqtail(x, df[, ncp, lower_tail, log_p]); Returns the probability under the right-tail starting at x for a chi-squared distribution with df degrees of freedom. pnorm(x[, mu, sigma, lower_tail, log_p]); The cumulative probability function of a normal distribution with mean mu and standard deviation sigma. ppois(x, lamb[, lower_tail, log_p]); The cumulative probability function of a Poisson distribution. qchisqtail(p, df[, ncp, lower_tail, log_p]); The quantile function of a chi-squared distribution with df degrees of freedom, inverts pchisqtail(). qnorm(p[, mu, sigma, lower_tail, log_p]); The quantile function of a normal distribution with mean mu and standard deviation sigma, inverts pnorm(). qpois(p, lamb[, lower_tail, log_p]); The quantile function of a Poisson distribution with rate parameter lamb, inverts ppois(). Randomness. rand_bool(p[, seed]); Returns True with probability p. rand_beta(a, b[, lower, upper, seed]); Samples from ",MatchSource.WIKI,docs/0.2/functions/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/index.html
https://hail.is/docs/0.2/functions/numeric.html:826,Availability,toler,tolerance,826,"﻿. Hail | ; Numeric functions. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Functions; Numeric functions. View page source. Numeric functions; Numeric functions. abs(x); Take the absolute value of a numeric value, array or ndarray. approx_equal(x, y[, tolerance, absolute, ...]); Tests whether two numbers are approximately equal. bit_and(x, y); Bitwise and x and y. bit_or(x, y); Bitwise or x and y. bit_xor(x, y); Bitwise exclusive-or x and y. bit_lshift(x, y); Bitwise left-shift x by y. bit_rshift(x, y[, logical]); Bitwise right-shift x by y. bit_not(x); Bitwise invert x. bit_count(x); Count the number of 1s in the in the two's complement binary representation of x. exp(x). expit(x). is_nan(x). is_finite(x). is_infinite(x). log(x[, base]); Take the logarithm of the x with base base. log10(x). logit(x). sign(x); Returns the sign of a numeric value, array or ndarray. sqrt(x). int(x); Convert to a 32-bit integer expression. int32(x); Convert to a 32-bit integer expression. int64(x); Convert to a 64-bit integer expression. float(x); Convert to a 64-bit floating point expression. float32(x); Convert to a 32-bit floating point expression. float64(x); Convert to a 64-bit floating point expression. floor(x). ceil(x). uniroot(f, min, max, *[, max_iter, epsilon, ...]); Finds a root of the function f within the interval [min, max]. Numeric collection functions. min(*exprs[, filter_missing]); Returns the minimum elem",MatchSource.WIKI,docs/0.2/functions/numeric.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/numeric.html
https://hail.is/docs/0.2/functions/numeric.html:3519,Availability,toler,tolerance,3519,"ter_missing]); Returns the product of values in the collection. sum(collection[, filter_missing]); Returns the sum of values in the collection. cumulative_sum(a[, filter_missing]); Returns an array of the cumulative sum of values in the array. argmin(array[, unique]); Return the index of the minimum value in the array. argmax(array[, unique]); Return the index of the maximum value in the array. corr(x, y); Compute the Pearson correlation coefficient between x and y. binary_search(array, elem); Binary search array for the insertion point of elem. hail.expr.functions.abs(x)[source]; Take the absolute value of a numeric value, array or ndarray.; Examples; >>> hl.eval(hl.abs(-5)); 5. >>> hl.eval(hl.abs([1.0, -2.5, -5.1])); [1.0, 2.5, 5.1]. Parameters:; x (NumericExpression, ArrayNumericExpression or NDArrayNumericExpression). Returns:; NumericExpression, ArrayNumericExpression or NDArrayNumericExpression. hail.expr.functions.approx_equal(x, y, tolerance=1e-06, absolute=False, nan_same=False)[source]; Tests whether two numbers are approximately equal.; Examples; >>> hl.eval(hl.approx_equal(0.25, 0.2500001)); True. >>> hl.eval(hl.approx_equal(0.25, 0.251, tolerance=1e-3, absolute=True)); False. Parameters:. x (NumericExpression); y (NumericExpression); tolerance (NumericExpression); absolute (BooleanExpression) – If True, compute abs(x - y) <= tolerance. Otherwise, compute; abs(x - y) <= max(tolerance * max(abs(x), abs(y)), 2 ** -1022).; nan_same (BooleanExpression) – If True, then NaN == NaN will evaluate to True. Otherwise,; it will return False. Returns:; BooleanExpression. hail.expr.functions.bit_and(x, y)[source]; Bitwise and x and y.; Examples; >>> hl.eval(hl.bit_and(5, 3)); 1. Notes; See the Python wiki; for more information about bit operators. Parameters:. x (Int32Expression or Int64Expression); y (Int32Expression or Int64Expression). Returns:; Int32Expression or Int64Expression. hail.expr.functions.bit_or(x, y)[source]; Bitwise or x and y.; Examples; >>> hl.",MatchSource.WIKI,docs/0.2/functions/numeric.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/numeric.html
https://hail.is/docs/0.2/functions/numeric.html:3734,Availability,toler,tolerance,3734,"s an array of the cumulative sum of values in the array. argmin(array[, unique]); Return the index of the minimum value in the array. argmax(array[, unique]); Return the index of the maximum value in the array. corr(x, y); Compute the Pearson correlation coefficient between x and y. binary_search(array, elem); Binary search array for the insertion point of elem. hail.expr.functions.abs(x)[source]; Take the absolute value of a numeric value, array or ndarray.; Examples; >>> hl.eval(hl.abs(-5)); 5. >>> hl.eval(hl.abs([1.0, -2.5, -5.1])); [1.0, 2.5, 5.1]. Parameters:; x (NumericExpression, ArrayNumericExpression or NDArrayNumericExpression). Returns:; NumericExpression, ArrayNumericExpression or NDArrayNumericExpression. hail.expr.functions.approx_equal(x, y, tolerance=1e-06, absolute=False, nan_same=False)[source]; Tests whether two numbers are approximately equal.; Examples; >>> hl.eval(hl.approx_equal(0.25, 0.2500001)); True. >>> hl.eval(hl.approx_equal(0.25, 0.251, tolerance=1e-3, absolute=True)); False. Parameters:. x (NumericExpression); y (NumericExpression); tolerance (NumericExpression); absolute (BooleanExpression) – If True, compute abs(x - y) <= tolerance. Otherwise, compute; abs(x - y) <= max(tolerance * max(abs(x), abs(y)), 2 ** -1022).; nan_same (BooleanExpression) – If True, then NaN == NaN will evaluate to True. Otherwise,; it will return False. Returns:; BooleanExpression. hail.expr.functions.bit_and(x, y)[source]; Bitwise and x and y.; Examples; >>> hl.eval(hl.bit_and(5, 3)); 1. Notes; See the Python wiki; for more information about bit operators. Parameters:. x (Int32Expression or Int64Expression); y (Int32Expression or Int64Expression). Returns:; Int32Expression or Int64Expression. hail.expr.functions.bit_or(x, y)[source]; Bitwise or x and y.; Examples; >>> hl.eval(hl.bit_or(5, 3)); 7. Notes; See the Python wiki; for more information about bit operators. Parameters:. x (Int32Expression or Int64Expression); y (Int32Expression or Int64Expression)",MatchSource.WIKI,docs/0.2/functions/numeric.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/numeric.html
https://hail.is/docs/0.2/functions/numeric.html:3833,Availability,toler,tolerance,3833,"mum value in the array. argmax(array[, unique]); Return the index of the maximum value in the array. corr(x, y); Compute the Pearson correlation coefficient between x and y. binary_search(array, elem); Binary search array for the insertion point of elem. hail.expr.functions.abs(x)[source]; Take the absolute value of a numeric value, array or ndarray.; Examples; >>> hl.eval(hl.abs(-5)); 5. >>> hl.eval(hl.abs([1.0, -2.5, -5.1])); [1.0, 2.5, 5.1]. Parameters:; x (NumericExpression, ArrayNumericExpression or NDArrayNumericExpression). Returns:; NumericExpression, ArrayNumericExpression or NDArrayNumericExpression. hail.expr.functions.approx_equal(x, y, tolerance=1e-06, absolute=False, nan_same=False)[source]; Tests whether two numbers are approximately equal.; Examples; >>> hl.eval(hl.approx_equal(0.25, 0.2500001)); True. >>> hl.eval(hl.approx_equal(0.25, 0.251, tolerance=1e-3, absolute=True)); False. Parameters:. x (NumericExpression); y (NumericExpression); tolerance (NumericExpression); absolute (BooleanExpression) – If True, compute abs(x - y) <= tolerance. Otherwise, compute; abs(x - y) <= max(tolerance * max(abs(x), abs(y)), 2 ** -1022).; nan_same (BooleanExpression) – If True, then NaN == NaN will evaluate to True. Otherwise,; it will return False. Returns:; BooleanExpression. hail.expr.functions.bit_and(x, y)[source]; Bitwise and x and y.; Examples; >>> hl.eval(hl.bit_and(5, 3)); 1. Notes; See the Python wiki; for more information about bit operators. Parameters:. x (Int32Expression or Int64Expression); y (Int32Expression or Int64Expression). Returns:; Int32Expression or Int64Expression. hail.expr.functions.bit_or(x, y)[source]; Bitwise or x and y.; Examples; >>> hl.eval(hl.bit_or(5, 3)); 7. Notes; See the Python wiki; for more information about bit operators. Parameters:. x (Int32Expression or Int64Expression); y (Int32Expression or Int64Expression). Returns:; Int32Expression or Int64Expression. hail.expr.functions.bit_xor(x, y)[source]; Bitwise exclusive-",MatchSource.WIKI,docs/0.2/functions/numeric.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/numeric.html
https://hail.is/docs/0.2/functions/numeric.html:3926,Availability,toler,tolerance,3926,"mum value in the array. argmax(array[, unique]); Return the index of the maximum value in the array. corr(x, y); Compute the Pearson correlation coefficient between x and y. binary_search(array, elem); Binary search array for the insertion point of elem. hail.expr.functions.abs(x)[source]; Take the absolute value of a numeric value, array or ndarray.; Examples; >>> hl.eval(hl.abs(-5)); 5. >>> hl.eval(hl.abs([1.0, -2.5, -5.1])); [1.0, 2.5, 5.1]. Parameters:; x (NumericExpression, ArrayNumericExpression or NDArrayNumericExpression). Returns:; NumericExpression, ArrayNumericExpression or NDArrayNumericExpression. hail.expr.functions.approx_equal(x, y, tolerance=1e-06, absolute=False, nan_same=False)[source]; Tests whether two numbers are approximately equal.; Examples; >>> hl.eval(hl.approx_equal(0.25, 0.2500001)); True. >>> hl.eval(hl.approx_equal(0.25, 0.251, tolerance=1e-3, absolute=True)); False. Parameters:. x (NumericExpression); y (NumericExpression); tolerance (NumericExpression); absolute (BooleanExpression) – If True, compute abs(x - y) <= tolerance. Otherwise, compute; abs(x - y) <= max(tolerance * max(abs(x), abs(y)), 2 ** -1022).; nan_same (BooleanExpression) – If True, then NaN == NaN will evaluate to True. Otherwise,; it will return False. Returns:; BooleanExpression. hail.expr.functions.bit_and(x, y)[source]; Bitwise and x and y.; Examples; >>> hl.eval(hl.bit_and(5, 3)); 1. Notes; See the Python wiki; for more information about bit operators. Parameters:. x (Int32Expression or Int64Expression); y (Int32Expression or Int64Expression). Returns:; Int32Expression or Int64Expression. hail.expr.functions.bit_or(x, y)[source]; Bitwise or x and y.; Examples; >>> hl.eval(hl.bit_or(5, 3)); 7. Notes; See the Python wiki; for more information about bit operators. Parameters:. x (Int32Expression or Int64Expression); y (Int32Expression or Int64Expression). Returns:; Int32Expression or Int64Expression. hail.expr.functions.bit_xor(x, y)[source]; Bitwise exclusive-",MatchSource.WIKI,docs/0.2/functions/numeric.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/numeric.html
https://hail.is/docs/0.2/functions/numeric.html:3975,Availability,toler,tolerance,3975,"ute the Pearson correlation coefficient between x and y. binary_search(array, elem); Binary search array for the insertion point of elem. hail.expr.functions.abs(x)[source]; Take the absolute value of a numeric value, array or ndarray.; Examples; >>> hl.eval(hl.abs(-5)); 5. >>> hl.eval(hl.abs([1.0, -2.5, -5.1])); [1.0, 2.5, 5.1]. Parameters:; x (NumericExpression, ArrayNumericExpression or NDArrayNumericExpression). Returns:; NumericExpression, ArrayNumericExpression or NDArrayNumericExpression. hail.expr.functions.approx_equal(x, y, tolerance=1e-06, absolute=False, nan_same=False)[source]; Tests whether two numbers are approximately equal.; Examples; >>> hl.eval(hl.approx_equal(0.25, 0.2500001)); True. >>> hl.eval(hl.approx_equal(0.25, 0.251, tolerance=1e-3, absolute=True)); False. Parameters:. x (NumericExpression); y (NumericExpression); tolerance (NumericExpression); absolute (BooleanExpression) – If True, compute abs(x - y) <= tolerance. Otherwise, compute; abs(x - y) <= max(tolerance * max(abs(x), abs(y)), 2 ** -1022).; nan_same (BooleanExpression) – If True, then NaN == NaN will evaluate to True. Otherwise,; it will return False. Returns:; BooleanExpression. hail.expr.functions.bit_and(x, y)[source]; Bitwise and x and y.; Examples; >>> hl.eval(hl.bit_and(5, 3)); 1. Notes; See the Python wiki; for more information about bit operators. Parameters:. x (Int32Expression or Int64Expression); y (Int32Expression or Int64Expression). Returns:; Int32Expression or Int64Expression. hail.expr.functions.bit_or(x, y)[source]; Bitwise or x and y.; Examples; >>> hl.eval(hl.bit_or(5, 3)); 7. Notes; See the Python wiki; for more information about bit operators. Parameters:. x (Int32Expression or Int64Expression); y (Int32Expression or Int64Expression). Returns:; Int32Expression or Int64Expression. hail.expr.functions.bit_xor(x, y)[source]; Bitwise exclusive-or x and y.; Examples; >>> hl.eval(hl.bit_xor(5, 3)); 6. Notes; See the Python wiki; for more information about bit o",MatchSource.WIKI,docs/0.2/functions/numeric.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/numeric.html
https://hail.is/docs/0.2/functions/numeric.html:17484,Availability,toler,tolerance,17484,"); None. Notes; Returns the index of the maximum value in the array.; If two or more elements are tied for maximum, then the unique parameter; will determine the result. If unique is False, then the first index; will be returned. If unique is True, then the result is missing.; If the array is empty, then the result is missing. Note; Missing elements are ignored. Parameters:. array (ArrayNumericExpression); unique (bool). Returns:; Expression of type tint32. hail.expr.functions.corr(x, y)[source]; Compute the; Pearson correlation coefficient; between x and y.; Examples; >>> hl.eval(hl.corr([1, 2, 4], [2, 3, 1])); -0.6546536707079772. Notes; Only indices where both x and y are non-missing will be included in the; calculation.; If x and y have length zero, then the result is missing. Parameters:. x (Expression of type array<tfloat64>); y (Expression of type array<tfloat64>). Returns:; Float64Expression. hail.expr.functions.uniroot(f, min, max, *, max_iter=1000, epsilon=2.220446049250313e-16, tolerance=0.0001220703)[source]; Finds a root of the function f within the interval [min, max].; Examples; >>> hl.eval(hl.uniroot(lambda x: x - 1, -5, 5)); 1.0. Notes; f(min) and f(max) must not have the same sign.; If no root can be found, the result of this call will be NA (missing).; uniroot() returns an estimate for a root with accuracy; 4 * epsilon * abs(x) + tolerance.; 4*EPSILON*abs(x) + tol. Parameters:. f (function ( (arg) -> Float64Expression)) – Must return a Float64Expression.; min (Float64Expression); max (Float64Expression); max_iter (int) – The maximum number of iterations before giving up.; epsilon (float) – The scaling factor in the accuracy of the root found.; tolerance (float) – The constant factor in approximate accuracy of the root found. Returns:; Float64Expression – The root of the function f. hail.expr.functions.binary_search(array, elem)[source]; Binary search array for the insertion point of elem. Parameters:. array (Expression of type tarray); elem (Exp",MatchSource.WIKI,docs/0.2/functions/numeric.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/numeric.html
https://hail.is/docs/0.2/functions/numeric.html:17852,Availability,toler,tolerance,17852,"ssing elements are ignored. Parameters:. array (ArrayNumericExpression); unique (bool). Returns:; Expression of type tint32. hail.expr.functions.corr(x, y)[source]; Compute the; Pearson correlation coefficient; between x and y.; Examples; >>> hl.eval(hl.corr([1, 2, 4], [2, 3, 1])); -0.6546536707079772. Notes; Only indices where both x and y are non-missing will be included in the; calculation.; If x and y have length zero, then the result is missing. Parameters:. x (Expression of type array<tfloat64>); y (Expression of type array<tfloat64>). Returns:; Float64Expression. hail.expr.functions.uniroot(f, min, max, *, max_iter=1000, epsilon=2.220446049250313e-16, tolerance=0.0001220703)[source]; Finds a root of the function f within the interval [min, max].; Examples; >>> hl.eval(hl.uniroot(lambda x: x - 1, -5, 5)); 1.0. Notes; f(min) and f(max) must not have the same sign.; If no root can be found, the result of this call will be NA (missing).; uniroot() returns an estimate for a root with accuracy; 4 * epsilon * abs(x) + tolerance.; 4*EPSILON*abs(x) + tol. Parameters:. f (function ( (arg) -> Float64Expression)) – Must return a Float64Expression.; min (Float64Expression); max (Float64Expression); max_iter (int) – The maximum number of iterations before giving up.; epsilon (float) – The scaling factor in the accuracy of the root found.; tolerance (float) – The constant factor in approximate accuracy of the root found. Returns:; Float64Expression – The root of the function f. hail.expr.functions.binary_search(array, elem)[source]; Binary search array for the insertion point of elem. Parameters:. array (Expression of type tarray); elem (Expression). Returns:; Int32Expression. Notes; This function assumes that array is sorted in ascending order, and does; not perform any sortedness check. Missing values sort last.; The returned index is the lower bound on the insertion point of elem into; the ordered array, or the index of the first element in array not smaller; than elem",MatchSource.WIKI,docs/0.2/functions/numeric.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/numeric.html
https://hail.is/docs/0.2/functions/numeric.html:18172,Availability,toler,tolerance,18172,".; If x and y have length zero, then the result is missing. Parameters:. x (Expression of type array<tfloat64>); y (Expression of type array<tfloat64>). Returns:; Float64Expression. hail.expr.functions.uniroot(f, min, max, *, max_iter=1000, epsilon=2.220446049250313e-16, tolerance=0.0001220703)[source]; Finds a root of the function f within the interval [min, max].; Examples; >>> hl.eval(hl.uniroot(lambda x: x - 1, -5, 5)); 1.0. Notes; f(min) and f(max) must not have the same sign.; If no root can be found, the result of this call will be NA (missing).; uniroot() returns an estimate for a root with accuracy; 4 * epsilon * abs(x) + tolerance.; 4*EPSILON*abs(x) + tol. Parameters:. f (function ( (arg) -> Float64Expression)) – Must return a Float64Expression.; min (Float64Expression); max (Float64Expression); max_iter (int) – The maximum number of iterations before giving up.; epsilon (float) – The scaling factor in the accuracy of the root found.; tolerance (float) – The constant factor in approximate accuracy of the root found. Returns:; Float64Expression – The root of the function f. hail.expr.functions.binary_search(array, elem)[source]; Binary search array for the insertion point of elem. Parameters:. array (Expression of type tarray); elem (Expression). Returns:; Int32Expression. Notes; This function assumes that array is sorted in ascending order, and does; not perform any sortedness check. Missing values sort last.; The returned index is the lower bound on the insertion point of elem into; the ordered array, or the index of the first element in array not smaller; than elem. This is a value between 0 and the length of array, inclusive; (if all elements in array are smaller than elem, the returned value is; the length of array or the index of the first missing value, if one; exists).; If either elem or array is missing, the result is missing.; Examples; >>> a = hl.array([0, 2, 4, 8]). >>> hl.eval(hl.binary_search(a, -1)); 0. >>> hl.eval(hl.binary_search(a, 1)); ",MatchSource.WIKI,docs/0.2/functions/numeric.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/numeric.html
https://hail.is/docs/0.2/functions/numeric.html:19313,Deployability,update,updated,19313," type array<tfloat64>). Returns:; Float64Expression. hail.expr.functions.uniroot(f, min, max, *, max_iter=1000, epsilon=2.220446049250313e-16, tolerance=0.0001220703)[source]; Finds a root of the function f within the interval [min, max].; Examples; >>> hl.eval(hl.uniroot(lambda x: x - 1, -5, 5)); 1.0. Notes; f(min) and f(max) must not have the same sign.; If no root can be found, the result of this call will be NA (missing).; uniroot() returns an estimate for a root with accuracy; 4 * epsilon * abs(x) + tolerance.; 4*EPSILON*abs(x) + tol. Parameters:. f (function ( (arg) -> Float64Expression)) – Must return a Float64Expression.; min (Float64Expression); max (Float64Expression); max_iter (int) – The maximum number of iterations before giving up.; epsilon (float) – The scaling factor in the accuracy of the root found.; tolerance (float) – The constant factor in approximate accuracy of the root found. Returns:; Float64Expression – The root of the function f. hail.expr.functions.binary_search(array, elem)[source]; Binary search array for the insertion point of elem. Parameters:. array (Expression of type tarray); elem (Expression). Returns:; Int32Expression. Notes; This function assumes that array is sorted in ascending order, and does; not perform any sortedness check. Missing values sort last.; The returned index is the lower bound on the insertion point of elem into; the ordered array, or the index of the first element in array not smaller; than elem. This is a value between 0 and the length of array, inclusive; (if all elements in array are smaller than elem, the returned value is; the length of array or the index of the first missing value, if one; exists).; If either elem or array is missing, the result is missing.; Examples; >>> a = hl.array([0, 2, 4, 8]). >>> hl.eval(hl.binary_search(a, -1)); 0. >>> hl.eval(hl.binary_search(a, 1)); 1. >>> hl.eval(hl.binary_search(a, 10)); 4. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/functions/numeric.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/numeric.html
https://hail.is/docs/0.2/functions/numeric.html:5360,Modifiability,extend,extended,5360,"Parameters:. x (Int32Expression or Int64Expression); y (Int32Expression or Int64Expression). Returns:; Int32Expression or Int64Expression. hail.expr.functions.bit_or(x, y)[source]; Bitwise or x and y.; Examples; >>> hl.eval(hl.bit_or(5, 3)); 7. Notes; See the Python wiki; for more information about bit operators. Parameters:. x (Int32Expression or Int64Expression); y (Int32Expression or Int64Expression). Returns:; Int32Expression or Int64Expression. hail.expr.functions.bit_xor(x, y)[source]; Bitwise exclusive-or x and y.; Examples; >>> hl.eval(hl.bit_xor(5, 3)); 6. Notes; See the Python wiki; for more information about bit operators. Parameters:. x (Int32Expression or Int64Expression); y (Int32Expression or Int64Expression). Returns:; Int32Expression or Int64Expression. hail.expr.functions.bit_lshift(x, y)[source]; Bitwise left-shift x by y.; Examples; >>> hl.eval(hl.bit_lshift(5, 3)); 40. >>> hl.eval(hl.bit_lshift(1, 8)); 256. Unlike Python, Hail integers are fixed-size (32 or 64 bits),; and bits extended beyond will be ignored:; >>> hl.eval(hl.bit_lshift(1, 31)); -2147483648. >>> hl.eval(hl.bit_lshift(1, 32)); 0. >>> hl.eval(hl.bit_lshift(hl.int64(1), 32)); 4294967296. >>> hl.eval(hl.bit_lshift(hl.int64(1), 64)); 0. Notes; See the Python wiki; for more information about bit operators. Parameters:. x (Int32Expression or Int64Expression); y (Int32Expression or Int64Expression). Returns:; Int32Expression or Int64Expression. hail.expr.functions.bit_rshift(x, y, logical=False)[source]; Bitwise right-shift x by y.; Examples; >>> hl.eval(hl.bit_rshift(256, 3)); 32. With logical=False (default), the sign is preserved:; >>> hl.eval(hl.bit_rshift(-1, 1)); -1. With logical=True, the sign bit is treated as any other:; >>> hl.eval(hl.bit_rshift(-1, 1, logical=True)); 2147483647. Notes; If logical is False, then the shift is a sign-preserving right shift.; If logical is True, then the shift is logical, with the sign bit; treated as any other bit.; See the Python wiki; for mo",MatchSource.WIKI,docs/0.2/functions/numeric.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/numeric.html
https://hail.is/docs/0.2/functions/numeric.html:8889,Modifiability,variab,variable-length,8889,"floor(x)[source]. hail.expr.functions.ceil(x)[source]. hail.expr.functions.sqrt(x)[source]. hail.expr.functions.sign(x)[source]; Returns the sign of a numeric value, array or ndarray.; Examples; >>> hl.eval(hl.sign(-1.23)); -1.0. >>> hl.eval(hl.sign([-4, 0, 5])); [-1, 0, 1]. >>> hl.eval(hl.sign([0.0, 3.14])); [0.0, 1.0]. >>> hl.eval(hl.sign(float('nan'))); nan. Notes; The sign function preserves type and maps nan to nan. Parameters:; x (NumericExpression, ArrayNumericExpression or NDArrayNumericExpression). Returns:; NumericExpression, ArrayNumericExpression or NDArrayNumericExpression. hail.expr.functions.min(*exprs, filter_missing=True)[source]; Returns the minimum element of a collection or of given numeric expressions.; Examples; Take the minimum value of an array:; >>> hl.eval(hl.min([1, 3, 5, 6, 7, 9])); 1. Take the minimum value of arguments:; >>> hl.eval(hl.min(1, 50, 2)); 1. Notes; Like the Python builtin min function, this function can either take a; single iterable expression (an array or set of numeric elements), or; variable-length arguments of numeric expressions. Note; If filter_missing is True, then the result is the minimum of; non-missing arguments or elements. If filter_missing is False, then; any missing argument or element causes the result to be missing.; If any element or argument is NaN, then the result is NaN. See also; nanmin(), max(), nanmax(). Parameters:. exprs (ArrayExpression or SetExpression or varargs of NumericExpression) – Single numeric array or set, or multiple numeric values.; filter_missing (bool) – Remove missing arguments/elements before computing minimum. Returns:; NumericExpression. hail.expr.functions.nanmin(*exprs, filter_missing=True)[source]; Returns the minimum value of a collection or of given arguments, excluding NaN.; Examples; Compute the minimum value of an array:; >>> hl.eval(hl.nanmin([1.1, 50.1, float('nan')])); 1.1. Take the minimum value of arguments:; >>> hl.eval(hl.nanmin(1.1, 50.1, float('nan'))); 1.",MatchSource.WIKI,docs/0.2/functions/numeric.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/numeric.html
https://hail.is/docs/0.2/functions/numeric.html:9990,Modifiability,variab,variable-length,9990,"ote; If filter_missing is True, then the result is the minimum of; non-missing arguments or elements. If filter_missing is False, then; any missing argument or element causes the result to be missing.; If any element or argument is NaN, then the result is NaN. See also; nanmin(), max(), nanmax(). Parameters:. exprs (ArrayExpression or SetExpression or varargs of NumericExpression) – Single numeric array or set, or multiple numeric values.; filter_missing (bool) – Remove missing arguments/elements before computing minimum. Returns:; NumericExpression. hail.expr.functions.nanmin(*exprs, filter_missing=True)[source]; Returns the minimum value of a collection or of given arguments, excluding NaN.; Examples; Compute the minimum value of an array:; >>> hl.eval(hl.nanmin([1.1, 50.1, float('nan')])); 1.1. Take the minimum value of arguments:; >>> hl.eval(hl.nanmin(1.1, 50.1, float('nan'))); 1.1. Notes; Like the Python builtin min function, this function can either take a; single iterable expression (an array or set of numeric elements), or; variable-length arguments of numeric expressions. Note; If filter_missing is True, then the result is the minimum of; non-missing arguments or elements. If filter_missing is False, then; any missing argument or element causes the result to be missing.; NaN arguments / array elements are ignored; the minimum value of NaN and; any non-NaN value x is x. See also; min(), max(), nanmax(). Parameters:. exprs (ArrayExpression or SetExpression or varargs of NumericExpression) – Single numeric array or set, or multiple numeric values.; filter_missing (bool) – Remove missing arguments/elements before computing minimum. Returns:; NumericExpression. hail.expr.functions.max(*exprs, filter_missing=True)[source]; Returns the maximum element of a collection or of given numeric expressions.; Examples; Take the maximum value of an array:; >>> hl.eval(hl.max([1, 3, 5, 6, 7, 9])); 9. Take the maximum value of values:; >>> hl.eval(hl.max(1, 50, 2)); 50. No",MatchSource.WIKI,docs/0.2/functions/numeric.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/numeric.html
https://hail.is/docs/0.2/functions/numeric.html:11086,Modifiability,variab,variable-length,11086,"ns. Note; If filter_missing is True, then the result is the minimum of; non-missing arguments or elements. If filter_missing is False, then; any missing argument or element causes the result to be missing.; NaN arguments / array elements are ignored; the minimum value of NaN and; any non-NaN value x is x. See also; min(), max(), nanmax(). Parameters:. exprs (ArrayExpression or SetExpression or varargs of NumericExpression) – Single numeric array or set, or multiple numeric values.; filter_missing (bool) – Remove missing arguments/elements before computing minimum. Returns:; NumericExpression. hail.expr.functions.max(*exprs, filter_missing=True)[source]; Returns the maximum element of a collection or of given numeric expressions.; Examples; Take the maximum value of an array:; >>> hl.eval(hl.max([1, 3, 5, 6, 7, 9])); 9. Take the maximum value of values:; >>> hl.eval(hl.max(1, 50, 2)); 50. Notes; Like the Python builtin max function, this function can either take a; single iterable expression (an array or set of numeric elements), or; variable-length arguments of numeric expressions. Note; If filter_missing is True, then the result is the maximum of; non-missing arguments or elements. If filter_missing is False, then; any missing argument or element causes the result to be missing.; If any element or argument is NaN, then the result is NaN. See also; nanmax(), min(), nanmin(). Parameters:. exprs (ArrayExpression or SetExpression or varargs of NumericExpression) – Single numeric array or set, or multiple numeric values.; filter_missing (bool) – Remove missing arguments/elements before computing maximum. Returns:; NumericExpression. hail.expr.functions.nanmax(*exprs, filter_missing=True)[source]; Returns the maximum value of a collection or of given arguments, excluding NaN.; Examples; Compute the maximum value of an array:; >>> hl.eval(hl.nanmax([1.1, 50.1, float('nan')])); 50.1. Take the maximum value of arguments:; >>> hl.eval(hl.nanmax(1.1, 50.1, float('nan'))); 5",MatchSource.WIKI,docs/0.2/functions/numeric.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/numeric.html
https://hail.is/docs/0.2/functions/numeric.html:12189,Modifiability,variab,variable-length,12189,"e; If filter_missing is True, then the result is the maximum of; non-missing arguments or elements. If filter_missing is False, then; any missing argument or element causes the result to be missing.; If any element or argument is NaN, then the result is NaN. See also; nanmax(), min(), nanmin(). Parameters:. exprs (ArrayExpression or SetExpression or varargs of NumericExpression) – Single numeric array or set, or multiple numeric values.; filter_missing (bool) – Remove missing arguments/elements before computing maximum. Returns:; NumericExpression. hail.expr.functions.nanmax(*exprs, filter_missing=True)[source]; Returns the maximum value of a collection or of given arguments, excluding NaN.; Examples; Compute the maximum value of an array:; >>> hl.eval(hl.nanmax([1.1, 50.1, float('nan')])); 50.1. Take the maximum value of arguments:; >>> hl.eval(hl.nanmax(1.1, 50.1, float('nan'))); 50.1. Notes; Like the Python builtin max function, this function can either take a; single iterable expression (an array or set of numeric elements), or; variable-length arguments of numeric expressions. Note; If filter_missing is True, then the result is the maximum of; non-missing arguments or elements. If filter_missing is False, then; any missing argument or element causes the result to be missing.; NaN arguments / array elements are ignored; the maximum value of NaN and; any non-NaN value x is x. See also; max(), min(), nanmin(). Parameters:. exprs (ArrayExpression or SetExpression or varargs of NumericExpression) – Single numeric array or set, or multiple numeric values.; filter_missing (bool) – Remove missing arguments/elements before computing maximum. Returns:; NumericExpression. hail.expr.functions.mean(collection, filter_missing=True)[source]; Returns the mean of all values in the collection.; Examples; >>> a = [1, 3, 5, 6, 7, 9]. >>> hl.eval(hl.mean(a)); 5.166666666666667. Note; Missing elements are ignored if filter_missing is True. If filter_missing; is False, then any mis",MatchSource.WIKI,docs/0.2/functions/numeric.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/numeric.html
https://hail.is/docs/0.2/functions/numeric.html:18601,Performance,perform,perform,18601," type array<tfloat64>). Returns:; Float64Expression. hail.expr.functions.uniroot(f, min, max, *, max_iter=1000, epsilon=2.220446049250313e-16, tolerance=0.0001220703)[source]; Finds a root of the function f within the interval [min, max].; Examples; >>> hl.eval(hl.uniroot(lambda x: x - 1, -5, 5)); 1.0. Notes; f(min) and f(max) must not have the same sign.; If no root can be found, the result of this call will be NA (missing).; uniroot() returns an estimate for a root with accuracy; 4 * epsilon * abs(x) + tolerance.; 4*EPSILON*abs(x) + tol. Parameters:. f (function ( (arg) -> Float64Expression)) – Must return a Float64Expression.; min (Float64Expression); max (Float64Expression); max_iter (int) – The maximum number of iterations before giving up.; epsilon (float) – The scaling factor in the accuracy of the root found.; tolerance (float) – The constant factor in approximate accuracy of the root found. Returns:; Float64Expression – The root of the function f. hail.expr.functions.binary_search(array, elem)[source]; Binary search array for the insertion point of elem. Parameters:. array (Expression of type tarray); elem (Expression). Returns:; Int32Expression. Notes; This function assumes that array is sorted in ascending order, and does; not perform any sortedness check. Missing values sort last.; The returned index is the lower bound on the insertion point of elem into; the ordered array, or the index of the first element in array not smaller; than elem. This is a value between 0 and the length of array, inclusive; (if all elements in array are smaller than elem, the returned value is; the length of array or the index of the first missing value, if one; exists).; If either elem or array is missing, the result is missing.; Examples; >>> a = hl.array([0, 2, 4, 8]). >>> hl.eval(hl.binary_search(a, -1)); 0. >>> hl.eval(hl.binary_search(a, 1)); 1. >>> hl.eval(hl.binary_search(a, 10)); 4. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/functions/numeric.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/numeric.html
https://hail.is/docs/0.2/functions/numeric.html:1083,Testability,log,logical,1083," Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Functions; Numeric functions. View page source. Numeric functions; Numeric functions. abs(x); Take the absolute value of a numeric value, array or ndarray. approx_equal(x, y[, tolerance, absolute, ...]); Tests whether two numbers are approximately equal. bit_and(x, y); Bitwise and x and y. bit_or(x, y); Bitwise or x and y. bit_xor(x, y); Bitwise exclusive-or x and y. bit_lshift(x, y); Bitwise left-shift x by y. bit_rshift(x, y[, logical]); Bitwise right-shift x by y. bit_not(x); Bitwise invert x. bit_count(x); Count the number of 1s in the in the two's complement binary representation of x. exp(x). expit(x). is_nan(x). is_finite(x). is_infinite(x). log(x[, base]); Take the logarithm of the x with base base. log10(x). logit(x). sign(x); Returns the sign of a numeric value, array or ndarray. sqrt(x). int(x); Convert to a 32-bit integer expression. int32(x); Convert to a 32-bit integer expression. int64(x); Convert to a 64-bit integer expression. float(x); Convert to a 64-bit floating point expression. float32(x); Convert to a 32-bit floating point expression. float64(x); Convert to a 64-bit floating point expression. floor(x). ceil(x). uniroot(f, min, max, *[, max_iter, epsilon, ...]); Finds a root of the function f within the interval [min, max]. Numeric collection functions. min(*exprs[, filter_missing]); Returns the minimum element of a collection or of given numeric expressions. nanmin(*exprs[, filter_missing]); Retur",MatchSource.WIKI,docs/0.2/functions/numeric.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/numeric.html
https://hail.is/docs/0.2/functions/numeric.html:1307,Testability,log,log,1307,"etics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Functions; Numeric functions. View page source. Numeric functions; Numeric functions. abs(x); Take the absolute value of a numeric value, array or ndarray. approx_equal(x, y[, tolerance, absolute, ...]); Tests whether two numbers are approximately equal. bit_and(x, y); Bitwise and x and y. bit_or(x, y); Bitwise or x and y. bit_xor(x, y); Bitwise exclusive-or x and y. bit_lshift(x, y); Bitwise left-shift x by y. bit_rshift(x, y[, logical]); Bitwise right-shift x by y. bit_not(x); Bitwise invert x. bit_count(x); Count the number of 1s in the in the two's complement binary representation of x. exp(x). expit(x). is_nan(x). is_finite(x). is_infinite(x). log(x[, base]); Take the logarithm of the x with base base. log10(x). logit(x). sign(x); Returns the sign of a numeric value, array or ndarray. sqrt(x). int(x); Convert to a 32-bit integer expression. int32(x); Convert to a 32-bit integer expression. int64(x); Convert to a 64-bit integer expression. float(x); Convert to a 64-bit floating point expression. float32(x); Convert to a 32-bit floating point expression. float64(x); Convert to a 64-bit floating point expression. floor(x). ceil(x). uniroot(f, min, max, *[, max_iter, epsilon, ...]); Finds a root of the function f within the interval [min, max]. Numeric collection functions. min(*exprs[, filter_missing]); Returns the minimum element of a collection or of given numeric expressions. nanmin(*exprs[, filter_missing]); Returns the minimum value of a collection or of given arguments, excluding NaN. max(*exprs[, filter_missing]); Returns the maximum element of a collection or of given numeric expressions. nanmax(*exprs[, filter_missing]); Returns the maximum value ",MatchSource.WIKI,docs/0.2/functions/numeric.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/numeric.html
https://hail.is/docs/0.2/functions/numeric.html:1332,Testability,log,logarithm,1332,"etics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Functions; Numeric functions. View page source. Numeric functions; Numeric functions. abs(x); Take the absolute value of a numeric value, array or ndarray. approx_equal(x, y[, tolerance, absolute, ...]); Tests whether two numbers are approximately equal. bit_and(x, y); Bitwise and x and y. bit_or(x, y); Bitwise or x and y. bit_xor(x, y); Bitwise exclusive-or x and y. bit_lshift(x, y); Bitwise left-shift x by y. bit_rshift(x, y[, logical]); Bitwise right-shift x by y. bit_not(x); Bitwise invert x. bit_count(x); Count the number of 1s in the in the two's complement binary representation of x. exp(x). expit(x). is_nan(x). is_finite(x). is_infinite(x). log(x[, base]); Take the logarithm of the x with base base. log10(x). logit(x). sign(x); Returns the sign of a numeric value, array or ndarray. sqrt(x). int(x); Convert to a 32-bit integer expression. int32(x); Convert to a 32-bit integer expression. int64(x); Convert to a 64-bit integer expression. float(x); Convert to a 64-bit floating point expression. float32(x); Convert to a 32-bit floating point expression. float64(x); Convert to a 64-bit floating point expression. floor(x). ceil(x). uniroot(f, min, max, *[, max_iter, epsilon, ...]); Finds a root of the function f within the interval [min, max]. Numeric collection functions. min(*exprs[, filter_missing]); Returns the minimum element of a collection or of given numeric expressions. nanmin(*exprs[, filter_missing]); Returns the minimum value of a collection or of given arguments, excluding NaN. max(*exprs[, filter_missing]); Returns the maximum element of a collection or of given numeric expressions. nanmax(*exprs[, filter_missing]); Returns the maximum value ",MatchSource.WIKI,docs/0.2/functions/numeric.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/numeric.html
https://hail.is/docs/0.2/functions/numeric.html:1377,Testability,log,logit,1377,"evel Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Functions; Numeric functions. View page source. Numeric functions; Numeric functions. abs(x); Take the absolute value of a numeric value, array or ndarray. approx_equal(x, y[, tolerance, absolute, ...]); Tests whether two numbers are approximately equal. bit_and(x, y); Bitwise and x and y. bit_or(x, y); Bitwise or x and y. bit_xor(x, y); Bitwise exclusive-or x and y. bit_lshift(x, y); Bitwise left-shift x by y. bit_rshift(x, y[, logical]); Bitwise right-shift x by y. bit_not(x); Bitwise invert x. bit_count(x); Count the number of 1s in the in the two's complement binary representation of x. exp(x). expit(x). is_nan(x). is_finite(x). is_infinite(x). log(x[, base]); Take the logarithm of the x with base base. log10(x). logit(x). sign(x); Returns the sign of a numeric value, array or ndarray. sqrt(x). int(x); Convert to a 32-bit integer expression. int32(x); Convert to a 32-bit integer expression. int64(x); Convert to a 64-bit integer expression. float(x); Convert to a 64-bit floating point expression. float32(x); Convert to a 32-bit floating point expression. float64(x); Convert to a 64-bit floating point expression. floor(x). ceil(x). uniroot(f, min, max, *[, max_iter, epsilon, ...]); Finds a root of the function f within the interval [min, max]. Numeric collection functions. min(*exprs[, filter_missing]); Returns the minimum element of a collection or of given numeric expressions. nanmin(*exprs[, filter_missing]); Returns the minimum value of a collection or of given arguments, excluding NaN. max(*exprs[, filter_missing]); Returns the maximum element of a collection or of given numeric expressions. nanmax(*exprs[, filter_missing]); Returns the maximum value of a collection or of given arguments, exclud",MatchSource.WIKI,docs/0.2/functions/numeric.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/numeric.html
https://hail.is/docs/0.2/functions/numeric.html:5831,Testability,log,logical,5831,"se exclusive-or x and y.; Examples; >>> hl.eval(hl.bit_xor(5, 3)); 6. Notes; See the Python wiki; for more information about bit operators. Parameters:. x (Int32Expression or Int64Expression); y (Int32Expression or Int64Expression). Returns:; Int32Expression or Int64Expression. hail.expr.functions.bit_lshift(x, y)[source]; Bitwise left-shift x by y.; Examples; >>> hl.eval(hl.bit_lshift(5, 3)); 40. >>> hl.eval(hl.bit_lshift(1, 8)); 256. Unlike Python, Hail integers are fixed-size (32 or 64 bits),; and bits extended beyond will be ignored:; >>> hl.eval(hl.bit_lshift(1, 31)); -2147483648. >>> hl.eval(hl.bit_lshift(1, 32)); 0. >>> hl.eval(hl.bit_lshift(hl.int64(1), 32)); 4294967296. >>> hl.eval(hl.bit_lshift(hl.int64(1), 64)); 0. Notes; See the Python wiki; for more information about bit operators. Parameters:. x (Int32Expression or Int64Expression); y (Int32Expression or Int64Expression). Returns:; Int32Expression or Int64Expression. hail.expr.functions.bit_rshift(x, y, logical=False)[source]; Bitwise right-shift x by y.; Examples; >>> hl.eval(hl.bit_rshift(256, 3)); 32. With logical=False (default), the sign is preserved:; >>> hl.eval(hl.bit_rshift(-1, 1)); -1. With logical=True, the sign bit is treated as any other:; >>> hl.eval(hl.bit_rshift(-1, 1, logical=True)); 2147483647. Notes; If logical is False, then the shift is a sign-preserving right shift.; If logical is True, then the shift is logical, with the sign bit; treated as any other bit.; See the Python wiki; for more information about bit operators. Parameters:. x (Int32Expression or Int64Expression); y (Int32Expression or Int64Expression); logical (bool). Returns:; Int32Expression or Int64Expression. hail.expr.functions.bit_not(x)[source]; Bitwise invert x.; Examples; >>> hl.eval(hl.bit_not(0)); -1. Notes; See the Python wiki; for more information about bit operators. Parameters:; x (Int32Expression or Int64Expression). Returns:; Int32Expression or Int64Expression. hail.expr.functions.bit_count(x)[source];",MatchSource.WIKI,docs/0.2/functions/numeric.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/numeric.html
https://hail.is/docs/0.2/functions/numeric.html:5940,Testability,log,logical,5940,"n about bit operators. Parameters:. x (Int32Expression or Int64Expression); y (Int32Expression or Int64Expression). Returns:; Int32Expression or Int64Expression. hail.expr.functions.bit_lshift(x, y)[source]; Bitwise left-shift x by y.; Examples; >>> hl.eval(hl.bit_lshift(5, 3)); 40. >>> hl.eval(hl.bit_lshift(1, 8)); 256. Unlike Python, Hail integers are fixed-size (32 or 64 bits),; and bits extended beyond will be ignored:; >>> hl.eval(hl.bit_lshift(1, 31)); -2147483648. >>> hl.eval(hl.bit_lshift(1, 32)); 0. >>> hl.eval(hl.bit_lshift(hl.int64(1), 32)); 4294967296. >>> hl.eval(hl.bit_lshift(hl.int64(1), 64)); 0. Notes; See the Python wiki; for more information about bit operators. Parameters:. x (Int32Expression or Int64Expression); y (Int32Expression or Int64Expression). Returns:; Int32Expression or Int64Expression. hail.expr.functions.bit_rshift(x, y, logical=False)[source]; Bitwise right-shift x by y.; Examples; >>> hl.eval(hl.bit_rshift(256, 3)); 32. With logical=False (default), the sign is preserved:; >>> hl.eval(hl.bit_rshift(-1, 1)); -1. With logical=True, the sign bit is treated as any other:; >>> hl.eval(hl.bit_rshift(-1, 1, logical=True)); 2147483647. Notes; If logical is False, then the shift is a sign-preserving right shift.; If logical is True, then the shift is logical, with the sign bit; treated as any other bit.; See the Python wiki; for more information about bit operators. Parameters:. x (Int32Expression or Int64Expression); y (Int32Expression or Int64Expression); logical (bool). Returns:; Int32Expression or Int64Expression. hail.expr.functions.bit_not(x)[source]; Bitwise invert x.; Examples; >>> hl.eval(hl.bit_not(0)); -1. Notes; See the Python wiki; for more information about bit operators. Parameters:; x (Int32Expression or Int64Expression). Returns:; Int32Expression or Int64Expression. hail.expr.functions.bit_count(x)[source]; Count the number of 1s in the in the two’s complement binary representation of x.; Examples; The binary representat",MatchSource.WIKI,docs/0.2/functions/numeric.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/numeric.html
https://hail.is/docs/0.2/functions/numeric.html:6033,Testability,log,logical,6033,"or Int64Expression). Returns:; Int32Expression or Int64Expression. hail.expr.functions.bit_lshift(x, y)[source]; Bitwise left-shift x by y.; Examples; >>> hl.eval(hl.bit_lshift(5, 3)); 40. >>> hl.eval(hl.bit_lshift(1, 8)); 256. Unlike Python, Hail integers are fixed-size (32 or 64 bits),; and bits extended beyond will be ignored:; >>> hl.eval(hl.bit_lshift(1, 31)); -2147483648. >>> hl.eval(hl.bit_lshift(1, 32)); 0. >>> hl.eval(hl.bit_lshift(hl.int64(1), 32)); 4294967296. >>> hl.eval(hl.bit_lshift(hl.int64(1), 64)); 0. Notes; See the Python wiki; for more information about bit operators. Parameters:. x (Int32Expression or Int64Expression); y (Int32Expression or Int64Expression). Returns:; Int32Expression or Int64Expression. hail.expr.functions.bit_rshift(x, y, logical=False)[source]; Bitwise right-shift x by y.; Examples; >>> hl.eval(hl.bit_rshift(256, 3)); 32. With logical=False (default), the sign is preserved:; >>> hl.eval(hl.bit_rshift(-1, 1)); -1. With logical=True, the sign bit is treated as any other:; >>> hl.eval(hl.bit_rshift(-1, 1, logical=True)); 2147483647. Notes; If logical is False, then the shift is a sign-preserving right shift.; If logical is True, then the shift is logical, with the sign bit; treated as any other bit.; See the Python wiki; for more information about bit operators. Parameters:. x (Int32Expression or Int64Expression); y (Int32Expression or Int64Expression); logical (bool). Returns:; Int32Expression or Int64Expression. hail.expr.functions.bit_not(x)[source]; Bitwise invert x.; Examples; >>> hl.eval(hl.bit_not(0)); -1. Notes; See the Python wiki; for more information about bit operators. Parameters:; x (Int32Expression or Int64Expression). Returns:; Int32Expression or Int64Expression. hail.expr.functions.bit_count(x)[source]; Count the number of 1s in the in the two’s complement binary representation of x.; Examples; The binary representation of 7 is 111, so:; >>> hl.eval(hl.bit_count(7)); 3. Parameters:; x (Int32Expression or Int64",MatchSource.WIKI,docs/0.2/functions/numeric.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/numeric.html
https://hail.is/docs/0.2/functions/numeric.html:6119,Testability,log,logical,6119,"on. hail.expr.functions.bit_lshift(x, y)[source]; Bitwise left-shift x by y.; Examples; >>> hl.eval(hl.bit_lshift(5, 3)); 40. >>> hl.eval(hl.bit_lshift(1, 8)); 256. Unlike Python, Hail integers are fixed-size (32 or 64 bits),; and bits extended beyond will be ignored:; >>> hl.eval(hl.bit_lshift(1, 31)); -2147483648. >>> hl.eval(hl.bit_lshift(1, 32)); 0. >>> hl.eval(hl.bit_lshift(hl.int64(1), 32)); 4294967296. >>> hl.eval(hl.bit_lshift(hl.int64(1), 64)); 0. Notes; See the Python wiki; for more information about bit operators. Parameters:. x (Int32Expression or Int64Expression); y (Int32Expression or Int64Expression). Returns:; Int32Expression or Int64Expression. hail.expr.functions.bit_rshift(x, y, logical=False)[source]; Bitwise right-shift x by y.; Examples; >>> hl.eval(hl.bit_rshift(256, 3)); 32. With logical=False (default), the sign is preserved:; >>> hl.eval(hl.bit_rshift(-1, 1)); -1. With logical=True, the sign bit is treated as any other:; >>> hl.eval(hl.bit_rshift(-1, 1, logical=True)); 2147483647. Notes; If logical is False, then the shift is a sign-preserving right shift.; If logical is True, then the shift is logical, with the sign bit; treated as any other bit.; See the Python wiki; for more information about bit operators. Parameters:. x (Int32Expression or Int64Expression); y (Int32Expression or Int64Expression); logical (bool). Returns:; Int32Expression or Int64Expression. hail.expr.functions.bit_not(x)[source]; Bitwise invert x.; Examples; >>> hl.eval(hl.bit_not(0)); -1. Notes; See the Python wiki; for more information about bit operators. Parameters:; x (Int32Expression or Int64Expression). Returns:; Int32Expression or Int64Expression. hail.expr.functions.bit_count(x)[source]; Count the number of 1s in the in the two’s complement binary representation of x.; Examples; The binary representation of 7 is 111, so:; >>> hl.eval(hl.bit_count(7)); 3. Parameters:; x (Int32Expression or Int64Expression). Returns:; Int32Expression. hail.expr.functions.exp(",MatchSource.WIKI,docs/0.2/functions/numeric.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/numeric.html
https://hail.is/docs/0.2/functions/numeric.html:6157,Testability,log,logical,6157,"ft-shift x by y.; Examples; >>> hl.eval(hl.bit_lshift(5, 3)); 40. >>> hl.eval(hl.bit_lshift(1, 8)); 256. Unlike Python, Hail integers are fixed-size (32 or 64 bits),; and bits extended beyond will be ignored:; >>> hl.eval(hl.bit_lshift(1, 31)); -2147483648. >>> hl.eval(hl.bit_lshift(1, 32)); 0. >>> hl.eval(hl.bit_lshift(hl.int64(1), 32)); 4294967296. >>> hl.eval(hl.bit_lshift(hl.int64(1), 64)); 0. Notes; See the Python wiki; for more information about bit operators. Parameters:. x (Int32Expression or Int64Expression); y (Int32Expression or Int64Expression). Returns:; Int32Expression or Int64Expression. hail.expr.functions.bit_rshift(x, y, logical=False)[source]; Bitwise right-shift x by y.; Examples; >>> hl.eval(hl.bit_rshift(256, 3)); 32. With logical=False (default), the sign is preserved:; >>> hl.eval(hl.bit_rshift(-1, 1)); -1. With logical=True, the sign bit is treated as any other:; >>> hl.eval(hl.bit_rshift(-1, 1, logical=True)); 2147483647. Notes; If logical is False, then the shift is a sign-preserving right shift.; If logical is True, then the shift is logical, with the sign bit; treated as any other bit.; See the Python wiki; for more information about bit operators. Parameters:. x (Int32Expression or Int64Expression); y (Int32Expression or Int64Expression); logical (bool). Returns:; Int32Expression or Int64Expression. hail.expr.functions.bit_not(x)[source]; Bitwise invert x.; Examples; >>> hl.eval(hl.bit_not(0)); -1. Notes; See the Python wiki; for more information about bit operators. Parameters:; x (Int32Expression or Int64Expression). Returns:; Int32Expression or Int64Expression. hail.expr.functions.bit_count(x)[source]; Count the number of 1s in the in the two’s complement binary representation of x.; Examples; The binary representation of 7 is 111, so:; >>> hl.eval(hl.bit_count(7)); 3. Parameters:; x (Int32Expression or Int64Expression). Returns:; Int32Expression. hail.expr.functions.exp(x)[source]. hail.expr.functions.expit(x)[source]. hail.expr",MatchSource.WIKI,docs/0.2/functions/numeric.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/numeric.html
https://hail.is/docs/0.2/functions/numeric.html:6228,Testability,log,logical,6228,"lshift(1, 8)); 256. Unlike Python, Hail integers are fixed-size (32 or 64 bits),; and bits extended beyond will be ignored:; >>> hl.eval(hl.bit_lshift(1, 31)); -2147483648. >>> hl.eval(hl.bit_lshift(1, 32)); 0. >>> hl.eval(hl.bit_lshift(hl.int64(1), 32)); 4294967296. >>> hl.eval(hl.bit_lshift(hl.int64(1), 64)); 0. Notes; See the Python wiki; for more information about bit operators. Parameters:. x (Int32Expression or Int64Expression); y (Int32Expression or Int64Expression). Returns:; Int32Expression or Int64Expression. hail.expr.functions.bit_rshift(x, y, logical=False)[source]; Bitwise right-shift x by y.; Examples; >>> hl.eval(hl.bit_rshift(256, 3)); 32. With logical=False (default), the sign is preserved:; >>> hl.eval(hl.bit_rshift(-1, 1)); -1. With logical=True, the sign bit is treated as any other:; >>> hl.eval(hl.bit_rshift(-1, 1, logical=True)); 2147483647. Notes; If logical is False, then the shift is a sign-preserving right shift.; If logical is True, then the shift is logical, with the sign bit; treated as any other bit.; See the Python wiki; for more information about bit operators. Parameters:. x (Int32Expression or Int64Expression); y (Int32Expression or Int64Expression); logical (bool). Returns:; Int32Expression or Int64Expression. hail.expr.functions.bit_not(x)[source]; Bitwise invert x.; Examples; >>> hl.eval(hl.bit_not(0)); -1. Notes; See the Python wiki; for more information about bit operators. Parameters:; x (Int32Expression or Int64Expression). Returns:; Int32Expression or Int64Expression. hail.expr.functions.bit_count(x)[source]; Count the number of 1s in the in the two’s complement binary representation of x.; Examples; The binary representation of 7 is 111, so:; >>> hl.eval(hl.bit_count(7)); 3. Parameters:; x (Int32Expression or Int64Expression). Returns:; Int32Expression. hail.expr.functions.exp(x)[source]. hail.expr.functions.expit(x)[source]. hail.expr.functions.is_nan(x)[source]. hail.expr.functions.is_finite(x)[source]. hail.expr.f",MatchSource.WIKI,docs/0.2/functions/numeric.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/numeric.html
https://hail.is/docs/0.2/functions/numeric.html:6263,Testability,log,logical,6263,"lshift(1, 8)); 256. Unlike Python, Hail integers are fixed-size (32 or 64 bits),; and bits extended beyond will be ignored:; >>> hl.eval(hl.bit_lshift(1, 31)); -2147483648. >>> hl.eval(hl.bit_lshift(1, 32)); 0. >>> hl.eval(hl.bit_lshift(hl.int64(1), 32)); 4294967296. >>> hl.eval(hl.bit_lshift(hl.int64(1), 64)); 0. Notes; See the Python wiki; for more information about bit operators. Parameters:. x (Int32Expression or Int64Expression); y (Int32Expression or Int64Expression). Returns:; Int32Expression or Int64Expression. hail.expr.functions.bit_rshift(x, y, logical=False)[source]; Bitwise right-shift x by y.; Examples; >>> hl.eval(hl.bit_rshift(256, 3)); 32. With logical=False (default), the sign is preserved:; >>> hl.eval(hl.bit_rshift(-1, 1)); -1. With logical=True, the sign bit is treated as any other:; >>> hl.eval(hl.bit_rshift(-1, 1, logical=True)); 2147483647. Notes; If logical is False, then the shift is a sign-preserving right shift.; If logical is True, then the shift is logical, with the sign bit; treated as any other bit.; See the Python wiki; for more information about bit operators. Parameters:. x (Int32Expression or Int64Expression); y (Int32Expression or Int64Expression); logical (bool). Returns:; Int32Expression or Int64Expression. hail.expr.functions.bit_not(x)[source]; Bitwise invert x.; Examples; >>> hl.eval(hl.bit_not(0)); -1. Notes; See the Python wiki; for more information about bit operators. Parameters:; x (Int32Expression or Int64Expression). Returns:; Int32Expression or Int64Expression. hail.expr.functions.bit_count(x)[source]; Count the number of 1s in the in the two’s complement binary representation of x.; Examples; The binary representation of 7 is 111, so:; >>> hl.eval(hl.bit_count(7)); 3. Parameters:; x (Int32Expression or Int64Expression). Returns:; Int32Expression. hail.expr.functions.exp(x)[source]. hail.expr.functions.expit(x)[source]. hail.expr.functions.is_nan(x)[source]. hail.expr.functions.is_finite(x)[source]. hail.expr.f",MatchSource.WIKI,docs/0.2/functions/numeric.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/numeric.html
https://hail.is/docs/0.2/functions/numeric.html:6474,Testability,log,logical,6474," >>> hl.eval(hl.bit_lshift(1, 32)); 0. >>> hl.eval(hl.bit_lshift(hl.int64(1), 32)); 4294967296. >>> hl.eval(hl.bit_lshift(hl.int64(1), 64)); 0. Notes; See the Python wiki; for more information about bit operators. Parameters:. x (Int32Expression or Int64Expression); y (Int32Expression or Int64Expression). Returns:; Int32Expression or Int64Expression. hail.expr.functions.bit_rshift(x, y, logical=False)[source]; Bitwise right-shift x by y.; Examples; >>> hl.eval(hl.bit_rshift(256, 3)); 32. With logical=False (default), the sign is preserved:; >>> hl.eval(hl.bit_rshift(-1, 1)); -1. With logical=True, the sign bit is treated as any other:; >>> hl.eval(hl.bit_rshift(-1, 1, logical=True)); 2147483647. Notes; If logical is False, then the shift is a sign-preserving right shift.; If logical is True, then the shift is logical, with the sign bit; treated as any other bit.; See the Python wiki; for more information about bit operators. Parameters:. x (Int32Expression or Int64Expression); y (Int32Expression or Int64Expression); logical (bool). Returns:; Int32Expression or Int64Expression. hail.expr.functions.bit_not(x)[source]; Bitwise invert x.; Examples; >>> hl.eval(hl.bit_not(0)); -1. Notes; See the Python wiki; for more information about bit operators. Parameters:; x (Int32Expression or Int64Expression). Returns:; Int32Expression or Int64Expression. hail.expr.functions.bit_count(x)[source]; Count the number of 1s in the in the two’s complement binary representation of x.; Examples; The binary representation of 7 is 111, so:; >>> hl.eval(hl.bit_count(7)); 3. Parameters:; x (Int32Expression or Int64Expression). Returns:; Int32Expression. hail.expr.functions.exp(x)[source]. hail.expr.functions.expit(x)[source]. hail.expr.functions.is_nan(x)[source]. hail.expr.functions.is_finite(x)[source]. hail.expr.functions.is_infinite(x)[source]. hail.expr.functions.log(x, base=None)[source]; Take the logarithm of the x with base base.; Examples; >>> hl.eval(hl.log(10)); 2.3025850",MatchSource.WIKI,docs/0.2/functions/numeric.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/numeric.html
https://hail.is/docs/0.2/functions/numeric.html:7324,Testability,log,log,7324," about bit operators. Parameters:. x (Int32Expression or Int64Expression); y (Int32Expression or Int64Expression); logical (bool). Returns:; Int32Expression or Int64Expression. hail.expr.functions.bit_not(x)[source]; Bitwise invert x.; Examples; >>> hl.eval(hl.bit_not(0)); -1. Notes; See the Python wiki; for more information about bit operators. Parameters:; x (Int32Expression or Int64Expression). Returns:; Int32Expression or Int64Expression. hail.expr.functions.bit_count(x)[source]; Count the number of 1s in the in the two’s complement binary representation of x.; Examples; The binary representation of 7 is 111, so:; >>> hl.eval(hl.bit_count(7)); 3. Parameters:; x (Int32Expression or Int64Expression). Returns:; Int32Expression. hail.expr.functions.exp(x)[source]. hail.expr.functions.expit(x)[source]. hail.expr.functions.is_nan(x)[source]. hail.expr.functions.is_finite(x)[source]. hail.expr.functions.is_infinite(x)[source]. hail.expr.functions.log(x, base=None)[source]; Take the logarithm of the x with base base.; Examples; >>> hl.eval(hl.log(10)); 2.302585092994046. >>> hl.eval(hl.log(10, 10)); 1.0. >>> hl.eval(hl.log(1024, 2)); 10.0. Notes; If the base argument is not supplied, then the natural logarithm is used. Parameters:. x (float or Expression of type tfloat64); base (float or Expression of type tfloat64). Returns:; Expression of type tfloat64. hail.expr.functions.log10(x)[source]. hail.expr.functions.logit(x)[source]. hail.expr.functions.floor(x)[source]. hail.expr.functions.ceil(x)[source]. hail.expr.functions.sqrt(x)[source]. hail.expr.functions.sign(x)[source]; Returns the sign of a numeric value, array or ndarray.; Examples; >>> hl.eval(hl.sign(-1.23)); -1.0. >>> hl.eval(hl.sign([-4, 0, 5])); [-1, 0, 1]. >>> hl.eval(hl.sign([0.0, 3.14])); [0.0, 1.0]. >>> hl.eval(hl.sign(float('nan'))); nan. Notes; The sign function preserves type and maps nan to nan. Parameters:; x (NumericExpression, ArrayNumericExpression or NDArrayNumericExpression). Retu",MatchSource.WIKI,docs/0.2/functions/numeric.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/numeric.html
https://hail.is/docs/0.2/functions/numeric.html:7361,Testability,log,logarithm,7361," about bit operators. Parameters:. x (Int32Expression or Int64Expression); y (Int32Expression or Int64Expression); logical (bool). Returns:; Int32Expression or Int64Expression. hail.expr.functions.bit_not(x)[source]; Bitwise invert x.; Examples; >>> hl.eval(hl.bit_not(0)); -1. Notes; See the Python wiki; for more information about bit operators. Parameters:; x (Int32Expression or Int64Expression). Returns:; Int32Expression or Int64Expression. hail.expr.functions.bit_count(x)[source]; Count the number of 1s in the in the two’s complement binary representation of x.; Examples; The binary representation of 7 is 111, so:; >>> hl.eval(hl.bit_count(7)); 3. Parameters:; x (Int32Expression or Int64Expression). Returns:; Int32Expression. hail.expr.functions.exp(x)[source]. hail.expr.functions.expit(x)[source]. hail.expr.functions.is_nan(x)[source]. hail.expr.functions.is_finite(x)[source]. hail.expr.functions.is_infinite(x)[source]. hail.expr.functions.log(x, base=None)[source]; Take the logarithm of the x with base base.; Examples; >>> hl.eval(hl.log(10)); 2.302585092994046. >>> hl.eval(hl.log(10, 10)); 1.0. >>> hl.eval(hl.log(1024, 2)); 10.0. Notes; If the base argument is not supplied, then the natural logarithm is used. Parameters:. x (float or Expression of type tfloat64); base (float or Expression of type tfloat64). Returns:; Expression of type tfloat64. hail.expr.functions.log10(x)[source]. hail.expr.functions.logit(x)[source]. hail.expr.functions.floor(x)[source]. hail.expr.functions.ceil(x)[source]. hail.expr.functions.sqrt(x)[source]. hail.expr.functions.sign(x)[source]; Returns the sign of a numeric value, array or ndarray.; Examples; >>> hl.eval(hl.sign(-1.23)); -1.0. >>> hl.eval(hl.sign([-4, 0, 5])); [-1, 0, 1]. >>> hl.eval(hl.sign([0.0, 3.14])); [0.0, 1.0]. >>> hl.eval(hl.sign(float('nan'))); nan. Notes; The sign function preserves type and maps nan to nan. Parameters:; x (NumericExpression, ArrayNumericExpression or NDArrayNumericExpression). Retu",MatchSource.WIKI,docs/0.2/functions/numeric.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/numeric.html
https://hail.is/docs/0.2/functions/numeric.html:7422,Testability,log,log,7422,"ion); y (Int32Expression or Int64Expression); logical (bool). Returns:; Int32Expression or Int64Expression. hail.expr.functions.bit_not(x)[source]; Bitwise invert x.; Examples; >>> hl.eval(hl.bit_not(0)); -1. Notes; See the Python wiki; for more information about bit operators. Parameters:; x (Int32Expression or Int64Expression). Returns:; Int32Expression or Int64Expression. hail.expr.functions.bit_count(x)[source]; Count the number of 1s in the in the two’s complement binary representation of x.; Examples; The binary representation of 7 is 111, so:; >>> hl.eval(hl.bit_count(7)); 3. Parameters:; x (Int32Expression or Int64Expression). Returns:; Int32Expression. hail.expr.functions.exp(x)[source]. hail.expr.functions.expit(x)[source]. hail.expr.functions.is_nan(x)[source]. hail.expr.functions.is_finite(x)[source]. hail.expr.functions.is_infinite(x)[source]. hail.expr.functions.log(x, base=None)[source]; Take the logarithm of the x with base base.; Examples; >>> hl.eval(hl.log(10)); 2.302585092994046. >>> hl.eval(hl.log(10, 10)); 1.0. >>> hl.eval(hl.log(1024, 2)); 10.0. Notes; If the base argument is not supplied, then the natural logarithm is used. Parameters:. x (float or Expression of type tfloat64); base (float or Expression of type tfloat64). Returns:; Expression of type tfloat64. hail.expr.functions.log10(x)[source]. hail.expr.functions.logit(x)[source]. hail.expr.functions.floor(x)[source]. hail.expr.functions.ceil(x)[source]. hail.expr.functions.sqrt(x)[source]. hail.expr.functions.sign(x)[source]; Returns the sign of a numeric value, array or ndarray.; Examples; >>> hl.eval(hl.sign(-1.23)); -1.0. >>> hl.eval(hl.sign([-4, 0, 5])); [-1, 0, 1]. >>> hl.eval(hl.sign([0.0, 3.14])); [0.0, 1.0]. >>> hl.eval(hl.sign(float('nan'))); nan. Notes; The sign function preserves type and maps nan to nan. Parameters:; x (NumericExpression, ArrayNumericExpression or NDArrayNumericExpression). Returns:; NumericExpression, ArrayNumericExpression or NDArrayNumericExp",MatchSource.WIKI,docs/0.2/functions/numeric.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/numeric.html
https://hail.is/docs/0.2/functions/numeric.html:7466,Testability,log,log,7466,"logical (bool). Returns:; Int32Expression or Int64Expression. hail.expr.functions.bit_not(x)[source]; Bitwise invert x.; Examples; >>> hl.eval(hl.bit_not(0)); -1. Notes; See the Python wiki; for more information about bit operators. Parameters:; x (Int32Expression or Int64Expression). Returns:; Int32Expression or Int64Expression. hail.expr.functions.bit_count(x)[source]; Count the number of 1s in the in the two’s complement binary representation of x.; Examples; The binary representation of 7 is 111, so:; >>> hl.eval(hl.bit_count(7)); 3. Parameters:; x (Int32Expression or Int64Expression). Returns:; Int32Expression. hail.expr.functions.exp(x)[source]. hail.expr.functions.expit(x)[source]. hail.expr.functions.is_nan(x)[source]. hail.expr.functions.is_finite(x)[source]. hail.expr.functions.is_infinite(x)[source]. hail.expr.functions.log(x, base=None)[source]; Take the logarithm of the x with base base.; Examples; >>> hl.eval(hl.log(10)); 2.302585092994046. >>> hl.eval(hl.log(10, 10)); 1.0. >>> hl.eval(hl.log(1024, 2)); 10.0. Notes; If the base argument is not supplied, then the natural logarithm is used. Parameters:. x (float or Expression of type tfloat64); base (float or Expression of type tfloat64). Returns:; Expression of type tfloat64. hail.expr.functions.log10(x)[source]. hail.expr.functions.logit(x)[source]. hail.expr.functions.floor(x)[source]. hail.expr.functions.ceil(x)[source]. hail.expr.functions.sqrt(x)[source]. hail.expr.functions.sign(x)[source]; Returns the sign of a numeric value, array or ndarray.; Examples; >>> hl.eval(hl.sign(-1.23)); -1.0. >>> hl.eval(hl.sign([-4, 0, 5])); [-1, 0, 1]. >>> hl.eval(hl.sign([0.0, 3.14])); [0.0, 1.0]. >>> hl.eval(hl.sign(float('nan'))); nan. Notes; The sign function preserves type and maps nan to nan. Parameters:; x (NumericExpression, ArrayNumericExpression or NDArrayNumericExpression). Returns:; NumericExpression, ArrayNumericExpression or NDArrayNumericExpression. hail.expr.functions.min(*exprs, filte",MatchSource.WIKI,docs/0.2/functions/numeric.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/numeric.html
https://hail.is/docs/0.2/functions/numeric.html:7500,Testability,log,log,7500,"ession or Int64Expression. hail.expr.functions.bit_not(x)[source]; Bitwise invert x.; Examples; >>> hl.eval(hl.bit_not(0)); -1. Notes; See the Python wiki; for more information about bit operators. Parameters:; x (Int32Expression or Int64Expression). Returns:; Int32Expression or Int64Expression. hail.expr.functions.bit_count(x)[source]; Count the number of 1s in the in the two’s complement binary representation of x.; Examples; The binary representation of 7 is 111, so:; >>> hl.eval(hl.bit_count(7)); 3. Parameters:; x (Int32Expression or Int64Expression). Returns:; Int32Expression. hail.expr.functions.exp(x)[source]. hail.expr.functions.expit(x)[source]. hail.expr.functions.is_nan(x)[source]. hail.expr.functions.is_finite(x)[source]. hail.expr.functions.is_infinite(x)[source]. hail.expr.functions.log(x, base=None)[source]; Take the logarithm of the x with base base.; Examples; >>> hl.eval(hl.log(10)); 2.302585092994046. >>> hl.eval(hl.log(10, 10)); 1.0. >>> hl.eval(hl.log(1024, 2)); 10.0. Notes; If the base argument is not supplied, then the natural logarithm is used. Parameters:. x (float or Expression of type tfloat64); base (float or Expression of type tfloat64). Returns:; Expression of type tfloat64. hail.expr.functions.log10(x)[source]. hail.expr.functions.logit(x)[source]. hail.expr.functions.floor(x)[source]. hail.expr.functions.ceil(x)[source]. hail.expr.functions.sqrt(x)[source]. hail.expr.functions.sign(x)[source]; Returns the sign of a numeric value, array or ndarray.; Examples; >>> hl.eval(hl.sign(-1.23)); -1.0. >>> hl.eval(hl.sign([-4, 0, 5])); [-1, 0, 1]. >>> hl.eval(hl.sign([0.0, 3.14])); [0.0, 1.0]. >>> hl.eval(hl.sign(float('nan'))); nan. Notes; The sign function preserves type and maps nan to nan. Parameters:; x (NumericExpression, ArrayNumericExpression or NDArrayNumericExpression). Returns:; NumericExpression, ArrayNumericExpression or NDArrayNumericExpression. hail.expr.functions.min(*exprs, filter_missing=True)[source]; Returns t",MatchSource.WIKI,docs/0.2/functions/numeric.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/numeric.html
https://hail.is/docs/0.2/functions/numeric.html:7583,Testability,log,logarithm,7583,"not(x)[source]; Bitwise invert x.; Examples; >>> hl.eval(hl.bit_not(0)); -1. Notes; See the Python wiki; for more information about bit operators. Parameters:; x (Int32Expression or Int64Expression). Returns:; Int32Expression or Int64Expression. hail.expr.functions.bit_count(x)[source]; Count the number of 1s in the in the two’s complement binary representation of x.; Examples; The binary representation of 7 is 111, so:; >>> hl.eval(hl.bit_count(7)); 3. Parameters:; x (Int32Expression or Int64Expression). Returns:; Int32Expression. hail.expr.functions.exp(x)[source]. hail.expr.functions.expit(x)[source]. hail.expr.functions.is_nan(x)[source]. hail.expr.functions.is_finite(x)[source]. hail.expr.functions.is_infinite(x)[source]. hail.expr.functions.log(x, base=None)[source]; Take the logarithm of the x with base base.; Examples; >>> hl.eval(hl.log(10)); 2.302585092994046. >>> hl.eval(hl.log(10, 10)); 1.0. >>> hl.eval(hl.log(1024, 2)); 10.0. Notes; If the base argument is not supplied, then the natural logarithm is used. Parameters:. x (float or Expression of type tfloat64); base (float or Expression of type tfloat64). Returns:; Expression of type tfloat64. hail.expr.functions.log10(x)[source]. hail.expr.functions.logit(x)[source]. hail.expr.functions.floor(x)[source]. hail.expr.functions.ceil(x)[source]. hail.expr.functions.sqrt(x)[source]. hail.expr.functions.sign(x)[source]; Returns the sign of a numeric value, array or ndarray.; Examples; >>> hl.eval(hl.sign(-1.23)); -1.0. >>> hl.eval(hl.sign([-4, 0, 5])); [-1, 0, 1]. >>> hl.eval(hl.sign([0.0, 3.14])); [0.0, 1.0]. >>> hl.eval(hl.sign(float('nan'))); nan. Notes; The sign function preserves type and maps nan to nan. Parameters:; x (NumericExpression, ArrayNumericExpression or NDArrayNumericExpression). Returns:; NumericExpression, ArrayNumericExpression or NDArrayNumericExpression. hail.expr.functions.min(*exprs, filter_missing=True)[source]; Returns the minimum element of a collection or of given numer",MatchSource.WIKI,docs/0.2/functions/numeric.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/numeric.html
https://hail.is/docs/0.2/functions/numeric.html:7800,Testability,log,logit,7800,"il.expr.functions.bit_count(x)[source]; Count the number of 1s in the in the two’s complement binary representation of x.; Examples; The binary representation of 7 is 111, so:; >>> hl.eval(hl.bit_count(7)); 3. Parameters:; x (Int32Expression or Int64Expression). Returns:; Int32Expression. hail.expr.functions.exp(x)[source]. hail.expr.functions.expit(x)[source]. hail.expr.functions.is_nan(x)[source]. hail.expr.functions.is_finite(x)[source]. hail.expr.functions.is_infinite(x)[source]. hail.expr.functions.log(x, base=None)[source]; Take the logarithm of the x with base base.; Examples; >>> hl.eval(hl.log(10)); 2.302585092994046. >>> hl.eval(hl.log(10, 10)); 1.0. >>> hl.eval(hl.log(1024, 2)); 10.0. Notes; If the base argument is not supplied, then the natural logarithm is used. Parameters:. x (float or Expression of type tfloat64); base (float or Expression of type tfloat64). Returns:; Expression of type tfloat64. hail.expr.functions.log10(x)[source]. hail.expr.functions.logit(x)[source]. hail.expr.functions.floor(x)[source]. hail.expr.functions.ceil(x)[source]. hail.expr.functions.sqrt(x)[source]. hail.expr.functions.sign(x)[source]; Returns the sign of a numeric value, array or ndarray.; Examples; >>> hl.eval(hl.sign(-1.23)); -1.0. >>> hl.eval(hl.sign([-4, 0, 5])); [-1, 0, 1]. >>> hl.eval(hl.sign([0.0, 3.14])); [0.0, 1.0]. >>> hl.eval(hl.sign(float('nan'))); nan. Notes; The sign function preserves type and maps nan to nan. Parameters:; x (NumericExpression, ArrayNumericExpression or NDArrayNumericExpression). Returns:; NumericExpression, ArrayNumericExpression or NDArrayNumericExpression. hail.expr.functions.min(*exprs, filter_missing=True)[source]; Returns the minimum element of a collection or of given numeric expressions.; Examples; Take the minimum value of an array:; >>> hl.eval(hl.min([1, 3, 5, 6, 7, 9])); 1. Take the minimum value of arguments:; >>> hl.eval(hl.min(1, 50, 2)); 1. Notes; Like the Python builtin min function, this function can eithe",MatchSource.WIKI,docs/0.2/functions/numeric.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/numeric.html
https://hail.is/docs/0.2/functions/random.html:4778,Deployability,pipeline,pipeline,4778,"6938964441,; 0.5550464170615771]. In fact, in this case we are getting the tail of; >>> table = hl.utils.range_table(7, 1).annotate(x=hl.rand_unif(0, 1, seed=0)); >>> table.x.collect(); [0.5820244750020055,; 0.33150686392731943,; 0.20526631289173847,; 0.6964416913998893,; 0.6092952493383876,; 0.6404026938964441,; 0.5550464170615771]. Reproducibility across sessions; The values of a random function are fully determined by three things:. The seed set on the function itself. If not specified, these are simply; generated sequentially.; Some data uniquely identifying the current position within a larger context,; e.g. Table, MatrixTable, or array. For instance, in a range_table(),; this data is simply the row id, as suggested by the previous examples.; The global seed. This is fixed for the entire session, and can only be set; using the global_seed argument to init(). To ensure reproducibility within a single hail session, it suffices to either; manually set the seed on every random function call, or to call; reset_global_randomness() at the start of a pipeline, which resets the; counter used to generate seeds.; >>> hl.reset_global_randomness(); >>> hl.eval(hl.array([hl.rand_unif(0, 1), hl.rand_unif(0, 1)])); [0.9828239225846387, 0.49094525115847415]. >>> hl.reset_global_randomness(); >>> hl.eval(hl.array([hl.rand_unif(0, 1), hl.rand_unif(0, 1)])); [0.9828239225846387, 0.49094525115847415]. To ensure reproducibility across sessions, one must in addition specify the; global_seed in init(). If not specified, the global seed is chosen; randomly. All documentation examples were computed using global_seed=0.; >>> hl.stop() ; >>> hl.init(global_seed=0) ; >>> hl.eval(hl.array([hl.rand_unif(0, 1), hl.rand_unif(0, 1)])) ; [0.9828239225846387, 0.49094525115847415]. rand_bool(p[, seed]); Returns True with probability p. rand_beta(a, b[, lower, upper, seed]); Samples from a beta distribution with parameters a (alpha) and b (beta). rand_cat(prob[, seed]); Samples from a categorical ",MatchSource.WIKI,docs/0.2/functions/random.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/random.html
https://hail.is/docs/0.2/functions/random.html:12833,Deployability,update,updated,12833,"0.; seed (int, optional) – Random seed.; size (int or tuple of int, optional). Returns:; Float64Expression. hail.expr.functions.rand_int32(a, b=None, *, seed=None)[source]; Samples from a uniform distribution of 32-bit integers.; If b is None, samples from the uniform distribution over [0, a). Otherwise, sample from the; uniform distribution over [a, b).; Examples; >>> hl.reset_global_randomness(); >>> hl.eval(hl.rand_int32(10)); 9. >>> hl.eval(hl.rand_int32(10, 15)); 14. >>> hl.eval(hl.rand_int32(10, 15)); 12. Parameters:. a (int or Int32Expression) – If b is None, the right boundary of the range; otherwise, the left boundary of range.; b (int or Int32Expression) – If specified, the right boundary of the range.; seed (int, optional) – Random seed. Returns:; Int32Expression. hail.expr.functions.rand_int64(a=None, b=None, *, seed=None)[source]; Samples from a uniform distribution of 64-bit integers.; If a and b are both specified, samples from the uniform distribution over [a, b).; If b is None, samples from the uniform distribution over [0, a).; If both a and b are None samples from the uniform distribution over all; 64-bit integers.; Examples; >>> hl.reset_global_randomness(); >>> hl.eval(hl.rand_int64(10)); 9. >>> hl.eval(hl.rand_int64(1 << 33, 1 << 35)); 33089740109. >>> hl.eval(hl.rand_int64(1 << 33, 1 << 35)); 18195458570. Parameters:. a (int or Int64Expression) – If b is None, the right boundary of the range; otherwise, the left boundary of range.; b (int or Int64Expression) – If specified, the right boundary of the range.; seed (int, optional) – Random seed. Returns:; Int64Expression. hail.expr.functions.shuffle(a, seed=None)[source]; Randomly permute an array; Example; >>> hl.reset_global_randomness(); >>> hl.eval(hl.shuffle(hl.range(5))); [4, 0, 2, 1, 3]. Parameters:. a (ArrayExpression) – Array to permute.; seed (int, optional) – Random seed. Returns:; ArrayExpression. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/functions/random.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/random.html
https://hail.is/docs/0.2/functions/random.html:4219,Usability,simpl,simply,4219,"nge_table(5, 5).annotate(x=hl.rand_unif(0, 1, seed=0)); >>> table.x.collect(); [0.5820244750020055,; 0.33150686392731943,; 0.20526631289173847,; 0.6964416913998893,; 0.6092952493383876]. However, moving it to a sufficiently different context will produce different; results:; >>> table = hl.utils.range_table(7, 1); >>> table = table.filter(table.idx >= 2).annotate(x=hl.rand_unif(0, 1, seed=0)); >>> table.x.collect(); [0.20526631289173847,; 0.6964416913998893,; 0.6092952493383876,; 0.6404026938964441,; 0.5550464170615771]. In fact, in this case we are getting the tail of; >>> table = hl.utils.range_table(7, 1).annotate(x=hl.rand_unif(0, 1, seed=0)); >>> table.x.collect(); [0.5820244750020055,; 0.33150686392731943,; 0.20526631289173847,; 0.6964416913998893,; 0.6092952493383876,; 0.6404026938964441,; 0.5550464170615771]. Reproducibility across sessions; The values of a random function are fully determined by three things:. The seed set on the function itself. If not specified, these are simply; generated sequentially.; Some data uniquely identifying the current position within a larger context,; e.g. Table, MatrixTable, or array. For instance, in a range_table(),; this data is simply the row id, as suggested by the previous examples.; The global seed. This is fixed for the entire session, and can only be set; using the global_seed argument to init(). To ensure reproducibility within a single hail session, it suffices to either; manually set the seed on every random function call, or to call; reset_global_randomness() at the start of a pipeline, which resets the; counter used to generate seeds.; >>> hl.reset_global_randomness(); >>> hl.eval(hl.array([hl.rand_unif(0, 1), hl.rand_unif(0, 1)])); [0.9828239225846387, 0.49094525115847415]. >>> hl.reset_global_randomness(); >>> hl.eval(hl.array([hl.rand_unif(0, 1), hl.rand_unif(0, 1)])); [0.9828239225846387, 0.49094525115847415]. To ensure reproducibility across sessions, one must in addition specify the; global_seed in init(",MatchSource.WIKI,docs/0.2/functions/random.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/random.html
https://hail.is/docs/0.2/functions/random.html:4413,Usability,simpl,simply,4413,"oving it to a sufficiently different context will produce different; results:; >>> table = hl.utils.range_table(7, 1); >>> table = table.filter(table.idx >= 2).annotate(x=hl.rand_unif(0, 1, seed=0)); >>> table.x.collect(); [0.20526631289173847,; 0.6964416913998893,; 0.6092952493383876,; 0.6404026938964441,; 0.5550464170615771]. In fact, in this case we are getting the tail of; >>> table = hl.utils.range_table(7, 1).annotate(x=hl.rand_unif(0, 1, seed=0)); >>> table.x.collect(); [0.5820244750020055,; 0.33150686392731943,; 0.20526631289173847,; 0.6964416913998893,; 0.6092952493383876,; 0.6404026938964441,; 0.5550464170615771]. Reproducibility across sessions; The values of a random function are fully determined by three things:. The seed set on the function itself. If not specified, these are simply; generated sequentially.; Some data uniquely identifying the current position within a larger context,; e.g. Table, MatrixTable, or array. For instance, in a range_table(),; this data is simply the row id, as suggested by the previous examples.; The global seed. This is fixed for the entire session, and can only be set; using the global_seed argument to init(). To ensure reproducibility within a single hail session, it suffices to either; manually set the seed on every random function call, or to call; reset_global_randomness() at the start of a pipeline, which resets the; counter used to generate seeds.; >>> hl.reset_global_randomness(); >>> hl.eval(hl.array([hl.rand_unif(0, 1), hl.rand_unif(0, 1)])); [0.9828239225846387, 0.49094525115847415]. >>> hl.reset_global_randomness(); >>> hl.eval(hl.array([hl.rand_unif(0, 1), hl.rand_unif(0, 1)])); [0.9828239225846387, 0.49094525115847415]. To ensure reproducibility across sessions, one must in addition specify the; global_seed in init(). If not specified, the global seed is chosen; randomly. All documentation examples were computed using global_seed=0.; >>> hl.stop() ; >>> hl.init(global_seed=0) ; >>> hl.eval(hl.array([hl.rand_u",MatchSource.WIKI,docs/0.2/functions/random.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/random.html
https://hail.is/docs/0.2/functions/stats.html:20108,Availability,error,error,20108,"Squared Distribution, we strongly recommend; the introduction of this paper:. Das, Abhranil; Geisler, Wilson (2020). “A method to integrate and classify normal; distributions”. Parameters:. x (float or Expression of type tfloat64) – The value at which to evaluate the cumulative distribution function (CDF).; w (list of float or Expression of type tarray of tfloat64) – A weight for each non-central chi-square term.; k (list of int or Expression of type tarray of tint32) – A degrees of freedom parameter for each non-central chi-square term.; lam (list of float or Expression of type tarray of tfloat64) – A non-centrality parameter for each non-central chi-square term. We use lam instead; of lambda because the latter is a reserved word in Python.; mu (float or Expression of type tfloat64) – The standard deviation of the normal term.; sigma (float or Expression of type tfloat64) – The standard deviation of the normal term.; max_iterations (int or Expression of type tint32) – The maximum number of iterations of the numerical integration before raising an error. The; default maximum number of iterations is 1e5.; min_accuracy (int or Expression of type tint32) – The minimum accuracy of the returned value. If the minimum accuracy is not achieved, this; function will raise an error. The default minimum accuracy is 1e-5. Returns:; StructExpression – This method returns a structure with the value as well as information about the numerical; integration. value : Float64Expression. If converged is true, the value of the CDF evaluated; at x. Otherwise, this is the last value the integration evaluated before aborting.; n_iterations : Int32Expression. The number of iterations before stopping.; converged : BooleanExpression. True if the min_accuracy was achieved and round; off error is not likely significant.; fault : Int32Expression. If converged is true, fault is zero. If converged is; false, fault is either one or two. One indicates that the requried accuracy was not; achieved. Two ",MatchSource.WIKI,docs/0.2/functions/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/stats.html
https://hail.is/docs/0.2/functions/stats.html:20330,Availability,error,error,20330,"o evaluate the cumulative distribution function (CDF).; w (list of float or Expression of type tarray of tfloat64) – A weight for each non-central chi-square term.; k (list of int or Expression of type tarray of tint32) – A degrees of freedom parameter for each non-central chi-square term.; lam (list of float or Expression of type tarray of tfloat64) – A non-centrality parameter for each non-central chi-square term. We use lam instead; of lambda because the latter is a reserved word in Python.; mu (float or Expression of type tfloat64) – The standard deviation of the normal term.; sigma (float or Expression of type tfloat64) – The standard deviation of the normal term.; max_iterations (int or Expression of type tint32) – The maximum number of iterations of the numerical integration before raising an error. The; default maximum number of iterations is 1e5.; min_accuracy (int or Expression of type tint32) – The minimum accuracy of the returned value. If the minimum accuracy is not achieved, this; function will raise an error. The default minimum accuracy is 1e-5. Returns:; StructExpression – This method returns a structure with the value as well as information about the numerical; integration. value : Float64Expression. If converged is true, the value of the CDF evaluated; at x. Otherwise, this is the last value the integration evaluated before aborting.; n_iterations : Int32Expression. The number of iterations before stopping.; converged : BooleanExpression. True if the min_accuracy was achieved and round; off error is not likely significant.; fault : Int32Expression. If converged is true, fault is zero. If converged is; false, fault is either one or two. One indicates that the requried accuracy was not; achieved. Two indicates the round-off error is possibly significant. hail.expr.functions.pnorm(x, mu=0, sigma=1, lower_tail=True, log_p=False)[source]; The cumulative probability function of a normal distribution with mean; mu and standard deviation sigma. Returns cu",MatchSource.WIKI,docs/0.2/functions/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/stats.html
https://hail.is/docs/0.2/functions/stats.html:20832,Availability,error,error,20832,"of type tfloat64) – The standard deviation of the normal term.; sigma (float or Expression of type tfloat64) – The standard deviation of the normal term.; max_iterations (int or Expression of type tint32) – The maximum number of iterations of the numerical integration before raising an error. The; default maximum number of iterations is 1e5.; min_accuracy (int or Expression of type tint32) – The minimum accuracy of the returned value. If the minimum accuracy is not achieved, this; function will raise an error. The default minimum accuracy is 1e-5. Returns:; StructExpression – This method returns a structure with the value as well as information about the numerical; integration. value : Float64Expression. If converged is true, the value of the CDF evaluated; at x. Otherwise, this is the last value the integration evaluated before aborting.; n_iterations : Int32Expression. The number of iterations before stopping.; converged : BooleanExpression. True if the min_accuracy was achieved and round; off error is not likely significant.; fault : Int32Expression. If converged is true, fault is zero. If converged is; false, fault is either one or two. One indicates that the requried accuracy was not; achieved. Two indicates the round-off error is possibly significant. hail.expr.functions.pnorm(x, mu=0, sigma=1, lower_tail=True, log_p=False)[source]; The cumulative probability function of a normal distribution with mean; mu and standard deviation sigma. Returns cumulative probability of; standard normal distribution by default.; Examples; >>> hl.eval(hl.pnorm(0)); 0.5. >>> hl.eval(hl.pnorm(1, mu=2, sigma=2)); 0.30853753872598694. >>> hl.eval(hl.pnorm(2, lower_tail=False)); 0.022750131948179212. >>> hl.eval(hl.pnorm(2, log_p=True)); -0.023012909328963493. Notes; Returns the left-tail probability p = Prob(\(Z < x\)) with \(Z\); a normal random variable. Defaults to a standard normal random variable. Parameters:. x (float or Expression of type tfloat64); mu (float or Expression o",MatchSource.WIKI,docs/0.2/functions/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/stats.html
https://hail.is/docs/0.2/functions/stats.html:20866,Availability,fault,fault,20866," term.; sigma (float or Expression of type tfloat64) – The standard deviation of the normal term.; max_iterations (int or Expression of type tint32) – The maximum number of iterations of the numerical integration before raising an error. The; default maximum number of iterations is 1e5.; min_accuracy (int or Expression of type tint32) – The minimum accuracy of the returned value. If the minimum accuracy is not achieved, this; function will raise an error. The default minimum accuracy is 1e-5. Returns:; StructExpression – This method returns a structure with the value as well as information about the numerical; integration. value : Float64Expression. If converged is true, the value of the CDF evaluated; at x. Otherwise, this is the last value the integration evaluated before aborting.; n_iterations : Int32Expression. The number of iterations before stopping.; converged : BooleanExpression. True if the min_accuracy was achieved and round; off error is not likely significant.; fault : Int32Expression. If converged is true, fault is zero. If converged is; false, fault is either one or two. One indicates that the requried accuracy was not; achieved. Two indicates the round-off error is possibly significant. hail.expr.functions.pnorm(x, mu=0, sigma=1, lower_tail=True, log_p=False)[source]; The cumulative probability function of a normal distribution with mean; mu and standard deviation sigma. Returns cumulative probability of; standard normal distribution by default.; Examples; >>> hl.eval(hl.pnorm(0)); 0.5. >>> hl.eval(hl.pnorm(1, mu=2, sigma=2)); 0.30853753872598694. >>> hl.eval(hl.pnorm(2, lower_tail=False)); 0.022750131948179212. >>> hl.eval(hl.pnorm(2, log_p=True)); -0.023012909328963493. Notes; Returns the left-tail probability p = Prob(\(Z < x\)) with \(Z\); a normal random variable. Defaults to a standard normal random variable. Parameters:. x (float or Expression of type tfloat64); mu (float or Expression of type tfloat64) – Mean (default = 0).; sigma (float or ",MatchSource.WIKI,docs/0.2/functions/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/stats.html
https://hail.is/docs/0.2/functions/stats.html:20913,Availability,fault,fault,20913,"ion of type tfloat64) – The standard deviation of the normal term.; max_iterations (int or Expression of type tint32) – The maximum number of iterations of the numerical integration before raising an error. The; default maximum number of iterations is 1e5.; min_accuracy (int or Expression of type tint32) – The minimum accuracy of the returned value. If the minimum accuracy is not achieved, this; function will raise an error. The default minimum accuracy is 1e-5. Returns:; StructExpression – This method returns a structure with the value as well as information about the numerical; integration. value : Float64Expression. If converged is true, the value of the CDF evaluated; at x. Otherwise, this is the last value the integration evaluated before aborting.; n_iterations : Int32Expression. The number of iterations before stopping.; converged : BooleanExpression. True if the min_accuracy was achieved and round; off error is not likely significant.; fault : Int32Expression. If converged is true, fault is zero. If converged is; false, fault is either one or two. One indicates that the requried accuracy was not; achieved. Two indicates the round-off error is possibly significant. hail.expr.functions.pnorm(x, mu=0, sigma=1, lower_tail=True, log_p=False)[source]; The cumulative probability function of a normal distribution with mean; mu and standard deviation sigma. Returns cumulative probability of; standard normal distribution by default.; Examples; >>> hl.eval(hl.pnorm(0)); 0.5. >>> hl.eval(hl.pnorm(1, mu=2, sigma=2)); 0.30853753872598694. >>> hl.eval(hl.pnorm(2, lower_tail=False)); 0.022750131948179212. >>> hl.eval(hl.pnorm(2, log_p=True)); -0.023012909328963493. Notes; Returns the left-tail probability p = Prob(\(Z < x\)) with \(Z\); a normal random variable. Defaults to a standard normal random variable. Parameters:. x (float or Expression of type tfloat64); mu (float or Expression of type tfloat64) – Mean (default = 0).; sigma (float or Expression of type tfloat64) – S",MatchSource.WIKI,docs/0.2/functions/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/stats.html
https://hail.is/docs/0.2/functions/stats.html:20952,Availability,fault,fault,20952,"n of the normal term.; max_iterations (int or Expression of type tint32) – The maximum number of iterations of the numerical integration before raising an error. The; default maximum number of iterations is 1e5.; min_accuracy (int or Expression of type tint32) – The minimum accuracy of the returned value. If the minimum accuracy is not achieved, this; function will raise an error. The default minimum accuracy is 1e-5. Returns:; StructExpression – This method returns a structure with the value as well as information about the numerical; integration. value : Float64Expression. If converged is true, the value of the CDF evaluated; at x. Otherwise, this is the last value the integration evaluated before aborting.; n_iterations : Int32Expression. The number of iterations before stopping.; converged : BooleanExpression. True if the min_accuracy was achieved and round; off error is not likely significant.; fault : Int32Expression. If converged is true, fault is zero. If converged is; false, fault is either one or two. One indicates that the requried accuracy was not; achieved. Two indicates the round-off error is possibly significant. hail.expr.functions.pnorm(x, mu=0, sigma=1, lower_tail=True, log_p=False)[source]; The cumulative probability function of a normal distribution with mean; mu and standard deviation sigma. Returns cumulative probability of; standard normal distribution by default.; Examples; >>> hl.eval(hl.pnorm(0)); 0.5. >>> hl.eval(hl.pnorm(1, mu=2, sigma=2)); 0.30853753872598694. >>> hl.eval(hl.pnorm(2, lower_tail=False)); 0.022750131948179212. >>> hl.eval(hl.pnorm(2, log_p=True)); -0.023012909328963493. Notes; Returns the left-tail probability p = Prob(\(Z < x\)) with \(Z\); a normal random variable. Defaults to a standard normal random variable. Parameters:. x (float or Expression of type tfloat64); mu (float or Expression of type tfloat64) – Mean (default = 0).; sigma (float or Expression of type tfloat64) – Standard deviation (default = 1).; lower_tail",MatchSource.WIKI,docs/0.2/functions/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/stats.html
https://hail.is/docs/0.2/functions/stats.html:21068,Availability,error,error,21068,"numerical integration before raising an error. The; default maximum number of iterations is 1e5.; min_accuracy (int or Expression of type tint32) – The minimum accuracy of the returned value. If the minimum accuracy is not achieved, this; function will raise an error. The default minimum accuracy is 1e-5. Returns:; StructExpression – This method returns a structure with the value as well as information about the numerical; integration. value : Float64Expression. If converged is true, the value of the CDF evaluated; at x. Otherwise, this is the last value the integration evaluated before aborting.; n_iterations : Int32Expression. The number of iterations before stopping.; converged : BooleanExpression. True if the min_accuracy was achieved and round; off error is not likely significant.; fault : Int32Expression. If converged is true, fault is zero. If converged is; false, fault is either one or two. One indicates that the requried accuracy was not; achieved. Two indicates the round-off error is possibly significant. hail.expr.functions.pnorm(x, mu=0, sigma=1, lower_tail=True, log_p=False)[source]; The cumulative probability function of a normal distribution with mean; mu and standard deviation sigma. Returns cumulative probability of; standard normal distribution by default.; Examples; >>> hl.eval(hl.pnorm(0)); 0.5. >>> hl.eval(hl.pnorm(1, mu=2, sigma=2)); 0.30853753872598694. >>> hl.eval(hl.pnorm(2, lower_tail=False)); 0.022750131948179212. >>> hl.eval(hl.pnorm(2, log_p=True)); -0.023012909328963493. Notes; Returns the left-tail probability p = Prob(\(Z < x\)) with \(Z\); a normal random variable. Defaults to a standard normal random variable. Parameters:. x (float or Expression of type tfloat64); mu (float or Expression of type tfloat64) – Mean (default = 0).; sigma (float or Expression of type tfloat64) – Standard deviation (default = 1).; lower_tail (bool or BooleanExpression) – If True, compute the probability of an outcome at or below x,; otherwise greater than",MatchSource.WIKI,docs/0.2/functions/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/stats.html
https://hail.is/docs/0.2/functions/stats.html:18503,Deployability,release,released,18503,"> hl.eval(hl.pgenchisq(40 , w=[-2, -1], k=[5, 2], lam=[3, 1], mu=-3, sigma=0).value); 1.0. >>> hl.eval(hl.pgenchisq(-80, w=[1, -10, 2], k=[1, 2, 3], lam=[2, 3, 7], mu=-10, sigma=0).value); 0.14284718767288906; >>> hl.eval(hl.pgenchisq(-20, w=[1, -10, 2], k=[1, 2, 3], lam=[2, 3, 7], mu=-10, sigma=0).value); 0.5950150356303258; >>> hl.eval(hl.pgenchisq(10 , w=[1, -10, 2], k=[1, 2, 3], lam=[2, 3, 7], mu=-10, sigma=0).value); 0.923219534175858; >>> hl.eval(hl.pgenchisq(40 , w=[1, -10, 2], k=[1, 2, 3], lam=[2, 3, 7], mu=-10, sigma=0).value); 0.9971746768781656. Notes; We follow Wikipedia’s notational conventions. Some texts refer to the weight vector (our w) as; \(\lambda\) or lb and the non-centrality vector (our lam) as nc.; We use the Davies’ algorithm which was published as:. Davies, Robert. “The distribution of a linear combination of chi-squared random variables.”; Applied Statistics 29 323-333. 1980. Davies included Fortran source code in the original publication. Davies also released a C; language port. Hail’s implementation is a fairly direct port; of the C implementation to Scala. Davies provides 39 test cases with the source code. The Hail; tests include all 39 test cases as well as a few additional tests.; Davies’ website cautions:. The method works well in most situations if you want only modest accuracy, say 0.0001. But; problems may arise if the sum is dominated by one or two terms with a total of only one or; two degrees of freedom and x is small. For an accessible introduction the Generalized Chi-Squared Distribution, we strongly recommend; the introduction of this paper:. Das, Abhranil; Geisler, Wilson (2020). “A method to integrate and classify normal; distributions”. Parameters:. x (float or Expression of type tfloat64) – The value at which to evaluate the cumulative distribution function (CDF).; w (list of float or Expression of type tarray of tfloat64) – A weight for each non-central chi-square term.; k (list of int or Expression of type tarray of ti",MatchSource.WIKI,docs/0.2/functions/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/stats.html
https://hail.is/docs/0.2/functions/stats.html:19174,Deployability,integrat,integrate,19174," lb and the non-centrality vector (our lam) as nc.; We use the Davies’ algorithm which was published as:. Davies, Robert. “The distribution of a linear combination of chi-squared random variables.”; Applied Statistics 29 323-333. 1980. Davies included Fortran source code in the original publication. Davies also released a C; language port. Hail’s implementation is a fairly direct port; of the C implementation to Scala. Davies provides 39 test cases with the source code. The Hail; tests include all 39 test cases as well as a few additional tests.; Davies’ website cautions:. The method works well in most situations if you want only modest accuracy, say 0.0001. But; problems may arise if the sum is dominated by one or two terms with a total of only one or; two degrees of freedom and x is small. For an accessible introduction the Generalized Chi-Squared Distribution, we strongly recommend; the introduction of this paper:. Das, Abhranil; Geisler, Wilson (2020). “A method to integrate and classify normal; distributions”. Parameters:. x (float or Expression of type tfloat64) – The value at which to evaluate the cumulative distribution function (CDF).; w (list of float or Expression of type tarray of tfloat64) – A weight for each non-central chi-square term.; k (list of int or Expression of type tarray of tint32) – A degrees of freedom parameter for each non-central chi-square term.; lam (list of float or Expression of type tarray of tfloat64) – A non-centrality parameter for each non-central chi-square term. We use lam instead; of lambda because the latter is a reserved word in Python.; mu (float or Expression of type tfloat64) – The standard deviation of the normal term.; sigma (float or Expression of type tfloat64) – The standard deviation of the normal term.; max_iterations (int or Expression of type tint32) – The maximum number of iterations of the numerical integration before raising an error. The; default maximum number of iterations is 1e5.; min_accuracy (int or Exp",MatchSource.WIKI,docs/0.2/functions/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/stats.html
https://hail.is/docs/0.2/functions/stats.html:20078,Deployability,integrat,integration,20078,"Squared Distribution, we strongly recommend; the introduction of this paper:. Das, Abhranil; Geisler, Wilson (2020). “A method to integrate and classify normal; distributions”. Parameters:. x (float or Expression of type tfloat64) – The value at which to evaluate the cumulative distribution function (CDF).; w (list of float or Expression of type tarray of tfloat64) – A weight for each non-central chi-square term.; k (list of int or Expression of type tarray of tint32) – A degrees of freedom parameter for each non-central chi-square term.; lam (list of float or Expression of type tarray of tfloat64) – A non-centrality parameter for each non-central chi-square term. We use lam instead; of lambda because the latter is a reserved word in Python.; mu (float or Expression of type tfloat64) – The standard deviation of the normal term.; sigma (float or Expression of type tfloat64) – The standard deviation of the normal term.; max_iterations (int or Expression of type tint32) – The maximum number of iterations of the numerical integration before raising an error. The; default maximum number of iterations is 1e5.; min_accuracy (int or Expression of type tint32) – The minimum accuracy of the returned value. If the minimum accuracy is not achieved, this; function will raise an error. The default minimum accuracy is 1e-5. Returns:; StructExpression – This method returns a structure with the value as well as information about the numerical; integration. value : Float64Expression. If converged is true, the value of the CDF evaluated; at x. Otherwise, this is the last value the integration evaluated before aborting.; n_iterations : Int32Expression. The number of iterations before stopping.; converged : BooleanExpression. True if the min_accuracy was achieved and round; off error is not likely significant.; fault : Int32Expression. If converged is true, fault is zero. If converged is; false, fault is either one or two. One indicates that the requried accuracy was not; achieved. Two ",MatchSource.WIKI,docs/0.2/functions/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/stats.html
https://hail.is/docs/0.2/functions/stats.html:20495,Deployability,integrat,integration,20495,"ral chi-square term.; k (list of int or Expression of type tarray of tint32) – A degrees of freedom parameter for each non-central chi-square term.; lam (list of float or Expression of type tarray of tfloat64) – A non-centrality parameter for each non-central chi-square term. We use lam instead; of lambda because the latter is a reserved word in Python.; mu (float or Expression of type tfloat64) – The standard deviation of the normal term.; sigma (float or Expression of type tfloat64) – The standard deviation of the normal term.; max_iterations (int or Expression of type tint32) – The maximum number of iterations of the numerical integration before raising an error. The; default maximum number of iterations is 1e5.; min_accuracy (int or Expression of type tint32) – The minimum accuracy of the returned value. If the minimum accuracy is not achieved, this; function will raise an error. The default minimum accuracy is 1e-5. Returns:; StructExpression – This method returns a structure with the value as well as information about the numerical; integration. value : Float64Expression. If converged is true, the value of the CDF evaluated; at x. Otherwise, this is the last value the integration evaluated before aborting.; n_iterations : Int32Expression. The number of iterations before stopping.; converged : BooleanExpression. True if the min_accuracy was achieved and round; off error is not likely significant.; fault : Int32Expression. If converged is true, fault is zero. If converged is; false, fault is either one or two. One indicates that the requried accuracy was not; achieved. Two indicates the round-off error is possibly significant. hail.expr.functions.pnorm(x, mu=0, sigma=1, lower_tail=True, log_p=False)[source]; The cumulative probability function of a normal distribution with mean; mu and standard deviation sigma. Returns cumulative probability of; standard normal distribution by default.; Examples; >>> hl.eval(hl.pnorm(0)); 0.5. >>> hl.eval(hl.pnorm(1, mu=2, sigma",MatchSource.WIKI,docs/0.2/functions/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/stats.html
https://hail.is/docs/0.2/functions/stats.html:20633,Deployability,integrat,integration,20633,"rray of tfloat64) – A non-centrality parameter for each non-central chi-square term. We use lam instead; of lambda because the latter is a reserved word in Python.; mu (float or Expression of type tfloat64) – The standard deviation of the normal term.; sigma (float or Expression of type tfloat64) – The standard deviation of the normal term.; max_iterations (int or Expression of type tint32) – The maximum number of iterations of the numerical integration before raising an error. The; default maximum number of iterations is 1e5.; min_accuracy (int or Expression of type tint32) – The minimum accuracy of the returned value. If the minimum accuracy is not achieved, this; function will raise an error. The default minimum accuracy is 1e-5. Returns:; StructExpression – This method returns a structure with the value as well as information about the numerical; integration. value : Float64Expression. If converged is true, the value of the CDF evaluated; at x. Otherwise, this is the last value the integration evaluated before aborting.; n_iterations : Int32Expression. The number of iterations before stopping.; converged : BooleanExpression. True if the min_accuracy was achieved and round; off error is not likely significant.; fault : Int32Expression. If converged is true, fault is zero. If converged is; false, fault is either one or two. One indicates that the requried accuracy was not; achieved. Two indicates the round-off error is possibly significant. hail.expr.functions.pnorm(x, mu=0, sigma=1, lower_tail=True, log_p=False)[source]; The cumulative probability function of a normal distribution with mean; mu and standard deviation sigma. Returns cumulative probability of; standard normal distribution by default.; Examples; >>> hl.eval(hl.pnorm(0)); 0.5. >>> hl.eval(hl.pnorm(1, mu=2, sigma=2)); 0.30853753872598694. >>> hl.eval(hl.pnorm(2, lower_tail=False)); 0.022750131948179212. >>> hl.eval(hl.pnorm(2, log_p=True)); -0.023012909328963493. Notes; Returns the left-tail probabili",MatchSource.WIKI,docs/0.2/functions/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/stats.html
https://hail.is/docs/0.2/functions/stats.html:28031,Deployability,update,updated,28031,"e tfloat64. hail.expr.functions.qnorm(p, mu=0, sigma=1, lower_tail=True, log_p=False)[source]; The quantile function of a normal distribution with mean mu and; standard deviation sigma, inverts pnorm(). Returns quantile of; standard normal distribution by default.; Examples; >>> hl.eval(hl.qnorm(0.90)); 1.2815515655446008. >>> hl.eval(hl.qnorm(0.90, mu=1, sigma=2)); 3.5631031310892016. >>> hl.eval(hl.qnorm(0.90, lower_tail=False)); -1.2815515655446008. >>> hl.eval(hl.qnorm(hl.log(0.90), log_p=True)); 1.2815515655446008. Notes; Returns left-quantile x for which p = Prob(\(Z\) < x) with \(Z\); a normal random variable with mean mu and standard deviation sigma.; Defaults to a standard normal random variable, and the probability p must; satisfy 0 < p < 1. Parameters:. p (float or Expression of type tfloat64) – Probability.; mu (float or Expression of type tfloat64) – Mean (default = 0).; sigma (float or Expression of type tfloat64) – Standard deviation (default = 1).; lower_tail (bool or BooleanExpression) – Corresponds to lower_tail parameter in pnorm().; log_p (bool or BooleanExpression) – Exponentiate p, corresponds to log_p parameter in pnorm(). Returns:; Expression of type tfloat64. hail.expr.functions.qpois(p, lamb, lower_tail=True, log_p=False)[source]; The quantile function of a Poisson distribution with rate parameter; lamb, inverts ppois().; Examples; >>> hl.eval(hl.qpois(0.99, 1)); 4. Notes; Returns the smallest integer \(x\) such that Prob(\(X \leq x\)) \(\geq\) p where \(X\); is a Poisson random variable with rate parameter lambda. Parameters:. p (float or Expression of type tfloat64); lamb (float or Expression of type tfloat64) – Rate parameter of Poisson distribution.; lower_tail (bool or BooleanExpression) – Corresponds to lower_tail parameter in inverse ppois().; log_p (bool or BooleanExpression) – Exponentiate p before testing. Returns:; Expression of type tfloat64. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/functions/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/stats.html
https://hail.is/docs/0.2/functions/stats.html:11568,Energy Efficiency,efficient,efficient,11568,"e parameter lamb.; Examples; >>> hl.eval(hl.dpois(5, 3)); 0.10081881344492458. Parameters:. x (float or Expression of type tfloat64) – Non-negative number at which to compute the probability density.; lamb (float or Expression of type tfloat64) – Poisson rate parameter. Must be non-negative.; log_p (bool or BooleanExpression) – If True, the natural logarithm of the probability density is returned. Returns:; Expression of type tfloat64 – The (log) probability density. hail.expr.functions.hardy_weinberg_test(n_hom_ref, n_het, n_hom_var, one_sided=False)[source]; Performs test of Hardy-Weinberg equilibrium.; Examples; >>> hl.eval(hl.hardy_weinberg_test(250, 500, 250)); Struct(het_freq_hwe=0.5002501250625313, p_value=0.9747844394217698). >>> hl.eval(hl.hardy_weinberg_test(37, 200, 85)); Struct(het_freq_hwe=0.48964964307448583, p_value=1.1337210383168987e-06). Notes; By default, this method performs a two-sided exact test with mid-p-value correction of; Hardy-Weinberg equilibrium; via an efficient implementation of the; Levene-Haldane distribution,; which models the number of heterozygous individuals under equilibrium.; The mean of this distribution is (n_ref * n_var) / (2n - 1), where; n_ref = 2*n_hom_ref + n_het is the number of reference alleles,; n_var = 2*n_hom_var + n_het is the number of variant alleles,; and n = n_hom_ref + n_het + n_hom_var is the number of individuals.; So the expected frequency of heterozygotes under equilibrium,; het_freq_hwe, is this mean divided by n.; To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set one_sided=True and the p-value returned will be; from the one-sided exact test. Parameters:. n_hom_ref (int or Expression of type tint32) – Number of homozygous reference genotypes.; n_het (int or Expression of type tint32) – Number of heterozygous genotypes.; n_hom_var (int or Expression of type tint32) – Number of homozygous variant genotypes.; one_sided (bool) – False by default. When True, ",MatchSource.WIKI,docs/0.2/functions/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/stats.html
https://hail.is/docs/0.2/functions/stats.html:19174,Integrability,integrat,integrate,19174," lb and the non-centrality vector (our lam) as nc.; We use the Davies’ algorithm which was published as:. Davies, Robert. “The distribution of a linear combination of chi-squared random variables.”; Applied Statistics 29 323-333. 1980. Davies included Fortran source code in the original publication. Davies also released a C; language port. Hail’s implementation is a fairly direct port; of the C implementation to Scala. Davies provides 39 test cases with the source code. The Hail; tests include all 39 test cases as well as a few additional tests.; Davies’ website cautions:. The method works well in most situations if you want only modest accuracy, say 0.0001. But; problems may arise if the sum is dominated by one or two terms with a total of only one or; two degrees of freedom and x is small. For an accessible introduction the Generalized Chi-Squared Distribution, we strongly recommend; the introduction of this paper:. Das, Abhranil; Geisler, Wilson (2020). “A method to integrate and classify normal; distributions”. Parameters:. x (float or Expression of type tfloat64) – The value at which to evaluate the cumulative distribution function (CDF).; w (list of float or Expression of type tarray of tfloat64) – A weight for each non-central chi-square term.; k (list of int or Expression of type tarray of tint32) – A degrees of freedom parameter for each non-central chi-square term.; lam (list of float or Expression of type tarray of tfloat64) – A non-centrality parameter for each non-central chi-square term. We use lam instead; of lambda because the latter is a reserved word in Python.; mu (float or Expression of type tfloat64) – The standard deviation of the normal term.; sigma (float or Expression of type tfloat64) – The standard deviation of the normal term.; max_iterations (int or Expression of type tint32) – The maximum number of iterations of the numerical integration before raising an error. The; default maximum number of iterations is 1e5.; min_accuracy (int or Exp",MatchSource.WIKI,docs/0.2/functions/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/stats.html
https://hail.is/docs/0.2/functions/stats.html:20078,Integrability,integrat,integration,20078,"Squared Distribution, we strongly recommend; the introduction of this paper:. Das, Abhranil; Geisler, Wilson (2020). “A method to integrate and classify normal; distributions”. Parameters:. x (float or Expression of type tfloat64) – The value at which to evaluate the cumulative distribution function (CDF).; w (list of float or Expression of type tarray of tfloat64) – A weight for each non-central chi-square term.; k (list of int or Expression of type tarray of tint32) – A degrees of freedom parameter for each non-central chi-square term.; lam (list of float or Expression of type tarray of tfloat64) – A non-centrality parameter for each non-central chi-square term. We use lam instead; of lambda because the latter is a reserved word in Python.; mu (float or Expression of type tfloat64) – The standard deviation of the normal term.; sigma (float or Expression of type tfloat64) – The standard deviation of the normal term.; max_iterations (int or Expression of type tint32) – The maximum number of iterations of the numerical integration before raising an error. The; default maximum number of iterations is 1e5.; min_accuracy (int or Expression of type tint32) – The minimum accuracy of the returned value. If the minimum accuracy is not achieved, this; function will raise an error. The default minimum accuracy is 1e-5. Returns:; StructExpression – This method returns a structure with the value as well as information about the numerical; integration. value : Float64Expression. If converged is true, the value of the CDF evaluated; at x. Otherwise, this is the last value the integration evaluated before aborting.; n_iterations : Int32Expression. The number of iterations before stopping.; converged : BooleanExpression. True if the min_accuracy was achieved and round; off error is not likely significant.; fault : Int32Expression. If converged is true, fault is zero. If converged is; false, fault is either one or two. One indicates that the requried accuracy was not; achieved. Two ",MatchSource.WIKI,docs/0.2/functions/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/stats.html
https://hail.is/docs/0.2/functions/stats.html:20495,Integrability,integrat,integration,20495,"ral chi-square term.; k (list of int or Expression of type tarray of tint32) – A degrees of freedom parameter for each non-central chi-square term.; lam (list of float or Expression of type tarray of tfloat64) – A non-centrality parameter for each non-central chi-square term. We use lam instead; of lambda because the latter is a reserved word in Python.; mu (float or Expression of type tfloat64) – The standard deviation of the normal term.; sigma (float or Expression of type tfloat64) – The standard deviation of the normal term.; max_iterations (int or Expression of type tint32) – The maximum number of iterations of the numerical integration before raising an error. The; default maximum number of iterations is 1e5.; min_accuracy (int or Expression of type tint32) – The minimum accuracy of the returned value. If the minimum accuracy is not achieved, this; function will raise an error. The default minimum accuracy is 1e-5. Returns:; StructExpression – This method returns a structure with the value as well as information about the numerical; integration. value : Float64Expression. If converged is true, the value of the CDF evaluated; at x. Otherwise, this is the last value the integration evaluated before aborting.; n_iterations : Int32Expression. The number of iterations before stopping.; converged : BooleanExpression. True if the min_accuracy was achieved and round; off error is not likely significant.; fault : Int32Expression. If converged is true, fault is zero. If converged is; false, fault is either one or two. One indicates that the requried accuracy was not; achieved. Two indicates the round-off error is possibly significant. hail.expr.functions.pnorm(x, mu=0, sigma=1, lower_tail=True, log_p=False)[source]; The cumulative probability function of a normal distribution with mean; mu and standard deviation sigma. Returns cumulative probability of; standard normal distribution by default.; Examples; >>> hl.eval(hl.pnorm(0)); 0.5. >>> hl.eval(hl.pnorm(1, mu=2, sigma",MatchSource.WIKI,docs/0.2/functions/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/stats.html
https://hail.is/docs/0.2/functions/stats.html:20633,Integrability,integrat,integration,20633,"rray of tfloat64) – A non-centrality parameter for each non-central chi-square term. We use lam instead; of lambda because the latter is a reserved word in Python.; mu (float or Expression of type tfloat64) – The standard deviation of the normal term.; sigma (float or Expression of type tfloat64) – The standard deviation of the normal term.; max_iterations (int or Expression of type tint32) – The maximum number of iterations of the numerical integration before raising an error. The; default maximum number of iterations is 1e5.; min_accuracy (int or Expression of type tint32) – The minimum accuracy of the returned value. If the minimum accuracy is not achieved, this; function will raise an error. The default minimum accuracy is 1e-5. Returns:; StructExpression – This method returns a structure with the value as well as information about the numerical; integration. value : Float64Expression. If converged is true, the value of the CDF evaluated; at x. Otherwise, this is the last value the integration evaluated before aborting.; n_iterations : Int32Expression. The number of iterations before stopping.; converged : BooleanExpression. True if the min_accuracy was achieved and round; off error is not likely significant.; fault : Int32Expression. If converged is true, fault is zero. If converged is; false, fault is either one or two. One indicates that the requried accuracy was not; achieved. Two indicates the round-off error is possibly significant. hail.expr.functions.pnorm(x, mu=0, sigma=1, lower_tail=True, log_p=False)[source]; The cumulative probability function of a normal distribution with mean; mu and standard deviation sigma. Returns cumulative probability of; standard normal distribution by default.; Examples; >>> hl.eval(hl.pnorm(0)); 0.5. >>> hl.eval(hl.pnorm(1, mu=2, sigma=2)); 0.30853753872598694. >>> hl.eval(hl.pnorm(2, lower_tail=False)); 0.022750131948179212. >>> hl.eval(hl.pnorm(2, log_p=True)); -0.023012909328963493. Notes; Returns the left-tail probabili",MatchSource.WIKI,docs/0.2/functions/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/stats.html
https://hail.is/docs/0.2/functions/stats.html:15610,Modifiability,variab,variables,15610,"qtail(5, 1, lower_tail=True)); 0.9746526813225317. >>> hl.eval(hl.pchisqtail(5, 1, log_p=True)); -3.6750823266311876. Parameters:. x (float or Expression of type tfloat64) – The value at which to evaluate the CDF.; df (float or Expression of type tfloat64) – Degrees of freedom.; ncp (float or Expression of type tfloat64) – Noncentrality parameter, defaults to 0 if unspecified.; lower_tail (bool or BooleanExpression) – If True, compute the probability of an outcome at or below x,; otherwise greater than x.; log_p (bool or BooleanExpression) – Return the natural logarithm of the probability. Returns:; Expression of type tfloat64. hail.expr.functions.pgenchisq(x, w, k, lam, mu, sigma, *, max_iterations=None, min_accuracy=None)[source]; The cumulative probability function of a generalized chi-squared distribution.; The generalized chi-squared distribution has many interpretations. We share here four; interpretations of the values of this distribution:. A linear combination of normal variables and squares of normal variables.; A weighted sum of sums of squares of normally distributed values plus a normally distributed; value.; A weighted sum of chi-squared distributed values plus a normally distributed value.; A “quadratic form” in a vector; of uncorrelated standard normal values. The parameters of this function correspond to the parameters of the third interpretation. \[\begin{aligned}; w &: R^n \quad k : Z^n \quad lam : R^n \quad mu : R \quad sigma : R \\; \\; x &\sim N(mu, sigma^2) \\; y_i &\sim \mathrm{NonCentralChiSquared}(k_i, lam_i) \\; \\; Z &= x + w y^T \\; &= x + \sum_i w_i y_i \\; Z &\sim \mathrm{GeneralizedNonCentralChiSquared}(w, k, lam, mu, sigma); \end{aligned}\]; The generalized chi-squared distribution often arises when working on linear models with standard; normal noise because the sum of the squares of the residuals should follow a generalized; chi-squared distribution.; Examples; The following plot shows three examples of the generalized chi-squared",MatchSource.WIKI,docs/0.2/functions/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/stats.html
https://hail.is/docs/0.2/functions/stats.html:15642,Modifiability,variab,variables,15642,"qtail(5, 1, lower_tail=True)); 0.9746526813225317. >>> hl.eval(hl.pchisqtail(5, 1, log_p=True)); -3.6750823266311876. Parameters:. x (float or Expression of type tfloat64) – The value at which to evaluate the CDF.; df (float or Expression of type tfloat64) – Degrees of freedom.; ncp (float or Expression of type tfloat64) – Noncentrality parameter, defaults to 0 if unspecified.; lower_tail (bool or BooleanExpression) – If True, compute the probability of an outcome at or below x,; otherwise greater than x.; log_p (bool or BooleanExpression) – Return the natural logarithm of the probability. Returns:; Expression of type tfloat64. hail.expr.functions.pgenchisq(x, w, k, lam, mu, sigma, *, max_iterations=None, min_accuracy=None)[source]; The cumulative probability function of a generalized chi-squared distribution.; The generalized chi-squared distribution has many interpretations. We share here four; interpretations of the values of this distribution:. A linear combination of normal variables and squares of normal variables.; A weighted sum of sums of squares of normally distributed values plus a normally distributed; value.; A weighted sum of chi-squared distributed values plus a normally distributed value.; A “quadratic form” in a vector; of uncorrelated standard normal values. The parameters of this function correspond to the parameters of the third interpretation. \[\begin{aligned}; w &: R^n \quad k : Z^n \quad lam : R^n \quad mu : R \quad sigma : R \\; \\; x &\sim N(mu, sigma^2) \\; y_i &\sim \mathrm{NonCentralChiSquared}(k_i, lam_i) \\; \\; Z &= x + w y^T \\; &= x + \sum_i w_i y_i \\; Z &\sim \mathrm{GeneralizedNonCentralChiSquared}(w, k, lam, mu, sigma); \end{aligned}\]; The generalized chi-squared distribution often arises when working on linear models with standard; normal noise because the sum of the squares of the residuals should follow a generalized; chi-squared distribution.; Examples; The following plot shows three examples of the generalized chi-squared",MatchSource.WIKI,docs/0.2/functions/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/stats.html
https://hail.is/docs/0.2/functions/stats.html:18376,Modifiability,variab,variables,18376,", k=[5, 2], lam=[3, 1], mu=-3, sigma=0).value); 0.516439358616939; >>> hl.eval(hl.pgenchisq(10 , w=[-2, -1], k=[5, 2], lam=[3, 1], mu=-3, sigma=0).value); 1.0; >>> hl.eval(hl.pgenchisq(40 , w=[-2, -1], k=[5, 2], lam=[3, 1], mu=-3, sigma=0).value); 1.0. >>> hl.eval(hl.pgenchisq(-80, w=[1, -10, 2], k=[1, 2, 3], lam=[2, 3, 7], mu=-10, sigma=0).value); 0.14284718767288906; >>> hl.eval(hl.pgenchisq(-20, w=[1, -10, 2], k=[1, 2, 3], lam=[2, 3, 7], mu=-10, sigma=0).value); 0.5950150356303258; >>> hl.eval(hl.pgenchisq(10 , w=[1, -10, 2], k=[1, 2, 3], lam=[2, 3, 7], mu=-10, sigma=0).value); 0.923219534175858; >>> hl.eval(hl.pgenchisq(40 , w=[1, -10, 2], k=[1, 2, 3], lam=[2, 3, 7], mu=-10, sigma=0).value); 0.9971746768781656. Notes; We follow Wikipedia’s notational conventions. Some texts refer to the weight vector (our w) as; \(\lambda\) or lb and the non-centrality vector (our lam) as nc.; We use the Davies’ algorithm which was published as:. Davies, Robert. “The distribution of a linear combination of chi-squared random variables.”; Applied Statistics 29 323-333. 1980. Davies included Fortran source code in the original publication. Davies also released a C; language port. Hail’s implementation is a fairly direct port; of the C implementation to Scala. Davies provides 39 test cases with the source code. The Hail; tests include all 39 test cases as well as a few additional tests.; Davies’ website cautions:. The method works well in most situations if you want only modest accuracy, say 0.0001. But; problems may arise if the sum is dominated by one or two terms with a total of only one or; two degrees of freedom and x is small. For an accessible introduction the Generalized Chi-Squared Distribution, we strongly recommend; the introduction of this paper:. Das, Abhranil; Geisler, Wilson (2020). “A method to integrate and classify normal; distributions”. Parameters:. x (float or Expression of type tfloat64) – The value at which to evaluate the cumulative distribution function (CDF",MatchSource.WIKI,docs/0.2/functions/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/stats.html
https://hail.is/docs/0.2/functions/stats.html:21684,Modifiability,variab,variable,21684,"n evaluated before aborting.; n_iterations : Int32Expression. The number of iterations before stopping.; converged : BooleanExpression. True if the min_accuracy was achieved and round; off error is not likely significant.; fault : Int32Expression. If converged is true, fault is zero. If converged is; false, fault is either one or two. One indicates that the requried accuracy was not; achieved. Two indicates the round-off error is possibly significant. hail.expr.functions.pnorm(x, mu=0, sigma=1, lower_tail=True, log_p=False)[source]; The cumulative probability function of a normal distribution with mean; mu and standard deviation sigma. Returns cumulative probability of; standard normal distribution by default.; Examples; >>> hl.eval(hl.pnorm(0)); 0.5. >>> hl.eval(hl.pnorm(1, mu=2, sigma=2)); 0.30853753872598694. >>> hl.eval(hl.pnorm(2, lower_tail=False)); 0.022750131948179212. >>> hl.eval(hl.pnorm(2, log_p=True)); -0.023012909328963493. Notes; Returns the left-tail probability p = Prob(\(Z < x\)) with \(Z\); a normal random variable. Defaults to a standard normal random variable. Parameters:. x (float or Expression of type tfloat64); mu (float or Expression of type tfloat64) – Mean (default = 0).; sigma (float or Expression of type tfloat64) – Standard deviation (default = 1).; lower_tail (bool or BooleanExpression) – If True, compute the probability of an outcome at or below x,; otherwise greater than x.; log_p (bool or BooleanExpression) – Return the natural logarithm of the probability. Returns:; Expression of type tfloat64. hail.expr.functions.pT(x, n, lower_tail=True, log_p=False)[source]; The cumulative probability function of a t-distribution with; n degrees of freedom.; Examples; >>> hl.eval(hl.pT(0, 10)); 0.5. >>> hl.eval(hl.pT(1, 10)); 0.82955343384897. >>> hl.eval(hl.pT(1, 10, lower_tail=False)); 0.17044656615103004. >>> hl.eval(hl.pT(1, 10, log_p=True)); -0.186867754489647. Notes; If lower_tail is true, returns Prob(\(X \leq\) x) where \(X\) is; a t-dis",MatchSource.WIKI,docs/0.2/functions/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/stats.html
https://hail.is/docs/0.2/functions/stats.html:21731,Modifiability,variab,variable,21731,"of iterations before stopping.; converged : BooleanExpression. True if the min_accuracy was achieved and round; off error is not likely significant.; fault : Int32Expression. If converged is true, fault is zero. If converged is; false, fault is either one or two. One indicates that the requried accuracy was not; achieved. Two indicates the round-off error is possibly significant. hail.expr.functions.pnorm(x, mu=0, sigma=1, lower_tail=True, log_p=False)[source]; The cumulative probability function of a normal distribution with mean; mu and standard deviation sigma. Returns cumulative probability of; standard normal distribution by default.; Examples; >>> hl.eval(hl.pnorm(0)); 0.5. >>> hl.eval(hl.pnorm(1, mu=2, sigma=2)); 0.30853753872598694. >>> hl.eval(hl.pnorm(2, lower_tail=False)); 0.022750131948179212. >>> hl.eval(hl.pnorm(2, log_p=True)); -0.023012909328963493. Notes; Returns the left-tail probability p = Prob(\(Z < x\)) with \(Z\); a normal random variable. Defaults to a standard normal random variable. Parameters:. x (float or Expression of type tfloat64); mu (float or Expression of type tfloat64) – Mean (default = 0).; sigma (float or Expression of type tfloat64) – Standard deviation (default = 1).; lower_tail (bool or BooleanExpression) – If True, compute the probability of an outcome at or below x,; otherwise greater than x.; log_p (bool or BooleanExpression) – Return the natural logarithm of the probability. Returns:; Expression of type tfloat64. hail.expr.functions.pT(x, n, lower_tail=True, log_p=False)[source]; The cumulative probability function of a t-distribution with; n degrees of freedom.; Examples; >>> hl.eval(hl.pT(0, 10)); 0.5. >>> hl.eval(hl.pT(1, 10)); 0.82955343384897. >>> hl.eval(hl.pT(1, 10, lower_tail=False)); 0.17044656615103004. >>> hl.eval(hl.pT(1, 10, log_p=True)); -0.186867754489647. Notes; If lower_tail is true, returns Prob(\(X \leq\) x) where \(X\) is; a t-distributed random variable with n degrees of freedom. If lower_tail; is fal",MatchSource.WIKI,docs/0.2/functions/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/stats.html
https://hail.is/docs/0.2/functions/stats.html:22660,Modifiability,variab,variable,22660,"bility p = Prob(\(Z < x\)) with \(Z\); a normal random variable. Defaults to a standard normal random variable. Parameters:. x (float or Expression of type tfloat64); mu (float or Expression of type tfloat64) – Mean (default = 0).; sigma (float or Expression of type tfloat64) – Standard deviation (default = 1).; lower_tail (bool or BooleanExpression) – If True, compute the probability of an outcome at or below x,; otherwise greater than x.; log_p (bool or BooleanExpression) – Return the natural logarithm of the probability. Returns:; Expression of type tfloat64. hail.expr.functions.pT(x, n, lower_tail=True, log_p=False)[source]; The cumulative probability function of a t-distribution with; n degrees of freedom.; Examples; >>> hl.eval(hl.pT(0, 10)); 0.5. >>> hl.eval(hl.pT(1, 10)); 0.82955343384897. >>> hl.eval(hl.pT(1, 10, lower_tail=False)); 0.17044656615103004. >>> hl.eval(hl.pT(1, 10, log_p=True)); -0.186867754489647. Notes; If lower_tail is true, returns Prob(\(X \leq\) x) where \(X\) is; a t-distributed random variable with n degrees of freedom. If lower_tail; is false, returns Prob(\(X\) > x). Parameters:. x (float or Expression of type tfloat64); n (float or Expression of type tfloat64) – Degrees of freedom of the t-distribution.; lower_tail (bool or BooleanExpression) – If True, compute the probability of an outcome at or below x,; otherwise greater than x.; log_p (bool or BooleanExpression) – Return the natural logarithm of the probability. Returns:; Expression of type tfloat64. hail.expr.functions.pF(x, df1, df2, lower_tail=True, log_p=False)[source]; The cumulative probability function of a F-distribution with parameters; df1 and df2.; Examples; >>> hl.eval(hl.pF(0, 3, 10)); 0.0. >>> hl.eval(hl.pF(1, 3, 10)); 0.5676627969783028. >>> hl.eval(hl.pF(1, 3, 10, lower_tail=False)); 0.4323372030216972. >>> hl.eval(hl.pF(1, 3, 10, log_p=True)); -0.566227703842908. Notes; If lower_tail is true, returns Prob(\(X \leq\) x) where \(X\) is; a random variable with dis",MatchSource.WIKI,docs/0.2/functions/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/stats.html
https://hail.is/docs/0.2/functions/stats.html:23612,Modifiability,variab,variable,23612,"e, returns Prob(\(X \leq\) x) where \(X\) is; a t-distributed random variable with n degrees of freedom. If lower_tail; is false, returns Prob(\(X\) > x). Parameters:. x (float or Expression of type tfloat64); n (float or Expression of type tfloat64) – Degrees of freedom of the t-distribution.; lower_tail (bool or BooleanExpression) – If True, compute the probability of an outcome at or below x,; otherwise greater than x.; log_p (bool or BooleanExpression) – Return the natural logarithm of the probability. Returns:; Expression of type tfloat64. hail.expr.functions.pF(x, df1, df2, lower_tail=True, log_p=False)[source]; The cumulative probability function of a F-distribution with parameters; df1 and df2.; Examples; >>> hl.eval(hl.pF(0, 3, 10)); 0.0. >>> hl.eval(hl.pF(1, 3, 10)); 0.5676627969783028. >>> hl.eval(hl.pF(1, 3, 10, lower_tail=False)); 0.4323372030216972. >>> hl.eval(hl.pF(1, 3, 10, log_p=True)); -0.566227703842908. Notes; If lower_tail is true, returns Prob(\(X \leq\) x) where \(X\) is; a random variable with distribution \(F`(df1, df2). If `lower_tail\); is false, returns Prob(\(X\) > x). Parameters:. x (float or Expression of type tfloat64); df1 (float or Expression of type tfloat64) – Parameter of the F-distribution; df2 (float or Expression of type tfloat64) – Parameter of the F-distribution; lower_tail (bool or BooleanExpression) – If True, compute the probability of an outcome at or below x,; otherwise greater than x.; log_p (bool or BooleanExpression) – Return the natural logarithm of the probability. Returns:; Expression of type tfloat64. hail.expr.functions.ppois(x, lamb, lower_tail=True, log_p=False)[source]; The cumulative probability function of a Poisson distribution.; Examples; >>> hl.eval(hl.ppois(2, 1)); 0.9196986029286058. Notes; If lower_tail is true, returns Prob(\(X \leq\) x) where \(X\) is a; Poisson random variable with rate parameter lamb. If lower_tail is false,; returns Prob(\(X\) > x). Parameters:. x (float or Expression of type t",MatchSource.WIKI,docs/0.2/functions/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/stats.html
https://hail.is/docs/0.2/functions/stats.html:24462,Modifiability,variab,variable,24462,"_tail=False)); 0.4323372030216972. >>> hl.eval(hl.pF(1, 3, 10, log_p=True)); -0.566227703842908. Notes; If lower_tail is true, returns Prob(\(X \leq\) x) where \(X\) is; a random variable with distribution \(F`(df1, df2). If `lower_tail\); is false, returns Prob(\(X\) > x). Parameters:. x (float or Expression of type tfloat64); df1 (float or Expression of type tfloat64) – Parameter of the F-distribution; df2 (float or Expression of type tfloat64) – Parameter of the F-distribution; lower_tail (bool or BooleanExpression) – If True, compute the probability of an outcome at or below x,; otherwise greater than x.; log_p (bool or BooleanExpression) – Return the natural logarithm of the probability. Returns:; Expression of type tfloat64. hail.expr.functions.ppois(x, lamb, lower_tail=True, log_p=False)[source]; The cumulative probability function of a Poisson distribution.; Examples; >>> hl.eval(hl.ppois(2, 1)); 0.9196986029286058. Notes; If lower_tail is true, returns Prob(\(X \leq\) x) where \(X\) is a; Poisson random variable with rate parameter lamb. If lower_tail is false,; returns Prob(\(X\) > x). Parameters:. x (float or Expression of type tfloat64); lamb (float or Expression of type tfloat64) – Rate parameter of Poisson distribution.; lower_tail (bool or BooleanExpression) – If True, compute the probability of an outcome at or below x,; otherwise greater than x.; log_p (bool or BooleanExpression) – Return the natural logarithm of the probability. Returns:; Expression of type tfloat64. hail.expr.functions.qchisqtail(p, df, ncp=None, lower_tail=False, log_p=False)[source]; The quantile function of a chi-squared distribution with df degrees of; freedom, inverts pchisqtail().; Examples; >>> hl.eval(hl.qchisqtail(0.05, 2)); 5.991464547107979. >>> hl.eval(hl.qchisqtail(0.05, 2, ncp=2)); 10.838131614372958. >>> hl.eval(hl.qchisqtail(0.05, 2, lower_tail=True)); 0.10258658877510107. >>> hl.eval(hl.qchisqtail(hl.log(0.05), 2, log_p=True)); 5.991464547107979. Notes; Returns r",MatchSource.WIKI,docs/0.2/functions/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/stats.html
https://hail.is/docs/0.2/functions/stats.html:25517,Modifiability,variab,variable,25517,"amb. If lower_tail is false,; returns Prob(\(X\) > x). Parameters:. x (float or Expression of type tfloat64); lamb (float or Expression of type tfloat64) – Rate parameter of Poisson distribution.; lower_tail (bool or BooleanExpression) – If True, compute the probability of an outcome at or below x,; otherwise greater than x.; log_p (bool or BooleanExpression) – Return the natural logarithm of the probability. Returns:; Expression of type tfloat64. hail.expr.functions.qchisqtail(p, df, ncp=None, lower_tail=False, log_p=False)[source]; The quantile function of a chi-squared distribution with df degrees of; freedom, inverts pchisqtail().; Examples; >>> hl.eval(hl.qchisqtail(0.05, 2)); 5.991464547107979. >>> hl.eval(hl.qchisqtail(0.05, 2, ncp=2)); 10.838131614372958. >>> hl.eval(hl.qchisqtail(0.05, 2, lower_tail=True)); 0.10258658877510107. >>> hl.eval(hl.qchisqtail(hl.log(0.05), 2, log_p=True)); 5.991464547107979. Notes; Returns right-quantile x for which p = Prob(\(Z^2\) > x) with; \(Z^2\) a chi-squared random variable with degrees of freedom specified; by df. The probability p must satisfy 0 < p < 1. Parameters:. p (float or Expression of type tfloat64) – Probability.; df (float or Expression of type tfloat64) – Degrees of freedom.; ncp (float or Expression of type tfloat64) – Corresponds to ncp parameter in pchisqtail().; lower_tail (bool or BooleanExpression) – Corresponds to lower_tail parameter in pchisqtail().; log_p (bool or BooleanExpression) – Exponentiate p, corresponds to log_p parameter in pchisqtail(). Returns:; Expression of type tfloat64. hail.expr.functions.qnorm(p, mu=0, sigma=1, lower_tail=True, log_p=False)[source]; The quantile function of a normal distribution with mean mu and; standard deviation sigma, inverts pnorm(). Returns quantile of; standard normal distribution by default.; Examples; >>> hl.eval(hl.qnorm(0.90)); 1.2815515655446008. >>> hl.eval(hl.qnorm(0.90, mu=1, sigma=2)); 3.5631031310892016. >>> hl.eval(hl.qnorm(0.90, lower_tail=False",MatchSource.WIKI,docs/0.2/functions/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/stats.html
https://hail.is/docs/0.2/functions/stats.html:26675,Modifiability,variab,variable,26675,"float64) – Probability.; df (float or Expression of type tfloat64) – Degrees of freedom.; ncp (float or Expression of type tfloat64) – Corresponds to ncp parameter in pchisqtail().; lower_tail (bool or BooleanExpression) – Corresponds to lower_tail parameter in pchisqtail().; log_p (bool or BooleanExpression) – Exponentiate p, corresponds to log_p parameter in pchisqtail(). Returns:; Expression of type tfloat64. hail.expr.functions.qnorm(p, mu=0, sigma=1, lower_tail=True, log_p=False)[source]; The quantile function of a normal distribution with mean mu and; standard deviation sigma, inverts pnorm(). Returns quantile of; standard normal distribution by default.; Examples; >>> hl.eval(hl.qnorm(0.90)); 1.2815515655446008. >>> hl.eval(hl.qnorm(0.90, mu=1, sigma=2)); 3.5631031310892016. >>> hl.eval(hl.qnorm(0.90, lower_tail=False)); -1.2815515655446008. >>> hl.eval(hl.qnorm(hl.log(0.90), log_p=True)); 1.2815515655446008. Notes; Returns left-quantile x for which p = Prob(\(Z\) < x) with \(Z\); a normal random variable with mean mu and standard deviation sigma.; Defaults to a standard normal random variable, and the probability p must; satisfy 0 < p < 1. Parameters:. p (float or Expression of type tfloat64) – Probability.; mu (float or Expression of type tfloat64) – Mean (default = 0).; sigma (float or Expression of type tfloat64) – Standard deviation (default = 1).; lower_tail (bool or BooleanExpression) – Corresponds to lower_tail parameter in pnorm().; log_p (bool or BooleanExpression) – Exponentiate p, corresponds to log_p parameter in pnorm(). Returns:; Expression of type tfloat64. hail.expr.functions.qpois(p, lamb, lower_tail=True, log_p=False)[source]; The quantile function of a Poisson distribution with rate parameter; lamb, inverts ppois().; Examples; >>> hl.eval(hl.qpois(0.99, 1)); 4. Notes; Returns the smallest integer \(x\) such that Prob(\(X \leq x\)) \(\geq\) p where \(X\); is a Poisson random variable with rate parameter lambda. Parameters:. p (float or Exp",MatchSource.WIKI,docs/0.2/functions/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/stats.html
https://hail.is/docs/0.2/functions/stats.html:26765,Modifiability,variab,variable,26765,"type tfloat64) – Corresponds to ncp parameter in pchisqtail().; lower_tail (bool or BooleanExpression) – Corresponds to lower_tail parameter in pchisqtail().; log_p (bool or BooleanExpression) – Exponentiate p, corresponds to log_p parameter in pchisqtail(). Returns:; Expression of type tfloat64. hail.expr.functions.qnorm(p, mu=0, sigma=1, lower_tail=True, log_p=False)[source]; The quantile function of a normal distribution with mean mu and; standard deviation sigma, inverts pnorm(). Returns quantile of; standard normal distribution by default.; Examples; >>> hl.eval(hl.qnorm(0.90)); 1.2815515655446008. >>> hl.eval(hl.qnorm(0.90, mu=1, sigma=2)); 3.5631031310892016. >>> hl.eval(hl.qnorm(0.90, lower_tail=False)); -1.2815515655446008. >>> hl.eval(hl.qnorm(hl.log(0.90), log_p=True)); 1.2815515655446008. Notes; Returns left-quantile x for which p = Prob(\(Z\) < x) with \(Z\); a normal random variable with mean mu and standard deviation sigma.; Defaults to a standard normal random variable, and the probability p must; satisfy 0 < p < 1. Parameters:. p (float or Expression of type tfloat64) – Probability.; mu (float or Expression of type tfloat64) – Mean (default = 0).; sigma (float or Expression of type tfloat64) – Standard deviation (default = 1).; lower_tail (bool or BooleanExpression) – Corresponds to lower_tail parameter in pnorm().; log_p (bool or BooleanExpression) – Exponentiate p, corresponds to log_p parameter in pnorm(). Returns:; Expression of type tfloat64. hail.expr.functions.qpois(p, lamb, lower_tail=True, log_p=False)[source]; The quantile function of a Poisson distribution with rate parameter; lamb, inverts ppois().; Examples; >>> hl.eval(hl.qpois(0.99, 1)); 4. Notes; Returns the smallest integer \(x\) such that Prob(\(X \leq x\)) \(\geq\) p where \(X\); is a Poisson random variable with rate parameter lambda. Parameters:. p (float or Expression of type tfloat64); lamb (float or Expression of type tfloat64) – Rate parameter of Poisson distribution.; lowe",MatchSource.WIKI,docs/0.2/functions/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/stats.html
https://hail.is/docs/0.2/functions/stats.html:27591,Modifiability,variab,variable,27591,"e tfloat64. hail.expr.functions.qnorm(p, mu=0, sigma=1, lower_tail=True, log_p=False)[source]; The quantile function of a normal distribution with mean mu and; standard deviation sigma, inverts pnorm(). Returns quantile of; standard normal distribution by default.; Examples; >>> hl.eval(hl.qnorm(0.90)); 1.2815515655446008. >>> hl.eval(hl.qnorm(0.90, mu=1, sigma=2)); 3.5631031310892016. >>> hl.eval(hl.qnorm(0.90, lower_tail=False)); -1.2815515655446008. >>> hl.eval(hl.qnorm(hl.log(0.90), log_p=True)); 1.2815515655446008. Notes; Returns left-quantile x for which p = Prob(\(Z\) < x) with \(Z\); a normal random variable with mean mu and standard deviation sigma.; Defaults to a standard normal random variable, and the probability p must; satisfy 0 < p < 1. Parameters:. p (float or Expression of type tfloat64) – Probability.; mu (float or Expression of type tfloat64) – Mean (default = 0).; sigma (float or Expression of type tfloat64) – Standard deviation (default = 1).; lower_tail (bool or BooleanExpression) – Corresponds to lower_tail parameter in pnorm().; log_p (bool or BooleanExpression) – Exponentiate p, corresponds to log_p parameter in pnorm(). Returns:; Expression of type tfloat64. hail.expr.functions.qpois(p, lamb, lower_tail=True, log_p=False)[source]; The quantile function of a Poisson distribution with rate parameter; lamb, inverts ppois().; Examples; >>> hl.eval(hl.qpois(0.99, 1)); 4. Notes; Returns the smallest integer \(x\) such that Prob(\(X \leq x\)) \(\geq\) p where \(X\); is a Poisson random variable with rate parameter lambda. Parameters:. p (float or Expression of type tfloat64); lamb (float or Expression of type tfloat64) – Rate parameter of Poisson distribution.; lower_tail (bool or BooleanExpression) – Corresponds to lower_tail parameter in inverse ppois().; log_p (bool or BooleanExpression) – Exponentiate p before testing. Returns:; Expression of type tfloat64. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/functions/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/stats.html
https://hail.is/docs/0.2/functions/stats.html:11469,Performance,perform,performs,11469,"e parameter lamb.; Examples; >>> hl.eval(hl.dpois(5, 3)); 0.10081881344492458. Parameters:. x (float or Expression of type tfloat64) – Non-negative number at which to compute the probability density.; lamb (float or Expression of type tfloat64) – Poisson rate parameter. Must be non-negative.; log_p (bool or BooleanExpression) – If True, the natural logarithm of the probability density is returned. Returns:; Expression of type tfloat64 – The (log) probability density. hail.expr.functions.hardy_weinberg_test(n_hom_ref, n_het, n_hom_var, one_sided=False)[source]; Performs test of Hardy-Weinberg equilibrium.; Examples; >>> hl.eval(hl.hardy_weinberg_test(250, 500, 250)); Struct(het_freq_hwe=0.5002501250625313, p_value=0.9747844394217698). >>> hl.eval(hl.hardy_weinberg_test(37, 200, 85)); Struct(het_freq_hwe=0.48964964307448583, p_value=1.1337210383168987e-06). Notes; By default, this method performs a two-sided exact test with mid-p-value correction of; Hardy-Weinberg equilibrium; via an efficient implementation of the; Levene-Haldane distribution,; which models the number of heterozygous individuals under equilibrium.; The mean of this distribution is (n_ref * n_var) / (2n - 1), where; n_ref = 2*n_hom_ref + n_het is the number of reference alleles,; n_var = 2*n_hom_var + n_het is the number of variant alleles,; and n = n_hom_ref + n_het + n_hom_var is the number of individuals.; So the expected frequency of heterozygotes under equilibrium,; het_freq_hwe, is this mean divided by n.; To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set one_sided=True and the p-value returned will be; from the one-sided exact test. Parameters:. n_hom_ref (int or Expression of type tint32) – Number of homozygous reference genotypes.; n_het (int or Expression of type tint32) – Number of heterozygous genotypes.; n_hom_var (int or Expression of type tint32) – Number of homozygous variant genotypes.; one_sided (bool) – False by default. When True, ",MatchSource.WIKI,docs/0.2/functions/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/stats.html
https://hail.is/docs/0.2/functions/stats.html:12076,Performance,perform,perform,12076,"einberg equilibrium.; Examples; >>> hl.eval(hl.hardy_weinberg_test(250, 500, 250)); Struct(het_freq_hwe=0.5002501250625313, p_value=0.9747844394217698). >>> hl.eval(hl.hardy_weinberg_test(37, 200, 85)); Struct(het_freq_hwe=0.48964964307448583, p_value=1.1337210383168987e-06). Notes; By default, this method performs a two-sided exact test with mid-p-value correction of; Hardy-Weinberg equilibrium; via an efficient implementation of the; Levene-Haldane distribution,; which models the number of heterozygous individuals under equilibrium.; The mean of this distribution is (n_ref * n_var) / (2n - 1), where; n_ref = 2*n_hom_ref + n_het is the number of reference alleles,; n_var = 2*n_hom_var + n_het is the number of variant alleles,; and n = n_hom_ref + n_het + n_hom_var is the number of individuals.; So the expected frequency of heterozygotes under equilibrium,; het_freq_hwe, is this mean divided by n.; To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set one_sided=True and the p-value returned will be; from the one-sided exact test. Parameters:. n_hom_ref (int or Expression of type tint32) – Number of homozygous reference genotypes.; n_het (int or Expression of type tint32) – Number of heterozygous genotypes.; n_hom_var (int or Expression of type tint32) – Number of homozygous variant genotypes.; one_sided (bool) – False by default. When True, perform one-sided test for excess heterozygosity. Returns:; StructExpression – A struct expression with two fields, het_freq_hwe; (tfloat64) and p_value (tfloat64). hail.expr.functions.binom_test(x, n, p, alternative)[source]; Performs a binomial test on p given x successes in n trials.; Returns the p-value from the exact binomial test of the null hypothesis that; success has probability p, given x successes in n trials.; The alternatives are interpreted as follows:; - 'less': a one-tailed test of the significance of x or fewer successes,; - 'greater': a one-tailed test of the signifi",MatchSource.WIKI,docs/0.2/functions/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/stats.html
https://hail.is/docs/0.2/functions/stats.html:12569,Performance,perform,perform,12569,"ation of the; Levene-Haldane distribution,; which models the number of heterozygous individuals under equilibrium.; The mean of this distribution is (n_ref * n_var) / (2n - 1), where; n_ref = 2*n_hom_ref + n_het is the number of reference alleles,; n_var = 2*n_hom_var + n_het is the number of variant alleles,; and n = n_hom_ref + n_het + n_hom_var is the number of individuals.; So the expected frequency of heterozygotes under equilibrium,; het_freq_hwe, is this mean divided by n.; To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set one_sided=True and the p-value returned will be; from the one-sided exact test. Parameters:. n_hom_ref (int or Expression of type tint32) – Number of homozygous reference genotypes.; n_het (int or Expression of type tint32) – Number of heterozygous genotypes.; n_hom_var (int or Expression of type tint32) – Number of homozygous variant genotypes.; one_sided (bool) – False by default. When True, perform one-sided test for excess heterozygosity. Returns:; StructExpression – A struct expression with two fields, het_freq_hwe; (tfloat64) and p_value (tfloat64). hail.expr.functions.binom_test(x, n, p, alternative)[source]; Performs a binomial test on p given x successes in n trials.; Returns the p-value from the exact binomial test of the null hypothesis that; success has probability p, given x successes in n trials.; The alternatives are interpreted as follows:; - 'less': a one-tailed test of the significance of x or fewer successes,; - 'greater': a one-tailed test of the significance of x or more successes, and; - 'two-sided': a two-tailed test of the significance of x or any equivalent or more unlikely outcome.; Examples; All the examples below use a fair coin as the null hypothesis. Zero is; interpreted as tail and one as heads.; Test if a coin is biased towards heads or tails after observing two heads; out of ten flips:; >>> hl.eval(hl.binom_test(2, 10, 0.5, 'two-sided')); 0.10937499999999994. ",MatchSource.WIKI,docs/0.2/functions/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/stats.html
https://hail.is/docs/0.2/functions/stats.html:20662,Safety,abort,aborting,20662,"rray of tfloat64) – A non-centrality parameter for each non-central chi-square term. We use lam instead; of lambda because the latter is a reserved word in Python.; mu (float or Expression of type tfloat64) – The standard deviation of the normal term.; sigma (float or Expression of type tfloat64) – The standard deviation of the normal term.; max_iterations (int or Expression of type tint32) – The maximum number of iterations of the numerical integration before raising an error. The; default maximum number of iterations is 1e5.; min_accuracy (int or Expression of type tint32) – The minimum accuracy of the returned value. If the minimum accuracy is not achieved, this; function will raise an error. The default minimum accuracy is 1e-5. Returns:; StructExpression – This method returns a structure with the value as well as information about the numerical; integration. value : Float64Expression. If converged is true, the value of the CDF evaluated; at x. Otherwise, this is the last value the integration evaluated before aborting.; n_iterations : Int32Expression. The number of iterations before stopping.; converged : BooleanExpression. True if the min_accuracy was achieved and round; off error is not likely significant.; fault : Int32Expression. If converged is true, fault is zero. If converged is; false, fault is either one or two. One indicates that the requried accuracy was not; achieved. Two indicates the round-off error is possibly significant. hail.expr.functions.pnorm(x, mu=0, sigma=1, lower_tail=True, log_p=False)[source]; The cumulative probability function of a normal distribution with mean; mu and standard deviation sigma. Returns cumulative probability of; standard normal distribution by default.; Examples; >>> hl.eval(hl.pnorm(0)); 0.5. >>> hl.eval(hl.pnorm(1, mu=2, sigma=2)); 0.30853753872598694. >>> hl.eval(hl.pnorm(2, lower_tail=False)); 0.022750131948179212. >>> hl.eval(hl.pnorm(2, log_p=True)); -0.023012909328963493. Notes; Returns the left-tail probabili",MatchSource.WIKI,docs/0.2/functions/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/stats.html
https://hail.is/docs/0.2/functions/stats.html:19000,Security,access,accessible,19000,"971746768781656. Notes; We follow Wikipedia’s notational conventions. Some texts refer to the weight vector (our w) as; \(\lambda\) or lb and the non-centrality vector (our lam) as nc.; We use the Davies’ algorithm which was published as:. Davies, Robert. “The distribution of a linear combination of chi-squared random variables.”; Applied Statistics 29 323-333. 1980. Davies included Fortran source code in the original publication. Davies also released a C; language port. Hail’s implementation is a fairly direct port; of the C implementation to Scala. Davies provides 39 test cases with the source code. The Hail; tests include all 39 test cases as well as a few additional tests.; Davies’ website cautions:. The method works well in most situations if you want only modest accuracy, say 0.0001. But; problems may arise if the sum is dominated by one or two terms with a total of only one or; two degrees of freedom and x is small. For an accessible introduction the Generalized Chi-Squared Distribution, we strongly recommend; the introduction of this paper:. Das, Abhranil; Geisler, Wilson (2020). “A method to integrate and classify normal; distributions”. Parameters:. x (float or Expression of type tfloat64) – The value at which to evaluate the cumulative distribution function (CDF).; w (list of float or Expression of type tarray of tfloat64) – A weight for each non-central chi-square term.; k (list of int or Expression of type tarray of tint32) – A degrees of freedom parameter for each non-central chi-square term.; lam (list of float or Expression of type tarray of tfloat64) – A non-centrality parameter for each non-central chi-square term. We use lam instead; of lambda because the latter is a reserved word in Python.; mu (float or Expression of type tfloat64) – The standard deviation of the normal term.; sigma (float or Expression of type tfloat64) – The standard deviation of the normal term.; max_iterations (int or Expression of type tint32) – The maximum number of iterati",MatchSource.WIKI,docs/0.2/functions/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/stats.html
https://hail.is/docs/0.2/functions/stats.html:784,Testability,test,test,784,"﻿. Hail | ; Statistical functions. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Functions; Statistical functions. View page source. Statistical functions. chi_squared_test(c1, c2, c3, c4); Performs chi-squared test of independence on a 2x2 contingency table. fisher_exact_test(c1, c2, c3, c4); Calculates the p-value, odds ratio, and 95% confidence interval using Fisher's exact test for a 2x2 table. contingency_table_test(c1, c2, c3, c4, ...); Performs chi-squared or Fisher's exact test of independence on a 2x2 contingency table. cochran_mantel_haenszel_test(a, b, c, d); Perform the Cochran-Mantel-Haenszel test for association. dbeta(x, a, b); Returns the probability density at x of a beta distribution with parameters a (alpha) and b (beta). dchisq(x, df[, ncp, log_p]); Compute the probability density at x of a chi-squared distribution with df degrees of freedom. dnorm(x[, mu, sigma, log_p]); Compute the probability density at x of a normal distribution with mean mu and standard deviation sigma. dpois(x, lamb[, log_p]); Compute the (log) probability density at x of a Poisson distribution with rate parameter lamb. hardy_weinberg_test(n_hom_ref, n_het, n_hom_var); Performs test of Hardy-Weinberg equilibrium. binom_test(x, n, p, alternative); Performs a binomial test on p given x successes in n trials. pchisqtail(x, df[, ncp, lower_tail, log_p]); Returns the probability under the right-tail starting at x for ",MatchSource.WIKI,docs/0.2/functions/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/stats.html
https://hail.is/docs/0.2/functions/stats.html:953,Testability,test,test,953,"﻿. Hail | ; Statistical functions. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Functions; Statistical functions. View page source. Statistical functions. chi_squared_test(c1, c2, c3, c4); Performs chi-squared test of independence on a 2x2 contingency table. fisher_exact_test(c1, c2, c3, c4); Calculates the p-value, odds ratio, and 95% confidence interval using Fisher's exact test for a 2x2 table. contingency_table_test(c1, c2, c3, c4, ...); Performs chi-squared or Fisher's exact test of independence on a 2x2 contingency table. cochran_mantel_haenszel_test(a, b, c, d); Perform the Cochran-Mantel-Haenszel test for association. dbeta(x, a, b); Returns the probability density at x of a beta distribution with parameters a (alpha) and b (beta). dchisq(x, df[, ncp, log_p]); Compute the probability density at x of a chi-squared distribution with df degrees of freedom. dnorm(x[, mu, sigma, log_p]); Compute the probability density at x of a normal distribution with mean mu and standard deviation sigma. dpois(x, lamb[, log_p]); Compute the (log) probability density at x of a Poisson distribution with rate parameter lamb. hardy_weinberg_test(n_hom_ref, n_het, n_hom_var); Performs test of Hardy-Weinberg equilibrium. binom_test(x, n, p, alternative); Performs a binomial test on p given x successes in n trials. pchisqtail(x, df[, ncp, lower_tail, log_p]); Returns the probability under the right-tail starting at x for ",MatchSource.WIKI,docs/0.2/functions/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/stats.html
https://hail.is/docs/0.2/functions/stats.html:1059,Testability,test,test,1059,"dback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Functions; Statistical functions. View page source. Statistical functions. chi_squared_test(c1, c2, c3, c4); Performs chi-squared test of independence on a 2x2 contingency table. fisher_exact_test(c1, c2, c3, c4); Calculates the p-value, odds ratio, and 95% confidence interval using Fisher's exact test for a 2x2 table. contingency_table_test(c1, c2, c3, c4, ...); Performs chi-squared or Fisher's exact test of independence on a 2x2 contingency table. cochran_mantel_haenszel_test(a, b, c, d); Perform the Cochran-Mantel-Haenszel test for association. dbeta(x, a, b); Returns the probability density at x of a beta distribution with parameters a (alpha) and b (beta). dchisq(x, df[, ncp, log_p]); Compute the probability density at x of a chi-squared distribution with df degrees of freedom. dnorm(x[, mu, sigma, log_p]); Compute the probability density at x of a normal distribution with mean mu and standard deviation sigma. dpois(x, lamb[, log_p]); Compute the (log) probability density at x of a Poisson distribution with rate parameter lamb. hardy_weinberg_test(n_hom_ref, n_het, n_hom_var); Performs test of Hardy-Weinberg equilibrium. binom_test(x, n, p, alternative); Performs a binomial test on p given x successes in n trials. pchisqtail(x, df[, ncp, lower_tail, log_p]); Returns the probability under the right-tail starting at x for a chi-squared distribution with df degrees of freedom. pgench",MatchSource.WIKI,docs/0.2/functions/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/stats.html
https://hail.is/docs/0.2/functions/stats.html:1186,Testability,test,test,1186,". Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Functions; Statistical functions. View page source. Statistical functions. chi_squared_test(c1, c2, c3, c4); Performs chi-squared test of independence on a 2x2 contingency table. fisher_exact_test(c1, c2, c3, c4); Calculates the p-value, odds ratio, and 95% confidence interval using Fisher's exact test for a 2x2 table. contingency_table_test(c1, c2, c3, c4, ...); Performs chi-squared or Fisher's exact test of independence on a 2x2 contingency table. cochran_mantel_haenszel_test(a, b, c, d); Perform the Cochran-Mantel-Haenszel test for association. dbeta(x, a, b); Returns the probability density at x of a beta distribution with parameters a (alpha) and b (beta). dchisq(x, df[, ncp, log_p]); Compute the probability density at x of a chi-squared distribution with df degrees of freedom. dnorm(x[, mu, sigma, log_p]); Compute the probability density at x of a normal distribution with mean mu and standard deviation sigma. dpois(x, lamb[, log_p]); Compute the (log) probability density at x of a Poisson distribution with rate parameter lamb. hardy_weinberg_test(n_hom_ref, n_het, n_hom_var); Performs test of Hardy-Weinberg equilibrium. binom_test(x, n, p, alternative); Performs a binomial test on p given x successes in n trials. pchisqtail(x, df[, ncp, lower_tail, log_p]); Returns the probability under the right-tail starting at x for a chi-squared distribution with df degrees of freedom. pgenchisq(x, w, k, lam, mu, sigma, *[, ...]); The cumulative probability function of a generalized ch",MatchSource.WIKI,docs/0.2/functions/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/stats.html
https://hail.is/docs/0.2/functions/stats.html:1621,Testability,log,log,1621,"ython API; Functions; Statistical functions. View page source. Statistical functions. chi_squared_test(c1, c2, c3, c4); Performs chi-squared test of independence on a 2x2 contingency table. fisher_exact_test(c1, c2, c3, c4); Calculates the p-value, odds ratio, and 95% confidence interval using Fisher's exact test for a 2x2 table. contingency_table_test(c1, c2, c3, c4, ...); Performs chi-squared or Fisher's exact test of independence on a 2x2 contingency table. cochran_mantel_haenszel_test(a, b, c, d); Perform the Cochran-Mantel-Haenszel test for association. dbeta(x, a, b); Returns the probability density at x of a beta distribution with parameters a (alpha) and b (beta). dchisq(x, df[, ncp, log_p]); Compute the probability density at x of a chi-squared distribution with df degrees of freedom. dnorm(x[, mu, sigma, log_p]); Compute the probability density at x of a normal distribution with mean mu and standard deviation sigma. dpois(x, lamb[, log_p]); Compute the (log) probability density at x of a Poisson distribution with rate parameter lamb. hardy_weinberg_test(n_hom_ref, n_het, n_hom_var); Performs test of Hardy-Weinberg equilibrium. binom_test(x, n, p, alternative); Performs a binomial test on p given x successes in n trials. pchisqtail(x, df[, ncp, lower_tail, log_p]); Returns the probability under the right-tail starting at x for a chi-squared distribution with df degrees of freedom. pgenchisq(x, w, k, lam, mu, sigma, *[, ...]); The cumulative probability function of a generalized chi-squared distribution. pnorm(x[, mu, sigma, lower_tail, log_p]); The cumulative probability function of a normal distribution with mean mu and standard deviation sigma. pT(x, n[, lower_tail, log_p]); The cumulative probability function of a t-distribution with n degrees of freedom. pF(x, df1, df2[, lower_tail, log_p]); The cumulative probability function of a F-distribution with parameters df1 and df2. ppois(x, lamb[, lower_tail, log_p]); The cumulative probability function of a ",MatchSource.WIKI,docs/0.2/functions/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/stats.html
https://hail.is/docs/0.2/functions/stats.html:1762,Testability,test,test,1762," c2, c3, c4); Performs chi-squared test of independence on a 2x2 contingency table. fisher_exact_test(c1, c2, c3, c4); Calculates the p-value, odds ratio, and 95% confidence interval using Fisher's exact test for a 2x2 table. contingency_table_test(c1, c2, c3, c4, ...); Performs chi-squared or Fisher's exact test of independence on a 2x2 contingency table. cochran_mantel_haenszel_test(a, b, c, d); Perform the Cochran-Mantel-Haenszel test for association. dbeta(x, a, b); Returns the probability density at x of a beta distribution with parameters a (alpha) and b (beta). dchisq(x, df[, ncp, log_p]); Compute the probability density at x of a chi-squared distribution with df degrees of freedom. dnorm(x[, mu, sigma, log_p]); Compute the probability density at x of a normal distribution with mean mu and standard deviation sigma. dpois(x, lamb[, log_p]); Compute the (log) probability density at x of a Poisson distribution with rate parameter lamb. hardy_weinberg_test(n_hom_ref, n_het, n_hom_var); Performs test of Hardy-Weinberg equilibrium. binom_test(x, n, p, alternative); Performs a binomial test on p given x successes in n trials. pchisqtail(x, df[, ncp, lower_tail, log_p]); Returns the probability under the right-tail starting at x for a chi-squared distribution with df degrees of freedom. pgenchisq(x, w, k, lam, mu, sigma, *[, ...]); The cumulative probability function of a generalized chi-squared distribution. pnorm(x[, mu, sigma, lower_tail, log_p]); The cumulative probability function of a normal distribution with mean mu and standard deviation sigma. pT(x, n[, lower_tail, log_p]); The cumulative probability function of a t-distribution with n degrees of freedom. pF(x, df1, df2[, lower_tail, log_p]); The cumulative probability function of a F-distribution with parameters df1 and df2. ppois(x, lamb[, lower_tail, log_p]); The cumulative probability function of a Poisson distribution. qchisqtail(p, df[, ncp, lower_tail, log_p]); The quantile function of a chi-squared di",MatchSource.WIKI,docs/0.2/functions/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/stats.html
https://hail.is/docs/0.2/functions/stats.html:1852,Testability,test,test,1852,"t_test(c1, c2, c3, c4); Calculates the p-value, odds ratio, and 95% confidence interval using Fisher's exact test for a 2x2 table. contingency_table_test(c1, c2, c3, c4, ...); Performs chi-squared or Fisher's exact test of independence on a 2x2 contingency table. cochran_mantel_haenszel_test(a, b, c, d); Perform the Cochran-Mantel-Haenszel test for association. dbeta(x, a, b); Returns the probability density at x of a beta distribution with parameters a (alpha) and b (beta). dchisq(x, df[, ncp, log_p]); Compute the probability density at x of a chi-squared distribution with df degrees of freedom. dnorm(x[, mu, sigma, log_p]); Compute the probability density at x of a normal distribution with mean mu and standard deviation sigma. dpois(x, lamb[, log_p]); Compute the (log) probability density at x of a Poisson distribution with rate parameter lamb. hardy_weinberg_test(n_hom_ref, n_het, n_hom_var); Performs test of Hardy-Weinberg equilibrium. binom_test(x, n, p, alternative); Performs a binomial test on p given x successes in n trials. pchisqtail(x, df[, ncp, lower_tail, log_p]); Returns the probability under the right-tail starting at x for a chi-squared distribution with df degrees of freedom. pgenchisq(x, w, k, lam, mu, sigma, *[, ...]); The cumulative probability function of a generalized chi-squared distribution. pnorm(x[, mu, sigma, lower_tail, log_p]); The cumulative probability function of a normal distribution with mean mu and standard deviation sigma. pT(x, n[, lower_tail, log_p]); The cumulative probability function of a t-distribution with n degrees of freedom. pF(x, df1, df2[, lower_tail, log_p]); The cumulative probability function of a F-distribution with parameters df1 and df2. ppois(x, lamb[, lower_tail, log_p]); The cumulative probability function of a Poisson distribution. qchisqtail(p, df[, ncp, lower_tail, log_p]); The quantile function of a chi-squared distribution with df degrees of freedom, inverts pchisqtail(). qnorm(p[, mu, sigma, lower_tail, l",MatchSource.WIKI,docs/0.2/functions/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/stats.html
https://hail.is/docs/0.2/functions/stats.html:3172,Testability,test,test,3172,"ed distribution. pnorm(x[, mu, sigma, lower_tail, log_p]); The cumulative probability function of a normal distribution with mean mu and standard deviation sigma. pT(x, n[, lower_tail, log_p]); The cumulative probability function of a t-distribution with n degrees of freedom. pF(x, df1, df2[, lower_tail, log_p]); The cumulative probability function of a F-distribution with parameters df1 and df2. ppois(x, lamb[, lower_tail, log_p]); The cumulative probability function of a Poisson distribution. qchisqtail(p, df[, ncp, lower_tail, log_p]); The quantile function of a chi-squared distribution with df degrees of freedom, inverts pchisqtail(). qnorm(p[, mu, sigma, lower_tail, log_p]); The quantile function of a normal distribution with mean mu and standard deviation sigma, inverts pnorm(). qpois(p, lamb[, lower_tail, log_p]); The quantile function of a Poisson distribution with rate parameter lamb, inverts ppois(). hail.expr.functions.chi_squared_test(c1, c2, c3, c4)[source]; Performs chi-squared test of independence on a 2x2 contingency table.; Examples; >>> hl.eval(hl.chi_squared_test(10, 10, 10, 10)); Struct(p_value=1.0, odds_ratio=1.0). >>> hl.eval(hl.chi_squared_test(51, 43, 22, 92)); Struct(p_value=1.4626257805267089e-07, odds_ratio=4.959830866807611). Notes; The odds ratio is given by (c1 / c2) / (c3 / c4).; Returned fields may be nan or inf. Parameters:. c1 (int or Expression of type tint32) – Value for cell 1.; c2 (int or Expression of type tint32) – Value for cell 2.; c3 (int or Expression of type tint32) – Value for cell 3.; c4 (int or Expression of type tint32) – Value for cell 4. Returns:; StructExpression – A tstruct expression with two fields, p_value; (tfloat64) and odds_ratio (tfloat64). hail.expr.functions.fisher_exact_test(c1, c2, c3, c4)[source]; Calculates the p-value, odds ratio, and 95% confidence interval using; Fisher’s exact test for a 2x2 table.; Examples; >>> hl.eval(hl.fisher_exact_test(10, 10, 10, 10)); Struct(p_value=1.0000000000000002, o",MatchSource.WIKI,docs/0.2/functions/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/stats.html
https://hail.is/docs/0.2/functions/stats.html:4044,Testability,test,test,4044,"og_p]); The quantile function of a Poisson distribution with rate parameter lamb, inverts ppois(). hail.expr.functions.chi_squared_test(c1, c2, c3, c4)[source]; Performs chi-squared test of independence on a 2x2 contingency table.; Examples; >>> hl.eval(hl.chi_squared_test(10, 10, 10, 10)); Struct(p_value=1.0, odds_ratio=1.0). >>> hl.eval(hl.chi_squared_test(51, 43, 22, 92)); Struct(p_value=1.4626257805267089e-07, odds_ratio=4.959830866807611). Notes; The odds ratio is given by (c1 / c2) / (c3 / c4).; Returned fields may be nan or inf. Parameters:. c1 (int or Expression of type tint32) – Value for cell 1.; c2 (int or Expression of type tint32) – Value for cell 2.; c3 (int or Expression of type tint32) – Value for cell 3.; c4 (int or Expression of type tint32) – Value for cell 4. Returns:; StructExpression – A tstruct expression with two fields, p_value; (tfloat64) and odds_ratio (tfloat64). hail.expr.functions.fisher_exact_test(c1, c2, c3, c4)[source]; Calculates the p-value, odds ratio, and 95% confidence interval using; Fisher’s exact test for a 2x2 table.; Examples; >>> hl.eval(hl.fisher_exact_test(10, 10, 10, 10)); Struct(p_value=1.0000000000000002, odds_ratio=1.0,; ci_95_lower=0.24385796914260355, ci_95_upper=4.100747675033819). >>> hl.eval(hl.fisher_exact_test(51, 43, 22, 92)); Struct(p_value=2.1564999740157304e-07, odds_ratio=4.918058171469967,; ci_95_lower=2.5659373368248444, ci_95_upper=9.677929632035475). Notes; This method is identical to the version implemented in; R with default; parameters (two-sided, alpha = 0.05, null hypothesis that the odds ratio equals 1).; Returned fields may be nan or inf. Parameters:. c1 (int or Expression of type tint32) – Value for cell 1.; c2 (int or Expression of type tint32) – Value for cell 2.; c3 (int or Expression of type tint32) – Value for cell 3.; c4 (int or Expression of type tint32) – Value for cell 4. Returns:; StructExpression – A tstruct expression with four fields, p_value; (tfloat64), odds_ratio (tfloat64),; ",MatchSource.WIKI,docs/0.2/functions/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/stats.html
https://hail.is/docs/0.2/functions/stats.html:5178,Testability,test,test,5178,"00000000002, odds_ratio=1.0,; ci_95_lower=0.24385796914260355, ci_95_upper=4.100747675033819). >>> hl.eval(hl.fisher_exact_test(51, 43, 22, 92)); Struct(p_value=2.1564999740157304e-07, odds_ratio=4.918058171469967,; ci_95_lower=2.5659373368248444, ci_95_upper=9.677929632035475). Notes; This method is identical to the version implemented in; R with default; parameters (two-sided, alpha = 0.05, null hypothesis that the odds ratio equals 1).; Returned fields may be nan or inf. Parameters:. c1 (int or Expression of type tint32) – Value for cell 1.; c2 (int or Expression of type tint32) – Value for cell 2.; c3 (int or Expression of type tint32) – Value for cell 3.; c4 (int or Expression of type tint32) – Value for cell 4. Returns:; StructExpression – A tstruct expression with four fields, p_value; (tfloat64), odds_ratio (tfloat64),; ci_95_lower (:py:data:.tfloat64`), and ci_95_upper; (tfloat64). hail.expr.functions.contingency_table_test(c1, c2, c3, c4, min_cell_count)[source]; Performs chi-squared or Fisher’s exact test of independence on a 2x2; contingency table.; Examples; >>> hl.eval(hl.contingency_table_test(51, 43, 22, 92, min_cell_count=22)); Struct(p_value=1.4626257805267089e-07, odds_ratio=4.959830866807611). >>> hl.eval(hl.contingency_table_test(51, 43, 22, 92, min_cell_count=23)); Struct(p_value=2.1564999740157304e-07, odds_ratio=4.918058171469967). Notes; If all cell counts are at least min_cell_count, the chi-squared test is; used. Otherwise, Fisher’s exact test is used.; Returned fields may be nan or inf. Parameters:. c1 (int or Expression of type tint32) – Value for cell 1.; c2 (int or Expression of type tint32) – Value for cell 2.; c3 (int or Expression of type tint32) – Value for cell 3.; c4 (int or Expression of type tint32) – Value for cell 4.; min_cell_count (int or Expression of type tint32) – Minimum count in every cell to use the chi-squared test. Returns:; StructExpression – A tstruct expression with two fields, p_value; (tfloat64) and odds_ratio ",MatchSource.WIKI,docs/0.2/functions/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/stats.html
https://hail.is/docs/0.2/functions/stats.html:5600,Testability,test,test,5600,"odds ratio equals 1).; Returned fields may be nan or inf. Parameters:. c1 (int or Expression of type tint32) – Value for cell 1.; c2 (int or Expression of type tint32) – Value for cell 2.; c3 (int or Expression of type tint32) – Value for cell 3.; c4 (int or Expression of type tint32) – Value for cell 4. Returns:; StructExpression – A tstruct expression with four fields, p_value; (tfloat64), odds_ratio (tfloat64),; ci_95_lower (:py:data:.tfloat64`), and ci_95_upper; (tfloat64). hail.expr.functions.contingency_table_test(c1, c2, c3, c4, min_cell_count)[source]; Performs chi-squared or Fisher’s exact test of independence on a 2x2; contingency table.; Examples; >>> hl.eval(hl.contingency_table_test(51, 43, 22, 92, min_cell_count=22)); Struct(p_value=1.4626257805267089e-07, odds_ratio=4.959830866807611). >>> hl.eval(hl.contingency_table_test(51, 43, 22, 92, min_cell_count=23)); Struct(p_value=2.1564999740157304e-07, odds_ratio=4.918058171469967). Notes; If all cell counts are at least min_cell_count, the chi-squared test is; used. Otherwise, Fisher’s exact test is used.; Returned fields may be nan or inf. Parameters:. c1 (int or Expression of type tint32) – Value for cell 1.; c2 (int or Expression of type tint32) – Value for cell 2.; c3 (int or Expression of type tint32) – Value for cell 3.; c4 (int or Expression of type tint32) – Value for cell 4.; min_cell_count (int or Expression of type tint32) – Minimum count in every cell to use the chi-squared test. Returns:; StructExpression – A tstruct expression with two fields, p_value; (tfloat64) and odds_ratio (tfloat64). hail.expr.functions.cochran_mantel_haenszel_test(a, b, c, d)[source]; Perform the Cochran-Mantel-Haenszel test for association.; Examples; >>> a = [56, 61, 73, 71]; >>> b = [69, 257, 65, 48]; >>> c = [40, 57, 71, 55]; >>> d = [77, 301, 79, 48]; >>> hl.eval(hl.cochran_mantel_haenszel_test(a, b, c, d)); Struct(test_statistic=5.0496881823306765, p_value=0.024630370456863417). >>> mt = ds.filter_rows(mt.locu",MatchSource.WIKI,docs/0.2/functions/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/stats.html
https://hail.is/docs/0.2/functions/stats.html:5641,Testability,test,test,5641,"eters:. c1 (int or Expression of type tint32) – Value for cell 1.; c2 (int or Expression of type tint32) – Value for cell 2.; c3 (int or Expression of type tint32) – Value for cell 3.; c4 (int or Expression of type tint32) – Value for cell 4. Returns:; StructExpression – A tstruct expression with four fields, p_value; (tfloat64), odds_ratio (tfloat64),; ci_95_lower (:py:data:.tfloat64`), and ci_95_upper; (tfloat64). hail.expr.functions.contingency_table_test(c1, c2, c3, c4, min_cell_count)[source]; Performs chi-squared or Fisher’s exact test of independence on a 2x2; contingency table.; Examples; >>> hl.eval(hl.contingency_table_test(51, 43, 22, 92, min_cell_count=22)); Struct(p_value=1.4626257805267089e-07, odds_ratio=4.959830866807611). >>> hl.eval(hl.contingency_table_test(51, 43, 22, 92, min_cell_count=23)); Struct(p_value=2.1564999740157304e-07, odds_ratio=4.918058171469967). Notes; If all cell counts are at least min_cell_count, the chi-squared test is; used. Otherwise, Fisher’s exact test is used.; Returned fields may be nan or inf. Parameters:. c1 (int or Expression of type tint32) – Value for cell 1.; c2 (int or Expression of type tint32) – Value for cell 2.; c3 (int or Expression of type tint32) – Value for cell 3.; c4 (int or Expression of type tint32) – Value for cell 4.; min_cell_count (int or Expression of type tint32) – Minimum count in every cell to use the chi-squared test. Returns:; StructExpression – A tstruct expression with two fields, p_value; (tfloat64) and odds_ratio (tfloat64). hail.expr.functions.cochran_mantel_haenszel_test(a, b, c, d)[source]; Perform the Cochran-Mantel-Haenszel test for association.; Examples; >>> a = [56, 61, 73, 71]; >>> b = [69, 257, 65, 48]; >>> c = [40, 57, 71, 55]; >>> d = [77, 301, 79, 48]; >>> hl.eval(hl.cochran_mantel_haenszel_test(a, b, c, d)); Struct(test_statistic=5.0496881823306765, p_value=0.024630370456863417). >>> mt = ds.filter_rows(mt.locus == hl.Locus(20, 10633237)); >>> mt.count_rows(); 1; >>> a, b,",MatchSource.WIKI,docs/0.2/functions/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/stats.html
https://hail.is/docs/0.2/functions/stats.html:6043,Testability,test,test,6043,"95_lower (:py:data:.tfloat64`), and ci_95_upper; (tfloat64). hail.expr.functions.contingency_table_test(c1, c2, c3, c4, min_cell_count)[source]; Performs chi-squared or Fisher’s exact test of independence on a 2x2; contingency table.; Examples; >>> hl.eval(hl.contingency_table_test(51, 43, 22, 92, min_cell_count=22)); Struct(p_value=1.4626257805267089e-07, odds_ratio=4.959830866807611). >>> hl.eval(hl.contingency_table_test(51, 43, 22, 92, min_cell_count=23)); Struct(p_value=2.1564999740157304e-07, odds_ratio=4.918058171469967). Notes; If all cell counts are at least min_cell_count, the chi-squared test is; used. Otherwise, Fisher’s exact test is used.; Returned fields may be nan or inf. Parameters:. c1 (int or Expression of type tint32) – Value for cell 1.; c2 (int or Expression of type tint32) – Value for cell 2.; c3 (int or Expression of type tint32) – Value for cell 3.; c4 (int or Expression of type tint32) – Value for cell 4.; min_cell_count (int or Expression of type tint32) – Minimum count in every cell to use the chi-squared test. Returns:; StructExpression – A tstruct expression with two fields, p_value; (tfloat64) and odds_ratio (tfloat64). hail.expr.functions.cochran_mantel_haenszel_test(a, b, c, d)[source]; Perform the Cochran-Mantel-Haenszel test for association.; Examples; >>> a = [56, 61, 73, 71]; >>> b = [69, 257, 65, 48]; >>> c = [40, 57, 71, 55]; >>> d = [77, 301, 79, 48]; >>> hl.eval(hl.cochran_mantel_haenszel_test(a, b, c, d)); Struct(test_statistic=5.0496881823306765, p_value=0.024630370456863417). >>> mt = ds.filter_rows(mt.locus == hl.Locus(20, 10633237)); >>> mt.count_rows(); 1; >>> a, b, c, d = mt.aggregate_entries(; ... hl.tuple([; ... hl.array([hl.agg.count_where(mt.GT.is_non_ref() & mt.pheno.is_case & mt.pheno.is_female), hl.agg.count_where(mt.GT.is_non_ref() & mt.pheno.is_case & ~mt.pheno.is_female)]),; ... hl.array([hl.agg.count_where(mt.GT.is_non_ref() & ~mt.pheno.is_case & mt.pheno.is_female), hl.agg.count_where(mt.GT.is_non_ref() &",MatchSource.WIKI,docs/0.2/functions/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/stats.html
https://hail.is/docs/0.2/functions/stats.html:6270,Testability,test,test,6270,"; >>> hl.eval(hl.contingency_table_test(51, 43, 22, 92, min_cell_count=22)); Struct(p_value=1.4626257805267089e-07, odds_ratio=4.959830866807611). >>> hl.eval(hl.contingency_table_test(51, 43, 22, 92, min_cell_count=23)); Struct(p_value=2.1564999740157304e-07, odds_ratio=4.918058171469967). Notes; If all cell counts are at least min_cell_count, the chi-squared test is; used. Otherwise, Fisher’s exact test is used.; Returned fields may be nan or inf. Parameters:. c1 (int or Expression of type tint32) – Value for cell 1.; c2 (int or Expression of type tint32) – Value for cell 2.; c3 (int or Expression of type tint32) – Value for cell 3.; c4 (int or Expression of type tint32) – Value for cell 4.; min_cell_count (int or Expression of type tint32) – Minimum count in every cell to use the chi-squared test. Returns:; StructExpression – A tstruct expression with two fields, p_value; (tfloat64) and odds_ratio (tfloat64). hail.expr.functions.cochran_mantel_haenszel_test(a, b, c, d)[source]; Perform the Cochran-Mantel-Haenszel test for association.; Examples; >>> a = [56, 61, 73, 71]; >>> b = [69, 257, 65, 48]; >>> c = [40, 57, 71, 55]; >>> d = [77, 301, 79, 48]; >>> hl.eval(hl.cochran_mantel_haenszel_test(a, b, c, d)); Struct(test_statistic=5.0496881823306765, p_value=0.024630370456863417). >>> mt = ds.filter_rows(mt.locus == hl.Locus(20, 10633237)); >>> mt.count_rows(); 1; >>> a, b, c, d = mt.aggregate_entries(; ... hl.tuple([; ... hl.array([hl.agg.count_where(mt.GT.is_non_ref() & mt.pheno.is_case & mt.pheno.is_female), hl.agg.count_where(mt.GT.is_non_ref() & mt.pheno.is_case & ~mt.pheno.is_female)]),; ... hl.array([hl.agg.count_where(mt.GT.is_non_ref() & ~mt.pheno.is_case & mt.pheno.is_female), hl.agg.count_where(mt.GT.is_non_ref() & ~mt.pheno.is_case & ~mt.pheno.is_female)]),; ... hl.array([hl.agg.count_where(~mt.GT.is_non_ref() & mt.pheno.is_case & mt.pheno.is_female), hl.agg.count_where(~mt.GT.is_non_ref() & mt.pheno.is_case & ~mt.pheno.is_female)]),; ... hl.array([hl.a",MatchSource.WIKI,docs/0.2/functions/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/stats.html
https://hail.is/docs/0.2/functions/stats.html:9473,Testability,log,logarithm,9473," be less than 1.; a (float or Expression of type tfloat64) – The alpha parameter in the beta distribution. The result is undefined; for non-positive a.; b (float or Expression of type tfloat64) – The beta parameter in the beta distribution. The result is undefined; for non-positive b. Returns:; Float64Expression. hail.expr.functions.dchisq(x, df, ncp=None, log_p=False)[source]; Compute the probability density at x of a chi-squared distribution with df; degrees of freedom.; Examples; >>> hl.eval(hl.dchisq(1, 2)); 0.3032653298563167. >>> hl.eval(hl.dchisq(1, 2, ncp=2)); 0.17472016746112667. >>> hl.eval(hl.dchisq(1, 2, log_p=True)); -1.1931471805599454. Parameters:. x (float or Expression of type tfloat64) – Non-negative number at which to compute the probability density.; df (float or Expression of type tfloat64) – Degrees of freedom.; ncp (float or Expression of type tfloat64) – Noncentrality parameter, defaults to 0 if unspecified.; log_p (bool or BooleanExpression) – If True, the natural logarithm of the probability density is returned. Returns:; Expression of type tfloat64 – The probability density. hail.expr.functions.dnorm(x, mu=0, sigma=1, log_p=False)[source]; Compute the probability density at x of a normal distribution with mean; mu and standard deviation sigma. Returns density of standard normal; distribution by default.; Examples; >>> hl.eval(hl.dnorm(1)); 0.24197072451914337. >>> hl.eval(hl.dnorm(1, mu=1, sigma=2)); 0.19947114020071635. >>> hl.eval(hl.dnorm(1, log_p=True)); -1.4189385332046727. Parameters:. x (float or Expression of type tfloat64) – Real number at which to compute the probability density.; mu (float or Expression of type tfloat64) – Mean (default = 0).; sigma (float or Expression of type tfloat64) – Standard deviation (default = 1).; log_p (bool or BooleanExpression) – If True, the natural logarithm of the probability density is returned. Returns:; Expression of type tfloat64 – The probability density. hail.expr.functions.dpois(x, lamb,",MatchSource.WIKI,docs/0.2/functions/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/stats.html
https://hail.is/docs/0.2/functions/stats.html:10319,Testability,log,logarithm,10319," ncp (float or Expression of type tfloat64) – Noncentrality parameter, defaults to 0 if unspecified.; log_p (bool or BooleanExpression) – If True, the natural logarithm of the probability density is returned. Returns:; Expression of type tfloat64 – The probability density. hail.expr.functions.dnorm(x, mu=0, sigma=1, log_p=False)[source]; Compute the probability density at x of a normal distribution with mean; mu and standard deviation sigma. Returns density of standard normal; distribution by default.; Examples; >>> hl.eval(hl.dnorm(1)); 0.24197072451914337. >>> hl.eval(hl.dnorm(1, mu=1, sigma=2)); 0.19947114020071635. >>> hl.eval(hl.dnorm(1, log_p=True)); -1.4189385332046727. Parameters:. x (float or Expression of type tfloat64) – Real number at which to compute the probability density.; mu (float or Expression of type tfloat64) – Mean (default = 0).; sigma (float or Expression of type tfloat64) – Standard deviation (default = 1).; log_p (bool or BooleanExpression) – If True, the natural logarithm of the probability density is returned. Returns:; Expression of type tfloat64 – The probability density. hail.expr.functions.dpois(x, lamb, log_p=False)[source]; Compute the (log) probability density at x of a Poisson distribution with rate parameter lamb.; Examples; >>> hl.eval(hl.dpois(5, 3)); 0.10081881344492458. Parameters:. x (float or Expression of type tfloat64) – Non-negative number at which to compute the probability density.; lamb (float or Expression of type tfloat64) – Poisson rate parameter. Must be non-negative.; log_p (bool or BooleanExpression) – If True, the natural logarithm of the probability density is returned. Returns:; Expression of type tfloat64 – The (log) probability density. hail.expr.functions.hardy_weinberg_test(n_hom_ref, n_het, n_hom_var, one_sided=False)[source]; Performs test of Hardy-Weinberg equilibrium.; Examples; >>> hl.eval(hl.hardy_weinberg_test(250, 500, 250)); Struct(het_freq_hwe=0.5002501250625313, p_value=0.9747844394217698). ",MatchSource.WIKI,docs/0.2/functions/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/stats.html
https://hail.is/docs/0.2/functions/stats.html:10505,Testability,log,log,10505,"d. Returns:; Expression of type tfloat64 – The probability density. hail.expr.functions.dnorm(x, mu=0, sigma=1, log_p=False)[source]; Compute the probability density at x of a normal distribution with mean; mu and standard deviation sigma. Returns density of standard normal; distribution by default.; Examples; >>> hl.eval(hl.dnorm(1)); 0.24197072451914337. >>> hl.eval(hl.dnorm(1, mu=1, sigma=2)); 0.19947114020071635. >>> hl.eval(hl.dnorm(1, log_p=True)); -1.4189385332046727. Parameters:. x (float or Expression of type tfloat64) – Real number at which to compute the probability density.; mu (float or Expression of type tfloat64) – Mean (default = 0).; sigma (float or Expression of type tfloat64) – Standard deviation (default = 1).; log_p (bool or BooleanExpression) – If True, the natural logarithm of the probability density is returned. Returns:; Expression of type tfloat64 – The probability density. hail.expr.functions.dpois(x, lamb, log_p=False)[source]; Compute the (log) probability density at x of a Poisson distribution with rate parameter lamb.; Examples; >>> hl.eval(hl.dpois(5, 3)); 0.10081881344492458. Parameters:. x (float or Expression of type tfloat64) – Non-negative number at which to compute the probability density.; lamb (float or Expression of type tfloat64) – Poisson rate parameter. Must be non-negative.; log_p (bool or BooleanExpression) – If True, the natural logarithm of the probability density is returned. Returns:; Expression of type tfloat64 – The (log) probability density. hail.expr.functions.hardy_weinberg_test(n_hom_ref, n_het, n_hom_var, one_sided=False)[source]; Performs test of Hardy-Weinberg equilibrium.; Examples; >>> hl.eval(hl.hardy_weinberg_test(250, 500, 250)); Struct(het_freq_hwe=0.5002501250625313, p_value=0.9747844394217698). >>> hl.eval(hl.hardy_weinberg_test(37, 200, 85)); Struct(het_freq_hwe=0.48964964307448583, p_value=1.1337210383168987e-06). Notes; By default, this method performs a two-sided exact test with mid-p-value co",MatchSource.WIKI,docs/0.2/functions/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/stats.html
https://hail.is/docs/0.2/functions/stats.html:10920,Testability,log,logarithm,10920,"=2)); 0.19947114020071635. >>> hl.eval(hl.dnorm(1, log_p=True)); -1.4189385332046727. Parameters:. x (float or Expression of type tfloat64) – Real number at which to compute the probability density.; mu (float or Expression of type tfloat64) – Mean (default = 0).; sigma (float or Expression of type tfloat64) – Standard deviation (default = 1).; log_p (bool or BooleanExpression) – If True, the natural logarithm of the probability density is returned. Returns:; Expression of type tfloat64 – The probability density. hail.expr.functions.dpois(x, lamb, log_p=False)[source]; Compute the (log) probability density at x of a Poisson distribution with rate parameter lamb.; Examples; >>> hl.eval(hl.dpois(5, 3)); 0.10081881344492458. Parameters:. x (float or Expression of type tfloat64) – Non-negative number at which to compute the probability density.; lamb (float or Expression of type tfloat64) – Poisson rate parameter. Must be non-negative.; log_p (bool or BooleanExpression) – If True, the natural logarithm of the probability density is returned. Returns:; Expression of type tfloat64 – The (log) probability density. hail.expr.functions.hardy_weinberg_test(n_hom_ref, n_het, n_hom_var, one_sided=False)[source]; Performs test of Hardy-Weinberg equilibrium.; Examples; >>> hl.eval(hl.hardy_weinberg_test(250, 500, 250)); Struct(het_freq_hwe=0.5002501250625313, p_value=0.9747844394217698). >>> hl.eval(hl.hardy_weinberg_test(37, 200, 85)); Struct(het_freq_hwe=0.48964964307448583, p_value=1.1337210383168987e-06). Notes; By default, this method performs a two-sided exact test with mid-p-value correction of; Hardy-Weinberg equilibrium; via an efficient implementation of the; Levene-Haldane distribution,; which models the number of heterozygous individuals under equilibrium.; The mean of this distribution is (n_ref * n_var) / (2n - 1), where; n_ref = 2*n_hom_ref + n_het is the number of reference alleles,; n_var = 2*n_hom_var + n_het is the number of variant alleles,; and n = n_hom_re",MatchSource.WIKI,docs/0.2/functions/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/stats.html
https://hail.is/docs/0.2/functions/stats.html:11015,Testability,log,log,11015,"ameters:. x (float or Expression of type tfloat64) – Real number at which to compute the probability density.; mu (float or Expression of type tfloat64) – Mean (default = 0).; sigma (float or Expression of type tfloat64) – Standard deviation (default = 1).; log_p (bool or BooleanExpression) – If True, the natural logarithm of the probability density is returned. Returns:; Expression of type tfloat64 – The probability density. hail.expr.functions.dpois(x, lamb, log_p=False)[source]; Compute the (log) probability density at x of a Poisson distribution with rate parameter lamb.; Examples; >>> hl.eval(hl.dpois(5, 3)); 0.10081881344492458. Parameters:. x (float or Expression of type tfloat64) – Non-negative number at which to compute the probability density.; lamb (float or Expression of type tfloat64) – Poisson rate parameter. Must be non-negative.; log_p (bool or BooleanExpression) – If True, the natural logarithm of the probability density is returned. Returns:; Expression of type tfloat64 – The (log) probability density. hail.expr.functions.hardy_weinberg_test(n_hom_ref, n_het, n_hom_var, one_sided=False)[source]; Performs test of Hardy-Weinberg equilibrium.; Examples; >>> hl.eval(hl.hardy_weinberg_test(250, 500, 250)); Struct(het_freq_hwe=0.5002501250625313, p_value=0.9747844394217698). >>> hl.eval(hl.hardy_weinberg_test(37, 200, 85)); Struct(het_freq_hwe=0.48964964307448583, p_value=1.1337210383168987e-06). Notes; By default, this method performs a two-sided exact test with mid-p-value correction of; Hardy-Weinberg equilibrium; via an efficient implementation of the; Levene-Haldane distribution,; which models the number of heterozygous individuals under equilibrium.; The mean of this distribution is (n_ref * n_var) / (2n - 1), where; n_ref = 2*n_hom_ref + n_het is the number of reference alleles,; n_var = 2*n_hom_var + n_het is the number of variant alleles,; and n = n_hom_ref + n_het + n_hom_var is the number of individuals.; So the expected frequency of heterozy",MatchSource.WIKI,docs/0.2/functions/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/stats.html
https://hail.is/docs/0.2/functions/stats.html:11146,Testability,test,test,11146,"oat or Expression of type tfloat64) – Mean (default = 0).; sigma (float or Expression of type tfloat64) – Standard deviation (default = 1).; log_p (bool or BooleanExpression) – If True, the natural logarithm of the probability density is returned. Returns:; Expression of type tfloat64 – The probability density. hail.expr.functions.dpois(x, lamb, log_p=False)[source]; Compute the (log) probability density at x of a Poisson distribution with rate parameter lamb.; Examples; >>> hl.eval(hl.dpois(5, 3)); 0.10081881344492458. Parameters:. x (float or Expression of type tfloat64) – Non-negative number at which to compute the probability density.; lamb (float or Expression of type tfloat64) – Poisson rate parameter. Must be non-negative.; log_p (bool or BooleanExpression) – If True, the natural logarithm of the probability density is returned. Returns:; Expression of type tfloat64 – The (log) probability density. hail.expr.functions.hardy_weinberg_test(n_hom_ref, n_het, n_hom_var, one_sided=False)[source]; Performs test of Hardy-Weinberg equilibrium.; Examples; >>> hl.eval(hl.hardy_weinberg_test(250, 500, 250)); Struct(het_freq_hwe=0.5002501250625313, p_value=0.9747844394217698). >>> hl.eval(hl.hardy_weinberg_test(37, 200, 85)); Struct(het_freq_hwe=0.48964964307448583, p_value=1.1337210383168987e-06). Notes; By default, this method performs a two-sided exact test with mid-p-value correction of; Hardy-Weinberg equilibrium; via an efficient implementation of the; Levene-Haldane distribution,; which models the number of heterozygous individuals under equilibrium.; The mean of this distribution is (n_ref * n_var) / (2n - 1), where; n_ref = 2*n_hom_ref + n_het is the number of reference alleles,; n_var = 2*n_hom_var + n_het is the number of variant alleles,; and n = n_hom_ref + n_het + n_hom_var is the number of individuals.; So the expected frequency of heterozygotes under equilibrium,; het_freq_hwe, is this mean divided by n.; To perform one-sided exact test of excess hetero",MatchSource.WIKI,docs/0.2/functions/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/stats.html
https://hail.is/docs/0.2/functions/stats.html:11496,Testability,test,test,11496,"e parameter lamb.; Examples; >>> hl.eval(hl.dpois(5, 3)); 0.10081881344492458. Parameters:. x (float or Expression of type tfloat64) – Non-negative number at which to compute the probability density.; lamb (float or Expression of type tfloat64) – Poisson rate parameter. Must be non-negative.; log_p (bool or BooleanExpression) – If True, the natural logarithm of the probability density is returned. Returns:; Expression of type tfloat64 – The (log) probability density. hail.expr.functions.hardy_weinberg_test(n_hom_ref, n_het, n_hom_var, one_sided=False)[source]; Performs test of Hardy-Weinberg equilibrium.; Examples; >>> hl.eval(hl.hardy_weinberg_test(250, 500, 250)); Struct(het_freq_hwe=0.5002501250625313, p_value=0.9747844394217698). >>> hl.eval(hl.hardy_weinberg_test(37, 200, 85)); Struct(het_freq_hwe=0.48964964307448583, p_value=1.1337210383168987e-06). Notes; By default, this method performs a two-sided exact test with mid-p-value correction of; Hardy-Weinberg equilibrium; via an efficient implementation of the; Levene-Haldane distribution,; which models the number of heterozygous individuals under equilibrium.; The mean of this distribution is (n_ref * n_var) / (2n - 1), where; n_ref = 2*n_hom_ref + n_het is the number of reference alleles,; n_var = 2*n_hom_var + n_het is the number of variant alleles,; and n = n_hom_ref + n_het + n_hom_var is the number of individuals.; So the expected frequency of heterozygotes under equilibrium,; het_freq_hwe, is this mean divided by n.; To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set one_sided=True and the p-value returned will be; from the one-sided exact test. Parameters:. n_hom_ref (int or Expression of type tint32) – Number of homozygous reference genotypes.; n_het (int or Expression of type tint32) – Number of heterozygous genotypes.; n_hom_var (int or Expression of type tint32) – Number of homozygous variant genotypes.; one_sided (bool) – False by default. When True, ",MatchSource.WIKI,docs/0.2/functions/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/stats.html
https://hail.is/docs/0.2/functions/stats.html:12100,Testability,test,test,12100,"einberg equilibrium.; Examples; >>> hl.eval(hl.hardy_weinberg_test(250, 500, 250)); Struct(het_freq_hwe=0.5002501250625313, p_value=0.9747844394217698). >>> hl.eval(hl.hardy_weinberg_test(37, 200, 85)); Struct(het_freq_hwe=0.48964964307448583, p_value=1.1337210383168987e-06). Notes; By default, this method performs a two-sided exact test with mid-p-value correction of; Hardy-Weinberg equilibrium; via an efficient implementation of the; Levene-Haldane distribution,; which models the number of heterozygous individuals under equilibrium.; The mean of this distribution is (n_ref * n_var) / (2n - 1), where; n_ref = 2*n_hom_ref + n_het is the number of reference alleles,; n_var = 2*n_hom_var + n_het is the number of variant alleles,; and n = n_hom_ref + n_het + n_hom_var is the number of individuals.; So the expected frequency of heterozygotes under equilibrium,; het_freq_hwe, is this mean divided by n.; To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set one_sided=True and the p-value returned will be; from the one-sided exact test. Parameters:. n_hom_ref (int or Expression of type tint32) – Number of homozygous reference genotypes.; n_het (int or Expression of type tint32) – Number of heterozygous genotypes.; n_hom_var (int or Expression of type tint32) – Number of homozygous variant genotypes.; one_sided (bool) – False by default. When True, perform one-sided test for excess heterozygosity. Returns:; StructExpression – A struct expression with two fields, het_freq_hwe; (tfloat64) and p_value (tfloat64). hail.expr.functions.binom_test(x, n, p, alternative)[source]; Performs a binomial test on p given x successes in n trials.; Returns the p-value from the exact binomial test of the null hypothesis that; success has probability p, given x successes in n trials.; The alternatives are interpreted as follows:; - 'less': a one-tailed test of the significance of x or fewer successes,; - 'greater': a one-tailed test of the signifi",MatchSource.WIKI,docs/0.2/functions/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/stats.html
https://hail.is/docs/0.2/functions/stats.html:12246,Testability,test,test,12246,"einberg equilibrium.; Examples; >>> hl.eval(hl.hardy_weinberg_test(250, 500, 250)); Struct(het_freq_hwe=0.5002501250625313, p_value=0.9747844394217698). >>> hl.eval(hl.hardy_weinberg_test(37, 200, 85)); Struct(het_freq_hwe=0.48964964307448583, p_value=1.1337210383168987e-06). Notes; By default, this method performs a two-sided exact test with mid-p-value correction of; Hardy-Weinberg equilibrium; via an efficient implementation of the; Levene-Haldane distribution,; which models the number of heterozygous individuals under equilibrium.; The mean of this distribution is (n_ref * n_var) / (2n - 1), where; n_ref = 2*n_hom_ref + n_het is the number of reference alleles,; n_var = 2*n_hom_var + n_het is the number of variant alleles,; and n = n_hom_ref + n_het + n_hom_var is the number of individuals.; So the expected frequency of heterozygotes under equilibrium,; het_freq_hwe, is this mean divided by n.; To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set one_sided=True and the p-value returned will be; from the one-sided exact test. Parameters:. n_hom_ref (int or Expression of type tint32) – Number of homozygous reference genotypes.; n_het (int or Expression of type tint32) – Number of heterozygous genotypes.; n_hom_var (int or Expression of type tint32) – Number of homozygous variant genotypes.; one_sided (bool) – False by default. When True, perform one-sided test for excess heterozygosity. Returns:; StructExpression – A struct expression with two fields, het_freq_hwe; (tfloat64) and p_value (tfloat64). hail.expr.functions.binom_test(x, n, p, alternative)[source]; Performs a binomial test on p given x successes in n trials.; Returns the p-value from the exact binomial test of the null hypothesis that; success has probability p, given x successes in n trials.; The alternatives are interpreted as follows:; - 'less': a one-tailed test of the significance of x or fewer successes,; - 'greater': a one-tailed test of the signifi",MatchSource.WIKI,docs/0.2/functions/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/stats.html
https://hail.is/docs/0.2/functions/stats.html:12587,Testability,test,test,12587,"ation of the; Levene-Haldane distribution,; which models the number of heterozygous individuals under equilibrium.; The mean of this distribution is (n_ref * n_var) / (2n - 1), where; n_ref = 2*n_hom_ref + n_het is the number of reference alleles,; n_var = 2*n_hom_var + n_het is the number of variant alleles,; and n = n_hom_ref + n_het + n_hom_var is the number of individuals.; So the expected frequency of heterozygotes under equilibrium,; het_freq_hwe, is this mean divided by n.; To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set one_sided=True and the p-value returned will be; from the one-sided exact test. Parameters:. n_hom_ref (int or Expression of type tint32) – Number of homozygous reference genotypes.; n_het (int or Expression of type tint32) – Number of heterozygous genotypes.; n_hom_var (int or Expression of type tint32) – Number of homozygous variant genotypes.; one_sided (bool) – False by default. When True, perform one-sided test for excess heterozygosity. Returns:; StructExpression – A struct expression with two fields, het_freq_hwe; (tfloat64) and p_value (tfloat64). hail.expr.functions.binom_test(x, n, p, alternative)[source]; Performs a binomial test on p given x successes in n trials.; Returns the p-value from the exact binomial test of the null hypothesis that; success has probability p, given x successes in n trials.; The alternatives are interpreted as follows:; - 'less': a one-tailed test of the significance of x or fewer successes,; - 'greater': a one-tailed test of the significance of x or more successes, and; - 'two-sided': a two-tailed test of the significance of x or any equivalent or more unlikely outcome.; Examples; All the examples below use a fair coin as the null hypothesis. Zero is; interpreted as tail and one as heads.; Test if a coin is biased towards heads or tails after observing two heads; out of ten flips:; >>> hl.eval(hl.binom_test(2, 10, 0.5, 'two-sided')); 0.10937499999999994. ",MatchSource.WIKI,docs/0.2/functions/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/stats.html
https://hail.is/docs/0.2/functions/stats.html:12817,Testability,test,test,12817," number of reference alleles,; n_var = 2*n_hom_var + n_het is the number of variant alleles,; and n = n_hom_ref + n_het + n_hom_var is the number of individuals.; So the expected frequency of heterozygotes under equilibrium,; het_freq_hwe, is this mean divided by n.; To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set one_sided=True and the p-value returned will be; from the one-sided exact test. Parameters:. n_hom_ref (int or Expression of type tint32) – Number of homozygous reference genotypes.; n_het (int or Expression of type tint32) – Number of heterozygous genotypes.; n_hom_var (int or Expression of type tint32) – Number of homozygous variant genotypes.; one_sided (bool) – False by default. When True, perform one-sided test for excess heterozygosity. Returns:; StructExpression – A struct expression with two fields, het_freq_hwe; (tfloat64) and p_value (tfloat64). hail.expr.functions.binom_test(x, n, p, alternative)[source]; Performs a binomial test on p given x successes in n trials.; Returns the p-value from the exact binomial test of the null hypothesis that; success has probability p, given x successes in n trials.; The alternatives are interpreted as follows:; - 'less': a one-tailed test of the significance of x or fewer successes,; - 'greater': a one-tailed test of the significance of x or more successes, and; - 'two-sided': a two-tailed test of the significance of x or any equivalent or more unlikely outcome.; Examples; All the examples below use a fair coin as the null hypothesis. Zero is; interpreted as tail and one as heads.; Test if a coin is biased towards heads or tails after observing two heads; out of ten flips:; >>> hl.eval(hl.binom_test(2, 10, 0.5, 'two-sided')); 0.10937499999999994. Test if a coin is biased towards tails after observing four heads out of ten; flips:; >>> hl.eval(hl.binom_test(4, 10, 0.5, 'less')); 0.3769531250000001. Test if a coin is biased towards heads after observing thirty-tw",MatchSource.WIKI,docs/0.2/functions/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/stats.html
https://hail.is/docs/0.2/functions/stats.html:12903,Testability,test,test,12903,"+ n_hom_var is the number of individuals.; So the expected frequency of heterozygotes under equilibrium,; het_freq_hwe, is this mean divided by n.; To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set one_sided=True and the p-value returned will be; from the one-sided exact test. Parameters:. n_hom_ref (int or Expression of type tint32) – Number of homozygous reference genotypes.; n_het (int or Expression of type tint32) – Number of heterozygous genotypes.; n_hom_var (int or Expression of type tint32) – Number of homozygous variant genotypes.; one_sided (bool) – False by default. When True, perform one-sided test for excess heterozygosity. Returns:; StructExpression – A struct expression with two fields, het_freq_hwe; (tfloat64) and p_value (tfloat64). hail.expr.functions.binom_test(x, n, p, alternative)[source]; Performs a binomial test on p given x successes in n trials.; Returns the p-value from the exact binomial test of the null hypothesis that; success has probability p, given x successes in n trials.; The alternatives are interpreted as follows:; - 'less': a one-tailed test of the significance of x or fewer successes,; - 'greater': a one-tailed test of the significance of x or more successes, and; - 'two-sided': a two-tailed test of the significance of x or any equivalent or more unlikely outcome.; Examples; All the examples below use a fair coin as the null hypothesis. Zero is; interpreted as tail and one as heads.; Test if a coin is biased towards heads or tails after observing two heads; out of ten flips:; >>> hl.eval(hl.binom_test(2, 10, 0.5, 'two-sided')); 0.10937499999999994. Test if a coin is biased towards tails after observing four heads out of ten; flips:; >>> hl.eval(hl.binom_test(4, 10, 0.5, 'less')); 0.3769531250000001. Test if a coin is biased towards heads after observing thirty-two heads out; of fifty flips:; >>> hl.eval(hl.binom_test(32, 50, 0.5, 'greater')); 0.03245432353613613. Parameters:. x (i",MatchSource.WIKI,docs/0.2/functions/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/stats.html
https://hail.is/docs/0.2/functions/stats.html:13065,Testability,test,test,13065,"e; correction instead, set one_sided=True and the p-value returned will be; from the one-sided exact test. Parameters:. n_hom_ref (int or Expression of type tint32) – Number of homozygous reference genotypes.; n_het (int or Expression of type tint32) – Number of heterozygous genotypes.; n_hom_var (int or Expression of type tint32) – Number of homozygous variant genotypes.; one_sided (bool) – False by default. When True, perform one-sided test for excess heterozygosity. Returns:; StructExpression – A struct expression with two fields, het_freq_hwe; (tfloat64) and p_value (tfloat64). hail.expr.functions.binom_test(x, n, p, alternative)[source]; Performs a binomial test on p given x successes in n trials.; Returns the p-value from the exact binomial test of the null hypothesis that; success has probability p, given x successes in n trials.; The alternatives are interpreted as follows:; - 'less': a one-tailed test of the significance of x or fewer successes,; - 'greater': a one-tailed test of the significance of x or more successes, and; - 'two-sided': a two-tailed test of the significance of x or any equivalent or more unlikely outcome.; Examples; All the examples below use a fair coin as the null hypothesis. Zero is; interpreted as tail and one as heads.; Test if a coin is biased towards heads or tails after observing two heads; out of ten flips:; >>> hl.eval(hl.binom_test(2, 10, 0.5, 'two-sided')); 0.10937499999999994. Test if a coin is biased towards tails after observing four heads out of ten; flips:; >>> hl.eval(hl.binom_test(4, 10, 0.5, 'less')); 0.3769531250000001. Test if a coin is biased towards heads after observing thirty-two heads out; of fifty flips:; >>> hl.eval(hl.binom_test(32, 50, 0.5, 'greater')); 0.03245432353613613. Parameters:. x (int or Expression of type tint32) – Number of successes.; n (int or Expression of type tint32) – Number of trials.; p (float or Expression of type tfloat64) – Probability of success, between 0 and 1.; alternative – : One ",MatchSource.WIKI,docs/0.2/functions/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/stats.html
https://hail.is/docs/0.2/functions/stats.html:13142,Testability,test,test,13142,"e; correction instead, set one_sided=True and the p-value returned will be; from the one-sided exact test. Parameters:. n_hom_ref (int or Expression of type tint32) – Number of homozygous reference genotypes.; n_het (int or Expression of type tint32) – Number of heterozygous genotypes.; n_hom_var (int or Expression of type tint32) – Number of homozygous variant genotypes.; one_sided (bool) – False by default. When True, perform one-sided test for excess heterozygosity. Returns:; StructExpression – A struct expression with two fields, het_freq_hwe; (tfloat64) and p_value (tfloat64). hail.expr.functions.binom_test(x, n, p, alternative)[source]; Performs a binomial test on p given x successes in n trials.; Returns the p-value from the exact binomial test of the null hypothesis that; success has probability p, given x successes in n trials.; The alternatives are interpreted as follows:; - 'less': a one-tailed test of the significance of x or fewer successes,; - 'greater': a one-tailed test of the significance of x or more successes, and; - 'two-sided': a two-tailed test of the significance of x or any equivalent or more unlikely outcome.; Examples; All the examples below use a fair coin as the null hypothesis. Zero is; interpreted as tail and one as heads.; Test if a coin is biased towards heads or tails after observing two heads; out of ten flips:; >>> hl.eval(hl.binom_test(2, 10, 0.5, 'two-sided')); 0.10937499999999994. Test if a coin is biased towards tails after observing four heads out of ten; flips:; >>> hl.eval(hl.binom_test(4, 10, 0.5, 'less')); 0.3769531250000001. Test if a coin is biased towards heads after observing thirty-two heads out; of fifty flips:; >>> hl.eval(hl.binom_test(32, 50, 0.5, 'greater')); 0.03245432353613613. Parameters:. x (int or Expression of type tint32) – Number of successes.; n (int or Expression of type tint32) – Number of trials.; p (float or Expression of type tfloat64) – Probability of success, between 0 and 1.; alternative – : One ",MatchSource.WIKI,docs/0.2/functions/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/stats.html
https://hail.is/docs/0.2/functions/stats.html:13224,Testability,test,test,13224,"e; correction instead, set one_sided=True and the p-value returned will be; from the one-sided exact test. Parameters:. n_hom_ref (int or Expression of type tint32) – Number of homozygous reference genotypes.; n_het (int or Expression of type tint32) – Number of heterozygous genotypes.; n_hom_var (int or Expression of type tint32) – Number of homozygous variant genotypes.; one_sided (bool) – False by default. When True, perform one-sided test for excess heterozygosity. Returns:; StructExpression – A struct expression with two fields, het_freq_hwe; (tfloat64) and p_value (tfloat64). hail.expr.functions.binom_test(x, n, p, alternative)[source]; Performs a binomial test on p given x successes in n trials.; Returns the p-value from the exact binomial test of the null hypothesis that; success has probability p, given x successes in n trials.; The alternatives are interpreted as follows:; - 'less': a one-tailed test of the significance of x or fewer successes,; - 'greater': a one-tailed test of the significance of x or more successes, and; - 'two-sided': a two-tailed test of the significance of x or any equivalent or more unlikely outcome.; Examples; All the examples below use a fair coin as the null hypothesis. Zero is; interpreted as tail and one as heads.; Test if a coin is biased towards heads or tails after observing two heads; out of ten flips:; >>> hl.eval(hl.binom_test(2, 10, 0.5, 'two-sided')); 0.10937499999999994. Test if a coin is biased towards tails after observing four heads out of ten; flips:; >>> hl.eval(hl.binom_test(4, 10, 0.5, 'less')); 0.3769531250000001. Test if a coin is biased towards heads after observing thirty-two heads out; of fifty flips:; >>> hl.eval(hl.binom_test(32, 50, 0.5, 'greater')); 0.03245432353613613. Parameters:. x (int or Expression of type tint32) – Number of successes.; n (int or Expression of type tint32) – Number of trials.; p (float or Expression of type tfloat64) – Probability of success, between 0 and 1.; alternative – : One ",MatchSource.WIKI,docs/0.2/functions/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/stats.html
https://hail.is/docs/0.2/functions/stats.html:15182,Testability,log,logarithm,15182,"ter”, “less”, (deprecated: “two.sided”). Returns:; Expression of type tfloat64 – p-value. hail.expr.functions.pchisqtail(x, df, ncp=None, lower_tail=False, log_p=False)[source]; Returns the probability under the right-tail starting at x for a chi-squared; distribution with df degrees of freedom.; Examples; >>> hl.eval(hl.pchisqtail(5, 1)); 0.025347318677468304. >>> hl.eval(hl.pchisqtail(5, 1, ncp=2)); 0.20571085634347097. >>> hl.eval(hl.pchisqtail(5, 1, lower_tail=True)); 0.9746526813225317. >>> hl.eval(hl.pchisqtail(5, 1, log_p=True)); -3.6750823266311876. Parameters:. x (float or Expression of type tfloat64) – The value at which to evaluate the CDF.; df (float or Expression of type tfloat64) – Degrees of freedom.; ncp (float or Expression of type tfloat64) – Noncentrality parameter, defaults to 0 if unspecified.; lower_tail (bool or BooleanExpression) – If True, compute the probability of an outcome at or below x,; otherwise greater than x.; log_p (bool or BooleanExpression) – Return the natural logarithm of the probability. Returns:; Expression of type tfloat64. hail.expr.functions.pgenchisq(x, w, k, lam, mu, sigma, *, max_iterations=None, min_accuracy=None)[source]; The cumulative probability function of a generalized chi-squared distribution.; The generalized chi-squared distribution has many interpretations. We share here four; interpretations of the values of this distribution:. A linear combination of normal variables and squares of normal variables.; A weighted sum of sums of squares of normally distributed values plus a normally distributed; value.; A weighted sum of chi-squared distributed values plus a normally distributed value.; A “quadratic form” in a vector; of uncorrelated standard normal values. The parameters of this function correspond to the parameters of the third interpretation. \[\begin{aligned}; w &: R^n \quad k : Z^n \quad lam : R^n \quad mu : R \quad sigma : R \\; \\; x &\sim N(mu, sigma^2) \\; y_i &\sim \mathrm{NonCentralChiSquared}(k_i",MatchSource.WIKI,docs/0.2/functions/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/stats.html
https://hail.is/docs/0.2/functions/stats.html:18632,Testability,test,test,18632,"10, 2], k=[1, 2, 3], lam=[2, 3, 7], mu=-10, sigma=0).value); 0.14284718767288906; >>> hl.eval(hl.pgenchisq(-20, w=[1, -10, 2], k=[1, 2, 3], lam=[2, 3, 7], mu=-10, sigma=0).value); 0.5950150356303258; >>> hl.eval(hl.pgenchisq(10 , w=[1, -10, 2], k=[1, 2, 3], lam=[2, 3, 7], mu=-10, sigma=0).value); 0.923219534175858; >>> hl.eval(hl.pgenchisq(40 , w=[1, -10, 2], k=[1, 2, 3], lam=[2, 3, 7], mu=-10, sigma=0).value); 0.9971746768781656. Notes; We follow Wikipedia’s notational conventions. Some texts refer to the weight vector (our w) as; \(\lambda\) or lb and the non-centrality vector (our lam) as nc.; We use the Davies’ algorithm which was published as:. Davies, Robert. “The distribution of a linear combination of chi-squared random variables.”; Applied Statistics 29 323-333. 1980. Davies included Fortran source code in the original publication. Davies also released a C; language port. Hail’s implementation is a fairly direct port; of the C implementation to Scala. Davies provides 39 test cases with the source code. The Hail; tests include all 39 test cases as well as a few additional tests.; Davies’ website cautions:. The method works well in most situations if you want only modest accuracy, say 0.0001. But; problems may arise if the sum is dominated by one or two terms with a total of only one or; two degrees of freedom and x is small. For an accessible introduction the Generalized Chi-Squared Distribution, we strongly recommend; the introduction of this paper:. Das, Abhranil; Geisler, Wilson (2020). “A method to integrate and classify normal; distributions”. Parameters:. x (float or Expression of type tfloat64) – The value at which to evaluate the cumulative distribution function (CDF).; w (list of float or Expression of type tarray of tfloat64) – A weight for each non-central chi-square term.; k (list of int or Expression of type tarray of tint32) – A degrees of freedom parameter for each non-central chi-square term.; lam (list of float or Expression of type tarray o",MatchSource.WIKI,docs/0.2/functions/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/stats.html
https://hail.is/docs/0.2/functions/stats.html:18675,Testability,test,tests,18675,"4284718767288906; >>> hl.eval(hl.pgenchisq(-20, w=[1, -10, 2], k=[1, 2, 3], lam=[2, 3, 7], mu=-10, sigma=0).value); 0.5950150356303258; >>> hl.eval(hl.pgenchisq(10 , w=[1, -10, 2], k=[1, 2, 3], lam=[2, 3, 7], mu=-10, sigma=0).value); 0.923219534175858; >>> hl.eval(hl.pgenchisq(40 , w=[1, -10, 2], k=[1, 2, 3], lam=[2, 3, 7], mu=-10, sigma=0).value); 0.9971746768781656. Notes; We follow Wikipedia’s notational conventions. Some texts refer to the weight vector (our w) as; \(\lambda\) or lb and the non-centrality vector (our lam) as nc.; We use the Davies’ algorithm which was published as:. Davies, Robert. “The distribution of a linear combination of chi-squared random variables.”; Applied Statistics 29 323-333. 1980. Davies included Fortran source code in the original publication. Davies also released a C; language port. Hail’s implementation is a fairly direct port; of the C implementation to Scala. Davies provides 39 test cases with the source code. The Hail; tests include all 39 test cases as well as a few additional tests.; Davies’ website cautions:. The method works well in most situations if you want only modest accuracy, say 0.0001. But; problems may arise if the sum is dominated by one or two terms with a total of only one or; two degrees of freedom and x is small. For an accessible introduction the Generalized Chi-Squared Distribution, we strongly recommend; the introduction of this paper:. Das, Abhranil; Geisler, Wilson (2020). “A method to integrate and classify normal; distributions”. Parameters:. x (float or Expression of type tfloat64) – The value at which to evaluate the cumulative distribution function (CDF).; w (list of float or Expression of type tarray of tfloat64) – A weight for each non-central chi-square term.; k (list of int or Expression of type tarray of tint32) – A degrees of freedom parameter for each non-central chi-square term.; lam (list of float or Expression of type tarray of tfloat64) – A non-centrality parameter for each non-central chi",MatchSource.WIKI,docs/0.2/functions/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/stats.html
https://hail.is/docs/0.2/functions/stats.html:18696,Testability,test,test,18696,"4284718767288906; >>> hl.eval(hl.pgenchisq(-20, w=[1, -10, 2], k=[1, 2, 3], lam=[2, 3, 7], mu=-10, sigma=0).value); 0.5950150356303258; >>> hl.eval(hl.pgenchisq(10 , w=[1, -10, 2], k=[1, 2, 3], lam=[2, 3, 7], mu=-10, sigma=0).value); 0.923219534175858; >>> hl.eval(hl.pgenchisq(40 , w=[1, -10, 2], k=[1, 2, 3], lam=[2, 3, 7], mu=-10, sigma=0).value); 0.9971746768781656. Notes; We follow Wikipedia’s notational conventions. Some texts refer to the weight vector (our w) as; \(\lambda\) or lb and the non-centrality vector (our lam) as nc.; We use the Davies’ algorithm which was published as:. Davies, Robert. “The distribution of a linear combination of chi-squared random variables.”; Applied Statistics 29 323-333. 1980. Davies included Fortran source code in the original publication. Davies also released a C; language port. Hail’s implementation is a fairly direct port; of the C implementation to Scala. Davies provides 39 test cases with the source code. The Hail; tests include all 39 test cases as well as a few additional tests.; Davies’ website cautions:. The method works well in most situations if you want only modest accuracy, say 0.0001. But; problems may arise if the sum is dominated by one or two terms with a total of only one or; two degrees of freedom and x is small. For an accessible introduction the Generalized Chi-Squared Distribution, we strongly recommend; the introduction of this paper:. Das, Abhranil; Geisler, Wilson (2020). “A method to integrate and classify normal; distributions”. Parameters:. x (float or Expression of type tfloat64) – The value at which to evaluate the cumulative distribution function (CDF).; w (list of float or Expression of type tarray of tfloat64) – A weight for each non-central chi-square term.; k (list of int or Expression of type tarray of tint32) – A degrees of freedom parameter for each non-central chi-square term.; lam (list of float or Expression of type tarray of tfloat64) – A non-centrality parameter for each non-central chi",MatchSource.WIKI,docs/0.2/functions/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/stats.html
https://hail.is/docs/0.2/functions/stats.html:18735,Testability,test,tests,18735,"4284718767288906; >>> hl.eval(hl.pgenchisq(-20, w=[1, -10, 2], k=[1, 2, 3], lam=[2, 3, 7], mu=-10, sigma=0).value); 0.5950150356303258; >>> hl.eval(hl.pgenchisq(10 , w=[1, -10, 2], k=[1, 2, 3], lam=[2, 3, 7], mu=-10, sigma=0).value); 0.923219534175858; >>> hl.eval(hl.pgenchisq(40 , w=[1, -10, 2], k=[1, 2, 3], lam=[2, 3, 7], mu=-10, sigma=0).value); 0.9971746768781656. Notes; We follow Wikipedia’s notational conventions. Some texts refer to the weight vector (our w) as; \(\lambda\) or lb and the non-centrality vector (our lam) as nc.; We use the Davies’ algorithm which was published as:. Davies, Robert. “The distribution of a linear combination of chi-squared random variables.”; Applied Statistics 29 323-333. 1980. Davies included Fortran source code in the original publication. Davies also released a C; language port. Hail’s implementation is a fairly direct port; of the C implementation to Scala. Davies provides 39 test cases with the source code. The Hail; tests include all 39 test cases as well as a few additional tests.; Davies’ website cautions:. The method works well in most situations if you want only modest accuracy, say 0.0001. But; problems may arise if the sum is dominated by one or two terms with a total of only one or; two degrees of freedom and x is small. For an accessible introduction the Generalized Chi-Squared Distribution, we strongly recommend; the introduction of this paper:. Das, Abhranil; Geisler, Wilson (2020). “A method to integrate and classify normal; distributions”. Parameters:. x (float or Expression of type tfloat64) – The value at which to evaluate the cumulative distribution function (CDF).; w (list of float or Expression of type tarray of tfloat64) – A weight for each non-central chi-square term.; k (list of int or Expression of type tarray of tint32) – A degrees of freedom parameter for each non-central chi-square term.; lam (list of float or Expression of type tarray of tfloat64) – A non-centrality parameter for each non-central chi",MatchSource.WIKI,docs/0.2/functions/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/stats.html
https://hail.is/docs/0.2/functions/stats.html:22129,Testability,log,logarithm,22129,"ons.pnorm(x, mu=0, sigma=1, lower_tail=True, log_p=False)[source]; The cumulative probability function of a normal distribution with mean; mu and standard deviation sigma. Returns cumulative probability of; standard normal distribution by default.; Examples; >>> hl.eval(hl.pnorm(0)); 0.5. >>> hl.eval(hl.pnorm(1, mu=2, sigma=2)); 0.30853753872598694. >>> hl.eval(hl.pnorm(2, lower_tail=False)); 0.022750131948179212. >>> hl.eval(hl.pnorm(2, log_p=True)); -0.023012909328963493. Notes; Returns the left-tail probability p = Prob(\(Z < x\)) with \(Z\); a normal random variable. Defaults to a standard normal random variable. Parameters:. x (float or Expression of type tfloat64); mu (float or Expression of type tfloat64) – Mean (default = 0).; sigma (float or Expression of type tfloat64) – Standard deviation (default = 1).; lower_tail (bool or BooleanExpression) – If True, compute the probability of an outcome at or below x,; otherwise greater than x.; log_p (bool or BooleanExpression) – Return the natural logarithm of the probability. Returns:; Expression of type tfloat64. hail.expr.functions.pT(x, n, lower_tail=True, log_p=False)[source]; The cumulative probability function of a t-distribution with; n degrees of freedom.; Examples; >>> hl.eval(hl.pT(0, 10)); 0.5. >>> hl.eval(hl.pT(1, 10)); 0.82955343384897. >>> hl.eval(hl.pT(1, 10, lower_tail=False)); 0.17044656615103004. >>> hl.eval(hl.pT(1, 10, log_p=True)); -0.186867754489647. Notes; If lower_tail is true, returns Prob(\(X \leq\) x) where \(X\) is; a t-distributed random variable with n degrees of freedom. If lower_tail; is false, returns Prob(\(X\) > x). Parameters:. x (float or Expression of type tfloat64); n (float or Expression of type tfloat64) – Degrees of freedom of the t-distribution.; lower_tail (bool or BooleanExpression) – If True, compute the probability of an outcome at or below x,; otherwise greater than x.; log_p (bool or BooleanExpression) – Return the natural logarithm of the probability. Returns:; Ex",MatchSource.WIKI,docs/0.2/functions/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/stats.html
https://hail.is/docs/0.2/functions/stats.html:23073,Testability,log,logarithm,23073,"eater than x.; log_p (bool or BooleanExpression) – Return the natural logarithm of the probability. Returns:; Expression of type tfloat64. hail.expr.functions.pT(x, n, lower_tail=True, log_p=False)[source]; The cumulative probability function of a t-distribution with; n degrees of freedom.; Examples; >>> hl.eval(hl.pT(0, 10)); 0.5. >>> hl.eval(hl.pT(1, 10)); 0.82955343384897. >>> hl.eval(hl.pT(1, 10, lower_tail=False)); 0.17044656615103004. >>> hl.eval(hl.pT(1, 10, log_p=True)); -0.186867754489647. Notes; If lower_tail is true, returns Prob(\(X \leq\) x) where \(X\) is; a t-distributed random variable with n degrees of freedom. If lower_tail; is false, returns Prob(\(X\) > x). Parameters:. x (float or Expression of type tfloat64); n (float or Expression of type tfloat64) – Degrees of freedom of the t-distribution.; lower_tail (bool or BooleanExpression) – If True, compute the probability of an outcome at or below x,; otherwise greater than x.; log_p (bool or BooleanExpression) – Return the natural logarithm of the probability. Returns:; Expression of type tfloat64. hail.expr.functions.pF(x, df1, df2, lower_tail=True, log_p=False)[source]; The cumulative probability function of a F-distribution with parameters; df1 and df2.; Examples; >>> hl.eval(hl.pF(0, 3, 10)); 0.0. >>> hl.eval(hl.pF(1, 3, 10)); 0.5676627969783028. >>> hl.eval(hl.pF(1, 3, 10, lower_tail=False)); 0.4323372030216972. >>> hl.eval(hl.pF(1, 3, 10, log_p=True)); -0.566227703842908. Notes; If lower_tail is true, returns Prob(\(X \leq\) x) where \(X\) is; a random variable with distribution \(F`(df1, df2). If `lower_tail\); is false, returns Prob(\(X\) > x). Parameters:. x (float or Expression of type tfloat64); df1 (float or Expression of type tfloat64) – Parameter of the F-distribution; df2 (float or Expression of type tfloat64) – Parameter of the F-distribution; lower_tail (bool or BooleanExpression) – If True, compute the probability of an outcome at or below x,; otherwise greater than x.; log_p (bo",MatchSource.WIKI,docs/0.2/functions/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/stats.html
https://hail.is/docs/0.2/functions/stats.html:24105,Testability,log,logarithm,24105,"robability. Returns:; Expression of type tfloat64. hail.expr.functions.pF(x, df1, df2, lower_tail=True, log_p=False)[source]; The cumulative probability function of a F-distribution with parameters; df1 and df2.; Examples; >>> hl.eval(hl.pF(0, 3, 10)); 0.0. >>> hl.eval(hl.pF(1, 3, 10)); 0.5676627969783028. >>> hl.eval(hl.pF(1, 3, 10, lower_tail=False)); 0.4323372030216972. >>> hl.eval(hl.pF(1, 3, 10, log_p=True)); -0.566227703842908. Notes; If lower_tail is true, returns Prob(\(X \leq\) x) where \(X\) is; a random variable with distribution \(F`(df1, df2). If `lower_tail\); is false, returns Prob(\(X\) > x). Parameters:. x (float or Expression of type tfloat64); df1 (float or Expression of type tfloat64) – Parameter of the F-distribution; df2 (float or Expression of type tfloat64) – Parameter of the F-distribution; lower_tail (bool or BooleanExpression) – If True, compute the probability of an outcome at or below x,; otherwise greater than x.; log_p (bool or BooleanExpression) – Return the natural logarithm of the probability. Returns:; Expression of type tfloat64. hail.expr.functions.ppois(x, lamb, lower_tail=True, log_p=False)[source]; The cumulative probability function of a Poisson distribution.; Examples; >>> hl.eval(hl.ppois(2, 1)); 0.9196986029286058. Notes; If lower_tail is true, returns Prob(\(X \leq\) x) where \(X\) is a; Poisson random variable with rate parameter lamb. If lower_tail is false,; returns Prob(\(X\) > x). Parameters:. x (float or Expression of type tfloat64); lamb (float or Expression of type tfloat64) – Rate parameter of Poisson distribution.; lower_tail (bool or BooleanExpression) – If True, compute the probability of an outcome at or below x,; otherwise greater than x.; log_p (bool or BooleanExpression) – Return the natural logarithm of the probability. Returns:; Expression of type tfloat64. hail.expr.functions.qchisqtail(p, df, ncp=None, lower_tail=False, log_p=False)[source]; The quantile function of a chi-squared distribution with d",MatchSource.WIKI,docs/0.2/functions/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/stats.html
https://hail.is/docs/0.2/functions/stats.html:24875,Testability,log,logarithm,24875,"sion of type tfloat64) – Parameter of the F-distribution; lower_tail (bool or BooleanExpression) – If True, compute the probability of an outcome at or below x,; otherwise greater than x.; log_p (bool or BooleanExpression) – Return the natural logarithm of the probability. Returns:; Expression of type tfloat64. hail.expr.functions.ppois(x, lamb, lower_tail=True, log_p=False)[source]; The cumulative probability function of a Poisson distribution.; Examples; >>> hl.eval(hl.ppois(2, 1)); 0.9196986029286058. Notes; If lower_tail is true, returns Prob(\(X \leq\) x) where \(X\) is a; Poisson random variable with rate parameter lamb. If lower_tail is false,; returns Prob(\(X\) > x). Parameters:. x (float or Expression of type tfloat64); lamb (float or Expression of type tfloat64) – Rate parameter of Poisson distribution.; lower_tail (bool or BooleanExpression) – If True, compute the probability of an outcome at or below x,; otherwise greater than x.; log_p (bool or BooleanExpression) – Return the natural logarithm of the probability. Returns:; Expression of type tfloat64. hail.expr.functions.qchisqtail(p, df, ncp=None, lower_tail=False, log_p=False)[source]; The quantile function of a chi-squared distribution with df degrees of; freedom, inverts pchisqtail().; Examples; >>> hl.eval(hl.qchisqtail(0.05, 2)); 5.991464547107979. >>> hl.eval(hl.qchisqtail(0.05, 2, ncp=2)); 10.838131614372958. >>> hl.eval(hl.qchisqtail(0.05, 2, lower_tail=True)); 0.10258658877510107. >>> hl.eval(hl.qchisqtail(hl.log(0.05), 2, log_p=True)); 5.991464547107979. Notes; Returns right-quantile x for which p = Prob(\(Z^2\) > x) with; \(Z^2\) a chi-squared random variable with degrees of freedom specified; by df. The probability p must satisfy 0 < p < 1. Parameters:. p (float or Expression of type tfloat64) – Probability.; df (float or Expression of type tfloat64) – Degrees of freedom.; ncp (float or Expression of type tfloat64) – Corresponds to ncp parameter in pchisqtail().; lower_tail (bool or Bool",MatchSource.WIKI,docs/0.2/functions/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/stats.html
https://hail.is/docs/0.2/functions/stats.html:25371,Testability,log,log,25371,"tes; If lower_tail is true, returns Prob(\(X \leq\) x) where \(X\) is a; Poisson random variable with rate parameter lamb. If lower_tail is false,; returns Prob(\(X\) > x). Parameters:. x (float or Expression of type tfloat64); lamb (float or Expression of type tfloat64) – Rate parameter of Poisson distribution.; lower_tail (bool or BooleanExpression) – If True, compute the probability of an outcome at or below x,; otherwise greater than x.; log_p (bool or BooleanExpression) – Return the natural logarithm of the probability. Returns:; Expression of type tfloat64. hail.expr.functions.qchisqtail(p, df, ncp=None, lower_tail=False, log_p=False)[source]; The quantile function of a chi-squared distribution with df degrees of; freedom, inverts pchisqtail().; Examples; >>> hl.eval(hl.qchisqtail(0.05, 2)); 5.991464547107979. >>> hl.eval(hl.qchisqtail(0.05, 2, ncp=2)); 10.838131614372958. >>> hl.eval(hl.qchisqtail(0.05, 2, lower_tail=True)); 0.10258658877510107. >>> hl.eval(hl.qchisqtail(hl.log(0.05), 2, log_p=True)); 5.991464547107979. Notes; Returns right-quantile x for which p = Prob(\(Z^2\) > x) with; \(Z^2\) a chi-squared random variable with degrees of freedom specified; by df. The probability p must satisfy 0 < p < 1. Parameters:. p (float or Expression of type tfloat64) – Probability.; df (float or Expression of type tfloat64) – Degrees of freedom.; ncp (float or Expression of type tfloat64) – Corresponds to ncp parameter in pchisqtail().; lower_tail (bool or BooleanExpression) – Corresponds to lower_tail parameter in pchisqtail().; log_p (bool or BooleanExpression) – Exponentiate p, corresponds to log_p parameter in pchisqtail(). Returns:; Expression of type tfloat64. hail.expr.functions.qnorm(p, mu=0, sigma=1, lower_tail=True, log_p=False)[source]; The quantile function of a normal distribution with mean mu and; standard deviation sigma, inverts pnorm(). Returns quantile of; standard normal distribution by default.; Examples; >>> hl.eval(hl.qnorm(0.90)); 1.2815515",MatchSource.WIKI,docs/0.2/functions/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/stats.html
https://hail.is/docs/0.2/functions/stats.html:26541,Testability,log,log,26541,"eedom specified; by df. The probability p must satisfy 0 < p < 1. Parameters:. p (float or Expression of type tfloat64) – Probability.; df (float or Expression of type tfloat64) – Degrees of freedom.; ncp (float or Expression of type tfloat64) – Corresponds to ncp parameter in pchisqtail().; lower_tail (bool or BooleanExpression) – Corresponds to lower_tail parameter in pchisqtail().; log_p (bool or BooleanExpression) – Exponentiate p, corresponds to log_p parameter in pchisqtail(). Returns:; Expression of type tfloat64. hail.expr.functions.qnorm(p, mu=0, sigma=1, lower_tail=True, log_p=False)[source]; The quantile function of a normal distribution with mean mu and; standard deviation sigma, inverts pnorm(). Returns quantile of; standard normal distribution by default.; Examples; >>> hl.eval(hl.qnorm(0.90)); 1.2815515655446008. >>> hl.eval(hl.qnorm(0.90, mu=1, sigma=2)); 3.5631031310892016. >>> hl.eval(hl.qnorm(0.90, lower_tail=False)); -1.2815515655446008. >>> hl.eval(hl.qnorm(hl.log(0.90), log_p=True)); 1.2815515655446008. Notes; Returns left-quantile x for which p = Prob(\(Z\) < x) with \(Z\); a normal random variable with mean mu and standard deviation sigma.; Defaults to a standard normal random variable, and the probability p must; satisfy 0 < p < 1. Parameters:. p (float or Expression of type tfloat64) – Probability.; mu (float or Expression of type tfloat64) – Mean (default = 0).; sigma (float or Expression of type tfloat64) – Standard deviation (default = 1).; lower_tail (bool or BooleanExpression) – Corresponds to lower_tail parameter in pnorm().; log_p (bool or BooleanExpression) – Exponentiate p, corresponds to log_p parameter in pnorm(). Returns:; Expression of type tfloat64. hail.expr.functions.qpois(p, lamb, lower_tail=True, log_p=False)[source]; The quantile function of a Poisson distribution with rate parameter; lamb, inverts ppois().; Examples; >>> hl.eval(hl.qpois(0.99, 1)); 4. Notes; Returns the smallest integer \(x\) such that Prob(\(X \leq x\",MatchSource.WIKI,docs/0.2/functions/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/stats.html
https://hail.is/docs/0.2/functions/stats.html:27926,Testability,test,testing,27926,"e tfloat64. hail.expr.functions.qnorm(p, mu=0, sigma=1, lower_tail=True, log_p=False)[source]; The quantile function of a normal distribution with mean mu and; standard deviation sigma, inverts pnorm(). Returns quantile of; standard normal distribution by default.; Examples; >>> hl.eval(hl.qnorm(0.90)); 1.2815515655446008. >>> hl.eval(hl.qnorm(0.90, mu=1, sigma=2)); 3.5631031310892016. >>> hl.eval(hl.qnorm(0.90, lower_tail=False)); -1.2815515655446008. >>> hl.eval(hl.qnorm(hl.log(0.90), log_p=True)); 1.2815515655446008. Notes; Returns left-quantile x for which p = Prob(\(Z\) < x) with \(Z\); a normal random variable with mean mu and standard deviation sigma.; Defaults to a standard normal random variable, and the probability p must; satisfy 0 < p < 1. Parameters:. p (float or Expression of type tfloat64) – Probability.; mu (float or Expression of type tfloat64) – Mean (default = 0).; sigma (float or Expression of type tfloat64) – Standard deviation (default = 1).; lower_tail (bool or BooleanExpression) – Corresponds to lower_tail parameter in pnorm().; log_p (bool or BooleanExpression) – Exponentiate p, corresponds to log_p parameter in pnorm(). Returns:; Expression of type tfloat64. hail.expr.functions.qpois(p, lamb, lower_tail=True, log_p=False)[source]; The quantile function of a Poisson distribution with rate parameter; lamb, inverts ppois().; Examples; >>> hl.eval(hl.qpois(0.99, 1)); 4. Notes; Returns the smallest integer \(x\) such that Prob(\(X \leq x\)) \(\geq\) p where \(X\); is a Poisson random variable with rate parameter lambda. Parameters:. p (float or Expression of type tfloat64); lamb (float or Expression of type tfloat64) – Rate parameter of Poisson distribution.; lower_tail (bool or BooleanExpression) – Corresponds to lower_tail parameter in inverse ppois().; log_p (bool or BooleanExpression) – Exponentiate p before testing. Returns:; Expression of type tfloat64. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/functions/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/stats.html
https://hail.is/docs/0.2/functions/string.html:6999,Deployability,update,updated,6999,"type tint32. hail.expr.functions.parse_int32(x)[source]; Parse a string as a 32-bit integer.; Examples; >>> hl.eval(hl.parse_int32('154')); 154. >>> hl.eval(hl.parse_int32('15.4')); None. >>> hl.eval(hl.parse_int32('asdf')); None. Notes; If the input is an invalid integer, then result of this call will be missing. Parameters:; x (StringExpression). Returns:; NumericExpression of type tint32. hail.expr.functions.parse_int64(x)[source]; Parse a string as a 64-bit integer.; Examples; >>> hl.eval(hl.parse_int64('154')); 154. >>> hl.eval(hl.parse_int64('15.4')); None. >>> hl.eval(hl.parse_int64('asdf')); None. Notes; If the input is an invalid integer, then result of this call will be missing. Parameters:; x (StringExpression). Returns:; NumericExpression of type tint64. hail.expr.functions.parse_float(x)[source]; Parse a string as a 64-bit floating point number.; Examples; >>> hl.eval(hl.parse_float('1.1')); 1.1. >>> hl.eval(hl.parse_float('asdf')); None. Notes; If the input is an invalid floating point number, then result of this call will be missing. Parameters:; x (StringExpression). Returns:; NumericExpression of type tfloat64. hail.expr.functions.parse_float32(x)[source]; Parse a string as a 32-bit floating point number.; Examples; >>> hl.eval(hl.parse_float32('1.1')); 1.100000023841858. >>> hl.eval(hl.parse_float32('asdf')); None. Notes; If the input is an invalid floating point number, then result of this call will be missing. Parameters:; x (StringExpression). Returns:; NumericExpression of type tfloat32. hail.expr.functions.parse_float64(x)[source]; Parse a string as a 64-bit floating point number.; Examples; >>> hl.eval(hl.parse_float64('1.1')); 1.1. >>> hl.eval(hl.parse_float64('asdf')); None. Notes; If the input is an invalid floating point number, then result of this call will be missing. Parameters:; x (StringExpression). Returns:; NumericExpression of type tfloat64. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/functions/string.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/string.html
https://hail.is/docs/0.2/functions/string.html:2188,Modifiability,variab,variable-length,2188,"nt(x); Parse a string as a 32-bit integer. parse_int32(x); Parse a string as a 32-bit integer. parse_int64(x); Parse a string as a 64-bit integer. parse_float(x); Parse a string as a 64-bit floating point number. parse_float32(x); Parse a string as a 32-bit floating point number. parse_float64(x); Parse a string as a 64-bit floating point number. hail.expr.functions.format(f, *args)[source]; Returns a formatted string using a specified format string and arguments.; Examples; >>> hl.eval(hl.format('%.3e', 0.09345332)); '9.345e-02'. >>> hl.eval(hl.format('%.4f', hl.missing(hl.tfloat64))); 'null'. >>> hl.eval(hl.format('%s %s %s', 'hello', hl.tuple([3, hl.locus('1', 2453)]), True)); 'hello (3, 1:2453) true'. Notes; See the Java documentation; for valid format specifiers and arguments.; Missing values are printed as 'null' except when using the; format flags ‘b’ and ‘B’ (printed as 'false' instead). Parameters:. f (StringExpression) – Java format string.; args (variable-length arguments of Expression) – Arguments to format. Returns:; StringExpression. hail.expr.functions.json(x)[source]; Convert an expression to a JSON string expression.; Examples; >>> hl.eval(hl.json([1,2,3,4,5])); '[1,2,3,4,5]'. >>> hl.eval(hl.json(hl.struct(a='Hello', b=0.12345, c=[1,2], d={'hi', 'bye'}))); '{""a"":""Hello"",""b"":0.12345,""c"":[1,2],""d"":[""bye"",""hi""]}'. Parameters:; x – Expression to convert. Returns:; StringExpression – String expression with JSON representation of x. hail.expr.functions.parse_json(x, dtype)[source]; Convert a JSON string to a structured expression.; Examples; >>> json_str = '{""a"": 5, ""b"": 1.1, ""c"": ""foo""}'; >>> parsed = hl.parse_json(json_str, dtype='struct{a: int32, b: float64, c: str}'); >>> hl.eval(parsed.a); 5. Parameters:. x (StringExpression) – JSON string.; dtype – Type of value to parse. Returns:; Expression. hail.expr.functions.hamming(s1, s2)[source]; Returns the Hamming distance between the two strings.; Examples; >>> hl.eval(hl.hamming('ATATA', 'ATGCA')); 2.",MatchSource.WIKI,docs/0.2/functions/string.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/string.html
https://hail.is/docs/0.2/genetics/hail.genetics.AlleleType.html:2441,Deployability,update,updated,2441,"eatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; genetics; AlleleType. View page source. AlleleType. class hail.genetics.AlleleType[source]; An enumeration for allele type.; Notes; The precise values of the enumeration constants are not guarenteed; to be stable and must not be relied upon.; Attributes. UNKNOWN; Unknown Allele Type. SNP; Single-nucleotide Polymorphism (SNP). MNP; Multi-nucleotide Polymorphism (MNP). INSERTION; Insertion. DELETION; Deletion. COMPLEX; Complex Polymorphism. STAR; Star Allele (alt=*). SYMBOLIC; Symbolic Allele. TRANSITION; Transition SNP. TRANSVERSION; Transversion SNP. pretty_name; A formatted (as opposed to uppercase) version of the member's name, to match allele_type(). Methods. strings; Returns the names of the allele types, for use with literal(). COMPLEX = 5; Complex Polymorphism. DELETION = 4; Deletion. INSERTION = 3; Insertion. MNP = 2; Multi-nucleotide Polymorphism (MNP). SNP = 1; Single-nucleotide Polymorphism (SNP). STAR = 6; Star Allele (alt=*). SYMBOLIC = 7; Symbolic Allele; e.g. alt=<INS>. TRANSITION = 8; Transition SNP; e.g. ref=A alt=G. Note; This is only really used internally in hail.vds.sample_qc() and; hail.methods.sample_qc(). TRANSVERSION = 9; Transversion SNP; e.g. ref=A alt=C. Note; This is only really used internally in hail.vds.sample_qc() and; hail.methods.sample_qc(). UNKNOWN = 0; Unknown Allele Type. property pretty_name; A formatted (as opposed to uppercase) version of the member’s name,; to match allele_type(); Examples; >>> AlleleType.INSERTION.pretty_name; 'Insertion'; >>> at = AlleleType(hl.eval(hl.numeric_allele_type('a', 'att'))); >>> at.pretty_name == hl.eval(hl.allele_type('a', 'att')); True. static strings()[source]; Returns the names of the allele types, for use with; literal(). Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/genetics/hail.genetics.AlleleType.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/genetics/hail.genetics.AlleleType.html
https://hail.is/docs/0.2/genetics/hail.genetics.Call.html:3966,Deployability,update,updated,3966,"ation of the called alleles. unphased_diploid_gt_index; Return the genotype index for unphased, diploid calls. property alleles; Get the alleles of this call. Returns:; list of int. is_diploid()[source]; True if the ploidy == 2. Return type:; bool. is_haploid()[source]; True if the ploidy == 1. Return type:; bool. is_het()[source]; True if the call contains two different alleles. Return type:; bool. is_het_non_ref()[source]; True if the call contains two different alternate alleles. Return type:; bool. is_het_ref()[source]; True if the call contains one reference and one alternate allele. Return type:; bool. is_hom_ref()[source]; True if the call has no alternate alleles. Return type:; bool. is_hom_var()[source]; True if the call contains identical alternate alleles. Return type:; bool. is_non_ref()[source]; True if the call contains any non-reference alleles. Return type:; bool. n_alt_alleles()[source]; Returns the count of non-reference alleles. Return type:; int. one_hot_alleles(n_alleles)[source]; Returns a list containing the one-hot encoded representation of the; called alleles.; Examples; >>> n_alleles = 2; >>> hom_ref = hl.Call([0, 0]); >>> het = hl.Call([0, 1]); >>> hom_var = hl.Call([1, 1]). >>> het.one_hot_alleles(n_alleles); [1, 1]. >>> hom_var.one_hot_alleles(n_alleles); [0, 2]. Notes; This one-hot representation is the positional sum of the one-hot; encoding for each called allele. For a biallelic variant, the; one-hot encoding for a reference allele is [1, 0] and the one-hot; encoding for an alternate allele is [0, 1]. Parameters:; n_alleles (int) – Number of total alleles, including the reference. Returns:; list of int. property phased; True if the call is phased. Returns:; bool. property ploidy; The number of alleles for this call. Returns:; int. unphased_diploid_gt_index()[source]; Return the genotype index for unphased, diploid calls. Returns:; int. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/genetics/hail.genetics.Call.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/genetics/hail.genetics.Call.html
https://hail.is/docs/0.2/genetics/hail.genetics.Locus.html:2045,Deployability,update,updated,2045,". Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; genetics; Locus. View page source. Locus. class hail.genetics.Locus[source]; An object that represents a location in the genome. Parameters:. contig (str) – Chromosome identifier.; position (int) – Chromosomal position (1-indexed).; reference_genome (str or ReferenceGenome) – Reference genome to use. Note; This object refers to the Python value returned by taking or collecting; Hail expressions, e.g. mt.locus.take(5). This is rare; it is much; more common to manipulate the LocusExpression object, which is; constructed using the following functions:. locus(); parse_locus(); locus_from_global_position(). Attributes. contig; Chromosome identifier. position; Chromosomal position (1-based). reference_genome; Reference genome. Methods. parse; Parses a locus object from a CHR:POS string. property contig; Chromosome identifier.; :rtype: str. classmethod parse(string, reference_genome='default')[source]; Parses a locus object from a CHR:POS string.; Examples; >>> l1 = hl.Locus.parse('1:101230'); >>> l2 = hl.Locus.parse('X:4201230'). Parameters:. string (str) – String to parse.; reference_genome (str or ReferenceGenome) – Reference genome to use. Default is default_reference(). Return type:; Locus. property position; Chromosomal position (1-based).; :rtype: int. property reference_genome; Reference genome. Returns:; ReferenceGenome. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/genetics/hail.genetics.Locus.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/genetics/hail.genetics.Locus.html
https://hail.is/docs/0.2/genetics/hail.genetics.Pedigree.html:2658,Deployability,update,updated,2658,". class hail.genetics.Pedigree[source]; Class containing a list of trios, with extra functionality. Parameters:; trios (list of Trio) – list of trio objects to include in pedigree. Attributes. trios; List of trio objects in this pedigree. Methods. complete_trios; List of trio objects that have a defined father and mother. filter_to; Filter the pedigree to a given list of sample IDs. read; Read a PLINK .fam file and return a pedigree object. write; Write a .fam file to the given path. complete_trios()[source]; List of trio objects that have a defined father and mother. Return type:; list of Trio. filter_to(samples)[source]; Filter the pedigree to a given list of sample IDs.; Notes; For any trio, the following steps will be applied:. If the proband is not in the list of samples provided, the trio is removed.; If the father is not in the list of samples provided, pat_id is set to None.; If the mother is not in the list of samples provided, mat_id is set to None. Parameters:; samples (list [str]) – Sample IDs to keep. Returns:; Pedigree. classmethod read(fam_path, delimiter='\\s+')[source]; Read a PLINK .fam file and return a pedigree object.; Examples; >>> ped = hl.Pedigree.read('data/test.fam'). Notes; See PLINK .fam file for; the required format. Parameters:. fam_path (str) – path to .fam file.; delimiter (str) – Field delimiter. Return type:; Pedigree. property trios; List of trio objects in this pedigree. Return type:; list of Trio. write(path)[source]; Write a .fam file to the given path.; Examples; >>> ped = hl.Pedigree.read('data/test.fam'); >>> ped.write('output/out.fam'). Notes; This method writes a PLINK .fam file. Caution; Phenotype information is not preserved in the Pedigree data; structure in Hail. Reading and writing a PLINK .fam file will; result in loss of this information. Use import_fam() to; manipulate this information. Parameters:; path (str) – output path. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/genetics/hail.genetics.Pedigree.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/genetics/hail.genetics.Pedigree.html
https://hail.is/docs/0.2/genetics/hail.genetics.Pedigree.html:1892,Testability,test,test,1892,". class hail.genetics.Pedigree[source]; Class containing a list of trios, with extra functionality. Parameters:; trios (list of Trio) – list of trio objects to include in pedigree. Attributes. trios; List of trio objects in this pedigree. Methods. complete_trios; List of trio objects that have a defined father and mother. filter_to; Filter the pedigree to a given list of sample IDs. read; Read a PLINK .fam file and return a pedigree object. write; Write a .fam file to the given path. complete_trios()[source]; List of trio objects that have a defined father and mother. Return type:; list of Trio. filter_to(samples)[source]; Filter the pedigree to a given list of sample IDs.; Notes; For any trio, the following steps will be applied:. If the proband is not in the list of samples provided, the trio is removed.; If the father is not in the list of samples provided, pat_id is set to None.; If the mother is not in the list of samples provided, mat_id is set to None. Parameters:; samples (list [str]) – Sample IDs to keep. Returns:; Pedigree. classmethod read(fam_path, delimiter='\\s+')[source]; Read a PLINK .fam file and return a pedigree object.; Examples; >>> ped = hl.Pedigree.read('data/test.fam'). Notes; See PLINK .fam file for; the required format. Parameters:. fam_path (str) – path to .fam file.; delimiter (str) – Field delimiter. Return type:; Pedigree. property trios; List of trio objects in this pedigree. Return type:; list of Trio. write(path)[source]; Write a .fam file to the given path.; Examples; >>> ped = hl.Pedigree.read('data/test.fam'); >>> ped.write('output/out.fam'). Notes; This method writes a PLINK .fam file. Caution; Phenotype information is not preserved in the Pedigree data; structure in Hail. Reading and writing a PLINK .fam file will; result in loss of this information. Use import_fam() to; manipulate this information. Parameters:; path (str) – output path. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/genetics/hail.genetics.Pedigree.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/genetics/hail.genetics.Pedigree.html
https://hail.is/docs/0.2/genetics/hail.genetics.Pedigree.html:2253,Testability,test,test,2253,". class hail.genetics.Pedigree[source]; Class containing a list of trios, with extra functionality. Parameters:; trios (list of Trio) – list of trio objects to include in pedigree. Attributes. trios; List of trio objects in this pedigree. Methods. complete_trios; List of trio objects that have a defined father and mother. filter_to; Filter the pedigree to a given list of sample IDs. read; Read a PLINK .fam file and return a pedigree object. write; Write a .fam file to the given path. complete_trios()[source]; List of trio objects that have a defined father and mother. Return type:; list of Trio. filter_to(samples)[source]; Filter the pedigree to a given list of sample IDs.; Notes; For any trio, the following steps will be applied:. If the proband is not in the list of samples provided, the trio is removed.; If the father is not in the list of samples provided, pat_id is set to None.; If the mother is not in the list of samples provided, mat_id is set to None. Parameters:; samples (list [str]) – Sample IDs to keep. Returns:; Pedigree. classmethod read(fam_path, delimiter='\\s+')[source]; Read a PLINK .fam file and return a pedigree object.; Examples; >>> ped = hl.Pedigree.read('data/test.fam'). Notes; See PLINK .fam file for; the required format. Parameters:. fam_path (str) – path to .fam file.; delimiter (str) – Field delimiter. Return type:; Pedigree. property trios; List of trio objects in this pedigree. Return type:; list of Trio. write(path)[source]; Write a .fam file to the given path.; Examples; >>> ped = hl.Pedigree.read('data/test.fam'); >>> ped.write('output/out.fam'). Notes; This method writes a PLINK .fam file. Caution; Phenotype information is not preserved in the Pedigree data; structure in Hail. Reading and writing a PLINK .fam file will; result in loss of this information. Use import_fam() to; manipulate this information. Parameters:; path (str) – output path. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/genetics/hail.genetics.Pedigree.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/genetics/hail.genetics.Pedigree.html
https://hail.is/docs/0.2/genetics/hail.genetics.ReferenceGenome.html:3460,Availability,avail,available,3460,"s (list of str) – Contig names.; lengths (dict of str to int) – Dict of contig names to contig lengths.; x_contigs (str or list of str) – Contigs to be treated as X chromosomes.; y_contigs (str or list of str) – Contigs to be treated as Y chromosomes.; mt_contigs (str or list of str) – Contigs to be treated as mitochondrial DNA.; par (list of tuple of (str, int, int)) – List of tuples with (contig, start, end). Attributes. contigs; Contig names. global_positions_dict; Get a dictionary mapping contig names to their global genomic positions. lengths; Dict of contig name to contig length. mt_contigs; Mitochondrial contigs. name; Name of reference genome. par; Pseudoautosomal regions. x_contigs; X contigs. y_contigs; Y contigs. Methods. add_liftover; Register a chain file for liftover. add_sequence; Load the reference sequence from a FASTA file. contig_length; Contig length. from_fasta_file; Create reference genome from a FASTA file. has_liftover; True if a liftover chain file is available from this reference genome to the destination reference. has_sequence; True if the reference sequence has been loaded. locus_from_global_position; "". read; Load reference genome from a JSON file. remove_liftover; Remove liftover to dest_reference_genome. remove_sequence; Remove the reference sequence. write; ""Write this reference genome to a file in JSON format. add_liftover(chain_file, dest_reference_genome)[source]; Register a chain file for liftover.; Examples; Access GRCh37 and GRCh38 using get_reference():; >>> rg37 = hl.get_reference('GRCh37') ; >>> rg38 = hl.get_reference('GRCh38') . Add a chain file from 37 to 38:; >>> rg37.add_liftover('gs://hail-common/references/grch37_to_grch38.over.chain.gz', rg38) . Notes; This method can only be run once per reference genome. Use; has_liftover() to test whether a chain file has been registered.; The chain file format is described; here.; Chain files are hosted on google cloud for some of Hail’s built-in; references:; GRCh37 to GRCh38; ",MatchSource.WIKI,docs/0.2/genetics/hail.genetics.ReferenceGenome.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/genetics/hail.genetics.ReferenceGenome.html
https://hail.is/docs/0.2/genetics/hail.genetics.ReferenceGenome.html:4614,Availability,down,download,4614,"Load reference genome from a JSON file. remove_liftover; Remove liftover to dest_reference_genome. remove_sequence; Remove the reference sequence. write; ""Write this reference genome to a file in JSON format. add_liftover(chain_file, dest_reference_genome)[source]; Register a chain file for liftover.; Examples; Access GRCh37 and GRCh38 using get_reference():; >>> rg37 = hl.get_reference('GRCh37') ; >>> rg38 = hl.get_reference('GRCh38') . Add a chain file from 37 to 38:; >>> rg37.add_liftover('gs://hail-common/references/grch37_to_grch38.over.chain.gz', rg38) . Notes; This method can only be run once per reference genome. Use; has_liftover() to test whether a chain file has been registered.; The chain file format is described; here.; Chain files are hosted on google cloud for some of Hail’s built-in; references:; GRCh37 to GRCh38; gs://hail-common/references/grch37_to_grch38.over.chain.gz; GRCh38 to GRCh37; gs://hail-common/references/grch38_to_grch37.over.chain.gz; Public download links are available; here. Parameters:. chain_file (str) – Path to chain file. Can be compressed (GZIP) or uncompressed.; dest_reference_genome (str or ReferenceGenome) – Reference genome to convert to. add_sequence(fasta_file, index_file=None)[source]; Load the reference sequence from a FASTA file.; Examples; Access the GRCh37 reference genome using get_reference():; >>> rg = hl.get_reference('GRCh37') . Add a sequence file:; >>> rg.add_sequence('gs://hail-common/references/human_g1k_v37.fasta.gz',; ... 'gs://hail-common/references/human_g1k_v37.fasta.fai') . Add a sequence file with the default index location:; >>> rg.add_sequence('gs://hail-common/references/human_g1k_v37.fasta.gz') . Notes; This method can only be run once per reference genome. Use; has_sequence() to test whether a sequence is loaded.; FASTA and index files are hosted on google cloud for some of Hail’s built-in; references:; GRCh37. FASTA file: gs://hail-common/references/human_g1k_v37.fasta.gz; Index file: gs://hail",MatchSource.WIKI,docs/0.2/genetics/hail.genetics.ReferenceGenome.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/genetics/hail.genetics.ReferenceGenome.html
https://hail.is/docs/0.2/genetics/hail.genetics.ReferenceGenome.html:4633,Availability,avail,available,4633,"Load reference genome from a JSON file. remove_liftover; Remove liftover to dest_reference_genome. remove_sequence; Remove the reference sequence. write; ""Write this reference genome to a file in JSON format. add_liftover(chain_file, dest_reference_genome)[source]; Register a chain file for liftover.; Examples; Access GRCh37 and GRCh38 using get_reference():; >>> rg37 = hl.get_reference('GRCh37') ; >>> rg38 = hl.get_reference('GRCh38') . Add a chain file from 37 to 38:; >>> rg37.add_liftover('gs://hail-common/references/grch37_to_grch38.over.chain.gz', rg38) . Notes; This method can only be run once per reference genome. Use; has_liftover() to test whether a chain file has been registered.; The chain file format is described; here.; Chain files are hosted on google cloud for some of Hail’s built-in; references:; GRCh37 to GRCh38; gs://hail-common/references/grch37_to_grch38.over.chain.gz; GRCh38 to GRCh37; gs://hail-common/references/grch38_to_grch37.over.chain.gz; Public download links are available; here. Parameters:. chain_file (str) – Path to chain file. Can be compressed (GZIP) or uncompressed.; dest_reference_genome (str or ReferenceGenome) – Reference genome to convert to. add_sequence(fasta_file, index_file=None)[source]; Load the reference sequence from a FASTA file.; Examples; Access the GRCh37 reference genome using get_reference():; >>> rg = hl.get_reference('GRCh37') . Add a sequence file:; >>> rg.add_sequence('gs://hail-common/references/human_g1k_v37.fasta.gz',; ... 'gs://hail-common/references/human_g1k_v37.fasta.fai') . Add a sequence file with the default index location:; >>> rg.add_sequence('gs://hail-common/references/human_g1k_v37.fasta.gz') . Notes; This method can only be run once per reference genome. Use; has_sequence() to test whether a sequence is loaded.; FASTA and index files are hosted on google cloud for some of Hail’s built-in; references:; GRCh37. FASTA file: gs://hail-common/references/human_g1k_v37.fasta.gz; Index file: gs://hail",MatchSource.WIKI,docs/0.2/genetics/hail.genetics.ReferenceGenome.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/genetics/hail.genetics.ReferenceGenome.html
https://hail.is/docs/0.2/genetics/hail.genetics.ReferenceGenome.html:5834,Availability,down,download,5834,"le, index_file=None)[source]; Load the reference sequence from a FASTA file.; Examples; Access the GRCh37 reference genome using get_reference():; >>> rg = hl.get_reference('GRCh37') . Add a sequence file:; >>> rg.add_sequence('gs://hail-common/references/human_g1k_v37.fasta.gz',; ... 'gs://hail-common/references/human_g1k_v37.fasta.fai') . Add a sequence file with the default index location:; >>> rg.add_sequence('gs://hail-common/references/human_g1k_v37.fasta.gz') . Notes; This method can only be run once per reference genome. Use; has_sequence() to test whether a sequence is loaded.; FASTA and index files are hosted on google cloud for some of Hail’s built-in; references:; GRCh37. FASTA file: gs://hail-common/references/human_g1k_v37.fasta.gz; Index file: gs://hail-common/references/human_g1k_v37.fasta.fai. GRCh38. FASTA file: gs://hail-common/references/Homo_sapiens_assembly38.fasta.gz; Index file: gs://hail-common/references/Homo_sapiens_assembly38.fasta.fai. Public download links are available; here. Parameters:. fasta_file (str) – Path to FASTA file. Can be compressed (GZIP) or uncompressed.; index_file (None or str) – Path to FASTA index file. Must be uncompressed. If None, replace; the fasta_file’s extension with fai. contig_length(contig)[source]; Contig length. Parameters:; contig (str) – Contig name. Returns:; int – Length of contig. property contigs; Contig names. Returns:; list of str. classmethod from_fasta_file(name, fasta_file, index_file, x_contigs=[], y_contigs=[], mt_contigs=[], par=[])[source]; Create reference genome from a FASTA file. Parameters:. name (str) – Name for new reference genome.; fasta_file (str) – Path to FASTA file. Can be compressed (GZIP) or uncompressed.; index_file (str) – Path to FASTA index file. Must be uncompressed.; x_contigs (str or list of str) – Contigs to be treated as X chromosomes.; y_contigs (str or list of str) – Contigs to be treated as Y chromosomes.; mt_contigs (str or list of str) – Contigs to be treated a",MatchSource.WIKI,docs/0.2/genetics/hail.genetics.ReferenceGenome.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/genetics/hail.genetics.ReferenceGenome.html
https://hail.is/docs/0.2/genetics/hail.genetics.ReferenceGenome.html:5853,Availability,avail,available,5853,"le, index_file=None)[source]; Load the reference sequence from a FASTA file.; Examples; Access the GRCh37 reference genome using get_reference():; >>> rg = hl.get_reference('GRCh37') . Add a sequence file:; >>> rg.add_sequence('gs://hail-common/references/human_g1k_v37.fasta.gz',; ... 'gs://hail-common/references/human_g1k_v37.fasta.fai') . Add a sequence file with the default index location:; >>> rg.add_sequence('gs://hail-common/references/human_g1k_v37.fasta.gz') . Notes; This method can only be run once per reference genome. Use; has_sequence() to test whether a sequence is loaded.; FASTA and index files are hosted on google cloud for some of Hail’s built-in; references:; GRCh37. FASTA file: gs://hail-common/references/human_g1k_v37.fasta.gz; Index file: gs://hail-common/references/human_g1k_v37.fasta.fai. GRCh38. FASTA file: gs://hail-common/references/Homo_sapiens_assembly38.fasta.gz; Index file: gs://hail-common/references/Homo_sapiens_assembly38.fasta.fai. Public download links are available; here. Parameters:. fasta_file (str) – Path to FASTA file. Can be compressed (GZIP) or uncompressed.; index_file (None or str) – Path to FASTA index file. Must be uncompressed. If None, replace; the fasta_file’s extension with fai. contig_length(contig)[source]; Contig length. Parameters:; contig (str) – Contig name. Returns:; int – Length of contig. property contigs; Contig names. Returns:; list of str. classmethod from_fasta_file(name, fasta_file, index_file, x_contigs=[], y_contigs=[], mt_contigs=[], par=[])[source]; Create reference genome from a FASTA file. Parameters:. name (str) – Name for new reference genome.; fasta_file (str) – Path to FASTA file. Can be compressed (GZIP) or uncompressed.; index_file (str) – Path to FASTA index file. Must be uncompressed.; x_contigs (str or list of str) – Contigs to be treated as X chromosomes.; y_contigs (str or list of str) – Contigs to be treated as Y chromosomes.; mt_contigs (str or list of str) – Contigs to be treated a",MatchSource.WIKI,docs/0.2/genetics/hail.genetics.ReferenceGenome.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/genetics/hail.genetics.ReferenceGenome.html
https://hail.is/docs/0.2/genetics/hail.genetics.ReferenceGenome.html:7240,Availability,avail,available,7240,"; Contig names. Returns:; list of str. classmethod from_fasta_file(name, fasta_file, index_file, x_contigs=[], y_contigs=[], mt_contigs=[], par=[])[source]; Create reference genome from a FASTA file. Parameters:. name (str) – Name for new reference genome.; fasta_file (str) – Path to FASTA file. Can be compressed (GZIP) or uncompressed.; index_file (str) – Path to FASTA index file. Must be uncompressed.; x_contigs (str or list of str) – Contigs to be treated as X chromosomes.; y_contigs (str or list of str) – Contigs to be treated as Y chromosomes.; mt_contigs (str or list of str) – Contigs to be treated as mitochondrial DNA.; par (list of tuple of (str, int, int)) – List of tuples with (contig, start, end). Returns:; ReferenceGenome. property global_positions_dict; Get a dictionary mapping contig names to their global genomic positions. Returns:; dict – A dictionary of contig names to global genomic positions. has_liftover(dest_reference_genome)[source]; True if a liftover chain file is available from this reference; genome to the destination reference. Parameters:; dest_reference_genome (str or ReferenceGenome). Returns:; bool. has_sequence()[source]; True if the reference sequence has been loaded. Returns:; bool. property lengths; Dict of contig name to contig length. Returns:; dict of str to int. locus_from_global_position(global_pos)[source]; ”; Constructs a locus from a global position in reference genome.; The inverse of Locus.position().; Examples; >>> rg = hl.get_reference('GRCh37'); >>> rg.locus_from_global_position(0); Locus(contig=1, position=1, reference_genome=GRCh37). >>> rg.locus_from_global_position(2824183054); Locus(contig=21, position=42584230, reference_genome=GRCh37). >>> rg = hl.get_reference('GRCh38'); >>> rg.locus_from_global_position(2824183054); Locus(contig=chr22, position=1, reference_genome=GRCh38). Parameters:; global_pos (int) – Zero-based global base position along the reference genome. Returns:; Locus. property mt_contigs; Mi",MatchSource.WIKI,docs/0.2/genetics/hail.genetics.ReferenceGenome.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/genetics/hail.genetics.ReferenceGenome.html
https://hail.is/docs/0.2/genetics/hail.genetics.ReferenceGenome.html:10210,Deployability,update,updated,10210,"ondrial contigs. Returns:; list of str. property name; Name of reference genome. Returns:; str. property par; Pseudoautosomal regions. Returns:; list of Interval. classmethod read(path)[source]; Load reference genome from a JSON file.; Notes; The JSON file must have the following format:; {""name"": ""my_reference_genome"",; ""contigs"": [{""name"": ""1"", ""length"": 10000000},; {""name"": ""2"", ""length"": 20000000},; {""name"": ""X"", ""length"": 19856300},; {""name"": ""Y"", ""length"": 78140000},; {""name"": ""MT"", ""length"": 532}],; ""xContigs"": [""X""],; ""yContigs"": [""Y""],; ""mtContigs"": [""MT""],; ""par"": [{""start"": {""contig"": ""X"",""position"": 60001},""end"": {""contig"": ""X"",""position"": 2699521}},; {""start"": {""contig"": ""Y"",""position"": 10001},""end"": {""contig"": ""Y"",""position"": 2649521}}]; }. name must be unique and not overlap with Hail’s pre-instantiated; references: 'GRCh37', 'GRCh38', 'GRCm38', 'CanFam3', and; 'default'.; The contig names in xContigs, yContigs, and mtContigs must be; present in contigs. The intervals listed in par must have contigs in; either xContigs or yContigs and must have positions between 0 and; the contig length given in contigs. Parameters:; path (str) – Path to JSON file. Returns:; ReferenceGenome. remove_liftover(dest_reference_genome)[source]; Remove liftover to dest_reference_genome. Parameters:; dest_reference_genome (str or ReferenceGenome). remove_sequence()[source]; Remove the reference sequence. write(output)[source]; “Write this reference genome to a file in JSON format.; Examples; >>> my_rg = hl.ReferenceGenome(""new_reference"", [""x"", ""y"", ""z""], {""x"": 500, ""y"": 300, ""z"": 200}); >>> my_rg.write(f""output/new_reference.json""). Notes; Use read() to reimport the exported; reference genome in a new HailContext session. Parameters:; output (str) – Path of JSON file to write. property x_contigs; X contigs. Returns:; list of str. property y_contigs; Y contigs. Returns:; list of str. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/genetics/hail.genetics.ReferenceGenome.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/genetics/hail.genetics.ReferenceGenome.html
https://hail.is/docs/0.2/genetics/hail.genetics.ReferenceGenome.html:3581,Performance,load,loaded,3581,"contig lengths.; x_contigs (str or list of str) – Contigs to be treated as X chromosomes.; y_contigs (str or list of str) – Contigs to be treated as Y chromosomes.; mt_contigs (str or list of str) – Contigs to be treated as mitochondrial DNA.; par (list of tuple of (str, int, int)) – List of tuples with (contig, start, end). Attributes. contigs; Contig names. global_positions_dict; Get a dictionary mapping contig names to their global genomic positions. lengths; Dict of contig name to contig length. mt_contigs; Mitochondrial contigs. name; Name of reference genome. par; Pseudoautosomal regions. x_contigs; X contigs. y_contigs; Y contigs. Methods. add_liftover; Register a chain file for liftover. add_sequence; Load the reference sequence from a FASTA file. contig_length; Contig length. from_fasta_file; Create reference genome from a FASTA file. has_liftover; True if a liftover chain file is available from this reference genome to the destination reference. has_sequence; True if the reference sequence has been loaded. locus_from_global_position; "". read; Load reference genome from a JSON file. remove_liftover; Remove liftover to dest_reference_genome. remove_sequence; Remove the reference sequence. write; ""Write this reference genome to a file in JSON format. add_liftover(chain_file, dest_reference_genome)[source]; Register a chain file for liftover.; Examples; Access GRCh37 and GRCh38 using get_reference():; >>> rg37 = hl.get_reference('GRCh37') ; >>> rg38 = hl.get_reference('GRCh38') . Add a chain file from 37 to 38:; >>> rg37.add_liftover('gs://hail-common/references/grch37_to_grch38.over.chain.gz', rg38) . Notes; This method can only be run once per reference genome. Use; has_liftover() to test whether a chain file has been registered.; The chain file format is described; here.; Chain files are hosted on google cloud for some of Hail’s built-in; references:; GRCh37 to GRCh38; gs://hail-common/references/grch37_to_grch38.over.chain.gz; GRCh38 to GRCh37; gs://hail-",MatchSource.WIKI,docs/0.2/genetics/hail.genetics.ReferenceGenome.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/genetics/hail.genetics.ReferenceGenome.html
https://hail.is/docs/0.2/genetics/hail.genetics.ReferenceGenome.html:5433,Performance,load,loaded,5433,"r some of Hail’s built-in; references:; GRCh37 to GRCh38; gs://hail-common/references/grch37_to_grch38.over.chain.gz; GRCh38 to GRCh37; gs://hail-common/references/grch38_to_grch37.over.chain.gz; Public download links are available; here. Parameters:. chain_file (str) – Path to chain file. Can be compressed (GZIP) or uncompressed.; dest_reference_genome (str or ReferenceGenome) – Reference genome to convert to. add_sequence(fasta_file, index_file=None)[source]; Load the reference sequence from a FASTA file.; Examples; Access the GRCh37 reference genome using get_reference():; >>> rg = hl.get_reference('GRCh37') . Add a sequence file:; >>> rg.add_sequence('gs://hail-common/references/human_g1k_v37.fasta.gz',; ... 'gs://hail-common/references/human_g1k_v37.fasta.fai') . Add a sequence file with the default index location:; >>> rg.add_sequence('gs://hail-common/references/human_g1k_v37.fasta.gz') . Notes; This method can only be run once per reference genome. Use; has_sequence() to test whether a sequence is loaded.; FASTA and index files are hosted on google cloud for some of Hail’s built-in; references:; GRCh37. FASTA file: gs://hail-common/references/human_g1k_v37.fasta.gz; Index file: gs://hail-common/references/human_g1k_v37.fasta.fai. GRCh38. FASTA file: gs://hail-common/references/Homo_sapiens_assembly38.fasta.gz; Index file: gs://hail-common/references/Homo_sapiens_assembly38.fasta.fai. Public download links are available; here. Parameters:. fasta_file (str) – Path to FASTA file. Can be compressed (GZIP) or uncompressed.; index_file (None or str) – Path to FASTA index file. Must be uncompressed. If None, replace; the fasta_file’s extension with fai. contig_length(contig)[source]; Contig length. Parameters:; contig (str) – Contig name. Returns:; int – Length of contig. property contigs; Contig names. Returns:; list of str. classmethod from_fasta_file(name, fasta_file, index_file, x_contigs=[], y_contigs=[], mt_contigs=[], par=[])[source]; Create reference ge",MatchSource.WIKI,docs/0.2/genetics/hail.genetics.ReferenceGenome.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/genetics/hail.genetics.ReferenceGenome.html
https://hail.is/docs/0.2/genetics/hail.genetics.ReferenceGenome.html:7450,Performance,load,loaded,7450," a FASTA file. Parameters:. name (str) – Name for new reference genome.; fasta_file (str) – Path to FASTA file. Can be compressed (GZIP) or uncompressed.; index_file (str) – Path to FASTA index file. Must be uncompressed.; x_contigs (str or list of str) – Contigs to be treated as X chromosomes.; y_contigs (str or list of str) – Contigs to be treated as Y chromosomes.; mt_contigs (str or list of str) – Contigs to be treated as mitochondrial DNA.; par (list of tuple of (str, int, int)) – List of tuples with (contig, start, end). Returns:; ReferenceGenome. property global_positions_dict; Get a dictionary mapping contig names to their global genomic positions. Returns:; dict – A dictionary of contig names to global genomic positions. has_liftover(dest_reference_genome)[source]; True if a liftover chain file is available from this reference; genome to the destination reference. Parameters:; dest_reference_genome (str or ReferenceGenome). Returns:; bool. has_sequence()[source]; True if the reference sequence has been loaded. Returns:; bool. property lengths; Dict of contig name to contig length. Returns:; dict of str to int. locus_from_global_position(global_pos)[source]; ”; Constructs a locus from a global position in reference genome.; The inverse of Locus.position().; Examples; >>> rg = hl.get_reference('GRCh37'); >>> rg.locus_from_global_position(0); Locus(contig=1, position=1, reference_genome=GRCh37). >>> rg.locus_from_global_position(2824183054); Locus(contig=21, position=42584230, reference_genome=GRCh37). >>> rg = hl.get_reference('GRCh38'); >>> rg.locus_from_global_position(2824183054); Locus(contig=chr22, position=1, reference_genome=GRCh38). Parameters:; global_pos (int) – Zero-based global base position along the reference genome. Returns:; Locus. property mt_contigs; Mitochondrial contigs. Returns:; list of str. property name; Name of reference genome. Returns:; str. property par; Pseudoautosomal regions. Returns:; list of Interval. classmethod read(p",MatchSource.WIKI,docs/0.2/genetics/hail.genetics.ReferenceGenome.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/genetics/hail.genetics.ReferenceGenome.html
https://hail.is/docs/0.2/genetics/hail.genetics.ReferenceGenome.html:1318,Security,access,access,1318,"lot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; genetics; ReferenceGenome. View page source. ReferenceGenome. class hail.genetics.ReferenceGenome[source]; An object that represents a reference genome.; Examples; >>> contigs = [""1"", ""X"", ""Y"", ""MT""]; >>> lengths = {""1"": 249250621, ""X"": 155270560, ""Y"": 59373566, ""MT"": 16569}; >>> par = [(""X"", 60001, 2699521)]; >>> my_ref = hl.ReferenceGenome(""my_ref"", contigs, lengths, ""X"", ""Y"", ""MT"", par). Notes; Hail comes with predefined reference genomes (case sensitive!):. GRCh37, Genome Reference Consortium Human Build 37; GRCh38, Genome Reference Consortium Human Build 38; GRCm38, Genome Reference Consortium Mouse Build 38; CanFam3, Canis lupus familiaris (dog). You can access these reference genome objects using get_reference():; >>> rg = hl.get_reference('GRCh37'); >>> rg = hl.get_reference('GRCh38'); >>> rg = hl.get_reference('GRCm38'); >>> rg = hl.get_reference('CanFam3'). Note that constructing a new reference genome, either by using the class; constructor or by using read will add the reference genome to the list of; known references; it is possible to access the reference genome using; get_reference() anytime afterwards. Note; Reference genome names must be unique. It is not possible to overwrite the; built-in reference genomes. Note; Hail allows setting a default reference so that the reference_genome; argument of import_vcf() does not need to be used; constantly. It is a current limitation of Hail that a custom reference; genome cannot be used as the default_reference argument of; init(). In order to set a custom reference genome as default,; pass the reference as an argument to default_reference() after; initializing Hail. Parameters:. name (str) – Name of reference. Must b",MatchSource.WIKI,docs/0.2/genetics/hail.genetics.ReferenceGenome.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/genetics/hail.genetics.ReferenceGenome.html
https://hail.is/docs/0.2/genetics/hail.genetics.ReferenceGenome.html:1714,Security,access,access,1714," ReferenceGenome. View page source. ReferenceGenome. class hail.genetics.ReferenceGenome[source]; An object that represents a reference genome.; Examples; >>> contigs = [""1"", ""X"", ""Y"", ""MT""]; >>> lengths = {""1"": 249250621, ""X"": 155270560, ""Y"": 59373566, ""MT"": 16569}; >>> par = [(""X"", 60001, 2699521)]; >>> my_ref = hl.ReferenceGenome(""my_ref"", contigs, lengths, ""X"", ""Y"", ""MT"", par). Notes; Hail comes with predefined reference genomes (case sensitive!):. GRCh37, Genome Reference Consortium Human Build 37; GRCh38, Genome Reference Consortium Human Build 38; GRCm38, Genome Reference Consortium Mouse Build 38; CanFam3, Canis lupus familiaris (dog). You can access these reference genome objects using get_reference():; >>> rg = hl.get_reference('GRCh37'); >>> rg = hl.get_reference('GRCh38'); >>> rg = hl.get_reference('GRCm38'); >>> rg = hl.get_reference('CanFam3'). Note that constructing a new reference genome, either by using the class; constructor or by using read will add the reference genome to the list of; known references; it is possible to access the reference genome using; get_reference() anytime afterwards. Note; Reference genome names must be unique. It is not possible to overwrite the; built-in reference genomes. Note; Hail allows setting a default reference so that the reference_genome; argument of import_vcf() does not need to be used; constantly. It is a current limitation of Hail that a custom reference; genome cannot be used as the default_reference argument of; init(). In order to set a custom reference genome as default,; pass the reference as an argument to default_reference() after; initializing Hail. Parameters:. name (str) – Name of reference. Must be unique and NOT one of Hail’s; predefined references: 'GRCh37', 'GRCh38', 'GRCm38',; 'CanFam3' and 'default'.; contigs (list of str) – Contig names.; lengths (dict of str to int) – Dict of contig names to contig lengths.; x_contigs (str or list of str) – Contigs to be treated as X chromosomes.; y_contig",MatchSource.WIKI,docs/0.2/genetics/hail.genetics.ReferenceGenome.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/genetics/hail.genetics.ReferenceGenome.html
https://hail.is/docs/0.2/genetics/hail.genetics.ReferenceGenome.html:4279,Testability,test,test,4279,"ence sequence from a FASTA file. contig_length; Contig length. from_fasta_file; Create reference genome from a FASTA file. has_liftover; True if a liftover chain file is available from this reference genome to the destination reference. has_sequence; True if the reference sequence has been loaded. locus_from_global_position; "". read; Load reference genome from a JSON file. remove_liftover; Remove liftover to dest_reference_genome. remove_sequence; Remove the reference sequence. write; ""Write this reference genome to a file in JSON format. add_liftover(chain_file, dest_reference_genome)[source]; Register a chain file for liftover.; Examples; Access GRCh37 and GRCh38 using get_reference():; >>> rg37 = hl.get_reference('GRCh37') ; >>> rg38 = hl.get_reference('GRCh38') . Add a chain file from 37 to 38:; >>> rg37.add_liftover('gs://hail-common/references/grch37_to_grch38.over.chain.gz', rg38) . Notes; This method can only be run once per reference genome. Use; has_liftover() to test whether a chain file has been registered.; The chain file format is described; here.; Chain files are hosted on google cloud for some of Hail’s built-in; references:; GRCh37 to GRCh38; gs://hail-common/references/grch37_to_grch38.over.chain.gz; GRCh38 to GRCh37; gs://hail-common/references/grch38_to_grch37.over.chain.gz; Public download links are available; here. Parameters:. chain_file (str) – Path to chain file. Can be compressed (GZIP) or uncompressed.; dest_reference_genome (str or ReferenceGenome) – Reference genome to convert to. add_sequence(fasta_file, index_file=None)[source]; Load the reference sequence from a FASTA file.; Examples; Access the GRCh37 reference genome using get_reference():; >>> rg = hl.get_reference('GRCh37') . Add a sequence file:; >>> rg.add_sequence('gs://hail-common/references/human_g1k_v37.fasta.gz',; ... 'gs://hail-common/references/human_g1k_v37.fasta.fai') . Add a sequence file with the default index location:; >>> rg.add_sequence('gs://hail-common/referen",MatchSource.WIKI,docs/0.2/genetics/hail.genetics.ReferenceGenome.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/genetics/hail.genetics.ReferenceGenome.html
https://hail.is/docs/0.2/genetics/hail.genetics.ReferenceGenome.html:5406,Testability,test,test,5406,"r some of Hail’s built-in; references:; GRCh37 to GRCh38; gs://hail-common/references/grch37_to_grch38.over.chain.gz; GRCh38 to GRCh37; gs://hail-common/references/grch38_to_grch37.over.chain.gz; Public download links are available; here. Parameters:. chain_file (str) – Path to chain file. Can be compressed (GZIP) or uncompressed.; dest_reference_genome (str or ReferenceGenome) – Reference genome to convert to. add_sequence(fasta_file, index_file=None)[source]; Load the reference sequence from a FASTA file.; Examples; Access the GRCh37 reference genome using get_reference():; >>> rg = hl.get_reference('GRCh37') . Add a sequence file:; >>> rg.add_sequence('gs://hail-common/references/human_g1k_v37.fasta.gz',; ... 'gs://hail-common/references/human_g1k_v37.fasta.fai') . Add a sequence file with the default index location:; >>> rg.add_sequence('gs://hail-common/references/human_g1k_v37.fasta.gz') . Notes; This method can only be run once per reference genome. Use; has_sequence() to test whether a sequence is loaded.; FASTA and index files are hosted on google cloud for some of Hail’s built-in; references:; GRCh37. FASTA file: gs://hail-common/references/human_g1k_v37.fasta.gz; Index file: gs://hail-common/references/human_g1k_v37.fasta.fai. GRCh38. FASTA file: gs://hail-common/references/Homo_sapiens_assembly38.fasta.gz; Index file: gs://hail-common/references/Homo_sapiens_assembly38.fasta.fai. Public download links are available; here. Parameters:. fasta_file (str) – Path to FASTA file. Can be compressed (GZIP) or uncompressed.; index_file (None or str) – Path to FASTA index file. Must be uncompressed. If None, replace; the fasta_file’s extension with fai. contig_length(contig)[source]; Contig length. Parameters:; contig (str) – Contig name. Returns:; int – Length of contig. property contigs; Contig names. Returns:; list of str. classmethod from_fasta_file(name, fasta_file, index_file, x_contigs=[], y_contigs=[], mt_contigs=[], par=[])[source]; Create reference ge",MatchSource.WIKI,docs/0.2/genetics/hail.genetics.ReferenceGenome.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/genetics/hail.genetics.ReferenceGenome.html
https://hail.is/docs/0.2/genetics/hail.genetics.Trio.html:2389,Deployability,update,updated,2389,"uration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; genetics; Trio. View page source. Trio. class hail.genetics.Trio[source]; Class containing information about nuclear family relatedness and sex. Parameters:. s (str) – Sample ID of proband.; fam_id (str or None) – Family ID.; pat_id (str or None) – Sample ID of father.; mat_id (str or None) – Sample ID of mother.; is_female (bool or None) – Sex of proband. Attributes. fam_id; Family ID. is_female; Returns True if the proband is a reported female, False if reported male, and None if no sex is defined. is_male; Returns True if the proband is a reported male, False if reported female, and None if no sex is defined. mat_id; ID of mother in trio, may be missing. pat_id; ID of father in trio, may be missing. s; ID of proband in trio, never missing. Methods. is_complete; Returns True if the trio has a defined mother and father. property fam_id; Family ID. Return type:; str or None. is_complete()[source]; Returns True if the trio has a defined mother and father.; The considered fields are mat_id() and pat_id().; Recall that s may never be missing. The fam_id(); and is_female() fields may be missing in a complete trio. Return type:; bool. property is_female; Returns True if the proband is a reported female,; False if reported male, and None if no sex is defined. Return type:; bool or None. property is_male; Returns True if the proband is a reported male,; False if reported female, and None if no sex is defined. Return type:; bool or None. property mat_id; ID of mother in trio, may be missing. Return type:; str or None. property pat_id; ID of father in trio, may be missing. Return type:; str or None. property s; ID of proband in trio, never missing. Return type:; str. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/genetics/hail.genetics.Trio.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/genetics/hail.genetics.Trio.html
https://hail.is/docs/0.2/genetics/index.html:1131,Deployability,update,updated,1131,"﻿. Hail | ; genetics. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; genetics. View page source. genetics. Classes. AlleleType; An enumeration for allele type. Call; An object that represents an individual's call at a genomic locus. Locus; An object that represents a location in the genome. Pedigree; Class containing a list of trios, with extra functionality. ReferenceGenome; An object that represents a reference genome. Trio; Class containing information about nuclear family relatedness and sex. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/genetics/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/genetics/index.html
https://hail.is/docs/0.2/ggplot/index.html:8491,Deployability,continuous,continuous,8491,", color=None)[source]; Creates a line plot with the area between the line and the x-axis filled in.; Supported aesthetics: x, y, fill, color, tooltip. Parameters:. mapping (Aesthetic) – Any aesthetics specific to this geom.; fill – Color of fill to draw, black by default. Overrides fill aesthetic.; color – Color of line to draw outlining non x-axis facing side, none by default. Overrides color aesthetic. Returns:; FigureAttribute – The geom to be applied. hail.ggplot.geom_ribbon(mapping={}, fill=None, color=None)[source]; Creates filled in area between two lines specified by x, ymin, and ymax; Supported aesthetics: x, ymin, ymax, color, fill, tooltip. Parameters:. mapping (Aesthetic) – Any aesthetics specific to this geom.; fill – Color of fill to draw, black by default. Overrides fill aesthetic.; color – Color of line to draw outlining both side, none by default. Overrides color aesthetic.; return:; :class:`FigureAttribute` – The geom to be applied. Scales. scale_x_continuous; The default continuous x scale. scale_x_discrete; The default discrete x scale. scale_x_genomic; The default genomic x scale. scale_x_log10; Transforms x axis to be log base 10 scaled. scale_x_reverse; Transforms x-axis to be vertically reversed. scale_y_continuous; The default continuous y scale. scale_y_discrete; The default discrete y scale. scale_y_log10; Transforms y-axis to be log base 10 scaled. scale_y_reverse; Transforms y-axis to be vertically reversed. scale_color_continuous; The default continuous color scale. scale_color_discrete; The default discrete color scale. scale_color_hue; Map discrete colors to evenly placed positions around the color wheel. scale_color_manual; A color scale that assigns strings to colors using the pool of colors specified as values. scale_color_identity; A color scale that assumes the expression specified in the color aesthetic can be used as a color. scale_fill_continuous; The default continuous fill scale. scale_fill_discrete; The default discrete f",MatchSource.WIKI,docs/0.2/ggplot/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/ggplot/index.html
https://hail.is/docs/0.2/ggplot/index.html:8758,Deployability,continuous,continuous,8758,"fault. Overrides fill aesthetic.; color – Color of line to draw outlining non x-axis facing side, none by default. Overrides color aesthetic. Returns:; FigureAttribute – The geom to be applied. hail.ggplot.geom_ribbon(mapping={}, fill=None, color=None)[source]; Creates filled in area between two lines specified by x, ymin, and ymax; Supported aesthetics: x, ymin, ymax, color, fill, tooltip. Parameters:. mapping (Aesthetic) – Any aesthetics specific to this geom.; fill – Color of fill to draw, black by default. Overrides fill aesthetic.; color – Color of line to draw outlining both side, none by default. Overrides color aesthetic.; return:; :class:`FigureAttribute` – The geom to be applied. Scales. scale_x_continuous; The default continuous x scale. scale_x_discrete; The default discrete x scale. scale_x_genomic; The default genomic x scale. scale_x_log10; Transforms x axis to be log base 10 scaled. scale_x_reverse; Transforms x-axis to be vertically reversed. scale_y_continuous; The default continuous y scale. scale_y_discrete; The default discrete y scale. scale_y_log10; Transforms y-axis to be log base 10 scaled. scale_y_reverse; Transforms y-axis to be vertically reversed. scale_color_continuous; The default continuous color scale. scale_color_discrete; The default discrete color scale. scale_color_hue; Map discrete colors to evenly placed positions around the color wheel. scale_color_manual; A color scale that assigns strings to colors using the pool of colors specified as values. scale_color_identity; A color scale that assumes the expression specified in the color aesthetic can be used as a color. scale_fill_continuous; The default continuous fill scale. scale_fill_discrete; The default discrete fill scale. scale_fill_hue; Map discrete fill colors to evenly placed positions around the color wheel. scale_fill_manual; A color scale that assigns strings to fill colors using the pool of colors specified as values. scale_fill_identity; A color scale that assumes t",MatchSource.WIKI,docs/0.2/ggplot/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/ggplot/index.html
https://hail.is/docs/0.2/ggplot/index.html:8983,Deployability,continuous,continuous,8983,"={}, fill=None, color=None)[source]; Creates filled in area between two lines specified by x, ymin, and ymax; Supported aesthetics: x, ymin, ymax, color, fill, tooltip. Parameters:. mapping (Aesthetic) – Any aesthetics specific to this geom.; fill – Color of fill to draw, black by default. Overrides fill aesthetic.; color – Color of line to draw outlining both side, none by default. Overrides color aesthetic.; return:; :class:`FigureAttribute` – The geom to be applied. Scales. scale_x_continuous; The default continuous x scale. scale_x_discrete; The default discrete x scale. scale_x_genomic; The default genomic x scale. scale_x_log10; Transforms x axis to be log base 10 scaled. scale_x_reverse; Transforms x-axis to be vertically reversed. scale_y_continuous; The default continuous y scale. scale_y_discrete; The default discrete y scale. scale_y_log10; Transforms y-axis to be log base 10 scaled. scale_y_reverse; Transforms y-axis to be vertically reversed. scale_color_continuous; The default continuous color scale. scale_color_discrete; The default discrete color scale. scale_color_hue; Map discrete colors to evenly placed positions around the color wheel. scale_color_manual; A color scale that assigns strings to colors using the pool of colors specified as values. scale_color_identity; A color scale that assumes the expression specified in the color aesthetic can be used as a color. scale_fill_continuous; The default continuous fill scale. scale_fill_discrete; The default discrete fill scale. scale_fill_hue; Map discrete fill colors to evenly placed positions around the color wheel. scale_fill_manual; A color scale that assigns strings to fill colors using the pool of colors specified as values. scale_fill_identity; A color scale that assumes the expression specified in the fill aesthetic can be used as a fill color. hail.ggplot.scale_x_continuous(name=None, breaks=None, labels=None, trans='identity')[source]; The default continuous x scale. Parameters:. name (str",MatchSource.WIKI,docs/0.2/ggplot/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/ggplot/index.html
https://hail.is/docs/0.2/ggplot/index.html:9418,Deployability,continuous,continuous,9418,"ureAttribute` – The geom to be applied. Scales. scale_x_continuous; The default continuous x scale. scale_x_discrete; The default discrete x scale. scale_x_genomic; The default genomic x scale. scale_x_log10; Transforms x axis to be log base 10 scaled. scale_x_reverse; Transforms x-axis to be vertically reversed. scale_y_continuous; The default continuous y scale. scale_y_discrete; The default discrete y scale. scale_y_log10; Transforms y-axis to be log base 10 scaled. scale_y_reverse; Transforms y-axis to be vertically reversed. scale_color_continuous; The default continuous color scale. scale_color_discrete; The default discrete color scale. scale_color_hue; Map discrete colors to evenly placed positions around the color wheel. scale_color_manual; A color scale that assigns strings to colors using the pool of colors specified as values. scale_color_identity; A color scale that assumes the expression specified in the color aesthetic can be used as a color. scale_fill_continuous; The default continuous fill scale. scale_fill_discrete; The default discrete fill scale. scale_fill_hue; Map discrete fill colors to evenly placed positions around the color wheel. scale_fill_manual; A color scale that assigns strings to fill colors using the pool of colors specified as values. scale_fill_identity; A color scale that assumes the expression specified in the fill aesthetic can be used as a fill color. hail.ggplot.scale_x_continuous(name=None, breaks=None, labels=None, trans='identity')[source]; The default continuous x scale. Parameters:. name (str) – The label to show on x-axis; breaks (list of float) – The locations to draw ticks on the x-axis.; labels (list of str) – The labels of the ticks on the axis.; trans (str) – The transformation to apply to the x-axis. Supports “identity”, “reverse”, “log10”. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_x_discrete(name=None, breaks=None, labels=None)[source]; The default discrete x scale. Parameters:. na",MatchSource.WIKI,docs/0.2/ggplot/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/ggplot/index.html
https://hail.is/docs/0.2/ggplot/index.html:9934,Deployability,continuous,continuous,9934,"verse; Transforms y-axis to be vertically reversed. scale_color_continuous; The default continuous color scale. scale_color_discrete; The default discrete color scale. scale_color_hue; Map discrete colors to evenly placed positions around the color wheel. scale_color_manual; A color scale that assigns strings to colors using the pool of colors specified as values. scale_color_identity; A color scale that assumes the expression specified in the color aesthetic can be used as a color. scale_fill_continuous; The default continuous fill scale. scale_fill_discrete; The default discrete fill scale. scale_fill_hue; Map discrete fill colors to evenly placed positions around the color wheel. scale_fill_manual; A color scale that assigns strings to fill colors using the pool of colors specified as values. scale_fill_identity; A color scale that assumes the expression specified in the fill aesthetic can be used as a fill color. hail.ggplot.scale_x_continuous(name=None, breaks=None, labels=None, trans='identity')[source]; The default continuous x scale. Parameters:. name (str) – The label to show on x-axis; breaks (list of float) – The locations to draw ticks on the x-axis.; labels (list of str) – The labels of the ticks on the axis.; trans (str) – The transformation to apply to the x-axis. Supports “identity”, “reverse”, “log10”. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_x_discrete(name=None, breaks=None, labels=None)[source]; The default discrete x scale. Parameters:. name (str) – The label to show on x-axis; breaks (list of str) – The locations to draw ticks on the x-axis.; labels (list of str) – The labels of the ticks on the axis. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_x_genomic(reference_genome, name=None)[source]; The default genomic x scale. This is used when the x aesthetic corresponds to a LocusExpression. Parameters:. reference_genome – The reference genome being used.; name (str) – The label to show on y",MatchSource.WIKI,docs/0.2/ggplot/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/ggplot/index.html
https://hail.is/docs/0.2/ggplot/index.html:11465,Deployability,continuous,continuous,11465,"label to show on x-axis; breaks (list of str) – The locations to draw ticks on the x-axis.; labels (list of str) – The labels of the ticks on the axis. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_x_genomic(reference_genome, name=None)[source]; The default genomic x scale. This is used when the x aesthetic corresponds to a LocusExpression. Parameters:. reference_genome – The reference genome being used.; name (str) – The label to show on y-axis. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_x_log10(name=None)[source]; Transforms x axis to be log base 10 scaled. Parameters:; name (str) – The label to show on x-axis. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_x_reverse(name=None)[source]; Transforms x-axis to be vertically reversed. Parameters:; name (str) – The label to show on x-axis. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_y_continuous(name=None, breaks=None, labels=None, trans='identity')[source]; The default continuous y scale. Parameters:. name (str) – The label to show on y-axis; breaks (list of float) – The locations to draw ticks on the y-axis.; labels (list of str) – The labels of the ticks on the axis.; trans (str) – The transformation to apply to the y-axis. Supports “identity”, “reverse”, “log10”. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_y_discrete(name=None, breaks=None, labels=None)[source]; The default discrete y scale. Parameters:. name (str) – The label to show on y-axis; breaks (list of str) – The locations to draw ticks on the y-axis.; labels (list of str) – The labels of the ticks on the axis. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_y_log10(name=None)[source]; Transforms y-axis to be log base 10 scaled. Parameters:; name (str) – The label to show on y-axis. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_y_reverse(name=None)[source]; Transforms y-axis",MatchSource.WIKI,docs/0.2/ggplot/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/ggplot/index.html
https://hail.is/docs/0.2/ggplot/index.html:12622,Deployability,continuous,continuous,12622,"abels (list of str) – The labels of the ticks on the axis.; trans (str) – The transformation to apply to the y-axis. Supports “identity”, “reverse”, “log10”. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_y_discrete(name=None, breaks=None, labels=None)[source]; The default discrete y scale. Parameters:. name (str) – The label to show on y-axis; breaks (list of str) – The locations to draw ticks on the y-axis.; labels (list of str) – The labels of the ticks on the axis. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_y_log10(name=None)[source]; Transforms y-axis to be log base 10 scaled. Parameters:; name (str) – The label to show on y-axis. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_y_reverse(name=None)[source]; Transforms y-axis to be vertically reversed. Parameters:; name (str) – The label to show on y-axis. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_color_continuous()[source]; The default continuous color scale. This linearly interpolates colors between the min and max observed values. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_color_discrete()[source]; The default discrete color scale. This maps each discrete value to a color. Equivalent to scale_color_hue. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_color_hue()[source]; Map discrete colors to evenly placed positions around the color wheel. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_color_manual(*, values)[source]; A color scale that assigns strings to colors using the pool of colors specified as values. Parameters:; values (list of str) – The colors to choose when assigning values to colors. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_color_identity()[source]; A color scale that assumes the expression specified in the color aesthetic can be used as a color. Returns:; FigureAttribute – The scale",MatchSource.WIKI,docs/0.2/ggplot/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/ggplot/index.html
https://hail.is/docs/0.2/ggplot/index.html:13684,Deployability,continuous,continuous,13684," colors between the min and max observed values. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_color_discrete()[source]; The default discrete color scale. This maps each discrete value to a color. Equivalent to scale_color_hue. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_color_hue()[source]; Map discrete colors to evenly placed positions around the color wheel. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_color_manual(*, values)[source]; A color scale that assigns strings to colors using the pool of colors specified as values. Parameters:; values (list of str) – The colors to choose when assigning values to colors. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_color_identity()[source]; A color scale that assumes the expression specified in the color aesthetic can be used as a color. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_fill_continuous()[source]; The default continuous fill scale. This linearly interpolates colors between the min and max observed values. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_fill_discrete()[source]; The default discrete fill scale. This maps each discrete value to a fill color. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_fill_hue()[source]; Map discrete fill colors to evenly placed positions around the color wheel. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_fill_manual(*, values)[source]; A color scale that assigns strings to fill colors using the pool of colors specified as values. Parameters:; values (list of str) – The colors to choose when assigning values to colors. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_fill_identity()[source]; A color scale that assumes the expression specified in the fill aesthetic can be used as a fill color. Returns:; FigureAttribute – The scale to be applied. Fa",MatchSource.WIKI,docs/0.2/ggplot/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/ggplot/index.html
https://hail.is/docs/0.2/ggplot/index.html:16683,Deployability,update,updated,16683,"ncol is set.; ncol (int) – The number of columns into which the facets will be spread.; scales (str) – Whether the scales are the same across facets. For more information and a list of supported options, see the ggplot documentation. Returns:; FigureAttribute – The faceter. hail.ggplot.vars(*args)[source]. Parameters:; *args (hail.expr.Expression) – Fields to facet by. Returns:; hail.expr.StructExpression – A struct to pass to a faceter. Labels. xlab(label); Sets the x-axis label of a plot. ylab(label); Sets the y-axis label of a plot. ggtitle(label); Sets the title of a plot. hail.ggplot.xlab(label)[source]; Sets the x-axis label of a plot. Parameters:; label (str) – The desired x-axis label of the plot. Returns:; FigureAttribute – Label object to change the x-axis label. hail.ggplot.ylab(label)[source]; Sets the y-axis label of a plot. Parameters:; label (str) – The desired y-axis label of the plot. Returns:; FigureAttribute – Label object to change the y-axis label. hail.ggplot.ggtitle(label)[source]; Sets the title of a plot. Parameters:; label (str) – The desired title of the plot. Returns:; FigureAttribute – Label object to change the title. Classes. class hail.ggplot.GGPlot(ht, aes, geoms=[], labels=<hail.ggplot.labels.Labels object>, coord_cartesian=None, scales=None, facet=None)[source]; The class representing a figure created using the hail.ggplot module.; Create one by using ggplot(). to_plotly()[source]; Turn the hail plot into a Plotly plot. Returns:; A Plotly figure that can be updated with plotly methods. show()[source]; Render and show the plot, either in a browser or notebook. write_image(path)[source]; Write out this plot as an image.; This requires you to have installed the python package kaleido from pypi. Parameters:; path (str) – The path to write the file to. class hail.ggplot.Aesthetic(properties)[source]. class hail.ggplot.FigureAttribute[source]. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/ggplot/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/ggplot/index.html
https://hail.is/docs/0.2/ggplot/index.html:16876,Deployability,install,installed,16876,"ncol is set.; ncol (int) – The number of columns into which the facets will be spread.; scales (str) – Whether the scales are the same across facets. For more information and a list of supported options, see the ggplot documentation. Returns:; FigureAttribute – The faceter. hail.ggplot.vars(*args)[source]. Parameters:; *args (hail.expr.Expression) – Fields to facet by. Returns:; hail.expr.StructExpression – A struct to pass to a faceter. Labels. xlab(label); Sets the x-axis label of a plot. ylab(label); Sets the y-axis label of a plot. ggtitle(label); Sets the title of a plot. hail.ggplot.xlab(label)[source]; Sets the x-axis label of a plot. Parameters:; label (str) – The desired x-axis label of the plot. Returns:; FigureAttribute – Label object to change the x-axis label. hail.ggplot.ylab(label)[source]; Sets the y-axis label of a plot. Parameters:; label (str) – The desired y-axis label of the plot. Returns:; FigureAttribute – Label object to change the y-axis label. hail.ggplot.ggtitle(label)[source]; Sets the title of a plot. Parameters:; label (str) – The desired title of the plot. Returns:; FigureAttribute – Label object to change the title. Classes. class hail.ggplot.GGPlot(ht, aes, geoms=[], labels=<hail.ggplot.labels.Labels object>, coord_cartesian=None, scales=None, facet=None)[source]; The class representing a figure created using the hail.ggplot module.; Create one by using ggplot(). to_plotly()[source]; Turn the hail plot into a Plotly plot. Returns:; A Plotly figure that can be updated with plotly methods. show()[source]; Render and show the plot, either in a browser or notebook. write_image(path)[source]; Write out this plot as an image.; This requires you to have installed the python package kaleido from pypi. Parameters:; path (str) – The path to write the file to. class hail.ggplot.Aesthetic(properties)[source]. class hail.ggplot.FigureAttribute[source]. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/ggplot/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/ggplot/index.html
https://hail.is/docs/0.2/ggplot/index.html:17132,Deployability,update,updated,17132,"ncol is set.; ncol (int) – The number of columns into which the facets will be spread.; scales (str) – Whether the scales are the same across facets. For more information and a list of supported options, see the ggplot documentation. Returns:; FigureAttribute – The faceter. hail.ggplot.vars(*args)[source]. Parameters:; *args (hail.expr.Expression) – Fields to facet by. Returns:; hail.expr.StructExpression – A struct to pass to a faceter. Labels. xlab(label); Sets the x-axis label of a plot. ylab(label); Sets the y-axis label of a plot. ggtitle(label); Sets the title of a plot. hail.ggplot.xlab(label)[source]; Sets the x-axis label of a plot. Parameters:; label (str) – The desired x-axis label of the plot. Returns:; FigureAttribute – Label object to change the x-axis label. hail.ggplot.ylab(label)[source]; Sets the y-axis label of a plot. Parameters:; label (str) – The desired y-axis label of the plot. Returns:; FigureAttribute – Label object to change the y-axis label. hail.ggplot.ggtitle(label)[source]; Sets the title of a plot. Parameters:; label (str) – The desired title of the plot. Returns:; FigureAttribute – Label object to change the title. Classes. class hail.ggplot.GGPlot(ht, aes, geoms=[], labels=<hail.ggplot.labels.Labels object>, coord_cartesian=None, scales=None, facet=None)[source]; The class representing a figure created using the hail.ggplot module.; Create one by using ggplot(). to_plotly()[source]; Turn the hail plot into a Plotly plot. Returns:; A Plotly figure that can be updated with plotly methods. show()[source]; Render and show the plot, either in a browser or notebook. write_image(path)[source]; Write out this plot as an image.; This requires you to have installed the python package kaleido from pypi. Parameters:; path (str) – The path to write the file to. class hail.ggplot.Aesthetic(properties)[source]. class hail.ggplot.FigureAttribute[source]. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/ggplot/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/ggplot/index.html
https://hail.is/docs/0.2/ggplot/index.html:1518,Integrability,interface,interface,1518,"ation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Plotting With hail.ggplot Overview. View page source. Plotting With hail.ggplot Overview. Warning; Plotting functionality is in early stages and is experimental. The hl.ggplot module is designed based on R’s tidyverse ggplot2 library. This module provides a subset of ggplot2’s; functionality to allow users to generate plots in much the same way they would in ggplot2.; This module is intended to be a new, more flexible way of plotting compared to the hl.plot module. This module; currently uses plotly to generate plots, as opposed to hl.plot, which uses bokeh. Core functions. ggplot; Create the initial plot object. aes; Create an aesthetic mapping. coord_cartesian; Set the boundaries of the plot. hail.ggplot.ggplot(table, mapping={})[source]; Create the initial plot object.; This function is the beginning of all plots using the hail.ggplot interface. Plots are constructed; by calling this function, then adding attributes to the plot to get the desired result.; Examples; Create a y = x^2 scatter plot; >>> ht = hl.utils.range_table(10); >>> ht = ht.annotate(squared = ht.idx**2); >>> my_plot = hl.ggplot.ggplot(ht, hl.ggplot.aes(x=ht.idx, y=ht.squared)) + hl.ggplot.geom_point(). Parameters:. table – The table containing the data to plot.; mapping – Default list of aesthetic mappings from table data to plot attributes. Returns:; GGPlot. hail.ggplot.aes(**kwargs)[source]; Create an aesthetic mapping. Parameters:; kwargs – Map aesthetic names to hail expressions based on table’s plot. Returns:; Aesthetic – The aesthetic mapping to be applied. hail.ggplot.coord_cartesian(xlim=None, ylim=None)[source]; Set the boundaries of the plot. Parameters:. xlim (tuple with two int) – The minimum and maximum x value to show on the plot.; ylim (tuple with two int) – The minimum and maximum y value to show on the plot. Returns:; FigureAttribu",MatchSource.WIKI,docs/0.2/ggplot/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/ggplot/index.html
https://hail.is/docs/0.2/ggplot/index.html:4731,Integrability,interface,interface,4731,"color=None, size=None, alpha=None)[source]; Create a scatter plot where each point is text from the text aesthetic.; Supported aesthetics: x, y, label, color, tooltip. Returns:; FigureAttribute – The geom to be applied. hail.ggplot.geom_bar(mapping={}, *, fill=None, color=None, alpha=None, position='stack', size=None)[source]; Create a bar chart that counts occurrences of the various values of the x aesthetic.; Supported aesthetics: x, color, fill, weight. Returns:; FigureAttribute – The geom to be applied. hail.ggplot.geom_col(mapping={}, *, fill=None, color=None, alpha=None, position='stack', size=None)[source]; Create a bar chart that uses bar heights specified in y aesthetic.; Supported aesthetics: x, y, color, fill. Returns:; FigureAttribute – The geom to be applied. hail.ggplot.geom_histogram(mapping={}, *, min_val=None, max_val=None, bins=None, fill=None, color=None, alpha=None, position='stack', size=None)[source]; Creates a histogram.; Note: this function currently does not support same interface as R’s ggplot.; Supported aesthetics: x, color, fill. Parameters:. mapping (Aesthetic) – Any aesthetics specific to this geom.; min_val (int or float) – Minimum value to include in histogram; max_val (int or float) – Maximum value to include in histogram; bins (int) – Number of bins to plot. 30 by default.; fill – A single fill color for all bars of histogram, overrides fill aesthetic.; color – A single outline color for all bars of histogram, overrides color aesthetic.; alpha (float) – A measure of transparency between 0 and 1.; position (str) – Tells how to deal with different groups of data at same point. Options are “stack” and “dodge”. Returns:; FigureAttribute – The geom to be applied. hail.ggplot.geom_density(mapping={}, *, k=1000, smoothing=0.5, fill=None, color=None, alpha=None, smoothed=False)[source]; Creates a smoothed density plot.; This method uses the hl.agg.approx_cdf aggregator to compute a sketch; of the distribution of the values of x. It th",MatchSource.WIKI,docs/0.2/ggplot/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/ggplot/index.html
https://hail.is/docs/0.2/ggplot/index.html:5848,Integrability,interface,interface,5848," aesthetics specific to this geom.; min_val (int or float) – Minimum value to include in histogram; max_val (int or float) – Maximum value to include in histogram; bins (int) – Number of bins to plot. 30 by default.; fill – A single fill color for all bars of histogram, overrides fill aesthetic.; color – A single outline color for all bars of histogram, overrides color aesthetic.; alpha (float) – A measure of transparency between 0 and 1.; position (str) – Tells how to deal with different groups of data at same point. Options are “stack” and “dodge”. Returns:; FigureAttribute – The geom to be applied. hail.ggplot.geom_density(mapping={}, *, k=1000, smoothing=0.5, fill=None, color=None, alpha=None, smoothed=False)[source]; Creates a smoothed density plot.; This method uses the hl.agg.approx_cdf aggregator to compute a sketch; of the distribution of the values of x. It then uses an ad hoc method to; estimate a smoothed pdf consistent with that cdf.; Note: this function currently does not support same interface as R’s ggplot.; Supported aesthetics: x, color, fill. Parameters:. mapping (Aesthetic) – Any aesthetics specific to this geom.; k (int) – Passed to the approx_cdf aggregator. The size of the aggregator scales; linearly with k. The default value of 1000 is likely sufficient for; most uses.; smoothing (float) – Controls the amount of smoothing applied.; fill – A single fill color for all density plots, overrides fill aesthetic.; color – A single line color for all density plots, overrides color aesthetic.; alpha (float) – A measure of transparency between 0 and 1.; smoothed (boolean) – If true, attempts to fit a smooth kernel density estimator.; If false, uses a custom method do generate a variable width histogram; directly from the approx_cdf results. Returns:; FigureAttribute – The geom to be applied. hail.ggplot.geom_hline(yintercept, *, linetype='solid', color=None)[source]; Plots a horizontal line at yintercept. Parameters:. yintercept (float) – Location to",MatchSource.WIKI,docs/0.2/ggplot/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/ggplot/index.html
https://hail.is/docs/0.2/ggplot/index.html:1080,Modifiability,flexible,flexible,1080," Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Plotting With hail.ggplot Overview. View page source. Plotting With hail.ggplot Overview. Warning; Plotting functionality is in early stages and is experimental. The hl.ggplot module is designed based on R’s tidyverse ggplot2 library. This module provides a subset of ggplot2’s; functionality to allow users to generate plots in much the same way they would in ggplot2.; This module is intended to be a new, more flexible way of plotting compared to the hl.plot module. This module; currently uses plotly to generate plots, as opposed to hl.plot, which uses bokeh. Core functions. ggplot; Create the initial plot object. aes; Create an aesthetic mapping. coord_cartesian; Set the boundaries of the plot. hail.ggplot.ggplot(table, mapping={})[source]; Create the initial plot object.; This function is the beginning of all plots using the hail.ggplot interface. Plots are constructed; by calling this function, then adding attributes to the plot to get the desired result.; Examples; Create a y = x^2 scatter plot; >>> ht = hl.utils.range_table(10); >>> ht = ht.annotate(squared = ht.idx**2); >>> my_plot = hl.ggplot.ggplot(ht, hl.ggplot.aes(x=ht.idx, y=ht.squared)) + hl.ggplot.geom_point(). Parameters:. table – The table containing the data to plot.; mapping – Default list of aesthetic mappings from table data to plot attributes. Returns:; GGPlot. hail.ggplot.aes(**kwargs)[source]; Create an aesthetic mapp",MatchSource.WIKI,docs/0.2/ggplot/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/ggplot/index.html
https://hail.is/docs/0.2/ggplot/index.html:6555,Modifiability,variab,variable,6555,"; Creates a smoothed density plot.; This method uses the hl.agg.approx_cdf aggregator to compute a sketch; of the distribution of the values of x. It then uses an ad hoc method to; estimate a smoothed pdf consistent with that cdf.; Note: this function currently does not support same interface as R’s ggplot.; Supported aesthetics: x, color, fill. Parameters:. mapping (Aesthetic) – Any aesthetics specific to this geom.; k (int) – Passed to the approx_cdf aggregator. The size of the aggregator scales; linearly with k. The default value of 1000 is likely sufficient for; most uses.; smoothing (float) – Controls the amount of smoothing applied.; fill – A single fill color for all density plots, overrides fill aesthetic.; color – A single line color for all density plots, overrides color aesthetic.; alpha (float) – A measure of transparency between 0 and 1.; smoothed (boolean) – If true, attempts to fit a smooth kernel density estimator.; If false, uses a custom method do generate a variable width histogram; directly from the approx_cdf results. Returns:; FigureAttribute – The geom to be applied. hail.ggplot.geom_hline(yintercept, *, linetype='solid', color=None)[source]; Plots a horizontal line at yintercept. Parameters:. yintercept (float) – Location to draw line.; linetype (str) – Type of line to draw. Choose from “solid”, “dashed”, “dotted”, “longdash”, “dotdash”.; color (str) – Color of line to draw, black by default. Returns:; FigureAttribute – The geom to be applied. hail.ggplot.geom_vline(xintercept, *, linetype='solid', color=None)[source]; Plots a vertical line at xintercept. Parameters:. xintercept (float) – Location to draw line.; linetype (str) – Type of line to draw. Choose from “solid”, “dashed”, “dotted”, “longdash”, “dotdash”.; color (str) – Color of line to draw, black by default. Returns:; FigureAttribute – The geom to be applied. hail.ggplot.geom_area(mapping={}, fill=None, color=None)[source]; Creates a line plot with the area between the line and ",MatchSource.WIKI,docs/0.2/ggplot/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/ggplot/index.html
https://hail.is/docs/0.2/ggplot/index.html:8644,Testability,log,log,8644,"p. Parameters:. mapping (Aesthetic) – Any aesthetics specific to this geom.; fill – Color of fill to draw, black by default. Overrides fill aesthetic.; color – Color of line to draw outlining non x-axis facing side, none by default. Overrides color aesthetic. Returns:; FigureAttribute – The geom to be applied. hail.ggplot.geom_ribbon(mapping={}, fill=None, color=None)[source]; Creates filled in area between two lines specified by x, ymin, and ymax; Supported aesthetics: x, ymin, ymax, color, fill, tooltip. Parameters:. mapping (Aesthetic) – Any aesthetics specific to this geom.; fill – Color of fill to draw, black by default. Overrides fill aesthetic.; color – Color of line to draw outlining both side, none by default. Overrides color aesthetic.; return:; :class:`FigureAttribute` – The geom to be applied. Scales. scale_x_continuous; The default continuous x scale. scale_x_discrete; The default discrete x scale. scale_x_genomic; The default genomic x scale. scale_x_log10; Transforms x axis to be log base 10 scaled. scale_x_reverse; Transforms x-axis to be vertically reversed. scale_y_continuous; The default continuous y scale. scale_y_discrete; The default discrete y scale. scale_y_log10; Transforms y-axis to be log base 10 scaled. scale_y_reverse; Transforms y-axis to be vertically reversed. scale_color_continuous; The default continuous color scale. scale_color_discrete; The default discrete color scale. scale_color_hue; Map discrete colors to evenly placed positions around the color wheel. scale_color_manual; A color scale that assigns strings to colors using the pool of colors specified as values. scale_color_identity; A color scale that assumes the expression specified in the color aesthetic can be used as a color. scale_fill_continuous; The default continuous fill scale. scale_fill_discrete; The default discrete fill scale. scale_fill_hue; Map discrete fill colors to evenly placed positions around the color wheel. scale_fill_manual; A color scale that assigns s",MatchSource.WIKI,docs/0.2/ggplot/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/ggplot/index.html
https://hail.is/docs/0.2/ggplot/index.html:8865,Testability,log,log,8865,"by default. Overrides color aesthetic. Returns:; FigureAttribute – The geom to be applied. hail.ggplot.geom_ribbon(mapping={}, fill=None, color=None)[source]; Creates filled in area between two lines specified by x, ymin, and ymax; Supported aesthetics: x, ymin, ymax, color, fill, tooltip. Parameters:. mapping (Aesthetic) – Any aesthetics specific to this geom.; fill – Color of fill to draw, black by default. Overrides fill aesthetic.; color – Color of line to draw outlining both side, none by default. Overrides color aesthetic.; return:; :class:`FigureAttribute` – The geom to be applied. Scales. scale_x_continuous; The default continuous x scale. scale_x_discrete; The default discrete x scale. scale_x_genomic; The default genomic x scale. scale_x_log10; Transforms x axis to be log base 10 scaled. scale_x_reverse; Transforms x-axis to be vertically reversed. scale_y_continuous; The default continuous y scale. scale_y_discrete; The default discrete y scale. scale_y_log10; Transforms y-axis to be log base 10 scaled. scale_y_reverse; Transforms y-axis to be vertically reversed. scale_color_continuous; The default continuous color scale. scale_color_discrete; The default discrete color scale. scale_color_hue; Map discrete colors to evenly placed positions around the color wheel. scale_color_manual; A color scale that assigns strings to colors using the pool of colors specified as values. scale_color_identity; A color scale that assumes the expression specified in the color aesthetic can be used as a color. scale_fill_continuous; The default continuous fill scale. scale_fill_discrete; The default discrete fill scale. scale_fill_hue; Map discrete fill colors to evenly placed positions around the color wheel. scale_fill_manual; A color scale that assigns strings to fill colors using the pool of colors specified as values. scale_fill_identity; A color scale that assumes the expression specified in the fill aesthetic can be used as a fill color. hail.ggplot.scale_x_continuou",MatchSource.WIKI,docs/0.2/ggplot/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/ggplot/index.html
https://hail.is/docs/0.2/ggplot/index.html:11027,Testability,log,log,11027,"; breaks (list of float) – The locations to draw ticks on the x-axis.; labels (list of str) – The labels of the ticks on the axis.; trans (str) – The transformation to apply to the x-axis. Supports “identity”, “reverse”, “log10”. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_x_discrete(name=None, breaks=None, labels=None)[source]; The default discrete x scale. Parameters:. name (str) – The label to show on x-axis; breaks (list of str) – The locations to draw ticks on the x-axis.; labels (list of str) – The labels of the ticks on the axis. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_x_genomic(reference_genome, name=None)[source]; The default genomic x scale. This is used when the x aesthetic corresponds to a LocusExpression. Parameters:. reference_genome – The reference genome being used.; name (str) – The label to show on y-axis. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_x_log10(name=None)[source]; Transforms x axis to be log base 10 scaled. Parameters:; name (str) – The label to show on x-axis. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_x_reverse(name=None)[source]; Transforms x-axis to be vertically reversed. Parameters:; name (str) – The label to show on x-axis. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_y_continuous(name=None, breaks=None, labels=None, trans='identity')[source]; The default continuous y scale. Parameters:. name (str) – The label to show on y-axis; breaks (list of float) – The locations to draw ticks on the y-axis.; labels (list of str) – The labels of the ticks on the axis.; trans (str) – The transformation to apply to the y-axis. Supports “identity”, “reverse”, “log10”. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_y_discrete(name=None, breaks=None, labels=None)[source]; The default discrete y scale. Parameters:. name (str) – The label to show on y-axis; breaks (list of str) – Th",MatchSource.WIKI,docs/0.2/ggplot/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/ggplot/index.html
https://hail.is/docs/0.2/ggplot/index.html:12233,Testability,log,log,12233,"s x-axis to be vertically reversed. Parameters:; name (str) – The label to show on x-axis. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_y_continuous(name=None, breaks=None, labels=None, trans='identity')[source]; The default continuous y scale. Parameters:. name (str) – The label to show on y-axis; breaks (list of float) – The locations to draw ticks on the y-axis.; labels (list of str) – The labels of the ticks on the axis.; trans (str) – The transformation to apply to the y-axis. Supports “identity”, “reverse”, “log10”. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_y_discrete(name=None, breaks=None, labels=None)[source]; The default discrete y scale. Parameters:. name (str) – The label to show on y-axis; breaks (list of str) – The locations to draw ticks on the y-axis.; labels (list of str) – The labels of the ticks on the axis. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_y_log10(name=None)[source]; Transforms y-axis to be log base 10 scaled. Parameters:; name (str) – The label to show on y-axis. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_y_reverse(name=None)[source]; Transforms y-axis to be vertically reversed. Parameters:; name (str) – The label to show on y-axis. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_color_continuous()[source]; The default continuous color scale. This linearly interpolates colors between the min and max observed values. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_color_discrete()[source]; The default discrete color scale. This maps each discrete value to a color. Equivalent to scale_color_hue. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_color_hue()[source]; Map discrete colors to evenly placed positions around the color wheel. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_color_manual(*, values)[source]; A color scale that",MatchSource.WIKI,docs/0.2/ggplot/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/ggplot/index.html
https://hail.is/docs/0.2/guides/agg.html:7072,Deployability,update,updated,7072,"p. description:; Group the columns of the matrix table by the column-indexed; field cohort and compute the call rate per cohort. code:; >>> result_mt = (mt.group_cols_by(mt.cohort); ... .aggregate(call_rate=hl.agg.fraction(hl.is_defined(mt.GT)))). dependencies:; MatrixTable.group_cols_by(), GroupedMatrixTable, GroupedMatrixTable.aggregate(). understanding:. Group the columns of the matrix table by; the column-indexed field cohort using MatrixTable.group_cols_by(),; which returns a GroupedMatrixTable. Then use; GroupedMatrixTable.aggregate() to compute an aggregation per column; group.; The result is a matrix table with an entry field call_rate that contains; the result of the aggregation. The new matrix table has a row schema equal; to the original row schema, a column schema equal to the fields passed to; group_cols_by, and an entry schema determined by the expression passed to; aggregate. Other column fields and entry fields are dropped. Aggregate Per Row Group. description:; Compute the number of calls with one or more non-reference; alleles per gene group. code:; >>> result_mt = (mt.group_rows_by(mt.gene); ... .aggregate(n_non_ref=hl.agg.count_where(mt.GT.is_non_ref()))). dependencies:; MatrixTable.group_rows_by(), GroupedMatrixTable, GroupedMatrixTable.aggregate(). understanding:. Group the rows of the matrix table by the row-indexed field gene; using MatrixTable.group_rows_by(), which returns a; GroupedMatrixTable. Then use GroupedMatrixTable.aggregate(); to compute an aggregation per grouped row.; The result is a matrix table with an entry field n_non_ref that contains; the result of the aggregation. This new matrix table has a row schema; equal to the fields passed to group_rows_by, a column schema equal to the; column schema of the original matrix table, and an entry schema determined; by the expression passed to aggregate. Other row fields and entry fields; are dropped. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/guides/agg.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/guides/agg.html
https://hail.is/docs/0.2/guides/agg.html:1188,Integrability,depend,dependencies,1188,"ence (Python API); Configuration Reference; Overview; How-To Guides; Aggregation; Table Aggregations; Aggregate Over Rows Into A Local Value; Aggregate Per Group. Matrix Table Aggregations; Aggregate Entries Per Row (Over Columns); Aggregate Entries Per Column (Over Rows); Aggregate Column Values Into a Local Value; Aggregate Row Values Into a Local Value; Aggregate Entry Values Into A Local Value; Aggregate Per Column Group; Aggregate Per Row Group. Annotation (Adding Fields); Genetics. Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. How-To Guides; Aggregation. View page source. Aggregation; For a full list of aggregators, see the aggregators; section of the API reference. Table Aggregations. Aggregate Over Rows Into A Local Value. One aggregation. description:; Compute the fraction of rows where SEX == 'M' in a table. code:; >>> ht.aggregate(hl.agg.fraction(ht.SEX == 'M')); 0.5. dependencies:; Table.aggregate(), aggregators.fraction(). Multiple aggregations. description:; Compute two aggregation statistics, the fraction of rows where; SEX == 'M' and the mean value of X, from the rows of a table. code:; >>> ht.aggregate(hl.struct(fraction_male = hl.agg.fraction(ht.SEX == 'M'),; ... mean_x = hl.agg.mean(ht.X))); Struct(fraction_male=0.5, mean_x=6.5). dependencies:; Table.aggregate(), aggregators.fraction(), aggregators.mean(), StructExpression. Aggregate Per Group. description:; Group the table ht by ID and compute the mean value of X per group. code:; >>> result_ht = ht.group_by(ht.ID).aggregate(mean_x=hl.agg.mean(ht.X)). dependencies:; Table.group_by(), GroupedTable.aggregate(), aggregators.mean(). Matrix Table Aggregations. Aggregate Entries Per Row (Over Columns). description:; Count the number of occurrences of each unique GT field per row, i.e.; aggregate over the columns of the matrix table.; Methods MatrixTable.filter_rows(), MatrixTable.select_rows(),; and Ma",MatchSource.WIKI,docs/0.2/guides/agg.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/guides/agg.html
https://hail.is/docs/0.2/guides/agg.html:1566,Integrability,depend,dependencies,1566,"ues Into A Local Value; Aggregate Per Column Group; Aggregate Per Row Group. Annotation (Adding Fields); Genetics. Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. How-To Guides; Aggregation. View page source. Aggregation; For a full list of aggregators, see the aggregators; section of the API reference. Table Aggregations. Aggregate Over Rows Into A Local Value. One aggregation. description:; Compute the fraction of rows where SEX == 'M' in a table. code:; >>> ht.aggregate(hl.agg.fraction(ht.SEX == 'M')); 0.5. dependencies:; Table.aggregate(), aggregators.fraction(). Multiple aggregations. description:; Compute two aggregation statistics, the fraction of rows where; SEX == 'M' and the mean value of X, from the rows of a table. code:; >>> ht.aggregate(hl.struct(fraction_male = hl.agg.fraction(ht.SEX == 'M'),; ... mean_x = hl.agg.mean(ht.X))); Struct(fraction_male=0.5, mean_x=6.5). dependencies:; Table.aggregate(), aggregators.fraction(), aggregators.mean(), StructExpression. Aggregate Per Group. description:; Group the table ht by ID and compute the mean value of X per group. code:; >>> result_ht = ht.group_by(ht.ID).aggregate(mean_x=hl.agg.mean(ht.X)). dependencies:; Table.group_by(), GroupedTable.aggregate(), aggregators.mean(). Matrix Table Aggregations. Aggregate Entries Per Row (Over Columns). description:; Count the number of occurrences of each unique GT field per row, i.e.; aggregate over the columns of the matrix table.; Methods MatrixTable.filter_rows(), MatrixTable.select_rows(),; and MatrixTable.transmute_rows() also support aggregation over columns. code:; >>> result_mt = mt.annotate_rows(gt_counter=hl.agg.counter(mt.GT)). dependencies:; MatrixTable.annotate_rows(), aggregators.counter(). Aggregate Entries Per Column (Over Rows). description:; Compute the mean of the GQ field per column, i.e. aggregate over the rows; of the MatrixTable.; Methods MatrixTab",MatchSource.WIKI,docs/0.2/guides/agg.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/guides/agg.html
https://hail.is/docs/0.2/guides/agg.html:1845,Integrability,depend,dependencies,1845,"on. View page source. Aggregation; For a full list of aggregators, see the aggregators; section of the API reference. Table Aggregations. Aggregate Over Rows Into A Local Value. One aggregation. description:; Compute the fraction of rows where SEX == 'M' in a table. code:; >>> ht.aggregate(hl.agg.fraction(ht.SEX == 'M')); 0.5. dependencies:; Table.aggregate(), aggregators.fraction(). Multiple aggregations. description:; Compute two aggregation statistics, the fraction of rows where; SEX == 'M' and the mean value of X, from the rows of a table. code:; >>> ht.aggregate(hl.struct(fraction_male = hl.agg.fraction(ht.SEX == 'M'),; ... mean_x = hl.agg.mean(ht.X))); Struct(fraction_male=0.5, mean_x=6.5). dependencies:; Table.aggregate(), aggregators.fraction(), aggregators.mean(), StructExpression. Aggregate Per Group. description:; Group the table ht by ID and compute the mean value of X per group. code:; >>> result_ht = ht.group_by(ht.ID).aggregate(mean_x=hl.agg.mean(ht.X)). dependencies:; Table.group_by(), GroupedTable.aggregate(), aggregators.mean(). Matrix Table Aggregations. Aggregate Entries Per Row (Over Columns). description:; Count the number of occurrences of each unique GT field per row, i.e.; aggregate over the columns of the matrix table.; Methods MatrixTable.filter_rows(), MatrixTable.select_rows(),; and MatrixTable.transmute_rows() also support aggregation over columns. code:; >>> result_mt = mt.annotate_rows(gt_counter=hl.agg.counter(mt.GT)). dependencies:; MatrixTable.annotate_rows(), aggregators.counter(). Aggregate Entries Per Column (Over Rows). description:; Compute the mean of the GQ field per column, i.e. aggregate over the rows; of the MatrixTable.; Methods MatrixTable.filter_cols(), MatrixTable.select_cols(),; and MatrixTable.transmute_cols() also support aggregation over rows. code:; >>> result_mt = mt.annotate_cols(gq_mean=hl.agg.mean(mt.GQ)). dependencies:; MatrixTable.annotate_cols(), aggregators.mean(). Aggregate Column Values Into a ",MatchSource.WIKI,docs/0.2/guides/agg.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/guides/agg.html
https://hail.is/docs/0.2/guides/agg.html:2339,Integrability,depend,dependencies,2339,"== 'M' and the mean value of X, from the rows of a table. code:; >>> ht.aggregate(hl.struct(fraction_male = hl.agg.fraction(ht.SEX == 'M'),; ... mean_x = hl.agg.mean(ht.X))); Struct(fraction_male=0.5, mean_x=6.5). dependencies:; Table.aggregate(), aggregators.fraction(), aggregators.mean(), StructExpression. Aggregate Per Group. description:; Group the table ht by ID and compute the mean value of X per group. code:; >>> result_ht = ht.group_by(ht.ID).aggregate(mean_x=hl.agg.mean(ht.X)). dependencies:; Table.group_by(), GroupedTable.aggregate(), aggregators.mean(). Matrix Table Aggregations. Aggregate Entries Per Row (Over Columns). description:; Count the number of occurrences of each unique GT field per row, i.e.; aggregate over the columns of the matrix table.; Methods MatrixTable.filter_rows(), MatrixTable.select_rows(),; and MatrixTable.transmute_rows() also support aggregation over columns. code:; >>> result_mt = mt.annotate_rows(gt_counter=hl.agg.counter(mt.GT)). dependencies:; MatrixTable.annotate_rows(), aggregators.counter(). Aggregate Entries Per Column (Over Rows). description:; Compute the mean of the GQ field per column, i.e. aggregate over the rows; of the MatrixTable.; Methods MatrixTable.filter_cols(), MatrixTable.select_cols(),; and MatrixTable.transmute_cols() also support aggregation over rows. code:; >>> result_mt = mt.annotate_cols(gq_mean=hl.agg.mean(mt.GQ)). dependencies:; MatrixTable.annotate_cols(), aggregators.mean(). Aggregate Column Values Into a Local Value. One aggregation. description:; Aggregate over the column-indexed field pheno.is_female to compute the; fraction of female samples in the matrix table. code:; >>> mt.aggregate_cols(hl.agg.fraction(mt.pheno.is_female)); 0.44. dependencies:; MatrixTable.aggregate_cols(), aggregators.fraction(). Multiple aggregations. description:; Perform multiple aggregations over column-indexed fields by using; a struct expression. The result is a single struct containing; two nested fields, fr",MatchSource.WIKI,docs/0.2/guides/agg.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/guides/agg.html
https://hail.is/docs/0.2/guides/agg.html:2760,Integrability,depend,dependencies,2760,">>> result_ht = ht.group_by(ht.ID).aggregate(mean_x=hl.agg.mean(ht.X)). dependencies:; Table.group_by(), GroupedTable.aggregate(), aggregators.mean(). Matrix Table Aggregations. Aggregate Entries Per Row (Over Columns). description:; Count the number of occurrences of each unique GT field per row, i.e.; aggregate over the columns of the matrix table.; Methods MatrixTable.filter_rows(), MatrixTable.select_rows(),; and MatrixTable.transmute_rows() also support aggregation over columns. code:; >>> result_mt = mt.annotate_rows(gt_counter=hl.agg.counter(mt.GT)). dependencies:; MatrixTable.annotate_rows(), aggregators.counter(). Aggregate Entries Per Column (Over Rows). description:; Compute the mean of the GQ field per column, i.e. aggregate over the rows; of the MatrixTable.; Methods MatrixTable.filter_cols(), MatrixTable.select_cols(),; and MatrixTable.transmute_cols() also support aggregation over rows. code:; >>> result_mt = mt.annotate_cols(gq_mean=hl.agg.mean(mt.GQ)). dependencies:; MatrixTable.annotate_cols(), aggregators.mean(). Aggregate Column Values Into a Local Value. One aggregation. description:; Aggregate over the column-indexed field pheno.is_female to compute the; fraction of female samples in the matrix table. code:; >>> mt.aggregate_cols(hl.agg.fraction(mt.pheno.is_female)); 0.44. dependencies:; MatrixTable.aggregate_cols(), aggregators.fraction(). Multiple aggregations. description:; Perform multiple aggregations over column-indexed fields by using; a struct expression. The result is a single struct containing; two nested fields, fraction_female and case_ratio. code:; >>> mt.aggregate_cols(hl.struct(; ... fraction_female=hl.agg.fraction(mt.pheno.is_female),; ... case_ratio=hl.agg.count_where(mt.is_case) / hl.agg.count())); Struct(fraction_female=0.44, case_ratio=1.0). dependencies:; MatrixTable.aggregate_cols(), aggregators.fraction(), aggregators.count_where(), StructExpression. Aggregate Row Values Into a Local Value. One aggregation. descri",MatchSource.WIKI,docs/0.2/guides/agg.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/guides/agg.html
https://hail.is/docs/0.2/guides/agg.html:3094,Integrability,depend,dependencies,3094,"of the matrix table.; Methods MatrixTable.filter_rows(), MatrixTable.select_rows(),; and MatrixTable.transmute_rows() also support aggregation over columns. code:; >>> result_mt = mt.annotate_rows(gt_counter=hl.agg.counter(mt.GT)). dependencies:; MatrixTable.annotate_rows(), aggregators.counter(). Aggregate Entries Per Column (Over Rows). description:; Compute the mean of the GQ field per column, i.e. aggregate over the rows; of the MatrixTable.; Methods MatrixTable.filter_cols(), MatrixTable.select_cols(),; and MatrixTable.transmute_cols() also support aggregation over rows. code:; >>> result_mt = mt.annotate_cols(gq_mean=hl.agg.mean(mt.GQ)). dependencies:; MatrixTable.annotate_cols(), aggregators.mean(). Aggregate Column Values Into a Local Value. One aggregation. description:; Aggregate over the column-indexed field pheno.is_female to compute the; fraction of female samples in the matrix table. code:; >>> mt.aggregate_cols(hl.agg.fraction(mt.pheno.is_female)); 0.44. dependencies:; MatrixTable.aggregate_cols(), aggregators.fraction(). Multiple aggregations. description:; Perform multiple aggregations over column-indexed fields by using; a struct expression. The result is a single struct containing; two nested fields, fraction_female and case_ratio. code:; >>> mt.aggregate_cols(hl.struct(; ... fraction_female=hl.agg.fraction(mt.pheno.is_female),; ... case_ratio=hl.agg.count_where(mt.is_case) / hl.agg.count())); Struct(fraction_female=0.44, case_ratio=1.0). dependencies:; MatrixTable.aggregate_cols(), aggregators.fraction(), aggregators.count_where(), StructExpression. Aggregate Row Values Into a Local Value. One aggregation. description:; Compute the mean value of the row-indexed field qual. code:; >>> mt.aggregate_rows(hl.agg.mean(mt.qual)); 140054.73333333334. dependencies:; MatrixTable.aggregate_rows(), aggregators.mean(). Multiple aggregations. description:; Perform two row aggregations: count the number of row values of qual; that are greater than 40, a",MatchSource.WIKI,docs/0.2/guides/agg.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/guides/agg.html
https://hail.is/docs/0.2/guides/agg.html:3593,Integrability,depend,dependencies,3593,"select_cols(),; and MatrixTable.transmute_cols() also support aggregation over rows. code:; >>> result_mt = mt.annotate_cols(gq_mean=hl.agg.mean(mt.GQ)). dependencies:; MatrixTable.annotate_cols(), aggregators.mean(). Aggregate Column Values Into a Local Value. One aggregation. description:; Aggregate over the column-indexed field pheno.is_female to compute the; fraction of female samples in the matrix table. code:; >>> mt.aggregate_cols(hl.agg.fraction(mt.pheno.is_female)); 0.44. dependencies:; MatrixTable.aggregate_cols(), aggregators.fraction(). Multiple aggregations. description:; Perform multiple aggregations over column-indexed fields by using; a struct expression. The result is a single struct containing; two nested fields, fraction_female and case_ratio. code:; >>> mt.aggregate_cols(hl.struct(; ... fraction_female=hl.agg.fraction(mt.pheno.is_female),; ... case_ratio=hl.agg.count_where(mt.is_case) / hl.agg.count())); Struct(fraction_female=0.44, case_ratio=1.0). dependencies:; MatrixTable.aggregate_cols(), aggregators.fraction(), aggregators.count_where(), StructExpression. Aggregate Row Values Into a Local Value. One aggregation. description:; Compute the mean value of the row-indexed field qual. code:; >>> mt.aggregate_rows(hl.agg.mean(mt.qual)); 140054.73333333334. dependencies:; MatrixTable.aggregate_rows(), aggregators.mean(). Multiple aggregations. description:; Perform two row aggregations: count the number of row values of qual; that are greater than 40, and compute the mean value of qual.; The result is a single struct containing two nested fields, n_high_quality and mean_qual. code:; >>> mt.aggregate_rows(; ... hl.struct(n_high_quality=hl.agg.count_where(mt.qual > 40),; ... mean_qual=hl.agg.mean(mt.qual))); Struct(n_high_quality=9, mean_qual=140054.73333333334). dependencies:; MatrixTable.aggregate_rows(), aggregators.count_where(), aggregators.mean(), StructExpression. Aggregate Entry Values Into A Local Value. description:; Compute the mean",MatchSource.WIKI,docs/0.2/guides/agg.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/guides/agg.html
https://hail.is/docs/0.2/guides/agg.html:3907,Integrability,depend,dependencies,3907,"column-indexed field pheno.is_female to compute the; fraction of female samples in the matrix table. code:; >>> mt.aggregate_cols(hl.agg.fraction(mt.pheno.is_female)); 0.44. dependencies:; MatrixTable.aggregate_cols(), aggregators.fraction(). Multiple aggregations. description:; Perform multiple aggregations over column-indexed fields by using; a struct expression. The result is a single struct containing; two nested fields, fraction_female and case_ratio. code:; >>> mt.aggregate_cols(hl.struct(; ... fraction_female=hl.agg.fraction(mt.pheno.is_female),; ... case_ratio=hl.agg.count_where(mt.is_case) / hl.agg.count())); Struct(fraction_female=0.44, case_ratio=1.0). dependencies:; MatrixTable.aggregate_cols(), aggregators.fraction(), aggregators.count_where(), StructExpression. Aggregate Row Values Into a Local Value. One aggregation. description:; Compute the mean value of the row-indexed field qual. code:; >>> mt.aggregate_rows(hl.agg.mean(mt.qual)); 140054.73333333334. dependencies:; MatrixTable.aggregate_rows(), aggregators.mean(). Multiple aggregations. description:; Perform two row aggregations: count the number of row values of qual; that are greater than 40, and compute the mean value of qual.; The result is a single struct containing two nested fields, n_high_quality and mean_qual. code:; >>> mt.aggregate_rows(; ... hl.struct(n_high_quality=hl.agg.count_where(mt.qual > 40),; ... mean_qual=hl.agg.mean(mt.qual))); Struct(n_high_quality=9, mean_qual=140054.73333333334). dependencies:; MatrixTable.aggregate_rows(), aggregators.count_where(), aggregators.mean(), StructExpression. Aggregate Entry Values Into A Local Value. description:; Compute the mean of the entry-indexed field GQ and the call rate of; the entry-indexed field GT. The result is returned as a single struct with; two nested fields. code:; >>> mt.aggregate_entries(; ... hl.struct(global_gq_mean=hl.agg.mean(mt.GQ),; ... call_rate=hl.agg.fraction(hl.is_defined(mt.GT)))); Struct(global_gq_mean=69.60",MatchSource.WIKI,docs/0.2/guides/agg.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/guides/agg.html
https://hail.is/docs/0.2/guides/agg.html:4422,Integrability,depend,dependencies,4422,"_female=hl.agg.fraction(mt.pheno.is_female),; ... case_ratio=hl.agg.count_where(mt.is_case) / hl.agg.count())); Struct(fraction_female=0.44, case_ratio=1.0). dependencies:; MatrixTable.aggregate_cols(), aggregators.fraction(), aggregators.count_where(), StructExpression. Aggregate Row Values Into a Local Value. One aggregation. description:; Compute the mean value of the row-indexed field qual. code:; >>> mt.aggregate_rows(hl.agg.mean(mt.qual)); 140054.73333333334. dependencies:; MatrixTable.aggregate_rows(), aggregators.mean(). Multiple aggregations. description:; Perform two row aggregations: count the number of row values of qual; that are greater than 40, and compute the mean value of qual.; The result is a single struct containing two nested fields, n_high_quality and mean_qual. code:; >>> mt.aggregate_rows(; ... hl.struct(n_high_quality=hl.agg.count_where(mt.qual > 40),; ... mean_qual=hl.agg.mean(mt.qual))); Struct(n_high_quality=9, mean_qual=140054.73333333334). dependencies:; MatrixTable.aggregate_rows(), aggregators.count_where(), aggregators.mean(), StructExpression. Aggregate Entry Values Into A Local Value. description:; Compute the mean of the entry-indexed field GQ and the call rate of; the entry-indexed field GT. The result is returned as a single struct with; two nested fields. code:; >>> mt.aggregate_entries(; ... hl.struct(global_gq_mean=hl.agg.mean(mt.GQ),; ... call_rate=hl.agg.fraction(hl.is_defined(mt.GT)))); Struct(global_gq_mean=69.60514541387025, call_rate=0.9933333333333333). dependencies:; MatrixTable.aggregate_entries(), aggregators.mean(), aggregators.fraction(), StructExpression. Aggregate Per Column Group. description:; Group the columns of the matrix table by the column-indexed; field cohort and compute the call rate per cohort. code:; >>> result_mt = (mt.group_cols_by(mt.cohort); ... .aggregate(call_rate=hl.agg.fraction(hl.is_defined(mt.GT)))). dependencies:; MatrixTable.group_cols_by(), GroupedMatrixTable, GroupedMatrixTable.agg",MatchSource.WIKI,docs/0.2/guides/agg.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/guides/agg.html
https://hail.is/docs/0.2/guides/agg.html:4965,Integrability,depend,dependencies,4965,"le aggregations. description:; Perform two row aggregations: count the number of row values of qual; that are greater than 40, and compute the mean value of qual.; The result is a single struct containing two nested fields, n_high_quality and mean_qual. code:; >>> mt.aggregate_rows(; ... hl.struct(n_high_quality=hl.agg.count_where(mt.qual > 40),; ... mean_qual=hl.agg.mean(mt.qual))); Struct(n_high_quality=9, mean_qual=140054.73333333334). dependencies:; MatrixTable.aggregate_rows(), aggregators.count_where(), aggregators.mean(), StructExpression. Aggregate Entry Values Into A Local Value. description:; Compute the mean of the entry-indexed field GQ and the call rate of; the entry-indexed field GT. The result is returned as a single struct with; two nested fields. code:; >>> mt.aggregate_entries(; ... hl.struct(global_gq_mean=hl.agg.mean(mt.GQ),; ... call_rate=hl.agg.fraction(hl.is_defined(mt.GT)))); Struct(global_gq_mean=69.60514541387025, call_rate=0.9933333333333333). dependencies:; MatrixTable.aggregate_entries(), aggregators.mean(), aggregators.fraction(), StructExpression. Aggregate Per Column Group. description:; Group the columns of the matrix table by the column-indexed; field cohort and compute the call rate per cohort. code:; >>> result_mt = (mt.group_cols_by(mt.cohort); ... .aggregate(call_rate=hl.agg.fraction(hl.is_defined(mt.GT)))). dependencies:; MatrixTable.group_cols_by(), GroupedMatrixTable, GroupedMatrixTable.aggregate(). understanding:. Group the columns of the matrix table by; the column-indexed field cohort using MatrixTable.group_cols_by(),; which returns a GroupedMatrixTable. Then use; GroupedMatrixTable.aggregate() to compute an aggregation per column; group.; The result is a matrix table with an entry field call_rate that contains; the result of the aggregation. The new matrix table has a row schema equal; to the original row schema, a column schema equal to the fields passed to; group_cols_by, and an entry schema determined by the expres",MatchSource.WIKI,docs/0.2/guides/agg.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/guides/agg.html
https://hail.is/docs/0.2/guides/agg.html:5349,Integrability,depend,dependencies,5349,")); Struct(n_high_quality=9, mean_qual=140054.73333333334). dependencies:; MatrixTable.aggregate_rows(), aggregators.count_where(), aggregators.mean(), StructExpression. Aggregate Entry Values Into A Local Value. description:; Compute the mean of the entry-indexed field GQ and the call rate of; the entry-indexed field GT. The result is returned as a single struct with; two nested fields. code:; >>> mt.aggregate_entries(; ... hl.struct(global_gq_mean=hl.agg.mean(mt.GQ),; ... call_rate=hl.agg.fraction(hl.is_defined(mt.GT)))); Struct(global_gq_mean=69.60514541387025, call_rate=0.9933333333333333). dependencies:; MatrixTable.aggregate_entries(), aggregators.mean(), aggregators.fraction(), StructExpression. Aggregate Per Column Group. description:; Group the columns of the matrix table by the column-indexed; field cohort and compute the call rate per cohort. code:; >>> result_mt = (mt.group_cols_by(mt.cohort); ... .aggregate(call_rate=hl.agg.fraction(hl.is_defined(mt.GT)))). dependencies:; MatrixTable.group_cols_by(), GroupedMatrixTable, GroupedMatrixTable.aggregate(). understanding:. Group the columns of the matrix table by; the column-indexed field cohort using MatrixTable.group_cols_by(),; which returns a GroupedMatrixTable. Then use; GroupedMatrixTable.aggregate() to compute an aggregation per column; group.; The result is a matrix table with an entry field call_rate that contains; the result of the aggregation. The new matrix table has a row schema equal; to the original row schema, a column schema equal to the fields passed to; group_cols_by, and an entry schema determined by the expression passed to; aggregate. Other column fields and entry fields are dropped. Aggregate Per Row Group. description:; Compute the number of calls with one or more non-reference; alleles per gene group. code:; >>> result_mt = (mt.group_rows_by(mt.gene); ... .aggregate(n_non_ref=hl.agg.count_where(mt.GT.is_non_ref()))). dependencies:; MatrixTable.group_rows_by(), GroupedMatrixTable, G",MatchSource.WIKI,docs/0.2/guides/agg.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/guides/agg.html
https://hail.is/docs/0.2/guides/agg.html:6297,Integrability,depend,dependencies,6297,"p. description:; Group the columns of the matrix table by the column-indexed; field cohort and compute the call rate per cohort. code:; >>> result_mt = (mt.group_cols_by(mt.cohort); ... .aggregate(call_rate=hl.agg.fraction(hl.is_defined(mt.GT)))). dependencies:; MatrixTable.group_cols_by(), GroupedMatrixTable, GroupedMatrixTable.aggregate(). understanding:. Group the columns of the matrix table by; the column-indexed field cohort using MatrixTable.group_cols_by(),; which returns a GroupedMatrixTable. Then use; GroupedMatrixTable.aggregate() to compute an aggregation per column; group.; The result is a matrix table with an entry field call_rate that contains; the result of the aggregation. The new matrix table has a row schema equal; to the original row schema, a column schema equal to the fields passed to; group_cols_by, and an entry schema determined by the expression passed to; aggregate. Other column fields and entry fields are dropped. Aggregate Per Row Group. description:; Compute the number of calls with one or more non-reference; alleles per gene group. code:; >>> result_mt = (mt.group_rows_by(mt.gene); ... .aggregate(n_non_ref=hl.agg.count_where(mt.GT.is_non_ref()))). dependencies:; MatrixTable.group_rows_by(), GroupedMatrixTable, GroupedMatrixTable.aggregate(). understanding:. Group the rows of the matrix table by the row-indexed field gene; using MatrixTable.group_rows_by(), which returns a; GroupedMatrixTable. Then use GroupedMatrixTable.aggregate(); to compute an aggregation per grouped row.; The result is a matrix table with an entry field n_non_ref that contains; the result of the aggregation. This new matrix table has a row schema; equal to the fields passed to group_rows_by, a column schema equal to the; column schema of the original matrix table, and an entry schema determined; by the expression passed to aggregate. Other row fields and entry fields; are dropped. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/guides/agg.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/guides/agg.html
https://hail.is/docs/0.2/guides/annotation.html:1703,Deployability,update,updated,1703,"﻿. Hail | ; Annotation. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Aggregation; Annotation (Adding Fields); Create a nested annotation; Remove a nested annotation. Genetics. Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. How-To Guides; Annotation. View page source. Annotation; Annotations are Hail’s way of adding data fields to Hail’s tables and matrix; tables. Create a nested annotation. description:; Add a new field gq_mean as a nested field inside info. code:; >>> mt = mt.annotate_rows(info=mt.info.annotate(gq_mean=hl.agg.mean(mt.GQ))). dependencies:; StructExpression.annotate(), MatrixTable.annotate_rows(). understanding:. To add a new field gq_mean as a nested field inside info,; instead of a top-level field, we need to annotate the info field itself.; Construct an expression mt.info.annotate(gq_mean=...) which adds the field; to info. Then, reassign this expression to info using; MatrixTable.annotate_rows(). Remove a nested annotation. description:; Drop a field AF, which is nested inside the info field. To drop a nested field AF, construct an expression mt.info.drop('AF'); which drops the field from its parent field, info. Then, reassign this; expression to info using MatrixTable.annotate_rows(). code:; >>> mt = mt.annotate_rows(info=mt.info.drop('AF')). dependencies:; StructExpression.drop(), MatrixTable.annotate_rows(). Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/guides/annotation.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/guides/annotation.html
https://hail.is/docs/0.2/guides/annotation.html:840,Integrability,depend,dependencies,840,"﻿. Hail | ; Annotation. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Aggregation; Annotation (Adding Fields); Create a nested annotation; Remove a nested annotation. Genetics. Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. How-To Guides; Annotation. View page source. Annotation; Annotations are Hail’s way of adding data fields to Hail’s tables and matrix; tables. Create a nested annotation. description:; Add a new field gq_mean as a nested field inside info. code:; >>> mt = mt.annotate_rows(info=mt.info.annotate(gq_mean=hl.agg.mean(mt.GQ))). dependencies:; StructExpression.annotate(), MatrixTable.annotate_rows(). understanding:. To add a new field gq_mean as a nested field inside info,; instead of a top-level field, we need to annotate the info field itself.; Construct an expression mt.info.annotate(gq_mean=...) which adds the field; to info. Then, reassign this expression to info using; MatrixTable.annotate_rows(). Remove a nested annotation. description:; Drop a field AF, which is nested inside the info field. To drop a nested field AF, construct an expression mt.info.drop('AF'); which drops the field from its parent field, info. Then, reassign this; expression to info using MatrixTable.annotate_rows(). code:; >>> mt = mt.annotate_rows(info=mt.info.drop('AF')). dependencies:; StructExpression.drop(), MatrixTable.annotate_rows(). Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/guides/annotation.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/guides/annotation.html
https://hail.is/docs/0.2/guides/annotation.html:1577,Integrability,depend,dependencies,1577,"﻿. Hail | ; Annotation. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Aggregation; Annotation (Adding Fields); Create a nested annotation; Remove a nested annotation. Genetics. Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. How-To Guides; Annotation. View page source. Annotation; Annotations are Hail’s way of adding data fields to Hail’s tables and matrix; tables. Create a nested annotation. description:; Add a new field gq_mean as a nested field inside info. code:; >>> mt = mt.annotate_rows(info=mt.info.annotate(gq_mean=hl.agg.mean(mt.GQ))). dependencies:; StructExpression.annotate(), MatrixTable.annotate_rows(). understanding:. To add a new field gq_mean as a nested field inside info,; instead of a top-level field, we need to annotate the info field itself.; Construct an expression mt.info.annotate(gq_mean=...) which adds the field; to info. Then, reassign this expression to info using; MatrixTable.annotate_rows(). Remove a nested annotation. description:; Drop a field AF, which is nested inside the info field. To drop a nested field AF, construct an expression mt.info.drop('AF'); which drops the field from its parent field, info. Then, reassign this; expression to info using MatrixTable.annotate_rows(). code:; >>> mt = mt.annotate_rows(info=mt.info.drop('AF')). dependencies:; StructExpression.drop(), MatrixTable.annotate_rows(). Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/guides/annotation.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/guides/annotation.html
https://hail.is/docs/0.2/guides/genetics.html:971,Deployability,pipeline,pipelines,971,"﻿. Hail | ; Genetics. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Aggregation; Annotation (Adding Fields); Genetics; Formatting; Convert variants in string format to separate locus and allele fields; Liftover variants from one coordinate system to another. Filtering and Pruning; Remove related individuals from a dataset; Filter loci by a list of locus intervals; Pruning Variants in Linkage Disequilibrium. Analysis; Linear Regression. PLINK Conversions; Polygenic Score Calculation. Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. How-To Guides; Genetics. View page source. Genetics; This page tailored how-to guides for small but commonly-used patterns; appearing in genetics pipelines. For documentation on the suite of; genetics functions built into Hail, see the genetics methods page. Formatting. Convert variants in string format to separate locus and allele fields. code:; >>> ht = ht.key_by(**hl.parse_variant(ht.variant)). dependencies:; parse_variant(), key_by(). understanding:. If your variants are strings of the format ‘chr:pos:ref:alt’, you may want; to convert them to separate locus and allele fields. This is useful if; you have imported a table with variants in string format and you would like to; join this table with other Hail tables that are keyed by locus and; alleles.; hl.parse_variant(ht.variant) constructs a StructExpression; containing two nested fields for the locus and alleles. The ** syntax unpacks; this struct so that the resulting table has two new fields, locus and; alleles. Liftover variants from one coordinate system to another. tags:; liftover. description:; Liftover a Table or MatrixTable from one reference genome to another. code:; First, we need to set up",MatchSource.WIKI,docs/0.2/guides/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/guides/genetics.html
https://hail.is/docs/0.2/guides/genetics.html:14748,Deployability,update,updated,14748,"er than mt. Lastly, the argument to root must be specified for both cases – otherwise; the ‘Male’ output will overwrite the ‘Female’ output.; The second approach uses the aggregators.group_by() and aggregators.linreg(); aggregators. The aggregation expression generates a dictionary where a key is a group; (value of the grouping variable) and the corresponding value is the linear regression statistics; for those samples in the group. The result of the aggregation expression is then used to annotate; the matrix table.; The linear_regression_rows() method is more efficient than the aggregators.linreg(); aggregator and can be extended to multiple phenotypes, but the aggregators.linreg(); aggregator is more flexible (multiple covariates can be vary by entry) and returns a richer; set of statistics. PLINK Conversions. Polygenic Score Calculation. plink:; >>> plink --bfile data --score scores.txt sum . tags:; PRS. description:; This command is analogous to plink’s –score command with the; sum option. Biallelic variants are required. code:; >>> mt = hl.import_plink(; ... bed=""data/ldsc.bed"", bim=""data/ldsc.bim"", fam=""data/ldsc.fam"",; ... quant_pheno=True, missing='-9'); >>> mt = hl.variant_qc(mt); >>> scores = hl.import_table('data/scores.txt', delimiter=' ', key='rsid',; ... types={'score': hl.tfloat32}); >>> mt = mt.annotate_rows(**scores[mt.rsid]); >>> flip = hl.case().when(mt.allele == mt.alleles[0], True).when(; ... mt.allele == mt.alleles[1], False).or_missing(); >>> mt = mt.annotate_rows(flip=flip); >>> mt = mt.annotate_rows(; ... prior=2 * hl.if_else(mt.flip, mt.variant_qc.AF[0], mt.variant_qc.AF[1])); >>> mt = mt.annotate_cols(; ... prs=hl.agg.sum(; ... mt.score * hl.coalesce(; ... hl.if_else(mt.flip, 2 - mt.GT.n_alt_alleles(),; ... mt.GT.n_alt_alleles()), mt.prior))). dependencies:; import_plink(), variant_qc(), import_table(),; coalesce(), case(), cond(), Call.n_alt_alleles(). Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/guides/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/guides/genetics.html
https://hail.is/docs/0.2/guides/genetics.html:7515,Energy Efficiency,efficient,efficient,7515,"; ... [hl.parse_locus_interval(x, reference_genome='GRCh37') for x in intervals]). dependencies:; methods.filter_intervals(), parse_locus_interval(). Pruning Variants in Linkage Disequilibrium. tags:; LD Prune. description:; Remove correlated variants from a matrix table. code:; >>> biallelic_mt = mt.filter_rows(hl.len(mt.alleles) == 2); >>> pruned_variant_table = hl.ld_prune(mt.GT, r2=0.2, bp_window_size=500000); >>> filtered_mt = mt.filter_rows(; ... hl.is_defined(pruned_variant_table[mt.row_key])). dependencies:; ld_prune(). understanding:. Hail’s ld_prune() method takes a matrix table and returns a table; with a subset of variants which are uncorrelated with each other. The method; requires a biallelic dataset, so we first filter our dataset to biallelic; variants. Next, we get a table of independent variants using ld_prune(),; which we can use to filter the rows of our original dataset.; Note that it is more efficient to do the final filtering step on the original; dataset, rather than on the biallelic dataset, so that the biallelic dataset; does not need to be recomputed. Analysis. Linear Regression. Single Phenotype. tags:; Linear Regression. description:; Compute linear regression statistics for a single phenotype. code:; Approach #1: Use the linear_regression_rows() method; >>> ht = hl.linear_regression_rows(y=mt.pheno.height,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). Approach #2: Use the aggregators.linreg() aggregator; >>> mt_linreg = mt.annotate_rows(linreg=hl.agg.linreg(y=mt.pheno.height,; ... x=[1, mt.GT.n_alt_alleles()])). dependencies:; linear_regression_rows(), aggregators.linreg(). understanding:. The linear_regression_rows() method is more efficient than using the aggregators.linreg(); aggregator. However, the aggregators.linreg() aggregator is more flexible (multiple covariates; can vary by entry) and returns a richer set of statistics. Multiple Phenotypes. tags:; Linear Regression. description:; Compute linear regression statistic",MatchSource.WIKI,docs/0.2/guides/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/guides/genetics.html
https://hail.is/docs/0.2/guides/genetics.html:8288,Energy Efficiency,efficient,efficient,8288,"equires a biallelic dataset, so we first filter our dataset to biallelic; variants. Next, we get a table of independent variants using ld_prune(),; which we can use to filter the rows of our original dataset.; Note that it is more efficient to do the final filtering step on the original; dataset, rather than on the biallelic dataset, so that the biallelic dataset; does not need to be recomputed. Analysis. Linear Regression. Single Phenotype. tags:; Linear Regression. description:; Compute linear regression statistics for a single phenotype. code:; Approach #1: Use the linear_regression_rows() method; >>> ht = hl.linear_regression_rows(y=mt.pheno.height,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). Approach #2: Use the aggregators.linreg() aggregator; >>> mt_linreg = mt.annotate_rows(linreg=hl.agg.linreg(y=mt.pheno.height,; ... x=[1, mt.GT.n_alt_alleles()])). dependencies:; linear_regression_rows(), aggregators.linreg(). understanding:. The linear_regression_rows() method is more efficient than using the aggregators.linreg(); aggregator. However, the aggregators.linreg() aggregator is more flexible (multiple covariates; can vary by entry) and returns a richer set of statistics. Multiple Phenotypes. tags:; Linear Regression. description:; Compute linear regression statistics for multiple phenotypes. code:; Approach #1: Use the linear_regression_rows() method for all phenotypes simultaneously; >>> ht_result = hl.linear_regression_rows(y=[mt.pheno.height, mt.pheno.blood_pressure],; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). Approach #2: Use the linear_regression_rows() method for each phenotype sequentially; >>> ht1 = hl.linear_regression_rows(y=mt.pheno.height,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). >>> ht2 = hl.linear_regression_rows(y=mt.pheno.blood_pressure,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). Approach #3: Use the aggregators.linreg() aggregator; >>> mt_linreg = mt.annotate_rows(; ... linreg_height=hl.agg.linreg(y=mt.p",MatchSource.WIKI,docs/0.2/guides/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/guides/genetics.html
https://hail.is/docs/0.2/guides/genetics.html:9549,Energy Efficiency,efficient,efficient,9549,"tion:; Compute linear regression statistics for multiple phenotypes. code:; Approach #1: Use the linear_regression_rows() method for all phenotypes simultaneously; >>> ht_result = hl.linear_regression_rows(y=[mt.pheno.height, mt.pheno.blood_pressure],; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). Approach #2: Use the linear_regression_rows() method for each phenotype sequentially; >>> ht1 = hl.linear_regression_rows(y=mt.pheno.height,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). >>> ht2 = hl.linear_regression_rows(y=mt.pheno.blood_pressure,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). Approach #3: Use the aggregators.linreg() aggregator; >>> mt_linreg = mt.annotate_rows(; ... linreg_height=hl.agg.linreg(y=mt.pheno.height,; ... x=[1, mt.GT.n_alt_alleles()]),; ... linreg_bp=hl.agg.linreg(y=mt.pheno.blood_pressure,; ... x=[1, mt.GT.n_alt_alleles()])). dependencies:; linear_regression_rows(), aggregators.linreg(). understanding:. The linear_regression_rows() method is more efficient than using the aggregators.linreg(); aggregator, especially when analyzing many phenotypes. However, the aggregators.linreg(); aggregator is more flexible (multiple covariates can vary by entry) and returns a richer set of; statistics. The linear_regression_rows() method drops samples that have a missing value for; any of the phenotypes. Therefore, Approach #1 may not be suitable for phenotypes with differential; patterns of missingness. Approach #2 will do two passes over the data while Approaches #1 and #3 will; do one pass over the data and compute the regression statistics for each phenotype simultaneously. Using Variants (SNPs) as Covariates. tags:; sample genotypes covariate. description:; Use sample genotype dosage at specific variant(s) as covariates in regression routines. code:; Create a sample annotation from the genotype dosage for each variant of; interest by combining the filter and collect aggregators:; >>> mt_annot = mt.annotate_cols(; ... snp1 = hl.agg.f",MatchSource.WIKI,docs/0.2/guides/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/guides/genetics.html
https://hail.is/docs/0.2/guides/genetics.html:13343,Energy Efficiency,efficient,efficient,13343,"up even though it can compute statistics for multiple phenotypes; simultaneously. This is because the linear_regression_rows() method drops samples that have a; missing value for any of the phenotypes. When the groups are mutually exclusive,; such as ‘Male’ and ‘Female’, no samples remain! Note that we cannot define male_pheno = ~female_pheno; because we subsequently need male_pheno to be an expression on the mt_linreg matrix table; rather than mt. Lastly, the argument to root must be specified for both cases – otherwise; the ‘Male’ output will overwrite the ‘Female’ output.; The second approach uses the aggregators.group_by() and aggregators.linreg(); aggregators. The aggregation expression generates a dictionary where a key is a group; (value of the grouping variable) and the corresponding value is the linear regression statistics; for those samples in the group. The result of the aggregation expression is then used to annotate; the matrix table.; The linear_regression_rows() method is more efficient than the aggregators.linreg(); aggregator and can be extended to multiple phenotypes, but the aggregators.linreg(); aggregator is more flexible (multiple covariates can be vary by entry) and returns a richer; set of statistics. PLINK Conversions. Polygenic Score Calculation. plink:; >>> plink --bfile data --score scores.txt sum . tags:; PRS. description:; This command is analogous to plink’s –score command with the; sum option. Biallelic variants are required. code:; >>> mt = hl.import_plink(; ... bed=""data/ldsc.bed"", bim=""data/ldsc.bim"", fam=""data/ldsc.fam"",; ... quant_pheno=True, missing='-9'); >>> mt = hl.variant_qc(mt); >>> scores = hl.import_table('data/scores.txt', delimiter=' ', key='rsid',; ... types={'score': hl.tfloat32}); >>> mt = mt.annotate_rows(**scores[mt.rsid]); >>> flip = hl.case().when(mt.allele == mt.alleles[0], True).when(; ... mt.allele == mt.alleles[1], False).or_missing(); >>> mt = mt.annotate_rows(flip=flip); >>> mt = mt.annotate_rows(; ... pr",MatchSource.WIKI,docs/0.2/guides/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/guides/genetics.html
https://hail.is/docs/0.2/guides/genetics.html:1228,Integrability,depend,dependencies,1228," How-To Guides; Aggregation; Annotation (Adding Fields); Genetics; Formatting; Convert variants in string format to separate locus and allele fields; Liftover variants from one coordinate system to another. Filtering and Pruning; Remove related individuals from a dataset; Filter loci by a list of locus intervals; Pruning Variants in Linkage Disequilibrium. Analysis; Linear Regression. PLINK Conversions; Polygenic Score Calculation. Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. How-To Guides; Genetics. View page source. Genetics; This page tailored how-to guides for small but commonly-used patterns; appearing in genetics pipelines. For documentation on the suite of; genetics functions built into Hail, see the genetics methods page. Formatting. Convert variants in string format to separate locus and allele fields. code:; >>> ht = ht.key_by(**hl.parse_variant(ht.variant)). dependencies:; parse_variant(), key_by(). understanding:. If your variants are strings of the format ‘chr:pos:ref:alt’, you may want; to convert them to separate locus and allele fields. This is useful if; you have imported a table with variants in string format and you would like to; join this table with other Hail tables that are keyed by locus and; alleles.; hl.parse_variant(ht.variant) constructs a StructExpression; containing two nested fields for the locus and alleles. The ** syntax unpacks; this struct so that the resulting table has two new fields, locus and; alleles. Liftover variants from one coordinate system to another. tags:; liftover. description:; Liftover a Table or MatrixTable from one reference genome to another. code:; First, we need to set up the two reference genomes (source and destination):; >>> rg37 = hl.get_reference('GRCh37') ; >>> rg38 = hl.get_reference('GRCh38') ; >>> rg37.add_liftover('gs://hail-common/references/grch37_to_grch38.over.chain.gz', rg38) . Then we can liftover ",MatchSource.WIKI,docs/0.2/guides/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/guides/genetics.html
https://hail.is/docs/0.2/guides/genetics.html:2966,Integrability,depend,dependencies,2966,"set up the two reference genomes (source and destination):; >>> rg37 = hl.get_reference('GRCh37') ; >>> rg38 = hl.get_reference('GRCh38') ; >>> rg37.add_liftover('gs://hail-common/references/grch37_to_grch38.over.chain.gz', rg38) . Then we can liftover the locus coordinates in a Table or MatrixTable (here, ht); from reference genome 'GRCh37' to 'GRCh38':; >>> ht = ht.annotate(new_locus=hl.liftover(ht.locus, 'GRCh38')) ; >>> ht = ht.filter(hl.is_defined(ht.new_locus)) ; >>> ht = ht.key_by(locus=ht.new_locus) . Note that this approach does not retain the old locus, nor does it verify; that the allele has not changed strand. We can keep the old one for; reference and filter out any liftover that changed strands using:; >>> ht = ht.annotate(new_locus=hl.liftover(ht.locus, 'GRCh38', include_strand=True),; ... old_locus=ht.locus) ; >>> ht = ht.filter(hl.is_defined(ht.new_locus) & ~ht.new_locus.is_negative_strand) ; >>> ht = ht.key_by(locus=ht.new_locus.result) . dependencies:; liftover(), add_liftover(), get_reference(). Filtering and Pruning. Remove related individuals from a dataset. tags:; kinship. description:; Compute a measure of kinship between individuals, and then; prune related individuals from a matrix table. code:; >>> pc_rel = hl.pc_relate(mt.GT, 0.001, k=2, statistics='kin'); >>> pairs = pc_rel.filter(pc_rel['kin'] > 0.125); >>> related_samples_to_remove = hl.maximal_independent_set(pairs.i, pairs.j,; ... keep=False); >>> result = mt.filter_cols(; ... hl.is_defined(related_samples_to_remove[mt.col_key]), keep=False). dependencies:; pc_relate(), maximal_independent_set(). understanding:. To remove related individuals from a dataset, we first compute a measure; of relatedness between individuals using pc_relate(). We filter this; result based on a kinship threshold, which gives us a table of related pairs.; From this table of pairs, we can compute the complement of the maximal; independent set using maximal_independent_set(). The parameter; keep=False in max",MatchSource.WIKI,docs/0.2/guides/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/guides/genetics.html
https://hail.is/docs/0.2/guides/genetics.html:3548,Integrability,depend,dependencies,3548,"it verify; that the allele has not changed strand. We can keep the old one for; reference and filter out any liftover that changed strands using:; >>> ht = ht.annotate(new_locus=hl.liftover(ht.locus, 'GRCh38', include_strand=True),; ... old_locus=ht.locus) ; >>> ht = ht.filter(hl.is_defined(ht.new_locus) & ~ht.new_locus.is_negative_strand) ; >>> ht = ht.key_by(locus=ht.new_locus.result) . dependencies:; liftover(), add_liftover(), get_reference(). Filtering and Pruning. Remove related individuals from a dataset. tags:; kinship. description:; Compute a measure of kinship between individuals, and then; prune related individuals from a matrix table. code:; >>> pc_rel = hl.pc_relate(mt.GT, 0.001, k=2, statistics='kin'); >>> pairs = pc_rel.filter(pc_rel['kin'] > 0.125); >>> related_samples_to_remove = hl.maximal_independent_set(pairs.i, pairs.j,; ... keep=False); >>> result = mt.filter_cols(; ... hl.is_defined(related_samples_to_remove[mt.col_key]), keep=False). dependencies:; pc_relate(), maximal_independent_set(). understanding:. To remove related individuals from a dataset, we first compute a measure; of relatedness between individuals using pc_relate(). We filter this; result based on a kinship threshold, which gives us a table of related pairs.; From this table of pairs, we can compute the complement of the maximal; independent set using maximal_independent_set(). The parameter; keep=False in maximal_independent_set specifies that we want the; complement of the set (the variants to remove), rather than the maximal; independent set itself. It’s important to use the complement for filtering,; rather than the set itself, because the maximal independent set will not contain; the singleton individuals.; Once we have a list of samples to remove, we can filter the columns of the; dataset to remove the related individuals. Filter loci by a list of locus intervals. From a table of intervals. tags:; genomic region, genomic range. description:; Import a text file of locus i",MatchSource.WIKI,docs/0.2/guides/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/guides/genetics.html
https://hail.is/docs/0.2/guides/genetics.html:4839,Integrability,depend,dependencies,4839,"s, we can compute the complement of the maximal; independent set using maximal_independent_set(). The parameter; keep=False in maximal_independent_set specifies that we want the; complement of the set (the variants to remove), rather than the maximal; independent set itself. It’s important to use the complement for filtering,; rather than the set itself, because the maximal independent set will not contain; the singleton individuals.; Once we have a list of samples to remove, we can filter the columns of the; dataset to remove the related individuals. Filter loci by a list of locus intervals. From a table of intervals. tags:; genomic region, genomic range. description:; Import a text file of locus intervals as a table, then use; this table to filter the loci in a matrix table. code:; >>> interval_table = hl.import_locus_intervals('data/gene.interval_list', reference_genome='GRCh37'); >>> filtered_mt = mt.filter_rows(hl.is_defined(interval_table[mt.locus])). dependencies:; import_locus_intervals(), MatrixTable.filter_rows(). understanding:. We have a matrix table mt containing the loci we would like to filter, and a; list of locus intervals stored in a file. We can import the intervals into a; table with import_locus_intervals().; Hail supports implicit joins between locus intervals and loci, so we can filter; our dataset to the rows defined in the join between the interval table and our; matrix table.; interval_table[mt.locus] joins the matrix table with the table of intervals; based on locus and interval<locus> matches. This is a StructExpression, which; will be defined if the locus was found in any interval, or missing if the locus; is outside all intervals.; To do our filtering, we can filter to the rows of our matrix table where the; struct expression interval_table[mt.locus] is defined.; This method will also work to filter a table of loci, as well as a matrix; table. From a UCSC BED file. description:; Import a UCSC BED file as a table of intervals, then us",MatchSource.WIKI,docs/0.2/guides/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/guides/genetics.html
https://hail.is/docs/0.2/guides/genetics.html:6080,Integrability,depend,dependencies,6080,"us_intervals().; Hail supports implicit joins between locus intervals and loci, so we can filter; our dataset to the rows defined in the join between the interval table and our; matrix table.; interval_table[mt.locus] joins the matrix table with the table of intervals; based on locus and interval<locus> matches. This is a StructExpression, which; will be defined if the locus was found in any interval, or missing if the locus; is outside all intervals.; To do our filtering, we can filter to the rows of our matrix table where the; struct expression interval_table[mt.locus] is defined.; This method will also work to filter a table of loci, as well as a matrix; table. From a UCSC BED file. description:; Import a UCSC BED file as a table of intervals, then use this; table to filter the loci in a matrix table. code:; >>> interval_table = hl.import_bed('data/file1.bed', reference_genome='GRCh37'); >>> filtered_mt = mt.filter_rows(hl.is_defined(interval_table[mt.locus])). dependencies:; import_bed(), MatrixTable.filter_rows(). Using hl.filter_intervals. description:; Filter using an interval table, suitable for a small list of; intervals. code:; >>> filtered_mt = hl.filter_intervals(mt, interval_table['interval'].collect()). dependencies:; methods.filter_intervals(). Declaring intervals with hl.parse_locus_interval. description:; Filter to declared intervals. code:; >>> intervals = ['1:100M-200M', '16:29.1M-30.2M', 'X']; >>> filtered_mt = hl.filter_intervals(; ... mt,; ... [hl.parse_locus_interval(x, reference_genome='GRCh37') for x in intervals]). dependencies:; methods.filter_intervals(), parse_locus_interval(). Pruning Variants in Linkage Disequilibrium. tags:; LD Prune. description:; Remove correlated variants from a matrix table. code:; >>> biallelic_mt = mt.filter_rows(hl.len(mt.alleles) == 2); >>> pruned_variant_table = hl.ld_prune(mt.GT, r2=0.2, bp_window_size=500000); >>> filtered_mt = mt.filter_rows(; ... hl.is_defined(pruned_variant_table[mt.row_key])). depen",MatchSource.WIKI,docs/0.2/guides/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/guides/genetics.html
https://hail.is/docs/0.2/guides/genetics.html:6339,Integrability,depend,dependencies,6339,"table of intervals; based on locus and interval<locus> matches. This is a StructExpression, which; will be defined if the locus was found in any interval, or missing if the locus; is outside all intervals.; To do our filtering, we can filter to the rows of our matrix table where the; struct expression interval_table[mt.locus] is defined.; This method will also work to filter a table of loci, as well as a matrix; table. From a UCSC BED file. description:; Import a UCSC BED file as a table of intervals, then use this; table to filter the loci in a matrix table. code:; >>> interval_table = hl.import_bed('data/file1.bed', reference_genome='GRCh37'); >>> filtered_mt = mt.filter_rows(hl.is_defined(interval_table[mt.locus])). dependencies:; import_bed(), MatrixTable.filter_rows(). Using hl.filter_intervals. description:; Filter using an interval table, suitable for a small list of; intervals. code:; >>> filtered_mt = hl.filter_intervals(mt, interval_table['interval'].collect()). dependencies:; methods.filter_intervals(). Declaring intervals with hl.parse_locus_interval. description:; Filter to declared intervals. code:; >>> intervals = ['1:100M-200M', '16:29.1M-30.2M', 'X']; >>> filtered_mt = hl.filter_intervals(; ... mt,; ... [hl.parse_locus_interval(x, reference_genome='GRCh37') for x in intervals]). dependencies:; methods.filter_intervals(), parse_locus_interval(). Pruning Variants in Linkage Disequilibrium. tags:; LD Prune. description:; Remove correlated variants from a matrix table. code:; >>> biallelic_mt = mt.filter_rows(hl.len(mt.alleles) == 2); >>> pruned_variant_table = hl.ld_prune(mt.GT, r2=0.2, bp_window_size=500000); >>> filtered_mt = mt.filter_rows(; ... hl.is_defined(pruned_variant_table[mt.row_key])). dependencies:; ld_prune(). understanding:. Hail’s ld_prune() method takes a matrix table and returns a table; with a subset of variants which are uncorrelated with each other. The method; requires a biallelic dataset, so we first filter our dataset to bia",MatchSource.WIKI,docs/0.2/guides/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/guides/genetics.html
https://hail.is/docs/0.2/guides/genetics.html:6670,Integrability,depend,dependencies,6670,"defined.; This method will also work to filter a table of loci, as well as a matrix; table. From a UCSC BED file. description:; Import a UCSC BED file as a table of intervals, then use this; table to filter the loci in a matrix table. code:; >>> interval_table = hl.import_bed('data/file1.bed', reference_genome='GRCh37'); >>> filtered_mt = mt.filter_rows(hl.is_defined(interval_table[mt.locus])). dependencies:; import_bed(), MatrixTable.filter_rows(). Using hl.filter_intervals. description:; Filter using an interval table, suitable for a small list of; intervals. code:; >>> filtered_mt = hl.filter_intervals(mt, interval_table['interval'].collect()). dependencies:; methods.filter_intervals(). Declaring intervals with hl.parse_locus_interval. description:; Filter to declared intervals. code:; >>> intervals = ['1:100M-200M', '16:29.1M-30.2M', 'X']; >>> filtered_mt = hl.filter_intervals(; ... mt,; ... [hl.parse_locus_interval(x, reference_genome='GRCh37') for x in intervals]). dependencies:; methods.filter_intervals(), parse_locus_interval(). Pruning Variants in Linkage Disequilibrium. tags:; LD Prune. description:; Remove correlated variants from a matrix table. code:; >>> biallelic_mt = mt.filter_rows(hl.len(mt.alleles) == 2); >>> pruned_variant_table = hl.ld_prune(mt.GT, r2=0.2, bp_window_size=500000); >>> filtered_mt = mt.filter_rows(; ... hl.is_defined(pruned_variant_table[mt.row_key])). dependencies:; ld_prune(). understanding:. Hail’s ld_prune() method takes a matrix table and returns a table; with a subset of variants which are uncorrelated with each other. The method; requires a biallelic dataset, so we first filter our dataset to biallelic; variants. Next, we get a table of independent variants using ld_prune(),; which we can use to filter the rows of our original dataset.; Note that it is more efficient to do the final filtering step on the original; dataset, rather than on the biallelic dataset, so that the biallelic dataset; does not need to be recomputed",MatchSource.WIKI,docs/0.2/guides/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/guides/genetics.html
https://hail.is/docs/0.2/guides/genetics.html:7095,Integrability,depend,dependencies,7095,", MatrixTable.filter_rows(). Using hl.filter_intervals. description:; Filter using an interval table, suitable for a small list of; intervals. code:; >>> filtered_mt = hl.filter_intervals(mt, interval_table['interval'].collect()). dependencies:; methods.filter_intervals(). Declaring intervals with hl.parse_locus_interval. description:; Filter to declared intervals. code:; >>> intervals = ['1:100M-200M', '16:29.1M-30.2M', 'X']; >>> filtered_mt = hl.filter_intervals(; ... mt,; ... [hl.parse_locus_interval(x, reference_genome='GRCh37') for x in intervals]). dependencies:; methods.filter_intervals(), parse_locus_interval(). Pruning Variants in Linkage Disequilibrium. tags:; LD Prune. description:; Remove correlated variants from a matrix table. code:; >>> biallelic_mt = mt.filter_rows(hl.len(mt.alleles) == 2); >>> pruned_variant_table = hl.ld_prune(mt.GT, r2=0.2, bp_window_size=500000); >>> filtered_mt = mt.filter_rows(; ... hl.is_defined(pruned_variant_table[mt.row_key])). dependencies:; ld_prune(). understanding:. Hail’s ld_prune() method takes a matrix table and returns a table; with a subset of variants which are uncorrelated with each other. The method; requires a biallelic dataset, so we first filter our dataset to biallelic; variants. Next, we get a table of independent variants using ld_prune(),; which we can use to filter the rows of our original dataset.; Note that it is more efficient to do the final filtering step on the original; dataset, rather than on the biallelic dataset, so that the biallelic dataset; does not need to be recomputed. Analysis. Linear Regression. Single Phenotype. tags:; Linear Regression. description:; Compute linear regression statistics for a single phenotype. code:; Approach #1: Use the linear_regression_rows() method; >>> ht = hl.linear_regression_rows(y=mt.pheno.height,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). Approach #2: Use the aggregators.linreg() aggregator; >>> mt_linreg = mt.annotate_rows(linreg=hl.agg.linreg",MatchSource.WIKI,docs/0.2/guides/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/guides/genetics.html
https://hail.is/docs/0.2/guides/genetics.html:8165,Integrability,depend,dependencies,8165,"urns a table; with a subset of variants which are uncorrelated with each other. The method; requires a biallelic dataset, so we first filter our dataset to biallelic; variants. Next, we get a table of independent variants using ld_prune(),; which we can use to filter the rows of our original dataset.; Note that it is more efficient to do the final filtering step on the original; dataset, rather than on the biallelic dataset, so that the biallelic dataset; does not need to be recomputed. Analysis. Linear Regression. Single Phenotype. tags:; Linear Regression. description:; Compute linear regression statistics for a single phenotype. code:; Approach #1: Use the linear_regression_rows() method; >>> ht = hl.linear_regression_rows(y=mt.pheno.height,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). Approach #2: Use the aggregators.linreg() aggregator; >>> mt_linreg = mt.annotate_rows(linreg=hl.agg.linreg(y=mt.pheno.height,; ... x=[1, mt.GT.n_alt_alleles()])). dependencies:; linear_regression_rows(), aggregators.linreg(). understanding:. The linear_regression_rows() method is more efficient than using the aggregators.linreg(); aggregator. However, the aggregators.linreg() aggregator is more flexible (multiple covariates; can vary by entry) and returns a richer set of statistics. Multiple Phenotypes. tags:; Linear Regression. description:; Compute linear regression statistics for multiple phenotypes. code:; Approach #1: Use the linear_regression_rows() method for all phenotypes simultaneously; >>> ht_result = hl.linear_regression_rows(y=[mt.pheno.height, mt.pheno.blood_pressure],; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). Approach #2: Use the linear_regression_rows() method for each phenotype sequentially; >>> ht1 = hl.linear_regression_rows(y=mt.pheno.height,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). >>> ht2 = hl.linear_regression_rows(y=mt.pheno.blood_pressure,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). Approach #3: Use the aggregators.l",MatchSource.WIKI,docs/0.2/guides/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/guides/genetics.html
https://hail.is/docs/0.2/guides/genetics.html:9426,Integrability,depend,dependencies,9426,"d returns a richer set of statistics. Multiple Phenotypes. tags:; Linear Regression. description:; Compute linear regression statistics for multiple phenotypes. code:; Approach #1: Use the linear_regression_rows() method for all phenotypes simultaneously; >>> ht_result = hl.linear_regression_rows(y=[mt.pheno.height, mt.pheno.blood_pressure],; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). Approach #2: Use the linear_regression_rows() method for each phenotype sequentially; >>> ht1 = hl.linear_regression_rows(y=mt.pheno.height,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). >>> ht2 = hl.linear_regression_rows(y=mt.pheno.blood_pressure,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). Approach #3: Use the aggregators.linreg() aggregator; >>> mt_linreg = mt.annotate_rows(; ... linreg_height=hl.agg.linreg(y=mt.pheno.height,; ... x=[1, mt.GT.n_alt_alleles()]),; ... linreg_bp=hl.agg.linreg(y=mt.pheno.blood_pressure,; ... x=[1, mt.GT.n_alt_alleles()])). dependencies:; linear_regression_rows(), aggregators.linreg(). understanding:. The linear_regression_rows() method is more efficient than using the aggregators.linreg(); aggregator, especially when analyzing many phenotypes. However, the aggregators.linreg(); aggregator is more flexible (multiple covariates can vary by entry) and returns a richer set of; statistics. The linear_regression_rows() method drops samples that have a missing value for; any of the phenotypes. Therefore, Approach #1 may not be suitable for phenotypes with differential; patterns of missingness. Approach #2 will do two passes over the data while Approaches #1 and #3 will; do one pass over the data and compute the regression statistics for each phenotype simultaneously. Using Variants (SNPs) as Covariates. tags:; sample genotypes covariate. description:; Use sample genotype dosage at specific variant(s) as covariates in regression routines. code:; Create a sample annotation from the genotype dosage for each variant of; interest by combini",MatchSource.WIKI,docs/0.2/guides/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/guides/genetics.html
https://hail.is/docs/0.2/guides/genetics.html:10343,Integrability,rout,routines,10343," x=[1, mt.GT.n_alt_alleles()]),; ... linreg_bp=hl.agg.linreg(y=mt.pheno.blood_pressure,; ... x=[1, mt.GT.n_alt_alleles()])). dependencies:; linear_regression_rows(), aggregators.linreg(). understanding:. The linear_regression_rows() method is more efficient than using the aggregators.linreg(); aggregator, especially when analyzing many phenotypes. However, the aggregators.linreg(); aggregator is more flexible (multiple covariates can vary by entry) and returns a richer set of; statistics. The linear_regression_rows() method drops samples that have a missing value for; any of the phenotypes. Therefore, Approach #1 may not be suitable for phenotypes with differential; patterns of missingness. Approach #2 will do two passes over the data while Approaches #1 and #3 will; do one pass over the data and compute the regression statistics for each phenotype simultaneously. Using Variants (SNPs) as Covariates. tags:; sample genotypes covariate. description:; Use sample genotype dosage at specific variant(s) as covariates in regression routines. code:; Create a sample annotation from the genotype dosage for each variant of; interest by combining the filter and collect aggregators:; >>> mt_annot = mt.annotate_cols(; ... snp1 = hl.agg.filter(hl.parse_variant('20:13714384:A:C') == mt.row_key,; ... hl.agg.collect(mt.GT.n_alt_alleles()))[0],; ... snp2 = hl.agg.filter(hl.parse_variant('20:17479730:T:C') == mt.row_key,; ... hl.agg.collect(mt.GT.n_alt_alleles()))[0]). Run the GWAS with linear_regression_rows() using variant dosages as covariates:; >>> gwas = hl.linear_regression_rows( ; ... x=mt_annot.GT.n_alt_alleles(),; ... y=mt_annot.pheno.blood_pressure,; ... covariates=[1, mt_annot.pheno.age, mt_annot.snp1, mt_annot.snp2]). dependencies:; linear_regression_rows(), aggregators.collect(), parse_variant(), variant_str(). Stratified by Group. tags:; Linear Regression. description:; Compute linear regression statistics for a single phenotype stratified by group. code:; Approach #1: ",MatchSource.WIKI,docs/0.2/guides/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/guides/genetics.html
https://hail.is/docs/0.2/guides/genetics.html:11042,Integrability,depend,dependencies,11042,"d #3 will; do one pass over the data and compute the regression statistics for each phenotype simultaneously. Using Variants (SNPs) as Covariates. tags:; sample genotypes covariate. description:; Use sample genotype dosage at specific variant(s) as covariates in regression routines. code:; Create a sample annotation from the genotype dosage for each variant of; interest by combining the filter and collect aggregators:; >>> mt_annot = mt.annotate_cols(; ... snp1 = hl.agg.filter(hl.parse_variant('20:13714384:A:C') == mt.row_key,; ... hl.agg.collect(mt.GT.n_alt_alleles()))[0],; ... snp2 = hl.agg.filter(hl.parse_variant('20:17479730:T:C') == mt.row_key,; ... hl.agg.collect(mt.GT.n_alt_alleles()))[0]). Run the GWAS with linear_regression_rows() using variant dosages as covariates:; >>> gwas = hl.linear_regression_rows( ; ... x=mt_annot.GT.n_alt_alleles(),; ... y=mt_annot.pheno.blood_pressure,; ... covariates=[1, mt_annot.pheno.age, mt_annot.snp1, mt_annot.snp2]). dependencies:; linear_regression_rows(), aggregators.collect(), parse_variant(), variant_str(). Stratified by Group. tags:; Linear Regression. description:; Compute linear regression statistics for a single phenotype stratified by group. code:; Approach #1: Use the linear_regression_rows() method for each group; >>> female_pheno = (hl.case(); ... .when(mt.pheno.is_female, mt.pheno.height); ... .or_missing()). >>> linreg_female = hl.linear_regression_rows(y=female_pheno,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). >>> male_pheno = (hl.case(); ... .when(~mt.pheno.is_female, mt.pheno.height); ... .or_missing()). >>> linreg_male = hl.linear_regression_rows(y=male_pheno,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). Approach #2: Use the aggregators.group_by() and aggregators.linreg() aggregators; >>> mt_linreg = mt.annotate_rows(; ... linreg=hl.agg.group_by(mt.pheno.is_female,; ... hl.agg.linreg(y=mt.pheno.height,; ... x=[1, mt.GT.n_alt_alleles()]))). dependencies:; linear_regression_rows(), aggregato",MatchSource.WIKI,docs/0.2/guides/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/guides/genetics.html
https://hail.is/docs/0.2/guides/genetics.html:12018,Integrability,depend,dependencies,12018,"pendencies:; linear_regression_rows(), aggregators.collect(), parse_variant(), variant_str(). Stratified by Group. tags:; Linear Regression. description:; Compute linear regression statistics for a single phenotype stratified by group. code:; Approach #1: Use the linear_regression_rows() method for each group; >>> female_pheno = (hl.case(); ... .when(mt.pheno.is_female, mt.pheno.height); ... .or_missing()). >>> linreg_female = hl.linear_regression_rows(y=female_pheno,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). >>> male_pheno = (hl.case(); ... .when(~mt.pheno.is_female, mt.pheno.height); ... .or_missing()). >>> linreg_male = hl.linear_regression_rows(y=male_pheno,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). Approach #2: Use the aggregators.group_by() and aggregators.linreg() aggregators; >>> mt_linreg = mt.annotate_rows(; ... linreg=hl.agg.group_by(mt.pheno.is_female,; ... hl.agg.linreg(y=mt.pheno.height,; ... x=[1, mt.GT.n_alt_alleles()]))). dependencies:; linear_regression_rows(), aggregators.group_by(), aggregators.linreg(). understanding:. We have presented two ways to compute linear regression statistics for each value of a grouping; variable. The first approach utilizes the linear_regression_rows() method and must be called; separately for each group even though it can compute statistics for multiple phenotypes; simultaneously. This is because the linear_regression_rows() method drops samples that have a; missing value for any of the phenotypes. When the groups are mutually exclusive,; such as ‘Male’ and ‘Female’, no samples remain! Note that we cannot define male_pheno = ~female_pheno; because we subsequently need male_pheno to be an expression on the mt_linreg matrix table; rather than mt. Lastly, the argument to root must be specified for both cases – otherwise; the ‘Male’ output will overwrite the ‘Female’ output.; The second approach uses the aggregators.group_by() and aggregators.linreg(); aggregators. The aggregation expression generate",MatchSource.WIKI,docs/0.2/guides/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/guides/genetics.html
https://hail.is/docs/0.2/guides/genetics.html:14579,Integrability,depend,dependencies,14579,"er than mt. Lastly, the argument to root must be specified for both cases – otherwise; the ‘Male’ output will overwrite the ‘Female’ output.; The second approach uses the aggregators.group_by() and aggregators.linreg(); aggregators. The aggregation expression generates a dictionary where a key is a group; (value of the grouping variable) and the corresponding value is the linear regression statistics; for those samples in the group. The result of the aggregation expression is then used to annotate; the matrix table.; The linear_regression_rows() method is more efficient than the aggregators.linreg(); aggregator and can be extended to multiple phenotypes, but the aggregators.linreg(); aggregator is more flexible (multiple covariates can be vary by entry) and returns a richer; set of statistics. PLINK Conversions. Polygenic Score Calculation. plink:; >>> plink --bfile data --score scores.txt sum . tags:; PRS. description:; This command is analogous to plink’s –score command with the; sum option. Biallelic variants are required. code:; >>> mt = hl.import_plink(; ... bed=""data/ldsc.bed"", bim=""data/ldsc.bim"", fam=""data/ldsc.fam"",; ... quant_pheno=True, missing='-9'); >>> mt = hl.variant_qc(mt); >>> scores = hl.import_table('data/scores.txt', delimiter=' ', key='rsid',; ... types={'score': hl.tfloat32}); >>> mt = mt.annotate_rows(**scores[mt.rsid]); >>> flip = hl.case().when(mt.allele == mt.alleles[0], True).when(; ... mt.allele == mt.alleles[1], False).or_missing(); >>> mt = mt.annotate_rows(flip=flip); >>> mt = mt.annotate_rows(; ... prior=2 * hl.if_else(mt.flip, mt.variant_qc.AF[0], mt.variant_qc.AF[1])); >>> mt = mt.annotate_cols(; ... prs=hl.agg.sum(; ... mt.score * hl.coalesce(; ... hl.if_else(mt.flip, 2 - mt.GT.n_alt_alleles(),; ... mt.GT.n_alt_alleles()), mt.prior))). dependencies:; import_plink(), variant_qc(), import_table(),; coalesce(), case(), cond(), Call.n_alt_alleles(). Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/guides/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/guides/genetics.html
https://hail.is/docs/0.2/guides/genetics.html:8400,Modifiability,flexible,flexible,8400,"; which we can use to filter the rows of our original dataset.; Note that it is more efficient to do the final filtering step on the original; dataset, rather than on the biallelic dataset, so that the biallelic dataset; does not need to be recomputed. Analysis. Linear Regression. Single Phenotype. tags:; Linear Regression. description:; Compute linear regression statistics for a single phenotype. code:; Approach #1: Use the linear_regression_rows() method; >>> ht = hl.linear_regression_rows(y=mt.pheno.height,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). Approach #2: Use the aggregators.linreg() aggregator; >>> mt_linreg = mt.annotate_rows(linreg=hl.agg.linreg(y=mt.pheno.height,; ... x=[1, mt.GT.n_alt_alleles()])). dependencies:; linear_regression_rows(), aggregators.linreg(). understanding:. The linear_regression_rows() method is more efficient than using the aggregators.linreg(); aggregator. However, the aggregators.linreg() aggregator is more flexible (multiple covariates; can vary by entry) and returns a richer set of statistics. Multiple Phenotypes. tags:; Linear Regression. description:; Compute linear regression statistics for multiple phenotypes. code:; Approach #1: Use the linear_regression_rows() method for all phenotypes simultaneously; >>> ht_result = hl.linear_regression_rows(y=[mt.pheno.height, mt.pheno.blood_pressure],; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). Approach #2: Use the linear_regression_rows() method for each phenotype sequentially; >>> ht1 = hl.linear_regression_rows(y=mt.pheno.height,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). >>> ht2 = hl.linear_regression_rows(y=mt.pheno.blood_pressure,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). Approach #3: Use the aggregators.linreg() aggregator; >>> mt_linreg = mt.annotate_rows(; ... linreg_height=hl.agg.linreg(y=mt.pheno.height,; ... x=[1, mt.GT.n_alt_alleles()]),; ... linreg_bp=hl.agg.linreg(y=mt.pheno.blood_pressure,; ... x=[1, mt.GT.n_alt_alleles()])). depen",MatchSource.WIKI,docs/0.2/guides/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/guides/genetics.html
https://hail.is/docs/0.2/guides/genetics.html:9705,Modifiability,flexible,flexible,9705,"regression_rows(y=[mt.pheno.height, mt.pheno.blood_pressure],; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). Approach #2: Use the linear_regression_rows() method for each phenotype sequentially; >>> ht1 = hl.linear_regression_rows(y=mt.pheno.height,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). >>> ht2 = hl.linear_regression_rows(y=mt.pheno.blood_pressure,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). Approach #3: Use the aggregators.linreg() aggregator; >>> mt_linreg = mt.annotate_rows(; ... linreg_height=hl.agg.linreg(y=mt.pheno.height,; ... x=[1, mt.GT.n_alt_alleles()]),; ... linreg_bp=hl.agg.linreg(y=mt.pheno.blood_pressure,; ... x=[1, mt.GT.n_alt_alleles()])). dependencies:; linear_regression_rows(), aggregators.linreg(). understanding:. The linear_regression_rows() method is more efficient than using the aggregators.linreg(); aggregator, especially when analyzing many phenotypes. However, the aggregators.linreg(); aggregator is more flexible (multiple covariates can vary by entry) and returns a richer set of; statistics. The linear_regression_rows() method drops samples that have a missing value for; any of the phenotypes. Therefore, Approach #1 may not be suitable for phenotypes with differential; patterns of missingness. Approach #2 will do two passes over the data while Approaches #1 and #3 will; do one pass over the data and compute the regression statistics for each phenotype simultaneously. Using Variants (SNPs) as Covariates. tags:; sample genotypes covariate. description:; Use sample genotype dosage at specific variant(s) as covariates in regression routines. code:; Create a sample annotation from the genotype dosage for each variant of; interest by combining the filter and collect aggregators:; >>> mt_annot = mt.annotate_cols(; ... snp1 = hl.agg.filter(hl.parse_variant('20:13714384:A:C') == mt.row_key,; ... hl.agg.collect(mt.GT.n_alt_alleles()))[0],; ... snp2 = hl.agg.filter(hl.parse_variant('20:17479730:T:C') == mt.row_key,; ... hl.",MatchSource.WIKI,docs/0.2/guides/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/guides/genetics.html
https://hail.is/docs/0.2/guides/genetics.html:12218,Modifiability,variab,variable,12218," Regression. description:; Compute linear regression statistics for a single phenotype stratified by group. code:; Approach #1: Use the linear_regression_rows() method for each group; >>> female_pheno = (hl.case(); ... .when(mt.pheno.is_female, mt.pheno.height); ... .or_missing()). >>> linreg_female = hl.linear_regression_rows(y=female_pheno,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). >>> male_pheno = (hl.case(); ... .when(~mt.pheno.is_female, mt.pheno.height); ... .or_missing()). >>> linreg_male = hl.linear_regression_rows(y=male_pheno,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). Approach #2: Use the aggregators.group_by() and aggregators.linreg() aggregators; >>> mt_linreg = mt.annotate_rows(; ... linreg=hl.agg.group_by(mt.pheno.is_female,; ... hl.agg.linreg(y=mt.pheno.height,; ... x=[1, mt.GT.n_alt_alleles()]))). dependencies:; linear_regression_rows(), aggregators.group_by(), aggregators.linreg(). understanding:. We have presented two ways to compute linear regression statistics for each value of a grouping; variable. The first approach utilizes the linear_regression_rows() method and must be called; separately for each group even though it can compute statistics for multiple phenotypes; simultaneously. This is because the linear_regression_rows() method drops samples that have a; missing value for any of the phenotypes. When the groups are mutually exclusive,; such as ‘Male’ and ‘Female’, no samples remain! Note that we cannot define male_pheno = ~female_pheno; because we subsequently need male_pheno to be an expression on the mt_linreg matrix table; rather than mt. Lastly, the argument to root must be specified for both cases – otherwise; the ‘Male’ output will overwrite the ‘Female’ output.; The second approach uses the aggregators.group_by() and aggregators.linreg(); aggregators. The aggregation expression generates a dictionary where a key is a group; (value of the grouping variable) and the corresponding value is the linear regression stati",MatchSource.WIKI,docs/0.2/guides/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/guides/genetics.html
https://hail.is/docs/0.2/guides/genetics.html:13106,Modifiability,variab,variable,13106,"standing:. We have presented two ways to compute linear regression statistics for each value of a grouping; variable. The first approach utilizes the linear_regression_rows() method and must be called; separately for each group even though it can compute statistics for multiple phenotypes; simultaneously. This is because the linear_regression_rows() method drops samples that have a; missing value for any of the phenotypes. When the groups are mutually exclusive,; such as ‘Male’ and ‘Female’, no samples remain! Note that we cannot define male_pheno = ~female_pheno; because we subsequently need male_pheno to be an expression on the mt_linreg matrix table; rather than mt. Lastly, the argument to root must be specified for both cases – otherwise; the ‘Male’ output will overwrite the ‘Female’ output.; The second approach uses the aggregators.group_by() and aggregators.linreg(); aggregators. The aggregation expression generates a dictionary where a key is a group; (value of the grouping variable) and the corresponding value is the linear regression statistics; for those samples in the group. The result of the aggregation expression is then used to annotate; the matrix table.; The linear_regression_rows() method is more efficient than the aggregators.linreg(); aggregator and can be extended to multiple phenotypes, but the aggregators.linreg(); aggregator is more flexible (multiple covariates can be vary by entry) and returns a richer; set of statistics. PLINK Conversions. Polygenic Score Calculation. plink:; >>> plink --bfile data --score scores.txt sum . tags:; PRS. description:; This command is analogous to plink’s –score command with the; sum option. Biallelic variants are required. code:; >>> mt = hl.import_plink(; ... bed=""data/ldsc.bed"", bim=""data/ldsc.bim"", fam=""data/ldsc.fam"",; ... quant_pheno=True, missing='-9'); >>> mt = hl.variant_qc(mt); >>> scores = hl.import_table('data/scores.txt', delimiter=' ', key='rsid',; ... types={'score': hl.tfloat32}); >>> mt = mt.",MatchSource.WIKI,docs/0.2/guides/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/guides/genetics.html
https://hail.is/docs/0.2/guides/genetics.html:13406,Modifiability,extend,extended,13406," This is because the linear_regression_rows() method drops samples that have a; missing value for any of the phenotypes. When the groups are mutually exclusive,; such as ‘Male’ and ‘Female’, no samples remain! Note that we cannot define male_pheno = ~female_pheno; because we subsequently need male_pheno to be an expression on the mt_linreg matrix table; rather than mt. Lastly, the argument to root must be specified for both cases – otherwise; the ‘Male’ output will overwrite the ‘Female’ output.; The second approach uses the aggregators.group_by() and aggregators.linreg(); aggregators. The aggregation expression generates a dictionary where a key is a group; (value of the grouping variable) and the corresponding value is the linear regression statistics; for those samples in the group. The result of the aggregation expression is then used to annotate; the matrix table.; The linear_regression_rows() method is more efficient than the aggregators.linreg(); aggregator and can be extended to multiple phenotypes, but the aggregators.linreg(); aggregator is more flexible (multiple covariates can be vary by entry) and returns a richer; set of statistics. PLINK Conversions. Polygenic Score Calculation. plink:; >>> plink --bfile data --score scores.txt sum . tags:; PRS. description:; This command is analogous to plink’s –score command with the; sum option. Biallelic variants are required. code:; >>> mt = hl.import_plink(; ... bed=""data/ldsc.bed"", bim=""data/ldsc.bim"", fam=""data/ldsc.fam"",; ... quant_pheno=True, missing='-9'); >>> mt = hl.variant_qc(mt); >>> scores = hl.import_table('data/scores.txt', delimiter=' ', key='rsid',; ... types={'score': hl.tfloat32}); >>> mt = mt.annotate_rows(**scores[mt.rsid]); >>> flip = hl.case().when(mt.allele == mt.alleles[0], True).when(; ... mt.allele == mt.alleles[1], False).or_missing(); >>> mt = mt.annotate_rows(flip=flip); >>> mt = mt.annotate_rows(; ... prior=2 * hl.if_else(mt.flip, mt.variant_qc.AF[0], mt.variant_qc.AF[1])); >>> mt = ",MatchSource.WIKI,docs/0.2/guides/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/guides/genetics.html
https://hail.is/docs/0.2/guides/genetics.html:13488,Modifiability,flexible,flexible,13488,"f the phenotypes. When the groups are mutually exclusive,; such as ‘Male’ and ‘Female’, no samples remain! Note that we cannot define male_pheno = ~female_pheno; because we subsequently need male_pheno to be an expression on the mt_linreg matrix table; rather than mt. Lastly, the argument to root must be specified for both cases – otherwise; the ‘Male’ output will overwrite the ‘Female’ output.; The second approach uses the aggregators.group_by() and aggregators.linreg(); aggregators. The aggregation expression generates a dictionary where a key is a group; (value of the grouping variable) and the corresponding value is the linear regression statistics; for those samples in the group. The result of the aggregation expression is then used to annotate; the matrix table.; The linear_regression_rows() method is more efficient than the aggregators.linreg(); aggregator and can be extended to multiple phenotypes, but the aggregators.linreg(); aggregator is more flexible (multiple covariates can be vary by entry) and returns a richer; set of statistics. PLINK Conversions. Polygenic Score Calculation. plink:; >>> plink --bfile data --score scores.txt sum . tags:; PRS. description:; This command is analogous to plink’s –score command with the; sum option. Biallelic variants are required. code:; >>> mt = hl.import_plink(; ... bed=""data/ldsc.bed"", bim=""data/ldsc.bim"", fam=""data/ldsc.fam"",; ... quant_pheno=True, missing='-9'); >>> mt = hl.variant_qc(mt); >>> scores = hl.import_table('data/scores.txt', delimiter=' ', key='rsid',; ... types={'score': hl.tfloat32}); >>> mt = mt.annotate_rows(**scores[mt.rsid]); >>> flip = hl.case().when(mt.allele == mt.alleles[0], True).when(; ... mt.allele == mt.alleles[1], False).or_missing(); >>> mt = mt.annotate_rows(flip=flip); >>> mt = mt.annotate_rows(; ... prior=2 * hl.if_else(mt.flip, mt.variant_qc.AF[0], mt.variant_qc.AF[1])); >>> mt = mt.annotate_cols(; ... prs=hl.agg.sum(; ... mt.score * hl.coalesce(; ... hl.if_else(mt.flip, 2 - mt.GT.",MatchSource.WIKI,docs/0.2/guides/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/guides/genetics.html
https://hail.is/docs/0.2/guides/genetics.html:904,Usability,guid,guides,904,"﻿. Hail | ; Genetics. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Aggregation; Annotation (Adding Fields); Genetics; Formatting; Convert variants in string format to separate locus and allele fields; Liftover variants from one coordinate system to another. Filtering and Pruning; Remove related individuals from a dataset; Filter loci by a list of locus intervals; Pruning Variants in Linkage Disequilibrium. Analysis; Linear Regression. PLINK Conversions; Polygenic Score Calculation. Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. How-To Guides; Genetics. View page source. Genetics; This page tailored how-to guides for small but commonly-used patterns; appearing in genetics pipelines. For documentation on the suite of; genetics functions built into Hail, see the genetics methods page. Formatting. Convert variants in string format to separate locus and allele fields. code:; >>> ht = ht.key_by(**hl.parse_variant(ht.variant)). dependencies:; parse_variant(), key_by(). understanding:. If your variants are strings of the format ‘chr:pos:ref:alt’, you may want; to convert them to separate locus and allele fields. This is useful if; you have imported a table with variants in string format and you would like to; join this table with other Hail tables that are keyed by locus and; alleles.; hl.parse_variant(ht.variant) constructs a StructExpression; containing two nested fields for the locus and alleles. The ** syntax unpacks; this struct so that the resulting table has two new fields, locus and; alleles. Liftover variants from one coordinate system to another. tags:; liftover. description:; Liftover a Table or MatrixTable from one reference genome to another. code:; First, we need to set up",MatchSource.WIKI,docs/0.2/guides/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/guides/genetics.html
https://hail.is/docs/0.2/install/azure.html:270,Deployability,install,installation,270,"﻿. Hail | ; Use Hail on Azure HDInsight. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Mac OS X; Linux; Google Dataproc; Azure HDInsight; Next Steps. Other Spark Clusters; After installation, try your first Hail query. Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Installing Hail; Use Hail on Azure HDInsight. View page source. Use Hail on Azure HDInsight; First, install Hail on your Mac OS X or Linux laptop or; desktop. The Hail pip package includes a tool called hailctl hdinsight which starts, stops, and; manipulates Hail-enabled HDInsight clusters.; Start an HDInsight cluster named “my-first-cluster”. Cluster names may only contain lowercase; letters, uppercase letter, and numbers. You must already have a storage account and resource; group.; hailctl hdinsight start MyFirstCluster MyStorageAccount MyResourceGroup. Be sure to record the generated http password so that you can access the cluster.; Create a file called “hail-script.py” and place the following analysis of a; randomly generated dataset with five-hundred samples and half-a-million; variants.; import hail as hl; mt = hl.balding_nichols_model(n_populations=3,; n_samples=500,; n_variants=500_000,; n_partitions=32); mt = mt.annotate_cols(drinks_coffee = hl.rand_bool(0.33)); gwas = hl.linear_regression_rows(y=mt.drinks_coffee,; x=mt.GT.n_alt_alleles(),; covariates=[1.0]); gwas.order_by(gwas.p_value).show(25). Submit the analysis to the cluster and wait for the results. You should not have; to wait more than a minute.; hailctl hdinsight submit MyFirstCluster MyStorageAccount HTTP_PASSWORD MyResourceGroup hail-script.py. When the script is done running you’ll see 25 rows of variant association; results.; You can also connect ",MatchSource.WIKI,docs/0.2/install/azure.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/install/azure.html
https://hail.is/docs/0.2/install/azure.html:656,Deployability,install,install,656,"﻿. Hail | ; Use Hail on Azure HDInsight. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Mac OS X; Linux; Google Dataproc; Azure HDInsight; Next Steps. Other Spark Clusters; After installation, try your first Hail query. Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Installing Hail; Use Hail on Azure HDInsight. View page source. Use Hail on Azure HDInsight; First, install Hail on your Mac OS X or Linux laptop or; desktop. The Hail pip package includes a tool called hailctl hdinsight which starts, stops, and; manipulates Hail-enabled HDInsight clusters.; Start an HDInsight cluster named “my-first-cluster”. Cluster names may only contain lowercase; letters, uppercase letter, and numbers. You must already have a storage account and resource; group.; hailctl hdinsight start MyFirstCluster MyStorageAccount MyResourceGroup. Be sure to record the generated http password so that you can access the cluster.; Create a file called “hail-script.py” and place the following analysis of a; randomly generated dataset with five-hundred samples and half-a-million; variants.; import hail as hl; mt = hl.balding_nichols_model(n_populations=3,; n_samples=500,; n_variants=500_000,; n_partitions=32); mt = mt.annotate_cols(drinks_coffee = hl.rand_bool(0.33)); gwas = hl.linear_regression_rows(y=mt.drinks_coffee,; x=mt.GT.n_alt_alleles(),; covariates=[1.0]); gwas.order_by(gwas.p_value).show(25). Submit the analysis to the cluster and wait for the results. You should not have; to wait more than a minute.; hailctl hdinsight submit MyFirstCluster MyStorageAccount HTTP_PASSWORD MyResourceGroup hail-script.py. When the script is done running you’ll see 25 rows of variant association; results.; You can also connect ",MatchSource.WIKI,docs/0.2/install/azure.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/install/azure.html
https://hail.is/docs/0.2/install/azure.html:2390,Deployability,update,updated,2390,"atsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Installing Hail; Use Hail on Azure HDInsight. View page source. Use Hail on Azure HDInsight; First, install Hail on your Mac OS X or Linux laptop or; desktop. The Hail pip package includes a tool called hailctl hdinsight which starts, stops, and; manipulates Hail-enabled HDInsight clusters.; Start an HDInsight cluster named “my-first-cluster”. Cluster names may only contain lowercase; letters, uppercase letter, and numbers. You must already have a storage account and resource; group.; hailctl hdinsight start MyFirstCluster MyStorageAccount MyResourceGroup. Be sure to record the generated http password so that you can access the cluster.; Create a file called “hail-script.py” and place the following analysis of a; randomly generated dataset with five-hundred samples and half-a-million; variants.; import hail as hl; mt = hl.balding_nichols_model(n_populations=3,; n_samples=500,; n_variants=500_000,; n_partitions=32); mt = mt.annotate_cols(drinks_coffee = hl.rand_bool(0.33)); gwas = hl.linear_regression_rows(y=mt.drinks_coffee,; x=mt.GT.n_alt_alleles(),; covariates=[1.0]); gwas.order_by(gwas.p_value).show(25). Submit the analysis to the cluster and wait for the results. You should not have; to wait more than a minute.; hailctl hdinsight submit MyFirstCluster MyStorageAccount HTTP_PASSWORD MyResourceGroup hail-script.py. When the script is done running you’ll see 25 rows of variant association; results.; You can also connect to a Jupyter Notebook running on the cluster at; https://MyFirstCluster.azurehdinisght.net/jupyter; When you are finished with the cluster stop it:; hailctl hdinsight stop MyFirstCluster MyStorageAccount MyResourceGroup. Next Steps. Read more about Hail on Azure HDInsight; Get the Hail cheatsheets; Follow the Hail GWAS Tutorial. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/install/azure.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/install/azure.html
https://hail.is/docs/0.2/install/azure.html:1156,Security,password,password,1156,". ; . Installation; Mac OS X; Linux; Google Dataproc; Azure HDInsight; Next Steps. Other Spark Clusters; After installation, try your first Hail query. Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Installing Hail; Use Hail on Azure HDInsight. View page source. Use Hail on Azure HDInsight; First, install Hail on your Mac OS X or Linux laptop or; desktop. The Hail pip package includes a tool called hailctl hdinsight which starts, stops, and; manipulates Hail-enabled HDInsight clusters.; Start an HDInsight cluster named “my-first-cluster”. Cluster names may only contain lowercase; letters, uppercase letter, and numbers. You must already have a storage account and resource; group.; hailctl hdinsight start MyFirstCluster MyStorageAccount MyResourceGroup. Be sure to record the generated http password so that you can access the cluster.; Create a file called “hail-script.py” and place the following analysis of a; randomly generated dataset with five-hundred samples and half-a-million; variants.; import hail as hl; mt = hl.balding_nichols_model(n_populations=3,; n_samples=500,; n_variants=500_000,; n_partitions=32); mt = mt.annotate_cols(drinks_coffee = hl.rand_bool(0.33)); gwas = hl.linear_regression_rows(y=mt.drinks_coffee,; x=mt.GT.n_alt_alleles(),; covariates=[1.0]); gwas.order_by(gwas.p_value).show(25). Submit the analysis to the cluster and wait for the results. You should not have; to wait more than a minute.; hailctl hdinsight submit MyFirstCluster MyStorageAccount HTTP_PASSWORD MyResourceGroup hail-script.py. When the script is done running you’ll see 25 rows of variant association; results.; You can also connect to a Jupyter Notebook running on the cluster at; https://MyFirstCluster.azurehdinisght.net/jupyter; When you are finished with the cluster stop it:; hailctl h",MatchSource.WIKI,docs/0.2/install/azure.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/install/azure.html
https://hail.is/docs/0.2/install/azure.html:1181,Security,access,access,1181,". ; . Installation; Mac OS X; Linux; Google Dataproc; Azure HDInsight; Next Steps. Other Spark Clusters; After installation, try your first Hail query. Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Installing Hail; Use Hail on Azure HDInsight. View page source. Use Hail on Azure HDInsight; First, install Hail on your Mac OS X or Linux laptop or; desktop. The Hail pip package includes a tool called hailctl hdinsight which starts, stops, and; manipulates Hail-enabled HDInsight clusters.; Start an HDInsight cluster named “my-first-cluster”. Cluster names may only contain lowercase; letters, uppercase letter, and numbers. You must already have a storage account and resource; group.; hailctl hdinsight start MyFirstCluster MyStorageAccount MyResourceGroup. Be sure to record the generated http password so that you can access the cluster.; Create a file called “hail-script.py” and place the following analysis of a; randomly generated dataset with five-hundred samples and half-a-million; variants.; import hail as hl; mt = hl.balding_nichols_model(n_populations=3,; n_samples=500,; n_variants=500_000,; n_partitions=32); mt = mt.annotate_cols(drinks_coffee = hl.rand_bool(0.33)); gwas = hl.linear_regression_rows(y=mt.drinks_coffee,; x=mt.GT.n_alt_alleles(),; covariates=[1.0]); gwas.order_by(gwas.p_value).show(25). Submit the analysis to the cluster and wait for the results. You should not have; to wait more than a minute.; hailctl hdinsight submit MyFirstCluster MyStorageAccount HTTP_PASSWORD MyResourceGroup hail-script.py. When the script is done running you’ll see 25 rows of variant association; results.; You can also connect to a Jupyter Notebook running on the cluster at; https://MyFirstCluster.azurehdinisght.net/jupyter; When you are finished with the cluster stop it:; hailctl h",MatchSource.WIKI,docs/0.2/install/azure.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/install/azure.html
https://hail.is/docs/0.2/install/dataproc.html:270,Deployability,install,installation,270,"﻿. Hail | ; Use Hail on Google Dataproc. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Mac OS X; Linux; Google Dataproc; Next Steps. Azure HDInsight; Other Spark Clusters; After installation, try your first Hail query. Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Installing Hail; Use Hail on Google Dataproc. View page source. Use Hail on Google Dataproc; First, install Hail on your Mac OS X or Linux laptop or; desktop. The Hail pip package includes a tool called hailctl dataproc which starts, stops, and; manipulates Hail-enabled Dataproc clusters.; Start a dataproc cluster named “my-first-cluster”. Cluster names may only; contain a mix lowercase letters and dashes. Starting a cluster can take as long; as two minutes.; hailctl dataproc start my-first-cluster. Create a file called “hail-script.py” and place the following analysis of a; randomly generated dataset with five-hundred samples and half-a-million; variants.; import hail as hl; mt = hl.balding_nichols_model(n_populations=3,; n_samples=500,; n_variants=500_000,; n_partitions=32); mt = mt.annotate_cols(drinks_coffee = hl.rand_bool(0.33)); gwas = hl.linear_regression_rows(y=mt.drinks_coffee,; x=mt.GT.n_alt_alleles(),; covariates=[1.0]); gwas.order_by(gwas.p_value).show(25). Submit the analysis to the cluster and wait for the results. You should not have; to wait more than a minute.; hailctl dataproc submit my-first-cluster hail-script.py. When the script is done running you’ll see 25 rows of variant association; results.; You can also start a Jupyter Notebook running on the cluster:; hailctl dataproc connect my-first-cluster notebook. When you are finished with the cluster stop it:; hailctl dataproc stop my-first-cluster. Next",MatchSource.WIKI,docs/0.2/install/dataproc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/install/dataproc.html
https://hail.is/docs/0.2/install/dataproc.html:656,Deployability,install,install,656,"﻿. Hail | ; Use Hail on Google Dataproc. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Mac OS X; Linux; Google Dataproc; Next Steps. Azure HDInsight; Other Spark Clusters; After installation, try your first Hail query. Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Installing Hail; Use Hail on Google Dataproc. View page source. Use Hail on Google Dataproc; First, install Hail on your Mac OS X or Linux laptop or; desktop. The Hail pip package includes a tool called hailctl dataproc which starts, stops, and; manipulates Hail-enabled Dataproc clusters.; Start a dataproc cluster named “my-first-cluster”. Cluster names may only; contain a mix lowercase letters and dashes. Starting a cluster can take as long; as two minutes.; hailctl dataproc start my-first-cluster. Create a file called “hail-script.py” and place the following analysis of a; randomly generated dataset with five-hundred samples and half-a-million; variants.; import hail as hl; mt = hl.balding_nichols_model(n_populations=3,; n_samples=500,; n_variants=500_000,; n_partitions=32); mt = mt.annotate_cols(drinks_coffee = hl.rand_bool(0.33)); gwas = hl.linear_regression_rows(y=mt.drinks_coffee,; x=mt.GT.n_alt_alleles(),; covariates=[1.0]); gwas.order_by(gwas.p_value).show(25). Submit the analysis to the cluster and wait for the results. You should not have; to wait more than a minute.; hailctl dataproc submit my-first-cluster hail-script.py. When the script is done running you’ll see 25 rows of variant association; results.; You can also start a Jupyter Notebook running on the cluster:; hailctl dataproc connect my-first-cluster notebook. When you are finished with the cluster stop it:; hailctl dataproc stop my-first-cluster. Next",MatchSource.WIKI,docs/0.2/install/dataproc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/install/dataproc.html
https://hail.is/docs/0.2/install/dataproc.html:2162,Deployability,update,updated,2162,"inux; Google Dataproc; Next Steps. Azure HDInsight; Other Spark Clusters; After installation, try your first Hail query. Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Installing Hail; Use Hail on Google Dataproc. View page source. Use Hail on Google Dataproc; First, install Hail on your Mac OS X or Linux laptop or; desktop. The Hail pip package includes a tool called hailctl dataproc which starts, stops, and; manipulates Hail-enabled Dataproc clusters.; Start a dataproc cluster named “my-first-cluster”. Cluster names may only; contain a mix lowercase letters and dashes. Starting a cluster can take as long; as two minutes.; hailctl dataproc start my-first-cluster. Create a file called “hail-script.py” and place the following analysis of a; randomly generated dataset with five-hundred samples and half-a-million; variants.; import hail as hl; mt = hl.balding_nichols_model(n_populations=3,; n_samples=500,; n_variants=500_000,; n_partitions=32); mt = mt.annotate_cols(drinks_coffee = hl.rand_bool(0.33)); gwas = hl.linear_regression_rows(y=mt.drinks_coffee,; x=mt.GT.n_alt_alleles(),; covariates=[1.0]); gwas.order_by(gwas.p_value).show(25). Submit the analysis to the cluster and wait for the results. You should not have; to wait more than a minute.; hailctl dataproc submit my-first-cluster hail-script.py. When the script is done running you’ll see 25 rows of variant association; results.; You can also start a Jupyter Notebook running on the cluster:; hailctl dataproc connect my-first-cluster notebook. When you are finished with the cluster stop it:; hailctl dataproc stop my-first-cluster. Next Steps. Read more about Hail on Google Cloud; Get the Hail cheatsheets; Follow the Hail GWAS Tutorial. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/install/dataproc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/install/dataproc.html
https://hail.is/docs/0.2/install/linux.html:256,Deployability,install,installation,256,"﻿. Hail | ; Install Hail on GNU/Linux. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Mac OS X; Linux; Google Dataproc; Azure HDInsight; Other Spark Clusters; After installation, try your first Hail query. Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Installing Hail; Install Hail on GNU/Linux. View page source. Install Hail on GNU/Linux. Install Java 11.; Install Python 3.9 or later.; Install a recent version of the C and C++ standard libraries. GCC 5.0, LLVM; version 3.4, or any later versions suffice.; Install BLAS and LAPACK.; Install Hail using pip. On a recent Debian-like system, the following should suffice:; apt-get install -y \; openjdk-11-jre-headless \; g++ \; python3.9 python3-pip \; libopenblas-base liblapack3; python3.9 -m pip install hail. Now let’s take Hail for a spin!. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/install/linux.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/install/linux.html
https://hail.is/docs/0.2/install/linux.html:922,Deployability,install,install,922,"﻿. Hail | ; Install Hail on GNU/Linux. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Mac OS X; Linux; Google Dataproc; Azure HDInsight; Other Spark Clusters; After installation, try your first Hail query. Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Installing Hail; Install Hail on GNU/Linux. View page source. Install Hail on GNU/Linux. Install Java 11.; Install Python 3.9 or later.; Install a recent version of the C and C++ standard libraries. GCC 5.0, LLVM; version 3.4, or any later versions suffice.; Install BLAS and LAPACK.; Install Hail using pip. On a recent Debian-like system, the following should suffice:; apt-get install -y \; openjdk-11-jre-headless \; g++ \; python3.9 python3-pip \; libopenblas-base liblapack3; python3.9 -m pip install hail. Now let’s take Hail for a spin!. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/install/linux.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/install/linux.html
https://hail.is/docs/0.2/install/linux.html:1041,Deployability,install,install,1041,"﻿. Hail | ; Install Hail on GNU/Linux. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Mac OS X; Linux; Google Dataproc; Azure HDInsight; Other Spark Clusters; After installation, try your first Hail query. Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Installing Hail; Install Hail on GNU/Linux. View page source. Install Hail on GNU/Linux. Install Java 11.; Install Python 3.9 or later.; Install a recent version of the C and C++ standard libraries. GCC 5.0, LLVM; version 3.4, or any later versions suffice.; Install BLAS and LAPACK.; Install Hail using pip. On a recent Debian-like system, the following should suffice:; apt-get install -y \; openjdk-11-jre-headless \; g++ \; python3.9 python3-pip \; libopenblas-base liblapack3; python3.9 -m pip install hail. Now let’s take Hail for a spin!. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/install/linux.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/install/linux.html
https://hail.is/docs/0.2/install/linux.html:1145,Deployability,update,updated,1145,"﻿. Hail | ; Install Hail on GNU/Linux. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Mac OS X; Linux; Google Dataproc; Azure HDInsight; Other Spark Clusters; After installation, try your first Hail query. Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Installing Hail; Install Hail on GNU/Linux. View page source. Install Hail on GNU/Linux. Install Java 11.; Install Python 3.9 or later.; Install a recent version of the C and C++ standard libraries. GCC 5.0, LLVM; version 3.4, or any later versions suffice.; Install BLAS and LAPACK.; Install Hail using pip. On a recent Debian-like system, the following should suffice:; apt-get install -y \; openjdk-11-jre-headless \; g++ \; python3.9 python3-pip \; libopenblas-base liblapack3; python3.9 -m pip install hail. Now let’s take Hail for a spin!. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/install/linux.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/install/linux.html
https://hail.is/docs/0.2/install/macosx.html:290,Deployability,install,installation,290,"﻿. Hail | ; Install Hail on Mac OS X. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Mac OS X; hailctl Autocompletion (Optional). Linux; Google Dataproc; Azure HDInsight; Other Spark Clusters; After installation, try your first Hail query. Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Installing Hail; Install Hail on Mac OS X. View page source. Install Hail on Mac OS X. Install Java 11. We recommend using a packaged installation from Azul; (make sure the OS version and architecture match your system) or using Homebrew:; brew tap homebrew/cask-versions; brew install --cask temurin8. You must pick a Java installation with a compatible architecture. If you have an Apple M1 or M2; you must use an “arm64” Java, otherwise you must use an “x86_64” Java. You can check if you have; an M1 or M2 either in the “Apple Menu > About This Mac” or by running uname -m Terminal.app. Install Python 3.9 or later. We recommend Miniconda.; Open Terminal.app and execute pip install hail. If this command fails with a message about “Rust”, please try this instead: pip install hail --only-binary=:all:.; Run your first Hail query!. hailctl Autocompletion (Optional). Install autocompletion with hailctl --install-completion zsh; Ensure this line is in your zsh config file (~/.zshrc) and then reload your terminal.; autoload -Uz compinit && compinit. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/install/macosx.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/install/macosx.html
https://hail.is/docs/0.2/install/macosx.html:710,Deployability,install,installation,710,"﻿. Hail | ; Install Hail on Mac OS X. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Mac OS X; hailctl Autocompletion (Optional). Linux; Google Dataproc; Azure HDInsight; Other Spark Clusters; After installation, try your first Hail query. Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Installing Hail; Install Hail on Mac OS X. View page source. Install Hail on Mac OS X. Install Java 11. We recommend using a packaged installation from Azul; (make sure the OS version and architecture match your system) or using Homebrew:; brew tap homebrew/cask-versions; brew install --cask temurin8. You must pick a Java installation with a compatible architecture. If you have an Apple M1 or M2; you must use an “arm64” Java, otherwise you must use an “x86_64” Java. You can check if you have; an M1 or M2 either in the “Apple Menu > About This Mac” or by running uname -m Terminal.app. Install Python 3.9 or later. We recommend Miniconda.; Open Terminal.app and execute pip install hail. If this command fails with a message about “Rust”, please try this instead: pip install hail --only-binary=:all:.; Run your first Hail query!. hailctl Autocompletion (Optional). Install autocompletion with hailctl --install-completion zsh; Ensure this line is in your zsh config file (~/.zshrc) and then reload your terminal.; autoload -Uz compinit && compinit. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/install/macosx.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/install/macosx.html
https://hail.is/docs/0.2/install/macosx.html:854,Deployability,install,install,854,"﻿. Hail | ; Install Hail on Mac OS X. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Mac OS X; hailctl Autocompletion (Optional). Linux; Google Dataproc; Azure HDInsight; Other Spark Clusters; After installation, try your first Hail query. Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Installing Hail; Install Hail on Mac OS X. View page source. Install Hail on Mac OS X. Install Java 11. We recommend using a packaged installation from Azul; (make sure the OS version and architecture match your system) or using Homebrew:; brew tap homebrew/cask-versions; brew install --cask temurin8. You must pick a Java installation with a compatible architecture. If you have an Apple M1 or M2; you must use an “arm64” Java, otherwise you must use an “x86_64” Java. You can check if you have; an M1 or M2 either in the “Apple Menu > About This Mac” or by running uname -m Terminal.app. Install Python 3.9 or later. We recommend Miniconda.; Open Terminal.app and execute pip install hail. If this command fails with a message about “Rust”, please try this instead: pip install hail --only-binary=:all:.; Run your first Hail query!. hailctl Autocompletion (Optional). Install autocompletion with hailctl --install-completion zsh; Ensure this line is in your zsh config file (~/.zshrc) and then reload your terminal.; autoload -Uz compinit && compinit. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/install/macosx.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/install/macosx.html
https://hail.is/docs/0.2/install/macosx.html:900,Deployability,install,installation,900,"﻿. Hail | ; Install Hail on Mac OS X. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Mac OS X; hailctl Autocompletion (Optional). Linux; Google Dataproc; Azure HDInsight; Other Spark Clusters; After installation, try your first Hail query. Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Installing Hail; Install Hail on Mac OS X. View page source. Install Hail on Mac OS X. Install Java 11. We recommend using a packaged installation from Azul; (make sure the OS version and architecture match your system) or using Homebrew:; brew tap homebrew/cask-versions; brew install --cask temurin8. You must pick a Java installation with a compatible architecture. If you have an Apple M1 or M2; you must use an “arm64” Java, otherwise you must use an “x86_64” Java. You can check if you have; an M1 or M2 either in the “Apple Menu > About This Mac” or by running uname -m Terminal.app. Install Python 3.9 or later. We recommend Miniconda.; Open Terminal.app and execute pip install hail. If this command fails with a message about “Rust”, please try this instead: pip install hail --only-binary=:all:.; Run your first Hail query!. hailctl Autocompletion (Optional). Install autocompletion with hailctl --install-completion zsh; Ensure this line is in your zsh config file (~/.zshrc) and then reload your terminal.; autoload -Uz compinit && compinit. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/install/macosx.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/install/macosx.html
https://hail.is/docs/0.2/install/macosx.html:1255,Deployability,install,install,1255,"﻿. Hail | ; Install Hail on Mac OS X. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Mac OS X; hailctl Autocompletion (Optional). Linux; Google Dataproc; Azure HDInsight; Other Spark Clusters; After installation, try your first Hail query. Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Installing Hail; Install Hail on Mac OS X. View page source. Install Hail on Mac OS X. Install Java 11. We recommend using a packaged installation from Azul; (make sure the OS version and architecture match your system) or using Homebrew:; brew tap homebrew/cask-versions; brew install --cask temurin8. You must pick a Java installation with a compatible architecture. If you have an Apple M1 or M2; you must use an “arm64” Java, otherwise you must use an “x86_64” Java. You can check if you have; an M1 or M2 either in the “Apple Menu > About This Mac” or by running uname -m Terminal.app. Install Python 3.9 or later. We recommend Miniconda.; Open Terminal.app and execute pip install hail. If this command fails with a message about “Rust”, please try this instead: pip install hail --only-binary=:all:.; Run your first Hail query!. hailctl Autocompletion (Optional). Install autocompletion with hailctl --install-completion zsh; Ensure this line is in your zsh config file (~/.zshrc) and then reload your terminal.; autoload -Uz compinit && compinit. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/install/macosx.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/install/macosx.html
https://hail.is/docs/0.2/install/macosx.html:1349,Deployability,install,install,1349,"﻿. Hail | ; Install Hail on Mac OS X. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Mac OS X; hailctl Autocompletion (Optional). Linux; Google Dataproc; Azure HDInsight; Other Spark Clusters; After installation, try your first Hail query. Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Installing Hail; Install Hail on Mac OS X. View page source. Install Hail on Mac OS X. Install Java 11. We recommend using a packaged installation from Azul; (make sure the OS version and architecture match your system) or using Homebrew:; brew tap homebrew/cask-versions; brew install --cask temurin8. You must pick a Java installation with a compatible architecture. If you have an Apple M1 or M2; you must use an “arm64” Java, otherwise you must use an “x86_64” Java. You can check if you have; an M1 or M2 either in the “Apple Menu > About This Mac” or by running uname -m Terminal.app. Install Python 3.9 or later. We recommend Miniconda.; Open Terminal.app and execute pip install hail. If this command fails with a message about “Rust”, please try this instead: pip install hail --only-binary=:all:.; Run your first Hail query!. hailctl Autocompletion (Optional). Install autocompletion with hailctl --install-completion zsh; Ensure this line is in your zsh config file (~/.zshrc) and then reload your terminal.; autoload -Uz compinit && compinit. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/install/macosx.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/install/macosx.html
https://hail.is/docs/0.2/install/macosx.html:1486,Deployability,install,install-completion,1486,"﻿. Hail | ; Install Hail on Mac OS X. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Mac OS X; hailctl Autocompletion (Optional). Linux; Google Dataproc; Azure HDInsight; Other Spark Clusters; After installation, try your first Hail query. Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Installing Hail; Install Hail on Mac OS X. View page source. Install Hail on Mac OS X. Install Java 11. We recommend using a packaged installation from Azul; (make sure the OS version and architecture match your system) or using Homebrew:; brew tap homebrew/cask-versions; brew install --cask temurin8. You must pick a Java installation with a compatible architecture. If you have an Apple M1 or M2; you must use an “arm64” Java, otherwise you must use an “x86_64” Java. You can check if you have; an M1 or M2 either in the “Apple Menu > About This Mac” or by running uname -m Terminal.app. Install Python 3.9 or later. We recommend Miniconda.; Open Terminal.app and execute pip install hail. If this command fails with a message about “Rust”, please try this instead: pip install hail --only-binary=:all:.; Run your first Hail query!. hailctl Autocompletion (Optional). Install autocompletion with hailctl --install-completion zsh; Ensure this line is in your zsh config file (~/.zshrc) and then reload your terminal.; autoload -Uz compinit && compinit. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/install/macosx.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/install/macosx.html
https://hail.is/docs/0.2/install/macosx.html:1689,Deployability,update,updated,1689,"﻿. Hail | ; Install Hail on Mac OS X. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Mac OS X; hailctl Autocompletion (Optional). Linux; Google Dataproc; Azure HDInsight; Other Spark Clusters; After installation, try your first Hail query. Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Installing Hail; Install Hail on Mac OS X. View page source. Install Hail on Mac OS X. Install Java 11. We recommend using a packaged installation from Azul; (make sure the OS version and architecture match your system) or using Homebrew:; brew tap homebrew/cask-versions; brew install --cask temurin8. You must pick a Java installation with a compatible architecture. If you have an Apple M1 or M2; you must use an “arm64” Java, otherwise you must use an “x86_64” Java. You can check if you have; an M1 or M2 either in the “Apple Menu > About This Mac” or by running uname -m Terminal.app. Install Python 3.9 or later. We recommend Miniconda.; Open Terminal.app and execute pip install hail. If this command fails with a message about “Rust”, please try this instead: pip install hail --only-binary=:all:.; Run your first Hail query!. hailctl Autocompletion (Optional). Install autocompletion with hailctl --install-completion zsh; Ensure this line is in your zsh config file (~/.zshrc) and then reload your terminal.; autoload -Uz compinit && compinit. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/install/macosx.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/install/macosx.html
https://hail.is/docs/0.2/install/macosx.html:1298,Integrability,message,message,1298,"﻿. Hail | ; Install Hail on Mac OS X. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Mac OS X; hailctl Autocompletion (Optional). Linux; Google Dataproc; Azure HDInsight; Other Spark Clusters; After installation, try your first Hail query. Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Installing Hail; Install Hail on Mac OS X. View page source. Install Hail on Mac OS X. Install Java 11. We recommend using a packaged installation from Azul; (make sure the OS version and architecture match your system) or using Homebrew:; brew tap homebrew/cask-versions; brew install --cask temurin8. You must pick a Java installation with a compatible architecture. If you have an Apple M1 or M2; you must use an “arm64” Java, otherwise you must use an “x86_64” Java. You can check if you have; an M1 or M2 either in the “Apple Menu > About This Mac” or by running uname -m Terminal.app. Install Python 3.9 or later. We recommend Miniconda.; Open Terminal.app and execute pip install hail. If this command fails with a message about “Rust”, please try this instead: pip install hail --only-binary=:all:.; Run your first Hail query!. hailctl Autocompletion (Optional). Install autocompletion with hailctl --install-completion zsh; Ensure this line is in your zsh config file (~/.zshrc) and then reload your terminal.; autoload -Uz compinit && compinit. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/install/macosx.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/install/macosx.html
https://hail.is/docs/0.2/install/macosx.html:1542,Modifiability,config,config,1542,"﻿. Hail | ; Install Hail on Mac OS X. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Mac OS X; hailctl Autocompletion (Optional). Linux; Google Dataproc; Azure HDInsight; Other Spark Clusters; After installation, try your first Hail query. Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Installing Hail; Install Hail on Mac OS X. View page source. Install Hail on Mac OS X. Install Java 11. We recommend using a packaged installation from Azul; (make sure the OS version and architecture match your system) or using Homebrew:; brew tap homebrew/cask-versions; brew install --cask temurin8. You must pick a Java installation with a compatible architecture. If you have an Apple M1 or M2; you must use an “arm64” Java, otherwise you must use an “x86_64” Java. You can check if you have; an M1 or M2 either in the “Apple Menu > About This Mac” or by running uname -m Terminal.app. Install Python 3.9 or later. We recommend Miniconda.; Open Terminal.app and execute pip install hail. If this command fails with a message about “Rust”, please try this instead: pip install hail --only-binary=:all:.; Run your first Hail query!. hailctl Autocompletion (Optional). Install autocompletion with hailctl --install-completion zsh; Ensure this line is in your zsh config file (~/.zshrc) and then reload your terminal.; autoload -Uz compinit && compinit. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/install/macosx.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/install/macosx.html
https://hail.is/docs/0.2/install/other-cluster.html:1351,Availability,down,downloads,1351,"n API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Installing Hail; Install Hail on a Spark Cluster. View page source. Install Hail on a Spark Cluster; If you are using Google Dataproc, please see these simpler instructions. If you; are using Azure HDInsight please see these simpler instructions.; Hail should work with any Spark 3.5.x cluster built with Scala 2.12.; Hail needs to be built from source on the leader node. Building Hail from source; requires:. Java 11 JDK.; Python 3.9 or later.; A recent C and a C++ compiler, GCC 5.0, LLVM 3.4, or later versions of either; suffice.; The LZ4 library.; BLAS and LAPACK. On a Debian-like system, the following should suffice:; apt-get update; apt-get install \; openjdk-11-jdk-headless \; g++ \; python3 python3-pip \; libopenblas-dev liblapack-dev \; liblz4-dev. The next block of commands downloads, builds, and installs Hail from source.; git clone https://github.com/hail-is/hail.git; cd hail/hail; make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.18 SPARK_VERSION=3.5.0. If you forget to install any of the requirements before running make install-on-cluster, it’s possible; to get into a bad state where make insists you don’t have a requirement that you have in fact installed.; Try doing make clean and then a fresh invocation of the make install-on-cluster line if this happens.; On every worker node of the cluster, you must install a BLAS and LAPACK library; such as the Intel MKL or OpenBLAS. On a Debian-like system you might try the; following on every worker node.; apt-get install libopenblas liblapack3. Hail is now installed! You can use ipython, python, and jupyter; notebook without any further configuration. We recommend against using the; pyspark command.; Let’s take Hail for a spin! Create a file called “hail-script.py” and place the; following analysis of a ",MatchSource.WIKI,docs/0.2/install/other-cluster.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/install/other-cluster.html
https://hail.is/docs/0.2/install/other-cluster.html:274,Deployability,install,installation,274,"﻿. Hail | ; Install Hail on a Spark Cluster. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Mac OS X; Linux; Google Dataproc; Azure HDInsight; Other Spark Clusters; Next Steps. After installation, try your first Hail query. Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Installing Hail; Install Hail on a Spark Cluster. View page source. Install Hail on a Spark Cluster; If you are using Google Dataproc, please see these simpler instructions. If you; are using Azure HDInsight please see these simpler instructions.; Hail should work with any Spark 3.5.x cluster built with Scala 2.12.; Hail needs to be built from source on the leader node. Building Hail from source; requires:. Java 11 JDK.; Python 3.9 or later.; A recent C and a C++ compiler, GCC 5.0, LLVM 3.4, or later versions of either; suffice.; The LZ4 library.; BLAS and LAPACK. On a Debian-like system, the following should suffice:; apt-get update; apt-get install \; openjdk-11-jdk-headless \; g++ \; python3 python3-pip \; libopenblas-dev liblapack-dev \; liblz4-dev. The next block of commands downloads, builds, and installs Hail from source.; git clone https://github.com/hail-is/hail.git; cd hail/hail; make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.18 SPARK_VERSION=3.5.0. If you forget to install any of the requirements before running make install-on-cluster, it’s possible; to get into a bad state where make insists you don’t have a requirement that you have in fact installed.; Try doing make clean and then a fresh invocation of the make install-on-cluster line if this happens.; On every worker node of the cluster, you must install a BLAS and LAPACK library; such as the Intel MKL or OpenBLAS. On a Debian-like sy",MatchSource.WIKI,docs/0.2/install/other-cluster.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/install/other-cluster.html
https://hail.is/docs/0.2/install/other-cluster.html:1195,Deployability,update,update,1195,"nsight; Other Spark Clusters; Next Steps. After installation, try your first Hail query. Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Installing Hail; Install Hail on a Spark Cluster. View page source. Install Hail on a Spark Cluster; If you are using Google Dataproc, please see these simpler instructions. If you; are using Azure HDInsight please see these simpler instructions.; Hail should work with any Spark 3.5.x cluster built with Scala 2.12.; Hail needs to be built from source on the leader node. Building Hail from source; requires:. Java 11 JDK.; Python 3.9 or later.; A recent C and a C++ compiler, GCC 5.0, LLVM 3.4, or later versions of either; suffice.; The LZ4 library.; BLAS and LAPACK. On a Debian-like system, the following should suffice:; apt-get update; apt-get install \; openjdk-11-jdk-headless \; g++ \; python3 python3-pip \; libopenblas-dev liblapack-dev \; liblz4-dev. The next block of commands downloads, builds, and installs Hail from source.; git clone https://github.com/hail-is/hail.git; cd hail/hail; make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.18 SPARK_VERSION=3.5.0. If you forget to install any of the requirements before running make install-on-cluster, it’s possible; to get into a bad state where make insists you don’t have a requirement that you have in fact installed.; Try doing make clean and then a fresh invocation of the make install-on-cluster line if this happens.; On every worker node of the cluster, you must install a BLAS and LAPACK library; such as the Intel MKL or OpenBLAS. On a Debian-like system you might try the; following on every worker node.; apt-get install libopenblas liblapack3. Hail is now installed! You can use ipython, python, and jupyter; notebook without any further configuration. We recommend against",MatchSource.WIKI,docs/0.2/install/other-cluster.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/install/other-cluster.html
https://hail.is/docs/0.2/install/other-cluster.html:1211,Deployability,install,install,1211,"nsight; Other Spark Clusters; Next Steps. After installation, try your first Hail query. Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Installing Hail; Install Hail on a Spark Cluster. View page source. Install Hail on a Spark Cluster; If you are using Google Dataproc, please see these simpler instructions. If you; are using Azure HDInsight please see these simpler instructions.; Hail should work with any Spark 3.5.x cluster built with Scala 2.12.; Hail needs to be built from source on the leader node. Building Hail from source; requires:. Java 11 JDK.; Python 3.9 or later.; A recent C and a C++ compiler, GCC 5.0, LLVM 3.4, or later versions of either; suffice.; The LZ4 library.; BLAS and LAPACK. On a Debian-like system, the following should suffice:; apt-get update; apt-get install \; openjdk-11-jdk-headless \; g++ \; python3 python3-pip \; libopenblas-dev liblapack-dev \; liblz4-dev. The next block of commands downloads, builds, and installs Hail from source.; git clone https://github.com/hail-is/hail.git; cd hail/hail; make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.18 SPARK_VERSION=3.5.0. If you forget to install any of the requirements before running make install-on-cluster, it’s possible; to get into a bad state where make insists you don’t have a requirement that you have in fact installed.; Try doing make clean and then a fresh invocation of the make install-on-cluster line if this happens.; On every worker node of the cluster, you must install a BLAS and LAPACK library; such as the Intel MKL or OpenBLAS. On a Debian-like system you might try the; following on every worker node.; apt-get install libopenblas liblapack3. Hail is now installed! You can use ipython, python, and jupyter; notebook without any further configuration. We recommend against",MatchSource.WIKI,docs/0.2/install/other-cluster.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/install/other-cluster.html
https://hail.is/docs/0.2/install/other-cluster.html:1374,Deployability,install,installs,1374,"n API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Installing Hail; Install Hail on a Spark Cluster. View page source. Install Hail on a Spark Cluster; If you are using Google Dataproc, please see these simpler instructions. If you; are using Azure HDInsight please see these simpler instructions.; Hail should work with any Spark 3.5.x cluster built with Scala 2.12.; Hail needs to be built from source on the leader node. Building Hail from source; requires:. Java 11 JDK.; Python 3.9 or later.; A recent C and a C++ compiler, GCC 5.0, LLVM 3.4, or later versions of either; suffice.; The LZ4 library.; BLAS and LAPACK. On a Debian-like system, the following should suffice:; apt-get update; apt-get install \; openjdk-11-jdk-headless \; g++ \; python3 python3-pip \; libopenblas-dev liblapack-dev \; liblz4-dev. The next block of commands downloads, builds, and installs Hail from source.; git clone https://github.com/hail-is/hail.git; cd hail/hail; make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.18 SPARK_VERSION=3.5.0. If you forget to install any of the requirements before running make install-on-cluster, it’s possible; to get into a bad state where make insists you don’t have a requirement that you have in fact installed.; Try doing make clean and then a fresh invocation of the make install-on-cluster line if this happens.; On every worker node of the cluster, you must install a BLAS and LAPACK library; such as the Intel MKL or OpenBLAS. On a Debian-like system you might try the; following on every worker node.; apt-get install libopenblas liblapack3. Hail is now installed! You can use ipython, python, and jupyter; notebook without any further configuration. We recommend against using the; pyspark command.; Let’s take Hail for a spin! Create a file called “hail-script.py” and place the; following analysis of a ",MatchSource.WIKI,docs/0.2/install/other-cluster.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/install/other-cluster.html
https://hail.is/docs/0.2/install/other-cluster.html:1468,Deployability,install,install-on-cluster,1468,"e Developers; Other Resources; Change Log And Version Policy. menu; Hail. Installing Hail; Install Hail on a Spark Cluster. View page source. Install Hail on a Spark Cluster; If you are using Google Dataproc, please see these simpler instructions. If you; are using Azure HDInsight please see these simpler instructions.; Hail should work with any Spark 3.5.x cluster built with Scala 2.12.; Hail needs to be built from source on the leader node. Building Hail from source; requires:. Java 11 JDK.; Python 3.9 or later.; A recent C and a C++ compiler, GCC 5.0, LLVM 3.4, or later versions of either; suffice.; The LZ4 library.; BLAS and LAPACK. On a Debian-like system, the following should suffice:; apt-get update; apt-get install \; openjdk-11-jdk-headless \; g++ \; python3 python3-pip \; libopenblas-dev liblapack-dev \; liblz4-dev. The next block of commands downloads, builds, and installs Hail from source.; git clone https://github.com/hail-is/hail.git; cd hail/hail; make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.18 SPARK_VERSION=3.5.0. If you forget to install any of the requirements before running make install-on-cluster, it’s possible; to get into a bad state where make insists you don’t have a requirement that you have in fact installed.; Try doing make clean and then a fresh invocation of the make install-on-cluster line if this happens.; On every worker node of the cluster, you must install a BLAS and LAPACK library; such as the Intel MKL or OpenBLAS. On a Debian-like system you might try the; following on every worker node.; apt-get install libopenblas liblapack3. Hail is now installed! You can use ipython, python, and jupyter; notebook without any further configuration. We recommend against using the; pyspark command.; Let’s take Hail for a spin! Create a file called “hail-script.py” and place the; following analysis of a randomly generated dataset with five-hundred samples and; half-a-million variants.; import hail as hl; mt = hl.balding_nich",MatchSource.WIKI,docs/0.2/install/other-cluster.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/install/other-cluster.html
https://hail.is/docs/0.2/install/other-cluster.html:1570,Deployability,install,install,1570,"er; If you are using Google Dataproc, please see these simpler instructions. If you; are using Azure HDInsight please see these simpler instructions.; Hail should work with any Spark 3.5.x cluster built with Scala 2.12.; Hail needs to be built from source on the leader node. Building Hail from source; requires:. Java 11 JDK.; Python 3.9 or later.; A recent C and a C++ compiler, GCC 5.0, LLVM 3.4, or later versions of either; suffice.; The LZ4 library.; BLAS and LAPACK. On a Debian-like system, the following should suffice:; apt-get update; apt-get install \; openjdk-11-jdk-headless \; g++ \; python3 python3-pip \; libopenblas-dev liblapack-dev \; liblz4-dev. The next block of commands downloads, builds, and installs Hail from source.; git clone https://github.com/hail-is/hail.git; cd hail/hail; make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.18 SPARK_VERSION=3.5.0. If you forget to install any of the requirements before running make install-on-cluster, it’s possible; to get into a bad state where make insists you don’t have a requirement that you have in fact installed.; Try doing make clean and then a fresh invocation of the make install-on-cluster line if this happens.; On every worker node of the cluster, you must install a BLAS and LAPACK library; such as the Intel MKL or OpenBLAS. On a Debian-like system you might try the; following on every worker node.; apt-get install libopenblas liblapack3. Hail is now installed! You can use ipython, python, and jupyter; notebook without any further configuration. We recommend against using the; pyspark command.; Let’s take Hail for a spin! Create a file called “hail-script.py” and place the; following analysis of a randomly generated dataset with five-hundred samples and; half-a-million variants.; import hail as hl; mt = hl.balding_nichols_model(n_populations=3,; n_samples=500,; n_variants=500_000,; n_partitions=32); mt = mt.annotate_cols(drinks_coffee = hl.rand_bool(0.33)); gwas = hl.linear_regression_ro",MatchSource.WIKI,docs/0.2/install/other-cluster.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/install/other-cluster.html
https://hail.is/docs/0.2/install/other-cluster.html:1622,Deployability,install,install-on-cluster,1622,"er; If you are using Google Dataproc, please see these simpler instructions. If you; are using Azure HDInsight please see these simpler instructions.; Hail should work with any Spark 3.5.x cluster built with Scala 2.12.; Hail needs to be built from source on the leader node. Building Hail from source; requires:. Java 11 JDK.; Python 3.9 or later.; A recent C and a C++ compiler, GCC 5.0, LLVM 3.4, or later versions of either; suffice.; The LZ4 library.; BLAS and LAPACK. On a Debian-like system, the following should suffice:; apt-get update; apt-get install \; openjdk-11-jdk-headless \; g++ \; python3 python3-pip \; libopenblas-dev liblapack-dev \; liblz4-dev. The next block of commands downloads, builds, and installs Hail from source.; git clone https://github.com/hail-is/hail.git; cd hail/hail; make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.18 SPARK_VERSION=3.5.0. If you forget to install any of the requirements before running make install-on-cluster, it’s possible; to get into a bad state where make insists you don’t have a requirement that you have in fact installed.; Try doing make clean and then a fresh invocation of the make install-on-cluster line if this happens.; On every worker node of the cluster, you must install a BLAS and LAPACK library; such as the Intel MKL or OpenBLAS. On a Debian-like system you might try the; following on every worker node.; apt-get install libopenblas liblapack3. Hail is now installed! You can use ipython, python, and jupyter; notebook without any further configuration. We recommend against using the; pyspark command.; Let’s take Hail for a spin! Create a file called “hail-script.py” and place the; following analysis of a randomly generated dataset with five-hundred samples and; half-a-million variants.; import hail as hl; mt = hl.balding_nichols_model(n_populations=3,; n_samples=500,; n_variants=500_000,; n_partitions=32); mt = mt.annotate_cols(drinks_coffee = hl.rand_bool(0.33)); gwas = hl.linear_regression_ro",MatchSource.WIKI,docs/0.2/install/other-cluster.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/install/other-cluster.html
https://hail.is/docs/0.2/install/other-cluster.html:1751,Deployability,install,installed,1751,"er; If you are using Google Dataproc, please see these simpler instructions. If you; are using Azure HDInsight please see these simpler instructions.; Hail should work with any Spark 3.5.x cluster built with Scala 2.12.; Hail needs to be built from source on the leader node. Building Hail from source; requires:. Java 11 JDK.; Python 3.9 or later.; A recent C and a C++ compiler, GCC 5.0, LLVM 3.4, or later versions of either; suffice.; The LZ4 library.; BLAS and LAPACK. On a Debian-like system, the following should suffice:; apt-get update; apt-get install \; openjdk-11-jdk-headless \; g++ \; python3 python3-pip \; libopenblas-dev liblapack-dev \; liblz4-dev. The next block of commands downloads, builds, and installs Hail from source.; git clone https://github.com/hail-is/hail.git; cd hail/hail; make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.18 SPARK_VERSION=3.5.0. If you forget to install any of the requirements before running make install-on-cluster, it’s possible; to get into a bad state where make insists you don’t have a requirement that you have in fact installed.; Try doing make clean and then a fresh invocation of the make install-on-cluster line if this happens.; On every worker node of the cluster, you must install a BLAS and LAPACK library; such as the Intel MKL or OpenBLAS. On a Debian-like system you might try the; following on every worker node.; apt-get install libopenblas liblapack3. Hail is now installed! You can use ipython, python, and jupyter; notebook without any further configuration. We recommend against using the; pyspark command.; Let’s take Hail for a spin! Create a file called “hail-script.py” and place the; following analysis of a randomly generated dataset with five-hundred samples and; half-a-million variants.; import hail as hl; mt = hl.balding_nichols_model(n_populations=3,; n_samples=500,; n_variants=500_000,; n_partitions=32); mt = mt.annotate_cols(drinks_coffee = hl.rand_bool(0.33)); gwas = hl.linear_regression_ro",MatchSource.WIKI,docs/0.2/install/other-cluster.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/install/other-cluster.html
https://hail.is/docs/0.2/install/other-cluster.html:1824,Deployability,install,install-on-cluster,1824," should work with any Spark 3.5.x cluster built with Scala 2.12.; Hail needs to be built from source on the leader node. Building Hail from source; requires:. Java 11 JDK.; Python 3.9 or later.; A recent C and a C++ compiler, GCC 5.0, LLVM 3.4, or later versions of either; suffice.; The LZ4 library.; BLAS and LAPACK. On a Debian-like system, the following should suffice:; apt-get update; apt-get install \; openjdk-11-jdk-headless \; g++ \; python3 python3-pip \; libopenblas-dev liblapack-dev \; liblz4-dev. The next block of commands downloads, builds, and installs Hail from source.; git clone https://github.com/hail-is/hail.git; cd hail/hail; make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.18 SPARK_VERSION=3.5.0. If you forget to install any of the requirements before running make install-on-cluster, it’s possible; to get into a bad state where make insists you don’t have a requirement that you have in fact installed.; Try doing make clean and then a fresh invocation of the make install-on-cluster line if this happens.; On every worker node of the cluster, you must install a BLAS and LAPACK library; such as the Intel MKL or OpenBLAS. On a Debian-like system you might try the; following on every worker node.; apt-get install libopenblas liblapack3. Hail is now installed! You can use ipython, python, and jupyter; notebook without any further configuration. We recommend against using the; pyspark command.; Let’s take Hail for a spin! Create a file called “hail-script.py” and place the; following analysis of a randomly generated dataset with five-hundred samples and; half-a-million variants.; import hail as hl; mt = hl.balding_nichols_model(n_populations=3,; n_samples=500,; n_variants=500_000,; n_partitions=32); mt = mt.annotate_cols(drinks_coffee = hl.rand_bool(0.33)); gwas = hl.linear_regression_rows(y=mt.drinks_coffee,; x=mt.GT.n_alt_alleles(),; covariates=[1.0]); gwas.order_by(gwas.p_value).show(25). Run the script and wait for the results. You shou",MatchSource.WIKI,docs/0.2/install/other-cluster.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/install/other-cluster.html
https://hail.is/docs/0.2/install/other-cluster.html:1912,Deployability,install,install,1912,"ader node. Building Hail from source; requires:. Java 11 JDK.; Python 3.9 or later.; A recent C and a C++ compiler, GCC 5.0, LLVM 3.4, or later versions of either; suffice.; The LZ4 library.; BLAS and LAPACK. On a Debian-like system, the following should suffice:; apt-get update; apt-get install \; openjdk-11-jdk-headless \; g++ \; python3 python3-pip \; libopenblas-dev liblapack-dev \; liblz4-dev. The next block of commands downloads, builds, and installs Hail from source.; git clone https://github.com/hail-is/hail.git; cd hail/hail; make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.18 SPARK_VERSION=3.5.0. If you forget to install any of the requirements before running make install-on-cluster, it’s possible; to get into a bad state where make insists you don’t have a requirement that you have in fact installed.; Try doing make clean and then a fresh invocation of the make install-on-cluster line if this happens.; On every worker node of the cluster, you must install a BLAS and LAPACK library; such as the Intel MKL or OpenBLAS. On a Debian-like system you might try the; following on every worker node.; apt-get install libopenblas liblapack3. Hail is now installed! You can use ipython, python, and jupyter; notebook without any further configuration. We recommend against using the; pyspark command.; Let’s take Hail for a spin! Create a file called “hail-script.py” and place the; following analysis of a randomly generated dataset with five-hundred samples and; half-a-million variants.; import hail as hl; mt = hl.balding_nichols_model(n_populations=3,; n_samples=500,; n_variants=500_000,; n_partitions=32); mt = mt.annotate_cols(drinks_coffee = hl.rand_bool(0.33)); gwas = hl.linear_regression_rows(y=mt.drinks_coffee,; x=mt.GT.n_alt_alleles(),; covariates=[1.0]); gwas.order_by(gwas.p_value).show(25). Run the script and wait for the results. You should not have to wait more than a; minute.; python3 hail-script.py. Slightly more configuration is necessary to ",MatchSource.WIKI,docs/0.2/install/other-cluster.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/install/other-cluster.html
https://hail.is/docs/0.2/install/other-cluster.html:2066,Deployability,install,install,2066,"f either; suffice.; The LZ4 library.; BLAS and LAPACK. On a Debian-like system, the following should suffice:; apt-get update; apt-get install \; openjdk-11-jdk-headless \; g++ \; python3 python3-pip \; libopenblas-dev liblapack-dev \; liblz4-dev. The next block of commands downloads, builds, and installs Hail from source.; git clone https://github.com/hail-is/hail.git; cd hail/hail; make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.18 SPARK_VERSION=3.5.0. If you forget to install any of the requirements before running make install-on-cluster, it’s possible; to get into a bad state where make insists you don’t have a requirement that you have in fact installed.; Try doing make clean and then a fresh invocation of the make install-on-cluster line if this happens.; On every worker node of the cluster, you must install a BLAS and LAPACK library; such as the Intel MKL or OpenBLAS. On a Debian-like system you might try the; following on every worker node.; apt-get install libopenblas liblapack3. Hail is now installed! You can use ipython, python, and jupyter; notebook without any further configuration. We recommend against using the; pyspark command.; Let’s take Hail for a spin! Create a file called “hail-script.py” and place the; following analysis of a randomly generated dataset with five-hundred samples and; half-a-million variants.; import hail as hl; mt = hl.balding_nichols_model(n_populations=3,; n_samples=500,; n_variants=500_000,; n_partitions=32); mt = mt.annotate_cols(drinks_coffee = hl.rand_bool(0.33)); gwas = hl.linear_regression_rows(y=mt.drinks_coffee,; x=mt.GT.n_alt_alleles(),; covariates=[1.0]); gwas.order_by(gwas.p_value).show(25). Run the script and wait for the results. You should not have to wait more than a; minute.; python3 hail-script.py. Slightly more configuration is necessary to spark-submit a Hail script:; HAIL_HOME=$(pip3 show hail | grep Location | awk -F' ' '{print $2 ""/hail""}'); spark-submit \; --jars $HAIL_HOME/hail-all-spar",MatchSource.WIKI,docs/0.2/install/other-cluster.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/install/other-cluster.html
https://hail.is/docs/0.2/install/other-cluster.html:2110,Deployability,install,installed,2110,"tem, the following should suffice:; apt-get update; apt-get install \; openjdk-11-jdk-headless \; g++ \; python3 python3-pip \; libopenblas-dev liblapack-dev \; liblz4-dev. The next block of commands downloads, builds, and installs Hail from source.; git clone https://github.com/hail-is/hail.git; cd hail/hail; make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.18 SPARK_VERSION=3.5.0. If you forget to install any of the requirements before running make install-on-cluster, it’s possible; to get into a bad state where make insists you don’t have a requirement that you have in fact installed.; Try doing make clean and then a fresh invocation of the make install-on-cluster line if this happens.; On every worker node of the cluster, you must install a BLAS and LAPACK library; such as the Intel MKL or OpenBLAS. On a Debian-like system you might try the; following on every worker node.; apt-get install libopenblas liblapack3. Hail is now installed! You can use ipython, python, and jupyter; notebook without any further configuration. We recommend against using the; pyspark command.; Let’s take Hail for a spin! Create a file called “hail-script.py” and place the; following analysis of a randomly generated dataset with five-hundred samples and; half-a-million variants.; import hail as hl; mt = hl.balding_nichols_model(n_populations=3,; n_samples=500,; n_variants=500_000,; n_partitions=32); mt = mt.annotate_cols(drinks_coffee = hl.rand_bool(0.33)); gwas = hl.linear_regression_rows(y=mt.drinks_coffee,; x=mt.GT.n_alt_alleles(),; covariates=[1.0]); gwas.order_by(gwas.p_value).show(25). Run the script and wait for the results. You should not have to wait more than a; minute.; python3 hail-script.py. Slightly more configuration is necessary to spark-submit a Hail script:; HAIL_HOME=$(pip3 show hail | grep Location | awk -F' ' '{print $2 ""/hail""}'); spark-submit \; --jars $HAIL_HOME/hail-all-spark.jar \; --conf spark.driver.extraClassPath=$HAIL_HOME/hail-all-spark.jar \",MatchSource.WIKI,docs/0.2/install/other-cluster.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/install/other-cluster.html
https://hail.is/docs/0.2/install/other-cluster.html:2192,Deployability,configurat,configuration,2192,"tem, the following should suffice:; apt-get update; apt-get install \; openjdk-11-jdk-headless \; g++ \; python3 python3-pip \; libopenblas-dev liblapack-dev \; liblz4-dev. The next block of commands downloads, builds, and installs Hail from source.; git clone https://github.com/hail-is/hail.git; cd hail/hail; make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.18 SPARK_VERSION=3.5.0. If you forget to install any of the requirements before running make install-on-cluster, it’s possible; to get into a bad state where make insists you don’t have a requirement that you have in fact installed.; Try doing make clean and then a fresh invocation of the make install-on-cluster line if this happens.; On every worker node of the cluster, you must install a BLAS and LAPACK library; such as the Intel MKL or OpenBLAS. On a Debian-like system you might try the; following on every worker node.; apt-get install libopenblas liblapack3. Hail is now installed! You can use ipython, python, and jupyter; notebook without any further configuration. We recommend against using the; pyspark command.; Let’s take Hail for a spin! Create a file called “hail-script.py” and place the; following analysis of a randomly generated dataset with five-hundred samples and; half-a-million variants.; import hail as hl; mt = hl.balding_nichols_model(n_populations=3,; n_samples=500,; n_variants=500_000,; n_partitions=32); mt = mt.annotate_cols(drinks_coffee = hl.rand_bool(0.33)); gwas = hl.linear_regression_rows(y=mt.drinks_coffee,; x=mt.GT.n_alt_alleles(),; covariates=[1.0]); gwas.order_by(gwas.p_value).show(25). Run the script and wait for the results. You should not have to wait more than a; minute.; python3 hail-script.py. Slightly more configuration is necessary to spark-submit a Hail script:; HAIL_HOME=$(pip3 show hail | grep Location | awk -F' ' '{print $2 ""/hail""}'); spark-submit \; --jars $HAIL_HOME/hail-all-spark.jar \; --conf spark.driver.extraClassPath=$HAIL_HOME/hail-all-spark.jar \",MatchSource.WIKI,docs/0.2/install/other-cluster.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/install/other-cluster.html
https://hail.is/docs/0.2/install/other-cluster.html:2893,Deployability,configurat,configuration,2893,"N=2.12.18 SPARK_VERSION=3.5.0. If you forget to install any of the requirements before running make install-on-cluster, it’s possible; to get into a bad state where make insists you don’t have a requirement that you have in fact installed.; Try doing make clean and then a fresh invocation of the make install-on-cluster line if this happens.; On every worker node of the cluster, you must install a BLAS and LAPACK library; such as the Intel MKL or OpenBLAS. On a Debian-like system you might try the; following on every worker node.; apt-get install libopenblas liblapack3. Hail is now installed! You can use ipython, python, and jupyter; notebook without any further configuration. We recommend against using the; pyspark command.; Let’s take Hail for a spin! Create a file called “hail-script.py” and place the; following analysis of a randomly generated dataset with five-hundred samples and; half-a-million variants.; import hail as hl; mt = hl.balding_nichols_model(n_populations=3,; n_samples=500,; n_variants=500_000,; n_partitions=32); mt = mt.annotate_cols(drinks_coffee = hl.rand_bool(0.33)); gwas = hl.linear_regression_rows(y=mt.drinks_coffee,; x=mt.GT.n_alt_alleles(),; covariates=[1.0]); gwas.order_by(gwas.p_value).show(25). Run the script and wait for the results. You should not have to wait more than a; minute.; python3 hail-script.py. Slightly more configuration is necessary to spark-submit a Hail script:; HAIL_HOME=$(pip3 show hail | grep Location | awk -F' ' '{print $2 ""/hail""}'); spark-submit \; --jars $HAIL_HOME/hail-all-spark.jar \; --conf spark.driver.extraClassPath=$HAIL_HOME/hail-all-spark.jar \; --conf spark.executor.extraClassPath=./hail-all-spark.jar \; --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \; --conf spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator \; hail-script.py. Next Steps. Get the Hail cheatsheets; Follow the Hail GWAS Tutorial. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/install/other-cluster.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/install/other-cluster.html
https://hail.is/docs/0.2/install/other-cluster.html:3494,Deployability,update,updated,3494,"N=2.12.18 SPARK_VERSION=3.5.0. If you forget to install any of the requirements before running make install-on-cluster, it’s possible; to get into a bad state where make insists you don’t have a requirement that you have in fact installed.; Try doing make clean and then a fresh invocation of the make install-on-cluster line if this happens.; On every worker node of the cluster, you must install a BLAS and LAPACK library; such as the Intel MKL or OpenBLAS. On a Debian-like system you might try the; following on every worker node.; apt-get install libopenblas liblapack3. Hail is now installed! You can use ipython, python, and jupyter; notebook without any further configuration. We recommend against using the; pyspark command.; Let’s take Hail for a spin! Create a file called “hail-script.py” and place the; following analysis of a randomly generated dataset with five-hundred samples and; half-a-million variants.; import hail as hl; mt = hl.balding_nichols_model(n_populations=3,; n_samples=500,; n_variants=500_000,; n_partitions=32); mt = mt.annotate_cols(drinks_coffee = hl.rand_bool(0.33)); gwas = hl.linear_regression_rows(y=mt.drinks_coffee,; x=mt.GT.n_alt_alleles(),; covariates=[1.0]); gwas.order_by(gwas.p_value).show(25). Run the script and wait for the results. You should not have to wait more than a; minute.; python3 hail-script.py. Slightly more configuration is necessary to spark-submit a Hail script:; HAIL_HOME=$(pip3 show hail | grep Location | awk -F' ' '{print $2 ""/hail""}'); spark-submit \; --jars $HAIL_HOME/hail-all-spark.jar \; --conf spark.driver.extraClassPath=$HAIL_HOME/hail-all-spark.jar \; --conf spark.executor.extraClassPath=./hail-all-spark.jar \; --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \; --conf spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator \; hail-script.py. Next Steps. Get the Hail cheatsheets; Follow the Hail GWAS Tutorial. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/install/other-cluster.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/install/other-cluster.html
https://hail.is/docs/0.2/install/other-cluster.html:2192,Modifiability,config,configuration,2192,"tem, the following should suffice:; apt-get update; apt-get install \; openjdk-11-jdk-headless \; g++ \; python3 python3-pip \; libopenblas-dev liblapack-dev \; liblz4-dev. The next block of commands downloads, builds, and installs Hail from source.; git clone https://github.com/hail-is/hail.git; cd hail/hail; make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.18 SPARK_VERSION=3.5.0. If you forget to install any of the requirements before running make install-on-cluster, it’s possible; to get into a bad state where make insists you don’t have a requirement that you have in fact installed.; Try doing make clean and then a fresh invocation of the make install-on-cluster line if this happens.; On every worker node of the cluster, you must install a BLAS and LAPACK library; such as the Intel MKL or OpenBLAS. On a Debian-like system you might try the; following on every worker node.; apt-get install libopenblas liblapack3. Hail is now installed! You can use ipython, python, and jupyter; notebook without any further configuration. We recommend against using the; pyspark command.; Let’s take Hail for a spin! Create a file called “hail-script.py” and place the; following analysis of a randomly generated dataset with five-hundred samples and; half-a-million variants.; import hail as hl; mt = hl.balding_nichols_model(n_populations=3,; n_samples=500,; n_variants=500_000,; n_partitions=32); mt = mt.annotate_cols(drinks_coffee = hl.rand_bool(0.33)); gwas = hl.linear_regression_rows(y=mt.drinks_coffee,; x=mt.GT.n_alt_alleles(),; covariates=[1.0]); gwas.order_by(gwas.p_value).show(25). Run the script and wait for the results. You should not have to wait more than a; minute.; python3 hail-script.py. Slightly more configuration is necessary to spark-submit a Hail script:; HAIL_HOME=$(pip3 show hail | grep Location | awk -F' ' '{print $2 ""/hail""}'); spark-submit \; --jars $HAIL_HOME/hail-all-spark.jar \; --conf spark.driver.extraClassPath=$HAIL_HOME/hail-all-spark.jar \",MatchSource.WIKI,docs/0.2/install/other-cluster.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/install/other-cluster.html
https://hail.is/docs/0.2/install/other-cluster.html:2893,Modifiability,config,configuration,2893,"N=2.12.18 SPARK_VERSION=3.5.0. If you forget to install any of the requirements before running make install-on-cluster, it’s possible; to get into a bad state where make insists you don’t have a requirement that you have in fact installed.; Try doing make clean and then a fresh invocation of the make install-on-cluster line if this happens.; On every worker node of the cluster, you must install a BLAS and LAPACK library; such as the Intel MKL or OpenBLAS. On a Debian-like system you might try the; following on every worker node.; apt-get install libopenblas liblapack3. Hail is now installed! You can use ipython, python, and jupyter; notebook without any further configuration. We recommend against using the; pyspark command.; Let’s take Hail for a spin! Create a file called “hail-script.py” and place the; following analysis of a randomly generated dataset with five-hundred samples and; half-a-million variants.; import hail as hl; mt = hl.balding_nichols_model(n_populations=3,; n_samples=500,; n_variants=500_000,; n_partitions=32); mt = mt.annotate_cols(drinks_coffee = hl.rand_bool(0.33)); gwas = hl.linear_regression_rows(y=mt.drinks_coffee,; x=mt.GT.n_alt_alleles(),; covariates=[1.0]); gwas.order_by(gwas.p_value).show(25). Run the script and wait for the results. You should not have to wait more than a; minute.; python3 hail-script.py. Slightly more configuration is necessary to spark-submit a Hail script:; HAIL_HOME=$(pip3 show hail | grep Location | awk -F' ' '{print $2 ""/hail""}'); spark-submit \; --jars $HAIL_HOME/hail-all-spark.jar \; --conf spark.driver.extraClassPath=$HAIL_HOME/hail-all-spark.jar \; --conf spark.executor.extraClassPath=./hail-all-spark.jar \; --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \; --conf spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator \; hail-script.py. Next Steps. Get the Hail cheatsheets; Follow the Hail GWAS Tutorial. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/install/other-cluster.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/install/other-cluster.html
https://hail.is/docs/0.2/install/other-cluster.html:712,Usability,simpl,simpler,712,"﻿. Hail | ; Install Hail on a Spark Cluster. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Mac OS X; Linux; Google Dataproc; Azure HDInsight; Other Spark Clusters; Next Steps. After installation, try your first Hail query. Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Installing Hail; Install Hail on a Spark Cluster. View page source. Install Hail on a Spark Cluster; If you are using Google Dataproc, please see these simpler instructions. If you; are using Azure HDInsight please see these simpler instructions.; Hail should work with any Spark 3.5.x cluster built with Scala 2.12.; Hail needs to be built from source on the leader node. Building Hail from source; requires:. Java 11 JDK.; Python 3.9 or later.; A recent C and a C++ compiler, GCC 5.0, LLVM 3.4, or later versions of either; suffice.; The LZ4 library.; BLAS and LAPACK. On a Debian-like system, the following should suffice:; apt-get update; apt-get install \; openjdk-11-jdk-headless \; g++ \; python3 python3-pip \; libopenblas-dev liblapack-dev \; liblz4-dev. The next block of commands downloads, builds, and installs Hail from source.; git clone https://github.com/hail-is/hail.git; cd hail/hail; make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.18 SPARK_VERSION=3.5.0. If you forget to install any of the requirements before running make install-on-cluster, it’s possible; to get into a bad state where make insists you don’t have a requirement that you have in fact installed.; Try doing make clean and then a fresh invocation of the make install-on-cluster line if this happens.; On every worker node of the cluster, you must install a BLAS and LAPACK library; such as the Intel MKL or OpenBLAS. On a Debian-like sy",MatchSource.WIKI,docs/0.2/install/other-cluster.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/install/other-cluster.html
https://hail.is/docs/0.2/install/other-cluster.html:785,Usability,simpl,simpler,785,"﻿. Hail | ; Install Hail on a Spark Cluster. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Mac OS X; Linux; Google Dataproc; Azure HDInsight; Other Spark Clusters; Next Steps. After installation, try your first Hail query. Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Installing Hail; Install Hail on a Spark Cluster. View page source. Install Hail on a Spark Cluster; If you are using Google Dataproc, please see these simpler instructions. If you; are using Azure HDInsight please see these simpler instructions.; Hail should work with any Spark 3.5.x cluster built with Scala 2.12.; Hail needs to be built from source on the leader node. Building Hail from source; requires:. Java 11 JDK.; Python 3.9 or later.; A recent C and a C++ compiler, GCC 5.0, LLVM 3.4, or later versions of either; suffice.; The LZ4 library.; BLAS and LAPACK. On a Debian-like system, the following should suffice:; apt-get update; apt-get install \; openjdk-11-jdk-headless \; g++ \; python3 python3-pip \; libopenblas-dev liblapack-dev \; liblz4-dev. The next block of commands downloads, builds, and installs Hail from source.; git clone https://github.com/hail-is/hail.git; cd hail/hail; make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.18 SPARK_VERSION=3.5.0. If you forget to install any of the requirements before running make install-on-cluster, it’s possible; to get into a bad state where make insists you don’t have a requirement that you have in fact installed.; Try doing make clean and then a fresh invocation of the make install-on-cluster line if this happens.; On every worker node of the cluster, you must install a BLAS and LAPACK library; such as the Intel MKL or OpenBLAS. On a Debian-like sy",MatchSource.WIKI,docs/0.2/install/other-cluster.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/install/other-cluster.html
https://hail.is/docs/0.2/install/try.html:252,Deployability,install,installation,252,"﻿. Hail | ; Your First Hail Query. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Mac OS X; Linux; Google Dataproc; Azure HDInsight; Other Spark Clusters; After installation, try your first Hail query; Next Steps. Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Installing Hail; Your First Hail Query. View page source. Your First Hail Query; We recommend using IPython, a super-powered Python terminal:; pip install ipython. Start an IPython session by copy-pasting the below into your Terminal.; ipython. Let’s randomly generate a dataset according to the Balding-Nichols; Model. The dataset has one-hundred variants and ten samples from three; populations.; import hail as hl; mt = hl.balding_nichols_model(n_populations=3,; n_samples=10,; n_variants=100); mt.show(). The last line, mt.show(), displays the dataset in a tabular form.; 2020-05-09 19:08:07 Hail: INFO: Coerced sorted dataset; +---------------+------------+------+------+------+------+; | locus | alleles | 0.GT | 1.GT | 2.GT | 3.GT |; +---------------+------------+------+------+------+------+; | locus<GRCh37> | array<str> | call | call | call | call |; +---------------+------------+------+------+------+------+; | 1:1 | [""A"",""C""] | 0/1 | 1/1 | 0/1 | 0/1 |; | 1:2 | [""A"",""C""] | 1/1 | 0/1 | 1/1 | 0/1 |; | 1:3 | [""A"",""C""] | 0/1 | 1/1 | 1/1 | 1/1 |; | 1:4 | [""A"",""C""] | 0/0 | 0/0 | 0/1 | 1/1 |; | 1:5 | [""A"",""C""] | 0/1 | 0/0 | 0/1 | 0/0 |; | 1:6 | [""A"",""C""] | 1/1 | 0/1 | 0/1 | 0/1 |; | 1:7 | [""A"",""C""] | 0/0 | 0/1 | 0/1 | 0/0 |; | 1:8 | [""A"",""C""] | 1/1 | 0/1 | 1/1 | 1/1 |; | 1:9 | [""A"",""C""] | 1/1 | 1/1 | 1/1 | 1/1 |; | 1:10 | [""A"",""C""] | 1/1 | 0/1 | 1/1 | 0/1 |; | 1:11 | [""A"",""C""] | 0/1 | 1/1 | 1/1 | 0/1 |; +---------------+------------+---",MatchSource.WIKI,docs/0.2/install/try.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/install/try.html
https://hail.is/docs/0.2/install/try.html:697,Deployability,install,install,697,"﻿. Hail | ; Your First Hail Query. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Mac OS X; Linux; Google Dataproc; Azure HDInsight; Other Spark Clusters; After installation, try your first Hail query; Next Steps. Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Installing Hail; Your First Hail Query. View page source. Your First Hail Query; We recommend using IPython, a super-powered Python terminal:; pip install ipython. Start an IPython session by copy-pasting the below into your Terminal.; ipython. Let’s randomly generate a dataset according to the Balding-Nichols; Model. The dataset has one-hundred variants and ten samples from three; populations.; import hail as hl; mt = hl.balding_nichols_model(n_populations=3,; n_samples=10,; n_variants=100); mt.show(). The last line, mt.show(), displays the dataset in a tabular form.; 2020-05-09 19:08:07 Hail: INFO: Coerced sorted dataset; +---------------+------------+------+------+------+------+; | locus | alleles | 0.GT | 1.GT | 2.GT | 3.GT |; +---------------+------------+------+------+------+------+; | locus<GRCh37> | array<str> | call | call | call | call |; +---------------+------------+------+------+------+------+; | 1:1 | [""A"",""C""] | 0/1 | 1/1 | 0/1 | 0/1 |; | 1:2 | [""A"",""C""] | 1/1 | 0/1 | 1/1 | 0/1 |; | 1:3 | [""A"",""C""] | 0/1 | 1/1 | 1/1 | 1/1 |; | 1:4 | [""A"",""C""] | 0/0 | 0/0 | 0/1 | 1/1 |; | 1:5 | [""A"",""C""] | 0/1 | 0/0 | 0/1 | 0/0 |; | 1:6 | [""A"",""C""] | 1/1 | 0/1 | 0/1 | 0/1 |; | 1:7 | [""A"",""C""] | 0/0 | 0/1 | 0/1 | 0/0 |; | 1:8 | [""A"",""C""] | 1/1 | 0/1 | 1/1 | 1/1 |; | 1:9 | [""A"",""C""] | 1/1 | 1/1 | 1/1 | 1/1 |; | 1:10 | [""A"",""C""] | 1/1 | 0/1 | 1/1 | 0/1 |; | 1:11 | [""A"",""C""] | 0/1 | 1/1 | 1/1 | 0/1 |; +---------------+------------+---",MatchSource.WIKI,docs/0.2/install/try.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/install/try.html
https://hail.is/docs/0.2/install/try.html:2263,Deployability,update,updated,2263,"; Next Steps. Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Installing Hail; Your First Hail Query. View page source. Your First Hail Query; We recommend using IPython, a super-powered Python terminal:; pip install ipython. Start an IPython session by copy-pasting the below into your Terminal.; ipython. Let’s randomly generate a dataset according to the Balding-Nichols; Model. The dataset has one-hundred variants and ten samples from three; populations.; import hail as hl; mt = hl.balding_nichols_model(n_populations=3,; n_samples=10,; n_variants=100); mt.show(). The last line, mt.show(), displays the dataset in a tabular form.; 2020-05-09 19:08:07 Hail: INFO: Coerced sorted dataset; +---------------+------------+------+------+------+------+; | locus | alleles | 0.GT | 1.GT | 2.GT | 3.GT |; +---------------+------------+------+------+------+------+; | locus<GRCh37> | array<str> | call | call | call | call |; +---------------+------------+------+------+------+------+; | 1:1 | [""A"",""C""] | 0/1 | 1/1 | 0/1 | 0/1 |; | 1:2 | [""A"",""C""] | 1/1 | 0/1 | 1/1 | 0/1 |; | 1:3 | [""A"",""C""] | 0/1 | 1/1 | 1/1 | 1/1 |; | 1:4 | [""A"",""C""] | 0/0 | 0/0 | 0/1 | 1/1 |; | 1:5 | [""A"",""C""] | 0/1 | 0/0 | 0/1 | 0/0 |; | 1:6 | [""A"",""C""] | 1/1 | 0/1 | 0/1 | 0/1 |; | 1:7 | [""A"",""C""] | 0/0 | 0/1 | 0/1 | 0/0 |; | 1:8 | [""A"",""C""] | 1/1 | 0/1 | 1/1 | 1/1 |; | 1:9 | [""A"",""C""] | 1/1 | 1/1 | 1/1 | 1/1 |; | 1:10 | [""A"",""C""] | 1/1 | 0/1 | 1/1 | 0/1 |; | 1:11 | [""A"",""C""] | 0/1 | 1/1 | 1/1 | 0/1 |; +---------------+------------+------+------+------+------+; showing top 11 rows; showing the first 4 of 10 columns</code></pre>. Next Steps. Get the Hail cheatsheets; Follow the Hail GWAS Tutorial; Learn how to use Hail on Google Cloud. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/install/try.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/install/try.html
https://hail.is/docs/0.2/install/try.html:667,Energy Efficiency,power,powered,667,"﻿. Hail | ; Your First Hail Query. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Mac OS X; Linux; Google Dataproc; Azure HDInsight; Other Spark Clusters; After installation, try your first Hail query; Next Steps. Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Installing Hail; Your First Hail Query. View page source. Your First Hail Query; We recommend using IPython, a super-powered Python terminal:; pip install ipython. Start an IPython session by copy-pasting the below into your Terminal.; ipython. Let’s randomly generate a dataset according to the Balding-Nichols; Model. The dataset has one-hundred variants and ten samples from three; populations.; import hail as hl; mt = hl.balding_nichols_model(n_populations=3,; n_samples=10,; n_variants=100); mt.show(). The last line, mt.show(), displays the dataset in a tabular form.; 2020-05-09 19:08:07 Hail: INFO: Coerced sorted dataset; +---------------+------------+------+------+------+------+; | locus | alleles | 0.GT | 1.GT | 2.GT | 3.GT |; +---------------+------------+------+------+------+------+; | locus<GRCh37> | array<str> | call | call | call | call |; +---------------+------------+------+------+------+------+; | 1:1 | [""A"",""C""] | 0/1 | 1/1 | 0/1 | 0/1 |; | 1:2 | [""A"",""C""] | 1/1 | 0/1 | 1/1 | 0/1 |; | 1:3 | [""A"",""C""] | 0/1 | 1/1 | 1/1 | 1/1 |; | 1:4 | [""A"",""C""] | 0/0 | 0/0 | 0/1 | 1/1 |; | 1:5 | [""A"",""C""] | 0/1 | 0/0 | 0/1 | 0/0 |; | 1:6 | [""A"",""C""] | 1/1 | 0/1 | 0/1 | 0/1 |; | 1:7 | [""A"",""C""] | 0/0 | 0/1 | 0/1 | 0/0 |; | 1:8 | [""A"",""C""] | 1/1 | 0/1 | 1/1 | 1/1 |; | 1:9 | [""A"",""C""] | 1/1 | 1/1 | 1/1 | 1/1 |; | 1:10 | [""A"",""C""] | 1/1 | 0/1 | 1/1 | 0/1 |; | 1:11 | [""A"",""C""] | 0/1 | 1/1 | 1/1 | 0/1 |; +---------------+------------+---",MatchSource.WIKI,docs/0.2/install/try.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/install/try.html
https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:3539,Availability,resilien,resilience,3539,"gh the same; effect can be achieved for * by using @. Warning; For binary operations, if the first operand is an ndarray and the; second operand is a block matrix, the result will be a ndarray of block; matrices. To achieve the desired behavior for + and *, place the; block matrix operand first; for -, /, and @, first convert; the ndarray to a block matrix using from_numpy(). Warning; Block matrix multiplication requires special care due to each block; of each operand being a dependency of multiple blocks in the product.; The \((i, j)\)-block in the product a @ b is computed by summing; the products of corresponding blocks in block row \(i\) of a and; block column \(j\) of b. So overall, in addition to this; multiplication and addition, the evaluation of a @ b realizes each; block of a as many times as the number of block columns of b; and realizes each block of b as many times as the number of; block rows of a.; This becomes a performance and resilience issue whenever a or b; is defined in terms of pending transformations (such as linear; algebra operations). For example, evaluating a @ (c @ d) will; effectively evaluate c @ d as many times as the number of block rows; in a.; To limit re-computation, write or cache transformed block matrix; operands before feeding them into matrix multiplication:; >>> c = BlockMatrix.read('c.bm') ; >>> d = BlockMatrix.read('d.bm') ; >>> (c @ d).write('cd.bm') ; >>> a = BlockMatrix.read('a.bm') ; >>> e = a @ BlockMatrix.read('cd.bm') . Indexing and slicing; Block matrices also support NumPy-style 2-dimensional; indexing and slicing,; with two differences.; First, slices start:stop:step must be non-empty with positive step.; Second, even if only one index is a slice, the resulting block matrix is still; 2-dimensional.; For example, for a block matrix bm with 10 rows and 10 columns:. bm[0, 0] is the element in row 0 and column 0 of bm.; bm[0:1, 0] is a block matrix with 1 row, 1 column,; and element bm[0, 0].; bm[2, :] is a block matri",MatchSource.WIKI,docs/0.2/linalg/hail.linalg.BlockMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html
https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:5612,Availability,down,downstream,5612,"ix with 1 row, 1 column,; and element bm[0, 0].; bm[2, :] is a block matrix with 1 row, 10 columns,; and elements from row 2 of bm.; bm[:3, -1] is a block matrix with 3 rows, 1 column,; and the first 3 elements of the last column of bm.; bm[::2, ::2] is a block matrix with 5 rows, 5 columns,; and all evenly-indexed elements of bm. Use filter(), filter_rows(), and filter_cols() to; subset to non-slice subsets of rows and columns, e.g. to rows [0, 2, 5].; Block-sparse representation; By default, block matrices compute and store all blocks explicitly.; However, some applications involve block matrices in which:. some blocks consist entirely of zeroes.; some blocks are not of interest. For example, statistical geneticists often want to compute and manipulate a; banded correlation matrix capturing “linkage disequilibrium” between nearby; variants along the genome. In this case, working with the full correlation; matrix for tens of millions of variants would be prohibitively expensive,; and in any case, entries far from the diagonal are either not of interest or; ought to be zeroed out before downstream linear algebra.; To enable such computations, block matrices do not require that all blocks; be realized explicitly. Implicit (dropped) blocks behave as blocks of; zeroes, so we refer to a block matrix in which at least one block is; implicitly zero as a block-sparse matrix. Otherwise, we say the matrix; is block-dense. The property is_sparse() encodes this state.; Dropped blocks are not stored in memory or on write(). In fact,; blocks that are dropped prior to an action like export() or; to_numpy() are never computed in the first place, nor are any blocks; of upstream operands on which only dropped blocks depend! In addition,; linear algebra is accelerated by avoiding, for example, explicit addition of; or multiplication by blocks of zeroes.; Block-sparse matrices may be created with; sparsify_band(),; sparsify_rectangles(),; sparsify_row_intervals(),; and sparsify_triangl",MatchSource.WIKI,docs/0.2/linalg/hail.linalg.BlockMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html
https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:7916,Availability,checkpoint,checkpoint,7916,"ter_cols(). These following methods always result in a block-dense matrix:. fill(); Addition or subtraction of a scalar or broadcasted vector.; Matrix multiplication, @. The following methods fail if any operand is block-sparse, but can be forced; by first applying densify(). Element-wise division between two block matrices.; Multiplication by a scalar or broadcasted vector which includes an; infinite or nan value.; Division by a scalar or broadcasted vector which includes a zero, infinite; or nan value.; Division of a scalar or broadcasted vector by a block matrix.; Element-wise exponentiation by a negative exponent.; Natural logarithm, log(). Attributes. T; Matrix transpose. block_size; Block size. element_type; The type of the elements. is_sparse; Returns True if block-sparse. n_cols; Number of columns. n_rows; Number of rows. shape; Shape of matrix. Methods. abs; Element-wise absolute value. cache; Persist this block matrix in memory. ceil; Element-wise ceiling. checkpoint; Checkpoint the block matrix. default_block_size; Default block side length. densify; Restore all dropped blocks as explicit blocks of zeros. diagonal; Extracts diagonal elements as a row vector. entries; Returns a table with the indices and value of each block matrix entry. export; Exports a stored block matrix as a delimited text file. export_blocks; Export each block of the block matrix as its own delimited text or binary file. export_rectangles; Export rectangular regions from a block matrix to delimited text or binary files. fill; Creates a block matrix with all elements the same value. filter; Filters matrix rows and columns. filter_cols; Filters matrix columns. filter_rows; Filters matrix rows. floor; Element-wise floor. from_entry_expr; Creates a block matrix using a matrix table entry expression. from_ndarray; Create a BlockMatrix from an ndarray. from_numpy; Distributes a NumPy ndarray as a block matrix. fromfile; Creates a block matrix from a binary file. log; Element-wise natural lo",MatchSource.WIKI,docs/0.2/linalg/hail.linalg.BlockMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html
https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:10731,Availability,checkpoint,checkpoint,10731,"gle field element. to_ndarray; Collects a BlockMatrix into a local hail ndarray expression on driver. to_numpy; Collects the block matrix into a NumPy ndarray. to_table_row_major; Returns a table where each row represents a row in the block matrix. tofile; Collects and writes data to a binary file. tree_matmul; Matrix multiplication in situations with large inner dimension. unpersist; Unpersists this block matrix from memory/disk. write; Writes the block matrix. write_from_entry_expr; Writes a block matrix from a matrix table entry expression. property T; Matrix transpose. Returns:; BlockMatrix. abs()[source]; Element-wise absolute value. Returns:; BlockMatrix. property block_size; Block size. Returns:; int. cache()[source]; Persist this block matrix in memory.; Notes; This method is an alias for persist(""MEMORY_ONLY""). Returns:; BlockMatrix – Cached block matrix. ceil()[source]; Element-wise ceiling. Returns:; BlockMatrix. checkpoint(path, overwrite=False, force_row_major=False, stage_locally=False)[source]; Checkpoint the block matrix. Danger; Do not write or checkpoint to a path that is already an input source for the query. This can cause data loss. Parameters:. path (str) – Path for output file.; overwrite (bool) – If True, overwrite an existing file at the destination.; force_row_major (bool) – If True, transform blocks in column-major format; to row-major format before checkpointing.; If False, checkpoint blocks in their current format.; stage_locally (bool) – If True, major output will be written to temporary local storage; before being copied to output. static default_block_size()[source]; Default block side length. densify()[source]; Restore all dropped blocks as explicit blocks of zeros. Returns:; BlockMatrix. diagonal()[source]; Extracts diagonal elements as a row vector. Returns:; BlockMatrix. property element_type; The type of the elements. entries(keyed=True)[source]; Returns a table with the indices and value of each block matrix entry.; Ex",MatchSource.WIKI,docs/0.2/linalg/hail.linalg.BlockMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html
https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:10872,Availability,checkpoint,checkpoint,10872,"numpy; Collects the block matrix into a NumPy ndarray. to_table_row_major; Returns a table where each row represents a row in the block matrix. tofile; Collects and writes data to a binary file. tree_matmul; Matrix multiplication in situations with large inner dimension. unpersist; Unpersists this block matrix from memory/disk. write; Writes the block matrix. write_from_entry_expr; Writes a block matrix from a matrix table entry expression. property T; Matrix transpose. Returns:; BlockMatrix. abs()[source]; Element-wise absolute value. Returns:; BlockMatrix. property block_size; Block size. Returns:; int. cache()[source]; Persist this block matrix in memory.; Notes; This method is an alias for persist(""MEMORY_ONLY""). Returns:; BlockMatrix – Cached block matrix. ceil()[source]; Element-wise ceiling. Returns:; BlockMatrix. checkpoint(path, overwrite=False, force_row_major=False, stage_locally=False)[source]; Checkpoint the block matrix. Danger; Do not write or checkpoint to a path that is already an input source for the query. This can cause data loss. Parameters:. path (str) – Path for output file.; overwrite (bool) – If True, overwrite an existing file at the destination.; force_row_major (bool) – If True, transform blocks in column-major format; to row-major format before checkpointing.; If False, checkpoint blocks in their current format.; stage_locally (bool) – If True, major output will be written to temporary local storage; before being copied to output. static default_block_size()[source]; Default block side length. densify()[source]; Restore all dropped blocks as explicit blocks of zeros. Returns:; BlockMatrix. diagonal()[source]; Extracts diagonal elements as a row vector. Returns:; BlockMatrix. property element_type; The type of the elements. entries(keyed=True)[source]; Returns a table with the indices and value of each block matrix entry.; Examples; >>> import numpy as np; >>> block_matrix = BlockMatrix.from_numpy(np.array([[5, 7], [2, 8]]), 2)",MatchSource.WIKI,docs/0.2/linalg/hail.linalg.BlockMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html
https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:11193,Availability,checkpoint,checkpointing,11193,"inner dimension. unpersist; Unpersists this block matrix from memory/disk. write; Writes the block matrix. write_from_entry_expr; Writes a block matrix from a matrix table entry expression. property T; Matrix transpose. Returns:; BlockMatrix. abs()[source]; Element-wise absolute value. Returns:; BlockMatrix. property block_size; Block size. Returns:; int. cache()[source]; Persist this block matrix in memory.; Notes; This method is an alias for persist(""MEMORY_ONLY""). Returns:; BlockMatrix – Cached block matrix. ceil()[source]; Element-wise ceiling. Returns:; BlockMatrix. checkpoint(path, overwrite=False, force_row_major=False, stage_locally=False)[source]; Checkpoint the block matrix. Danger; Do not write or checkpoint to a path that is already an input source for the query. This can cause data loss. Parameters:. path (str) – Path for output file.; overwrite (bool) – If True, overwrite an existing file at the destination.; force_row_major (bool) – If True, transform blocks in column-major format; to row-major format before checkpointing.; If False, checkpoint blocks in their current format.; stage_locally (bool) – If True, major output will be written to temporary local storage; before being copied to output. static default_block_size()[source]; Default block side length. densify()[source]; Restore all dropped blocks as explicit blocks of zeros. Returns:; BlockMatrix. diagonal()[source]; Extracts diagonal elements as a row vector. Returns:; BlockMatrix. property element_type; The type of the elements. entries(keyed=True)[source]; Returns a table with the indices and value of each block matrix entry.; Examples; >>> import numpy as np; >>> block_matrix = BlockMatrix.from_numpy(np.array([[5, 7], [2, 8]]), 2); >>> entries_table = block_matrix.entries(); >>> entries_table.show(); +-------+-------+----------+; | i | j | entry |; +-------+-------+----------+; | int64 | int64 | float64 |; +-------+-------+----------+; | 0 | 0 | 5.00e+00 |; | 0 | 1 | 7.00e+00 |; |",MatchSource.WIKI,docs/0.2/linalg/hail.linalg.BlockMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html
https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:11219,Availability,checkpoint,checkpoint,11219,"es the block matrix. write_from_entry_expr; Writes a block matrix from a matrix table entry expression. property T; Matrix transpose. Returns:; BlockMatrix. abs()[source]; Element-wise absolute value. Returns:; BlockMatrix. property block_size; Block size. Returns:; int. cache()[source]; Persist this block matrix in memory.; Notes; This method is an alias for persist(""MEMORY_ONLY""). Returns:; BlockMatrix – Cached block matrix. ceil()[source]; Element-wise ceiling. Returns:; BlockMatrix. checkpoint(path, overwrite=False, force_row_major=False, stage_locally=False)[source]; Checkpoint the block matrix. Danger; Do not write or checkpoint to a path that is already an input source for the query. This can cause data loss. Parameters:. path (str) – Path for output file.; overwrite (bool) – If True, overwrite an existing file at the destination.; force_row_major (bool) – If True, transform blocks in column-major format; to row-major format before checkpointing.; If False, checkpoint blocks in their current format.; stage_locally (bool) – If True, major output will be written to temporary local storage; before being copied to output. static default_block_size()[source]; Default block side length. densify()[source]; Restore all dropped blocks as explicit blocks of zeros. Returns:; BlockMatrix. diagonal()[source]; Extracts diagonal elements as a row vector. Returns:; BlockMatrix. property element_type; The type of the elements. entries(keyed=True)[source]; Returns a table with the indices and value of each block matrix entry.; Examples; >>> import numpy as np; >>> block_matrix = BlockMatrix.from_numpy(np.array([[5, 7], [2, 8]]), 2); >>> entries_table = block_matrix.entries(); >>> entries_table.show(); +-------+-------+----------+; | i | j | entry |; +-------+-------+----------+; | int64 | int64 | float64 |; +-------+-------+----------+; | 0 | 0 | 5.00e+00 |; | 0 | 1 | 7.00e+00 |; | 1 | 0 | 2.00e+00 |; | 1 | 1 | 8.00e+00 |; +-------+-------+----------+. Notes; The re",MatchSource.WIKI,docs/0.2/linalg/hail.linalg.BlockMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html
https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:21928,Availability,error,error,21928,"s of rows to keep. Must be non-empty and increasing. Returns:; BlockMatrix. floor()[source]; Element-wise floor. Returns:; BlockMatrix. classmethod from_entry_expr(entry_expr, mean_impute=False, center=False, normalize=False, axis='rows', block_size=None)[source]; Creates a block matrix using a matrix table entry expression.; Examples; >>> mt = hl.balding_nichols_model(3, 25, 50); >>> bm = BlockMatrix.from_entry_expr(mt.GT.n_alt_alleles()). Notes; This convenience method writes the block matrix to a temporary file on; persistent disk and then reads the file. If you want to store the; resulting block matrix, use write_from_entry_expr() directly to; avoid writing the result twice. See write_from_entry_expr() for; further documentation. Warning; If the rows of the matrix table have been filtered to a small fraction,; then MatrixTable.repartition() before this method to improve; performance.; If you encounter a Hadoop write/replication error, increase the; number of persistent workers or the disk size per persistent worker,; or use write_from_entry_expr() to write to external storage.; This method opens n_cols / block_size files concurrently per task.; To not blow out memory when the number of columns is very large,; limit the Hadoop write buffer size; e.g. on GCP, set this property on; cluster startup (the default is 64MB):; --properties 'core:fs.gs.io.buffersize.write=1048576. Parameters:. entry_expr (Float64Expression) – Entry expression for numeric matrix entries.; mean_impute (bool) – If true, set missing values to the row mean before centering or; normalizing. If false, missing values will raise an error.; center (bool) – If true, subtract the row mean.; normalize (bool) – If true and center=False, divide by the row magnitude.; If true and center=True, divide the centered value by the; centered row magnitude.; axis (str) – One of “rows” or “cols”: axis by which to normalize or center.; block_size (int, optional) – Block size. Default given by BlockMatrix.default_",MatchSource.WIKI,docs/0.2/linalg/hail.linalg.BlockMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html
https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:22610,Availability,error,error,22610,"ix, use write_from_entry_expr() directly to; avoid writing the result twice. See write_from_entry_expr() for; further documentation. Warning; If the rows of the matrix table have been filtered to a small fraction,; then MatrixTable.repartition() before this method to improve; performance.; If you encounter a Hadoop write/replication error, increase the; number of persistent workers or the disk size per persistent worker,; or use write_from_entry_expr() to write to external storage.; This method opens n_cols / block_size files concurrently per task.; To not blow out memory when the number of columns is very large,; limit the Hadoop write buffer size; e.g. on GCP, set this property on; cluster startup (the default is 64MB):; --properties 'core:fs.gs.io.buffersize.write=1048576. Parameters:. entry_expr (Float64Expression) – Entry expression for numeric matrix entries.; mean_impute (bool) – If true, set missing values to the row mean before centering or; normalizing. If false, missing values will raise an error.; center (bool) – If true, subtract the row mean.; normalize (bool) – If true and center=False, divide by the row magnitude.; If true and center=True, divide the centered value by the; centered row magnitude.; axis (str) – One of “rows” or “cols”: axis by which to normalize or center.; block_size (int, optional) – Block size. Default given by BlockMatrix.default_block_size(). classmethod from_ndarray(ndarray_expression, block_size=4096)[source]; Create a BlockMatrix from an ndarray. classmethod from_numpy(ndarray, block_size=None)[source]; Distributes a NumPy ndarray; as a block matrix.; Examples; >>> import numpy as np; >>> a = np.random.rand(10, 20); >>> bm = BlockMatrix.from_numpy(a). Notes; The ndarray must have two dimensions, each of non-zero size.; The number of entries must be less than \(2^{31}\). Parameters:. ndarray (numpy.ndarray) – ndarray with two dimensions, each of non-zero size.; block_size (int, optional) – Block size. Default given by default",MatchSource.WIKI,docs/0.2/linalg/hail.linalg.BlockMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html
https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:25496,Availability,redundant,redundant,25496," ndarray must have type float64 for the output of; func:numpy.tofile to be a valid binary input to fromfile().; This is not checked.; The number of entries must be less than \(2^{31}\). Parameters:. uri (str, optional) – URI of binary input file.; n_rows (int) – Number of rows.; n_cols (int) – Number of columns.; block_size (int, optional) – Block size. Default given by default_block_size(). See also; from_numpy(). property is_sparse; Returns True if block-sparse.; Notes; A block matrix is block-sparse if at least of its blocks is dropped,; i.e. implicitly a block of zeros. Returns:; bool. log()[source]; Element-wise natural logarithm. Returns:; BlockMatrix. property n_cols; Number of columns. Returns:; int. property n_rows; Number of rows. Returns:; int. persist(storage_level='MEMORY_AND_DISK')[source]; Persists this block matrix in memory or on disk.; Notes; The BlockMatrix.persist() and BlockMatrix.cache(); methods store the current block matrix on disk or in memory temporarily; to avoid redundant computation and improve the performance of Hail; pipelines. This method is not a substitution for; BlockMatrix.write(), which stores a permanent file.; Most users should use the “MEMORY_AND_DISK” storage level. See the Spark; documentation; for a more in-depth discussion of persisting data. Parameters:; storage_level (str) – Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns:; BlockMatrix – Persisted block matrix. classmethod random(n_rows, n_cols, block_size=None, seed=None, gaussian=True)[source]; Creates a block matrix with standard normal or uniform random entries.; Examples; Create a block matrix with 10 rows, 20 columns, and standard normal entries:; >>> bm = BlockMatrix.random(10, 20). Parameters:. n_rows (int) – Number of rows.; n_cols (int) – Number of columns.; block_size (int, optional) – ",MatchSource.WIKI,docs/0.2/linalg/hail.linalg.BlockMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html
https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:44035,Availability,checkpoint,checkpoints,44035,"numpy.ndarray of the same dimensions:; >>> a = np.fromfile('/local/file').reshape((10, 20)) . Notes; This method, analogous to numpy.tofile,; produces a binary file of float64 values in row-major order, which can; be read by functions such as numpy.fromfile; (if a local file) and BlockMatrix.fromfile().; Binary files produced and consumed by tofile() and; fromfile() are not platform independent, so should only be used; for inter-operating with NumPy, not storage. Use; BlockMatrix.write() and BlockMatrix.read() to save and load; block matrices, since these methods write and read blocks in parallel; and are platform independent.; The number of entries must be less than \(2^{31}\). Parameters:; uri (str, optional) – URI of binary output file. See also; to_numpy(). tree_matmul(b, *, splits, path_prefix=None)[source]; Matrix multiplication in situations with large inner dimension.; This function splits a single matrix multiplication into split_on_inner smaller matrix multiplications,; does the smaller multiplications, checkpoints them with names defined by file_name_prefix, and adds them; together. This is useful in cases when the multiplication of two large matrices results in a much smaller matrix. Parameters:. b (numpy.ndarray or BlockMatrix); splits (int (keyword only argument)) – The number of smaller multiplications to do.; path_prefix (str (keyword only argument)) – The prefix of the path to write the block matrices to. If unspecified, writes to a tmpdir. Returns:; BlockMatrix. unpersist()[source]; Unpersists this block matrix from memory/disk.; Notes; This function will have no effect on a block matrix that was not previously; persisted. Returns:; BlockMatrix – Unpersisted block matrix. write(path, overwrite=False, force_row_major=False, stage_locally=False)[source]; Writes the block matrix. Danger; Do not write or checkpoint to a path that is already an input source for the query. This can cause data loss. Parameters:. path (str) – Path for output file.; over",MatchSource.WIKI,docs/0.2/linalg/hail.linalg.BlockMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html
https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:44858,Availability,checkpoint,checkpoint,44858,"nner dimension.; This function splits a single matrix multiplication into split_on_inner smaller matrix multiplications,; does the smaller multiplications, checkpoints them with names defined by file_name_prefix, and adds them; together. This is useful in cases when the multiplication of two large matrices results in a much smaller matrix. Parameters:. b (numpy.ndarray or BlockMatrix); splits (int (keyword only argument)) – The number of smaller multiplications to do.; path_prefix (str (keyword only argument)) – The prefix of the path to write the block matrices to. If unspecified, writes to a tmpdir. Returns:; BlockMatrix. unpersist()[source]; Unpersists this block matrix from memory/disk.; Notes; This function will have no effect on a block matrix that was not previously; persisted. Returns:; BlockMatrix – Unpersisted block matrix. write(path, overwrite=False, force_row_major=False, stage_locally=False)[source]; Writes the block matrix. Danger; Do not write or checkpoint to a path that is already an input source for the query. This can cause data loss. Parameters:. path (str) – Path for output file.; overwrite (bool) – If True, overwrite an existing file at the destination.; force_row_major (bool) – If True, transform blocks in column-major format; to row-major format before writing.; If False, write blocks in their current format.; stage_locally (bool) – If True, major output will be written to temporary local storage; before being copied to output. static write_from_entry_expr(entry_expr, path, overwrite=False, mean_impute=False, center=False, normalize=False, axis='rows', block_size=None)[source]; Writes a block matrix from a matrix table entry expression.; Examples; >>> mt = hl.balding_nichols_model(3, 25, 50); >>> BlockMatrix.write_from_entry_expr(mt.GT.n_alt_alleles(),; ... 'output/model.bm'). Notes; The resulting file can be loaded with BlockMatrix.read().; Blocks are stored row-major.; If a pipelined transformation significantly downsamples the rows of ",MatchSource.WIKI,docs/0.2/linalg/hail.linalg.BlockMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html
https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:45855,Availability,down,downsamples,45855,"rce for the query. This can cause data loss. Parameters:. path (str) – Path for output file.; overwrite (bool) – If True, overwrite an existing file at the destination.; force_row_major (bool) – If True, transform blocks in column-major format; to row-major format before writing.; If False, write blocks in their current format.; stage_locally (bool) – If True, major output will be written to temporary local storage; before being copied to output. static write_from_entry_expr(entry_expr, path, overwrite=False, mean_impute=False, center=False, normalize=False, axis='rows', block_size=None)[source]; Writes a block matrix from a matrix table entry expression.; Examples; >>> mt = hl.balding_nichols_model(3, 25, 50); >>> BlockMatrix.write_from_entry_expr(mt.GT.n_alt_alleles(),; ... 'output/model.bm'). Notes; The resulting file can be loaded with BlockMatrix.read().; Blocks are stored row-major.; If a pipelined transformation significantly downsamples the rows of the; underlying matrix table, then repartitioning the matrix table ahead of; this method will greatly improve its performance.; By default, this method will fail if any values are missing (to be clear,; special float values like nan are not missing values). Set mean_impute to replace missing values with the row mean before; possibly centering or normalizing. If all values are missing, the row; mean is nan.; Set center to shift each row to have mean zero before possibly; normalizing.; Set normalize to normalize each row to have unit length. To standardize each row, regarded as an empirical distribution, to have; mean 0 and variance 1, set center and normalize and then multiply; the result by sqrt(n_cols). Warning; If the rows of the matrix table have been filtered to a small fraction,; then MatrixTable.repartition() before this method to improve; performance.; This method opens n_cols / block_size files concurrently per task.; To not blow out memory when the number of columns is very large,; limit the Hadoop write",MatchSource.WIKI,docs/0.2/linalg/hail.linalg.BlockMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html
https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:47387,Availability,error,error,47387,"wnsamples the rows of the; underlying matrix table, then repartitioning the matrix table ahead of; this method will greatly improve its performance.; By default, this method will fail if any values are missing (to be clear,; special float values like nan are not missing values). Set mean_impute to replace missing values with the row mean before; possibly centering or normalizing. If all values are missing, the row; mean is nan.; Set center to shift each row to have mean zero before possibly; normalizing.; Set normalize to normalize each row to have unit length. To standardize each row, regarded as an empirical distribution, to have; mean 0 and variance 1, set center and normalize and then multiply; the result by sqrt(n_cols). Warning; If the rows of the matrix table have been filtered to a small fraction,; then MatrixTable.repartition() before this method to improve; performance.; This method opens n_cols / block_size files concurrently per task.; To not blow out memory when the number of columns is very large,; limit the Hadoop write buffer size; e.g. on GCP, set this property on; cluster startup (the default is 64MB):; --properties 'core:fs.gs.io.buffersize.write=1048576. Parameters:. entry_expr (Float64Expression) – Entry expression for numeric matrix entries.; path (str) – Path for output.; overwrite (bool) – If True, overwrite an existing file at the destination.; mean_impute (bool) – If true, set missing values to the row mean before centering or; normalizing. If false, missing values will raise an error.; center (bool) – If true, subtract the row mean.; normalize (bool) – If true and center=False, divide by the row magnitude.; If true and center=True, divide the centered value by the; centered row magnitude.; axis (str) – One of “rows” or “cols”: axis by which to normalize or center.; block_size (int, optional) – Block size. Default given by BlockMatrix.default_block_size(). Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/linalg/hail.linalg.BlockMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html
https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:25555,Deployability,pipeline,pipelines,25555," ndarray must have type float64 for the output of; func:numpy.tofile to be a valid binary input to fromfile().; This is not checked.; The number of entries must be less than \(2^{31}\). Parameters:. uri (str, optional) – URI of binary input file.; n_rows (int) – Number of rows.; n_cols (int) – Number of columns.; block_size (int, optional) – Block size. Default given by default_block_size(). See also; from_numpy(). property is_sparse; Returns True if block-sparse.; Notes; A block matrix is block-sparse if at least of its blocks is dropped,; i.e. implicitly a block of zeros. Returns:; bool. log()[source]; Element-wise natural logarithm. Returns:; BlockMatrix. property n_cols; Number of columns. Returns:; int. property n_rows; Number of rows. Returns:; int. persist(storage_level='MEMORY_AND_DISK')[source]; Persists this block matrix in memory or on disk.; Notes; The BlockMatrix.persist() and BlockMatrix.cache(); methods store the current block matrix on disk or in memory temporarily; to avoid redundant computation and improve the performance of Hail; pipelines. This method is not a substitution for; BlockMatrix.write(), which stores a permanent file.; Most users should use the “MEMORY_AND_DISK” storage level. See the Spark; documentation; for a more in-depth discussion of persisting data. Parameters:; storage_level (str) – Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns:; BlockMatrix – Persisted block matrix. classmethod random(n_rows, n_cols, block_size=None, seed=None, gaussian=True)[source]; Creates a block matrix with standard normal or uniform random entries.; Examples; Create a block matrix with 10 rows, 20 columns, and standard normal entries:; >>> bm = BlockMatrix.random(10, 20). Parameters:. n_rows (int) – Number of rows.; n_cols (int) – Number of columns.; block_size (int, optional) – ",MatchSource.WIKI,docs/0.2/linalg/hail.linalg.BlockMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html
https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:39085,Deployability,configurat,configuration,39085,"d stage. Furthermore, due to finite; precision, the zero eigenvalues of \(X^T X\) or \(X X^T\) will; only be approximately zero.; If the rank is not known ahead, examining the relative sizes of the; trailing singular values should reveal where the spectrum switches from; non-zero to “zero” eigenvalues. With 64-bit floating point, zero; eigenvalues are typically about 1e-16 times the largest eigenvalue.; The corresponding singular vectors should be sliced away before an; action which realizes the block-matrix-side singular vectors.; svd() sets the singular values corresponding to negative; eigenvalues to exactly 0.0. Warning; The first and third stages invoke distributed matrix multiplication with; parallelism bounded by the number of resulting blocks, whereas the; second stage is executed on the leader (master) node. For matrices of; large minimum dimension, it may be preferable to run these stages; separately.; The performance of the second stage depends critically on the number of; leader (master) cores and the NumPy / SciPy configuration, viewable with; np.show_config(). For Intel machines, we recommend installing the; MKL package for Anaconda.; Consequently, the optimal value of complexity_bound is highly; configuration-dependent. Parameters:. compute_uv (bool) – If False, only compute the singular values (or eigenvalues).; complexity_bound (int) – Maximum value of \(\sqrt[3]{nmr}\) for which; scipy.linalg.svd() is used. Returns:. u (numpy.ndarray or BlockMatrix) – Left singular vectors \(U\), as a block matrix if \(n > m\) and; \(\sqrt[3]{nmr}\) exceeds complexity_bound.; Only returned if compute_uv is True.; s (numpy.ndarray) – Singular values from \(\Sigma\) in descending order.; vt (numpy.ndarray or BlockMatrix) – Right singular vectors \(V^T`\), as a block matrix if \(n \leq m\) and; \(\sqrt[3]{nmr}\) exceeds complexity_bound.; Only returned if compute_uv is True. to_matrix_table_row_major(n_partitions=None, maximum_cache_memory_in_bytes=None)[source]; Ret",MatchSource.WIKI,docs/0.2/linalg/hail.linalg.BlockMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html
https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:39166,Deployability,install,installing,39166,".; If the rank is not known ahead, examining the relative sizes of the; trailing singular values should reveal where the spectrum switches from; non-zero to “zero” eigenvalues. With 64-bit floating point, zero; eigenvalues are typically about 1e-16 times the largest eigenvalue.; The corresponding singular vectors should be sliced away before an; action which realizes the block-matrix-side singular vectors.; svd() sets the singular values corresponding to negative; eigenvalues to exactly 0.0. Warning; The first and third stages invoke distributed matrix multiplication with; parallelism bounded by the number of resulting blocks, whereas the; second stage is executed on the leader (master) node. For matrices of; large minimum dimension, it may be preferable to run these stages; separately.; The performance of the second stage depends critically on the number of; leader (master) cores and the NumPy / SciPy configuration, viewable with; np.show_config(). For Intel machines, we recommend installing the; MKL package for Anaconda.; Consequently, the optimal value of complexity_bound is highly; configuration-dependent. Parameters:. compute_uv (bool) – If False, only compute the singular values (or eigenvalues).; complexity_bound (int) – Maximum value of \(\sqrt[3]{nmr}\) for which; scipy.linalg.svd() is used. Returns:. u (numpy.ndarray or BlockMatrix) – Left singular vectors \(U\), as a block matrix if \(n > m\) and; \(\sqrt[3]{nmr}\) exceeds complexity_bound.; Only returned if compute_uv is True.; s (numpy.ndarray) – Singular values from \(\Sigma\) in descending order.; vt (numpy.ndarray or BlockMatrix) – Right singular vectors \(V^T`\), as a block matrix if \(n \leq m\) and; \(\sqrt[3]{nmr}\) exceeds complexity_bound.; Only returned if compute_uv is True. to_matrix_table_row_major(n_partitions=None, maximum_cache_memory_in_bytes=None)[source]; Returns a matrix table with row key of row_idx and col key col_idx, whose; entries are structs of a single field element. Parameter",MatchSource.WIKI,docs/0.2/linalg/hail.linalg.BlockMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html
https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:39272,Deployability,configurat,configuration-dependent,39272,"ingular values should reveal where the spectrum switches from; non-zero to “zero” eigenvalues. With 64-bit floating point, zero; eigenvalues are typically about 1e-16 times the largest eigenvalue.; The corresponding singular vectors should be sliced away before an; action which realizes the block-matrix-side singular vectors.; svd() sets the singular values corresponding to negative; eigenvalues to exactly 0.0. Warning; The first and third stages invoke distributed matrix multiplication with; parallelism bounded by the number of resulting blocks, whereas the; second stage is executed on the leader (master) node. For matrices of; large minimum dimension, it may be preferable to run these stages; separately.; The performance of the second stage depends critically on the number of; leader (master) cores and the NumPy / SciPy configuration, viewable with; np.show_config(). For Intel machines, we recommend installing the; MKL package for Anaconda.; Consequently, the optimal value of complexity_bound is highly; configuration-dependent. Parameters:. compute_uv (bool) – If False, only compute the singular values (or eigenvalues).; complexity_bound (int) – Maximum value of \(\sqrt[3]{nmr}\) for which; scipy.linalg.svd() is used. Returns:. u (numpy.ndarray or BlockMatrix) – Left singular vectors \(U\), as a block matrix if \(n > m\) and; \(\sqrt[3]{nmr}\) exceeds complexity_bound.; Only returned if compute_uv is True.; s (numpy.ndarray) – Singular values from \(\Sigma\) in descending order.; vt (numpy.ndarray or BlockMatrix) – Right singular vectors \(V^T`\), as a block matrix if \(n \leq m\) and; \(\sqrt[3]{nmr}\) exceeds complexity_bound.; Only returned if compute_uv is True. to_matrix_table_row_major(n_partitions=None, maximum_cache_memory_in_bytes=None)[source]; Returns a matrix table with row key of row_idx and col key col_idx, whose; entries are structs of a single field element. Parameters:. n_partitions (int or None) – Number of partitions of the matrix table.; maximu",MatchSource.WIKI,docs/0.2/linalg/hail.linalg.BlockMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html
https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:45816,Deployability,pipeline,pipelined,45816,"rce for the query. This can cause data loss. Parameters:. path (str) – Path for output file.; overwrite (bool) – If True, overwrite an existing file at the destination.; force_row_major (bool) – If True, transform blocks in column-major format; to row-major format before writing.; If False, write blocks in their current format.; stage_locally (bool) – If True, major output will be written to temporary local storage; before being copied to output. static write_from_entry_expr(entry_expr, path, overwrite=False, mean_impute=False, center=False, normalize=False, axis='rows', block_size=None)[source]; Writes a block matrix from a matrix table entry expression.; Examples; >>> mt = hl.balding_nichols_model(3, 25, 50); >>> BlockMatrix.write_from_entry_expr(mt.GT.n_alt_alleles(),; ... 'output/model.bm'). Notes; The resulting file can be loaded with BlockMatrix.read().; Blocks are stored row-major.; If a pipelined transformation significantly downsamples the rows of the; underlying matrix table, then repartitioning the matrix table ahead of; this method will greatly improve its performance.; By default, this method will fail if any values are missing (to be clear,; special float values like nan are not missing values). Set mean_impute to replace missing values with the row mean before; possibly centering or normalizing. If all values are missing, the row; mean is nan.; Set center to shift each row to have mean zero before possibly; normalizing.; Set normalize to normalize each row to have unit length. To standardize each row, regarded as an empirical distribution, to have; mean 0 and variance 1, set center and normalize and then multiply; the result by sqrt(n_cols). Warning; If the rows of the matrix table have been filtered to a small fraction,; then MatrixTable.repartition() before this method to improve; performance.; This method opens n_cols / block_size files concurrently per task.; To not blow out memory when the number of columns is very large,; limit the Hadoop write",MatchSource.WIKI,docs/0.2/linalg/hail.linalg.BlockMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html
https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:47829,Deployability,update,updated,47829,"wnsamples the rows of the; underlying matrix table, then repartitioning the matrix table ahead of; this method will greatly improve its performance.; By default, this method will fail if any values are missing (to be clear,; special float values like nan are not missing values). Set mean_impute to replace missing values with the row mean before; possibly centering or normalizing. If all values are missing, the row; mean is nan.; Set center to shift each row to have mean zero before possibly; normalizing.; Set normalize to normalize each row to have unit length. To standardize each row, regarded as an empirical distribution, to have; mean 0 and variance 1, set center and normalize and then multiply; the result by sqrt(n_cols). Warning; If the rows of the matrix table have been filtered to a small fraction,; then MatrixTable.repartition() before this method to improve; performance.; This method opens n_cols / block_size files concurrently per task.; To not blow out memory when the number of columns is very large,; limit the Hadoop write buffer size; e.g. on GCP, set this property on; cluster startup (the default is 64MB):; --properties 'core:fs.gs.io.buffersize.write=1048576. Parameters:. entry_expr (Float64Expression) – Entry expression for numeric matrix entries.; path (str) – Path for output.; overwrite (bool) – If True, overwrite an existing file at the destination.; mean_impute (bool) – If true, set missing values to the row mean before centering or; normalizing. If false, missing values will raise an error.; center (bool) – If true, subtract the row mean.; normalize (bool) – If true and center=False, divide by the row magnitude.; If true and center=True, divide the centered value by the; centered row magnitude.; axis (str) – One of “rows” or “cols”: axis by which to normalize or center.; block_size (int, optional) – Block size. Default given by BlockMatrix.default_block_size(). Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/linalg/hail.linalg.BlockMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html
https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:1701,Energy Efficiency,power,power,1701,"Log And Version Policy. menu; Hail. Python API; Hail Query Python API; linalg; BlockMatrix. View page source. BlockMatrix. class hail.linalg.BlockMatrix[source]; Hail’s block-distributed matrix of tfloat64 elements. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. A block matrix is a distributed analogue of a two-dimensional; NumPy ndarray with; shape (n_rows, n_cols) and NumPy dtype float64.; Import the class with:; >>> from hail.linalg import BlockMatrix. Under the hood, block matrices are partitioned like a checkerboard into; square blocks with side length a common block size. Blocks in the final row; or column of blocks may be truncated, so block size need not evenly divide; the matrix dimensions. Block size defaults to the value given by; default_block_size().; Operations and broadcasting; The core operations are consistent with NumPy: +, -, *, and; / for element-wise addition, subtraction, multiplication, and division;; @ for matrix multiplication; T for transpose; and ** for; element-wise exponentiation to a scalar power.; For element-wise binary operations, each operand may be a block matrix, an; ndarray, or a scalar (int or float). For matrix; multiplication, each operand may be a block matrix or an ndarray. If either; operand is a block matrix, the result is a block matrix. Binary operations; between block matrices require that both operands have the same block size.; To interoperate with block matrices, ndarray operands must be one or two; dimensional with dtype convertible to float64. One-dimensional ndarrays; of shape (n) are promoted to two-dimensional ndarrays of shape (1,; n), i.e. a single row.; Block matrices support broadcasting of +, -, *, and /; between matrices of different shapes, consistent with the NumPy; broadcasting rules.; There is one exception: block matrices do not currently support element-wise; “outer product” of a single row and a single column",MatchSource.WIKI,docs/0.2/linalg/hail.linalg.BlockMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html
https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:9621,Energy Efficiency,reduce,reduced,9621,"x rows. floor; Element-wise floor. from_entry_expr; Creates a block matrix using a matrix table entry expression. from_ndarray; Create a BlockMatrix from an ndarray. from_numpy; Distributes a NumPy ndarray as a block matrix. fromfile; Creates a block matrix from a binary file. log; Element-wise natural logarithm. persist; Persists this block matrix in memory or on disk. random; Creates a block matrix with standard normal or uniform random entries. read; Reads a block matrix. rectangles_to_numpy; Instantiates a NumPy ndarray from files of rectangles written out using export_rectangles() or export_blocks(). sparsify_band; Filter to a diagonal band. sparsify_rectangles; Filter to blocks overlapping the union of rectangular regions. sparsify_row_intervals; Creates a block-sparse matrix by filtering to an interval for each row. sparsify_triangle; Filter to the upper or lower triangle. sqrt; Element-wise square root. sum; Sums array elements over one or both axes. svd; Computes the reduced singular value decomposition. to_matrix_table_row_major; Returns a matrix table with row key of row_idx and col key col_idx, whose entries are structs of a single field element. to_ndarray; Collects a BlockMatrix into a local hail ndarray expression on driver. to_numpy; Collects the block matrix into a NumPy ndarray. to_table_row_major; Returns a table where each row represents a row in the block matrix. tofile; Collects and writes data to a binary file. tree_matmul; Matrix multiplication in situations with large inner dimension. unpersist; Unpersists this block matrix from memory/disk. write; Writes the block matrix. write_from_entry_expr; Writes a block matrix from a matrix table entry expression. property T; Matrix transpose. Returns:; BlockMatrix. abs()[source]; Element-wise absolute value. Returns:; BlockMatrix. property block_size; Block size. Returns:; int. cache()[source]; Persist this block matrix in memory.; Notes; This method is an alias for persist(""MEMORY_ONLY""). Return",MatchSource.WIKI,docs/0.2/linalg/hail.linalg.BlockMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html
https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:14756,Energy Efficiency,efficient,efficient,14756,"le.gz',; ... header=' '.join(['idx', 'A', 'B', 'C']),; ... add_index=True,; ... parallel='header_per_shard',; ... partition_size=2). This produces two compressed files which uncompress to:; idx A B C; 0 1.0 0.8 0.7; 1 0.8 1.0 0.3. idx A B C; 2 0.7 0.3 1.0. Warning; The block matrix must be stored in row-major format, as results; from BlockMatrix.write() with force_row_major=True and from; BlockMatrix.write_from_entry_expr(). Otherwise,; export() will fail. Notes; The five options for entries are illustrated below.; Full:; 1.0 0.8 0.7; 0.8 1.0 0.3; 0.7 0.3 1.0. Lower triangle:; 1.0; 0.8 1.0; 0.7 0.3 1.0. Strict lower triangle:; 0.8; 0.7 0.3. Upper triangle:; 1.0 0.8 0.7; 1.0 0.3; 1.0. Strict upper triangle:; 0.8 0.7; 0.3. The number of columns must be less than \(2^{31}\).; The number of partitions (file shards) exported equals the ceiling; of n_rows / partition_size. By default, there is one partition; per row of blocks in the block matrix. The number of partitions; should be at least the number of cores for efficient parallelism.; Setting the partition size to an exact (rather than approximate); divisor or multiple of the block size reduces superfluous shuffling; of data.; If parallel is None, these file shards are then serially; concatenated by one core into one file, a slow process. See; other options below.; It is highly recommended to export large files with a .bgz extension,; which will use a block gzipped compression codec. These files can be; read natively with Python’s gzip.open and R’s read.table. Parameters:. path_in (str) – Path to input block matrix, stored row-major on disk.; path_out (str) – Path for export.; Use extension .gz for gzip or .bgz for block gzip.; delimiter (str) – Column delimiter.; header (str, optional) – If provided, header is prepended before the first row of data.; add_index (bool) – If True, add an initial column with the absolute row index.; parallel (str, optional) – If 'header_per_shard', create a folder with one file per; parti",MatchSource.WIKI,docs/0.2/linalg/hail.linalg.BlockMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html
https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:14884,Energy Efficiency,reduce,reduces,14884,"ition_size=2). This produces two compressed files which uncompress to:; idx A B C; 0 1.0 0.8 0.7; 1 0.8 1.0 0.3. idx A B C; 2 0.7 0.3 1.0. Warning; The block matrix must be stored in row-major format, as results; from BlockMatrix.write() with force_row_major=True and from; BlockMatrix.write_from_entry_expr(). Otherwise,; export() will fail. Notes; The five options for entries are illustrated below.; Full:; 1.0 0.8 0.7; 0.8 1.0 0.3; 0.7 0.3 1.0. Lower triangle:; 1.0; 0.8 1.0; 0.7 0.3 1.0. Strict lower triangle:; 0.8; 0.7 0.3. Upper triangle:; 1.0 0.8 0.7; 1.0 0.3; 1.0. Strict upper triangle:; 0.8 0.7; 0.3. The number of columns must be less than \(2^{31}\).; The number of partitions (file shards) exported equals the ceiling; of n_rows / partition_size. By default, there is one partition; per row of blocks in the block matrix. The number of partitions; should be at least the number of cores for efficient parallelism.; Setting the partition size to an exact (rather than approximate); divisor or multiple of the block size reduces superfluous shuffling; of data.; If parallel is None, these file shards are then serially; concatenated by one core into one file, a slow process. See; other options below.; It is highly recommended to export large files with a .bgz extension,; which will use a block gzipped compression codec. These files can be; read natively with Python’s gzip.open and R’s read.table. Parameters:. path_in (str) – Path to input block matrix, stored row-major on disk.; path_out (str) – Path for export.; Use extension .gz for gzip or .bgz for block gzip.; delimiter (str) – Column delimiter.; header (str, optional) – If provided, header is prepended before the first row of data.; add_index (bool) – If True, add an initial column with the absolute row index.; parallel (str, optional) – If 'header_per_shard', create a folder with one file per; partition, each with a header if provided.; If 'separate_header', create a folder with one file per; partition without a hea",MatchSource.WIKI,docs/0.2/linalg/hail.linalg.BlockMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html
https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:20461,Energy Efficiency,efficient,efficient,20461,"{29}\). Parameters:. path_out (str) – Path for folder of exported files.; rectangles (list of list of int) – List of rectangles of the form; [row_start, row_stop, col_start, col_stop].; delimiter (str) – Column delimiter.; binary (bool) – If true, export elements as raw bytes in row major order. classmethod fill(n_rows, n_cols, value, block_size=None)[source]; Creates a block matrix with all elements the same value.; Examples; Create a block matrix with 10 rows, 20 columns, and all elements equal to 1.0:; >>> bm = BlockMatrix.fill(10, 20, 1.0). Parameters:. n_rows (int) – Number of rows.; n_cols (int) – Number of columns.; value (float) – Value of all elements.; block_size (int, optional) – Block size. Default given by default_block_size(). Returns:; BlockMatrix. filter(rows_to_keep, cols_to_keep)[source]; Filters matrix rows and columns.; Notes; This method has the same effect as BlockMatrix.filter_cols(); followed by BlockMatrix.filter_rows() (or vice versa), but; filters the block matrix in a single pass which may be more efficient. Parameters:. rows_to_keep (list of int) – Indices of rows to keep. Must be non-empty and increasing.; cols_to_keep (list of int) – Indices of columns to keep. Must be non-empty and increasing. Returns:; BlockMatrix. filter_cols(cols_to_keep)[source]; Filters matrix columns. Parameters:; cols_to_keep (list of int) – Indices of columns to keep. Must be non-empty and increasing. Returns:; BlockMatrix. filter_rows(rows_to_keep)[source]; Filters matrix rows. Parameters:; rows_to_keep (list of int) – Indices of rows to keep. Must be non-empty and increasing. Returns:; BlockMatrix. floor()[source]; Element-wise floor. Returns:; BlockMatrix. classmethod from_entry_expr(entry_expr, mean_impute=False, center=False, normalize=False, axis='rows', block_size=None)[source]; Creates a block matrix using a matrix table entry expression.; Examples; >>> mt = hl.balding_nichols_model(3, 25, 50); >>> bm = BlockMatrix.from_entry_expr(mt.GT.n_alt_all",MatchSource.WIKI,docs/0.2/linalg/hail.linalg.BlockMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html
https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:29944,Energy Efficiency,efficient,efficient,29944," Notes; This method creates a block-sparse matrix by zeroing out all blocks; which are disjoint from a diagonal band. By default,; all elements outside the band but inside blocks that overlap the; band are set to zero as well.; The band is defined in terms of inclusive lower and upper indices; relative to the diagonal. For example, the indices -1, 0, and 1; correspond to the sub-diagonal, diagonal, and super-diagonal,; respectively. The diagonal band contains the elements at positions; \((i, j)\) such that. \[\mathrm{lower} \leq j - i \leq \mathrm{upper}.\]; lower must be less than or equal to upper, but their values may; exceed the dimensions of the matrix, the band need not include the; diagonal, and the matrix need not be square. Parameters:. lower (int) – Index of lowest band relative to the diagonal.; upper (int) – Index of highest band relative to the diagonal.; blocks_only (bool) – If False, set all elements outside the band to zero.; If True, only set all blocks outside the band to blocks; of zeros; this is more efficient. Returns:; BlockMatrix – Sparse block matrix. sparsify_rectangles(rectangles)[source]; Filter to blocks overlapping the union of rectangular regions.; Examples; Consider the following block matrix:; >>> import numpy as np; >>> nd = np.array([[ 1.0, 2.0, 3.0, 4.0],; ... [ 5.0, 6.0, 7.0, 8.0],; ... [ 9.0, 10.0, 11.0, 12.0],; ... [13.0, 14.0, 15.0, 16.0]]); >>> bm = BlockMatrix.from_numpy(nd, block_size=2). Filter to blocks covering three rectangles and collect to NumPy:; >>> bm.sparsify_rectangles([[0, 1, 0, 1], [0, 3, 0, 2], [1, 2, 0, 4]]).to_numpy() ; array([[ 1., 2., 3., 4.],; [ 5., 6., 7., 8.],; [ 9., 10., 0., 0.],; [13., 14., 0., 0.]]). Notes; This method creates a block-sparse matrix by zeroing out (dropping); all blocks which are disjoint from the union of a set of rectangular; regions. Partially overlapping blocks are not modified.; Each rectangle is encoded as a list of length four of; the form [row_start, row_stop, col_start, col_s",MatchSource.WIKI,docs/0.2/linalg/hail.linalg.BlockMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html
https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:33301,Energy Efficiency,efficient,efficient,33301,"1, 0, 2, 2],; ... stops= [2, 0, 3, 4],; ... blocks_only=True); ... .to_numpy()) ; array([[ 1., 2., 0., 0.],; [ 5., 6., 0., 0.],; [ 0., 0., 11., 12.],; [ 0., 0., 15., 16.]]). Notes; This method creates a block-sparse matrix by zeroing out all blocks; which are disjoint from all row intervals. By default, all elements; outside the row intervals but inside blocks that overlap the row; intervals are set to zero as well.; starts and stops must both have length equal to the number of; rows. The interval for row i is [starts[i], stops[i]). In; particular, 0 <= starts[i] <= stops[i] <= n_cols is required; for all i.; This method requires the number of rows to be less than \(2^{31}\). Parameters:. starts (list of int, or numpy.ndarray of int) – Start indices for each row (inclusive).; stops (list of int, or numpy.ndarray of int) – Stop indices for each row (exclusive).; blocks_only (bool) – If False, set all elements outside row intervals to zero.; If True, only set all blocks outside row intervals to blocks; of zeros; this is more efficient. Returns:; BlockMatrix – Sparse block matrix. sparsify_triangle(lower=False, blocks_only=False)[source]; Filter to the upper or lower triangle.; Examples; Consider the following block matrix:; >>> import numpy as np; >>> nd = np.array([[ 1.0, 2.0, 3.0, 4.0],; ... [ 5.0, 6.0, 7.0, 8.0],; ... [ 9.0, 10.0, 11.0, 12.0],; ... [13.0, 14.0, 15.0, 16.0]]); >>> bm = BlockMatrix.from_numpy(nd, block_size=2). Filter to the upper triangle and collect to NumPy:; >>> bm.sparsify_triangle().to_numpy() ; array([[ 1., 2., 3., 4.],; [ 0., 6., 7., 8.],; [ 0., 0., 11., 12.],; [ 0., 0., 0., 16.]]). Set all blocks fully outside the upper triangle to zero; and collect to NumPy:; >>> bm.sparsify_triangle(blocks_only=True).to_numpy() ; array([[ 1., 2., 3., 4.],; [ 5., 6., 7., 8.],; [ 0., 0., 11., 12.],; [ 0., 0., 15., 16.]]). Notes; This method creates a block-sparse matrix by zeroing out all blocks; which are disjoint from the (non-strict) upper or lower triang",MatchSource.WIKI,docs/0.2/linalg/hail.linalg.BlockMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html
https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:34646,Energy Efficiency,efficient,efficient,34646,"0, 10.0, 11.0, 12.0],; ... [13.0, 14.0, 15.0, 16.0]]); >>> bm = BlockMatrix.from_numpy(nd, block_size=2). Filter to the upper triangle and collect to NumPy:; >>> bm.sparsify_triangle().to_numpy() ; array([[ 1., 2., 3., 4.],; [ 0., 6., 7., 8.],; [ 0., 0., 11., 12.],; [ 0., 0., 0., 16.]]). Set all blocks fully outside the upper triangle to zero; and collect to NumPy:; >>> bm.sparsify_triangle(blocks_only=True).to_numpy() ; array([[ 1., 2., 3., 4.],; [ 5., 6., 7., 8.],; [ 0., 0., 11., 12.],; [ 0., 0., 15., 16.]]). Notes; This method creates a block-sparse matrix by zeroing out all blocks; which are disjoint from the (non-strict) upper or lower triangle. By; default, all elements outside the triangle but inside blocks that; overlap the triangle are set to zero as well. Parameters:. lower (bool) – If False, keep the upper triangle.; If True, keep the lower triangle.; blocks_only (bool) – If False, set all elements outside the triangle to zero.; If True, only set all blocks outside the triangle to; blocks of zeros; this is more efficient. Returns:; BlockMatrix – Sparse block matrix. sqrt()[source]; Element-wise square root. Returns:; BlockMatrix. sum(axis=None)[source]; Sums array elements over one or both axes.; Examples; >>> import numpy as np; >>> nd = np.array([[ 1.0, 2.0, 3.0],; ... [ 4.0, 5.0, 6.0]]); >>> bm = BlockMatrix.from_numpy(nd); >>> bm.sum(); 21.0. >>> bm.sum(axis=0).to_numpy(); array([[5., 7., 9.]]). >>> bm.sum(axis=1).to_numpy(); array([[ 6.],; [15.]]). Parameters:; axis (int, optional) – Axis over which to sum.; By default, sum all elements.; If 0, sum over rows.; If 1, sum over columns. Returns:; float or BlockMatrix – If None, returns a float.; If 0, returns a block matrix with a single row.; If 1, returns a block matrix with a single column. svd(compute_uv=True, complexity_bound=8192)[source]; Computes the reduced singular value decomposition.; Examples; >>> x = BlockMatrix.from_numpy(np.array([[-2.0, 0.0, 3.0],; ... [-1.0, 2.0, 4.0]])); >>> x.svd(",MatchSource.WIKI,docs/0.2/linalg/hail.linalg.BlockMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html
https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:35464,Energy Efficiency,reduce,reduced,35464,"If True, keep the lower triangle.; blocks_only (bool) – If False, set all elements outside the triangle to zero.; If True, only set all blocks outside the triangle to; blocks of zeros; this is more efficient. Returns:; BlockMatrix – Sparse block matrix. sqrt()[source]; Element-wise square root. Returns:; BlockMatrix. sum(axis=None)[source]; Sums array elements over one or both axes.; Examples; >>> import numpy as np; >>> nd = np.array([[ 1.0, 2.0, 3.0],; ... [ 4.0, 5.0, 6.0]]); >>> bm = BlockMatrix.from_numpy(nd); >>> bm.sum(); 21.0. >>> bm.sum(axis=0).to_numpy(); array([[5., 7., 9.]]). >>> bm.sum(axis=1).to_numpy(); array([[ 6.],; [15.]]). Parameters:; axis (int, optional) – Axis over which to sum.; By default, sum all elements.; If 0, sum over rows.; If 1, sum over columns. Returns:; float or BlockMatrix – If None, returns a float.; If 0, returns a block matrix with a single row.; If 1, returns a block matrix with a single column. svd(compute_uv=True, complexity_bound=8192)[source]; Computes the reduced singular value decomposition.; Examples; >>> x = BlockMatrix.from_numpy(np.array([[-2.0, 0.0, 3.0],; ... [-1.0, 2.0, 4.0]])); >>> x.svd(); (array([[-0.60219551, -0.79834865],; [-0.79834865, 0.60219551]]),; array([5.61784832, 1.56197958]),; array([[ 0.35649586, -0.28421866, -0.89001711],; [ 0.6366932 , 0.77106707, 0.00879404]])). Notes; This method leverages distributed matrix multiplication to compute; reduced singular value decomposition (SVD); for matrices that would otherwise be too large to work with locally,; provided that at least one dimension is less than or equal to 46300.; Let \(X\) be an \(n \times m\) matrix and let; \(r = \min(n, m)\). In particular, \(X\) can have at most; \(r\) non-zero singular values. The reduced SVD of \(X\); has the form. \[X = U \Sigma V^T\]; where. \(U\) is an \(n \times r\) matrix whose columns are; (orthonormal) left singular vectors,; \(\Sigma\) is an \(r \times r\) diagonal matrix of non-negative; singular values in desce",MatchSource.WIKI,docs/0.2/linalg/hail.linalg.BlockMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html
https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:35878,Energy Efficiency,reduce,reduced,35878,"; >>> bm = BlockMatrix.from_numpy(nd); >>> bm.sum(); 21.0. >>> bm.sum(axis=0).to_numpy(); array([[5., 7., 9.]]). >>> bm.sum(axis=1).to_numpy(); array([[ 6.],; [15.]]). Parameters:; axis (int, optional) – Axis over which to sum.; By default, sum all elements.; If 0, sum over rows.; If 1, sum over columns. Returns:; float or BlockMatrix – If None, returns a float.; If 0, returns a block matrix with a single row.; If 1, returns a block matrix with a single column. svd(compute_uv=True, complexity_bound=8192)[source]; Computes the reduced singular value decomposition.; Examples; >>> x = BlockMatrix.from_numpy(np.array([[-2.0, 0.0, 3.0],; ... [-1.0, 2.0, 4.0]])); >>> x.svd(); (array([[-0.60219551, -0.79834865],; [-0.79834865, 0.60219551]]),; array([5.61784832, 1.56197958]),; array([[ 0.35649586, -0.28421866, -0.89001711],; [ 0.6366932 , 0.77106707, 0.00879404]])). Notes; This method leverages distributed matrix multiplication to compute; reduced singular value decomposition (SVD); for matrices that would otherwise be too large to work with locally,; provided that at least one dimension is less than or equal to 46300.; Let \(X\) be an \(n \times m\) matrix and let; \(r = \min(n, m)\). In particular, \(X\) can have at most; \(r\) non-zero singular values. The reduced SVD of \(X\); has the form. \[X = U \Sigma V^T\]; where. \(U\) is an \(n \times r\) matrix whose columns are; (orthonormal) left singular vectors,; \(\Sigma\) is an \(r \times r\) diagonal matrix of non-negative; singular values in descending order,; \(V^T\) is an \(r \times m\) matrix whose rows are; (orthonormal) right singular vectors. If the singular values in \(\Sigma\) are distinct, then the; decomposition is unique up to multiplication of corresponding left and; right singular vectors by -1. The computational complexity of SVD is; roughly \(nmr\).; We now describe the implementation in more detail.; If \(\sqrt[3]{nmr}\) is less than or equal to complexity_bound,; then \(X\) is localized to an ndarray on",MatchSource.WIKI,docs/0.2/linalg/hail.linalg.BlockMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html
https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:36204,Energy Efficiency,reduce,reduced,36204,"sum over columns. Returns:; float or BlockMatrix – If None, returns a float.; If 0, returns a block matrix with a single row.; If 1, returns a block matrix with a single column. svd(compute_uv=True, complexity_bound=8192)[source]; Computes the reduced singular value decomposition.; Examples; >>> x = BlockMatrix.from_numpy(np.array([[-2.0, 0.0, 3.0],; ... [-1.0, 2.0, 4.0]])); >>> x.svd(); (array([[-0.60219551, -0.79834865],; [-0.79834865, 0.60219551]]),; array([5.61784832, 1.56197958]),; array([[ 0.35649586, -0.28421866, -0.89001711],; [ 0.6366932 , 0.77106707, 0.00879404]])). Notes; This method leverages distributed matrix multiplication to compute; reduced singular value decomposition (SVD); for matrices that would otherwise be too large to work with locally,; provided that at least one dimension is less than or equal to 46300.; Let \(X\) be an \(n \times m\) matrix and let; \(r = \min(n, m)\). In particular, \(X\) can have at most; \(r\) non-zero singular values. The reduced SVD of \(X\); has the form. \[X = U \Sigma V^T\]; where. \(U\) is an \(n \times r\) matrix whose columns are; (orthonormal) left singular vectors,; \(\Sigma\) is an \(r \times r\) diagonal matrix of non-negative; singular values in descending order,; \(V^T\) is an \(r \times m\) matrix whose rows are; (orthonormal) right singular vectors. If the singular values in \(\Sigma\) are distinct, then the; decomposition is unique up to multiplication of corresponding left and; right singular vectors by -1. The computational complexity of SVD is; roughly \(nmr\).; We now describe the implementation in more detail.; If \(\sqrt[3]{nmr}\) is less than or equal to complexity_bound,; then \(X\) is localized to an ndarray on which; scipy.linalg.svd() is called. In this case, all components are; returned as ndarrays.; If \(\sqrt[3]{nmr}\) is greater than complexity_bound, then the; reduced SVD is computed via the smaller gramian matrix of \(X\). For; \(n > m\), the three stages are:. Compute (and localize) t",MatchSource.WIKI,docs/0.2/linalg/hail.linalg.BlockMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html
https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:37091,Energy Efficiency,reduce,reduced,37091,"m\) matrix and let; \(r = \min(n, m)\). In particular, \(X\) can have at most; \(r\) non-zero singular values. The reduced SVD of \(X\); has the form. \[X = U \Sigma V^T\]; where. \(U\) is an \(n \times r\) matrix whose columns are; (orthonormal) left singular vectors,; \(\Sigma\) is an \(r \times r\) diagonal matrix of non-negative; singular values in descending order,; \(V^T\) is an \(r \times m\) matrix whose rows are; (orthonormal) right singular vectors. If the singular values in \(\Sigma\) are distinct, then the; decomposition is unique up to multiplication of corresponding left and; right singular vectors by -1. The computational complexity of SVD is; roughly \(nmr\).; We now describe the implementation in more detail.; If \(\sqrt[3]{nmr}\) is less than or equal to complexity_bound,; then \(X\) is localized to an ndarray on which; scipy.linalg.svd() is called. In this case, all components are; returned as ndarrays.; If \(\sqrt[3]{nmr}\) is greater than complexity_bound, then the; reduced SVD is computed via the smaller gramian matrix of \(X\). For; \(n > m\), the three stages are:. Compute (and localize) the gramian matrix \(X^T X\),; Compute the eigenvalues and right singular vectors via the; symmetric eigendecomposition \(X^T X = V S V^T\) with; numpy.linalg.eigh() or scipy.linalg.eigh(),; Compute the singular values as \(\Sigma = S^\frac{1}{2}\) and the; the left singular vectors as the block matrix; \(U = X V \Sigma^{-1}\). In this case, since block matrix multiplication is lazy, it is efficient; to subsequently slice \(U\) (e.g. based on the singular values), or; discard \(U\) entirely.; If \(n \leq m\), the three stages instead use the gramian; \(X X^T = U S U^T\) and return \(V^T\) as the; block matrix \(\Sigma^{-1} U^T X\). Warning; Computing reduced SVD via the gramian presents an added wrinkle when; \(X\) is not full rank, as the block-matrix-side null-basis is not; computable by the formula in the third stage. Furthermore, due to finite; precision, ",MatchSource.WIKI,docs/0.2/linalg/hail.linalg.BlockMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html
https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:37611,Energy Efficiency,efficient,efficient,37611,"ct, then the; decomposition is unique up to multiplication of corresponding left and; right singular vectors by -1. The computational complexity of SVD is; roughly \(nmr\).; We now describe the implementation in more detail.; If \(\sqrt[3]{nmr}\) is less than or equal to complexity_bound,; then \(X\) is localized to an ndarray on which; scipy.linalg.svd() is called. In this case, all components are; returned as ndarrays.; If \(\sqrt[3]{nmr}\) is greater than complexity_bound, then the; reduced SVD is computed via the smaller gramian matrix of \(X\). For; \(n > m\), the three stages are:. Compute (and localize) the gramian matrix \(X^T X\),; Compute the eigenvalues and right singular vectors via the; symmetric eigendecomposition \(X^T X = V S V^T\) with; numpy.linalg.eigh() or scipy.linalg.eigh(),; Compute the singular values as \(\Sigma = S^\frac{1}{2}\) and the; the left singular vectors as the block matrix; \(U = X V \Sigma^{-1}\). In this case, since block matrix multiplication is lazy, it is efficient; to subsequently slice \(U\) (e.g. based on the singular values), or; discard \(U\) entirely.; If \(n \leq m\), the three stages instead use the gramian; \(X X^T = U S U^T\) and return \(V^T\) as the; block matrix \(\Sigma^{-1} U^T X\). Warning; Computing reduced SVD via the gramian presents an added wrinkle when; \(X\) is not full rank, as the block-matrix-side null-basis is not; computable by the formula in the third stage. Furthermore, due to finite; precision, the zero eigenvalues of \(X^T X\) or \(X X^T\) will; only be approximately zero.; If the rank is not known ahead, examining the relative sizes of the; trailing singular values should reveal where the spectrum switches from; non-zero to “zero” eigenvalues. With 64-bit floating point, zero; eigenvalues are typically about 1e-16 times the largest eigenvalue.; The corresponding singular vectors should be sliced away before an; action which realizes the block-matrix-side singular vectors.; svd() sets the singu",MatchSource.WIKI,docs/0.2/linalg/hail.linalg.BlockMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html
https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:37877,Energy Efficiency,reduce,reduced,37877,"vd() is called. In this case, all components are; returned as ndarrays.; If \(\sqrt[3]{nmr}\) is greater than complexity_bound, then the; reduced SVD is computed via the smaller gramian matrix of \(X\). For; \(n > m\), the three stages are:. Compute (and localize) the gramian matrix \(X^T X\),; Compute the eigenvalues and right singular vectors via the; symmetric eigendecomposition \(X^T X = V S V^T\) with; numpy.linalg.eigh() or scipy.linalg.eigh(),; Compute the singular values as \(\Sigma = S^\frac{1}{2}\) and the; the left singular vectors as the block matrix; \(U = X V \Sigma^{-1}\). In this case, since block matrix multiplication is lazy, it is efficient; to subsequently slice \(U\) (e.g. based on the singular values), or; discard \(U\) entirely.; If \(n \leq m\), the three stages instead use the gramian; \(X X^T = U S U^T\) and return \(V^T\) as the; block matrix \(\Sigma^{-1} U^T X\). Warning; Computing reduced SVD via the gramian presents an added wrinkle when; \(X\) is not full rank, as the block-matrix-side null-basis is not; computable by the formula in the third stage. Furthermore, due to finite; precision, the zero eigenvalues of \(X^T X\) or \(X X^T\) will; only be approximately zero.; If the rank is not known ahead, examining the relative sizes of the; trailing singular values should reveal where the spectrum switches from; non-zero to “zero” eigenvalues. With 64-bit floating point, zero; eigenvalues are typically about 1e-16 times the largest eigenvalue.; The corresponding singular vectors should be sliced away before an; action which realizes the block-matrix-side singular vectors.; svd() sets the singular values corresponding to negative; eigenvalues to exactly 0.0. Warning; The first and third stages invoke distributed matrix multiplication with; parallelism bounded by the number of resulting blocks, whereas the; second stage is executed on the leader (master) node. For matrices of; large minimum dimension, it may be preferable to run these stages;",MatchSource.WIKI,docs/0.2/linalg/hail.linalg.BlockMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html
https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:40596,Energy Efficiency,reduce,reduce,40596,"t[3]{nmr}\) exceeds complexity_bound.; Only returned if compute_uv is True.; s (numpy.ndarray) – Singular values from \(\Sigma\) in descending order.; vt (numpy.ndarray or BlockMatrix) – Right singular vectors \(V^T`\), as a block matrix if \(n \leq m\) and; \(\sqrt[3]{nmr}\) exceeds complexity_bound.; Only returned if compute_uv is True. to_matrix_table_row_major(n_partitions=None, maximum_cache_memory_in_bytes=None)[source]; Returns a matrix table with row key of row_idx and col key col_idx, whose; entries are structs of a single field element. Parameters:. n_partitions (int or None) – Number of partitions of the matrix table.; maximum_cache_memory_in_bytes (int or None) – The amount of memory to reserve, per partition, to cache rows of the; matrix in memory. This value must be at least large enough to hold; one row of the matrix in memory. If this value is exactly the size of; one row, then a partition makes a network request for every row of; every block. Larger values reduce the number of network requests. If; memory permits, setting this value to the size of one output; partition permits one network request per block per partition. Notes; Does not support block-sparse matrices. Returns:; MatrixTable – Matrix table where each entry corresponds to an entry in the block matrix. to_ndarray()[source]; Collects a BlockMatrix into a local hail ndarray expression on driver. This should not; be done for large matrices. Returns:; NDArrayExpression. to_numpy(_force_blocking=False)[source]; Collects the block matrix into a NumPy ndarray.; Examples; >>> bm = BlockMatrix.random(10, 20); >>> a = bm.to_numpy(). Notes; The resulting ndarray will have the same shape as the block matrix. Returns:; numpy.ndarray. to_table_row_major(n_partitions=None, maximum_cache_memory_in_bytes=None)[source]; Returns a table where each row represents a row in the block matrix. The resulting table has the following fields:; row_idx (:py:data.`tint64`, key field) – Row index; entries (tarray o",MatchSource.WIKI,docs/0.2/linalg/hail.linalg.BlockMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html
https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:42522,Energy Efficiency,reduce,reduce,42522,"lds:; row_idx (:py:data.`tint64`, key field) – Row index; entries (tarray of tfloat64) – Entries for the row. Examples; >>> import numpy as np; >>> block_matrix = BlockMatrix.from_numpy(np.array([[1, 2], [3, 4], [5, 6]]), 2); >>> t = block_matrix.to_table_row_major(); >>> t.show(); +---------+---------------------+; | row_idx | entries |; +---------+---------------------+; | int64 | array<float64> |; +---------+---------------------+; | 0 | [1.00e+00,2.00e+00] |; | 1 | [3.00e+00,4.00e+00] |; | 2 | [5.00e+00,6.00e+00] |; +---------+---------------------+. Parameters:. n_partitions (int or None) – Number of partitions of the table.; maximum_cache_memory_in_bytes (int or None) – The amount of memory to reserve, per partition, to cache rows of the; matrix in memory. This value must be at least large enough to hold; one row of the matrix in memory. If this value is exactly the size of; one row, then a partition makes a network request for every row of; every block. Larger values reduce the number of network requests. If; memory permits, setting this value to the size of one output; partition permits one network request per block per partition. Notes; Does not support block-sparse matrices. Returns:; Table – Table where each row corresponds to a row in the block matrix. tofile(uri)[source]; Collects and writes data to a binary file.; Examples; >>> import numpy as np; >>> bm = BlockMatrix.random(10, 20); >>> bm.tofile('file:///local/file') . To create a numpy.ndarray of the same dimensions:; >>> a = np.fromfile('/local/file').reshape((10, 20)) . Notes; This method, analogous to numpy.tofile,; produces a binary file of float64 values in row-major order, which can; be read by functions such as numpy.fromfile; (if a local file) and BlockMatrix.fromfile().; Binary files produced and consumed by tofile() and; fromfile() are not platform independent, so should only be used; for inter-operating with NumPy, not storage. Use; BlockMatrix.write() and BlockMatrix.read() to save and l",MatchSource.WIKI,docs/0.2/linalg/hail.linalg.BlockMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html
https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:895,Integrability,interface,interface,895,"﻿. Hail | ; BlockMatrix. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; linalg; BlockMatrix. View page source. BlockMatrix. class hail.linalg.BlockMatrix[source]; Hail’s block-distributed matrix of tfloat64 elements. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. A block matrix is a distributed analogue of a two-dimensional; NumPy ndarray with; shape (n_rows, n_cols) and NumPy dtype float64.; Import the class with:; >>> from hail.linalg import BlockMatrix. Under the hood, block matrices are partitioned like a checkerboard into; square blocks with side length a common block size. Blocks in the final row; or column of blocks may be truncated, so block size need not evenly divide; the matrix dimensions. Block size defaults to the value given by; default_block_size().; Operations and broadcasting; The core operations are consistent with NumPy: +, -, *, and; / for element-wise addition, subtraction, multiplication, and division;; @ for matrix multiplication; T for transpose; and ** for; element-wise exponentiation to a scalar power.; For element-wise binary operations, each operand may be a block matrix, an; ndarray, or a scalar (int or float). For matrix; multiplication, each operand may be a block matrix or an ndarray. If either; operand is a block matrix, the result is a block matrix. Binary operations; between block ",MatchSource.WIKI,docs/0.2/linalg/hail.linalg.BlockMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html
https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:3062,Integrability,depend,dependency,3062,"nds have the same block size.; To interoperate with block matrices, ndarray operands must be one or two; dimensional with dtype convertible to float64. One-dimensional ndarrays; of shape (n) are promoted to two-dimensional ndarrays of shape (1,; n), i.e. a single row.; Block matrices support broadcasting of +, -, *, and /; between matrices of different shapes, consistent with the NumPy; broadcasting rules.; There is one exception: block matrices do not currently support element-wise; “outer product” of a single row and a single column, although the same; effect can be achieved for * by using @. Warning; For binary operations, if the first operand is an ndarray and the; second operand is a block matrix, the result will be a ndarray of block; matrices. To achieve the desired behavior for + and *, place the; block matrix operand first; for -, /, and @, first convert; the ndarray to a block matrix using from_numpy(). Warning; Block matrix multiplication requires special care due to each block; of each operand being a dependency of multiple blocks in the product.; The \((i, j)\)-block in the product a @ b is computed by summing; the products of corresponding blocks in block row \(i\) of a and; block column \(j\) of b. So overall, in addition to this; multiplication and addition, the evaluation of a @ b realizes each; block of a as many times as the number of block columns of b; and realizes each block of b as many times as the number of; block rows of a.; This becomes a performance and resilience issue whenever a or b; is defined in terms of pending transformations (such as linear; algebra operations). For example, evaluating a @ (c @ d) will; effectively evaluate c @ d as many times as the number of block rows; in a.; To limit re-computation, write or cache transformed block matrix; operands before feeding them into matrix multiplication:; >>> c = BlockMatrix.read('c.bm') ; >>> d = BlockMatrix.read('d.bm') ; >>> (c @ d).write('cd.bm') ; >>> a = BlockMatrix.read('a.bm') ",MatchSource.WIKI,docs/0.2/linalg/hail.linalg.BlockMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html
https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:6237,Integrability,depend,depend,6237,", statistical geneticists often want to compute and manipulate a; banded correlation matrix capturing “linkage disequilibrium” between nearby; variants along the genome. In this case, working with the full correlation; matrix for tens of millions of variants would be prohibitively expensive,; and in any case, entries far from the diagonal are either not of interest or; ought to be zeroed out before downstream linear algebra.; To enable such computations, block matrices do not require that all blocks; be realized explicitly. Implicit (dropped) blocks behave as blocks of; zeroes, so we refer to a block matrix in which at least one block is; implicitly zero as a block-sparse matrix. Otherwise, we say the matrix; is block-dense. The property is_sparse() encodes this state.; Dropped blocks are not stored in memory or on write(). In fact,; blocks that are dropped prior to an action like export() or; to_numpy() are never computed in the first place, nor are any blocks; of upstream operands on which only dropped blocks depend! In addition,; linear algebra is accelerated by avoiding, for example, explicit addition of; or multiplication by blocks of zeroes.; Block-sparse matrices may be created with; sparsify_band(),; sparsify_rectangles(),; sparsify_row_intervals(),; and sparsify_triangle().; The following methods naturally propagate block-sparsity:. Addition and subtraction “union” realized blocks.; Element-wise multiplication “intersects” realized blocks.; Transpose “transposes” realized blocks.; abs() and sqrt() preserve the realized blocks.; sum() along an axis realizes those blocks for which at least one; block summand is realized.; Matrix slicing, and more generally filter(), filter_rows(),; and filter_cols(). These following methods always result in a block-dense matrix:. fill(); Addition or subtraction of a scalar or broadcasted vector.; Matrix multiplication, @. The following methods fail if any operand is block-sparse, but can be forced; by first applying densify()",MatchSource.WIKI,docs/0.2/linalg/hail.linalg.BlockMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html
https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:39004,Integrability,depend,depends,39004,"d stage. Furthermore, due to finite; precision, the zero eigenvalues of \(X^T X\) or \(X X^T\) will; only be approximately zero.; If the rank is not known ahead, examining the relative sizes of the; trailing singular values should reveal where the spectrum switches from; non-zero to “zero” eigenvalues. With 64-bit floating point, zero; eigenvalues are typically about 1e-16 times the largest eigenvalue.; The corresponding singular vectors should be sliced away before an; action which realizes the block-matrix-side singular vectors.; svd() sets the singular values corresponding to negative; eigenvalues to exactly 0.0. Warning; The first and third stages invoke distributed matrix multiplication with; parallelism bounded by the number of resulting blocks, whereas the; second stage is executed on the leader (master) node. For matrices of; large minimum dimension, it may be preferable to run these stages; separately.; The performance of the second stage depends critically on the number of; leader (master) cores and the NumPy / SciPy configuration, viewable with; np.show_config(). For Intel machines, we recommend installing the; MKL package for Anaconda.; Consequently, the optimal value of complexity_bound is highly; configuration-dependent. Parameters:. compute_uv (bool) – If False, only compute the singular values (or eigenvalues).; complexity_bound (int) – Maximum value of \(\sqrt[3]{nmr}\) for which; scipy.linalg.svd() is used. Returns:. u (numpy.ndarray or BlockMatrix) – Left singular vectors \(U\), as a block matrix if \(n > m\) and; \(\sqrt[3]{nmr}\) exceeds complexity_bound.; Only returned if compute_uv is True.; s (numpy.ndarray) – Singular values from \(\Sigma\) in descending order.; vt (numpy.ndarray or BlockMatrix) – Right singular vectors \(V^T`\), as a block matrix if \(n \leq m\) and; \(\sqrt[3]{nmr}\) exceeds complexity_bound.; Only returned if compute_uv is True. to_matrix_table_row_major(n_partitions=None, maximum_cache_memory_in_bytes=None)[source]; Ret",MatchSource.WIKI,docs/0.2/linalg/hail.linalg.BlockMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html
https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:39286,Integrability,depend,dependent,39286,"ingular values should reveal where the spectrum switches from; non-zero to “zero” eigenvalues. With 64-bit floating point, zero; eigenvalues are typically about 1e-16 times the largest eigenvalue.; The corresponding singular vectors should be sliced away before an; action which realizes the block-matrix-side singular vectors.; svd() sets the singular values corresponding to negative; eigenvalues to exactly 0.0. Warning; The first and third stages invoke distributed matrix multiplication with; parallelism bounded by the number of resulting blocks, whereas the; second stage is executed on the leader (master) node. For matrices of; large minimum dimension, it may be preferable to run these stages; separately.; The performance of the second stage depends critically on the number of; leader (master) cores and the NumPy / SciPy configuration, viewable with; np.show_config(). For Intel machines, we recommend installing the; MKL package for Anaconda.; Consequently, the optimal value of complexity_bound is highly; configuration-dependent. Parameters:. compute_uv (bool) – If False, only compute the singular values (or eigenvalues).; complexity_bound (int) – Maximum value of \(\sqrt[3]{nmr}\) for which; scipy.linalg.svd() is used. Returns:. u (numpy.ndarray or BlockMatrix) – Left singular vectors \(U\), as a block matrix if \(n > m\) and; \(\sqrt[3]{nmr}\) exceeds complexity_bound.; Only returned if compute_uv is True.; s (numpy.ndarray) – Singular values from \(\Sigma\) in descending order.; vt (numpy.ndarray or BlockMatrix) – Right singular vectors \(V^T`\), as a block matrix if \(n \leq m\) and; \(\sqrt[3]{nmr}\) exceeds complexity_bound.; Only returned if compute_uv is True. to_matrix_table_row_major(n_partitions=None, maximum_cache_memory_in_bytes=None)[source]; Returns a matrix table with row key of row_idx and col key col_idx, whose; entries are structs of a single field element. Parameters:. n_partitions (int or None) – Number of partitions of the matrix table.; maximu",MatchSource.WIKI,docs/0.2/linalg/hail.linalg.BlockMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html
https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:39085,Modifiability,config,configuration,39085,"d stage. Furthermore, due to finite; precision, the zero eigenvalues of \(X^T X\) or \(X X^T\) will; only be approximately zero.; If the rank is not known ahead, examining the relative sizes of the; trailing singular values should reveal where the spectrum switches from; non-zero to “zero” eigenvalues. With 64-bit floating point, zero; eigenvalues are typically about 1e-16 times the largest eigenvalue.; The corresponding singular vectors should be sliced away before an; action which realizes the block-matrix-side singular vectors.; svd() sets the singular values corresponding to negative; eigenvalues to exactly 0.0. Warning; The first and third stages invoke distributed matrix multiplication with; parallelism bounded by the number of resulting blocks, whereas the; second stage is executed on the leader (master) node. For matrices of; large minimum dimension, it may be preferable to run these stages; separately.; The performance of the second stage depends critically on the number of; leader (master) cores and the NumPy / SciPy configuration, viewable with; np.show_config(). For Intel machines, we recommend installing the; MKL package for Anaconda.; Consequently, the optimal value of complexity_bound is highly; configuration-dependent. Parameters:. compute_uv (bool) – If False, only compute the singular values (or eigenvalues).; complexity_bound (int) – Maximum value of \(\sqrt[3]{nmr}\) for which; scipy.linalg.svd() is used. Returns:. u (numpy.ndarray or BlockMatrix) – Left singular vectors \(U\), as a block matrix if \(n > m\) and; \(\sqrt[3]{nmr}\) exceeds complexity_bound.; Only returned if compute_uv is True.; s (numpy.ndarray) – Singular values from \(\Sigma\) in descending order.; vt (numpy.ndarray or BlockMatrix) – Right singular vectors \(V^T`\), as a block matrix if \(n \leq m\) and; \(\sqrt[3]{nmr}\) exceeds complexity_bound.; Only returned if compute_uv is True. to_matrix_table_row_major(n_partitions=None, maximum_cache_memory_in_bytes=None)[source]; Ret",MatchSource.WIKI,docs/0.2/linalg/hail.linalg.BlockMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html
https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:39272,Modifiability,config,configuration-dependent,39272,"ingular values should reveal where the spectrum switches from; non-zero to “zero” eigenvalues. With 64-bit floating point, zero; eigenvalues are typically about 1e-16 times the largest eigenvalue.; The corresponding singular vectors should be sliced away before an; action which realizes the block-matrix-side singular vectors.; svd() sets the singular values corresponding to negative; eigenvalues to exactly 0.0. Warning; The first and third stages invoke distributed matrix multiplication with; parallelism bounded by the number of resulting blocks, whereas the; second stage is executed on the leader (master) node. For matrices of; large minimum dimension, it may be preferable to run these stages; separately.; The performance of the second stage depends critically on the number of; leader (master) cores and the NumPy / SciPy configuration, viewable with; np.show_config(). For Intel machines, we recommend installing the; MKL package for Anaconda.; Consequently, the optimal value of complexity_bound is highly; configuration-dependent. Parameters:. compute_uv (bool) – If False, only compute the singular values (or eigenvalues).; complexity_bound (int) – Maximum value of \(\sqrt[3]{nmr}\) for which; scipy.linalg.svd() is used. Returns:. u (numpy.ndarray or BlockMatrix) – Left singular vectors \(U\), as a block matrix if \(n > m\) and; \(\sqrt[3]{nmr}\) exceeds complexity_bound.; Only returned if compute_uv is True.; s (numpy.ndarray) – Singular values from \(\Sigma\) in descending order.; vt (numpy.ndarray or BlockMatrix) – Right singular vectors \(V^T`\), as a block matrix if \(n \leq m\) and; \(\sqrt[3]{nmr}\) exceeds complexity_bound.; Only returned if compute_uv is True. to_matrix_table_row_major(n_partitions=None, maximum_cache_memory_in_bytes=None)[source]; Returns a matrix table with row key of row_idx and col key col_idx, whose; entries are structs of a single field element. Parameters:. n_partitions (int or None) – Number of partitions of the matrix table.; maximu",MatchSource.WIKI,docs/0.2/linalg/hail.linalg.BlockMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html
https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:3523,Performance,perform,performance,3523,"gh the same; effect can be achieved for * by using @. Warning; For binary operations, if the first operand is an ndarray and the; second operand is a block matrix, the result will be a ndarray of block; matrices. To achieve the desired behavior for + and *, place the; block matrix operand first; for -, /, and @, first convert; the ndarray to a block matrix using from_numpy(). Warning; Block matrix multiplication requires special care due to each block; of each operand being a dependency of multiple blocks in the product.; The \((i, j)\)-block in the product a @ b is computed by summing; the products of corresponding blocks in block row \(i\) of a and; block column \(j\) of b. So overall, in addition to this; multiplication and addition, the evaluation of a @ b realizes each; block of a as many times as the number of block columns of b; and realizes each block of b as many times as the number of; block rows of a.; This becomes a performance and resilience issue whenever a or b; is defined in terms of pending transformations (such as linear; algebra operations). For example, evaluating a @ (c @ d) will; effectively evaluate c @ d as many times as the number of block rows; in a.; To limit re-computation, write or cache transformed block matrix; operands before feeding them into matrix multiplication:; >>> c = BlockMatrix.read('c.bm') ; >>> d = BlockMatrix.read('d.bm') ; >>> (c @ d).write('cd.bm') ; >>> a = BlockMatrix.read('a.bm') ; >>> e = a @ BlockMatrix.read('cd.bm') . Indexing and slicing; Block matrices also support NumPy-style 2-dimensional; indexing and slicing,; with two differences.; First, slices start:stop:step must be non-empty with positive step.; Second, even if only one index is a slice, the resulting block matrix is still; 2-dimensional.; For example, for a block matrix bm with 10 rows and 10 columns:. bm[0, 0] is the element in row 0 and column 0 of bm.; bm[0:1, 0] is a block matrix with 1 row, 1 column,; and element bm[0, 0].; bm[2, :] is a block matri",MatchSource.WIKI,docs/0.2/linalg/hail.linalg.BlockMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html
https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:3811,Performance,cache,cache,3811,"; block matrix operand first; for -, /, and @, first convert; the ndarray to a block matrix using from_numpy(). Warning; Block matrix multiplication requires special care due to each block; of each operand being a dependency of multiple blocks in the product.; The \((i, j)\)-block in the product a @ b is computed by summing; the products of corresponding blocks in block row \(i\) of a and; block column \(j\) of b. So overall, in addition to this; multiplication and addition, the evaluation of a @ b realizes each; block of a as many times as the number of block columns of b; and realizes each block of b as many times as the number of; block rows of a.; This becomes a performance and resilience issue whenever a or b; is defined in terms of pending transformations (such as linear; algebra operations). For example, evaluating a @ (c @ d) will; effectively evaluate c @ d as many times as the number of block rows; in a.; To limit re-computation, write or cache transformed block matrix; operands before feeding them into matrix multiplication:; >>> c = BlockMatrix.read('c.bm') ; >>> d = BlockMatrix.read('d.bm') ; >>> (c @ d).write('cd.bm') ; >>> a = BlockMatrix.read('a.bm') ; >>> e = a @ BlockMatrix.read('cd.bm') . Indexing and slicing; Block matrices also support NumPy-style 2-dimensional; indexing and slicing,; with two differences.; First, slices start:stop:step must be non-empty with positive step.; Second, even if only one index is a slice, the resulting block matrix is still; 2-dimensional.; For example, for a block matrix bm with 10 rows and 10 columns:. bm[0, 0] is the element in row 0 and column 0 of bm.; bm[0:1, 0] is a block matrix with 1 row, 1 column,; and element bm[0, 0].; bm[2, :] is a block matrix with 1 row, 10 columns,; and elements from row 2 of bm.; bm[:3, -1] is a block matrix with 3 rows, 1 column,; and the first 3 elements of the last column of bm.; bm[::2, ::2] is a block matrix with 5 rows, 5 columns,; and all evenly-indexed elements of bm. Use fil",MatchSource.WIKI,docs/0.2/linalg/hail.linalg.BlockMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html
https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:7844,Performance,cache,cache,7844,"; Matrix slicing, and more generally filter(), filter_rows(),; and filter_cols(). These following methods always result in a block-dense matrix:. fill(); Addition or subtraction of a scalar or broadcasted vector.; Matrix multiplication, @. The following methods fail if any operand is block-sparse, but can be forced; by first applying densify(). Element-wise division between two block matrices.; Multiplication by a scalar or broadcasted vector which includes an; infinite or nan value.; Division by a scalar or broadcasted vector which includes a zero, infinite; or nan value.; Division of a scalar or broadcasted vector by a block matrix.; Element-wise exponentiation by a negative exponent.; Natural logarithm, log(). Attributes. T; Matrix transpose. block_size; Block size. element_type; The type of the elements. is_sparse; Returns True if block-sparse. n_cols; Number of columns. n_rows; Number of rows. shape; Shape of matrix. Methods. abs; Element-wise absolute value. cache; Persist this block matrix in memory. ceil; Element-wise ceiling. checkpoint; Checkpoint the block matrix. default_block_size; Default block side length. densify; Restore all dropped blocks as explicit blocks of zeros. diagonal; Extracts diagonal elements as a row vector. entries; Returns a table with the indices and value of each block matrix entry. export; Exports a stored block matrix as a delimited text file. export_blocks; Export each block of the block matrix as its own delimited text or binary file. export_rectangles; Export rectangular regions from a block matrix to delimited text or binary files. fill; Creates a block matrix with all elements the same value. filter; Filters matrix rows and columns. filter_cols; Filters matrix columns. filter_rows; Filters matrix rows. floor; Element-wise floor. from_entry_expr; Creates a block matrix using a matrix table entry expression. from_ndarray; Create a BlockMatrix from an ndarray. from_numpy; Distributes a NumPy ndarray as a block matrix. fromfile; ",MatchSource.WIKI,docs/0.2/linalg/hail.linalg.BlockMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html
https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:10509,Performance,cache,cache,10509,"t-wise square root. sum; Sums array elements over one or both axes. svd; Computes the reduced singular value decomposition. to_matrix_table_row_major; Returns a matrix table with row key of row_idx and col key col_idx, whose entries are structs of a single field element. to_ndarray; Collects a BlockMatrix into a local hail ndarray expression on driver. to_numpy; Collects the block matrix into a NumPy ndarray. to_table_row_major; Returns a table where each row represents a row in the block matrix. tofile; Collects and writes data to a binary file. tree_matmul; Matrix multiplication in situations with large inner dimension. unpersist; Unpersists this block matrix from memory/disk. write; Writes the block matrix. write_from_entry_expr; Writes a block matrix from a matrix table entry expression. property T; Matrix transpose. Returns:; BlockMatrix. abs()[source]; Element-wise absolute value. Returns:; BlockMatrix. property block_size; Block size. Returns:; int. cache()[source]; Persist this block matrix in memory.; Notes; This method is an alias for persist(""MEMORY_ONLY""). Returns:; BlockMatrix – Cached block matrix. ceil()[source]; Element-wise ceiling. Returns:; BlockMatrix. checkpoint(path, overwrite=False, force_row_major=False, stage_locally=False)[source]; Checkpoint the block matrix. Danger; Do not write or checkpoint to a path that is already an input source for the query. This can cause data loss. Parameters:. path (str) – Path for output file.; overwrite (bool) – If True, overwrite an existing file at the destination.; force_row_major (bool) – If True, transform blocks in column-major format; to row-major format before checkpointing.; If False, checkpoint blocks in their current format.; stage_locally (bool) – If True, major output will be written to temporary local storage; before being copied to output. static default_block_size()[source]; Default block side length. densify()[source]; Restore all dropped blocks as explicit blocks of zeros. Returns:; Bl",MatchSource.WIKI,docs/0.2/linalg/hail.linalg.BlockMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html
https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:21870,Performance,perform,performance,21870,"eturns:; BlockMatrix. filter_rows(rows_to_keep)[source]; Filters matrix rows. Parameters:; rows_to_keep (list of int) – Indices of rows to keep. Must be non-empty and increasing. Returns:; BlockMatrix. floor()[source]; Element-wise floor. Returns:; BlockMatrix. classmethod from_entry_expr(entry_expr, mean_impute=False, center=False, normalize=False, axis='rows', block_size=None)[source]; Creates a block matrix using a matrix table entry expression.; Examples; >>> mt = hl.balding_nichols_model(3, 25, 50); >>> bm = BlockMatrix.from_entry_expr(mt.GT.n_alt_alleles()). Notes; This convenience method writes the block matrix to a temporary file on; persistent disk and then reads the file. If you want to store the; resulting block matrix, use write_from_entry_expr() directly to; avoid writing the result twice. See write_from_entry_expr() for; further documentation. Warning; If the rows of the matrix table have been filtered to a small fraction,; then MatrixTable.repartition() before this method to improve; performance.; If you encounter a Hadoop write/replication error, increase the; number of persistent workers or the disk size per persistent worker,; or use write_from_entry_expr() to write to external storage.; This method opens n_cols / block_size files concurrently per task.; To not blow out memory when the number of columns is very large,; limit the Hadoop write buffer size; e.g. on GCP, set this property on; cluster startup (the default is 64MB):; --properties 'core:fs.gs.io.buffersize.write=1048576. Parameters:. entry_expr (Float64Expression) – Entry expression for numeric matrix entries.; mean_impute (bool) – If true, set missing values to the row mean before centering or; normalizing. If false, missing values will raise an error.; center (bool) – If true, subtract the row mean.; normalize (bool) – If true and center=False, divide by the row magnitude.; If true and center=True, divide the centered value by the; centered row magnitude.; axis (str) – One of “rows” o",MatchSource.WIKI,docs/0.2/linalg/hail.linalg.BlockMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html
https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:22125,Performance,concurren,concurrently,22125,"ix. classmethod from_entry_expr(entry_expr, mean_impute=False, center=False, normalize=False, axis='rows', block_size=None)[source]; Creates a block matrix using a matrix table entry expression.; Examples; >>> mt = hl.balding_nichols_model(3, 25, 50); >>> bm = BlockMatrix.from_entry_expr(mt.GT.n_alt_alleles()). Notes; This convenience method writes the block matrix to a temporary file on; persistent disk and then reads the file. If you want to store the; resulting block matrix, use write_from_entry_expr() directly to; avoid writing the result twice. See write_from_entry_expr() for; further documentation. Warning; If the rows of the matrix table have been filtered to a small fraction,; then MatrixTable.repartition() before this method to improve; performance.; If you encounter a Hadoop write/replication error, increase the; number of persistent workers or the disk size per persistent worker,; or use write_from_entry_expr() to write to external storage.; This method opens n_cols / block_size files concurrently per task.; To not blow out memory when the number of columns is very large,; limit the Hadoop write buffer size; e.g. on GCP, set this property on; cluster startup (the default is 64MB):; --properties 'core:fs.gs.io.buffersize.write=1048576. Parameters:. entry_expr (Float64Expression) – Entry expression for numeric matrix entries.; mean_impute (bool) – If true, set missing values to the row mean before centering or; normalizing. If false, missing values will raise an error.; center (bool) – If true, subtract the row mean.; normalize (bool) – If true and center=False, divide by the row magnitude.; If true and center=True, divide the centered value by the; centered row magnitude.; axis (str) – One of “rows” or “cols”: axis by which to normalize or center.; block_size (int, optional) – Block size. Default given by BlockMatrix.default_block_size(). classmethod from_ndarray(ndarray_expression, block_size=4096)[source]; Create a BlockMatrix from an ndarray. classmet",MatchSource.WIKI,docs/0.2/linalg/hail.linalg.BlockMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html
https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:24370,Performance,load,load,24370," less than \(2^{31}\). Parameters:. ndarray (numpy.ndarray) – ndarray with two dimensions, each of non-zero size.; block_size (int, optional) – Block size. Default given by default_block_size(). Returns:; BlockMatrix. classmethod fromfile(uri, n_rows, n_cols, block_size=None, *, _assert_type=None)[source]; Creates a block matrix from a binary file.; Examples; >>> import numpy as np; >>> a = np.random.rand(10, 20); >>> a.tofile('/local/file') . To create a block matrix of the same dimensions:; >>> bm = BlockMatrix.fromfile('file:///local/file', 10, 20) . Notes; This method, analogous to numpy.fromfile,; reads a binary file of float64 values in row-major order, such as that; produced by numpy.tofile; or BlockMatrix.tofile().; Binary files produced and consumed by tofile() and; fromfile() are not platform independent, so should only be used; for inter-operating with NumPy, not storage. Use; BlockMatrix.write() and BlockMatrix.read() to save and load; block matrices, since these methods write and read blocks in parallel; and are platform independent.; A NumPy ndarray must have type float64 for the output of; func:numpy.tofile to be a valid binary input to fromfile().; This is not checked.; The number of entries must be less than \(2^{31}\). Parameters:. uri (str, optional) – URI of binary input file.; n_rows (int) – Number of rows.; n_cols (int) – Number of columns.; block_size (int, optional) – Block size. Default given by default_block_size(). See also; from_numpy(). property is_sparse; Returns True if block-sparse.; Notes; A block matrix is block-sparse if at least of its blocks is dropped,; i.e. implicitly a block of zeros. Returns:; bool. log()[source]; Element-wise natural logarithm. Returns:; BlockMatrix. property n_cols; Number of columns. Returns:; int. property n_rows; Number of rows. Returns:; int. persist(storage_level='MEMORY_AND_DISK')[source]; Persists this block matrix in memory or on disk.; Notes; The BlockMatrix.persist() and BlockMatrix.cache(); ",MatchSource.WIKI,docs/0.2/linalg/hail.linalg.BlockMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html
https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:25405,Performance,cache,cache,25405," ndarray must have type float64 for the output of; func:numpy.tofile to be a valid binary input to fromfile().; This is not checked.; The number of entries must be less than \(2^{31}\). Parameters:. uri (str, optional) – URI of binary input file.; n_rows (int) – Number of rows.; n_cols (int) – Number of columns.; block_size (int, optional) – Block size. Default given by default_block_size(). See also; from_numpy(). property is_sparse; Returns True if block-sparse.; Notes; A block matrix is block-sparse if at least of its blocks is dropped,; i.e. implicitly a block of zeros. Returns:; bool. log()[source]; Element-wise natural logarithm. Returns:; BlockMatrix. property n_cols; Number of columns. Returns:; int. property n_rows; Number of rows. Returns:; int. persist(storage_level='MEMORY_AND_DISK')[source]; Persists this block matrix in memory or on disk.; Notes; The BlockMatrix.persist() and BlockMatrix.cache(); methods store the current block matrix on disk or in memory temporarily; to avoid redundant computation and improve the performance of Hail; pipelines. This method is not a substitution for; BlockMatrix.write(), which stores a permanent file.; Most users should use the “MEMORY_AND_DISK” storage level. See the Spark; documentation; for a more in-depth discussion of persisting data. Parameters:; storage_level (str) – Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns:; BlockMatrix – Persisted block matrix. classmethod random(n_rows, n_cols, block_size=None, seed=None, gaussian=True)[source]; Creates a block matrix with standard normal or uniform random entries.; Examples; Create a block matrix with 10 rows, 20 columns, and standard normal entries:; >>> bm = BlockMatrix.random(10, 20). Parameters:. n_rows (int) – Number of rows.; n_cols (int) – Number of columns.; block_size (int, optional) – ",MatchSource.WIKI,docs/0.2/linalg/hail.linalg.BlockMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html
https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:25534,Performance,perform,performance,25534," ndarray must have type float64 for the output of; func:numpy.tofile to be a valid binary input to fromfile().; This is not checked.; The number of entries must be less than \(2^{31}\). Parameters:. uri (str, optional) – URI of binary input file.; n_rows (int) – Number of rows.; n_cols (int) – Number of columns.; block_size (int, optional) – Block size. Default given by default_block_size(). See also; from_numpy(). property is_sparse; Returns True if block-sparse.; Notes; A block matrix is block-sparse if at least of its blocks is dropped,; i.e. implicitly a block of zeros. Returns:; bool. log()[source]; Element-wise natural logarithm. Returns:; BlockMatrix. property n_cols; Number of columns. Returns:; int. property n_rows; Number of rows. Returns:; int. persist(storage_level='MEMORY_AND_DISK')[source]; Persists this block matrix in memory or on disk.; Notes; The BlockMatrix.persist() and BlockMatrix.cache(); methods store the current block matrix on disk or in memory temporarily; to avoid redundant computation and improve the performance of Hail; pipelines. This method is not a substitution for; BlockMatrix.write(), which stores a permanent file.; Most users should use the “MEMORY_AND_DISK” storage level. See the Spark; documentation; for a more in-depth discussion of persisting data. Parameters:; storage_level (str) – Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns:; BlockMatrix – Persisted block matrix. classmethod random(n_rows, n_cols, block_size=None, seed=None, gaussian=True)[source]; Creates a block matrix with standard normal or uniform random entries.; Examples; Create a block matrix with 10 rows, 20 columns, and standard normal entries:; >>> bm = BlockMatrix.random(10, 20). Parameters:. n_rows (int) – Number of rows.; n_cols (int) – Number of columns.; block_size (int, optional) – ",MatchSource.WIKI,docs/0.2/linalg/hail.linalg.BlockMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html
https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:38972,Performance,perform,performance,38972,"d stage. Furthermore, due to finite; precision, the zero eigenvalues of \(X^T X\) or \(X X^T\) will; only be approximately zero.; If the rank is not known ahead, examining the relative sizes of the; trailing singular values should reveal where the spectrum switches from; non-zero to “zero” eigenvalues. With 64-bit floating point, zero; eigenvalues are typically about 1e-16 times the largest eigenvalue.; The corresponding singular vectors should be sliced away before an; action which realizes the block-matrix-side singular vectors.; svd() sets the singular values corresponding to negative; eigenvalues to exactly 0.0. Warning; The first and third stages invoke distributed matrix multiplication with; parallelism bounded by the number of resulting blocks, whereas the; second stage is executed on the leader (master) node. For matrices of; large minimum dimension, it may be preferable to run these stages; separately.; The performance of the second stage depends critically on the number of; leader (master) cores and the NumPy / SciPy configuration, viewable with; np.show_config(). For Intel machines, we recommend installing the; MKL package for Anaconda.; Consequently, the optimal value of complexity_bound is highly; configuration-dependent. Parameters:. compute_uv (bool) – If False, only compute the singular values (or eigenvalues).; complexity_bound (int) – Maximum value of \(\sqrt[3]{nmr}\) for which; scipy.linalg.svd() is used. Returns:. u (numpy.ndarray or BlockMatrix) – Left singular vectors \(U\), as a block matrix if \(n > m\) and; \(\sqrt[3]{nmr}\) exceeds complexity_bound.; Only returned if compute_uv is True.; s (numpy.ndarray) – Singular values from \(\Sigma\) in descending order.; vt (numpy.ndarray or BlockMatrix) – Right singular vectors \(V^T`\), as a block matrix if \(n \leq m\) and; \(\sqrt[3]{nmr}\) exceeds complexity_bound.; Only returned if compute_uv is True. to_matrix_table_row_major(n_partitions=None, maximum_cache_memory_in_bytes=None)[source]; Ret",MatchSource.WIKI,docs/0.2/linalg/hail.linalg.BlockMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html
https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:40343,Performance,cache,cache,40343,"ompute_uv (bool) – If False, only compute the singular values (or eigenvalues).; complexity_bound (int) – Maximum value of \(\sqrt[3]{nmr}\) for which; scipy.linalg.svd() is used. Returns:. u (numpy.ndarray or BlockMatrix) – Left singular vectors \(U\), as a block matrix if \(n > m\) and; \(\sqrt[3]{nmr}\) exceeds complexity_bound.; Only returned if compute_uv is True.; s (numpy.ndarray) – Singular values from \(\Sigma\) in descending order.; vt (numpy.ndarray or BlockMatrix) – Right singular vectors \(V^T`\), as a block matrix if \(n \leq m\) and; \(\sqrt[3]{nmr}\) exceeds complexity_bound.; Only returned if compute_uv is True. to_matrix_table_row_major(n_partitions=None, maximum_cache_memory_in_bytes=None)[source]; Returns a matrix table with row key of row_idx and col key col_idx, whose; entries are structs of a single field element. Parameters:. n_partitions (int or None) – Number of partitions of the matrix table.; maximum_cache_memory_in_bytes (int or None) – The amount of memory to reserve, per partition, to cache rows of the; matrix in memory. This value must be at least large enough to hold; one row of the matrix in memory. If this value is exactly the size of; one row, then a partition makes a network request for every row of; every block. Larger values reduce the number of network requests. If; memory permits, setting this value to the size of one output; partition permits one network request per block per partition. Notes; Does not support block-sparse matrices. Returns:; MatrixTable – Matrix table where each entry corresponds to an entry in the block matrix. to_ndarray()[source]; Collects a BlockMatrix into a local hail ndarray expression on driver. This should not; be done for large matrices. Returns:; NDArrayExpression. to_numpy(_force_blocking=False)[source]; Collects the block matrix into a NumPy ndarray.; Examples; >>> bm = BlockMatrix.random(10, 20); >>> a = bm.to_numpy(). Notes; The resulting ndarray will have the same shape as the block matrix",MatchSource.WIKI,docs/0.2/linalg/hail.linalg.BlockMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html
https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:42269,Performance,cache,cache,42269,". Notes; The resulting ndarray will have the same shape as the block matrix. Returns:; numpy.ndarray. to_table_row_major(n_partitions=None, maximum_cache_memory_in_bytes=None)[source]; Returns a table where each row represents a row in the block matrix. The resulting table has the following fields:; row_idx (:py:data.`tint64`, key field) – Row index; entries (tarray of tfloat64) – Entries for the row. Examples; >>> import numpy as np; >>> block_matrix = BlockMatrix.from_numpy(np.array([[1, 2], [3, 4], [5, 6]]), 2); >>> t = block_matrix.to_table_row_major(); >>> t.show(); +---------+---------------------+; | row_idx | entries |; +---------+---------------------+; | int64 | array<float64> |; +---------+---------------------+; | 0 | [1.00e+00,2.00e+00] |; | 1 | [3.00e+00,4.00e+00] |; | 2 | [5.00e+00,6.00e+00] |; +---------+---------------------+. Parameters:. n_partitions (int or None) – Number of partitions of the table.; maximum_cache_memory_in_bytes (int or None) – The amount of memory to reserve, per partition, to cache rows of the; matrix in memory. This value must be at least large enough to hold; one row of the matrix in memory. If this value is exactly the size of; one row, then a partition makes a network request for every row of; every block. Larger values reduce the number of network requests. If; memory permits, setting this value to the size of one output; partition permits one network request per block per partition. Notes; Does not support block-sparse matrices. Returns:; Table – Table where each row corresponds to a row in the block matrix. tofile(uri)[source]; Collects and writes data to a binary file.; Examples; >>> import numpy as np; >>> bm = BlockMatrix.random(10, 20); >>> bm.tofile('file:///local/file') . To create a numpy.ndarray of the same dimensions:; >>> a = np.fromfile('/local/file').reshape((10, 20)) . Notes; This method, analogous to numpy.tofile,; produces a binary file of float64 values in row-major order, which can; be read by function",MatchSource.WIKI,docs/0.2/linalg/hail.linalg.BlockMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html
https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:43533,Performance,load,load,43533,"its, setting this value to the size of one output; partition permits one network request per block per partition. Notes; Does not support block-sparse matrices. Returns:; Table – Table where each row corresponds to a row in the block matrix. tofile(uri)[source]; Collects and writes data to a binary file.; Examples; >>> import numpy as np; >>> bm = BlockMatrix.random(10, 20); >>> bm.tofile('file:///local/file') . To create a numpy.ndarray of the same dimensions:; >>> a = np.fromfile('/local/file').reshape((10, 20)) . Notes; This method, analogous to numpy.tofile,; produces a binary file of float64 values in row-major order, which can; be read by functions such as numpy.fromfile; (if a local file) and BlockMatrix.fromfile().; Binary files produced and consumed by tofile() and; fromfile() are not platform independent, so should only be used; for inter-operating with NumPy, not storage. Use; BlockMatrix.write() and BlockMatrix.read() to save and load; block matrices, since these methods write and read blocks in parallel; and are platform independent.; The number of entries must be less than \(2^{31}\). Parameters:; uri (str, optional) – URI of binary output file. See also; to_numpy(). tree_matmul(b, *, splits, path_prefix=None)[source]; Matrix multiplication in situations with large inner dimension.; This function splits a single matrix multiplication into split_on_inner smaller matrix multiplications,; does the smaller multiplications, checkpoints them with names defined by file_name_prefix, and adds them; together. This is useful in cases when the multiplication of two large matrices results in a much smaller matrix. Parameters:. b (numpy.ndarray or BlockMatrix); splits (int (keyword only argument)) – The number of smaller multiplications to do.; path_prefix (str (keyword only argument)) – The prefix of the path to write the block matrices to. If unspecified, writes to a tmpdir. Returns:; BlockMatrix. unpersist()[source]; Unpersists this block matrix from memory/dis",MatchSource.WIKI,docs/0.2/linalg/hail.linalg.BlockMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html
https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:45748,Performance,load,loaded,45748,"rite=False, force_row_major=False, stage_locally=False)[source]; Writes the block matrix. Danger; Do not write or checkpoint to a path that is already an input source for the query. This can cause data loss. Parameters:. path (str) – Path for output file.; overwrite (bool) – If True, overwrite an existing file at the destination.; force_row_major (bool) – If True, transform blocks in column-major format; to row-major format before writing.; If False, write blocks in their current format.; stage_locally (bool) – If True, major output will be written to temporary local storage; before being copied to output. static write_from_entry_expr(entry_expr, path, overwrite=False, mean_impute=False, center=False, normalize=False, axis='rows', block_size=None)[source]; Writes a block matrix from a matrix table entry expression.; Examples; >>> mt = hl.balding_nichols_model(3, 25, 50); >>> BlockMatrix.write_from_entry_expr(mt.GT.n_alt_alleles(),; ... 'output/model.bm'). Notes; The resulting file can be loaded with BlockMatrix.read().; Blocks are stored row-major.; If a pipelined transformation significantly downsamples the rows of the; underlying matrix table, then repartitioning the matrix table ahead of; this method will greatly improve its performance.; By default, this method will fail if any values are missing (to be clear,; special float values like nan are not missing values). Set mean_impute to replace missing values with the row mean before; possibly centering or normalizing. If all values are missing, the row; mean is nan.; Set center to shift each row to have mean zero before possibly; normalizing.; Set normalize to normalize each row to have unit length. To standardize each row, regarded as an empirical distribution, to have; mean 0 and variance 1, set center and normalize and then multiply; the result by sqrt(n_cols). Warning; If the rows of the matrix table have been filtered to a small fraction,; then MatrixTable.repartition() before this method to improve; perfor",MatchSource.WIKI,docs/0.2/linalg/hail.linalg.BlockMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html
https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:45993,Performance,perform,performance,45993,"rce for the query. This can cause data loss. Parameters:. path (str) – Path for output file.; overwrite (bool) – If True, overwrite an existing file at the destination.; force_row_major (bool) – If True, transform blocks in column-major format; to row-major format before writing.; If False, write blocks in their current format.; stage_locally (bool) – If True, major output will be written to temporary local storage; before being copied to output. static write_from_entry_expr(entry_expr, path, overwrite=False, mean_impute=False, center=False, normalize=False, axis='rows', block_size=None)[source]; Writes a block matrix from a matrix table entry expression.; Examples; >>> mt = hl.balding_nichols_model(3, 25, 50); >>> BlockMatrix.write_from_entry_expr(mt.GT.n_alt_alleles(),; ... 'output/model.bm'). Notes; The resulting file can be loaded with BlockMatrix.read().; Blocks are stored row-major.; If a pipelined transformation significantly downsamples the rows of the; underlying matrix table, then repartitioning the matrix table ahead of; this method will greatly improve its performance.; By default, this method will fail if any values are missing (to be clear,; special float values like nan are not missing values). Set mean_impute to replace missing values with the row mean before; possibly centering or normalizing. If all values are missing, the row; mean is nan.; Set center to shift each row to have mean zero before possibly; normalizing.; Set normalize to normalize each row to have unit length. To standardize each row, regarded as an empirical distribution, to have; mean 0 and variance 1, set center and normalize and then multiply; the result by sqrt(n_cols). Warning; If the rows of the matrix table have been filtered to a small fraction,; then MatrixTable.repartition() before this method to improve; performance.; This method opens n_cols / block_size files concurrently per task.; To not blow out memory when the number of columns is very large,; limit the Hadoop write",MatchSource.WIKI,docs/0.2/linalg/hail.linalg.BlockMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html
https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:46737,Performance,perform,performance,46737,"; The resulting file can be loaded with BlockMatrix.read().; Blocks are stored row-major.; If a pipelined transformation significantly downsamples the rows of the; underlying matrix table, then repartitioning the matrix table ahead of; this method will greatly improve its performance.; By default, this method will fail if any values are missing (to be clear,; special float values like nan are not missing values). Set mean_impute to replace missing values with the row mean before; possibly centering or normalizing. If all values are missing, the row; mean is nan.; Set center to shift each row to have mean zero before possibly; normalizing.; Set normalize to normalize each row to have unit length. To standardize each row, regarded as an empirical distribution, to have; mean 0 and variance 1, set center and normalize and then multiply; the result by sqrt(n_cols). Warning; If the rows of the matrix table have been filtered to a small fraction,; then MatrixTable.repartition() before this method to improve; performance.; This method opens n_cols / block_size files concurrently per task.; To not blow out memory when the number of columns is very large,; limit the Hadoop write buffer size; e.g. on GCP, set this property on; cluster startup (the default is 64MB):; --properties 'core:fs.gs.io.buffersize.write=1048576. Parameters:. entry_expr (Float64Expression) – Entry expression for numeric matrix entries.; path (str) – Path for output.; overwrite (bool) – If True, overwrite an existing file at the destination.; mean_impute (bool) – If true, set missing values to the row mean before centering or; normalizing. If false, missing values will raise an error.; center (bool) – If true, subtract the row mean.; normalize (bool) – If true and center=False, divide by the row magnitude.; If true and center=True, divide the centered value by the; centered row magnitude.; axis (str) – One of “rows” or “cols”: axis by which to normalize or center.; block_size (int, optional) – Block size. ",MatchSource.WIKI,docs/0.2/linalg/hail.linalg.BlockMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html
https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:46795,Performance,concurren,concurrently,46795,"ocks are stored row-major.; If a pipelined transformation significantly downsamples the rows of the; underlying matrix table, then repartitioning the matrix table ahead of; this method will greatly improve its performance.; By default, this method will fail if any values are missing (to be clear,; special float values like nan are not missing values). Set mean_impute to replace missing values with the row mean before; possibly centering or normalizing. If all values are missing, the row; mean is nan.; Set center to shift each row to have mean zero before possibly; normalizing.; Set normalize to normalize each row to have unit length. To standardize each row, regarded as an empirical distribution, to have; mean 0 and variance 1, set center and normalize and then multiply; the result by sqrt(n_cols). Warning; If the rows of the matrix table have been filtered to a small fraction,; then MatrixTable.repartition() before this method to improve; performance.; This method opens n_cols / block_size files concurrently per task.; To not blow out memory when the number of columns is very large,; limit the Hadoop write buffer size; e.g. on GCP, set this property on; cluster startup (the default is 64MB):; --properties 'core:fs.gs.io.buffersize.write=1048576. Parameters:. entry_expr (Float64Expression) – Entry expression for numeric matrix entries.; path (str) – Path for output.; overwrite (bool) – If True, overwrite an existing file at the destination.; mean_impute (bool) – If true, set missing values to the row mean before centering or; normalizing. If false, missing values will raise an error.; center (bool) – If true, subtract the row mean.; normalize (bool) – If true and center=False, divide by the row magnitude.; If true and center=True, divide the centered value by the; centered row magnitude.; axis (str) – One of “rows” or “cols”: axis by which to normalize or center.; block_size (int, optional) – Block size. Default given by BlockMatrix.default_block_size(). Previous; N",MatchSource.WIKI,docs/0.2/linalg/hail.linalg.BlockMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html
https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:6292,Safety,avoid,avoiding,6292,", statistical geneticists often want to compute and manipulate a; banded correlation matrix capturing “linkage disequilibrium” between nearby; variants along the genome. In this case, working with the full correlation; matrix for tens of millions of variants would be prohibitively expensive,; and in any case, entries far from the diagonal are either not of interest or; ought to be zeroed out before downstream linear algebra.; To enable such computations, block matrices do not require that all blocks; be realized explicitly. Implicit (dropped) blocks behave as blocks of; zeroes, so we refer to a block matrix in which at least one block is; implicitly zero as a block-sparse matrix. Otherwise, we say the matrix; is block-dense. The property is_sparse() encodes this state.; Dropped blocks are not stored in memory or on write(). In fact,; blocks that are dropped prior to an action like export() or; to_numpy() are never computed in the first place, nor are any blocks; of upstream operands on which only dropped blocks depend! In addition,; linear algebra is accelerated by avoiding, for example, explicit addition of; or multiplication by blocks of zeroes.; Block-sparse matrices may be created with; sparsify_band(),; sparsify_rectangles(),; sparsify_row_intervals(),; and sparsify_triangle().; The following methods naturally propagate block-sparsity:. Addition and subtraction “union” realized blocks.; Element-wise multiplication “intersects” realized blocks.; Transpose “transposes” realized blocks.; abs() and sqrt() preserve the realized blocks.; sum() along an axis realizes those blocks for which at least one; block summand is realized.; Matrix slicing, and more generally filter(), filter_rows(),; and filter_cols(). These following methods always result in a block-dense matrix:. fill(); Addition or subtraction of a scalar or broadcasted vector.; Matrix multiplication, @. The following methods fail if any operand is block-sparse, but can be forced; by first applying densify()",MatchSource.WIKI,docs/0.2/linalg/hail.linalg.BlockMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html
https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:21638,Safety,avoid,avoid,21638,"ces of columns to keep. Must be non-empty and increasing. Returns:; BlockMatrix. filter_cols(cols_to_keep)[source]; Filters matrix columns. Parameters:; cols_to_keep (list of int) – Indices of columns to keep. Must be non-empty and increasing. Returns:; BlockMatrix. filter_rows(rows_to_keep)[source]; Filters matrix rows. Parameters:; rows_to_keep (list of int) – Indices of rows to keep. Must be non-empty and increasing. Returns:; BlockMatrix. floor()[source]; Element-wise floor. Returns:; BlockMatrix. classmethod from_entry_expr(entry_expr, mean_impute=False, center=False, normalize=False, axis='rows', block_size=None)[source]; Creates a block matrix using a matrix table entry expression.; Examples; >>> mt = hl.balding_nichols_model(3, 25, 50); >>> bm = BlockMatrix.from_entry_expr(mt.GT.n_alt_alleles()). Notes; This convenience method writes the block matrix to a temporary file on; persistent disk and then reads the file. If you want to store the; resulting block matrix, use write_from_entry_expr() directly to; avoid writing the result twice. See write_from_entry_expr() for; further documentation. Warning; If the rows of the matrix table have been filtered to a small fraction,; then MatrixTable.repartition() before this method to improve; performance.; If you encounter a Hadoop write/replication error, increase the; number of persistent workers or the disk size per persistent worker,; or use write_from_entry_expr() to write to external storage.; This method opens n_cols / block_size files concurrently per task.; To not blow out memory when the number of columns is very large,; limit the Hadoop write buffer size; e.g. on GCP, set this property on; cluster startup (the default is 64MB):; --properties 'core:fs.gs.io.buffersize.write=1048576. Parameters:. entry_expr (Float64Expression) – Entry expression for numeric matrix entries.; mean_impute (bool) – If true, set missing values to the row mean before centering or; normalizing. If false, missing values will raise a",MatchSource.WIKI,docs/0.2/linalg/hail.linalg.BlockMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html
https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:25490,Safety,avoid,avoid,25490," ndarray must have type float64 for the output of; func:numpy.tofile to be a valid binary input to fromfile().; This is not checked.; The number of entries must be less than \(2^{31}\). Parameters:. uri (str, optional) – URI of binary input file.; n_rows (int) – Number of rows.; n_cols (int) – Number of columns.; block_size (int, optional) – Block size. Default given by default_block_size(). See also; from_numpy(). property is_sparse; Returns True if block-sparse.; Notes; A block matrix is block-sparse if at least of its blocks is dropped,; i.e. implicitly a block of zeros. Returns:; bool. log()[source]; Element-wise natural logarithm. Returns:; BlockMatrix. property n_cols; Number of columns. Returns:; int. property n_rows; Number of rows. Returns:; int. persist(storage_level='MEMORY_AND_DISK')[source]; Persists this block matrix in memory or on disk.; Notes; The BlockMatrix.persist() and BlockMatrix.cache(); methods store the current block matrix on disk or in memory temporarily; to avoid redundant computation and improve the performance of Hail; pipelines. This method is not a substitution for; BlockMatrix.write(), which stores a permanent file.; Most users should use the “MEMORY_AND_DISK” storage level. See the Spark; documentation; for a more in-depth discussion of persisting data. Parameters:; storage_level (str) – Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns:; BlockMatrix – Persisted block matrix. classmethod random(n_rows, n_cols, block_size=None, seed=None, gaussian=True)[source]; Creates a block matrix with standard normal or uniform random entries.; Examples; Create a block matrix with 10 rows, 20 columns, and standard normal entries:; >>> bm = BlockMatrix.random(10, 20). Parameters:. n_rows (int) – Number of rows.; n_cols (int) – Number of columns.; block_size (int, optional) – ",MatchSource.WIKI,docs/0.2/linalg/hail.linalg.BlockMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html
https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:25496,Safety,redund,redundant,25496," ndarray must have type float64 for the output of; func:numpy.tofile to be a valid binary input to fromfile().; This is not checked.; The number of entries must be less than \(2^{31}\). Parameters:. uri (str, optional) – URI of binary input file.; n_rows (int) – Number of rows.; n_cols (int) – Number of columns.; block_size (int, optional) – Block size. Default given by default_block_size(). See also; from_numpy(). property is_sparse; Returns True if block-sparse.; Notes; A block matrix is block-sparse if at least of its blocks is dropped,; i.e. implicitly a block of zeros. Returns:; bool. log()[source]; Element-wise natural logarithm. Returns:; BlockMatrix. property n_cols; Number of columns. Returns:; int. property n_rows; Number of rows. Returns:; int. persist(storage_level='MEMORY_AND_DISK')[source]; Persists this block matrix in memory or on disk.; Notes; The BlockMatrix.persist() and BlockMatrix.cache(); methods store the current block matrix on disk or in memory temporarily; to avoid redundant computation and improve the performance of Hail; pipelines. This method is not a substitution for; BlockMatrix.write(), which stores a permanent file.; Most users should use the “MEMORY_AND_DISK” storage level. See the Spark; documentation; for a more in-depth discussion of persisting data. Parameters:; storage_level (str) – Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns:; BlockMatrix – Persisted block matrix. classmethod random(n_rows, n_cols, block_size=None, seed=None, gaussian=True)[source]; Creates a block matrix with standard normal or uniform random entries.; Examples; Create a block matrix with 10 rows, 20 columns, and standard normal entries:; >>> bm = BlockMatrix.random(10, 20). Parameters:. n_rows (int) – Number of rows.; n_cols (int) – Number of columns.; block_size (int, optional) – ",MatchSource.WIKI,docs/0.2/linalg/hail.linalg.BlockMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html
https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:848,Testability,test,tested,848,"﻿. Hail | ; BlockMatrix. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; linalg; BlockMatrix. View page source. BlockMatrix. class hail.linalg.BlockMatrix[source]; Hail’s block-distributed matrix of tfloat64 elements. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. A block matrix is a distributed analogue of a two-dimensional; NumPy ndarray with; shape (n_rows, n_cols) and NumPy dtype float64.; Import the class with:; >>> from hail.linalg import BlockMatrix. Under the hood, block matrices are partitioned like a checkerboard into; square blocks with side length a common block size. Blocks in the final row; or column of blocks may be truncated, so block size need not evenly divide; the matrix dimensions. Block size defaults to the value given by; default_block_size().; Operations and broadcasting; The core operations are consistent with NumPy: +, -, *, and; / for element-wise addition, subtraction, multiplication, and division;; @ for matrix multiplication; T for transpose; and ** for; element-wise exponentiation to a scalar power.; For element-wise binary operations, each operand may be a block matrix, an; ndarray, or a scalar (int or float). For matrix; multiplication, each operand may be a block matrix or an ndarray. If either; operand is a block matrix, the result is a block matrix. Binary operations; between block ",MatchSource.WIKI,docs/0.2/linalg/hail.linalg.BlockMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html
https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:7570,Testability,log,logarithm,7570," Addition and subtraction “union” realized blocks.; Element-wise multiplication “intersects” realized blocks.; Transpose “transposes” realized blocks.; abs() and sqrt() preserve the realized blocks.; sum() along an axis realizes those blocks for which at least one; block summand is realized.; Matrix slicing, and more generally filter(), filter_rows(),; and filter_cols(). These following methods always result in a block-dense matrix:. fill(); Addition or subtraction of a scalar or broadcasted vector.; Matrix multiplication, @. The following methods fail if any operand is block-sparse, but can be forced; by first applying densify(). Element-wise division between two block matrices.; Multiplication by a scalar or broadcasted vector which includes an; infinite or nan value.; Division by a scalar or broadcasted vector which includes a zero, infinite; or nan value.; Division of a scalar or broadcasted vector by a block matrix.; Element-wise exponentiation by a negative exponent.; Natural logarithm, log(). Attributes. T; Matrix transpose. block_size; Block size. element_type; The type of the elements. is_sparse; Returns True if block-sparse. n_cols; Number of columns. n_rows; Number of rows. shape; Shape of matrix. Methods. abs; Element-wise absolute value. cache; Persist this block matrix in memory. ceil; Element-wise ceiling. checkpoint; Checkpoint the block matrix. default_block_size; Default block side length. densify; Restore all dropped blocks as explicit blocks of zeros. diagonal; Extracts diagonal elements as a row vector. entries; Returns a table with the indices and value of each block matrix entry. export; Exports a stored block matrix as a delimited text file. export_blocks; Export each block of the block matrix as its own delimited text or binary file. export_rectangles; Export rectangular regions from a block matrix to delimited text or binary files. fill; Creates a block matrix with all elements the same value. filter; Filters matrix rows and columns. filter_",MatchSource.WIKI,docs/0.2/linalg/hail.linalg.BlockMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html
https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:7581,Testability,log,log,7581," Addition and subtraction “union” realized blocks.; Element-wise multiplication “intersects” realized blocks.; Transpose “transposes” realized blocks.; abs() and sqrt() preserve the realized blocks.; sum() along an axis realizes those blocks for which at least one; block summand is realized.; Matrix slicing, and more generally filter(), filter_rows(),; and filter_cols(). These following methods always result in a block-dense matrix:. fill(); Addition or subtraction of a scalar or broadcasted vector.; Matrix multiplication, @. The following methods fail if any operand is block-sparse, but can be forced; by first applying densify(). Element-wise division between two block matrices.; Multiplication by a scalar or broadcasted vector which includes an; infinite or nan value.; Division by a scalar or broadcasted vector which includes a zero, infinite; or nan value.; Division of a scalar or broadcasted vector by a block matrix.; Element-wise exponentiation by a negative exponent.; Natural logarithm, log(). Attributes. T; Matrix transpose. block_size; Block size. element_type; The type of the elements. is_sparse; Returns True if block-sparse. n_cols; Number of columns. n_rows; Number of rows. shape; Shape of matrix. Methods. abs; Element-wise absolute value. cache; Persist this block matrix in memory. ceil; Element-wise ceiling. checkpoint; Checkpoint the block matrix. default_block_size; Default block side length. densify; Restore all dropped blocks as explicit blocks of zeros. diagonal; Extracts diagonal elements as a row vector. entries; Returns a table with the indices and value of each block matrix entry. export; Exports a stored block matrix as a delimited text file. export_blocks; Export each block of the block matrix as its own delimited text or binary file. export_rectangles; Export rectangular regions from a block matrix to delimited text or binary files. fill; Creates a block matrix with all elements the same value. filter; Filters matrix rows and columns. filter_",MatchSource.WIKI,docs/0.2/linalg/hail.linalg.BlockMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html
https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:8908,Testability,log,log,8908,"t; Checkpoint the block matrix. default_block_size; Default block side length. densify; Restore all dropped blocks as explicit blocks of zeros. diagonal; Extracts diagonal elements as a row vector. entries; Returns a table with the indices and value of each block matrix entry. export; Exports a stored block matrix as a delimited text file. export_blocks; Export each block of the block matrix as its own delimited text or binary file. export_rectangles; Export rectangular regions from a block matrix to delimited text or binary files. fill; Creates a block matrix with all elements the same value. filter; Filters matrix rows and columns. filter_cols; Filters matrix columns. filter_rows; Filters matrix rows. floor; Element-wise floor. from_entry_expr; Creates a block matrix using a matrix table entry expression. from_ndarray; Create a BlockMatrix from an ndarray. from_numpy; Distributes a NumPy ndarray as a block matrix. fromfile; Creates a block matrix from a binary file. log; Element-wise natural logarithm. persist; Persists this block matrix in memory or on disk. random; Creates a block matrix with standard normal or uniform random entries. read; Reads a block matrix. rectangles_to_numpy; Instantiates a NumPy ndarray from files of rectangles written out using export_rectangles() or export_blocks(). sparsify_band; Filter to a diagonal band. sparsify_rectangles; Filter to blocks overlapping the union of rectangular regions. sparsify_row_intervals; Creates a block-sparse matrix by filtering to an interval for each row. sparsify_triangle; Filter to the upper or lower triangle. sqrt; Element-wise square root. sum; Sums array elements over one or both axes. svd; Computes the reduced singular value decomposition. to_matrix_table_row_major; Returns a matrix table with row key of row_idx and col key col_idx, whose entries are structs of a single field element. to_ndarray; Collects a BlockMatrix into a local hail ndarray expression on driver. to_numpy; Collects the block matrix ",MatchSource.WIKI,docs/0.2/linalg/hail.linalg.BlockMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html
https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:8934,Testability,log,logarithm,8934,"t; Checkpoint the block matrix. default_block_size; Default block side length. densify; Restore all dropped blocks as explicit blocks of zeros. diagonal; Extracts diagonal elements as a row vector. entries; Returns a table with the indices and value of each block matrix entry. export; Exports a stored block matrix as a delimited text file. export_blocks; Export each block of the block matrix as its own delimited text or binary file. export_rectangles; Export rectangular regions from a block matrix to delimited text or binary files. fill; Creates a block matrix with all elements the same value. filter; Filters matrix rows and columns. filter_cols; Filters matrix columns. filter_rows; Filters matrix rows. floor; Element-wise floor. from_entry_expr; Creates a block matrix using a matrix table entry expression. from_ndarray; Create a BlockMatrix from an ndarray. from_numpy; Distributes a NumPy ndarray as a block matrix. fromfile; Creates a block matrix from a binary file. log; Element-wise natural logarithm. persist; Persists this block matrix in memory or on disk. random; Creates a block matrix with standard normal or uniform random entries. read; Reads a block matrix. rectangles_to_numpy; Instantiates a NumPy ndarray from files of rectangles written out using export_rectangles() or export_blocks(). sparsify_band; Filter to a diagonal band. sparsify_rectangles; Filter to blocks overlapping the union of rectangular regions. sparsify_row_intervals; Creates a block-sparse matrix by filtering to an interval for each row. sparsify_triangle; Filter to the upper or lower triangle. sqrt; Element-wise square root. sum; Sums array elements over one or both axes. svd; Computes the reduced singular value decomposition. to_matrix_table_row_major; Returns a matrix table with row key of row_idx and col key col_idx, whose entries are structs of a single field element. to_ndarray; Collects a BlockMatrix into a local hail ndarray expression on driver. to_numpy; Collects the block matrix ",MatchSource.WIKI,docs/0.2/linalg/hail.linalg.BlockMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html
https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:25083,Testability,log,log,25083,"y numpy.tofile; or BlockMatrix.tofile().; Binary files produced and consumed by tofile() and; fromfile() are not platform independent, so should only be used; for inter-operating with NumPy, not storage. Use; BlockMatrix.write() and BlockMatrix.read() to save and load; block matrices, since these methods write and read blocks in parallel; and are platform independent.; A NumPy ndarray must have type float64 for the output of; func:numpy.tofile to be a valid binary input to fromfile().; This is not checked.; The number of entries must be less than \(2^{31}\). Parameters:. uri (str, optional) – URI of binary input file.; n_rows (int) – Number of rows.; n_cols (int) – Number of columns.; block_size (int, optional) – Block size. Default given by default_block_size(). See also; from_numpy(). property is_sparse; Returns True if block-sparse.; Notes; A block matrix is block-sparse if at least of its blocks is dropped,; i.e. implicitly a block of zeros. Returns:; bool. log()[source]; Element-wise natural logarithm. Returns:; BlockMatrix. property n_cols; Number of columns. Returns:; int. property n_rows; Number of rows. Returns:; int. persist(storage_level='MEMORY_AND_DISK')[source]; Persists this block matrix in memory or on disk.; Notes; The BlockMatrix.persist() and BlockMatrix.cache(); methods store the current block matrix on disk or in memory temporarily; to avoid redundant computation and improve the performance of Hail; pipelines. This method is not a substitution for; BlockMatrix.write(), which stores a permanent file.; Most users should use the “MEMORY_AND_DISK” storage level. See the Spark; documentation; for a more in-depth discussion of persisting data. Parameters:; storage_level (str) – Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns:; BlockMatrix – Persisted block matrix. classmethod ran",MatchSource.WIKI,docs/0.2/linalg/hail.linalg.BlockMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html
https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:25120,Testability,log,logarithm,25120,"y numpy.tofile; or BlockMatrix.tofile().; Binary files produced and consumed by tofile() and; fromfile() are not platform independent, so should only be used; for inter-operating with NumPy, not storage. Use; BlockMatrix.write() and BlockMatrix.read() to save and load; block matrices, since these methods write and read blocks in parallel; and are platform independent.; A NumPy ndarray must have type float64 for the output of; func:numpy.tofile to be a valid binary input to fromfile().; This is not checked.; The number of entries must be less than \(2^{31}\). Parameters:. uri (str, optional) – URI of binary input file.; n_rows (int) – Number of rows.; n_cols (int) – Number of columns.; block_size (int, optional) – Block size. Default given by default_block_size(). See also; from_numpy(). property is_sparse; Returns True if block-sparse.; Notes; A block matrix is block-sparse if at least of its blocks is dropped,; i.e. implicitly a block of zeros. Returns:; bool. log()[source]; Element-wise natural logarithm. Returns:; BlockMatrix. property n_cols; Number of columns. Returns:; int. property n_rows; Number of rows. Returns:; int. persist(storage_level='MEMORY_AND_DISK')[source]; Persists this block matrix in memory or on disk.; Notes; The BlockMatrix.persist() and BlockMatrix.cache(); methods store the current block matrix on disk or in memory temporarily; to avoid redundant computation and improve the performance of Hail; pipelines. This method is not a substitution for; BlockMatrix.write(), which stores a permanent file.; Most users should use the “MEMORY_AND_DISK” storage level. See the Spark; documentation; for a more in-depth discussion of persisting data. Parameters:; storage_level (str) – Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns:; BlockMatrix – Persisted block matrix. classmethod ran",MatchSource.WIKI,docs/0.2/linalg/hail.linalg.BlockMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html
https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:46074,Usability,clear,clear,46074,"tion.; force_row_major (bool) – If True, transform blocks in column-major format; to row-major format before writing.; If False, write blocks in their current format.; stage_locally (bool) – If True, major output will be written to temporary local storage; before being copied to output. static write_from_entry_expr(entry_expr, path, overwrite=False, mean_impute=False, center=False, normalize=False, axis='rows', block_size=None)[source]; Writes a block matrix from a matrix table entry expression.; Examples; >>> mt = hl.balding_nichols_model(3, 25, 50); >>> BlockMatrix.write_from_entry_expr(mt.GT.n_alt_alleles(),; ... 'output/model.bm'). Notes; The resulting file can be loaded with BlockMatrix.read().; Blocks are stored row-major.; If a pipelined transformation significantly downsamples the rows of the; underlying matrix table, then repartitioning the matrix table ahead of; this method will greatly improve its performance.; By default, this method will fail if any values are missing (to be clear,; special float values like nan are not missing values). Set mean_impute to replace missing values with the row mean before; possibly centering or normalizing. If all values are missing, the row; mean is nan.; Set center to shift each row to have mean zero before possibly; normalizing.; Set normalize to normalize each row to have unit length. To standardize each row, regarded as an empirical distribution, to have; mean 0 and variance 1, set center and normalize and then multiply; the result by sqrt(n_cols). Warning; If the rows of the matrix table have been filtered to a small fraction,; then MatrixTable.repartition() before this method to improve; performance.; This method opens n_cols / block_size files concurrently per task.; To not blow out memory when the number of columns is very large,; limit the Hadoop write buffer size; e.g. on GCP, set this property on; cluster startup (the default is 64MB):; --properties 'core:fs.gs.io.buffersize.write=1048576. Parameters:. entry_ex",MatchSource.WIKI,docs/0.2/linalg/hail.linalg.BlockMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html
https://hail.is/docs/0.2/linalg/index.html:792,Deployability,pipeline,pipelines,792,"﻿. Hail | ; linalg. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; linalg. View page source. linalg; File formats and interface for numeric matrices are experimental.; Improvements to Hail 0.2 may necessitate re-writing pipelines and files; to maintain compatibility. Classes. BlockMatrix; Hail's block-distributed matrix of tfloat64 elements. Modules. utils. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/linalg/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/linalg/index.html
https://hail.is/docs/0.2/linalg/index.html:989,Deployability,update,updated,989,"﻿. Hail | ; linalg. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; linalg. View page source. linalg; File formats and interface for numeric matrices are experimental.; Improvements to Hail 0.2 may necessitate re-writing pipelines and files; to maintain compatibility. Classes. BlockMatrix; Hail's block-distributed matrix of tfloat64 elements. Modules. utils. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/linalg/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/linalg/index.html
https://hail.is/docs/0.2/linalg/index.html:690,Integrability,interface,interface,690,"﻿. Hail | ; linalg. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; linalg. View page source. linalg; File formats and interface for numeric matrices are experimental.; Improvements to Hail 0.2 may necessitate re-writing pipelines and files; to maintain compatibility. Classes. BlockMatrix; Hail's block-distributed matrix of tfloat64 elements. Modules. utils. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/linalg/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/linalg/index.html
https://hail.is/docs/0.2/methods/genetics.html:2300,Availability,error,errors,2300,"ter_alleles(mt, f); Filter alternate alleles. filter_alleles_hts(mt, f[, subset]); Filter alternate alleles and update standard GATK entry fields. hwe_normalized_pca(call_expr[, k, ...]); Run principal component analysis (PCA) on the Hardy-Weinberg-normalized genotype call matrix. genetic_relatedness_matrix(call_expr); Compute the genetic relatedness matrix (GRM). realized_relationship_matrix(call_expr); Computes the realized relationship matrix (RRM). impute_sex(call[, aaf_threshold, ...]); Impute sex of samples by calculating inbreeding coefficient on the X chromosome. ld_matrix(entry_expr, locus_expr, radius[, ...]); Computes the windowed correlation (linkage disequilibrium) matrix between variants. ld_prune(call_expr[, r2, bp_window_size, ...]); Returns a maximal subset of variants that are nearly uncorrelated within each window. compute_charr(ds[, min_af, max_af, min_dp, ...]); Compute CHARR, the DNA sample contamination estimator. mendel_errors(call, pedigree); Find Mendel errors; count per variant, individual and nuclear family. de_novo(mt, pedigree, pop_frequency_prior, *); Call putative de novo events from trio data. nirvana(dataset, config[, block_size, name]); Annotate variants using Nirvana. sample_qc(mt[, name]); Compute per-sample metrics useful for quality control. _logistic_skat(group, weight, y, x, covariates); The logistic sequence kernel association test (SKAT). skat(key_expr, weight_expr, y, x, covariates); Test each keyed group of rows for association by linear or logistic SKAT test. lambda_gc(p_value[, approximate]); Compute genomic inflation factor (lambda GC) from an Expression of p-values. split_multi(ds[, keep_star, left_aligned, ...]); Split multiallelic variants. split_multi_hts(ds[, keep_star, ...]); Split multiallelic variants for datasets that contain one or more fields from a standard high-throughput sequencing entry schema. summarize_variants(mt[, show, handler]); Summarize the variants present in a dataset and print the results. tran",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:6696,Availability,avail,available,6696,"tor_fa:{self.data_mount}/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:{self.data_mount}/loftee_data/phylocsf_gerp.sql,gerp_file:{self.data_mount}/loftee_data/GERP_scores.final.sorted.txt.gz -o STDOUT; '''. The following environment variables are added to the job’s environment:. VEP_BLOCK_SIZE - The maximum number of variants provided as input to each invocation of VEP.; VEP_PART_ID - Partition ID.; VEP_DATA_MOUNT - Location where the vep data is mounted (same as data_mount in the config).; VEP_CONSEQUENCE - Integer equal to 0 or 1 on whether csq is False or True.; VEP_TOLERATE_PARSE_ERROR - Integer equal to 0 or 1 on whether tolerate_parse_error is False or True.; VEP_OUTPUT_FILE - String specifying the local path where the output TSV file with the VEP result should be located.; VEP_INPUT_FILE - String specifying the local path where the input VCF shard is located for all jobs. The VEP_INPUT_FILE environment variable is not available for the single job that computes the consequence header when; csq=True. class hail.methods.VEPConfigGRCh37Version85(*, data_bucket, data_mount, image, regions, cloud, data_bucket_is_requester_pays)[source]; The Hail-maintained VEP configuration for GRCh37 for VEP version 85.; This class takes the following constructor arguments:. data_bucket (str) – The location where the VEP data is stored.; data_mount (str) – The location in the container where the data should be mounted.; image (str) – The docker image to run VEP.; cloud (str) – The cloud where the Batch Service is located.; data_bucket_is_requester_pays (bool) – True if the data bucket is requester pays.; regions (list of str) – A list of regions the VEP jobs can run in. class hail.methods.VEPConfigGRCh38Version95(*, data_bucket, data_mount, image, regions, cloud, data_bucket_is_requester_pays)[source]; The Hail-maintained VEP configuration for GRCh38 for VEP version 95.; This class takes the following constructor arguments:. data_bucke",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:25546,Availability,down,downcode,25546,"GQ: int32; PL: array<int32>. Use MatrixTable.select_entries() to rearrange these fields if; necessary.; The following new fields are generated:. old_locus (locus) – The old locus, before filtering and computing; the minimal representation.; old_alleles (array<str>) – The old alleles, before filtering and; computing the minimal representation.; old_to_new (array<int32>) – An array that maps old allele index to; new allele index. Its length is the same as old_alleles. Alleles that; are filtered are missing.; new_to_old (array<int32>) – An array that maps new allele index to; the old allele index. Its length is the same as the modified alleles; field. Downcode algorithm; We will illustrate the behavior on the example genotype below; when filtering the first alternate allele (allele 1) at a site; with 1 reference allele and 2 alternate alleles.; GT: 1/2; GQ: 10; AD: 0,50,35. 0 | 1000; 1 | 1000 10; 2 | 1000 0 20; +-----------------; 0 1 2. The downcode algorithm recodes occurances of filtered alleles; to occurances of the reference allele (e.g. 1 -> 0 in our; example). So the depths of filtered alleles in the AD field; are added to the depth of the reference allele. Where; downcoding filtered alleles merges distinct genotypes, the; minimum PL is used (since PL is on a log scale, this roughly; corresponds to adding probabilities). The PLs are then; re-normalized (shifted) so that the most likely genotype has a; PL of 0, and GT is set to this genotype. If an allele is; filtered, this algorithm acts similarly to; split_multi_hts().; The downcode algorithm would produce the following:; GT: 0/1; GQ: 10; AD: 35,50. 0 | 20; 1 | 0 10; +-----------; 0 1. In summary:. GT: Downcode filtered alleles to reference.; AD: Columns of filtered alleles are eliminated and their; values are added to the reference column, e.g., filtering; alleles 1 and 2 transforms 25,5,10,20 to 40,20.; DP: No change.; PL: Downcode filtered alleles to reference, combine PLs; using minimum for each overloaded g",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:25780,Availability,down,downcoding,25780,"r>) – The old alleles, before filtering and; computing the minimal representation.; old_to_new (array<int32>) – An array that maps old allele index to; new allele index. Its length is the same as old_alleles. Alleles that; are filtered are missing.; new_to_old (array<int32>) – An array that maps new allele index to; the old allele index. Its length is the same as the modified alleles; field. Downcode algorithm; We will illustrate the behavior on the example genotype below; when filtering the first alternate allele (allele 1) at a site; with 1 reference allele and 2 alternate alleles.; GT: 1/2; GQ: 10; AD: 0,50,35. 0 | 1000; 1 | 1000 10; 2 | 1000 0 20; +-----------------; 0 1 2. The downcode algorithm recodes occurances of filtered alleles; to occurances of the reference allele (e.g. 1 -> 0 in our; example). So the depths of filtered alleles in the AD field; are added to the depth of the reference allele. Where; downcoding filtered alleles merges distinct genotypes, the; minimum PL is used (since PL is on a log scale, this roughly; corresponds to adding probabilities). The PLs are then; re-normalized (shifted) so that the most likely genotype has a; PL of 0, and GT is set to this genotype. If an allele is; filtered, this algorithm acts similarly to; split_multi_hts().; The downcode algorithm would produce the following:; GT: 0/1; GQ: 10; AD: 35,50. 0 | 20; 1 | 0 10; +-----------; 0 1. In summary:. GT: Downcode filtered alleles to reference.; AD: Columns of filtered alleles are eliminated and their; values are added to the reference column, e.g., filtering; alleles 1 and 2 transforms 25,5,10,20 to 40,20.; DP: No change.; PL: Downcode filtered alleles to reference, combine PLs; using minimum for each overloaded genotype, and shift so; the overall minimum PL is 0.; GQ: The second-lowest PL (after shifting). Subset algorithm; We will illustrate the behavior on the example genotype below; when filtering the first alternate allele (allele 1) at a site; with 1 reference alle",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:26148,Availability,down,downcode,26148,"lele index. Its length is the same as the modified alleles; field. Downcode algorithm; We will illustrate the behavior on the example genotype below; when filtering the first alternate allele (allele 1) at a site; with 1 reference allele and 2 alternate alleles.; GT: 1/2; GQ: 10; AD: 0,50,35. 0 | 1000; 1 | 1000 10; 2 | 1000 0 20; +-----------------; 0 1 2. The downcode algorithm recodes occurances of filtered alleles; to occurances of the reference allele (e.g. 1 -> 0 in our; example). So the depths of filtered alleles in the AD field; are added to the depth of the reference allele. Where; downcoding filtered alleles merges distinct genotypes, the; minimum PL is used (since PL is on a log scale, this roughly; corresponds to adding probabilities). The PLs are then; re-normalized (shifted) so that the most likely genotype has a; PL of 0, and GT is set to this genotype. If an allele is; filtered, this algorithm acts similarly to; split_multi_hts().; The downcode algorithm would produce the following:; GT: 0/1; GQ: 10; AD: 35,50. 0 | 20; 1 | 0 10; +-----------; 0 1. In summary:. GT: Downcode filtered alleles to reference.; AD: Columns of filtered alleles are eliminated and their; values are added to the reference column, e.g., filtering; alleles 1 and 2 transforms 25,5,10,20 to 40,20.; DP: No change.; PL: Downcode filtered alleles to reference, combine PLs; using minimum for each overloaded genotype, and shift so; the overall minimum PL is 0.; GQ: The second-lowest PL (after shifting). Subset algorithm; We will illustrate the behavior on the example genotype below; when filtering the first alternate allele (allele 1) at a site; with 1 reference allele and 2 alternate alleles.; GT: 1/2; GQ: 10; AD: 0,50,35. 0 | 1000; 1 | 1000 10; 2 | 1000 0 20; +-----------------; 0 1 2. The subset algorithm subsets the AD and PL arrays; (i.e. removes entries corresponding to filtered alleles) and; then sets GT to the genotype with the minimum PL. Note that; if the genotype changes (as i",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:28423,Availability,down,downcode,28423," the filtered alleles are not real so we should discard; any probability mass associated with them.; The subset algorithm would produce the following:; GT: 1/1; GQ: 980; AD: 0,50. 0 | 980; 1 | 980 0; +-----------; 0 1. In summary:. GT: Set to most likely genotype based on the PLs ignoring; the filtered allele(s).; AD: The filtered alleles’ columns are eliminated, e.g.,; filtering alleles 1 and 2 transforms 25,5,10,20 to; 25,20.; DP: Unchanged.; PL: Columns involving filtered alleles are eliminated and; the remaining columns’ values are shifted so the minimum; value is 0.; GQ: The second-lowest PL (after shifting). Warning; filter_alleles_hts() does not update any row fields other than; locus and alleles. This means that row fields like allele count (AC) can; become meaningless unless they are also updated. You can update them with; annotate_rows(). See also; filter_alleles(). Parameters:. mt (MatrixTable); f (callable) – Function from (allele: StringExpression, allele_index:; Int32Expression) to BooleanExpression; subset (bool) – Subset PL field if True, otherwise downcode PL field. The; calculation of GT and GQ also depend on whether one subsets or; downcodes the PL. Returns:; MatrixTable. hail.methods.hwe_normalized_pca(call_expr, k=10, compute_loadings=False)[source]; Run principal component analysis (PCA) on the Hardy-Weinberg-normalized; genotype call matrix.; Examples; >>> eigenvalues, scores, loadings = hl.hwe_normalized_pca(dataset.GT, k=5). Notes; This method specializes pca() for the common use case; of PCA in statistical genetics, that of projecting samples to a small; number of ancestry coordinates. Variants that are all homozygous reference; or all homozygous alternate are unnormalizable and removed before; evaluation. See pca() for more details.; Users of PLINK/GCTA should be aware that Hail computes the GRM slightly; differently with regard to missing data. In Hail, the; \(ij\) entry of the GRM \(MM^T\) is simply the dot product of rows; \(i\) and \(",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:28511,Availability,down,downcodes,28511,"llowing:; GT: 1/1; GQ: 980; AD: 0,50. 0 | 980; 1 | 980 0; +-----------; 0 1. In summary:. GT: Set to most likely genotype based on the PLs ignoring; the filtered allele(s).; AD: The filtered alleles’ columns are eliminated, e.g.,; filtering alleles 1 and 2 transforms 25,5,10,20 to; 25,20.; DP: Unchanged.; PL: Columns involving filtered alleles are eliminated and; the remaining columns’ values are shifted so the minimum; value is 0.; GQ: The second-lowest PL (after shifting). Warning; filter_alleles_hts() does not update any row fields other than; locus and alleles. This means that row fields like allele count (AC) can; become meaningless unless they are also updated. You can update them with; annotate_rows(). See also; filter_alleles(). Parameters:. mt (MatrixTable); f (callable) – Function from (allele: StringExpression, allele_index:; Int32Expression) to BooleanExpression; subset (bool) – Subset PL field if True, otherwise downcode PL field. The; calculation of GT and GQ also depend on whether one subsets or; downcodes the PL. Returns:; MatrixTable. hail.methods.hwe_normalized_pca(call_expr, k=10, compute_loadings=False)[source]; Run principal component analysis (PCA) on the Hardy-Weinberg-normalized; genotype call matrix.; Examples; >>> eigenvalues, scores, loadings = hl.hwe_normalized_pca(dataset.GT, k=5). Notes; This method specializes pca() for the common use case; of PCA in statistical genetics, that of projecting samples to a small; number of ancestry coordinates. Variants that are all homozygous reference; or all homozygous alternate are unnormalizable and removed before; evaluation. See pca() for more details.; Users of PLINK/GCTA should be aware that Hail computes the GRM slightly; differently with regard to missing data. In Hail, the; \(ij\) entry of the GRM \(MM^T\) is simply the dot product of rows; \(i\) and \(j\) of \(M\); in terms of \(C\) it is. \[\frac{1}{m}\sum_{l\in\mathcal{C}_i\cap\mathcal{C}_j}\frac{(C_{il}-2p_l)(C_{jl} - 2p_l)}{2p_l(1-p_l)}\]",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:41620,Availability,error,error,41620,"g values are mean-imputed within variant.; The method produces a symmetric block-sparse matrix supported in a; neighborhood of the diagonal. If variants \(i\) and \(j\) are on the; same contig and within radius base pairs (inclusive) then the; \((i, j)\) element is their; Pearson correlation coefficient.; Otherwise, the \((i, j)\) element is 0.0.; Rows with a constant value (i.e., zero variance) will result in nan; correlation values. To avoid this, first check that all variants vary or; filter out constant variants (for example, with the help of; aggregators.stats()).; If the global_position() on locus_expr is not in ascending order,; this method will fail. Ascending order should hold for a matrix table keyed; by locus or variant (and the associated row table), or for a table that’s; been ordered by locus_expr.; Set coord_expr to use a value other than position to define the windows.; This row-indexed numeric expression must be non-missing, non-nan, on the; same source as locus_expr, and ascending with respect to locus; position for each contig; otherwise the method will raise an error. Warning; See the warnings in row_correlation(). In particular, for large; matrices it may be preferable to run its stages separately.; entry_expr and locus_expr are implicitly aligned by row-index, though; they need not be on the same source. If their sources differ in the number; of rows, an error will be raised; otherwise, unintended misalignment may; silently produce unexpected results. Parameters:. entry_expr (Float64Expression) – Entry-indexed numeric expression on matrix table.; locus_expr (LocusExpression) – Row-indexed locus expression on a table or matrix table that is; row-aligned with the matrix table of entry_expr.; radius (int or float) – Radius of window for row values.; coord_expr (Float64Expression, optional) – Row-indexed numeric expression for the row value on the same table or; matrix table as locus_expr.; By default, the row value is given by the locus position.; ",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:41921,Availability,error,error,41921,"relation values. To avoid this, first check that all variants vary or; filter out constant variants (for example, with the help of; aggregators.stats()).; If the global_position() on locus_expr is not in ascending order,; this method will fail. Ascending order should hold for a matrix table keyed; by locus or variant (and the associated row table), or for a table that’s; been ordered by locus_expr.; Set coord_expr to use a value other than position to define the windows.; This row-indexed numeric expression must be non-missing, non-nan, on the; same source as locus_expr, and ascending with respect to locus; position for each contig; otherwise the method will raise an error. Warning; See the warnings in row_correlation(). In particular, for large; matrices it may be preferable to run its stages separately.; entry_expr and locus_expr are implicitly aligned by row-index, though; they need not be on the same source. If their sources differ in the number; of rows, an error will be raised; otherwise, unintended misalignment may; silently produce unexpected results. Parameters:. entry_expr (Float64Expression) – Entry-indexed numeric expression on matrix table.; locus_expr (LocusExpression) – Row-indexed locus expression on a table or matrix table that is; row-aligned with the matrix table of entry_expr.; radius (int or float) – Radius of window for row values.; coord_expr (Float64Expression, optional) – Row-indexed numeric expression for the row value on the same table or; matrix table as locus_expr.; By default, the row value is given by the locus position.; block_size (int, optional) – Block size. Default given by BlockMatrix.default_block_size(). Returns:; BlockMatrix – Windowed correlation matrix between variants.; Row and column indices correspond to matrix table variant index. hail.methods.ld_prune(call_expr, r2=0.2, bp_window_size=1000000, memory_per_core=256, keep_higher_maf=True, block_size=None)[source]; Returns a maximal subset of variants that are nearly uncor",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:45580,Availability,error,errors,45580,"l pruning in; this stage. Minor allele frequency is not taken into account. The; parallelism is the number of matrix table partitions.; The second, “global correlation” stage uses block-sparse matrix; multiplication to compute correlation between each pair of remaining; variants within bp_window_size base pairs, and then forms a graph of; correlated variants. The parallelism of writing the locally-pruned matrix; table as a block matrix is n_locally_pruned_variants / block_size.; The third, “global pruning” stage applies maximal_independent_set(); to prune variants from this graph until no edges remain. This algorithm; iteratively removes the variant with the highest vertex degree. If; keep_higher_maf is true, then in the case of a tie for highest degree,; the variant with lowest minor allele frequency is removed. Warning; The locally-pruned matrix table and block matrix are stored as temporary files; on persistent disk. See the warnings on BlockMatrix.from_entry_expr with; regard to memory and Hadoop replication errors. Parameters:. call_expr (CallExpression) – Entry-indexed call expression on a matrix table with row-indexed; variants and column-indexed samples.; r2 (float) – Squared correlation threshold (exclusive upper bound).; Must be in the range [0.0, 1.0].; bp_window_size (int) – Window size in base pairs (inclusive upper bound).; memory_per_core (int) – Memory in MB per core for local pruning queue.; keep_higher_maf (int) – If True, break ties at each step of the global pruning stage by; preferring to keep variants with higher minor allele frequency.; block_size (int, optional) – Block size for block matrices in the second stage.; Default given by BlockMatrix.default_block_size(). Returns:; Table – Table of a maximal independent set of variants. hail.methods.compute_charr(ds, min_af=0.05, max_af=0.95, min_dp=10, max_dp=100, min_gq=20, ref_AF=None)[source]; Compute CHARR, the DNA sample contamination estimator. Danger; This functionality is experimental. It m",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:47753,Availability,error,errors,47753,"ion. Note; It is possible to use gnomAD reference allele frequencies with the following:; >>> gnomad_sites = hl.experimental.load_dataset('gnomad_genome_sites', version='3.1.2') ; >>> charr_result = hl.compute_charr(mt, ref_af=(1 - gnomad_sites[mt.row_key].freq[0].AF)) . If the dataset is loaded from a gvcf and has NON_REF alleles, drop the last allele with the following or load it with the hail vcf combiner:; >>> mt = mt.key_rows_by(locus=mt.locus, alleles=mt.alleles[:-1]). Parameters:. ds (MatrixTable or VariantDataset) – Dataset.; min_af – Minimum reference allele frequency to filter variants.; max_af – Maximum reference allele frequency to filter variants.; min_dp – Minimum sequencing depth to filter variants.; max_dp – Maximum sequencing depth to filter variants.; min_gq – Minimum genotype quality to filter variants; ref_AF – Reference AF expression. Necessary when the sample size is below 10,000. Returns:; Table. hail.methods.mendel_errors(call, pedigree)[source]; Find Mendel errors; count per variant, individual and nuclear family. Note; Requires the column key to be one field of type tstr. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). Note; Requires the dataset to contain no multiallelic variants.; Use split_multi() or split_multi_hts() to split; multiallelic sites, or MatrixTable.filter_rows() to remove; them. Examples; Find all violations of Mendelian inheritance in each (dad, mom, kid) trio in; a pedigree and return four tables (all errors, errors by family, errors by; individual, errors by variant):; >>> ped = hl.Pedigree.read('data/trios.fam'); >>> all_errors, per_fam, per_sample, per_variant = hl.mendel_errors(dataset['GT'], ped). Export all mendel errors to a text file:; >>> all_errors.export('output/all_mendel_errors.tsv'). Annotate columns with the number of Mendel errors:; >>> annotated_samples = dataset.annotate_cols(mendel=per_sample[dataset.s]). Annotate rows with the number of Mendel",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:48286,Availability,error,errors,48286," or VariantDataset) – Dataset.; min_af – Minimum reference allele frequency to filter variants.; max_af – Maximum reference allele frequency to filter variants.; min_dp – Minimum sequencing depth to filter variants.; max_dp – Maximum sequencing depth to filter variants.; min_gq – Minimum genotype quality to filter variants; ref_AF – Reference AF expression. Necessary when the sample size is below 10,000. Returns:; Table. hail.methods.mendel_errors(call, pedigree)[source]; Find Mendel errors; count per variant, individual and nuclear family. Note; Requires the column key to be one field of type tstr. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). Note; Requires the dataset to contain no multiallelic variants.; Use split_multi() or split_multi_hts() to split; multiallelic sites, or MatrixTable.filter_rows() to remove; them. Examples; Find all violations of Mendelian inheritance in each (dad, mom, kid) trio in; a pedigree and return four tables (all errors, errors by family, errors by; individual, errors by variant):; >>> ped = hl.Pedigree.read('data/trios.fam'); >>> all_errors, per_fam, per_sample, per_variant = hl.mendel_errors(dataset['GT'], ped). Export all mendel errors to a text file:; >>> all_errors.export('output/all_mendel_errors.tsv'). Annotate columns with the number of Mendel errors:; >>> annotated_samples = dataset.annotate_cols(mendel=per_sample[dataset.s]). Annotate rows with the number of Mendel errors:; >>> annotated_variants = dataset.annotate_rows(mendel=per_variant[dataset.locus, dataset.alleles]). Notes; The example above returns four tables, which contain Mendelian violations; grouped in various ways. These tables are modeled after the PLINK mendel; formats, resembling; the .mendel, .fmendel, .imendel, and .lmendel formats,; respectively.; First table: all Mendel errors. This table contains one row per Mendel; error, keyed by the variant and proband id. locus (tlocus) – Variant locus, ke",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:48294,Availability,error,errors,48294," or VariantDataset) – Dataset.; min_af – Minimum reference allele frequency to filter variants.; max_af – Maximum reference allele frequency to filter variants.; min_dp – Minimum sequencing depth to filter variants.; max_dp – Maximum sequencing depth to filter variants.; min_gq – Minimum genotype quality to filter variants; ref_AF – Reference AF expression. Necessary when the sample size is below 10,000. Returns:; Table. hail.methods.mendel_errors(call, pedigree)[source]; Find Mendel errors; count per variant, individual and nuclear family. Note; Requires the column key to be one field of type tstr. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). Note; Requires the dataset to contain no multiallelic variants.; Use split_multi() or split_multi_hts() to split; multiallelic sites, or MatrixTable.filter_rows() to remove; them. Examples; Find all violations of Mendelian inheritance in each (dad, mom, kid) trio in; a pedigree and return four tables (all errors, errors by family, errors by; individual, errors by variant):; >>> ped = hl.Pedigree.read('data/trios.fam'); >>> all_errors, per_fam, per_sample, per_variant = hl.mendel_errors(dataset['GT'], ped). Export all mendel errors to a text file:; >>> all_errors.export('output/all_mendel_errors.tsv'). Annotate columns with the number of Mendel errors:; >>> annotated_samples = dataset.annotate_cols(mendel=per_sample[dataset.s]). Annotate rows with the number of Mendel errors:; >>> annotated_variants = dataset.annotate_rows(mendel=per_variant[dataset.locus, dataset.alleles]). Notes; The example above returns four tables, which contain Mendelian violations; grouped in various ways. These tables are modeled after the PLINK mendel; formats, resembling; the .mendel, .fmendel, .imendel, and .lmendel formats,; respectively.; First table: all Mendel errors. This table contains one row per Mendel; error, keyed by the variant and proband id. locus (tlocus) – Variant locus, ke",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:48312,Availability,error,errors,48312," or VariantDataset) – Dataset.; min_af – Minimum reference allele frequency to filter variants.; max_af – Maximum reference allele frequency to filter variants.; min_dp – Minimum sequencing depth to filter variants.; max_dp – Maximum sequencing depth to filter variants.; min_gq – Minimum genotype quality to filter variants; ref_AF – Reference AF expression. Necessary when the sample size is below 10,000. Returns:; Table. hail.methods.mendel_errors(call, pedigree)[source]; Find Mendel errors; count per variant, individual and nuclear family. Note; Requires the column key to be one field of type tstr. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). Note; Requires the dataset to contain no multiallelic variants.; Use split_multi() or split_multi_hts() to split; multiallelic sites, or MatrixTable.filter_rows() to remove; them. Examples; Find all violations of Mendelian inheritance in each (dad, mom, kid) trio in; a pedigree and return four tables (all errors, errors by family, errors by; individual, errors by variant):; >>> ped = hl.Pedigree.read('data/trios.fam'); >>> all_errors, per_fam, per_sample, per_variant = hl.mendel_errors(dataset['GT'], ped). Export all mendel errors to a text file:; >>> all_errors.export('output/all_mendel_errors.tsv'). Annotate columns with the number of Mendel errors:; >>> annotated_samples = dataset.annotate_cols(mendel=per_sample[dataset.s]). Annotate rows with the number of Mendel errors:; >>> annotated_variants = dataset.annotate_rows(mendel=per_variant[dataset.locus, dataset.alleles]). Notes; The example above returns four tables, which contain Mendelian violations; grouped in various ways. These tables are modeled after the PLINK mendel; formats, resembling; the .mendel, .fmendel, .imendel, and .lmendel formats,; respectively.; First table: all Mendel errors. This table contains one row per Mendel; error, keyed by the variant and proband id. locus (tlocus) – Variant locus, ke",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:48335,Availability,error,errors,48335," or VariantDataset) – Dataset.; min_af – Minimum reference allele frequency to filter variants.; max_af – Maximum reference allele frequency to filter variants.; min_dp – Minimum sequencing depth to filter variants.; max_dp – Maximum sequencing depth to filter variants.; min_gq – Minimum genotype quality to filter variants; ref_AF – Reference AF expression. Necessary when the sample size is below 10,000. Returns:; Table. hail.methods.mendel_errors(call, pedigree)[source]; Find Mendel errors; count per variant, individual and nuclear family. Note; Requires the column key to be one field of type tstr. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). Note; Requires the dataset to contain no multiallelic variants.; Use split_multi() or split_multi_hts() to split; multiallelic sites, or MatrixTable.filter_rows() to remove; them. Examples; Find all violations of Mendelian inheritance in each (dad, mom, kid) trio in; a pedigree and return four tables (all errors, errors by family, errors by; individual, errors by variant):; >>> ped = hl.Pedigree.read('data/trios.fam'); >>> all_errors, per_fam, per_sample, per_variant = hl.mendel_errors(dataset['GT'], ped). Export all mendel errors to a text file:; >>> all_errors.export('output/all_mendel_errors.tsv'). Annotate columns with the number of Mendel errors:; >>> annotated_samples = dataset.annotate_cols(mendel=per_sample[dataset.s]). Annotate rows with the number of Mendel errors:; >>> annotated_variants = dataset.annotate_rows(mendel=per_variant[dataset.locus, dataset.alleles]). Notes; The example above returns four tables, which contain Mendelian violations; grouped in various ways. These tables are modeled after the PLINK mendel; formats, resembling; the .mendel, .fmendel, .imendel, and .lmendel formats,; respectively.; First table: all Mendel errors. This table contains one row per Mendel; error, keyed by the variant and proband id. locus (tlocus) – Variant locus, ke",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:48509,Availability,error,errors,48509,"lter variants.; min_gq – Minimum genotype quality to filter variants; ref_AF – Reference AF expression. Necessary when the sample size is below 10,000. Returns:; Table. hail.methods.mendel_errors(call, pedigree)[source]; Find Mendel errors; count per variant, individual and nuclear family. Note; Requires the column key to be one field of type tstr. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). Note; Requires the dataset to contain no multiallelic variants.; Use split_multi() or split_multi_hts() to split; multiallelic sites, or MatrixTable.filter_rows() to remove; them. Examples; Find all violations of Mendelian inheritance in each (dad, mom, kid) trio in; a pedigree and return four tables (all errors, errors by family, errors by; individual, errors by variant):; >>> ped = hl.Pedigree.read('data/trios.fam'); >>> all_errors, per_fam, per_sample, per_variant = hl.mendel_errors(dataset['GT'], ped). Export all mendel errors to a text file:; >>> all_errors.export('output/all_mendel_errors.tsv'). Annotate columns with the number of Mendel errors:; >>> annotated_samples = dataset.annotate_cols(mendel=per_sample[dataset.s]). Annotate rows with the number of Mendel errors:; >>> annotated_variants = dataset.annotate_rows(mendel=per_variant[dataset.locus, dataset.alleles]). Notes; The example above returns four tables, which contain Mendelian violations; grouped in various ways. These tables are modeled after the PLINK mendel; formats, resembling; the .mendel, .fmendel, .imendel, and .lmendel formats,; respectively.; First table: all Mendel errors. This table contains one row per Mendel; error, keyed by the variant and proband id. locus (tlocus) – Variant locus, key field.; alleles (tarray of tstr) – Variant alleles, key field.; (column key of dataset) (tstr) – Proband ID, key field.; fam_id (tstr) – Family ID.; mendel_code (tint32) – Mendel error code, see below. Second table: errors per nuclear family. This table",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:48631,Availability,error,errors,48631,"ary when the sample size is below 10,000. Returns:; Table. hail.methods.mendel_errors(call, pedigree)[source]; Find Mendel errors; count per variant, individual and nuclear family. Note; Requires the column key to be one field of type tstr. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). Note; Requires the dataset to contain no multiallelic variants.; Use split_multi() or split_multi_hts() to split; multiallelic sites, or MatrixTable.filter_rows() to remove; them. Examples; Find all violations of Mendelian inheritance in each (dad, mom, kid) trio in; a pedigree and return four tables (all errors, errors by family, errors by; individual, errors by variant):; >>> ped = hl.Pedigree.read('data/trios.fam'); >>> all_errors, per_fam, per_sample, per_variant = hl.mendel_errors(dataset['GT'], ped). Export all mendel errors to a text file:; >>> all_errors.export('output/all_mendel_errors.tsv'). Annotate columns with the number of Mendel errors:; >>> annotated_samples = dataset.annotate_cols(mendel=per_sample[dataset.s]). Annotate rows with the number of Mendel errors:; >>> annotated_variants = dataset.annotate_rows(mendel=per_variant[dataset.locus, dataset.alleles]). Notes; The example above returns four tables, which contain Mendelian violations; grouped in various ways. These tables are modeled after the PLINK mendel; formats, resembling; the .mendel, .fmendel, .imendel, and .lmendel formats,; respectively.; First table: all Mendel errors. This table contains one row per Mendel; error, keyed by the variant and proband id. locus (tlocus) – Variant locus, key field.; alleles (tarray of tstr) – Variant alleles, key field.; (column key of dataset) (tstr) – Proband ID, key field.; fam_id (tstr) – Family ID.; mendel_code (tint32) – Mendel error code, see below. Second table: errors per nuclear family. This table contains one row; per nuclear family, keyed by the parents. pat_id (tstr) – Paternal ID. (key field); mat_id (",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:48757,Availability,error,errors,48757,"rs; count per variant, individual and nuclear family. Note; Requires the column key to be one field of type tstr. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). Note; Requires the dataset to contain no multiallelic variants.; Use split_multi() or split_multi_hts() to split; multiallelic sites, or MatrixTable.filter_rows() to remove; them. Examples; Find all violations of Mendelian inheritance in each (dad, mom, kid) trio in; a pedigree and return four tables (all errors, errors by family, errors by; individual, errors by variant):; >>> ped = hl.Pedigree.read('data/trios.fam'); >>> all_errors, per_fam, per_sample, per_variant = hl.mendel_errors(dataset['GT'], ped). Export all mendel errors to a text file:; >>> all_errors.export('output/all_mendel_errors.tsv'). Annotate columns with the number of Mendel errors:; >>> annotated_samples = dataset.annotate_cols(mendel=per_sample[dataset.s]). Annotate rows with the number of Mendel errors:; >>> annotated_variants = dataset.annotate_rows(mendel=per_variant[dataset.locus, dataset.alleles]). Notes; The example above returns four tables, which contain Mendelian violations; grouped in various ways. These tables are modeled after the PLINK mendel; formats, resembling; the .mendel, .fmendel, .imendel, and .lmendel formats,; respectively.; First table: all Mendel errors. This table contains one row per Mendel; error, keyed by the variant and proband id. locus (tlocus) – Variant locus, key field.; alleles (tarray of tstr) – Variant alleles, key field.; (column key of dataset) (tstr) – Proband ID, key field.; fam_id (tstr) – Family ID.; mendel_code (tint32) – Mendel error code, see below. Second table: errors per nuclear family. This table contains one row; per nuclear family, keyed by the parents. pat_id (tstr) – Paternal ID. (key field); mat_id (tstr) – Maternal ID. (key field); fam_id (tstr) – Family ID.; children (tint32) – Number of children in this nuclear family.; er",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:49138,Availability,error,errors,49138,"filter_rows() to remove; them. Examples; Find all violations of Mendelian inheritance in each (dad, mom, kid) trio in; a pedigree and return four tables (all errors, errors by family, errors by; individual, errors by variant):; >>> ped = hl.Pedigree.read('data/trios.fam'); >>> all_errors, per_fam, per_sample, per_variant = hl.mendel_errors(dataset['GT'], ped). Export all mendel errors to a text file:; >>> all_errors.export('output/all_mendel_errors.tsv'). Annotate columns with the number of Mendel errors:; >>> annotated_samples = dataset.annotate_cols(mendel=per_sample[dataset.s]). Annotate rows with the number of Mendel errors:; >>> annotated_variants = dataset.annotate_rows(mendel=per_variant[dataset.locus, dataset.alleles]). Notes; The example above returns four tables, which contain Mendelian violations; grouped in various ways. These tables are modeled after the PLINK mendel; formats, resembling; the .mendel, .fmendel, .imendel, and .lmendel formats,; respectively.; First table: all Mendel errors. This table contains one row per Mendel; error, keyed by the variant and proband id. locus (tlocus) – Variant locus, key field.; alleles (tarray of tstr) – Variant alleles, key field.; (column key of dataset) (tstr) – Proband ID, key field.; fam_id (tstr) – Family ID.; mendel_code (tint32) – Mendel error code, see below. Second table: errors per nuclear family. This table contains one row; per nuclear family, keyed by the parents. pat_id (tstr) – Paternal ID. (key field); mat_id (tstr) – Maternal ID. (key field); fam_id (tstr) – Family ID.; children (tint32) – Number of children in this nuclear family.; errors (tint64) – Number of Mendel errors in this nuclear family.; snp_errors (tint64) – Number of Mendel errors at SNPs in this; nuclear family. Third table: errors per individual. This table contains one row per; individual. Each error is counted toward the proband, father, and mother; according to the Implicated in the table below. (column key of dataset) (tstr) – Sam",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:49186,Availability,error,error,49186,"s of Mendelian inheritance in each (dad, mom, kid) trio in; a pedigree and return four tables (all errors, errors by family, errors by; individual, errors by variant):; >>> ped = hl.Pedigree.read('data/trios.fam'); >>> all_errors, per_fam, per_sample, per_variant = hl.mendel_errors(dataset['GT'], ped). Export all mendel errors to a text file:; >>> all_errors.export('output/all_mendel_errors.tsv'). Annotate columns with the number of Mendel errors:; >>> annotated_samples = dataset.annotate_cols(mendel=per_sample[dataset.s]). Annotate rows with the number of Mendel errors:; >>> annotated_variants = dataset.annotate_rows(mendel=per_variant[dataset.locus, dataset.alleles]). Notes; The example above returns four tables, which contain Mendelian violations; grouped in various ways. These tables are modeled after the PLINK mendel; formats, resembling; the .mendel, .fmendel, .imendel, and .lmendel formats,; respectively.; First table: all Mendel errors. This table contains one row per Mendel; error, keyed by the variant and proband id. locus (tlocus) – Variant locus, key field.; alleles (tarray of tstr) – Variant alleles, key field.; (column key of dataset) (tstr) – Proband ID, key field.; fam_id (tstr) – Family ID.; mendel_code (tint32) – Mendel error code, see below. Second table: errors per nuclear family. This table contains one row; per nuclear family, keyed by the parents. pat_id (tstr) – Paternal ID. (key field); mat_id (tstr) – Maternal ID. (key field); fam_id (tstr) – Family ID.; children (tint32) – Number of children in this nuclear family.; errors (tint64) – Number of Mendel errors in this nuclear family.; snp_errors (tint64) – Number of Mendel errors at SNPs in this; nuclear family. Third table: errors per individual. This table contains one row per; individual. Each error is counted toward the proband, father, and mother; according to the Implicated in the table below. (column key of dataset) (tstr) – Sample ID (key field).; fam_id (tstr) – Family ID.; errors (t",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:49445,Availability,error,error,49445,"er_variant = hl.mendel_errors(dataset['GT'], ped). Export all mendel errors to a text file:; >>> all_errors.export('output/all_mendel_errors.tsv'). Annotate columns with the number of Mendel errors:; >>> annotated_samples = dataset.annotate_cols(mendel=per_sample[dataset.s]). Annotate rows with the number of Mendel errors:; >>> annotated_variants = dataset.annotate_rows(mendel=per_variant[dataset.locus, dataset.alleles]). Notes; The example above returns four tables, which contain Mendelian violations; grouped in various ways. These tables are modeled after the PLINK mendel; formats, resembling; the .mendel, .fmendel, .imendel, and .lmendel formats,; respectively.; First table: all Mendel errors. This table contains one row per Mendel; error, keyed by the variant and proband id. locus (tlocus) – Variant locus, key field.; alleles (tarray of tstr) – Variant alleles, key field.; (column key of dataset) (tstr) – Proband ID, key field.; fam_id (tstr) – Family ID.; mendel_code (tint32) – Mendel error code, see below. Second table: errors per nuclear family. This table contains one row; per nuclear family, keyed by the parents. pat_id (tstr) – Paternal ID. (key field); mat_id (tstr) – Maternal ID. (key field); fam_id (tstr) – Family ID.; children (tint32) – Number of children in this nuclear family.; errors (tint64) – Number of Mendel errors in this nuclear family.; snp_errors (tint64) – Number of Mendel errors at SNPs in this; nuclear family. Third table: errors per individual. This table contains one row per; individual. Each error is counted toward the proband, father, and mother; according to the Implicated in the table below. (column key of dataset) (tstr) – Sample ID (key field).; fam_id (tstr) – Family ID.; errors (tint64) – Number of Mendel errors involving this; individual.; snp_errors (tint64) – Number of Mendel errors involving this; individual at SNPs. Fourth table: errors per variant. locus (tlocus) – Variant locus, key field.; alleles (tarray of tstr) – Vari",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:49482,Availability,error,errors,49482,"d). Export all mendel errors to a text file:; >>> all_errors.export('output/all_mendel_errors.tsv'). Annotate columns with the number of Mendel errors:; >>> annotated_samples = dataset.annotate_cols(mendel=per_sample[dataset.s]). Annotate rows with the number of Mendel errors:; >>> annotated_variants = dataset.annotate_rows(mendel=per_variant[dataset.locus, dataset.alleles]). Notes; The example above returns four tables, which contain Mendelian violations; grouped in various ways. These tables are modeled after the PLINK mendel; formats, resembling; the .mendel, .fmendel, .imendel, and .lmendel formats,; respectively.; First table: all Mendel errors. This table contains one row per Mendel; error, keyed by the variant and proband id. locus (tlocus) – Variant locus, key field.; alleles (tarray of tstr) – Variant alleles, key field.; (column key of dataset) (tstr) – Proband ID, key field.; fam_id (tstr) – Family ID.; mendel_code (tint32) – Mendel error code, see below. Second table: errors per nuclear family. This table contains one row; per nuclear family, keyed by the parents. pat_id (tstr) – Paternal ID. (key field); mat_id (tstr) – Maternal ID. (key field); fam_id (tstr) – Family ID.; children (tint32) – Number of children in this nuclear family.; errors (tint64) – Number of Mendel errors in this nuclear family.; snp_errors (tint64) – Number of Mendel errors at SNPs in this; nuclear family. Third table: errors per individual. This table contains one row per; individual. Each error is counted toward the proband, father, and mother; according to the Implicated in the table below. (column key of dataset) (tstr) – Sample ID (key field).; fam_id (tstr) – Family ID.; errors (tint64) – Number of Mendel errors involving this; individual.; snp_errors (tint64) – Number of Mendel errors involving this; individual at SNPs. Fourth table: errors per variant. locus (tlocus) – Variant locus, key field.; alleles (tarray of tstr) – Variant alleles, key field.; errors (tint64) – Numbe",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:49756,Availability,error,errors,49756,"s = dataset.annotate_rows(mendel=per_variant[dataset.locus, dataset.alleles]). Notes; The example above returns four tables, which contain Mendelian violations; grouped in various ways. These tables are modeled after the PLINK mendel; formats, resembling; the .mendel, .fmendel, .imendel, and .lmendel formats,; respectively.; First table: all Mendel errors. This table contains one row per Mendel; error, keyed by the variant and proband id. locus (tlocus) – Variant locus, key field.; alleles (tarray of tstr) – Variant alleles, key field.; (column key of dataset) (tstr) – Proband ID, key field.; fam_id (tstr) – Family ID.; mendel_code (tint32) – Mendel error code, see below. Second table: errors per nuclear family. This table contains one row; per nuclear family, keyed by the parents. pat_id (tstr) – Paternal ID. (key field); mat_id (tstr) – Maternal ID. (key field); fam_id (tstr) – Family ID.; children (tint32) – Number of children in this nuclear family.; errors (tint64) – Number of Mendel errors in this nuclear family.; snp_errors (tint64) – Number of Mendel errors at SNPs in this; nuclear family. Third table: errors per individual. This table contains one row per; individual. Each error is counted toward the proband, father, and mother; according to the Implicated in the table below. (column key of dataset) (tstr) – Sample ID (key field).; fam_id (tstr) – Family ID.; errors (tint64) – Number of Mendel errors involving this; individual.; snp_errors (tint64) – Number of Mendel errors involving this; individual at SNPs. Fourth table: errors per variant. locus (tlocus) – Variant locus, key field.; alleles (tarray of tstr) – Variant alleles, key field.; errors (tint64) – Number of Mendel errors in this variant. This method only considers complete trios (two parents and proband with; defined sex). The code of each Mendel error is determined by the table; below, extending the; Plink classification.; In the table, the copy state of a locus with respect to a trio is defined;",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:49791,Availability,error,errors,49791,"s = dataset.annotate_rows(mendel=per_variant[dataset.locus, dataset.alleles]). Notes; The example above returns four tables, which contain Mendelian violations; grouped in various ways. These tables are modeled after the PLINK mendel; formats, resembling; the .mendel, .fmendel, .imendel, and .lmendel formats,; respectively.; First table: all Mendel errors. This table contains one row per Mendel; error, keyed by the variant and proband id. locus (tlocus) – Variant locus, key field.; alleles (tarray of tstr) – Variant alleles, key field.; (column key of dataset) (tstr) – Proband ID, key field.; fam_id (tstr) – Family ID.; mendel_code (tint32) – Mendel error code, see below. Second table: errors per nuclear family. This table contains one row; per nuclear family, keyed by the parents. pat_id (tstr) – Paternal ID. (key field); mat_id (tstr) – Maternal ID. (key field); fam_id (tstr) – Family ID.; children (tint32) – Number of children in this nuclear family.; errors (tint64) – Number of Mendel errors in this nuclear family.; snp_errors (tint64) – Number of Mendel errors at SNPs in this; nuclear family. Third table: errors per individual. This table contains one row per; individual. Each error is counted toward the proband, father, and mother; according to the Implicated in the table below. (column key of dataset) (tstr) – Sample ID (key field).; fam_id (tstr) – Family ID.; errors (tint64) – Number of Mendel errors involving this; individual.; snp_errors (tint64) – Number of Mendel errors involving this; individual at SNPs. Fourth table: errors per variant. locus (tlocus) – Variant locus, key field.; alleles (tarray of tstr) – Variant alleles, key field.; errors (tint64) – Number of Mendel errors in this variant. This method only considers complete trios (two parents and proband with; defined sex). The code of each Mendel error is determined by the table; below, extending the; Plink classification.; In the table, the copy state of a locus with respect to a trio is defined;",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:49862,Availability,error,errors,49862,"s]). Notes; The example above returns four tables, which contain Mendelian violations; grouped in various ways. These tables are modeled after the PLINK mendel; formats, resembling; the .mendel, .fmendel, .imendel, and .lmendel formats,; respectively.; First table: all Mendel errors. This table contains one row per Mendel; error, keyed by the variant and proband id. locus (tlocus) – Variant locus, key field.; alleles (tarray of tstr) – Variant alleles, key field.; (column key of dataset) (tstr) – Proband ID, key field.; fam_id (tstr) – Family ID.; mendel_code (tint32) – Mendel error code, see below. Second table: errors per nuclear family. This table contains one row; per nuclear family, keyed by the parents. pat_id (tstr) – Paternal ID. (key field); mat_id (tstr) – Maternal ID. (key field); fam_id (tstr) – Family ID.; children (tint32) – Number of children in this nuclear family.; errors (tint64) – Number of Mendel errors in this nuclear family.; snp_errors (tint64) – Number of Mendel errors at SNPs in this; nuclear family. Third table: errors per individual. This table contains one row per; individual. Each error is counted toward the proband, father, and mother; according to the Implicated in the table below. (column key of dataset) (tstr) – Sample ID (key field).; fam_id (tstr) – Family ID.; errors (tint64) – Number of Mendel errors involving this; individual.; snp_errors (tint64) – Number of Mendel errors involving this; individual at SNPs. Fourth table: errors per variant. locus (tlocus) – Variant locus, key field.; alleles (tarray of tstr) – Variant alleles, key field.; errors (tint64) – Number of Mendel errors in this variant. This method only considers complete trios (two parents and proband with; defined sex). The code of each Mendel error is determined by the table; below, extending the; Plink classification.; In the table, the copy state of a locus with respect to a trio is defined; as follows, where PAR is the pseudoautosomal region (PAR) of X and Y; de",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:49915,Availability,error,errors,49915,"ontain Mendelian violations; grouped in various ways. These tables are modeled after the PLINK mendel; formats, resembling; the .mendel, .fmendel, .imendel, and .lmendel formats,; respectively.; First table: all Mendel errors. This table contains one row per Mendel; error, keyed by the variant and proband id. locus (tlocus) – Variant locus, key field.; alleles (tarray of tstr) – Variant alleles, key field.; (column key of dataset) (tstr) – Proband ID, key field.; fam_id (tstr) – Family ID.; mendel_code (tint32) – Mendel error code, see below. Second table: errors per nuclear family. This table contains one row; per nuclear family, keyed by the parents. pat_id (tstr) – Paternal ID. (key field); mat_id (tstr) – Maternal ID. (key field); fam_id (tstr) – Family ID.; children (tint32) – Number of children in this nuclear family.; errors (tint64) – Number of Mendel errors in this nuclear family.; snp_errors (tint64) – Number of Mendel errors at SNPs in this; nuclear family. Third table: errors per individual. This table contains one row per; individual. Each error is counted toward the proband, father, and mother; according to the Implicated in the table below. (column key of dataset) (tstr) – Sample ID (key field).; fam_id (tstr) – Family ID.; errors (tint64) – Number of Mendel errors involving this; individual.; snp_errors (tint64) – Number of Mendel errors involving this; individual at SNPs. Fourth table: errors per variant. locus (tlocus) – Variant locus, key field.; alleles (tarray of tstr) – Variant alleles, key field.; errors (tint64) – Number of Mendel errors in this variant. This method only considers complete trios (two parents and proband with; defined sex). The code of each Mendel error is determined by the table; below, extending the; Plink classification.; In the table, the copy state of a locus with respect to a trio is defined; as follows, where PAR is the pseudoautosomal region (PAR) of X and Y; defined by the reference genome and the autosome is defined ",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:49988,Availability,error,error,49988,"ling; the .mendel, .fmendel, .imendel, and .lmendel formats,; respectively.; First table: all Mendel errors. This table contains one row per Mendel; error, keyed by the variant and proband id. locus (tlocus) – Variant locus, key field.; alleles (tarray of tstr) – Variant alleles, key field.; (column key of dataset) (tstr) – Proband ID, key field.; fam_id (tstr) – Family ID.; mendel_code (tint32) – Mendel error code, see below. Second table: errors per nuclear family. This table contains one row; per nuclear family, keyed by the parents. pat_id (tstr) – Paternal ID. (key field); mat_id (tstr) – Maternal ID. (key field); fam_id (tstr) – Family ID.; children (tint32) – Number of children in this nuclear family.; errors (tint64) – Number of Mendel errors in this nuclear family.; snp_errors (tint64) – Number of Mendel errors at SNPs in this; nuclear family. Third table: errors per individual. This table contains one row per; individual. Each error is counted toward the proband, father, and mother; according to the Implicated in the table below. (column key of dataset) (tstr) – Sample ID (key field).; fam_id (tstr) – Family ID.; errors (tint64) – Number of Mendel errors involving this; individual.; snp_errors (tint64) – Number of Mendel errors involving this; individual at SNPs. Fourth table: errors per variant. locus (tlocus) – Variant locus, key field.; alleles (tarray of tstr) – Variant alleles, key field.; errors (tint64) – Number of Mendel errors in this variant. This method only considers complete trios (two parents and proband with; defined sex). The code of each Mendel error is determined by the table; below, extending the; Plink classification.; In the table, the copy state of a locus with respect to a trio is defined; as follows, where PAR is the pseudoautosomal region (PAR) of X and Y; defined by the reference genome and the autosome is defined by; in_autosome(). Auto – in autosome or in PAR or female child; HemiX – in non-PAR of X and male child; HemiY – in no",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:50178,Availability,error,errors,50178,"nt and proband id. locus (tlocus) – Variant locus, key field.; alleles (tarray of tstr) – Variant alleles, key field.; (column key of dataset) (tstr) – Proband ID, key field.; fam_id (tstr) – Family ID.; mendel_code (tint32) – Mendel error code, see below. Second table: errors per nuclear family. This table contains one row; per nuclear family, keyed by the parents. pat_id (tstr) – Paternal ID. (key field); mat_id (tstr) – Maternal ID. (key field); fam_id (tstr) – Family ID.; children (tint32) – Number of children in this nuclear family.; errors (tint64) – Number of Mendel errors in this nuclear family.; snp_errors (tint64) – Number of Mendel errors at SNPs in this; nuclear family. Third table: errors per individual. This table contains one row per; individual. Each error is counted toward the proband, father, and mother; according to the Implicated in the table below. (column key of dataset) (tstr) – Sample ID (key field).; fam_id (tstr) – Family ID.; errors (tint64) – Number of Mendel errors involving this; individual.; snp_errors (tint64) – Number of Mendel errors involving this; individual at SNPs. Fourth table: errors per variant. locus (tlocus) – Variant locus, key field.; alleles (tarray of tstr) – Variant alleles, key field.; errors (tint64) – Number of Mendel errors in this variant. This method only considers complete trios (two parents and proband with; defined sex). The code of each Mendel error is determined by the table; below, extending the; Plink classification.; In the table, the copy state of a locus with respect to a trio is defined; as follows, where PAR is the pseudoautosomal region (PAR) of X and Y; defined by the reference genome and the autosome is defined by; in_autosome(). Auto – in autosome or in PAR or female child; HemiX – in non-PAR of X and male child; HemiY – in non-PAR of Y and male child. Any refers to the set { HomRef, Het, HomVar, NoCall } and ~; denotes complement in this set. Code; Dad; Mom; Kid; Copy State | Implicated. 1; HomVar",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:50213,Availability,error,errors,50213,"nt and proband id. locus (tlocus) – Variant locus, key field.; alleles (tarray of tstr) – Variant alleles, key field.; (column key of dataset) (tstr) – Proband ID, key field.; fam_id (tstr) – Family ID.; mendel_code (tint32) – Mendel error code, see below. Second table: errors per nuclear family. This table contains one row; per nuclear family, keyed by the parents. pat_id (tstr) – Paternal ID. (key field); mat_id (tstr) – Maternal ID. (key field); fam_id (tstr) – Family ID.; children (tint32) – Number of children in this nuclear family.; errors (tint64) – Number of Mendel errors in this nuclear family.; snp_errors (tint64) – Number of Mendel errors at SNPs in this; nuclear family. Third table: errors per individual. This table contains one row per; individual. Each error is counted toward the proband, father, and mother; according to the Implicated in the table below. (column key of dataset) (tstr) – Sample ID (key field).; fam_id (tstr) – Family ID.; errors (tint64) – Number of Mendel errors involving this; individual.; snp_errors (tint64) – Number of Mendel errors involving this; individual at SNPs. Fourth table: errors per variant. locus (tlocus) – Variant locus, key field.; alleles (tarray of tstr) – Variant alleles, key field.; errors (tint64) – Number of Mendel errors in this variant. This method only considers complete trios (two parents and proband with; defined sex). The code of each Mendel error is determined by the table; below, extending the; Plink classification.; In the table, the copy state of a locus with respect to a trio is defined; as follows, where PAR is the pseudoautosomal region (PAR) of X and Y; defined by the reference genome and the autosome is defined by; in_autosome(). Auto – in autosome or in PAR or female child; HemiX – in non-PAR of X and male child; HemiY – in non-PAR of Y and male child. Any refers to the set { HomRef, Het, HomVar, NoCall } and ~; denotes complement in this set. Code; Dad; Mom; Kid; Copy State | Implicated. 1; HomVar",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:50288,Availability,error,errors,50288,"y of tstr) – Variant alleles, key field.; (column key of dataset) (tstr) – Proband ID, key field.; fam_id (tstr) – Family ID.; mendel_code (tint32) – Mendel error code, see below. Second table: errors per nuclear family. This table contains one row; per nuclear family, keyed by the parents. pat_id (tstr) – Paternal ID. (key field); mat_id (tstr) – Maternal ID. (key field); fam_id (tstr) – Family ID.; children (tint32) – Number of children in this nuclear family.; errors (tint64) – Number of Mendel errors in this nuclear family.; snp_errors (tint64) – Number of Mendel errors at SNPs in this; nuclear family. Third table: errors per individual. This table contains one row per; individual. Each error is counted toward the proband, father, and mother; according to the Implicated in the table below. (column key of dataset) (tstr) – Sample ID (key field).; fam_id (tstr) – Family ID.; errors (tint64) – Number of Mendel errors involving this; individual.; snp_errors (tint64) – Number of Mendel errors involving this; individual at SNPs. Fourth table: errors per variant. locus (tlocus) – Variant locus, key field.; alleles (tarray of tstr) – Variant alleles, key field.; errors (tint64) – Number of Mendel errors in this variant. This method only considers complete trios (two parents and proband with; defined sex). The code of each Mendel error is determined by the table; below, extending the; Plink classification.; In the table, the copy state of a locus with respect to a trio is defined; as follows, where PAR is the pseudoautosomal region (PAR) of X and Y; defined by the reference genome and the autosome is defined by; in_autosome(). Auto – in autosome or in PAR or female child; HemiX – in non-PAR of X and male child; HemiY – in non-PAR of Y and male child. Any refers to the set { HomRef, Het, HomVar, NoCall } and ~; denotes complement in this set. Code; Dad; Mom; Kid; Copy State | Implicated. 1; HomVar; HomVar; Het; Auto; Dad, Mom, Kid. 2; HomRef; HomRef; Het; Auto; Dad, Mom, K",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:50345,Availability,error,errors,50345,"taset) (tstr) – Proband ID, key field.; fam_id (tstr) – Family ID.; mendel_code (tint32) – Mendel error code, see below. Second table: errors per nuclear family. This table contains one row; per nuclear family, keyed by the parents. pat_id (tstr) – Paternal ID. (key field); mat_id (tstr) – Maternal ID. (key field); fam_id (tstr) – Family ID.; children (tint32) – Number of children in this nuclear family.; errors (tint64) – Number of Mendel errors in this nuclear family.; snp_errors (tint64) – Number of Mendel errors at SNPs in this; nuclear family. Third table: errors per individual. This table contains one row per; individual. Each error is counted toward the proband, father, and mother; according to the Implicated in the table below. (column key of dataset) (tstr) – Sample ID (key field).; fam_id (tstr) – Family ID.; errors (tint64) – Number of Mendel errors involving this; individual.; snp_errors (tint64) – Number of Mendel errors involving this; individual at SNPs. Fourth table: errors per variant. locus (tlocus) – Variant locus, key field.; alleles (tarray of tstr) – Variant alleles, key field.; errors (tint64) – Number of Mendel errors in this variant. This method only considers complete trios (two parents and proband with; defined sex). The code of each Mendel error is determined by the table; below, extending the; Plink classification.; In the table, the copy state of a locus with respect to a trio is defined; as follows, where PAR is the pseudoautosomal region (PAR) of X and Y; defined by the reference genome and the autosome is defined by; in_autosome(). Auto – in autosome or in PAR or female child; HemiX – in non-PAR of X and male child; HemiY – in non-PAR of Y and male child. Any refers to the set { HomRef, Het, HomVar, NoCall } and ~; denotes complement in this set. Code; Dad; Mom; Kid; Copy State | Implicated. 1; HomVar; HomVar; Het; Auto; Dad, Mom, Kid. 2; HomRef; HomRef; Het; Auto; Dad, Mom, Kid. 3; HomRef; ~HomRef; HomVar; Auto; Dad, Kid. 4; ~HomRef",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:50465,Availability,error,errors,50465,"nuclear family. This table contains one row; per nuclear family, keyed by the parents. pat_id (tstr) – Paternal ID. (key field); mat_id (tstr) – Maternal ID. (key field); fam_id (tstr) – Family ID.; children (tint32) – Number of children in this nuclear family.; errors (tint64) – Number of Mendel errors in this nuclear family.; snp_errors (tint64) – Number of Mendel errors at SNPs in this; nuclear family. Third table: errors per individual. This table contains one row per; individual. Each error is counted toward the proband, father, and mother; according to the Implicated in the table below. (column key of dataset) (tstr) – Sample ID (key field).; fam_id (tstr) – Family ID.; errors (tint64) – Number of Mendel errors involving this; individual.; snp_errors (tint64) – Number of Mendel errors involving this; individual at SNPs. Fourth table: errors per variant. locus (tlocus) – Variant locus, key field.; alleles (tarray of tstr) – Variant alleles, key field.; errors (tint64) – Number of Mendel errors in this variant. This method only considers complete trios (two parents and proband with; defined sex). The code of each Mendel error is determined by the table; below, extending the; Plink classification.; In the table, the copy state of a locus with respect to a trio is defined; as follows, where PAR is the pseudoautosomal region (PAR) of X and Y; defined by the reference genome and the autosome is defined by; in_autosome(). Auto – in autosome or in PAR or female child; HemiX – in non-PAR of X and male child; HemiY – in non-PAR of Y and male child. Any refers to the set { HomRef, Het, HomVar, NoCall } and ~; denotes complement in this set. Code; Dad; Mom; Kid; Copy State | Implicated. 1; HomVar; HomVar; Het; Auto; Dad, Mom, Kid. 2; HomRef; HomRef; Het; Auto; Dad, Mom, Kid. 3; HomRef; ~HomRef; HomVar; Auto; Dad, Kid. 4; ~HomRef; HomRef; HomVar; Auto; Mom, Kid. 5; HomRef; HomRef; HomVar; Auto; Kid. 6; HomVar; ~HomVar; HomRef; Auto; Dad, Kid. 7; ~HomVar; HomVar; HomRef; Au",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:50500,Availability,error,errors,50500,"nuclear family. This table contains one row; per nuclear family, keyed by the parents. pat_id (tstr) – Paternal ID. (key field); mat_id (tstr) – Maternal ID. (key field); fam_id (tstr) – Family ID.; children (tint32) – Number of children in this nuclear family.; errors (tint64) – Number of Mendel errors in this nuclear family.; snp_errors (tint64) – Number of Mendel errors at SNPs in this; nuclear family. Third table: errors per individual. This table contains one row per; individual. Each error is counted toward the proband, father, and mother; according to the Implicated in the table below. (column key of dataset) (tstr) – Sample ID (key field).; fam_id (tstr) – Family ID.; errors (tint64) – Number of Mendel errors involving this; individual.; snp_errors (tint64) – Number of Mendel errors involving this; individual at SNPs. Fourth table: errors per variant. locus (tlocus) – Variant locus, key field.; alleles (tarray of tstr) – Variant alleles, key field.; errors (tint64) – Number of Mendel errors in this variant. This method only considers complete trios (two parents and proband with; defined sex). The code of each Mendel error is determined by the table; below, extending the; Plink classification.; In the table, the copy state of a locus with respect to a trio is defined; as follows, where PAR is the pseudoautosomal region (PAR) of X and Y; defined by the reference genome and the autosome is defined by; in_autosome(). Auto – in autosome or in PAR or female child; HemiX – in non-PAR of X and male child; HemiY – in non-PAR of Y and male child. Any refers to the set { HomRef, Het, HomVar, NoCall } and ~; denotes complement in this set. Code; Dad; Mom; Kid; Copy State | Implicated. 1; HomVar; HomVar; Het; Auto; Dad, Mom, Kid. 2; HomRef; HomRef; Het; Auto; Dad, Mom, Kid. 3; HomRef; ~HomRef; HomVar; Auto; Dad, Kid. 4; ~HomRef; HomRef; HomVar; Auto; Mom, Kid. 5; HomRef; HomRef; HomVar; Auto; Kid. 6; HomVar; ~HomVar; HomRef; Auto; Dad, Kid. 7; ~HomVar; HomVar; HomRef; Au",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:50635,Availability,error,error,50635,"); fam_id (tstr) – Family ID.; children (tint32) – Number of children in this nuclear family.; errors (tint64) – Number of Mendel errors in this nuclear family.; snp_errors (tint64) – Number of Mendel errors at SNPs in this; nuclear family. Third table: errors per individual. This table contains one row per; individual. Each error is counted toward the proband, father, and mother; according to the Implicated in the table below. (column key of dataset) (tstr) – Sample ID (key field).; fam_id (tstr) – Family ID.; errors (tint64) – Number of Mendel errors involving this; individual.; snp_errors (tint64) – Number of Mendel errors involving this; individual at SNPs. Fourth table: errors per variant. locus (tlocus) – Variant locus, key field.; alleles (tarray of tstr) – Variant alleles, key field.; errors (tint64) – Number of Mendel errors in this variant. This method only considers complete trios (two parents and proband with; defined sex). The code of each Mendel error is determined by the table; below, extending the; Plink classification.; In the table, the copy state of a locus with respect to a trio is defined; as follows, where PAR is the pseudoautosomal region (PAR) of X and Y; defined by the reference genome and the autosome is defined by; in_autosome(). Auto – in autosome or in PAR or female child; HemiX – in non-PAR of X and male child; HemiY – in non-PAR of Y and male child. Any refers to the set { HomRef, Het, HomVar, NoCall } and ~; denotes complement in this set. Code; Dad; Mom; Kid; Copy State | Implicated. 1; HomVar; HomVar; Het; Auto; Dad, Mom, Kid. 2; HomRef; HomRef; Het; Auto; Dad, Mom, Kid. 3; HomRef; ~HomRef; HomVar; Auto; Dad, Kid. 4; ~HomRef; HomRef; HomVar; Auto; Mom, Kid. 5; HomRef; HomRef; HomVar; Auto; Kid. 6; HomVar; ~HomVar; HomRef; Auto; Dad, Kid. 7; ~HomVar; HomVar; HomRef; Auto; Mom, Kid. 8; HomVar; HomVar; HomRef; Auto; Kid. 9; Any; HomVar; HomRef; HemiX; Mom, Kid. 10; Any; HomRef; HomVar; HemiX; Mom, Kid. 11; HomVar; Any; HomRef; HemiY; D",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:56639,Availability,error,error,56639,"{P}(x_{\mathrm{mother}} = AA \mid \mathrm{mother} = AA) \\; {} \cdot {} &\mathrm{P}(x_{\mathrm{proband}} = AB \mid \mathrm{proband} = AB); \end{aligned}; \right). \]. \[\begin{aligned}; \mathrm{P}(x = (AA, AA, AB) \mid m) = &\left(; \begin{aligned}; &\mathrm{P}(x_{\mathrm{father}} = AA \mid \mathrm{father} = AB); \cdot \mathrm{P}(x_{\mathrm{mother}} = AA \mid \mathrm{mother} = AA) \\; {} + {} &\mathrm{P}(x_{\mathrm{father}} = AA \mid \mathrm{father} = AA); \cdot \mathrm{P}(x_{\mathrm{mother}} = AA \mid \mathrm{mother} = AB); \end{aligned}; \right) \\; &{} \cdot \mathrm{P}(x_{\mathrm{proband}} = AB \mid \mathrm{proband} = AB); \end{aligned}. \]; (Technically, the second factorization assumes there is exactly (rather; than at least) one alternate allele among the parents, which may be; justified on the grounds that it is typically the most likely case by far.); While this posterior probability is a good metric for grouping putative de; novo mutations by validation likelihood, there exist error modes in; high-throughput sequencing data that are not appropriately accounted for by; the phred-scaled genotype likelihoods. To this end, a number of hard filters; are applied in order to assign validation likelihood.; These filters are different for SNPs and insertions/deletions. In the below; rules, the following variables are used:. DR refers to the ratio of the read depth in the proband to the; combined read depth in the parents.; DP refers to the read depth (DP field) of the proband.; AB refers to the read allele balance of the proband (number of; alternate reads divided by total reads).; AC refers to the count of alternate alleles across all individuals; in the dataset at the site.; p refers to \(\mathrm{P_{\text{de novo}}}\).; min_p refers to the min_p function parameter. HIGH-quality SNV:; (p > 0.99) AND (AB > 0.3) AND (AC == 1); OR; (p > 0.99) AND (AB > 0.3) AND (DR > 0.2); OR; (p > 0.5) AND (AB > 0.3) AND (AC < 10) AND (DP > 10). MEDIUM-quality SNV:; (p > 0.5) AND (AB ",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:66414,Availability,error,error,66414,"r quality control. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). Examples; Compute sample QC metrics and remove low-quality samples:; >>> dataset = hl.sample_qc(dataset, name='sample_qc'); >>> filtered_dataset = dataset.filter_cols((dataset.sample_qc.dp_stats.mean > 20) & (dataset.sample_qc.r_ti_tv > 1.5)). Notes; This method computes summary statistics per sample from a genetic matrix and stores; the results as a new column-indexed struct field in the matrix, named based on the; name parameter.; If mt contains an entry field DP of type tint32, then the; field dp_stats is computed. If mt contains an entry field GQ of type; tint32, then the field gq_stats is computed. Both dp_stats; and gq_stats are structs with with four fields:. mean (float64) – Mean value.; stdev (float64) – Standard deviation (zero degrees of freedom).; min (int32) – Minimum value.; max (int32) – Maximum value. If the dataset does not contain an entry field GT of type; tcall, then an error is raised. The following fields are always; computed from GT:. call_rate (float64) – Fraction of calls not missing or filtered.; Equivalent to n_called divided by count_rows().; n_called (int64) – Number of non-missing calls.; n_not_called (int64) – Number of missing calls.; n_filtered (int64) – Number of filtered entries.; n_hom_ref (int64) – Number of homozygous reference calls.; n_het (int64) – Number of heterozygous calls.; n_hom_var (int64) – Number of homozygous alternate calls.; n_non_ref (int64) – Sum of n_het and n_hom_var.; n_snp (int64) – Number of SNP alternate alleles.; n_insertion (int64) – Number of insertion alternate alleles.; n_deletion (int64) – Number of deletion alternate alleles.; n_singleton (int64) – Number of private alleles. Reference alleles are never counted as singletons, even if; every other allele at a site is non-reference.; n_transition (int64) – Number of transition (A-G, C-T) alternate alleles.; n_transversion (int64",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:72779,Availability,fault,fault,72779," association testing for; sequencing data with the sequence kernel association test. Am J Hum Genet. 2011 Jul; 15;89(1):82-93. doi: 10.1016/j.ajhg.2011.05.029. Epub 2011 Jul 7. PMID: 21737059; PMCID:; PMC3135811. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3135811/. Examples; Generate a dataset with a phenotype noisily computed from the genotypes:; >>> hl.reset_global_randomness(); >>> mt = hl.balding_nichols_model(1, n_samples=100, n_variants=20); >>> mt = mt.annotate_rows(gene = mt.locus.position // 12); >>> mt = mt.annotate_rows(weight = 1); >>> mt = mt.annotate_cols(phenotype = (hl.agg.sum(mt.GT.n_alt_alleles()) - 20 + hl.rand_norm(0, 1)) > 0.5). Test if the phenotype is significantly associated with the genotype:; >>> skat = hl._logistic_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0]); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | 1.78e+02 | 1.68e-04 | 0 |; | 1 | 9 | 1.39e+02 | 1.82e-03 | 0 |; +-------+-------+----------+----------+-------+. The same test, but using the original paper’s suggested weights which are derived from the; allele frequency.; >>> mt = hl.variant_qc(mt); >>> skat = hl._logistic_skat(; ... mt.gene,; ... hl.dbeta(mt.variant_qc.AF[0], 1, 25),; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0]); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | 8.04e+00 | 3.50e-01 | 0 |; | 1 | 9 | 1.22e+00 | 5.04e-01 | 0 |; +-------+-------+----------+----------+-------+. Our simulated data was unweighted, so the null hypothesis appears true. In real da",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:73456,Availability,fault,fault,73456,"type is significantly associated with the genotype:; >>> skat = hl._logistic_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0]); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | 1.78e+02 | 1.68e-04 | 0 |; | 1 | 9 | 1.39e+02 | 1.82e-03 | 0 |; +-------+-------+----------+----------+-------+. The same test, but using the original paper’s suggested weights which are derived from the; allele frequency.; >>> mt = hl.variant_qc(mt); >>> skat = hl._logistic_skat(; ... mt.gene,; ... hl.dbeta(mt.variant_qc.AF[0], 1, 25),; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0]); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | 8.04e+00 | 3.50e-01 | 0 |; | 1 | 9 | 1.22e+00 | 5.04e-01 | 0 |; +-------+-------+----------+----------+-------+. Our simulated data was unweighted, so the null hypothesis appears true. In real datasets, we; expect the allele frequency to correlate with effect size.; Notice that, in the second group, the fault flag is set to 1. This indicates that the numerical; integration to calculate the p-value failed to achieve the required accuracy (by default,; 1e-6). In this particular case, the null hypothesis is likely true and the numerical integration; returned a (nonsensical) value greater than one.; The max_size parameter allows us to skip large genes that would cause “out of memory” errors:; >>> skat = hl._logistic_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0],; ... max_size=10); >>> skat.show(); +-------",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:73926,Availability,fault,fault,73926,"-----+-------+; | 0 | 11 | 1.78e+02 | 1.68e-04 | 0 |; | 1 | 9 | 1.39e+02 | 1.82e-03 | 0 |; +-------+-------+----------+----------+-------+. The same test, but using the original paper’s suggested weights which are derived from the; allele frequency.; >>> mt = hl.variant_qc(mt); >>> skat = hl._logistic_skat(; ... mt.gene,; ... hl.dbeta(mt.variant_qc.AF[0], 1, 25),; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0]); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | 8.04e+00 | 3.50e-01 | 0 |; | 1 | 9 | 1.22e+00 | 5.04e-01 | 0 |; +-------+-------+----------+----------+-------+. Our simulated data was unweighted, so the null hypothesis appears true. In real datasets, we; expect the allele frequency to correlate with effect size.; Notice that, in the second group, the fault flag is set to 1. This indicates that the numerical; integration to calculate the p-value failed to achieve the required accuracy (by default,; 1e-6). In this particular case, the null hypothesis is likely true and the numerical integration; returned a (nonsensical) value greater than one.; The max_size parameter allows us to skip large genes that would cause “out of memory” errors:; >>> skat = hl._logistic_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0],; ... max_size=10); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | NA | NA | NA |; | 1 | 9 | 1.39e+02 | 1.82e-03 | 0 |; +-------+-------+----------+----------+-------+. Notes; In the SKAT R package, the “weights” are actually the square root of the w",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:74310,Availability,error,errors,74310," 25),; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0]); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | 8.04e+00 | 3.50e-01 | 0 |; | 1 | 9 | 1.22e+00 | 5.04e-01 | 0 |; +-------+-------+----------+----------+-------+. Our simulated data was unweighted, so the null hypothesis appears true. In real datasets, we; expect the allele frequency to correlate with effect size.; Notice that, in the second group, the fault flag is set to 1. This indicates that the numerical; integration to calculate the p-value failed to achieve the required accuracy (by default,; 1e-6). In this particular case, the null hypothesis is likely true and the numerical integration; returned a (nonsensical) value greater than one.; The max_size parameter allows us to skip large genes that would cause “out of memory” errors:; >>> skat = hl._logistic_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0],; ... max_size=10); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | NA | NA | NA |; | 1 | 9 | 1.39e+02 | 1.82e-03 | 0 |; +-------+-------+----------+----------+-------+. Notes; In the SKAT R package, the “weights” are actually the square root of the weight expression; from the paper. This method uses the definition from the paper.; The paper includes an explicit intercept term but this method expects the user to specify the; intercept as an extra covariate with the value 1.; This method does not perform small sample size correction.; The q_stat return value is not the \(Q\) statistic from the paper. We m",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:74570,Availability,fault,fault,74570," 8.04e+00 | 3.50e-01 | 0 |; | 1 | 9 | 1.22e+00 | 5.04e-01 | 0 |; +-------+-------+----------+----------+-------+. Our simulated data was unweighted, so the null hypothesis appears true. In real datasets, we; expect the allele frequency to correlate with effect size.; Notice that, in the second group, the fault flag is set to 1. This indicates that the numerical; integration to calculate the p-value failed to achieve the required accuracy (by default,; 1e-6). In this particular case, the null hypothesis is likely true and the numerical integration; returned a (nonsensical) value greater than one.; The max_size parameter allows us to skip large genes that would cause “out of memory” errors:; >>> skat = hl._logistic_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0],; ... max_size=10); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | NA | NA | NA |; | 1 | 9 | 1.39e+02 | 1.82e-03 | 0 |; +-------+-------+----------+----------+-------+. Notes; In the SKAT R package, the “weights” are actually the square root of the weight expression; from the paper. This method uses the definition from the paper.; The paper includes an explicit intercept term but this method expects the user to specify the; intercept as an extra covariate with the value 1.; This method does not perform small sample size correction.; The q_stat return value is not the \(Q\) statistic from the paper. We match the output; of the SKAT R package which returns \(\tilde{Q}\):. \[\tilde{Q} = \frac{Q}{2}\]. Parameters:. group (Expression) – Row-indexed expression indicating to which group a variant belongs. This is typically a gene; name or an interval.; weight (Float64Expression) – Row-indexed expression for weights. Must be non-negative.; y (Flo",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:76333,Availability,error,errors,76333,"AT R package which returns \(\tilde{Q}\):. \[\tilde{Q} = \frac{Q}{2}\]. Parameters:. group (Expression) – Row-indexed expression indicating to which group a variant belongs. This is typically a gene; name or an interval.; weight (Float64Expression) – Row-indexed expression for weights. Must be non-negative.; y (Float64Expression) – Column-indexed response (dependent variable) expression.; x (Float64Expression) – Entry-indexed expression for input (independent variable).; covariates (list of Float64Expression) – List of column-indexed covariate expressions. You must explicitly provide an intercept term; if desired. You must provide at least one covariate.; max_size (int) – Maximum size of group on which to run the test. Groups which exceed this size will have a; missing p-value and missing q statistic. Defaults to 46340.; null_max_iterations (int) – The maximum number of iterations when fitting the logistic null model. Defaults to 25.; null_tolerance (float) – The null model logisitic regression converges when the errors is less than this. Defaults to; 1e-6.; accuracy (float) – The accuracy of the p-value if fault value is zero. Defaults to 1e-6.; iterations (int) – The maximum number of iterations used to calculate the p-value (which has no closed; form). Defaults to 1e5. Returns:; Table – One row per-group. The key is group. The row fields are:. group : the group parameter.; size : tint64, the number of variants in this group.; q_stat : tfloat64, the \(Q\) statistic, see Notes for why this differs from the paper.; p_value : tfloat64, the test p-value for the null hypothesis that the genotypes; have no linear influence on the phenotypes.; fault : tint32, the fault flag from pgenchisq(). The global fields are:. n_complete_samples : tint32, the number of samples with neither a missing; phenotype nor a missing covariate.; y_residual : tint32, the residual phenotype from the null model. This may be; interpreted as the component of the phenotype not explained by the covar",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:76429,Availability,fault,fault,76429,"-indexed expression indicating to which group a variant belongs. This is typically a gene; name or an interval.; weight (Float64Expression) – Row-indexed expression for weights. Must be non-negative.; y (Float64Expression) – Column-indexed response (dependent variable) expression.; x (Float64Expression) – Entry-indexed expression for input (independent variable).; covariates (list of Float64Expression) – List of column-indexed covariate expressions. You must explicitly provide an intercept term; if desired. You must provide at least one covariate.; max_size (int) – Maximum size of group on which to run the test. Groups which exceed this size will have a; missing p-value and missing q statistic. Defaults to 46340.; null_max_iterations (int) – The maximum number of iterations when fitting the logistic null model. Defaults to 25.; null_tolerance (float) – The null model logisitic regression converges when the errors is less than this. Defaults to; 1e-6.; accuracy (float) – The accuracy of the p-value if fault value is zero. Defaults to 1e-6.; iterations (int) – The maximum number of iterations used to calculate the p-value (which has no closed; form). Defaults to 1e5. Returns:; Table – One row per-group. The key is group. The row fields are:. group : the group parameter.; size : tint64, the number of variants in this group.; q_stat : tfloat64, the \(Q\) statistic, see Notes for why this differs from the paper.; p_value : tfloat64, the test p-value for the null hypothesis that the genotypes; have no linear influence on the phenotypes.; fault : tint32, the fault flag from pgenchisq(). The global fields are:. n_complete_samples : tint32, the number of samples with neither a missing; phenotype nor a missing covariate.; y_residual : tint32, the residual phenotype from the null model. This may be; interpreted as the component of the phenotype not explained by the covariates alone.; s2 : tfloat64, the variance of the residuals, \(\sigma^2\) in the paper.; null_fit:. b : tndar",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:76971,Availability,fault,fault,76971,"ize of group on which to run the test. Groups which exceed this size will have a; missing p-value and missing q statistic. Defaults to 46340.; null_max_iterations (int) – The maximum number of iterations when fitting the logistic null model. Defaults to 25.; null_tolerance (float) – The null model logisitic regression converges when the errors is less than this. Defaults to; 1e-6.; accuracy (float) – The accuracy of the p-value if fault value is zero. Defaults to 1e-6.; iterations (int) – The maximum number of iterations used to calculate the p-value (which has no closed; form). Defaults to 1e5. Returns:; Table – One row per-group. The key is group. The row fields are:. group : the group parameter.; size : tint64, the number of variants in this group.; q_stat : tfloat64, the \(Q\) statistic, see Notes for why this differs from the paper.; p_value : tfloat64, the test p-value for the null hypothesis that the genotypes; have no linear influence on the phenotypes.; fault : tint32, the fault flag from pgenchisq(). The global fields are:. n_complete_samples : tint32, the number of samples with neither a missing; phenotype nor a missing covariate.; y_residual : tint32, the residual phenotype from the null model. This may be; interpreted as the component of the phenotype not explained by the covariates alone.; s2 : tfloat64, the variance of the residuals, \(\sigma^2\) in the paper.; null_fit:. b : tndarray vector of coefficients.; score : tndarray vector of score statistics.; fisher : tndarray matrix of fisher statistics.; mu : tndarray the expected value under the null model.; n_iterations : tint32 the number of iterations before termination.; log_lkhd : tfloat64 the log-likelihood of the final iteration.; converged : tbool True if the null model converged.; exploded : tbool True if the null model failed to converge due to numerical; explosion. hail.methods.skat(key_expr, weight_expr, y, x, covariates, logistic=False, max_size=46340, accuracy=1e-06, iterations=10000)[sour",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:76991,Availability,fault,fault,76991,"ize of group on which to run the test. Groups which exceed this size will have a; missing p-value and missing q statistic. Defaults to 46340.; null_max_iterations (int) – The maximum number of iterations when fitting the logistic null model. Defaults to 25.; null_tolerance (float) – The null model logisitic regression converges when the errors is less than this. Defaults to; 1e-6.; accuracy (float) – The accuracy of the p-value if fault value is zero. Defaults to 1e-6.; iterations (int) – The maximum number of iterations used to calculate the p-value (which has no closed; form). Defaults to 1e5. Returns:; Table – One row per-group. The key is group. The row fields are:. group : the group parameter.; size : tint64, the number of variants in this group.; q_stat : tfloat64, the \(Q\) statistic, see Notes for why this differs from the paper.; p_value : tfloat64, the test p-value for the null hypothesis that the genotypes; have no linear influence on the phenotypes.; fault : tint32, the fault flag from pgenchisq(). The global fields are:. n_complete_samples : tint32, the number of samples with neither a missing; phenotype nor a missing covariate.; y_residual : tint32, the residual phenotype from the null model. This may be; interpreted as the component of the phenotype not explained by the covariates alone.; s2 : tfloat64, the variance of the residuals, \(\sigma^2\) in the paper.; null_fit:. b : tndarray vector of coefficients.; score : tndarray vector of score statistics.; fisher : tndarray matrix of fisher statistics.; mu : tndarray the expected value under the null model.; n_iterations : tint32 the number of iterations before termination.; log_lkhd : tfloat64 the log-likelihood of the final iteration.; converged : tbool True if the null model converged.; exploded : tbool True if the null model failed to converge due to numerical; explosion. hail.methods.skat(key_expr, weight_expr, y, x, covariates, logistic=False, max_size=46340, accuracy=1e-06, iterations=10000)[sour",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:80310,Availability,fault,fault,80310," mean-imputed over these columns.; As in the example, the intercept covariate 1 must be included; explicitly if desired. Notes; This method provides a scalable implementation of the score-based; variance-component test originally described in; Rare-Variant Association Testing for Sequencing Data with the Sequence Kernel Association Test.; Row weights must be non-negative. Rows with missing weights are ignored. In; the R package skat—which assumes rows are variants—default weights; are given by evaluating the Beta(1, 25) density at the minor allele; frequency. To replicate these weights in Hail using alternate allele; frequencies stored in a row-indexed field AF, one can use the expression:; >>> hl.dbeta(hl.min(ds2.AF), 1.0, 25.0) ** 2. In the logistic case, the response y must either be numeric (with all; present values 0 or 1) or Boolean, in which case true and false are coded; as 1 and 0, respectively.; The resulting Table provides the group’s key (id), thenumber of; rows in the group (size), the variance component score q_stat, the SKAT; p-value, and a fault flag. For the toy example above, the table has the; form:. id; size; q_stat; p_value; fault. geneA; 2; 4.136; 0.205; 0. geneB; 1; 5.659; 0.195; 0. geneC; 3; 4.122; 0.192; 0. Groups larger than max_size appear with missing q_stat, p_value, and; fault. The hard limit on the number of rows in a group is 46340.; Note that the variance component score q_stat agrees with Q in the R; package skat, but both differ from \(Q\) in the paper by the factor; \(\frac{1}{2\sigma^2}\) in the linear case and \(\frac{1}{2}\) in; the logistic case, where \(\sigma^2\) is the unbiased estimator of; residual variance for the linear null model. The R package also applies a; “small-sample adjustment” to the null distribution in the logistic case; when the sample size is less than 2000. Hail does not apply this; adjustment.; The fault flag is an integer indicating whether any issues occurred when; running the Davies algorithm to compu",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:80402,Availability,fault,fault,80402,"alable implementation of the score-based; variance-component test originally described in; Rare-Variant Association Testing for Sequencing Data with the Sequence Kernel Association Test.; Row weights must be non-negative. Rows with missing weights are ignored. In; the R package skat—which assumes rows are variants—default weights; are given by evaluating the Beta(1, 25) density at the minor allele; frequency. To replicate these weights in Hail using alternate allele; frequencies stored in a row-indexed field AF, one can use the expression:; >>> hl.dbeta(hl.min(ds2.AF), 1.0, 25.0) ** 2. In the logistic case, the response y must either be numeric (with all; present values 0 or 1) or Boolean, in which case true and false are coded; as 1 and 0, respectively.; The resulting Table provides the group’s key (id), thenumber of; rows in the group (size), the variance component score q_stat, the SKAT; p-value, and a fault flag. For the toy example above, the table has the; form:. id; size; q_stat; p_value; fault. geneA; 2; 4.136; 0.205; 0. geneB; 1; 5.659; 0.195; 0. geneC; 3; 4.122; 0.192; 0. Groups larger than max_size appear with missing q_stat, p_value, and; fault. The hard limit on the number of rows in a group is 46340.; Note that the variance component score q_stat agrees with Q in the R; package skat, but both differ from \(Q\) in the paper by the factor; \(\frac{1}{2\sigma^2}\) in the linear case and \(\frac{1}{2}\) in; the logistic case, where \(\sigma^2\) is the unbiased estimator of; residual variance for the linear null model. The R package also applies a; “small-sample adjustment” to the null distribution in the logistic case; when the sample size is less than 2000. Hail does not apply this; adjustment.; The fault flag is an integer indicating whether any issues occurred when; running the Davies algorithm to compute the p-value as the right tail of a; weighted sum of \(\chi^2(1)\) distributions. fault value; Description. 0; no issues. 1; accuracy NOT achieved. 2; ",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:80560,Availability,fault,fault,80560,"ng Data with the Sequence Kernel Association Test.; Row weights must be non-negative. Rows with missing weights are ignored. In; the R package skat—which assumes rows are variants—default weights; are given by evaluating the Beta(1, 25) density at the minor allele; frequency. To replicate these weights in Hail using alternate allele; frequencies stored in a row-indexed field AF, one can use the expression:; >>> hl.dbeta(hl.min(ds2.AF), 1.0, 25.0) ** 2. In the logistic case, the response y must either be numeric (with all; present values 0 or 1) or Boolean, in which case true and false are coded; as 1 and 0, respectively.; The resulting Table provides the group’s key (id), thenumber of; rows in the group (size), the variance component score q_stat, the SKAT; p-value, and a fault flag. For the toy example above, the table has the; form:. id; size; q_stat; p_value; fault. geneA; 2; 4.136; 0.205; 0. geneB; 1; 5.659; 0.195; 0. geneC; 3; 4.122; 0.192; 0. Groups larger than max_size appear with missing q_stat, p_value, and; fault. The hard limit on the number of rows in a group is 46340.; Note that the variance component score q_stat agrees with Q in the R; package skat, but both differ from \(Q\) in the paper by the factor; \(\frac{1}{2\sigma^2}\) in the linear case and \(\frac{1}{2}\) in; the logistic case, where \(\sigma^2\) is the unbiased estimator of; residual variance for the linear null model. The R package also applies a; “small-sample adjustment” to the null distribution in the logistic case; when the sample size is less than 2000. Hail does not apply this; adjustment.; The fault flag is an integer indicating whether any issues occurred when; running the Davies algorithm to compute the p-value as the right tail of a; weighted sum of \(\chi^2(1)\) distributions. fault value; Description. 0; no issues. 1; accuracy NOT achieved. 2; round-off error possibly significant. 3; invalid parameters. 4; unable to locate integration parameters. 5; out of memory. Parameters:. k",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:81131,Availability,fault,fault,81131,"ows in the group (size), the variance component score q_stat, the SKAT; p-value, and a fault flag. For the toy example above, the table has the; form:. id; size; q_stat; p_value; fault. geneA; 2; 4.136; 0.205; 0. geneB; 1; 5.659; 0.195; 0. geneC; 3; 4.122; 0.192; 0. Groups larger than max_size appear with missing q_stat, p_value, and; fault. The hard limit on the number of rows in a group is 46340.; Note that the variance component score q_stat agrees with Q in the R; package skat, but both differ from \(Q\) in the paper by the factor; \(\frac{1}{2\sigma^2}\) in the linear case and \(\frac{1}{2}\) in; the logistic case, where \(\sigma^2\) is the unbiased estimator of; residual variance for the linear null model. The R package also applies a; “small-sample adjustment” to the null distribution in the logistic case; when the sample size is less than 2000. Hail does not apply this; adjustment.; The fault flag is an integer indicating whether any issues occurred when; running the Davies algorithm to compute the p-value as the right tail of a; weighted sum of \(\chi^2(1)\) distributions. fault value; Description. 0; no issues. 1; accuracy NOT achieved. 2; round-off error possibly significant. 3; invalid parameters. 4; unable to locate integration parameters. 5; out of memory. Parameters:. key_expr (Expression) – Row-indexed expression for key associated to each row.; weight_expr (Float64Expression) – Row-indexed expression for row weights.; y (Float64Expression) – Column-indexed response expression.; If logistic is True, all non-missing values must evaluate to 0 or; 1. Note that a BooleanExpression will be implicitly converted; to a Float64Expression with this property.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – List of column-indexed covariate expressions.; logistic (bool or tuple of int and float) – If false, use the linear test. If true, use the logistic test with no; more than 25 logistic iterations ",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:81322,Availability,fault,fault,81322,"example above, the table has the; form:. id; size; q_stat; p_value; fault. geneA; 2; 4.136; 0.205; 0. geneB; 1; 5.659; 0.195; 0. geneC; 3; 4.122; 0.192; 0. Groups larger than max_size appear with missing q_stat, p_value, and; fault. The hard limit on the number of rows in a group is 46340.; Note that the variance component score q_stat agrees with Q in the R; package skat, but both differ from \(Q\) in the paper by the factor; \(\frac{1}{2\sigma^2}\) in the linear case and \(\frac{1}{2}\) in; the logistic case, where \(\sigma^2\) is the unbiased estimator of; residual variance for the linear null model. The R package also applies a; “small-sample adjustment” to the null distribution in the logistic case; when the sample size is less than 2000. Hail does not apply this; adjustment.; The fault flag is an integer indicating whether any issues occurred when; running the Davies algorithm to compute the p-value as the right tail of a; weighted sum of \(\chi^2(1)\) distributions. fault value; Description. 0; no issues. 1; accuracy NOT achieved. 2; round-off error possibly significant. 3; invalid parameters. 4; unable to locate integration parameters. 5; out of memory. Parameters:. key_expr (Expression) – Row-indexed expression for key associated to each row.; weight_expr (Float64Expression) – Row-indexed expression for row weights.; y (Float64Expression) – Column-indexed response expression.; If logistic is True, all non-missing values must evaluate to 0 or; 1. Note that a BooleanExpression will be implicitly converted; to a Float64Expression with this property.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – List of column-indexed covariate expressions.; logistic (bool or tuple of int and float) – If false, use the linear test. If true, use the logistic test with no; more than 25 logistic iterations and a convergence tolerance of 1e-6. If; a tuple is given, use the logistic test with the tuple elements as the",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:81401,Availability,error,error,81401,". geneA; 2; 4.136; 0.205; 0. geneB; 1; 5.659; 0.195; 0. geneC; 3; 4.122; 0.192; 0. Groups larger than max_size appear with missing q_stat, p_value, and; fault. The hard limit on the number of rows in a group is 46340.; Note that the variance component score q_stat agrees with Q in the R; package skat, but both differ from \(Q\) in the paper by the factor; \(\frac{1}{2\sigma^2}\) in the linear case and \(\frac{1}{2}\) in; the logistic case, where \(\sigma^2\) is the unbiased estimator of; residual variance for the linear null model. The R package also applies a; “small-sample adjustment” to the null distribution in the logistic case; when the sample size is less than 2000. Hail does not apply this; adjustment.; The fault flag is an integer indicating whether any issues occurred when; running the Davies algorithm to compute the p-value as the right tail of a; weighted sum of \(\chi^2(1)\) distributions. fault value; Description. 0; no issues. 1; accuracy NOT achieved. 2; round-off error possibly significant. 3; invalid parameters. 4; unable to locate integration parameters. 5; out of memory. Parameters:. key_expr (Expression) – Row-indexed expression for key associated to each row.; weight_expr (Float64Expression) – Row-indexed expression for row weights.; y (Float64Expression) – Column-indexed response expression.; If logistic is True, all non-missing values must evaluate to 0 or; 1. Note that a BooleanExpression will be implicitly converted; to a Float64Expression with this property.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – List of column-indexed covariate expressions.; logistic (bool or tuple of int and float) – If false, use the linear test. If true, use the logistic test with no; more than 25 logistic iterations and a convergence tolerance of 1e-6. If; a tuple is given, use the logistic test with the tuple elements as the; maximum nubmer of iterations and convergence tolerance, respectively.; m",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:82241,Availability,toler,tolerance,82241,"ning the Davies algorithm to compute the p-value as the right tail of a; weighted sum of \(\chi^2(1)\) distributions. fault value; Description. 0; no issues. 1; accuracy NOT achieved. 2; round-off error possibly significant. 3; invalid parameters. 4; unable to locate integration parameters. 5; out of memory. Parameters:. key_expr (Expression) – Row-indexed expression for key associated to each row.; weight_expr (Float64Expression) – Row-indexed expression for row weights.; y (Float64Expression) – Column-indexed response expression.; If logistic is True, all non-missing values must evaluate to 0 or; 1. Note that a BooleanExpression will be implicitly converted; to a Float64Expression with this property.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – List of column-indexed covariate expressions.; logistic (bool or tuple of int and float) – If false, use the linear test. If true, use the logistic test with no; more than 25 logistic iterations and a convergence tolerance of 1e-6. If; a tuple is given, use the logistic test with the tuple elements as the; maximum nubmer of iterations and convergence tolerance, respectively.; max_size (int) – Maximum size of group on which to run the test.; accuracy (float) – Accuracy achieved by the Davies algorithm if fault value is zero.; iterations (int) – Maximum number of iterations attempted by the Davies algorithm. Returns:; Table – Table of SKAT results. hail.methods.lambda_gc(p_value, approximate=True)[source]; Compute genomic inflation factor (lambda GC) from an Expression of p-values. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. Parameters:. p_value (NumericExpression) – Row-indexed numeric expression of p-values.; approximate (bool) – If False, computes exact lambda GC (slower and uses more memory). Returns:; float – Genomic inflation factor (lambda genomic control). ha",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:82381,Availability,toler,tolerance,82381,"e; Description. 0; no issues. 1; accuracy NOT achieved. 2; round-off error possibly significant. 3; invalid parameters. 4; unable to locate integration parameters. 5; out of memory. Parameters:. key_expr (Expression) – Row-indexed expression for key associated to each row.; weight_expr (Float64Expression) – Row-indexed expression for row weights.; y (Float64Expression) – Column-indexed response expression.; If logistic is True, all non-missing values must evaluate to 0 or; 1. Note that a BooleanExpression will be implicitly converted; to a Float64Expression with this property.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – List of column-indexed covariate expressions.; logistic (bool or tuple of int and float) – If false, use the linear test. If true, use the logistic test with no; more than 25 logistic iterations and a convergence tolerance of 1e-6. If; a tuple is given, use the logistic test with the tuple elements as the; maximum nubmer of iterations and convergence tolerance, respectively.; max_size (int) – Maximum size of group on which to run the test.; accuracy (float) – Accuracy achieved by the Davies algorithm if fault value is zero.; iterations (int) – Maximum number of iterations attempted by the Davies algorithm. Returns:; Table – Table of SKAT results. hail.methods.lambda_gc(p_value, approximate=True)[source]; Compute genomic inflation factor (lambda GC) from an Expression of p-values. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. Parameters:. p_value (NumericExpression) – Row-indexed numeric expression of p-values.; approximate (bool) – If False, computes exact lambda GC (slower and uses more memory). Returns:; float – Genomic inflation factor (lambda genomic control). hail.methods.split_multi(ds, keep_star=False, left_aligned=False, *, permit_shuffle=False)[source]; Split multiallelic variants. ",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:82537,Availability,fault,fault,82537,"Parameters:. key_expr (Expression) – Row-indexed expression for key associated to each row.; weight_expr (Float64Expression) – Row-indexed expression for row weights.; y (Float64Expression) – Column-indexed response expression.; If logistic is True, all non-missing values must evaluate to 0 or; 1. Note that a BooleanExpression will be implicitly converted; to a Float64Expression with this property.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – List of column-indexed covariate expressions.; logistic (bool or tuple of int and float) – If false, use the linear test. If true, use the logistic test with no; more than 25 logistic iterations and a convergence tolerance of 1e-6. If; a tuple is given, use the logistic test with the tuple elements as the; maximum nubmer of iterations and convergence tolerance, respectively.; max_size (int) – Maximum size of group on which to run the test.; accuracy (float) – Accuracy achieved by the Davies algorithm if fault value is zero.; iterations (int) – Maximum number of iterations attempted by the Davies algorithm. Returns:; Table – Table of SKAT results. hail.methods.lambda_gc(p_value, approximate=True)[source]; Compute genomic inflation factor (lambda GC) from an Expression of p-values. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. Parameters:. p_value (NumericExpression) – Row-indexed numeric expression of p-values.; approximate (bool) – If False, computes exact lambda GC (slower and uses more memory). Returns:; float – Genomic inflation factor (lambda genomic control). hail.methods.split_multi(ds, keep_star=False, left_aligned=False, *, permit_shuffle=False)[source]; Split multiallelic variants. Warning; In order to support a wide variety of data types, this function splits only; the variants on a MatrixTable, but not the genotypes. Use; split_multi_hts() if possible, or spl",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:84547,Availability,error,errors,84547," the entry modification methods: MatrixTable.annotate_entries(),; MatrixTable.select_entries(), MatrixTable.transmute_entries().; The resulting dataset will be keyed by the split locus and alleles.; split_multi() adds the following fields:. was_split (bool) – True if this variant was originally; multiallelic, otherwise False.; a_index (int) – The original index of this alternate allele in the; multiallelic representation (NB: 1 is the first alternate allele or the; only alternate allele in a biallelic variant). For example, 1:100:A:T,C; splits into two variants: 1:100:A:T with a_index = 1 and 1:100:A:C; with a_index = 2.; old_locus (locus) – The original, unsplit locus.; old_alleles (array<str>) – The original, unsplit alleles. All other fields are left unchanged. Warning; This method assumes ds contains at most one non-split variant per locus. This assumption permits the; most efficient implementation of the splitting algorithm. If your queries involving split_multi; crash with errors about out-of-order keys, this assumption may be violated. Otherwise, this; warning likely does not apply to your dataset.; If each locus in ds contains one multiallelic variant and one or more biallelic variants, you; can filter to the multiallelic variants, split those, and then combine the split variants with; the original biallelic variants.; For example, the following code splits a dataset mt which contains a mixture of split and; non-split variants.; >>> bi = mt.filter_rows(hl.len(mt.alleles) == 2); >>> bi = bi.annotate_rows(a_index=1, was_split=False, old_locus=bi.locus, old_alleles=bi.alleles); >>> multi = mt.filter_rows(hl.len(mt.alleles) > 2); >>> split = hl.split_multi(multi); >>> mt = split.union_rows(bi). Example; split_multi_hts(), which splits multiallelic variants for the HTS; genotype schema and updates the entry fields by downcoding the genotype, is; implemented as:; >>> sm = hl.split_multi(ds); >>> pl = hl.or_missing(; ... hl.is_defined(sm.PL),; ... (hl.range(0, 3).ma",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:85405,Availability,down,downcoding,85405," most one non-split variant per locus. This assumption permits the; most efficient implementation of the splitting algorithm. If your queries involving split_multi; crash with errors about out-of-order keys, this assumption may be violated. Otherwise, this; warning likely does not apply to your dataset.; If each locus in ds contains one multiallelic variant and one or more biallelic variants, you; can filter to the multiallelic variants, split those, and then combine the split variants with; the original biallelic variants.; For example, the following code splits a dataset mt which contains a mixture of split and; non-split variants.; >>> bi = mt.filter_rows(hl.len(mt.alleles) == 2); >>> bi = bi.annotate_rows(a_index=1, was_split=False, old_locus=bi.locus, old_alleles=bi.alleles); >>> multi = mt.filter_rows(hl.len(mt.alleles) > 2); >>> split = hl.split_multi(multi); >>> mt = split.union_rows(bi). Example; split_multi_hts(), which splits multiallelic variants for the HTS; genotype schema and updates the entry fields by downcoding the genotype, is; implemented as:; >>> sm = hl.split_multi(ds); >>> pl = hl.or_missing(; ... hl.is_defined(sm.PL),; ... (hl.range(0, 3).map(lambda i: hl.min(hl.range(0, hl.len(sm.PL)); ... .filter(lambda j: hl.downcode(hl.unphased_diploid_gt_index_call(j), sm.a_index) == hl.unphased_diploid_gt_index_call(i)); ... .map(lambda j: sm.PL[j]))))); >>> split_ds = sm.annotate_entries(; ... GT=hl.downcode(sm.GT, sm.a_index),; ... AD=hl.or_missing(hl.is_defined(sm.AD),; ... [hl.sum(sm.AD) - sm.AD[sm.a_index], sm.AD[sm.a_index]]),; ... DP=sm.DP,; ... PL=pl,; ... GQ=hl.gq_from_pl(pl)).drop('old_locus', 'old_alleles'). See also; split_multi_hts(). Parameters:. ds (MatrixTable or Table) – An unsplit dataset.; keep_star (bool) – Do not filter out * alleles.; left_aligned (bool) – If True, variants are assumed to be left aligned and have unique; loci. This avoids a shuffle. If the assumption is violated, an error; is generated.; permit_shuffle (bool) – If T",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:85626,Availability,down,downcode,85626,"ning likely does not apply to your dataset.; If each locus in ds contains one multiallelic variant and one or more biallelic variants, you; can filter to the multiallelic variants, split those, and then combine the split variants with; the original biallelic variants.; For example, the following code splits a dataset mt which contains a mixture of split and; non-split variants.; >>> bi = mt.filter_rows(hl.len(mt.alleles) == 2); >>> bi = bi.annotate_rows(a_index=1, was_split=False, old_locus=bi.locus, old_alleles=bi.alleles); >>> multi = mt.filter_rows(hl.len(mt.alleles) > 2); >>> split = hl.split_multi(multi); >>> mt = split.union_rows(bi). Example; split_multi_hts(), which splits multiallelic variants for the HTS; genotype schema and updates the entry fields by downcoding the genotype, is; implemented as:; >>> sm = hl.split_multi(ds); >>> pl = hl.or_missing(; ... hl.is_defined(sm.PL),; ... (hl.range(0, 3).map(lambda i: hl.min(hl.range(0, hl.len(sm.PL)); ... .filter(lambda j: hl.downcode(hl.unphased_diploid_gt_index_call(j), sm.a_index) == hl.unphased_diploid_gt_index_call(i)); ... .map(lambda j: sm.PL[j]))))); >>> split_ds = sm.annotate_entries(; ... GT=hl.downcode(sm.GT, sm.a_index),; ... AD=hl.or_missing(hl.is_defined(sm.AD),; ... [hl.sum(sm.AD) - sm.AD[sm.a_index], sm.AD[sm.a_index]]),; ... DP=sm.DP,; ... PL=pl,; ... GQ=hl.gq_from_pl(pl)).drop('old_locus', 'old_alleles'). See also; split_multi_hts(). Parameters:. ds (MatrixTable or Table) – An unsplit dataset.; keep_star (bool) – Do not filter out * alleles.; left_aligned (bool) – If True, variants are assumed to be left aligned and have unique; loci. This avoids a shuffle. If the assumption is violated, an error; is generated.; permit_shuffle (bool) – If True, permit a data shuffle to sort out-of-order split results.; This will only be required if input data has duplicate loci, one of; which contains more than one alternate allele. Returns:; MatrixTable or Table. hail.methods.split_multi_hts(ds, keep_star=False",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:85808,Availability,down,downcode,85808,"plit those, and then combine the split variants with; the original biallelic variants.; For example, the following code splits a dataset mt which contains a mixture of split and; non-split variants.; >>> bi = mt.filter_rows(hl.len(mt.alleles) == 2); >>> bi = bi.annotate_rows(a_index=1, was_split=False, old_locus=bi.locus, old_alleles=bi.alleles); >>> multi = mt.filter_rows(hl.len(mt.alleles) > 2); >>> split = hl.split_multi(multi); >>> mt = split.union_rows(bi). Example; split_multi_hts(), which splits multiallelic variants for the HTS; genotype schema and updates the entry fields by downcoding the genotype, is; implemented as:; >>> sm = hl.split_multi(ds); >>> pl = hl.or_missing(; ... hl.is_defined(sm.PL),; ... (hl.range(0, 3).map(lambda i: hl.min(hl.range(0, hl.len(sm.PL)); ... .filter(lambda j: hl.downcode(hl.unphased_diploid_gt_index_call(j), sm.a_index) == hl.unphased_diploid_gt_index_call(i)); ... .map(lambda j: sm.PL[j]))))); >>> split_ds = sm.annotate_entries(; ... GT=hl.downcode(sm.GT, sm.a_index),; ... AD=hl.or_missing(hl.is_defined(sm.AD),; ... [hl.sum(sm.AD) - sm.AD[sm.a_index], sm.AD[sm.a_index]]),; ... DP=sm.DP,; ... PL=pl,; ... GQ=hl.gq_from_pl(pl)).drop('old_locus', 'old_alleles'). See also; split_multi_hts(). Parameters:. ds (MatrixTable or Table) – An unsplit dataset.; keep_star (bool) – Do not filter out * alleles.; left_aligned (bool) – If True, variants are assumed to be left aligned and have unique; loci. This avoids a shuffle. If the assumption is violated, an error; is generated.; permit_shuffle (bool) – If True, permit a data shuffle to sort out-of-order split results.; This will only be required if input data has duplicate loci, one of; which contains more than one alternate allele. Returns:; MatrixTable or Table. hail.methods.split_multi_hts(ds, keep_star=False, left_aligned=False, vep_root='vep', *, permit_shuffle=False)[source]; Split multiallelic variants for datasets that contain one or more fields; from a standard high-throughput seq",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:86322,Availability,error,error,86322," splits multiallelic variants for the HTS; genotype schema and updates the entry fields by downcoding the genotype, is; implemented as:; >>> sm = hl.split_multi(ds); >>> pl = hl.or_missing(; ... hl.is_defined(sm.PL),; ... (hl.range(0, 3).map(lambda i: hl.min(hl.range(0, hl.len(sm.PL)); ... .filter(lambda j: hl.downcode(hl.unphased_diploid_gt_index_call(j), sm.a_index) == hl.unphased_diploid_gt_index_call(i)); ... .map(lambda j: sm.PL[j]))))); >>> split_ds = sm.annotate_entries(; ... GT=hl.downcode(sm.GT, sm.a_index),; ... AD=hl.or_missing(hl.is_defined(sm.AD),; ... [hl.sum(sm.AD) - sm.AD[sm.a_index], sm.AD[sm.a_index]]),; ... DP=sm.DP,; ... PL=pl,; ... GQ=hl.gq_from_pl(pl)).drop('old_locus', 'old_alleles'). See also; split_multi_hts(). Parameters:. ds (MatrixTable or Table) – An unsplit dataset.; keep_star (bool) – Do not filter out * alleles.; left_aligned (bool) – If True, variants are assumed to be left aligned and have unique; loci. This avoids a shuffle. If the assumption is violated, an error; is generated.; permit_shuffle (bool) – If True, permit a data shuffle to sort out-of-order split results.; This will only be required if input data has duplicate loci, one of; which contains more than one alternate allele. Returns:; MatrixTable or Table. hail.methods.split_multi_hts(ds, keep_star=False, left_aligned=False, vep_root='vep', *, permit_shuffle=False)[source]; Split multiallelic variants for datasets that contain one or more fields; from a standard high-throughput sequencing entry schema.; struct {; GT: call,; AD: array<int32>,; DP: int32,; GQ: int32,; PL: array<int32>,; PGT: call,; PID: str; }. For other entry fields, write your own splitting logic using; MatrixTable.annotate_entries().; Examples; >>> hl.split_multi_hts(dataset).write('output/split.mt'). Warning; This method assumes ds contains at most one non-split variant per locus. This assumption permits the; most efficient implementation of the splitting algorithm. If your queries involving split_multi_",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:87331,Availability,error,errors,87331,"erated.; permit_shuffle (bool) – If True, permit a data shuffle to sort out-of-order split results.; This will only be required if input data has duplicate loci, one of; which contains more than one alternate allele. Returns:; MatrixTable or Table. hail.methods.split_multi_hts(ds, keep_star=False, left_aligned=False, vep_root='vep', *, permit_shuffle=False)[source]; Split multiallelic variants for datasets that contain one or more fields; from a standard high-throughput sequencing entry schema.; struct {; GT: call,; AD: array<int32>,; DP: int32,; GQ: int32,; PL: array<int32>,; PGT: call,; PID: str; }. For other entry fields, write your own splitting logic using; MatrixTable.annotate_entries().; Examples; >>> hl.split_multi_hts(dataset).write('output/split.mt'). Warning; This method assumes ds contains at most one non-split variant per locus. This assumption permits the; most efficient implementation of the splitting algorithm. If your queries involving split_multi_hts; crash with errors about out-of-order keys, this assumption may be violated. Otherwise, this; warning likely does not apply to your dataset.; If each locus in ds contains one multiallelic variant and one or more biallelic variants, you; can filter to the multiallelic variants, split those, and then combine the split variants with; the original biallelic variants.; For example, the following code splits a dataset mt which contains a mixture of split and; non-split variants.; >>> bi = mt.filter_rows(hl.len(mt.alleles) == 2); >>> bi = bi.annotate_rows(a_index=1, was_split=False); >>> multi = mt.filter_rows(hl.len(mt.alleles) > 2); >>> split = hl.split_multi_hts(multi); >>> mt = split.union_rows(bi). Notes; We will explain by example. Consider a hypothetical 3-allelic; variant:; A C,T 0/2:7,2,6:15:45:99,50,99,0,45,99. split_multi_hts() will create two biallelic variants (one for each; alternate allele) at the same position; A C 0/0:13,2:15:45:0,45,99; A T 0/1:9,6:15:50:50,0,99. Each multiallelic GT or PGT ",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:88345,Availability,down,downcoded,88345,"t-of-order keys, this assumption may be violated. Otherwise, this; warning likely does not apply to your dataset.; If each locus in ds contains one multiallelic variant and one or more biallelic variants, you; can filter to the multiallelic variants, split those, and then combine the split variants with; the original biallelic variants.; For example, the following code splits a dataset mt which contains a mixture of split and; non-split variants.; >>> bi = mt.filter_rows(hl.len(mt.alleles) == 2); >>> bi = bi.annotate_rows(a_index=1, was_split=False); >>> multi = mt.filter_rows(hl.len(mt.alleles) > 2); >>> split = hl.split_multi_hts(multi); >>> mt = split.union_rows(bi). Notes; We will explain by example. Consider a hypothetical 3-allelic; variant:; A C,T 0/2:7,2,6:15:45:99,50,99,0,45,99. split_multi_hts() will create two biallelic variants (one for each; alternate allele) at the same position; A C 0/0:13,2:15:45:0,45,99; A T 0/1:9,6:15:50:50,0,99. Each multiallelic GT or PGT field is downcoded once for each alternate allele. A; call for an alternate allele maps to 1 in the biallelic variant; corresponding to itself and 0 otherwise. For example, in the example above,; 0/2 maps to 0/0 and 0/1. The genotype 1/2 maps to 0/1 and 0/1.; The biallelic alt AD entry is just the multiallelic AD entry; corresponding to the alternate allele. The ref AD entry is the sum of the; other multiallelic entries.; The biallelic DP is the same as the multiallelic DP.; The biallelic PL entry for a genotype g is the minimum over PL entries; for multiallelic genotypes that downcode to g. For example, the PL for (A,; T) at 0/1 is the minimum of the PLs for 0/1 (50) and 1/2 (45), and thus 45.; Fixing an alternate allele and biallelic variant, downcoding gives a map; from multiallelic to biallelic alleles and genotypes. The biallelic AD entry; for an allele is just the sum of the multiallelic AD entries for alleles; that map to that allele. Similarly, the biallelic PL entry for a genotype is; th",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:88920,Availability,down,downcode,88920,"a_index=1, was_split=False); >>> multi = mt.filter_rows(hl.len(mt.alleles) > 2); >>> split = hl.split_multi_hts(multi); >>> mt = split.union_rows(bi). Notes; We will explain by example. Consider a hypothetical 3-allelic; variant:; A C,T 0/2:7,2,6:15:45:99,50,99,0,45,99. split_multi_hts() will create two biallelic variants (one for each; alternate allele) at the same position; A C 0/0:13,2:15:45:0,45,99; A T 0/1:9,6:15:50:50,0,99. Each multiallelic GT or PGT field is downcoded once for each alternate allele. A; call for an alternate allele maps to 1 in the biallelic variant; corresponding to itself and 0 otherwise. For example, in the example above,; 0/2 maps to 0/0 and 0/1. The genotype 1/2 maps to 0/1 and 0/1.; The biallelic alt AD entry is just the multiallelic AD entry; corresponding to the alternate allele. The ref AD entry is the sum of the; other multiallelic entries.; The biallelic DP is the same as the multiallelic DP.; The biallelic PL entry for a genotype g is the minimum over PL entries; for multiallelic genotypes that downcode to g. For example, the PL for (A,; T) at 0/1 is the minimum of the PLs for 0/1 (50) and 1/2 (45), and thus 45.; Fixing an alternate allele and biallelic variant, downcoding gives a map; from multiallelic to biallelic alleles and genotypes. The biallelic AD entry; for an allele is just the sum of the multiallelic AD entries for alleles; that map to that allele. Similarly, the biallelic PL entry for a genotype is; the minimum over multiallelic PL entries for genotypes that map to that; genotype.; GQ is recomputed from PL if PL is provided and is not; missing. If not, it is copied from the original GQ.; Here is a second example for a het non-ref; A C,T 1/2:2,8,6:16:45:99,50,99,45,0,99. splits as; A C 0/1:8,8:16:45:45,0,99; A T 0/1:10,6:16:50:50,0,99. VCF Info Fields; Hail does not split fields in the info field. This means that if a; multiallelic site with info.AC value [10, 2] is split, each split; site will contain the same array [1",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:89091,Availability,down,downcoding,89091,"; A C,T 0/2:7,2,6:15:45:99,50,99,0,45,99. split_multi_hts() will create two biallelic variants (one for each; alternate allele) at the same position; A C 0/0:13,2:15:45:0,45,99; A T 0/1:9,6:15:50:50,0,99. Each multiallelic GT or PGT field is downcoded once for each alternate allele. A; call for an alternate allele maps to 1 in the biallelic variant; corresponding to itself and 0 otherwise. For example, in the example above,; 0/2 maps to 0/0 and 0/1. The genotype 1/2 maps to 0/1 and 0/1.; The biallelic alt AD entry is just the multiallelic AD entry; corresponding to the alternate allele. The ref AD entry is the sum of the; other multiallelic entries.; The biallelic DP is the same as the multiallelic DP.; The biallelic PL entry for a genotype g is the minimum over PL entries; for multiallelic genotypes that downcode to g. For example, the PL for (A,; T) at 0/1 is the minimum of the PLs for 0/1 (50) and 1/2 (45), and thus 45.; Fixing an alternate allele and biallelic variant, downcoding gives a map; from multiallelic to biallelic alleles and genotypes. The biallelic AD entry; for an allele is just the sum of the multiallelic AD entries for alleles; that map to that allele. Similarly, the biallelic PL entry for a genotype is; the minimum over multiallelic PL entries for genotypes that map to that; genotype.; GQ is recomputed from PL if PL is provided and is not; missing. If not, it is copied from the original GQ.; Here is a second example for a het non-ref; A C,T 1/2:2,8,6:16:45:99,50,99,45,0,99. splits as; A C 0/1:8,8:16:45:45,0,99; A T 0/1:10,6:16:50:50,0,99. VCF Info Fields; Hail does not split fields in the info field. This means that if a; multiallelic site with info.AC value [10, 2] is split, each split; site will contain the same array [10, 2]. The provided allele index; field a_index can be used to select the value corresponding to the split; allele’s position:; >>> split_ds = hl.split_multi_hts(dataset); >>> split_ds = split_ds.filter_rows(split_ds.info.AC[split",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:91595,Availability,error,error,91595,"mple:; >>> split_ds = hl.split_multi_hts(dataset); >>> split_ds = split_ds.annotate_rows(info = split_ds.info.annotate(AC = split_ds.info.AC[split_ds.a_index - 1])); >>> hl.export_vcf(split_ds, 'output/export.vcf') . The info field AC in data/export.vcf will have Number=1.; New Fields; split_multi_hts() adds the following fields:. was_split (bool) – True if this variant was originally; multiallelic, otherwise False.; a_index (int) – The original index of this alternate allele in the; multiallelic representation (NB: 1 is the first alternate allele or the; only alternate allele in a biallelic variant). For example, 1:100:A:T,C; splits into two variants: 1:100:A:T with a_index = 1 and 1:100:A:C; with a_index = 2. See also; split_multi(). Parameters:. ds (MatrixTable or Table) – An unsplit dataset.; keep_star (bool) – Do not filter out * alleles.; left_aligned (bool) – If True, variants are assumed to be left; aligned and have unique loci. This avoids a shuffle. If the assumption; is violated, an error is generated.; vep_root (str) – Top-level location of vep data. All variable-length VEP fields; (intergenic_consequences, motif_feature_consequences,; regulatory_feature_consequences, and transcript_consequences); will be split properly (i.e. a_index corresponding to the VEP allele_num).; permit_shuffle (bool) – If True, permit a data shuffle to sort out-of-order split results.; This will only be required if input data has duplicate loci, one of; which contains more than one alternate allele. Returns:; MatrixTable or Table – A biallelic variant dataset. hail.methods.summarize_variants(mt, show=True, *, handler=None)[source]; Summarize the variants present in a dataset and print the results.; Examples; >>> hl.summarize_variants(dataset) ; ==============================; Number of variants: 346; ==============================; Alleles per variant; -------------------; 2 alleles: 346 variants; ==============================; Variants per contig; -------------------; 20: 346",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:93307,Availability,down,down,93307,"amples; >>> hl.summarize_variants(dataset) ; ==============================; Number of variants: 346; ==============================; Alleles per variant; -------------------; 2 alleles: 346 variants; ==============================; Variants per contig; -------------------; 20: 346 variants; ==============================; Allele type distribution; ------------------------; SNP: 301 alleles; Deletion: 27 alleles; Insertion: 18 alleles; ==============================. Parameters:. mt (MatrixTable or Table) – Matrix table with a variant (locus / alleles) row key.; show (bool) – If True, print results instead of returning them.; handler. Notes; The result returned if show is False is a Struct with; five fields:. n_variants (int): Number of variants present in the matrix table.; allele_types (dict [str, int]): Number of alternate alleles in; each allele allele category.; contigs (dict [str, int]): Number of variants on each contig.; allele_counts (dict [int, int]): Number of variants broken down; by number of alleles (biallelic is 2, for example).; r_ti_tv (float): Ratio of transition alternate alleles to; transversion alternate alleles. Returns:; None or Struct – Returns None if show is True, or returns results as a struct. hail.methods.transmission_disequilibrium_test(dataset, pedigree)[source]; Performs the transmission disequilibrium test on trios. Note; Requires the column key to be one field of type tstr. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). Note; Requires the dataset to contain no multiallelic variants.; Use split_multi() or split_multi_hts() to split; multiallelic sites, or MatrixTable.filter_rows() to remove; them. Examples; Compute TDT association statistics and show the first two results:; >>> pedigree = hl.Pedigree.read('data/tdt_trios.fam'); >>> tdt_table = hl.transmission_disequilibrium_test(tdt_dataset, pedigree); >>> tdt_table.show(2) ; +---------------+------------+-------+-------+-",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:95698,Availability,error,errors,95698,"7262169 | [""T"",""C""] | NA | NA | NA | NA |; +---------------+------------+-------+-------+----------+----------+. Export variants with p-values below 0.001:; >>> tdt_table = tdt_table.filter(tdt_table.p_value < 0.001); >>> tdt_table.export(f""output/tdt_results.tsv""). Notes; The; transmission disequilibrium test; compares the number of times the alternate allele is transmitted (t) versus; not transmitted (u) from a heterozgyous parent to an affected child. The null; hypothesis holds that each case is equally likely. The TDT statistic is given by. \[(t - u)^2 \over (t + u)\]; and asymptotically follows a chi-squared distribution with one degree of; freedom under the null hypothesis.; transmission_disequilibrium_test() only considers complete trios (two; parents and a proband with defined sex) and only returns results for the; autosome, as defined by in_autosome(), and; chromosome X. Transmissions and non-transmissions are counted only for the; configurations of genotypes and copy state in the table below, in order to; filter out Mendel errors and configurations where transmission is; guaranteed. The copy state of a locus with respect to a trio is defined as; follows:. Auto – in autosome or in PAR of X or female child; HemiX – in non-PAR of X and male child. Here PAR is the pseudoautosomal region; of X and Y defined by ReferenceGenome, which many variant callers; map to chromosome X. Kid; Dad; Mom; Copy State; t; u. HomRef; Het; Het; Auto; 0; 2. HomRef; HomRef; Het; Auto; 0; 1. HomRef; Het; HomRef; Auto; 0; 1. Het; Het; Het; Auto; 1; 1. Het; HomRef; Het; Auto; 1; 0. Het; Het; HomRef; Auto; 1; 0. Het; HomVar; Het; Auto; 0; 1. Het; Het; HomVar; Auto; 0; 1. HomVar; Het; Het; Auto; 2; 0. HomVar; Het; HomVar; Auto; 1; 0. HomVar; HomVar; Het; Auto; 1; 0. HomRef; HomRef; Het; HemiX; 0; 1. HomRef; HomVar; Het; HemiX; 0; 1. HomVar; HomRef; Het; HemiX; 1; 0. HomVar; HomVar; Het; HemiX; 1; 0. transmission_disequilibrium_test() produces a table with the following columns:. locus (tl",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:99331,Availability,error,error,99331,"er_entry (tstruct) - Mother entry fields. Parameters:; pedigree (Pedigree). Returns:; MatrixTable. hail.methods.variant_qc(mt, name='variant_qc')[source]; Compute common variant statistics (quality control metrics). Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). Examples; >>> dataset_result = hl.variant_qc(dataset). Notes; This method computes variant statistics from the genotype data, returning; a new struct field name with the following metrics based on the fields; present in the entry schema.; If mt contains an entry field DP of type tint32, then the; field dp_stats is computed. If mt contains an entry field GQ of type; tint32, then the field gq_stats is computed. Both dp_stats; and gq_stats are structs with with four fields:. mean (float64) – Mean value.; stdev (float64) – Standard deviation (zero degrees of freedom).; min (int32) – Minimum value.; max (int32) – Maximum value. If the dataset does not contain an entry field GT of type; tcall, then an error is raised. The following fields are always; computed from GT:. AF (array<float64>) – Calculated allele frequency, one element; per allele, including the reference. Sums to one. Equivalent to; AC / AN.; AC (array<int32>) – Calculated allele count, one element per; allele, including the reference. Sums to AN.; AN (int32) – Total number of called alleles.; homozygote_count (array<int32>) – Number of homozygotes per; allele. One element per allele, including the reference.; call_rate (float64) – Fraction of calls neither missing nor filtered.; Equivalent to n_called / count_cols().; n_called (int64) – Number of samples with a defined GT.; n_not_called (int64) – Number of samples with a missing GT.; n_filtered (int64) – Number of filtered entries.; n_het (int64) – Number of heterozygous samples.; n_non_ref (int64) – Number of samples with at least one called; non-reference allele.; het_freq_hwe (float64) – Expected frequency of heterozygous; samples under ",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:804,Deployability,configurat,configuration,804,"﻿. Hail | ; Genetics. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Methods; Genetics. View page source. Genetics. VEPConfig(); Base class for configuring VEP. VEPConfigGRCh37Version85(*, data_bucket, ...); The Hail-maintained VEP configuration for GRCh37 for VEP version 85. VEPConfigGRCh38Version95(*, data_bucket, ...); The Hail-maintained VEP configuration for GRCh38 for VEP version 95. balding_nichols_model(n_populations, ...[, ...]); Generate a matrix table of variants, samples, and genotypes using the Balding-Nichols or Pritchard-Stephens-Donnelly model. concordance(left, right, *[, ...]); Calculate call concordance with another dataset. filter_intervals(ds, intervals[, keep]); Filter rows with a list of intervals. filter_alleles(mt, f); Filter alternate alleles. filter_alleles_hts(mt, f[, subset]); Filter alternate alleles and update standard GATK entry fields. hwe_normalized_pca(call_expr[, k, ...]); Run principal component analysis (PCA) on the Hardy-Weinberg-normalized genotype call matrix. genetic_relatedness_matrix(call_expr); Compute the genetic relatedness matrix (GRM). realized_relationship_matrix(call_expr); Computes the realized relationship matrix (RRM). impute_sex(call[, aaf_threshold, ...]); Impute sex of samples by calculating inbreeding coefficient on the X chromosome. ld_matrix(entry_expr, locus_expr, radius[, ...]); Computes the windowed correlation (linkage disequilibrium) matrix b",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:920,Deployability,configurat,configuration,920,"﻿. Hail | ; Genetics. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Methods; Genetics. View page source. Genetics. VEPConfig(); Base class for configuring VEP. VEPConfigGRCh37Version85(*, data_bucket, ...); The Hail-maintained VEP configuration for GRCh37 for VEP version 85. VEPConfigGRCh38Version95(*, data_bucket, ...); The Hail-maintained VEP configuration for GRCh38 for VEP version 95. balding_nichols_model(n_populations, ...[, ...]); Generate a matrix table of variants, samples, and genotypes using the Balding-Nichols or Pritchard-Stephens-Donnelly model. concordance(left, right, *[, ...]); Calculate call concordance with another dataset. filter_intervals(ds, intervals[, keep]); Filter rows with a list of intervals. filter_alleles(mt, f); Filter alternate alleles. filter_alleles_hts(mt, f[, subset]); Filter alternate alleles and update standard GATK entry fields. hwe_normalized_pca(call_expr[, k, ...]); Run principal component analysis (PCA) on the Hardy-Weinberg-normalized genotype call matrix. genetic_relatedness_matrix(call_expr); Compute the genetic relatedness matrix (GRM). realized_relationship_matrix(call_expr); Computes the realized relationship matrix (RRM). impute_sex(call[, aaf_threshold, ...]); Impute sex of samples by calculating inbreeding coefficient on the X chromosome. ld_matrix(entry_expr, locus_expr, radius[, ...]); Computes the windowed correlation (linkage disequilibrium) matrix b",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:1418,Deployability,update,update,1418,"ailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Methods; Genetics. View page source. Genetics. VEPConfig(); Base class for configuring VEP. VEPConfigGRCh37Version85(*, data_bucket, ...); The Hail-maintained VEP configuration for GRCh37 for VEP version 85. VEPConfigGRCh38Version95(*, data_bucket, ...); The Hail-maintained VEP configuration for GRCh38 for VEP version 95. balding_nichols_model(n_populations, ...[, ...]); Generate a matrix table of variants, samples, and genotypes using the Balding-Nichols or Pritchard-Stephens-Donnelly model. concordance(left, right, *[, ...]); Calculate call concordance with another dataset. filter_intervals(ds, intervals[, keep]); Filter rows with a list of intervals. filter_alleles(mt, f); Filter alternate alleles. filter_alleles_hts(mt, f[, subset]); Filter alternate alleles and update standard GATK entry fields. hwe_normalized_pca(call_expr[, k, ...]); Run principal component analysis (PCA) on the Hardy-Weinberg-normalized genotype call matrix. genetic_relatedness_matrix(call_expr); Compute the genetic relatedness matrix (GRM). realized_relationship_matrix(call_expr); Computes the realized relationship matrix (RRM). impute_sex(call[, aaf_threshold, ...]); Impute sex of samples by calculating inbreeding coefficient on the X chromosome. ld_matrix(entry_expr, locus_expr, radius[, ...]); Computes the windowed correlation (linkage disequilibrium) matrix between variants. ld_prune(call_expr[, r2, bp_window_size, ...]); Returns a maximal subset of variants that are nearly uncorrelated within each window. compute_charr(ds[, min_af, max_af, min_dp, ...]); Compute CHARR, the DNA sample contamination estimator. mendel_errors(call, pedigree); Find Mendel errors; count per variant, individual and nuclear family. de_novo(mt, pedigree, pop_frequency_prior, *",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:3818,Deployability,configurat,configuration,3818,"Compute genomic inflation factor (lambda GC) from an Expression of p-values. split_multi(ds[, keep_star, left_aligned, ...]); Split multiallelic variants. split_multi_hts(ds[, keep_star, ...]); Split multiallelic variants for datasets that contain one or more fields from a standard high-throughput sequencing entry schema. summarize_variants(mt[, show, handler]); Summarize the variants present in a dataset and print the results. transmission_disequilibrium_test(dataset, ...); Performs the transmission disequilibrium test on trios. trio_matrix(dataset, pedigree[, complete_trios]); Builds and returns a matrix where columns correspond to trios and entries contain genotypes for the trio. variant_qc(mt[, name]); Compute common variant statistics (quality control metrics). vep(dataset[, config, block_size, name, ...]); Annotate variants with VEP. class hail.methods.VEPConfig[source]; Base class for configuring VEP.; To define a custom VEP configuration to for Query on Batch, construct a new class that inherits from VEPConfig; and has the following parameters defined:. json_type (HailType): The type of the VEP JSON schema (as produced by VEP when invoked with the –json option).; data_bucket (str) – The location where the VEP data is stored.; data_mount (str) – The location in the container where the data should be mounted.; batch_run_command (list of str) – The command line to run for a VEP job for a partition.; batch_run_csq_header_command (list of str) – The command line to run when generating the consequence header.; env (dict of str to str) – A map of environment variables to values to add to the environment when invoking the command.; cloud (str) – The cloud where the Batch Service is located.; image (str) – The docker image to run VEP.; data_bucket_is_requester_pays (bool) – True if the data bucket is requester pays.; regions (list of str) – A list of regions the VEP jobs can run in. In addition, the method command must be defined with the following signature. The ou",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:6938,Deployability,configurat,configuration,6938,"lf.data_mount}/loftee_data/GERP_scores.final.sorted.txt.gz -o STDOUT; '''. The following environment variables are added to the job’s environment:. VEP_BLOCK_SIZE - The maximum number of variants provided as input to each invocation of VEP.; VEP_PART_ID - Partition ID.; VEP_DATA_MOUNT - Location where the vep data is mounted (same as data_mount in the config).; VEP_CONSEQUENCE - Integer equal to 0 or 1 on whether csq is False or True.; VEP_TOLERATE_PARSE_ERROR - Integer equal to 0 or 1 on whether tolerate_parse_error is False or True.; VEP_OUTPUT_FILE - String specifying the local path where the output TSV file with the VEP result should be located.; VEP_INPUT_FILE - String specifying the local path where the input VCF shard is located for all jobs. The VEP_INPUT_FILE environment variable is not available for the single job that computes the consequence header when; csq=True. class hail.methods.VEPConfigGRCh37Version85(*, data_bucket, data_mount, image, regions, cloud, data_bucket_is_requester_pays)[source]; The Hail-maintained VEP configuration for GRCh37 for VEP version 85.; This class takes the following constructor arguments:. data_bucket (str) – The location where the VEP data is stored.; data_mount (str) – The location in the container where the data should be mounted.; image (str) – The docker image to run VEP.; cloud (str) – The cloud where the Batch Service is located.; data_bucket_is_requester_pays (bool) – True if the data bucket is requester pays.; regions (list of str) – A list of regions the VEP jobs can run in. class hail.methods.VEPConfigGRCh38Version95(*, data_bucket, data_mount, image, regions, cloud, data_bucket_is_requester_pays)[source]; The Hail-maintained VEP configuration for GRCh38 for VEP version 95.; This class takes the following constructor arguments:. data_bucket (str) – The location where the VEP data is stored.; data_mount (str) – The location in the container where the data should be mounted.; image (str) – The docker image to run V",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:7602,Deployability,configurat,configuration,7602,"NPUT_FILE - String specifying the local path where the input VCF shard is located for all jobs. The VEP_INPUT_FILE environment variable is not available for the single job that computes the consequence header when; csq=True. class hail.methods.VEPConfigGRCh37Version85(*, data_bucket, data_mount, image, regions, cloud, data_bucket_is_requester_pays)[source]; The Hail-maintained VEP configuration for GRCh37 for VEP version 85.; This class takes the following constructor arguments:. data_bucket (str) – The location where the VEP data is stored.; data_mount (str) – The location in the container where the data should be mounted.; image (str) – The docker image to run VEP.; cloud (str) – The cloud where the Batch Service is located.; data_bucket_is_requester_pays (bool) – True if the data bucket is requester pays.; regions (list of str) – A list of regions the VEP jobs can run in. class hail.methods.VEPConfigGRCh38Version95(*, data_bucket, data_mount, image, regions, cloud, data_bucket_is_requester_pays)[source]; The Hail-maintained VEP configuration for GRCh38 for VEP version 95.; This class takes the following constructor arguments:. data_bucket (str) – The location where the VEP data is stored.; data_mount (str) – The location in the container where the data should be mounted.; image (str) – The docker image to run VEP.; cloud (str) – The cloud where the Batch Service is located.; data_bucket_is_requester_pays (bool) – True if the data bucket is set to requester pays.; regions (list of str) – A list of regions the VEP jobs can run in. hail.methods.balding_nichols_model(n_populations, n_samples, n_variants, n_partitions=None, pop_dist=None, fst=None, af_dist=None, reference_genome='default', mixture=False, *, phased=False)[source]; Generate a matrix table of variants, samples, and genotypes using the; Balding-Nichols or Pritchard-Stephens-Donnelly model.; Examples; Generate a matrix table of genotypes with 1000 variants and 100 samples; across 3 populations:; >>> hl.r",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:12521,Deployability,continuous,continuous,12521,"odels genotypes of individuals from a structured; population comprising \(K\) homogeneous modern populations that have; each diverged from a single ancestral population (a star phylogeny). Each; sample is assigned a population by sampling from the categorical; distribution \(\pi\). Note that the actual size of each population is; random.; Variants are modeled as biallelic and unlinked. Ancestral allele; frequencies are drawn independently for each variant from a frequency; spectrum \(P_0\). The extent of genetic drift of each modern population; from the ancestral population is defined by the corresponding \(F_{ST}\); parameter \(F_k\) (here and below, lowercase indices run over a range; bounded by the corresponding uppercase parameter, e.g. \(k = 1, \ldots,; K\)). For each variant and population, allele frequencies are drawn from a; beta distribution; whose parameters are determined by the ancestral allele frequency and; \(F_{ST}\) parameter. The beta distribution gives a continuous; approximation of the effect of genetic drift. We denote sample population; assignments by \(k_n\), ancestral allele frequencies by \(p_m\),; population allele frequencies by \(p_{k, m}\), and diploid, unphased; genotype calls by \(g_{n, m}\) (0, 1, and 2 correspond to homozygous; reference, heterozygous, and homozygous variant, respectively).; The generative model is then given by:. \[\begin{aligned}; k_n \,&\sim\, \pi \\; p_m \,&\sim\, P_0 \\; p_{k,m} \mid p_m\,&\sim\, \mathrm{Beta}(\mu = p_m,\, \sigma^2 = F_k p_m (1 - p_m)) \\; g_{n,m} \mid k_n, p_{k, m} \,&\sim\, \mathrm{Binomial}(2, p_{k_n, m}); \end{aligned}. \]; The beta distribution by its mean and variance above; the usual parameters; are \(a = (1 - p) \frac{1 - F}{F}\) and \(b = p \frac{1 - F}{F}\) with; \(F = F_k\) and \(p = p_m\).; The resulting dataset has the following fields.; Global fields:. bn.n_populations (tint32) – Number of populations.; bn.n_samples (tint32) – Number of samples.; bn.n_variants (tint32) – Number of v",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:23360,Deployability,update,update,23360,"w allele index to; the old allele index. Its length is the same as the modified alleles; field. If all alternate alleles of a variant are filtered out, the variant itself; is filtered out.; Using f; The f argument is a function or lambda evaluated per alternate allele to; determine whether that allele is kept. If f evaluates to True, the; allele is kept. If f evaluates to False or missing, the allele is; removed.; f is a function that takes two arguments: the allele string (of type; StringExpression) and the allele index (of type; Int32Expression), and returns a boolean expression. This can; be either a defined function or a lambda. For example, these two usages; are equivalent:; (with a lambda); >>> ds_result = hl.filter_alleles(ds, lambda allele, i: hl.is_snp(ds.alleles[0], allele)). (with a defined function); >>> def filter_f(allele, allele_index):; ... return hl.is_snp(ds.alleles[0], allele); >>> ds_result = hl.filter_alleles(ds, filter_f). Warning; filter_alleles() does not update any fields other than locus and; alleles. This means that row fields like allele count (AC) and entry; fields like allele depth (AD) can become meaningless unless they are also; updated. You can update them with annotate_rows() and; annotate_entries(). See also; filter_alleles_hts(). Parameters:. mt (MatrixTable) – Dataset.; f (callable) – Function from (allele: StringExpression, allele_index:; Int32Expression) to BooleanExpression. Returns:; MatrixTable. hail.methods.filter_alleles_hts(mt, f, subset=False)[source]; Filter alternate alleles and update standard GATK entry fields.; Examples; Filter to SNP alleles using the subset strategy:; >>> ds_result = hl.filter_alleles_hts(; ... ds,; ... lambda allele, _: hl.is_snp(ds.alleles[0], allele),; ... subset=True). Update the AC field of the resulting dataset:; >>> updated_info = ds_result.info.annotate(AC = ds_result.new_to_old.map(lambda i: ds_result.info.AC[i-1])); >>> ds_result = ds_result.annotate_rows(info = updated_info). Notes; Fo",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:23545,Deployability,update,updated,23545,"lleles of a variant are filtered out, the variant itself; is filtered out.; Using f; The f argument is a function or lambda evaluated per alternate allele to; determine whether that allele is kept. If f evaluates to True, the; allele is kept. If f evaluates to False or missing, the allele is; removed.; f is a function that takes two arguments: the allele string (of type; StringExpression) and the allele index (of type; Int32Expression), and returns a boolean expression. This can; be either a defined function or a lambda. For example, these two usages; are equivalent:; (with a lambda); >>> ds_result = hl.filter_alleles(ds, lambda allele, i: hl.is_snp(ds.alleles[0], allele)). (with a defined function); >>> def filter_f(allele, allele_index):; ... return hl.is_snp(ds.alleles[0], allele); >>> ds_result = hl.filter_alleles(ds, filter_f). Warning; filter_alleles() does not update any fields other than locus and; alleles. This means that row fields like allele count (AC) and entry; fields like allele depth (AD) can become meaningless unless they are also; updated. You can update them with annotate_rows() and; annotate_entries(). See also; filter_alleles_hts(). Parameters:. mt (MatrixTable) – Dataset.; f (callable) – Function from (allele: StringExpression, allele_index:; Int32Expression) to BooleanExpression. Returns:; MatrixTable. hail.methods.filter_alleles_hts(mt, f, subset=False)[source]; Filter alternate alleles and update standard GATK entry fields.; Examples; Filter to SNP alleles using the subset strategy:; >>> ds_result = hl.filter_alleles_hts(; ... ds,; ... lambda allele, _: hl.is_snp(ds.alleles[0], allele),; ... subset=True). Update the AC field of the resulting dataset:; >>> updated_info = ds_result.info.annotate(AC = ds_result.new_to_old.map(lambda i: ds_result.info.AC[i-1])); >>> ds_result = ds_result.annotate_rows(info = updated_info). Notes; For usage of the f argument, see the filter_alleles(); documentation.; filter_alleles_hts() requires the dataset have",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:23562,Deployability,update,update,23562,"unction or lambda evaluated per alternate allele to; determine whether that allele is kept. If f evaluates to True, the; allele is kept. If f evaluates to False or missing, the allele is; removed.; f is a function that takes two arguments: the allele string (of type; StringExpression) and the allele index (of type; Int32Expression), and returns a boolean expression. This can; be either a defined function or a lambda. For example, these two usages; are equivalent:; (with a lambda); >>> ds_result = hl.filter_alleles(ds, lambda allele, i: hl.is_snp(ds.alleles[0], allele)). (with a defined function); >>> def filter_f(allele, allele_index):; ... return hl.is_snp(ds.alleles[0], allele); >>> ds_result = hl.filter_alleles(ds, filter_f). Warning; filter_alleles() does not update any fields other than locus and; alleles. This means that row fields like allele count (AC) and entry; fields like allele depth (AD) can become meaningless unless they are also; updated. You can update them with annotate_rows() and; annotate_entries(). See also; filter_alleles_hts(). Parameters:. mt (MatrixTable) – Dataset.; f (callable) – Function from (allele: StringExpression, allele_index:; Int32Expression) to BooleanExpression. Returns:; MatrixTable. hail.methods.filter_alleles_hts(mt, f, subset=False)[source]; Filter alternate alleles and update standard GATK entry fields.; Examples; Filter to SNP alleles using the subset strategy:; >>> ds_result = hl.filter_alleles_hts(; ... ds,; ... lambda allele, _: hl.is_snp(ds.alleles[0], allele),; ... subset=True). Update the AC field of the resulting dataset:; >>> updated_info = ds_result.info.annotate(AC = ds_result.new_to_old.map(lambda i: ds_result.info.AC[i-1])); >>> ds_result = ds_result.annotate_rows(info = updated_info). Notes; For usage of the f argument, see the filter_alleles(); documentation.; filter_alleles_hts() requires the dataset have the GATK VCF schema,; namely the following entry fields in this order:; GT: call; AD: array<int32>; DP: ",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:23919,Deployability,update,update,23919," type; Int32Expression), and returns a boolean expression. This can; be either a defined function or a lambda. For example, these two usages; are equivalent:; (with a lambda); >>> ds_result = hl.filter_alleles(ds, lambda allele, i: hl.is_snp(ds.alleles[0], allele)). (with a defined function); >>> def filter_f(allele, allele_index):; ... return hl.is_snp(ds.alleles[0], allele); >>> ds_result = hl.filter_alleles(ds, filter_f). Warning; filter_alleles() does not update any fields other than locus and; alleles. This means that row fields like allele count (AC) and entry; fields like allele depth (AD) can become meaningless unless they are also; updated. You can update them with annotate_rows() and; annotate_entries(). See also; filter_alleles_hts(). Parameters:. mt (MatrixTable) – Dataset.; f (callable) – Function from (allele: StringExpression, allele_index:; Int32Expression) to BooleanExpression. Returns:; MatrixTable. hail.methods.filter_alleles_hts(mt, f, subset=False)[source]; Filter alternate alleles and update standard GATK entry fields.; Examples; Filter to SNP alleles using the subset strategy:; >>> ds_result = hl.filter_alleles_hts(; ... ds,; ... lambda allele, _: hl.is_snp(ds.alleles[0], allele),; ... subset=True). Update the AC field of the resulting dataset:; >>> updated_info = ds_result.info.annotate(AC = ds_result.new_to_old.map(lambda i: ds_result.info.AC[i-1])); >>> ds_result = ds_result.annotate_rows(info = updated_info). Notes; For usage of the f argument, see the filter_alleles(); documentation.; filter_alleles_hts() requires the dataset have the GATK VCF schema,; namely the following entry fields in this order:; GT: call; AD: array<int32>; DP: int32; GQ: int32; PL: array<int32>. Use MatrixTable.select_entries() to rearrange these fields if; necessary.; The following new fields are generated:. old_locus (locus) – The old locus, before filtering and computing; the minimal representation.; old_alleles (array<str>) – The old alleles, before filtering an",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:28003,Deployability,update,update,28003,"the AD and PL arrays; (i.e. removes entries corresponding to filtered alleles) and; then sets GT to the genotype with the minimum PL. Note that; if the genotype changes (as in the example), the PLs are; re-normalized (shifted) so that the most likely genotype has a; PL of 0. Qualitatively, subsetting corresponds to the belief; that the filtered alleles are not real so we should discard; any probability mass associated with them.; The subset algorithm would produce the following:; GT: 1/1; GQ: 980; AD: 0,50. 0 | 980; 1 | 980 0; +-----------; 0 1. In summary:. GT: Set to most likely genotype based on the PLs ignoring; the filtered allele(s).; AD: The filtered alleles’ columns are eliminated, e.g.,; filtering alleles 1 and 2 transforms 25,5,10,20 to; 25,20.; DP: Unchanged.; PL: Columns involving filtered alleles are eliminated and; the remaining columns’ values are shifted so the minimum; value is 0.; GQ: The second-lowest PL (after shifting). Warning; filter_alleles_hts() does not update any row fields other than; locus and alleles. This means that row fields like allele count (AC) can; become meaningless unless they are also updated. You can update them with; annotate_rows(). See also; filter_alleles(). Parameters:. mt (MatrixTable); f (callable) – Function from (allele: StringExpression, allele_index:; Int32Expression) to BooleanExpression; subset (bool) – Subset PL field if True, otherwise downcode PL field. The; calculation of GT and GQ also depend on whether one subsets or; downcodes the PL. Returns:; MatrixTable. hail.methods.hwe_normalized_pca(call_expr, k=10, compute_loadings=False)[source]; Run principal component analysis (PCA) on the Hardy-Weinberg-normalized; genotype call matrix.; Examples; >>> eigenvalues, scores, loadings = hl.hwe_normalized_pca(dataset.GT, k=5). Notes; This method specializes pca() for the common use case; of PCA in statistical genetics, that of projecting samples to a small; number of ancestry coordinates. Variants that are all homoz",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:28151,Deployability,update,updated,28151,"o the genotype with the minimum PL. Note that; if the genotype changes (as in the example), the PLs are; re-normalized (shifted) so that the most likely genotype has a; PL of 0. Qualitatively, subsetting corresponds to the belief; that the filtered alleles are not real so we should discard; any probability mass associated with them.; The subset algorithm would produce the following:; GT: 1/1; GQ: 980; AD: 0,50. 0 | 980; 1 | 980 0; +-----------; 0 1. In summary:. GT: Set to most likely genotype based on the PLs ignoring; the filtered allele(s).; AD: The filtered alleles’ columns are eliminated, e.g.,; filtering alleles 1 and 2 transforms 25,5,10,20 to; 25,20.; DP: Unchanged.; PL: Columns involving filtered alleles are eliminated and; the remaining columns’ values are shifted so the minimum; value is 0.; GQ: The second-lowest PL (after shifting). Warning; filter_alleles_hts() does not update any row fields other than; locus and alleles. This means that row fields like allele count (AC) can; become meaningless unless they are also updated. You can update them with; annotate_rows(). See also; filter_alleles(). Parameters:. mt (MatrixTable); f (callable) – Function from (allele: StringExpression, allele_index:; Int32Expression) to BooleanExpression; subset (bool) – Subset PL field if True, otherwise downcode PL field. The; calculation of GT and GQ also depend on whether one subsets or; downcodes the PL. Returns:; MatrixTable. hail.methods.hwe_normalized_pca(call_expr, k=10, compute_loadings=False)[source]; Run principal component analysis (PCA) on the Hardy-Weinberg-normalized; genotype call matrix.; Examples; >>> eigenvalues, scores, loadings = hl.hwe_normalized_pca(dataset.GT, k=5). Notes; This method specializes pca() for the common use case; of PCA in statistical genetics, that of projecting samples to a small; number of ancestry coordinates. Variants that are all homozygous reference; or all homozygous alternate are unnormalizable and removed before; evaluation. Se",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:28168,Deployability,update,update,28168,"s in the example), the PLs are; re-normalized (shifted) so that the most likely genotype has a; PL of 0. Qualitatively, subsetting corresponds to the belief; that the filtered alleles are not real so we should discard; any probability mass associated with them.; The subset algorithm would produce the following:; GT: 1/1; GQ: 980; AD: 0,50. 0 | 980; 1 | 980 0; +-----------; 0 1. In summary:. GT: Set to most likely genotype based on the PLs ignoring; the filtered allele(s).; AD: The filtered alleles’ columns are eliminated, e.g.,; filtering alleles 1 and 2 transforms 25,5,10,20 to; 25,20.; DP: Unchanged.; PL: Columns involving filtered alleles are eliminated and; the remaining columns’ values are shifted so the minimum; value is 0.; GQ: The second-lowest PL (after shifting). Warning; filter_alleles_hts() does not update any row fields other than; locus and alleles. This means that row fields like allele count (AC) can; become meaningless unless they are also updated. You can update them with; annotate_rows(). See also; filter_alleles(). Parameters:. mt (MatrixTable); f (callable) – Function from (allele: StringExpression, allele_index:; Int32Expression) to BooleanExpression; subset (bool) – Subset PL field if True, otherwise downcode PL field. The; calculation of GT and GQ also depend on whether one subsets or; downcodes the PL. Returns:; MatrixTable. hail.methods.hwe_normalized_pca(call_expr, k=10, compute_loadings=False)[source]; Run principal component analysis (PCA) on the Hardy-Weinberg-normalized; genotype call matrix.; Examples; >>> eigenvalues, scores, loadings = hl.hwe_normalized_pca(dataset.GT, k=5). Notes; This method specializes pca() for the common use case; of PCA in statistical genetics, that of projecting samples to a small; number of ancestry coordinates. Variants that are all homozygous reference; or all homozygous alternate are unnormalizable and removed before; evaluation. See pca() for more details.; Users of PLINK/GCTA should be aware that Hail c",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:54399,Deployability,configurat,configuration,54399,"ample_allele_frequency parameter is True,; then the computed allele frequency is not included in the calculation, and the; prior is the maximum of the pop_frequency_prior and 1 / 3e7.; proband (struct) – Proband column fields from mt.; father (struct) – Father column fields from mt.; mother (struct) – Mother column fields from mt.; proband_entry (struct) – Proband entry fields from mt.; father_entry (struct) – Father entry fields from mt.; proband_entry (struct) – Mother entry fields from mt.; is_female (bool) – True if proband is female.; p_de_novo (float64) – Unfiltered posterior probability; that the event is de novo rather than a missed heterozygous; event in a parent.; confidence (str) Validation confidence. One of: 'HIGH',; 'MEDIUM', 'LOW'. The key of the table is ['locus', 'alleles', 'id'].; The model looks for de novo events in which both parents are homozygous; reference and the proband is a heterozygous. The model makes the simplifying; assumption that when this configuration x = (AA, AA, AB) of calls; occurs, exactly one of the following is true:. d: a de novo mutation occurred in the proband and all calls are; accurate.; m: at least one parental allele is actually heterozygous and; the proband call is accurate. We can then estimate the posterior probability of a de novo mutation as:. \[\mathrm{P_{\text{de novo}}} = \frac{\mathrm{P}(d \mid x)}{\mathrm{P}(d \mid x) + \mathrm{P}(m \mid x)}\]; Applying Bayes rule to the numerator and denominator yields. \[\frac{\mathrm{P}(x \mid d)\,\mathrm{P}(d)}{\mathrm{P}(x \mid d)\,\mathrm{P}(d) +; \mathrm{P}(x \mid m)\,\mathrm{P}(m)}\]; The prior on de novo mutation is estimated from the rate in the literature:. \[\mathrm{P}(d) = \frac{1 \, \text{mutation}}{30{,}000{,}000 \, \text{bases}}\]; The prior used for at least one alternate allele between the parents; depends on the alternate allele frequency:. \[\mathrm{P}(m) = 1 - (1 - AF)^4\]; The likelihoods \(\mathrm{P}(x \mid d)\) and \(\mathrm{P}(x \mid m)\); are computed",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:59554,Deployability,configurat,configuration,59554,"alling.; min_p – Minimum posterior probability to be considered for de novo calling.; max_parent_ab – Maximum parent allele balance.; min_child_ab – Minimum proband allele balance/; min_dp_ratio – Minimum ratio between proband read depth and parental read depth.; ignore_in_sample_allele_frequency – Ignore in-sample allele frequency in computing site prior. Experimental. Returns:; Table. hail.methods.nirvana(dataset, config, block_size=500000, name='nirvana')[source]; Annotate variants using Nirvana. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). nirvana() runs Nirvana on the current dataset and adds a; new row field in the location specified by name.; Examples; Add Nirvana annotations to the dataset:; >>> result = hl.nirvana(dataset, ""data/nirvana.properties"") . Configuration; nirvana() requires a configuration file. The format is a; .properties file, where each; line defines a property as a key-value pair of the form key = value.; nirvana() supports the following properties:. hail.nirvana.dotnet – Location of dotnet. Optional, default: dotnet.; hail.nirvana.path – Value of the PATH environment variable when; invoking Nirvana. Optional, by default PATH is not set.; hail.nirvana.location – Location of Nirvana.dll. Required.; hail.nirvana.reference – Location of reference genome. Required.; hail.nirvana.cache – Location of cache. Required.; hail.nirvana.supplementaryAnnotationDirectory – Location of; Supplementary Database. Optional, no supplementary database by default. Here is an example nirvana.properties configuration file:; hail.nirvana.location = /path/to/dotnet/netcoreapp2.0/Nirvana.dll; hail.nirvana.reference = /path/to/nirvana/References/Homo_sapiens.GRCh37.Nirvana.dat; hail.nirvana.cache = /path/to/nirvana/Cache/GRCh37/Ensembl; hail.nirvana.supplementaryAnnotati",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:60277,Deployability,configurat,configuration,60277,"ay of tstr). nirvana() runs Nirvana on the current dataset and adds a; new row field in the location specified by name.; Examples; Add Nirvana annotations to the dataset:; >>> result = hl.nirvana(dataset, ""data/nirvana.properties"") . Configuration; nirvana() requires a configuration file. The format is a; .properties file, where each; line defines a property as a key-value pair of the form key = value.; nirvana() supports the following properties:. hail.nirvana.dotnet – Location of dotnet. Optional, default: dotnet.; hail.nirvana.path – Value of the PATH environment variable when; invoking Nirvana. Optional, by default PATH is not set.; hail.nirvana.location – Location of Nirvana.dll. Required.; hail.nirvana.reference – Location of reference genome. Required.; hail.nirvana.cache – Location of cache. Required.; hail.nirvana.supplementaryAnnotationDirectory – Location of; Supplementary Database. Optional, no supplementary database by default. Here is an example nirvana.properties configuration file:; hail.nirvana.location = /path/to/dotnet/netcoreapp2.0/Nirvana.dll; hail.nirvana.reference = /path/to/nirvana/References/Homo_sapiens.GRCh37.Nirvana.dat; hail.nirvana.cache = /path/to/nirvana/Cache/GRCh37/Ensembl; hail.nirvana.supplementaryAnnotationDirectory = /path/to/nirvana/SupplementaryDatabase/GRCh37. Annotations; A new row field is added in the location specified by name with the; following schema:; struct {; chromosome: str,; refAllele: str,; position: int32,; altAlleles: array<str>,; cytogeneticBand: str,; quality: float64,; filters: array<str>,; jointSomaticNormalQuality: int32,; copyNumber: int32,; strandBias: float64,; recalibratedQuality: float64,; variants: array<struct {; altAllele: str,; refAllele: str,; chromosome: str,; begin: int32,; end: int32,; phylopScore: float64,; isReferenceMinor: bool,; variantType: str,; vid: str,; hgvsg: str,; isRecomposedVariant: bool,; isDecomposedVariant: bool,; regulatoryRegions: array<struct {; id: str,; type: str,; conseque",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:65054,Deployability,configurat,configuration,65054,"nce: array<str>,; hgvsc: str,; hgvsp: str,; isCanonical: bool,; polyPhenScore: float64,; polyPhenPrediction: str,; proteinId: str,; proteinPos: str,; siftScore: float64,; siftPrediction: str; }>,; ensembl: array<struct {; transcript: str,; bioType: str,; aminoAcids: str,; cdnaPos: str,; codons: str,; cdsPos: str,; exons: str,; introns: str,; geneId: str,; hgnc: str,; consequence: array<str>,; hgvsc: str,; hgvsp: str,; isCanonical: bool,; polyPhenScore: float64,; polyPhenPrediction: str,; proteinId: str,; proteinPos: str,; siftScore: float64,; siftPrediction: str; }>; },; overlappingGenes: array<str>; }>; genes: array<struct {; name: str,; omim: array<struct {; mimNumber: int32,; hgnc: str,; description: str,; phenotypes: array<struct {; mimNumber: int32,; phenotype: str,; mapping: str,; inheritance: array<str>,; comments: str; }>; }>; exac: struct {; pLi: float64,; pRec: float64,; pNull: float64; }; }>; }. Parameters:. dataset (MatrixTable or Table) – Dataset.; config (str) – Path to Nirvana configuration file.; block_size (int) – Number of rows to process per Nirvana invocation.; name (str) – Name for resulting row field. Returns:; MatrixTable or Table – Dataset with new row-indexed field name containing Nirvana annotations. hail.methods.sample_qc(mt, name='sample_qc')[source]; Compute per-sample metrics useful for quality control. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). Examples; Compute sample QC metrics and remove low-quality samples:; >>> dataset = hl.sample_qc(dataset, name='sample_qc'); >>> filtered_dataset = dataset.filter_cols((dataset.sample_qc.dp_stats.mean > 20) & (dataset.sample_qc.r_ti_tv > 1.5)). Notes; This method computes summary statistics per sample from a genetic matrix and stores; the results as a new column-indexed struct field in the matrix, named based on the; name parameter.; If mt contains an entry field DP of type tint32, then the; field dp_stats is computed. If mt contai",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:73985,Deployability,integrat,integration,73985,"-+-------+----------+----------+-------+. The same test, but using the original paper’s suggested weights which are derived from the; allele frequency.; >>> mt = hl.variant_qc(mt); >>> skat = hl._logistic_skat(; ... mt.gene,; ... hl.dbeta(mt.variant_qc.AF[0], 1, 25),; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0]); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | 8.04e+00 | 3.50e-01 | 0 |; | 1 | 9 | 1.22e+00 | 5.04e-01 | 0 |; +-------+-------+----------+----------+-------+. Our simulated data was unweighted, so the null hypothesis appears true. In real datasets, we; expect the allele frequency to correlate with effect size.; Notice that, in the second group, the fault flag is set to 1. This indicates that the numerical; integration to calculate the p-value failed to achieve the required accuracy (by default,; 1e-6). In this particular case, the null hypothesis is likely true and the numerical integration; returned a (nonsensical) value greater than one.; The max_size parameter allows us to skip large genes that would cause “out of memory” errors:; >>> skat = hl._logistic_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0],; ... max_size=10); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | NA | NA | NA |; | 1 | 9 | 1.39e+02 | 1.82e-03 | 0 |; +-------+-------+----------+----------+-------+. Notes; In the SKAT R package, the “weights” are actually the square root of the weight expression; from the paper. This method uses the definition from the paper.; The paper inclu",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:74161,Deployability,integrat,integration,74161,"ele frequency.; >>> mt = hl.variant_qc(mt); >>> skat = hl._logistic_skat(; ... mt.gene,; ... hl.dbeta(mt.variant_qc.AF[0], 1, 25),; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0]); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | 8.04e+00 | 3.50e-01 | 0 |; | 1 | 9 | 1.22e+00 | 5.04e-01 | 0 |; +-------+-------+----------+----------+-------+. Our simulated data was unweighted, so the null hypothesis appears true. In real datasets, we; expect the allele frequency to correlate with effect size.; Notice that, in the second group, the fault flag is set to 1. This indicates that the numerical; integration to calculate the p-value failed to achieve the required accuracy (by default,; 1e-6). In this particular case, the null hypothesis is likely true and the numerical integration; returned a (nonsensical) value greater than one.; The max_size parameter allows us to skip large genes that would cause “out of memory” errors:; >>> skat = hl._logistic_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0],; ... max_size=10); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | NA | NA | NA |; | 1 | 9 | 1.39e+02 | 1.82e-03 | 0 |; +-------+-------+----------+----------+-------+. Notes; In the SKAT R package, the “weights” are actually the square root of the weight expression; from the paper. This method uses the definition from the paper.; The paper includes an explicit intercept term but this method expects the user to specify the; intercept as an extra covariate with the value 1.; This ",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:81472,Deployability,integrat,integration,81472,"4.122; 0.192; 0. Groups larger than max_size appear with missing q_stat, p_value, and; fault. The hard limit on the number of rows in a group is 46340.; Note that the variance component score q_stat agrees with Q in the R; package skat, but both differ from \(Q\) in the paper by the factor; \(\frac{1}{2\sigma^2}\) in the linear case and \(\frac{1}{2}\) in; the logistic case, where \(\sigma^2\) is the unbiased estimator of; residual variance for the linear null model. The R package also applies a; “small-sample adjustment” to the null distribution in the logistic case; when the sample size is less than 2000. Hail does not apply this; adjustment.; The fault flag is an integer indicating whether any issues occurred when; running the Davies algorithm to compute the p-value as the right tail of a; weighted sum of \(\chi^2(1)\) distributions. fault value; Description. 0; no issues. 1; accuracy NOT achieved. 2; round-off error possibly significant. 3; invalid parameters. 4; unable to locate integration parameters. 5; out of memory. Parameters:. key_expr (Expression) – Row-indexed expression for key associated to each row.; weight_expr (Float64Expression) – Row-indexed expression for row weights.; y (Float64Expression) – Column-indexed response expression.; If logistic is True, all non-missing values must evaluate to 0 or; 1. Note that a BooleanExpression will be implicitly converted; to a Float64Expression with this property.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – List of column-indexed covariate expressions.; logistic (bool or tuple of int and float) – If false, use the linear test. If true, use the logistic test with no; more than 25 logistic iterations and a convergence tolerance of 1e-6. If; a tuple is given, use the logistic test with the tuple elements as the; maximum nubmer of iterations and convergence tolerance, respectively.; max_size (int) – Maximum size of group on which to run the test.; ",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:85377,Deployability,update,updates,85377," most one non-split variant per locus. This assumption permits the; most efficient implementation of the splitting algorithm. If your queries involving split_multi; crash with errors about out-of-order keys, this assumption may be violated. Otherwise, this; warning likely does not apply to your dataset.; If each locus in ds contains one multiallelic variant and one or more biallelic variants, you; can filter to the multiallelic variants, split those, and then combine the split variants with; the original biallelic variants.; For example, the following code splits a dataset mt which contains a mixture of split and; non-split variants.; >>> bi = mt.filter_rows(hl.len(mt.alleles) == 2); >>> bi = bi.annotate_rows(a_index=1, was_split=False, old_locus=bi.locus, old_alleles=bi.alleles); >>> multi = mt.filter_rows(hl.len(mt.alleles) > 2); >>> split = hl.split_multi(multi); >>> mt = split.union_rows(bi). Example; split_multi_hts(), which splits multiallelic variants for the HTS; genotype schema and updates the entry fields by downcoding the genotype, is; implemented as:; >>> sm = hl.split_multi(ds); >>> pl = hl.or_missing(; ... hl.is_defined(sm.PL),; ... (hl.range(0, 3).map(lambda i: hl.min(hl.range(0, hl.len(sm.PL)); ... .filter(lambda j: hl.downcode(hl.unphased_diploid_gt_index_call(j), sm.a_index) == hl.unphased_diploid_gt_index_call(i)); ... .map(lambda j: sm.PL[j]))))); >>> split_ds = sm.annotate_entries(; ... GT=hl.downcode(sm.GT, sm.a_index),; ... AD=hl.or_missing(hl.is_defined(sm.AD),; ... [hl.sum(sm.AD) - sm.AD[sm.a_index], sm.AD[sm.a_index]]),; ... DP=sm.DP,; ... PL=pl,; ... GQ=hl.gq_from_pl(pl)).drop('old_locus', 'old_alleles'). See also; split_multi_hts(). Parameters:. ds (MatrixTable or Table) – An unsplit dataset.; keep_star (bool) – Do not filter out * alleles.; left_aligned (bool) – If True, variants are assumed to be left aligned and have unique; loci. This avoids a shuffle. If the assumption is violated, an error; is generated.; permit_shuffle (bool) – If T",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:95604,Deployability,configurat,configurations,95604,"7262169 | [""T"",""C""] | NA | NA | NA | NA |; +---------------+------------+-------+-------+----------+----------+. Export variants with p-values below 0.001:; >>> tdt_table = tdt_table.filter(tdt_table.p_value < 0.001); >>> tdt_table.export(f""output/tdt_results.tsv""). Notes; The; transmission disequilibrium test; compares the number of times the alternate allele is transmitted (t) versus; not transmitted (u) from a heterozgyous parent to an affected child. The null; hypothesis holds that each case is equally likely. The TDT statistic is given by. \[(t - u)^2 \over (t + u)\]; and asymptotically follows a chi-squared distribution with one degree of; freedom under the null hypothesis.; transmission_disequilibrium_test() only considers complete trios (two; parents and a proband with defined sex) and only returns results for the; autosome, as defined by in_autosome(), and; chromosome X. Transmissions and non-transmissions are counted only for the; configurations of genotypes and copy state in the table below, in order to; filter out Mendel errors and configurations where transmission is; guaranteed. The copy state of a locus with respect to a trio is defined as; follows:. Auto – in autosome or in PAR of X or female child; HemiX – in non-PAR of X and male child. Here PAR is the pseudoautosomal region; of X and Y defined by ReferenceGenome, which many variant callers; map to chromosome X. Kid; Dad; Mom; Copy State; t; u. HomRef; Het; Het; Auto; 0; 2. HomRef; HomRef; Het; Auto; 0; 1. HomRef; Het; HomRef; Auto; 0; 1. Het; Het; Het; Auto; 1; 1. Het; HomRef; Het; Auto; 1; 0. Het; Het; HomRef; Auto; 1; 0. Het; HomVar; Het; Auto; 0; 1. Het; Het; HomVar; Auto; 0; 1. HomVar; Het; Het; Auto; 2; 0. HomVar; Het; HomVar; Auto; 1; 0. HomVar; HomVar; Het; Auto; 1; 0. HomRef; HomRef; Het; HemiX; 0; 1. HomRef; HomVar; Het; HemiX; 0; 1. HomVar; HomRef; Het; HemiX; 1; 0. HomVar; HomVar; Het; HemiX; 1; 0. transmission_disequilibrium_test() produces a table with the following columns:. locus (tl",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:95709,Deployability,configurat,configurations,95709,"7262169 | [""T"",""C""] | NA | NA | NA | NA |; +---------------+------------+-------+-------+----------+----------+. Export variants with p-values below 0.001:; >>> tdt_table = tdt_table.filter(tdt_table.p_value < 0.001); >>> tdt_table.export(f""output/tdt_results.tsv""). Notes; The; transmission disequilibrium test; compares the number of times the alternate allele is transmitted (t) versus; not transmitted (u) from a heterozgyous parent to an affected child. The null; hypothesis holds that each case is equally likely. The TDT statistic is given by. \[(t - u)^2 \over (t + u)\]; and asymptotically follows a chi-squared distribution with one degree of; freedom under the null hypothesis.; transmission_disequilibrium_test() only considers complete trios (two; parents and a proband with defined sex) and only returns results for the; autosome, as defined by in_autosome(), and; chromosome X. Transmissions and non-transmissions are counted only for the; configurations of genotypes and copy state in the table below, in order to; filter out Mendel errors and configurations where transmission is; guaranteed. The copy state of a locus with respect to a trio is defined as; follows:. Auto – in autosome or in PAR of X or female child; HemiX – in non-PAR of X and male child. Here PAR is the pseudoautosomal region; of X and Y defined by ReferenceGenome, which many variant callers; map to chromosome X. Kid; Dad; Mom; Copy State; t; u. HomRef; Het; Het; Auto; 0; 2. HomRef; HomRef; Het; Auto; 0; 1. HomRef; Het; HomRef; Auto; 0; 1. Het; Het; Het; Auto; 1; 1. Het; HomRef; Het; Auto; 1; 0. Het; Het; HomRef; Auto; 1; 0. Het; HomVar; Het; Auto; 0; 1. Het; Het; HomVar; Auto; 0; 1. HomVar; Het; Het; Auto; 2; 0. HomVar; Het; HomVar; Auto; 1; 0. HomVar; HomVar; Het; Auto; 1; 0. HomRef; HomRef; Het; HemiX; 0; 1. HomRef; HomVar; Het; HemiX; 0; 1. HomVar; HomRef; Het; HemiX; 1; 0. HomVar; HomVar; Het; HemiX; 1; 0. transmission_disequilibrium_test() produces a table with the following columns:. locus (tl",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:101590,Deployability,configurat,configuration,101590,"einberg equilibrium for excess heterozygosity.; See functions.hardy_weinberg_test() for details. Warning; het_freq_hwe and p_value_hwe are calculated as in; functions.hardy_weinberg_test(), with non-diploid calls; (ploidy != 2) ignored in the counts. As this test is only; statistically rigorous in the biallelic setting, variant_qc(); sets both fields to missing for multiallelic variants. Consider using; split_multi() to split multi-allelic variants beforehand. Parameters:. mt (MatrixTable) – Dataset.; name (str) – Name for resulting field. Returns:; MatrixTable. hail.methods.vep(dataset, config=None, block_size=1000, name='vep', csq=False, tolerate_parse_error=False)[source]; Annotate variants with VEP. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). vep() runs Variant Effect Predictor on the; current dataset and adds the result as a row field.; Examples; Add VEP annotations to the dataset:; >>> result = hl.vep(dataset, ""data/vep-configuration.json"") . Notes; Installation; This VEP command only works if you have already installed VEP on your; computing environment. If you use hailctl dataproc to start Hail clusters,; installing VEP is achieved by specifying the –vep flag. For more detailed instructions,; see Variant Effect Predictor (VEP). If you use hailctl hdinsight, see Variant Effect Predictor (VEP).; Spark Configuration; vep() needs a configuration file to tell it how to run VEP. This is the config argument; to the VEP function. If you are using hailctl dataproc as mentioned above, you can just use the; default argument for config and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below.; The format of the configuration file is JSON, and vep(); expects a JSON object with three fields:. command (array of string) – The VEP command line to run. The string literal __OUTPUT_FORMAT_FLAG__ is replaced with –json or –vcf depending on csq.; env (ob",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:101682,Deployability,install,installed,101682,"for details. Warning; het_freq_hwe and p_value_hwe are calculated as in; functions.hardy_weinberg_test(), with non-diploid calls; (ploidy != 2) ignored in the counts. As this test is only; statistically rigorous in the biallelic setting, variant_qc(); sets both fields to missing for multiallelic variants. Consider using; split_multi() to split multi-allelic variants beforehand. Parameters:. mt (MatrixTable) – Dataset.; name (str) – Name for resulting field. Returns:; MatrixTable. hail.methods.vep(dataset, config=None, block_size=1000, name='vep', csq=False, tolerate_parse_error=False)[source]; Annotate variants with VEP. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). vep() runs Variant Effect Predictor on the; current dataset and adds the result as a row field.; Examples; Add VEP annotations to the dataset:; >>> result = hl.vep(dataset, ""data/vep-configuration.json"") . Notes; Installation; This VEP command only works if you have already installed VEP on your; computing environment. If you use hailctl dataproc to start Hail clusters,; installing VEP is achieved by specifying the –vep flag. For more detailed instructions,; see Variant Effect Predictor (VEP). If you use hailctl hdinsight, see Variant Effect Predictor (VEP).; Spark Configuration; vep() needs a configuration file to tell it how to run VEP. This is the config argument; to the VEP function. If you are using hailctl dataproc as mentioned above, you can just use the; default argument for config and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below.; The format of the configuration file is JSON, and vep(); expects a JSON object with three fields:. command (array of string) – The VEP command line to run. The string literal __OUTPUT_FORMAT_FLAG__ is replaced with –json or –vcf depending on csq.; env (object) – A map of environment variables to values to add to the environment when invo",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:101781,Deployability,install,installing,101781,"on-diploid calls; (ploidy != 2) ignored in the counts. As this test is only; statistically rigorous in the biallelic setting, variant_qc(); sets both fields to missing for multiallelic variants. Consider using; split_multi() to split multi-allelic variants beforehand. Parameters:. mt (MatrixTable) – Dataset.; name (str) – Name for resulting field. Returns:; MatrixTable. hail.methods.vep(dataset, config=None, block_size=1000, name='vep', csq=False, tolerate_parse_error=False)[source]; Annotate variants with VEP. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). vep() runs Variant Effect Predictor on the; current dataset and adds the result as a row field.; Examples; Add VEP annotations to the dataset:; >>> result = hl.vep(dataset, ""data/vep-configuration.json"") . Notes; Installation; This VEP command only works if you have already installed VEP on your; computing environment. If you use hailctl dataproc to start Hail clusters,; installing VEP is achieved by specifying the –vep flag. For more detailed instructions,; see Variant Effect Predictor (VEP). If you use hailctl hdinsight, see Variant Effect Predictor (VEP).; Spark Configuration; vep() needs a configuration file to tell it how to run VEP. This is the config argument; to the VEP function. If you are using hailctl dataproc as mentioned above, you can just use the; default argument for config and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below.; The format of the configuration file is JSON, and vep(); expects a JSON object with three fields:. command (array of string) – The VEP command line to run. The string literal __OUTPUT_FORMAT_FLAG__ is replaced with –json or –vcf depending on csq.; env (object) – A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; vep_json_schema (string): The type of the V",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:102008,Deployability,configurat,configuration,102008,"it multi-allelic variants beforehand. Parameters:. mt (MatrixTable) – Dataset.; name (str) – Name for resulting field. Returns:; MatrixTable. hail.methods.vep(dataset, config=None, block_size=1000, name='vep', csq=False, tolerate_parse_error=False)[source]; Annotate variants with VEP. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). vep() runs Variant Effect Predictor on the; current dataset and adds the result as a row field.; Examples; Add VEP annotations to the dataset:; >>> result = hl.vep(dataset, ""data/vep-configuration.json"") . Notes; Installation; This VEP command only works if you have already installed VEP on your; computing environment. If you use hailctl dataproc to start Hail clusters,; installing VEP is achieved by specifying the –vep flag. For more detailed instructions,; see Variant Effect Predictor (VEP). If you use hailctl hdinsight, see Variant Effect Predictor (VEP).; Spark Configuration; vep() needs a configuration file to tell it how to run VEP. This is the config argument; to the VEP function. If you are using hailctl dataproc as mentioned above, you can just use the; default argument for config and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below.; The format of the configuration file is JSON, and vep(); expects a JSON object with three fields:. command (array of string) – The VEP command line to run. The string literal __OUTPUT_FORMAT_FLAG__ is replaced with –json or –vcf depending on csq.; env (object) – A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; vep_json_schema (string): The type of the VEP JSON schema (as produced by the VEP when invoked with the –json option). Note: This is the old-style ‘parseable’ Hail type syntax. This will change. Here is an example configuration file for invoking VEP release 85; installed i",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:102349,Deployability,configurat,configuration,102349,"eles (type tarray of tstr). vep() runs Variant Effect Predictor on the; current dataset and adds the result as a row field.; Examples; Add VEP annotations to the dataset:; >>> result = hl.vep(dataset, ""data/vep-configuration.json"") . Notes; Installation; This VEP command only works if you have already installed VEP on your; computing environment. If you use hailctl dataproc to start Hail clusters,; installing VEP is achieved by specifying the –vep flag. For more detailed instructions,; see Variant Effect Predictor (VEP). If you use hailctl hdinsight, see Variant Effect Predictor (VEP).; Spark Configuration; vep() needs a configuration file to tell it how to run VEP. This is the config argument; to the VEP function. If you are using hailctl dataproc as mentioned above, you can just use the; default argument for config and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below.; The format of the configuration file is JSON, and vep(); expects a JSON object with three fields:. command (array of string) – The VEP command line to run. The string literal __OUTPUT_FORMAT_FLAG__ is replaced with –json or –vcf depending on csq.; env (object) – A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; vep_json_schema (string): The type of the VEP JSON schema (as produced by the VEP when invoked with the –json option). Note: This is the old-style ‘parseable’ Hail type syntax. This will change. Here is an example configuration file for invoking VEP release 85; installed in /vep with the Loftee plugin:; {; ""command"": [; ""/vep"",; ""--format"", ""vcf"",; ""__OUTPUT_FORMAT_FLAG__"",; ""--everything"",; ""--allele_number"",; ""--no_stats"",; ""--cache"", ""--offline"",; ""--minimal"",; ""--assembly"", ""GRCh37"",; ""--plugin"", ""LoF,human_ancestor_fa:/root/.vep/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:/root/.ve",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:102953,Deployability,configurat,configuration,102953,"If you are using hailctl dataproc as mentioned above, you can just use the; default argument for config and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below.; The format of the configuration file is JSON, and vep(); expects a JSON object with three fields:. command (array of string) – The VEP command line to run. The string literal __OUTPUT_FORMAT_FLAG__ is replaced with –json or –vcf depending on csq.; env (object) – A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; vep_json_schema (string): The type of the VEP JSON schema (as produced by the VEP when invoked with the –json option). Note: This is the old-style ‘parseable’ Hail type syntax. This will change. Here is an example configuration file for invoking VEP release 85; installed in /vep with the Loftee plugin:; {; ""command"": [; ""/vep"",; ""--format"", ""vcf"",; ""__OUTPUT_FORMAT_FLAG__"",; ""--everything"",; ""--allele_number"",; ""--no_stats"",; ""--cache"", ""--offline"",; ""--minimal"",; ""--assembly"", ""GRCh37"",; ""--plugin"", ""LoF,human_ancestor_fa:/root/.vep/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:/root/.vep/loftee_data/phylocsf_gerp.sql,gerp_file:/root/.vep/loftee_data/GERP_scores.final.sorted.txt.gz"",; ""-o"", ""STDOUT""; ],; ""env"": {; ""PERL5LIB"": ""/vep_data/loftee""; },; ""vep_json_schema"": ""Struct{assembly_name:String,allele_string:String,ancestral:String,colocated_variants:Array[Struct{aa_allele:String,aa_maf:Float64,afr_allele:String,afr_maf:Float64,allele_string:String,amr_allele:String,amr_maf:Float64,clin_sig:Array[String],end:Int32,eas_allele:String,eas_maf:Float64,ea_allele:String,ea_maf:Float64,eur_allele:String,eur_maf:Float64,exac_adj_allele:String,exac_adj_maf:Float64,exac_allele:String,exac_afr_allele:String,exac_afr_maf:Float64,exac_amr_allele:String,exac_amr_maf:Float64,exac_eas_allele:String,exac_eas_maf:",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:102989,Deployability,release,release,102989,"If you are using hailctl dataproc as mentioned above, you can just use the; default argument for config and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below.; The format of the configuration file is JSON, and vep(); expects a JSON object with three fields:. command (array of string) – The VEP command line to run. The string literal __OUTPUT_FORMAT_FLAG__ is replaced with –json or –vcf depending on csq.; env (object) – A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; vep_json_schema (string): The type of the VEP JSON schema (as produced by the VEP when invoked with the –json option). Note: This is the old-style ‘parseable’ Hail type syntax. This will change. Here is an example configuration file for invoking VEP release 85; installed in /vep with the Loftee plugin:; {; ""command"": [; ""/vep"",; ""--format"", ""vcf"",; ""__OUTPUT_FORMAT_FLAG__"",; ""--everything"",; ""--allele_number"",; ""--no_stats"",; ""--cache"", ""--offline"",; ""--minimal"",; ""--assembly"", ""GRCh37"",; ""--plugin"", ""LoF,human_ancestor_fa:/root/.vep/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:/root/.vep/loftee_data/phylocsf_gerp.sql,gerp_file:/root/.vep/loftee_data/GERP_scores.final.sorted.txt.gz"",; ""-o"", ""STDOUT""; ],; ""env"": {; ""PERL5LIB"": ""/vep_data/loftee""; },; ""vep_json_schema"": ""Struct{assembly_name:String,allele_string:String,ancestral:String,colocated_variants:Array[Struct{aa_allele:String,aa_maf:Float64,afr_allele:String,afr_maf:Float64,allele_string:String,amr_allele:String,amr_maf:Float64,clin_sig:Array[String],end:Int32,eas_allele:String,eas_maf:Float64,ea_allele:String,ea_maf:Float64,eur_allele:String,eur_maf:Float64,exac_adj_allele:String,exac_adj_maf:Float64,exac_allele:String,exac_afr_allele:String,exac_afr_maf:Float64,exac_amr_allele:String,exac_amr_maf:Float64,exac_eas_allele:String,exac_eas_maf:",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:103001,Deployability,install,installed,103001,"If you are using hailctl dataproc as mentioned above, you can just use the; default argument for config and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below.; The format of the configuration file is JSON, and vep(); expects a JSON object with three fields:. command (array of string) – The VEP command line to run. The string literal __OUTPUT_FORMAT_FLAG__ is replaced with –json or –vcf depending on csq.; env (object) – A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; vep_json_schema (string): The type of the VEP JSON schema (as produced by the VEP when invoked with the –json option). Note: This is the old-style ‘parseable’ Hail type syntax. This will change. Here is an example configuration file for invoking VEP release 85; installed in /vep with the Loftee plugin:; {; ""command"": [; ""/vep"",; ""--format"", ""vcf"",; ""__OUTPUT_FORMAT_FLAG__"",; ""--everything"",; ""--allele_number"",; ""--no_stats"",; ""--cache"", ""--offline"",; ""--minimal"",; ""--assembly"", ""GRCh37"",; ""--plugin"", ""LoF,human_ancestor_fa:/root/.vep/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:/root/.vep/loftee_data/phylocsf_gerp.sql,gerp_file:/root/.vep/loftee_data/GERP_scores.final.sorted.txt.gz"",; ""-o"", ""STDOUT""; ],; ""env"": {; ""PERL5LIB"": ""/vep_data/loftee""; },; ""vep_json_schema"": ""Struct{assembly_name:String,allele_string:String,ancestral:String,colocated_variants:Array[Struct{aa_allele:String,aa_maf:Float64,afr_allele:String,afr_maf:Float64,allele_string:String,amr_allele:String,amr_maf:Float64,clin_sig:Array[String],end:Int32,eas_allele:String,eas_maf:Float64,ea_allele:String,ea_maf:Float64,eur_allele:String,eur_maf:Float64,exac_adj_allele:String,exac_adj_maf:Float64,exac_allele:String,exac_afr_allele:String,exac_afr_maf:Float64,exac_amr_allele:String,exac_amr_maf:Float64,exac_eas_allele:String,exac_eas_maf:",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:106024,Deployability,configurat,configuration,106024,"ring],impact:String,minimised:Int32,regulatory_feature_id:String,variant_allele:String}],seq_region_name:String,start:Int32,strand:Int32,transcript_consequences:Array[Struct{allele_num:Int32,amino_acids:String,biotype:String,canonical:Int32,ccds:String,cdna_start:Int32,cdna_end:Int32,cds_end:Int32,cds_start:Int32,codons:String,consequence_terms:Array[String],distance:Int32,domains:Array[Struct{db:String,name:String}],exon:String,gene_id:String,gene_pheno:Int32,gene_symbol:String,gene_symbol_source:String,hgnc_id:String,hgvsc:String,hgvsp:String,hgvs_offset:Int32,impact:String,intron:String,lof:String,lof_flags:String,lof_filter:String,lof_info:String,minimised:Int32,polyphen_prediction:String,polyphen_score:Float64,protein_end:Int32,protein_start:Int32,protein_id:String,sift_prediction:String,sift_score:Float64,strand:Int32,swissprot:String,transcript_id:String,trembl:String,uniparc:String,variant_allele:String}],variant_class:String}""; }. The configuration files used by``hailctl dataproc`` can be found at the following locations:. GRCh37: gs://hail-us-central1-vep/vep85-loftee-gcloud.json; GRCh38: gs://hail-us-central1-vep/vep95-GRCh38-loftee-gcloud.json. If no config file is specified, this function will check to see if environment variable VEP_CONFIG_URI is set with a path to a config file.; Batch Service Configuration; If no config is specified, Hail will use the user’s Service configuration parameters to find a supported VEP configuration.; However, if you wish to use your own implementation of VEP, then see the documentation for VEPConfig.; Annotations; A new row field is added in the location specified by name with type given; by the type given by the json_vep_schema (if csq is False) or; tarray of tstr (if csq is True).; If csq is True, then the CSQ header string is also added as a global; field with name name + '_csq_header'. Parameters:. dataset (MatrixTable or Table) – Dataset.; config (str or VEPConfig, optional) – Path to VEP configuration file or a VEP",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:106471,Deployability,configurat,configuration,106471,"truct{db:String,name:String}],exon:String,gene_id:String,gene_pheno:Int32,gene_symbol:String,gene_symbol_source:String,hgnc_id:String,hgvsc:String,hgvsp:String,hgvs_offset:Int32,impact:String,intron:String,lof:String,lof_flags:String,lof_filter:String,lof_info:String,minimised:Int32,polyphen_prediction:String,polyphen_score:Float64,protein_end:Int32,protein_start:Int32,protein_id:String,sift_prediction:String,sift_score:Float64,strand:Int32,swissprot:String,transcript_id:String,trembl:String,uniparc:String,variant_allele:String}],variant_class:String}""; }. The configuration files used by``hailctl dataproc`` can be found at the following locations:. GRCh37: gs://hail-us-central1-vep/vep85-loftee-gcloud.json; GRCh38: gs://hail-us-central1-vep/vep95-GRCh38-loftee-gcloud.json. If no config file is specified, this function will check to see if environment variable VEP_CONFIG_URI is set with a path to a config file.; Batch Service Configuration; If no config is specified, Hail will use the user’s Service configuration parameters to find a supported VEP configuration.; However, if you wish to use your own implementation of VEP, then see the documentation for VEPConfig.; Annotations; A new row field is added in the location specified by name with type given; by the type given by the json_vep_schema (if csq is False) or; tarray of tstr (if csq is True).; If csq is True, then the CSQ header string is also added as a global; field with name name + '_csq_header'. Parameters:. dataset (MatrixTable or Table) – Dataset.; config (str or VEPConfig, optional) – Path to VEP configuration file or a VEPConfig object.; block_size (int) – Number of rows to process per VEP invocation.; name (str) – Name for resulting row field.; csq (bool) – If True, annotates with the VCF CSQ field as a tstr.; If False, annotates as the vep_json_schema.; tolerate_parse_error (bool) – If True, ignore invalid JSON produced by VEP and return a missing annotation. Returns:; MatrixTable or Table – Dataset with",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:106520,Deployability,configurat,configuration,106520,"truct{db:String,name:String}],exon:String,gene_id:String,gene_pheno:Int32,gene_symbol:String,gene_symbol_source:String,hgnc_id:String,hgvsc:String,hgvsp:String,hgvs_offset:Int32,impact:String,intron:String,lof:String,lof_flags:String,lof_filter:String,lof_info:String,minimised:Int32,polyphen_prediction:String,polyphen_score:Float64,protein_end:Int32,protein_start:Int32,protein_id:String,sift_prediction:String,sift_score:Float64,strand:Int32,swissprot:String,transcript_id:String,trembl:String,uniparc:String,variant_allele:String}],variant_class:String}""; }. The configuration files used by``hailctl dataproc`` can be found at the following locations:. GRCh37: gs://hail-us-central1-vep/vep85-loftee-gcloud.json; GRCh38: gs://hail-us-central1-vep/vep95-GRCh38-loftee-gcloud.json. If no config file is specified, this function will check to see if environment variable VEP_CONFIG_URI is set with a path to a config file.; Batch Service Configuration; If no config is specified, Hail will use the user’s Service configuration parameters to find a supported VEP configuration.; However, if you wish to use your own implementation of VEP, then see the documentation for VEPConfig.; Annotations; A new row field is added in the location specified by name with type given; by the type given by the json_vep_schema (if csq is False) or; tarray of tstr (if csq is True).; If csq is True, then the CSQ header string is also added as a global; field with name name + '_csq_header'. Parameters:. dataset (MatrixTable or Table) – Dataset.; config (str or VEPConfig, optional) – Path to VEP configuration file or a VEPConfig object.; block_size (int) – Number of rows to process per VEP invocation.; name (str) – Name for resulting row field.; csq (bool) – If True, annotates with the VCF CSQ field as a tstr.; If False, annotates as the vep_json_schema.; tolerate_parse_error (bool) – If True, ignore invalid JSON produced by VEP and return a missing annotation. Returns:; MatrixTable or Table – Dataset with",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:107039,Deployability,configurat,configuration,107039,"tring,hgvsp:String,hgvs_offset:Int32,impact:String,intron:String,lof:String,lof_flags:String,lof_filter:String,lof_info:String,minimised:Int32,polyphen_prediction:String,polyphen_score:Float64,protein_end:Int32,protein_start:Int32,protein_id:String,sift_prediction:String,sift_score:Float64,strand:Int32,swissprot:String,transcript_id:String,trembl:String,uniparc:String,variant_allele:String}],variant_class:String}""; }. The configuration files used by``hailctl dataproc`` can be found at the following locations:. GRCh37: gs://hail-us-central1-vep/vep85-loftee-gcloud.json; GRCh38: gs://hail-us-central1-vep/vep95-GRCh38-loftee-gcloud.json. If no config file is specified, this function will check to see if environment variable VEP_CONFIG_URI is set with a path to a config file.; Batch Service Configuration; If no config is specified, Hail will use the user’s Service configuration parameters to find a supported VEP configuration.; However, if you wish to use your own implementation of VEP, then see the documentation for VEPConfig.; Annotations; A new row field is added in the location specified by name with type given; by the type given by the json_vep_schema (if csq is False) or; tarray of tstr (if csq is True).; If csq is True, then the CSQ header string is also added as a global; field with name name + '_csq_header'. Parameters:. dataset (MatrixTable or Table) – Dataset.; config (str or VEPConfig, optional) – Path to VEP configuration file or a VEPConfig object.; block_size (int) – Number of rows to process per VEP invocation.; name (str) – Name for resulting row field.; csq (bool) – If True, annotates with the VCF CSQ field as a tstr.; If False, annotates as the vep_json_schema.; tolerate_parse_error (bool) – If True, ignore invalid JSON produced by VEP and return a missing annotation. Returns:; MatrixTable or Table – Dataset with new row-indexed field name containing VEP annotations. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:107570,Deployability,update,updated,107570,"tring,hgvsp:String,hgvs_offset:Int32,impact:String,intron:String,lof:String,lof_flags:String,lof_filter:String,lof_info:String,minimised:Int32,polyphen_prediction:String,polyphen_score:Float64,protein_end:Int32,protein_start:Int32,protein_id:String,sift_prediction:String,sift_score:Float64,strand:Int32,swissprot:String,transcript_id:String,trembl:String,uniparc:String,variant_allele:String}],variant_class:String}""; }. The configuration files used by``hailctl dataproc`` can be found at the following locations:. GRCh37: gs://hail-us-central1-vep/vep85-loftee-gcloud.json; GRCh38: gs://hail-us-central1-vep/vep95-GRCh38-loftee-gcloud.json. If no config file is specified, this function will check to see if environment variable VEP_CONFIG_URI is set with a path to a config file.; Batch Service Configuration; If no config is specified, Hail will use the user’s Service configuration parameters to find a supported VEP configuration.; However, if you wish to use your own implementation of VEP, then see the documentation for VEPConfig.; Annotations; A new row field is added in the location specified by name with type given; by the type given by the json_vep_schema (if csq is False) or; tarray of tstr (if csq is True).; If csq is True, then the CSQ header string is also added as a global; field with name name + '_csq_header'. Parameters:. dataset (MatrixTable or Table) – Dataset.; config (str or VEPConfig, optional) – Path to VEP configuration file or a VEPConfig object.; block_size (int) – Number of rows to process per VEP invocation.; name (str) – Name for resulting row field.; csq (bool) – If True, annotates with the VCF CSQ field as a tstr.; If False, annotates as the vep_json_schema.; tolerate_parse_error (bool) – If True, ignore invalid JSON produced by VEP and return a missing annotation. Returns:; MatrixTable or Table – Dataset with new row-indexed field name containing VEP annotations. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:70529,Energy Efficiency,reduce,reduced,70529," with mean \(p_i\) and variance; \(\sigma^2_i = p_i(1 - p_i)\), the binomial variance.; \(G W G^T\), is a symmetric positive-definite matrix when the weights are non-negative. We describe below our interpretation of the mathematics as described in the main body and; appendix of Wu, et al. According to the paper, the distribution of \(Q\) is given by a; generalized chi-squared distribution whose weights are the eigenvalues of a symmetric matrix; which we call \(Z Z^T\):. \[\begin{align*}; V_{ii} &= \sigma^2_i \\; W_{ii} &= w_i \quad\quad \textrm{the weight for variant } i \\; \\; P_0 &= V - V X (X^T V X)^{-1} X^T V \\; Z Z^T &= P_0^{1/2} G W G^T P_0^{1/2}; \end{align*}\]; The eigenvalues of \(Z Z^T\) and \(Z^T Z\) are the squared singular values of \(Z\);; therefore, we instead focus on \(Z^T Z\). In the expressions below, we elide transpositions; of symmetric matrices:. \[\begin{align*}; Z Z^T &= P_0^{1/2} G W G^T P_0^{1/2} \\; Z &= P_0^{1/2} G W^{1/2} \\; Z^T Z &= W^{1/2} G^T P_0 G W^{1/2}; \end{align*}\]; Before substituting the definition of \(P_0\), simplify it using the reduced QR; decomposition:. \[\begin{align*}; Q R &= V^{1/2} X \\; R^T Q^T &= X^T V^{1/2} \\; \\; P_0 &= V - V X (X^T V X)^{-1} X^T V \\; &= V - V X (R^T Q^T Q R)^{-1} X^T V \\; &= V - V X (R^T R)^{-1} X^T V \\; &= V - V X R^{-1} (R^T)^{-1} X^T V \\; &= V - V^{1/2} Q (R^T)^{-1} X^T V^{1/2} \\; &= V - V^{1/2} Q Q^T V^{1/2} \\; &= V^{1/2} (I - Q Q^T) V^{1/2} \\; \end{align*}\]; Substitute this simplified expression into \(Z\):. \[\begin{align*}; Z^T Z &= W^{1/2} G^T V^{1/2} (I - Q Q^T) V^{1/2} G W^{1/2} \\; \end{align*}\]; Split this symmetric matrix by observing that \(I - Q Q^T\) is idempotent:. \[\begin{align*}; I - Q Q^T &= (I - Q Q^T)(I - Q Q^T)^T \\; \\; Z &= (I - Q Q^T) V^{1/2} G W^{1/2} \\; Z &= (G - Q Q^T G) V^{1/2} W^{1/2}; \end{align*}\]; Finally, the squared singular values of \(Z\) are the eigenvalues of \(Z^T Z\), so; \(Q\) should be distributed as follows:. \[\begin{align*}; U S V^T ",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:84444,Energy Efficiency,efficient,efficient,84444," not the genotypes. Use; split_multi_hts() if possible, or split the genotypes yourself using; one of the entry modification methods: MatrixTable.annotate_entries(),; MatrixTable.select_entries(), MatrixTable.transmute_entries().; The resulting dataset will be keyed by the split locus and alleles.; split_multi() adds the following fields:. was_split (bool) – True if this variant was originally; multiallelic, otherwise False.; a_index (int) – The original index of this alternate allele in the; multiallelic representation (NB: 1 is the first alternate allele or the; only alternate allele in a biallelic variant). For example, 1:100:A:T,C; splits into two variants: 1:100:A:T with a_index = 1 and 1:100:A:C; with a_index = 2.; old_locus (locus) – The original, unsplit locus.; old_alleles (array<str>) – The original, unsplit alleles. All other fields are left unchanged. Warning; This method assumes ds contains at most one non-split variant per locus. This assumption permits the; most efficient implementation of the splitting algorithm. If your queries involving split_multi; crash with errors about out-of-order keys, this assumption may be violated. Otherwise, this; warning likely does not apply to your dataset.; If each locus in ds contains one multiallelic variant and one or more biallelic variants, you; can filter to the multiallelic variants, split those, and then combine the split variants with; the original biallelic variants.; For example, the following code splits a dataset mt which contains a mixture of split and; non-split variants.; >>> bi = mt.filter_rows(hl.len(mt.alleles) == 2); >>> bi = bi.annotate_rows(a_index=1, was_split=False, old_locus=bi.locus, old_alleles=bi.alleles); >>> multi = mt.filter_rows(hl.len(mt.alleles) > 2); >>> split = hl.split_multi(multi); >>> mt = split.union_rows(bi). Example; split_multi_hts(), which splits multiallelic variants for the HTS; genotype schema and updates the entry fields by downcoding the genotype, is; implemented as:; >>",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:87224,Energy Efficiency,efficient,efficient,87224,"t aligned and have unique; loci. This avoids a shuffle. If the assumption is violated, an error; is generated.; permit_shuffle (bool) – If True, permit a data shuffle to sort out-of-order split results.; This will only be required if input data has duplicate loci, one of; which contains more than one alternate allele. Returns:; MatrixTable or Table. hail.methods.split_multi_hts(ds, keep_star=False, left_aligned=False, vep_root='vep', *, permit_shuffle=False)[source]; Split multiallelic variants for datasets that contain one or more fields; from a standard high-throughput sequencing entry schema.; struct {; GT: call,; AD: array<int32>,; DP: int32,; GQ: int32,; PL: array<int32>,; PGT: call,; PID: str; }. For other entry fields, write your own splitting logic using; MatrixTable.annotate_entries().; Examples; >>> hl.split_multi_hts(dataset).write('output/split.mt'). Warning; This method assumes ds contains at most one non-split variant per locus. This assumption permits the; most efficient implementation of the splitting algorithm. If your queries involving split_multi_hts; crash with errors about out-of-order keys, this assumption may be violated. Otherwise, this; warning likely does not apply to your dataset.; If each locus in ds contains one multiallelic variant and one or more biallelic variants, you; can filter to the multiallelic variants, split those, and then combine the split variants with; the original biallelic variants.; For example, the following code splits a dataset mt which contains a mixture of split and; non-split variants.; >>> bi = mt.filter_rows(hl.len(mt.alleles) == 2); >>> bi = bi.annotate_rows(a_index=1, was_split=False); >>> multi = mt.filter_rows(hl.len(mt.alleles) > 2); >>> split = hl.split_multi_hts(multi); >>> mt = split.union_rows(bi). Notes; We will explain by example. Consider a hypothetical 3-allelic; variant:; A C,T 0/2:7,2,6:15:45:99,50,99,0,45,99. split_multi_hts() will create two biallelic variants (one for each; alternate allele) at",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:28477,Integrability,depend,depend,28477,"llowing:; GT: 1/1; GQ: 980; AD: 0,50. 0 | 980; 1 | 980 0; +-----------; 0 1. In summary:. GT: Set to most likely genotype based on the PLs ignoring; the filtered allele(s).; AD: The filtered alleles’ columns are eliminated, e.g.,; filtering alleles 1 and 2 transforms 25,5,10,20 to; 25,20.; DP: Unchanged.; PL: Columns involving filtered alleles are eliminated and; the remaining columns’ values are shifted so the minimum; value is 0.; GQ: The second-lowest PL (after shifting). Warning; filter_alleles_hts() does not update any row fields other than; locus and alleles. This means that row fields like allele count (AC) can; become meaningless unless they are also updated. You can update them with; annotate_rows(). See also; filter_alleles(). Parameters:. mt (MatrixTable); f (callable) – Function from (allele: StringExpression, allele_index:; Int32Expression) to BooleanExpression; subset (bool) – Subset PL field if True, otherwise downcode PL field. The; calculation of GT and GQ also depend on whether one subsets or; downcodes the PL. Returns:; MatrixTable. hail.methods.hwe_normalized_pca(call_expr, k=10, compute_loadings=False)[source]; Run principal component analysis (PCA) on the Hardy-Weinberg-normalized; genotype call matrix.; Examples; >>> eigenvalues, scores, loadings = hl.hwe_normalized_pca(dataset.GT, k=5). Notes; This method specializes pca() for the common use case; of PCA in statistical genetics, that of projecting samples to a small; number of ancestry coordinates. Variants that are all homozygous reference; or all homozygous alternate are unnormalizable and removed before; evaluation. See pca() for more details.; Users of PLINK/GCTA should be aware that Hail computes the GRM slightly; differently with regard to missing data. In Hail, the; \(ij\) entry of the GRM \(MM^T\) is simply the dot product of rows; \(i\) and \(j\) of \(M\); in terms of \(C\) it is. \[\frac{1}{m}\sum_{l\in\mathcal{C}_i\cap\mathcal{C}_j}\frac{(C_{il}-2p_l)(C_{jl} - 2p_l)}{2p_l(1-p_l)}\]",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:46610,Integrability,interface,interface,46610,"ers:. call_expr (CallExpression) – Entry-indexed call expression on a matrix table with row-indexed; variants and column-indexed samples.; r2 (float) – Squared correlation threshold (exclusive upper bound).; Must be in the range [0.0, 1.0].; bp_window_size (int) – Window size in base pairs (inclusive upper bound).; memory_per_core (int) – Memory in MB per core for local pruning queue.; keep_higher_maf (int) – If True, break ties at each step of the global pruning stage by; preferring to keep variants with higher minor allele frequency.; block_size (int, optional) – Block size for block matrices in the second stage.; Default given by BlockMatrix.default_block_size(). Returns:; Table – Table of a maximal independent set of variants. hail.methods.compute_charr(ds, min_af=0.05, max_af=0.95, min_dp=10, max_dp=100, min_gq=20, ref_AF=None)[source]; Compute CHARR, the DNA sample contamination estimator. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. Notes; The returned table has the sample ID field, plus the field:. charr (float64): CHARR contamination estimation. Note; It is possible to use gnomAD reference allele frequencies with the following:; >>> gnomad_sites = hl.experimental.load_dataset('gnomad_genome_sites', version='3.1.2') ; >>> charr_result = hl.compute_charr(mt, ref_af=(1 - gnomad_sites[mt.row_key].freq[0].AF)) . If the dataset is loaded from a gvcf and has NON_REF alleles, drop the last allele with the following or load it with the hail vcf combiner:; >>> mt = mt.key_rows_by(locus=mt.locus, alleles=mt.alleles[:-1]). Parameters:. ds (MatrixTable or VariantDataset) – Dataset.; min_af – Minimum reference allele frequency to filter variants.; max_af – Maximum reference allele frequency to filter variants.; min_dp – Minimum sequencing depth to filter variants.; max_dp – Maximum sequencing depth to filter variants.; min_gq – Minimum genotype quality to filter variants; ref_AF",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:55250,Integrability,depend,depends,55250,"locus', 'alleles', 'id'].; The model looks for de novo events in which both parents are homozygous; reference and the proband is a heterozygous. The model makes the simplifying; assumption that when this configuration x = (AA, AA, AB) of calls; occurs, exactly one of the following is true:. d: a de novo mutation occurred in the proband and all calls are; accurate.; m: at least one parental allele is actually heterozygous and; the proband call is accurate. We can then estimate the posterior probability of a de novo mutation as:. \[\mathrm{P_{\text{de novo}}} = \frac{\mathrm{P}(d \mid x)}{\mathrm{P}(d \mid x) + \mathrm{P}(m \mid x)}\]; Applying Bayes rule to the numerator and denominator yields. \[\frac{\mathrm{P}(x \mid d)\,\mathrm{P}(d)}{\mathrm{P}(x \mid d)\,\mathrm{P}(d) +; \mathrm{P}(x \mid m)\,\mathrm{P}(m)}\]; The prior on de novo mutation is estimated from the rate in the literature:. \[\mathrm{P}(d) = \frac{1 \, \text{mutation}}{30{,}000{,}000 \, \text{bases}}\]; The prior used for at least one alternate allele between the parents; depends on the alternate allele frequency:. \[\mathrm{P}(m) = 1 - (1 - AF)^4\]; The likelihoods \(\mathrm{P}(x \mid d)\) and \(\mathrm{P}(x \mid m)\); are computed from the PL (genotype likelihood) fields using these; factorizations:. \[\mathrm{P}(x = (AA, AA, AB) \mid d) = \left(; \begin{aligned}; &\mathrm{P}(x_{\mathrm{father}} = AA \mid \mathrm{father} = AA) \\; {} \cdot {} &\mathrm{P}(x_{\mathrm{mother}} = AA \mid \mathrm{mother} = AA) \\; {} \cdot {} &\mathrm{P}(x_{\mathrm{proband}} = AB \mid \mathrm{proband} = AB); \end{aligned}; \right). \]. \[\begin{aligned}; \mathrm{P}(x = (AA, AA, AB) \mid m) = &\left(; \begin{aligned}; &\mathrm{P}(x_{\mathrm{father}} = AA \mid \mathrm{father} = AB); \cdot \mathrm{P}(x_{\mathrm{mother}} = AA \mid \mathrm{mother} = AA) \\; {} + {} &\mathrm{P}(x_{\mathrm{father}} = AA \mid \mathrm{father} = AA); \cdot \mathrm{P}(x_{\mathrm{mother}} = AA \mid \mathrm{mother} = AB); \end{aligned}; \right) \\; ",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:59156,Integrability,interface,interface,59156,"ele balance in a parent is above the max_parent_ab parameter, or; if the posterior probability p is smaller than the min_p parameter. Parameters:. mt (MatrixTable) – High-throughput sequencing dataset.; pedigree (Pedigree) – Sample pedigree.; pop_frequency_prior (Float64Expression) – Expression for population alternate allele frequency prior.; min_gq – Minimum proband GQ to be considered for de novo calling.; min_p – Minimum posterior probability to be considered for de novo calling.; max_parent_ab – Maximum parent allele balance.; min_child_ab – Minimum proband allele balance/; min_dp_ratio – Minimum ratio between proband read depth and parental read depth.; ignore_in_sample_allele_frequency – Ignore in-sample allele frequency in computing site prior. Experimental. Returns:; Table. hail.methods.nirvana(dataset, config, block_size=500000, name='nirvana')[source]; Annotate variants using Nirvana. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). nirvana() runs Nirvana on the current dataset and adds a; new row field in the location specified by name.; Examples; Add Nirvana annotations to the dataset:; >>> result = hl.nirvana(dataset, ""data/nirvana.properties"") . Configuration; nirvana() requires a configuration file. The format is a; .properties file, where each; line defines a property as a key-value pair of the form key = value.; nirvana() supports the following properties:. hail.nirvana.dotnet – Location of dotnet. Optional, default: dotnet.; hail.nirvana.path – Value of the PATH environment variable when; invoking Nirvana. Optional, by default PATH is not set.; hail.nirvana.location – Location of Nirvana.dll. Required.; hail.nirvana.reference – Location of reference genome. Required.; hail.nirvana.cache – Location of cache. Required.; hail.nirvana.supplementaryAnnotatio",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:73985,Integrability,integrat,integration,73985,"-+-------+----------+----------+-------+. The same test, but using the original paper’s suggested weights which are derived from the; allele frequency.; >>> mt = hl.variant_qc(mt); >>> skat = hl._logistic_skat(; ... mt.gene,; ... hl.dbeta(mt.variant_qc.AF[0], 1, 25),; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0]); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | 8.04e+00 | 3.50e-01 | 0 |; | 1 | 9 | 1.22e+00 | 5.04e-01 | 0 |; +-------+-------+----------+----------+-------+. Our simulated data was unweighted, so the null hypothesis appears true. In real datasets, we; expect the allele frequency to correlate with effect size.; Notice that, in the second group, the fault flag is set to 1. This indicates that the numerical; integration to calculate the p-value failed to achieve the required accuracy (by default,; 1e-6). In this particular case, the null hypothesis is likely true and the numerical integration; returned a (nonsensical) value greater than one.; The max_size parameter allows us to skip large genes that would cause “out of memory” errors:; >>> skat = hl._logistic_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0],; ... max_size=10); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | NA | NA | NA |; | 1 | 9 | 1.39e+02 | 1.82e-03 | 0 |; +-------+-------+----------+----------+-------+. Notes; In the SKAT R package, the “weights” are actually the square root of the weight expression; from the paper. This method uses the definition from the paper.; The paper inclu",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:74161,Integrability,integrat,integration,74161,"ele frequency.; >>> mt = hl.variant_qc(mt); >>> skat = hl._logistic_skat(; ... mt.gene,; ... hl.dbeta(mt.variant_qc.AF[0], 1, 25),; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0]); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | 8.04e+00 | 3.50e-01 | 0 |; | 1 | 9 | 1.22e+00 | 5.04e-01 | 0 |; +-------+-------+----------+----------+-------+. Our simulated data was unweighted, so the null hypothesis appears true. In real datasets, we; expect the allele frequency to correlate with effect size.; Notice that, in the second group, the fault flag is set to 1. This indicates that the numerical; integration to calculate the p-value failed to achieve the required accuracy (by default,; 1e-6). In this particular case, the null hypothesis is likely true and the numerical integration; returned a (nonsensical) value greater than one.; The max_size parameter allows us to skip large genes that would cause “out of memory” errors:; >>> skat = hl._logistic_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0],; ... max_size=10); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | NA | NA | NA |; | 1 | 9 | 1.39e+02 | 1.82e-03 | 0 |; +-------+-------+----------+----------+-------+. Notes; In the SKAT R package, the “weights” are actually the square root of the weight expression; from the paper. This method uses the definition from the paper.; The paper includes an explicit intercept term but this method expects the user to specify the; intercept as an extra covariate with the value 1.; This ",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:75663,Integrability,depend,dependent,75663," | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | NA | NA | NA |; | 1 | 9 | 1.39e+02 | 1.82e-03 | 0 |; +-------+-------+----------+----------+-------+. Notes; In the SKAT R package, the “weights” are actually the square root of the weight expression; from the paper. This method uses the definition from the paper.; The paper includes an explicit intercept term but this method expects the user to specify the; intercept as an extra covariate with the value 1.; This method does not perform small sample size correction.; The q_stat return value is not the \(Q\) statistic from the paper. We match the output; of the SKAT R package which returns \(\tilde{Q}\):. \[\tilde{Q} = \frac{Q}{2}\]. Parameters:. group (Expression) – Row-indexed expression indicating to which group a variant belongs. This is typically a gene; name or an interval.; weight (Float64Expression) – Row-indexed expression for weights. Must be non-negative.; y (Float64Expression) – Column-indexed response (dependent variable) expression.; x (Float64Expression) – Entry-indexed expression for input (independent variable).; covariates (list of Float64Expression) – List of column-indexed covariate expressions. You must explicitly provide an intercept term; if desired. You must provide at least one covariate.; max_size (int) – Maximum size of group on which to run the test. Groups which exceed this size will have a; missing p-value and missing q statistic. Defaults to 46340.; null_max_iterations (int) – The maximum number of iterations when fitting the logistic null model. Defaults to 25.; null_tolerance (float) – The null model logisitic regression converges when the errors is less than this. Defaults to; 1e-6.; accuracy (float) – The accuracy of the p-value if fault value is zero. Defaults to 1e-6.; iterations (int) – The maximum number of iterations used to calculate the p-value (which has no closed; form). Defaults to 1e5. Returns:; Table – One row per-group. The key is group. T",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:81472,Integrability,integrat,integration,81472,"4.122; 0.192; 0. Groups larger than max_size appear with missing q_stat, p_value, and; fault. The hard limit on the number of rows in a group is 46340.; Note that the variance component score q_stat agrees with Q in the R; package skat, but both differ from \(Q\) in the paper by the factor; \(\frac{1}{2\sigma^2}\) in the linear case and \(\frac{1}{2}\) in; the logistic case, where \(\sigma^2\) is the unbiased estimator of; residual variance for the linear null model. The R package also applies a; “small-sample adjustment” to the null distribution in the logistic case; when the sample size is less than 2000. Hail does not apply this; adjustment.; The fault flag is an integer indicating whether any issues occurred when; running the Davies algorithm to compute the p-value as the right tail of a; weighted sum of \(\chi^2(1)\) distributions. fault value; Description. 0; no issues. 1; accuracy NOT achieved. 2; round-off error possibly significant. 3; invalid parameters. 4; unable to locate integration parameters. 5; out of memory. Parameters:. key_expr (Expression) – Row-indexed expression for key associated to each row.; weight_expr (Float64Expression) – Row-indexed expression for row weights.; y (Float64Expression) – Column-indexed response expression.; If logistic is True, all non-missing values must evaluate to 0 or; 1. Note that a BooleanExpression will be implicitly converted; to a Float64Expression with this property.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – List of column-indexed covariate expressions.; logistic (bool or tuple of int and float) – If false, use the linear test. If true, use the logistic test with no; more than 25 logistic iterations and a convergence tolerance of 1e-6. If; a tuple is given, use the logistic test with the tuple elements as the; maximum nubmer of iterations and convergence tolerance, respectively.; max_size (int) – Maximum size of group on which to run the test.; ",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:82925,Integrability,interface,interface,82925,"erty.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – List of column-indexed covariate expressions.; logistic (bool or tuple of int and float) – If false, use the linear test. If true, use the logistic test with no; more than 25 logistic iterations and a convergence tolerance of 1e-6. If; a tuple is given, use the logistic test with the tuple elements as the; maximum nubmer of iterations and convergence tolerance, respectively.; max_size (int) – Maximum size of group on which to run the test.; accuracy (float) – Accuracy achieved by the Davies algorithm if fault value is zero.; iterations (int) – Maximum number of iterations attempted by the Davies algorithm. Returns:; Table – Table of SKAT results. hail.methods.lambda_gc(p_value, approximate=True)[source]; Compute genomic inflation factor (lambda GC) from an Expression of p-values. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. Parameters:. p_value (NumericExpression) – Row-indexed numeric expression of p-values.; approximate (bool) – If False, computes exact lambda GC (slower and uses more memory). Returns:; float – Genomic inflation factor (lambda genomic control). hail.methods.split_multi(ds, keep_star=False, left_aligned=False, *, permit_shuffle=False)[source]; Split multiallelic variants. Warning; In order to support a wide variety of data types, this function splits only; the variants on a MatrixTable, but not the genotypes. Use; split_multi_hts() if possible, or split the genotypes yourself using; one of the entry modification methods: MatrixTable.annotate_entries(),; MatrixTable.select_entries(), MatrixTable.transmute_entries().; The resulting dataset will be keyed by the split locus and alleles.; split_multi() adds the following fields:. was_split (bool) – True if this variant was originally; multiallelic, otherwise False.; a_index (int) – The original",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:102560,Integrability,depend,depending,102560,"ns to the dataset:; >>> result = hl.vep(dataset, ""data/vep-configuration.json"") . Notes; Installation; This VEP command only works if you have already installed VEP on your; computing environment. If you use hailctl dataproc to start Hail clusters,; installing VEP is achieved by specifying the –vep flag. For more detailed instructions,; see Variant Effect Predictor (VEP). If you use hailctl hdinsight, see Variant Effect Predictor (VEP).; Spark Configuration; vep() needs a configuration file to tell it how to run VEP. This is the config argument; to the VEP function. If you are using hailctl dataproc as mentioned above, you can just use the; default argument for config and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below.; The format of the configuration file is JSON, and vep(); expects a JSON object with three fields:. command (array of string) – The VEP command line to run. The string literal __OUTPUT_FORMAT_FLAG__ is replaced with –json or –vcf depending on csq.; env (object) – A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; vep_json_schema (string): The type of the VEP JSON schema (as produced by the VEP when invoked with the –json option). Note: This is the old-style ‘parseable’ Hail type syntax. This will change. Here is an example configuration file for invoking VEP release 85; installed in /vep with the Loftee plugin:; {; ""command"": [; ""/vep"",; ""--format"", ""vcf"",; ""__OUTPUT_FORMAT_FLAG__"",; ""--everything"",; ""--allele_number"",; ""--no_stats"",; ""--cache"", ""--offline"",; ""--minimal"",; ""--assembly"", ""GRCh37"",; ""--plugin"", ""LoF,human_ancestor_fa:/root/.vep/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:/root/.vep/loftee_data/phylocsf_gerp.sql,gerp_file:/root/.vep/loftee_data/GERP_scores.final.sorted.txt.gz"",; ""-o"", ""STDOUT""; ],; ""env"": {; ""PERL5LIB"": ""/vep_data/",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:716,Modifiability,config,configuring,716,"﻿. Hail | ; Genetics. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Methods; Genetics. View page source. Genetics. VEPConfig(); Base class for configuring VEP. VEPConfigGRCh37Version85(*, data_bucket, ...); The Hail-maintained VEP configuration for GRCh37 for VEP version 85. VEPConfigGRCh38Version95(*, data_bucket, ...); The Hail-maintained VEP configuration for GRCh38 for VEP version 95. balding_nichols_model(n_populations, ...[, ...]); Generate a matrix table of variants, samples, and genotypes using the Balding-Nichols or Pritchard-Stephens-Donnelly model. concordance(left, right, *[, ...]); Calculate call concordance with another dataset. filter_intervals(ds, intervals[, keep]); Filter rows with a list of intervals. filter_alleles(mt, f); Filter alternate alleles. filter_alleles_hts(mt, f[, subset]); Filter alternate alleles and update standard GATK entry fields. hwe_normalized_pca(call_expr[, k, ...]); Run principal component analysis (PCA) on the Hardy-Weinberg-normalized genotype call matrix. genetic_relatedness_matrix(call_expr); Compute the genetic relatedness matrix (GRM). realized_relationship_matrix(call_expr); Computes the realized relationship matrix (RRM). impute_sex(call[, aaf_threshold, ...]); Impute sex of samples by calculating inbreeding coefficient on the X chromosome. ld_matrix(entry_expr, locus_expr, radius[, ...]); Computes the windowed correlation (linkage disequilibrium) matrix b",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:804,Modifiability,config,configuration,804,"﻿. Hail | ; Genetics. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Methods; Genetics. View page source. Genetics. VEPConfig(); Base class for configuring VEP. VEPConfigGRCh37Version85(*, data_bucket, ...); The Hail-maintained VEP configuration for GRCh37 for VEP version 85. VEPConfigGRCh38Version95(*, data_bucket, ...); The Hail-maintained VEP configuration for GRCh38 for VEP version 95. balding_nichols_model(n_populations, ...[, ...]); Generate a matrix table of variants, samples, and genotypes using the Balding-Nichols or Pritchard-Stephens-Donnelly model. concordance(left, right, *[, ...]); Calculate call concordance with another dataset. filter_intervals(ds, intervals[, keep]); Filter rows with a list of intervals. filter_alleles(mt, f); Filter alternate alleles. filter_alleles_hts(mt, f[, subset]); Filter alternate alleles and update standard GATK entry fields. hwe_normalized_pca(call_expr[, k, ...]); Run principal component analysis (PCA) on the Hardy-Weinberg-normalized genotype call matrix. genetic_relatedness_matrix(call_expr); Compute the genetic relatedness matrix (GRM). realized_relationship_matrix(call_expr); Computes the realized relationship matrix (RRM). impute_sex(call[, aaf_threshold, ...]); Impute sex of samples by calculating inbreeding coefficient on the X chromosome. ld_matrix(entry_expr, locus_expr, radius[, ...]); Computes the windowed correlation (linkage disequilibrium) matrix b",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:920,Modifiability,config,configuration,920,"﻿. Hail | ; Genetics. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Methods; Genetics. View page source. Genetics. VEPConfig(); Base class for configuring VEP. VEPConfigGRCh37Version85(*, data_bucket, ...); The Hail-maintained VEP configuration for GRCh37 for VEP version 85. VEPConfigGRCh38Version95(*, data_bucket, ...); The Hail-maintained VEP configuration for GRCh38 for VEP version 95. balding_nichols_model(n_populations, ...[, ...]); Generate a matrix table of variants, samples, and genotypes using the Balding-Nichols or Pritchard-Stephens-Donnelly model. concordance(left, right, *[, ...]); Calculate call concordance with another dataset. filter_intervals(ds, intervals[, keep]); Filter rows with a list of intervals. filter_alleles(mt, f); Filter alternate alleles. filter_alleles_hts(mt, f[, subset]); Filter alternate alleles and update standard GATK entry fields. hwe_normalized_pca(call_expr[, k, ...]); Run principal component analysis (PCA) on the Hardy-Weinberg-normalized genotype call matrix. genetic_relatedness_matrix(call_expr); Compute the genetic relatedness matrix (GRM). realized_relationship_matrix(call_expr); Computes the realized relationship matrix (RRM). impute_sex(call[, aaf_threshold, ...]); Impute sex of samples by calculating inbreeding coefficient on the X chromosome. ld_matrix(entry_expr, locus_expr, radius[, ...]); Computes the windowed correlation (linkage disequilibrium) matrix b",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:2467,Modifiability,config,config,2467,"..]); Run principal component analysis (PCA) on the Hardy-Weinberg-normalized genotype call matrix. genetic_relatedness_matrix(call_expr); Compute the genetic relatedness matrix (GRM). realized_relationship_matrix(call_expr); Computes the realized relationship matrix (RRM). impute_sex(call[, aaf_threshold, ...]); Impute sex of samples by calculating inbreeding coefficient on the X chromosome. ld_matrix(entry_expr, locus_expr, radius[, ...]); Computes the windowed correlation (linkage disequilibrium) matrix between variants. ld_prune(call_expr[, r2, bp_window_size, ...]); Returns a maximal subset of variants that are nearly uncorrelated within each window. compute_charr(ds[, min_af, max_af, min_dp, ...]); Compute CHARR, the DNA sample contamination estimator. mendel_errors(call, pedigree); Find Mendel errors; count per variant, individual and nuclear family. de_novo(mt, pedigree, pop_frequency_prior, *); Call putative de novo events from trio data. nirvana(dataset, config[, block_size, name]); Annotate variants using Nirvana. sample_qc(mt[, name]); Compute per-sample metrics useful for quality control. _logistic_skat(group, weight, y, x, covariates); The logistic sequence kernel association test (SKAT). skat(key_expr, weight_expr, y, x, covariates); Test each keyed group of rows for association by linear or logistic SKAT test. lambda_gc(p_value[, approximate]); Compute genomic inflation factor (lambda GC) from an Expression of p-values. split_multi(ds[, keep_star, left_aligned, ...]); Split multiallelic variants. split_multi_hts(ds[, keep_star, ...]); Split multiallelic variants for datasets that contain one or more fields from a standard high-throughput sequencing entry schema. summarize_variants(mt[, show, handler]); Summarize the variants present in a dataset and print the results. transmission_disequilibrium_test(dataset, ...); Performs the transmission disequilibrium test on trios. trio_matrix(dataset, pedigree[, complete_trios]); Builds and returns a matrix wher",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:3662,Modifiability,config,config,3662," sequence kernel association test (SKAT). skat(key_expr, weight_expr, y, x, covariates); Test each keyed group of rows for association by linear or logistic SKAT test. lambda_gc(p_value[, approximate]); Compute genomic inflation factor (lambda GC) from an Expression of p-values. split_multi(ds[, keep_star, left_aligned, ...]); Split multiallelic variants. split_multi_hts(ds[, keep_star, ...]); Split multiallelic variants for datasets that contain one or more fields from a standard high-throughput sequencing entry schema. summarize_variants(mt[, show, handler]); Summarize the variants present in a dataset and print the results. transmission_disequilibrium_test(dataset, ...); Performs the transmission disequilibrium test on trios. trio_matrix(dataset, pedigree[, complete_trios]); Builds and returns a matrix where columns correspond to trios and entries contain genotypes for the trio. variant_qc(mt[, name]); Compute common variant statistics (quality control metrics). vep(dataset[, config, block_size, name, ...]); Annotate variants with VEP. class hail.methods.VEPConfig[source]; Base class for configuring VEP.; To define a custom VEP configuration to for Query on Batch, construct a new class that inherits from VEPConfig; and has the following parameters defined:. json_type (HailType): The type of the VEP JSON schema (as produced by VEP when invoked with the –json option).; data_bucket (str) – The location where the VEP data is stored.; data_mount (str) – The location in the container where the data should be mounted.; batch_run_command (list of str) – The command line to run for a VEP job for a partition.; batch_run_csq_header_command (list of str) – The command line to run when generating the consequence header.; env (dict of str to str) – A map of environment variables to values to add to the environment when invoking the command.; cloud (str) – The cloud where the Batch Service is located.; image (str) – The docker image to run VEP.; data_bucket_is_requester_pays (",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:3777,Modifiability,config,configuring,3777,"keyed group of rows for association by linear or logistic SKAT test. lambda_gc(p_value[, approximate]); Compute genomic inflation factor (lambda GC) from an Expression of p-values. split_multi(ds[, keep_star, left_aligned, ...]); Split multiallelic variants. split_multi_hts(ds[, keep_star, ...]); Split multiallelic variants for datasets that contain one or more fields from a standard high-throughput sequencing entry schema. summarize_variants(mt[, show, handler]); Summarize the variants present in a dataset and print the results. transmission_disequilibrium_test(dataset, ...); Performs the transmission disequilibrium test on trios. trio_matrix(dataset, pedigree[, complete_trios]); Builds and returns a matrix where columns correspond to trios and entries contain genotypes for the trio. variant_qc(mt[, name]); Compute common variant statistics (quality control metrics). vep(dataset[, config, block_size, name, ...]); Annotate variants with VEP. class hail.methods.VEPConfig[source]; Base class for configuring VEP.; To define a custom VEP configuration to for Query on Batch, construct a new class that inherits from VEPConfig; and has the following parameters defined:. json_type (HailType): The type of the VEP JSON schema (as produced by VEP when invoked with the –json option).; data_bucket (str) – The location where the VEP data is stored.; data_mount (str) – The location in the container where the data should be mounted.; batch_run_command (list of str) – The command line to run for a VEP job for a partition.; batch_run_csq_header_command (list of str) – The command line to run when generating the consequence header.; env (dict of str to str) – A map of environment variables to values to add to the environment when invoking the command.; cloud (str) – The cloud where the Batch Service is located.; image (str) – The docker image to run VEP.; data_bucket_is_requester_pays (bool) – True if the data bucket is requester pays.; regions (list of str) – A list of regions the VE",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:3818,Modifiability,config,configuration,3818,"Compute genomic inflation factor (lambda GC) from an Expression of p-values. split_multi(ds[, keep_star, left_aligned, ...]); Split multiallelic variants. split_multi_hts(ds[, keep_star, ...]); Split multiallelic variants for datasets that contain one or more fields from a standard high-throughput sequencing entry schema. summarize_variants(mt[, show, handler]); Summarize the variants present in a dataset and print the results. transmission_disequilibrium_test(dataset, ...); Performs the transmission disequilibrium test on trios. trio_matrix(dataset, pedigree[, complete_trios]); Builds and returns a matrix where columns correspond to trios and entries contain genotypes for the trio. variant_qc(mt[, name]); Compute common variant statistics (quality control metrics). vep(dataset[, config, block_size, name, ...]); Annotate variants with VEP. class hail.methods.VEPConfig[source]; Base class for configuring VEP.; To define a custom VEP configuration to for Query on Batch, construct a new class that inherits from VEPConfig; and has the following parameters defined:. json_type (HailType): The type of the VEP JSON schema (as produced by VEP when invoked with the –json option).; data_bucket (str) – The location where the VEP data is stored.; data_mount (str) – The location in the container where the data should be mounted.; batch_run_command (list of str) – The command line to run for a VEP job for a partition.; batch_run_csq_header_command (list of str) – The command line to run when generating the consequence header.; env (dict of str to str) – A map of environment variables to values to add to the environment when invoking the command.; cloud (str) – The cloud where the Batch Service is located.; image (str) – The docker image to run VEP.; data_bucket_is_requester_pays (bool) – True if the data bucket is requester pays.; regions (list of str) – A list of regions the VEP jobs can run in. In addition, the method command must be defined with the following signature. The ou",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:3882,Modifiability,inherit,inherits,3882,"Compute genomic inflation factor (lambda GC) from an Expression of p-values. split_multi(ds[, keep_star, left_aligned, ...]); Split multiallelic variants. split_multi_hts(ds[, keep_star, ...]); Split multiallelic variants for datasets that contain one or more fields from a standard high-throughput sequencing entry schema. summarize_variants(mt[, show, handler]); Summarize the variants present in a dataset and print the results. transmission_disequilibrium_test(dataset, ...); Performs the transmission disequilibrium test on trios. trio_matrix(dataset, pedigree[, complete_trios]); Builds and returns a matrix where columns correspond to trios and entries contain genotypes for the trio. variant_qc(mt[, name]); Compute common variant statistics (quality control metrics). vep(dataset[, config, block_size, name, ...]); Annotate variants with VEP. class hail.methods.VEPConfig[source]; Base class for configuring VEP.; To define a custom VEP configuration to for Query on Batch, construct a new class that inherits from VEPConfig; and has the following parameters defined:. json_type (HailType): The type of the VEP JSON schema (as produced by VEP when invoked with the –json option).; data_bucket (str) – The location where the VEP data is stored.; data_mount (str) – The location in the container where the data should be mounted.; batch_run_command (list of str) – The command line to run for a VEP job for a partition.; batch_run_csq_header_command (list of str) – The command line to run when generating the consequence header.; env (dict of str to str) – A map of environment variables to values to add to the environment when invoking the command.; cloud (str) – The cloud where the Batch Service is located.; image (str) – The docker image to run VEP.; data_bucket_is_requester_pays (bool) – True if the data bucket is requester pays.; regions (list of str) – A list of regions the VEP jobs can run in. In addition, the method command must be defined with the following signature. The ou",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:4458,Modifiability,variab,variables,4458,"eturns a matrix where columns correspond to trios and entries contain genotypes for the trio. variant_qc(mt[, name]); Compute common variant statistics (quality control metrics). vep(dataset[, config, block_size, name, ...]); Annotate variants with VEP. class hail.methods.VEPConfig[source]; Base class for configuring VEP.; To define a custom VEP configuration to for Query on Batch, construct a new class that inherits from VEPConfig; and has the following parameters defined:. json_type (HailType): The type of the VEP JSON schema (as produced by VEP when invoked with the –json option).; data_bucket (str) – The location where the VEP data is stored.; data_mount (str) – The location in the container where the data should be mounted.; batch_run_command (list of str) – The command line to run for a VEP job for a partition.; batch_run_csq_header_command (list of str) – The command line to run when generating the consequence header.; env (dict of str to str) – A map of environment variables to values to add to the environment when invoking the command.; cloud (str) – The cloud where the Batch Service is located.; image (str) – The docker image to run VEP.; data_bucket_is_requester_pays (bool) – True if the data bucket is requester pays.; regions (list of str) – A list of regions the VEP jobs can run in. In addition, the method command must be defined with the following signature. The output is the exact command to run the; VEP executable. The inputs are consequence and tolerate_parse_error which are user-defined parameters to vep(),; part_id which is the partition ID, input_file which is the path to the input file where the input data can be found, and; output_file is the path to the output file where the VEP annotations are written to. An example is shown below:; def command(self,; consequence: bool,; tolerate_parse_error: bool,; part_id: int,; input_file: Optional[str],; output_file: str) -> List[str]:; vcf_or_json = '--vcf' if consequence else '--json'; input_file = f'-",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:5690,Modifiability,plugin,plugin,5690,"is requester pays.; regions (list of str) – A list of regions the VEP jobs can run in. In addition, the method command must be defined with the following signature. The output is the exact command to run the; VEP executable. The inputs are consequence and tolerate_parse_error which are user-defined parameters to vep(),; part_id which is the partition ID, input_file which is the path to the input file where the input data can be found, and; output_file is the path to the output file where the VEP annotations are written to. An example is shown below:; def command(self,; consequence: bool,; tolerate_parse_error: bool,; part_id: int,; input_file: Optional[str],; output_file: str) -> List[str]:; vcf_or_json = '--vcf' if consequence else '--json'; input_file = f'--input_file {input_file}' if input_file else ''; return f'''/vep/vep {input_file} --format vcf {vcf_or_json} --everything --allele_number --no_stats --cache --offline --minimal --assembly GRCh37 --dir={self.data_mount} --plugin LoF,human_ancestor_fa:{self.data_mount}/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:{self.data_mount}/loftee_data/phylocsf_gerp.sql,gerp_file:{self.data_mount}/loftee_data/GERP_scores.final.sorted.txt.gz -o STDOUT; '''. The following environment variables are added to the job’s environment:. VEP_BLOCK_SIZE - The maximum number of variants provided as input to each invocation of VEP.; VEP_PART_ID - Partition ID.; VEP_DATA_MOUNT - Location where the vep data is mounted (same as data_mount in the config).; VEP_CONSEQUENCE - Integer equal to 0 or 1 on whether csq is False or True.; VEP_TOLERATE_PARSE_ERROR - Integer equal to 0 or 1 on whether tolerate_parse_error is False or True.; VEP_OUTPUT_FILE - String specifying the local path where the output TSV file with the VEP result should be located.; VEP_INPUT_FILE - String specifying the local path where the input VCF shard is located for all jobs. The VEP_INPUT_FILE environment variable is not avail",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:5990,Modifiability,variab,variables,5990," parameters to vep(),; part_id which is the partition ID, input_file which is the path to the input file where the input data can be found, and; output_file is the path to the output file where the VEP annotations are written to. An example is shown below:; def command(self,; consequence: bool,; tolerate_parse_error: bool,; part_id: int,; input_file: Optional[str],; output_file: str) -> List[str]:; vcf_or_json = '--vcf' if consequence else '--json'; input_file = f'--input_file {input_file}' if input_file else ''; return f'''/vep/vep {input_file} --format vcf {vcf_or_json} --everything --allele_number --no_stats --cache --offline --minimal --assembly GRCh37 --dir={self.data_mount} --plugin LoF,human_ancestor_fa:{self.data_mount}/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:{self.data_mount}/loftee_data/phylocsf_gerp.sql,gerp_file:{self.data_mount}/loftee_data/GERP_scores.final.sorted.txt.gz -o STDOUT; '''. The following environment variables are added to the job’s environment:. VEP_BLOCK_SIZE - The maximum number of variants provided as input to each invocation of VEP.; VEP_PART_ID - Partition ID.; VEP_DATA_MOUNT - Location where the vep data is mounted (same as data_mount in the config).; VEP_CONSEQUENCE - Integer equal to 0 or 1 on whether csq is False or True.; VEP_TOLERATE_PARSE_ERROR - Integer equal to 0 or 1 on whether tolerate_parse_error is False or True.; VEP_OUTPUT_FILE - String specifying the local path where the output TSV file with the VEP result should be located.; VEP_INPUT_FILE - String specifying the local path where the input VCF shard is located for all jobs. The VEP_INPUT_FILE environment variable is not available for the single job that computes the consequence header when; csq=True. class hail.methods.VEPConfigGRCh37Version85(*, data_bucket, data_mount, image, regions, cloud, data_bucket_is_requester_pays)[source]; The Hail-maintained VEP configuration for GRCh37 for VEP version 85.; This class takes",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:6243,Modifiability,config,config,6243,"otations are written to. An example is shown below:; def command(self,; consequence: bool,; tolerate_parse_error: bool,; part_id: int,; input_file: Optional[str],; output_file: str) -> List[str]:; vcf_or_json = '--vcf' if consequence else '--json'; input_file = f'--input_file {input_file}' if input_file else ''; return f'''/vep/vep {input_file} --format vcf {vcf_or_json} --everything --allele_number --no_stats --cache --offline --minimal --assembly GRCh37 --dir={self.data_mount} --plugin LoF,human_ancestor_fa:{self.data_mount}/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:{self.data_mount}/loftee_data/phylocsf_gerp.sql,gerp_file:{self.data_mount}/loftee_data/GERP_scores.final.sorted.txt.gz -o STDOUT; '''. The following environment variables are added to the job’s environment:. VEP_BLOCK_SIZE - The maximum number of variants provided as input to each invocation of VEP.; VEP_PART_ID - Partition ID.; VEP_DATA_MOUNT - Location where the vep data is mounted (same as data_mount in the config).; VEP_CONSEQUENCE - Integer equal to 0 or 1 on whether csq is False or True.; VEP_TOLERATE_PARSE_ERROR - Integer equal to 0 or 1 on whether tolerate_parse_error is False or True.; VEP_OUTPUT_FILE - String specifying the local path where the output TSV file with the VEP result should be located.; VEP_INPUT_FILE - String specifying the local path where the input VCF shard is located for all jobs. The VEP_INPUT_FILE environment variable is not available for the single job that computes the consequence header when; csq=True. class hail.methods.VEPConfigGRCh37Version85(*, data_bucket, data_mount, image, regions, cloud, data_bucket_is_requester_pays)[source]; The Hail-maintained VEP configuration for GRCh37 for VEP version 85.; This class takes the following constructor arguments:. data_bucket (str) – The location where the VEP data is stored.; data_mount (str) – The location in the container where the data should be mounted.; image (str) – The ",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:6680,Modifiability,variab,variable,6680,"tor_fa:{self.data_mount}/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:{self.data_mount}/loftee_data/phylocsf_gerp.sql,gerp_file:{self.data_mount}/loftee_data/GERP_scores.final.sorted.txt.gz -o STDOUT; '''. The following environment variables are added to the job’s environment:. VEP_BLOCK_SIZE - The maximum number of variants provided as input to each invocation of VEP.; VEP_PART_ID - Partition ID.; VEP_DATA_MOUNT - Location where the vep data is mounted (same as data_mount in the config).; VEP_CONSEQUENCE - Integer equal to 0 or 1 on whether csq is False or True.; VEP_TOLERATE_PARSE_ERROR - Integer equal to 0 or 1 on whether tolerate_parse_error is False or True.; VEP_OUTPUT_FILE - String specifying the local path where the output TSV file with the VEP result should be located.; VEP_INPUT_FILE - String specifying the local path where the input VCF shard is located for all jobs. The VEP_INPUT_FILE environment variable is not available for the single job that computes the consequence header when; csq=True. class hail.methods.VEPConfigGRCh37Version85(*, data_bucket, data_mount, image, regions, cloud, data_bucket_is_requester_pays)[source]; The Hail-maintained VEP configuration for GRCh37 for VEP version 85.; This class takes the following constructor arguments:. data_bucket (str) – The location where the VEP data is stored.; data_mount (str) – The location in the container where the data should be mounted.; image (str) – The docker image to run VEP.; cloud (str) – The cloud where the Batch Service is located.; data_bucket_is_requester_pays (bool) – True if the data bucket is requester pays.; regions (list of str) – A list of regions the VEP jobs can run in. class hail.methods.VEPConfigGRCh38Version95(*, data_bucket, data_mount, image, regions, cloud, data_bucket_is_requester_pays)[source]; The Hail-maintained VEP configuration for GRCh38 for VEP version 95.; This class takes the following constructor arguments:. data_bucke",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:6938,Modifiability,config,configuration,6938,"lf.data_mount}/loftee_data/GERP_scores.final.sorted.txt.gz -o STDOUT; '''. The following environment variables are added to the job’s environment:. VEP_BLOCK_SIZE - The maximum number of variants provided as input to each invocation of VEP.; VEP_PART_ID - Partition ID.; VEP_DATA_MOUNT - Location where the vep data is mounted (same as data_mount in the config).; VEP_CONSEQUENCE - Integer equal to 0 or 1 on whether csq is False or True.; VEP_TOLERATE_PARSE_ERROR - Integer equal to 0 or 1 on whether tolerate_parse_error is False or True.; VEP_OUTPUT_FILE - String specifying the local path where the output TSV file with the VEP result should be located.; VEP_INPUT_FILE - String specifying the local path where the input VCF shard is located for all jobs. The VEP_INPUT_FILE environment variable is not available for the single job that computes the consequence header when; csq=True. class hail.methods.VEPConfigGRCh37Version85(*, data_bucket, data_mount, image, regions, cloud, data_bucket_is_requester_pays)[source]; The Hail-maintained VEP configuration for GRCh37 for VEP version 85.; This class takes the following constructor arguments:. data_bucket (str) – The location where the VEP data is stored.; data_mount (str) – The location in the container where the data should be mounted.; image (str) – The docker image to run VEP.; cloud (str) – The cloud where the Batch Service is located.; data_bucket_is_requester_pays (bool) – True if the data bucket is requester pays.; regions (list of str) – A list of regions the VEP jobs can run in. class hail.methods.VEPConfigGRCh38Version95(*, data_bucket, data_mount, image, regions, cloud, data_bucket_is_requester_pays)[source]; The Hail-maintained VEP configuration for GRCh38 for VEP version 95.; This class takes the following constructor arguments:. data_bucket (str) – The location where the VEP data is stored.; data_mount (str) – The location in the container where the data should be mounted.; image (str) – The docker image to run V",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:7602,Modifiability,config,configuration,7602,"NPUT_FILE - String specifying the local path where the input VCF shard is located for all jobs. The VEP_INPUT_FILE environment variable is not available for the single job that computes the consequence header when; csq=True. class hail.methods.VEPConfigGRCh37Version85(*, data_bucket, data_mount, image, regions, cloud, data_bucket_is_requester_pays)[source]; The Hail-maintained VEP configuration for GRCh37 for VEP version 85.; This class takes the following constructor arguments:. data_bucket (str) – The location where the VEP data is stored.; data_mount (str) – The location in the container where the data should be mounted.; image (str) – The docker image to run VEP.; cloud (str) – The cloud where the Batch Service is located.; data_bucket_is_requester_pays (bool) – True if the data bucket is requester pays.; regions (list of str) – A list of regions the VEP jobs can run in. class hail.methods.VEPConfigGRCh38Version95(*, data_bucket, data_mount, image, regions, cloud, data_bucket_is_requester_pays)[source]; The Hail-maintained VEP configuration for GRCh38 for VEP version 95.; This class takes the following constructor arguments:. data_bucket (str) – The location where the VEP data is stored.; data_mount (str) – The location in the container where the data should be mounted.; image (str) – The docker image to run VEP.; cloud (str) – The cloud where the Batch Service is located.; data_bucket_is_requester_pays (bool) – True if the data bucket is set to requester pays.; regions (list of str) – A list of regions the VEP jobs can run in. hail.methods.balding_nichols_model(n_populations, n_samples, n_variants, n_partitions=None, pop_dist=None, fst=None, af_dist=None, reference_genome='default', mixture=False, *, phased=False)[source]; Generate a matrix table of variants, samples, and genotypes using the; Balding-Nichols or Pritchard-Stephens-Donnelly model.; Examples; Generate a matrix table of genotypes with 1000 variants and 100 samples; across 3 populations:; >>> hl.r",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:48202,Modifiability,inherit,inheritance,48202," or VariantDataset) – Dataset.; min_af – Minimum reference allele frequency to filter variants.; max_af – Maximum reference allele frequency to filter variants.; min_dp – Minimum sequencing depth to filter variants.; max_dp – Maximum sequencing depth to filter variants.; min_gq – Minimum genotype quality to filter variants; ref_AF – Reference AF expression. Necessary when the sample size is below 10,000. Returns:; Table. hail.methods.mendel_errors(call, pedigree)[source]; Find Mendel errors; count per variant, individual and nuclear family. Note; Requires the column key to be one field of type tstr. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). Note; Requires the dataset to contain no multiallelic variants.; Use split_multi() or split_multi_hts() to split; multiallelic sites, or MatrixTable.filter_rows() to remove; them. Examples; Find all violations of Mendelian inheritance in each (dad, mom, kid) trio in; a pedigree and return four tables (all errors, errors by family, errors by; individual, errors by variant):; >>> ped = hl.Pedigree.read('data/trios.fam'); >>> all_errors, per_fam, per_sample, per_variant = hl.mendel_errors(dataset['GT'], ped). Export all mendel errors to a text file:; >>> all_errors.export('output/all_mendel_errors.tsv'). Annotate columns with the number of Mendel errors:; >>> annotated_samples = dataset.annotate_cols(mendel=per_sample[dataset.s]). Annotate rows with the number of Mendel errors:; >>> annotated_variants = dataset.annotate_rows(mendel=per_variant[dataset.locus, dataset.alleles]). Notes; The example above returns four tables, which contain Mendelian violations; grouped in various ways. These tables are modeled after the PLINK mendel; formats, resembling; the .mendel, .fmendel, .imendel, and .lmendel formats,; respectively.; First table: all Mendel errors. This table contains one row per Mendel; error, keyed by the variant and proband id. locus (tlocus) – Variant locus, ke",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:50676,Modifiability,extend,extending,50676,"); fam_id (tstr) – Family ID.; children (tint32) – Number of children in this nuclear family.; errors (tint64) – Number of Mendel errors in this nuclear family.; snp_errors (tint64) – Number of Mendel errors at SNPs in this; nuclear family. Third table: errors per individual. This table contains one row per; individual. Each error is counted toward the proband, father, and mother; according to the Implicated in the table below. (column key of dataset) (tstr) – Sample ID (key field).; fam_id (tstr) – Family ID.; errors (tint64) – Number of Mendel errors involving this; individual.; snp_errors (tint64) – Number of Mendel errors involving this; individual at SNPs. Fourth table: errors per variant. locus (tlocus) – Variant locus, key field.; alleles (tarray of tstr) – Variant alleles, key field.; errors (tint64) – Number of Mendel errors in this variant. This method only considers complete trios (two parents and proband with; defined sex). The code of each Mendel error is determined by the table; below, extending the; Plink classification.; In the table, the copy state of a locus with respect to a trio is defined; as follows, where PAR is the pseudoautosomal region (PAR) of X and Y; defined by the reference genome and the autosome is defined by; in_autosome(). Auto – in autosome or in PAR or female child; HemiX – in non-PAR of X and male child; HemiY – in non-PAR of Y and male child. Any refers to the set { HomRef, Het, HomVar, NoCall } and ~; denotes complement in this set. Code; Dad; Mom; Kid; Copy State | Implicated. 1; HomVar; HomVar; Het; Auto; Dad, Mom, Kid. 2; HomRef; HomRef; Het; Auto; Dad, Mom, Kid. 3; HomRef; ~HomRef; HomVar; Auto; Dad, Kid. 4; ~HomRef; HomRef; HomVar; Auto; Mom, Kid. 5; HomRef; HomRef; HomVar; Auto; Kid. 6; HomVar; ~HomVar; HomRef; Auto; Dad, Kid. 7; ~HomVar; HomVar; HomRef; Auto; Mom, Kid. 8; HomVar; HomVar; HomRef; Auto; Kid. 9; Any; HomVar; HomRef; HemiX; Mom, Kid. 10; Any; HomRef; HomVar; HemiX; Mom, Kid. 11; HomVar; Any; HomRef; HemiY; D",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:54399,Modifiability,config,configuration,54399,"ample_allele_frequency parameter is True,; then the computed allele frequency is not included in the calculation, and the; prior is the maximum of the pop_frequency_prior and 1 / 3e7.; proband (struct) – Proband column fields from mt.; father (struct) – Father column fields from mt.; mother (struct) – Mother column fields from mt.; proband_entry (struct) – Proband entry fields from mt.; father_entry (struct) – Father entry fields from mt.; proband_entry (struct) – Mother entry fields from mt.; is_female (bool) – True if proband is female.; p_de_novo (float64) – Unfiltered posterior probability; that the event is de novo rather than a missed heterozygous; event in a parent.; confidence (str) Validation confidence. One of: 'HIGH',; 'MEDIUM', 'LOW'. The key of the table is ['locus', 'alleles', 'id'].; The model looks for de novo events in which both parents are homozygous; reference and the proband is a heterozygous. The model makes the simplifying; assumption that when this configuration x = (AA, AA, AB) of calls; occurs, exactly one of the following is true:. d: a de novo mutation occurred in the proband and all calls are; accurate.; m: at least one parental allele is actually heterozygous and; the proband call is accurate. We can then estimate the posterior probability of a de novo mutation as:. \[\mathrm{P_{\text{de novo}}} = \frac{\mathrm{P}(d \mid x)}{\mathrm{P}(d \mid x) + \mathrm{P}(m \mid x)}\]; Applying Bayes rule to the numerator and denominator yields. \[\frac{\mathrm{P}(x \mid d)\,\mathrm{P}(d)}{\mathrm{P}(x \mid d)\,\mathrm{P}(d) +; \mathrm{P}(x \mid m)\,\mathrm{P}(m)}\]; The prior on de novo mutation is estimated from the rate in the literature:. \[\mathrm{P}(d) = \frac{1 \, \text{mutation}}{30{,}000{,}000 \, \text{bases}}\]; The prior used for at least one alternate allele between the parents; depends on the alternate allele frequency:. \[\mathrm{P}(m) = 1 - (1 - AF)^4\]; The likelihoods \(\mathrm{P}(x \mid d)\) and \(\mathrm{P}(x \mid m)\); are computed",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:56963,Modifiability,variab,variables,56963,"dot \mathrm{P}(x_{\mathrm{mother}} = AA \mid \mathrm{mother} = AA) \\; {} + {} &\mathrm{P}(x_{\mathrm{father}} = AA \mid \mathrm{father} = AA); \cdot \mathrm{P}(x_{\mathrm{mother}} = AA \mid \mathrm{mother} = AB); \end{aligned}; \right) \\; &{} \cdot \mathrm{P}(x_{\mathrm{proband}} = AB \mid \mathrm{proband} = AB); \end{aligned}. \]; (Technically, the second factorization assumes there is exactly (rather; than at least) one alternate allele among the parents, which may be; justified on the grounds that it is typically the most likely case by far.); While this posterior probability is a good metric for grouping putative de; novo mutations by validation likelihood, there exist error modes in; high-throughput sequencing data that are not appropriately accounted for by; the phred-scaled genotype likelihoods. To this end, a number of hard filters; are applied in order to assign validation likelihood.; These filters are different for SNPs and insertions/deletions. In the below; rules, the following variables are used:. DR refers to the ratio of the read depth in the proband to the; combined read depth in the parents.; DP refers to the read depth (DP field) of the proband.; AB refers to the read allele balance of the proband (number of; alternate reads divided by total reads).; AC refers to the count of alternate alleles across all individuals; in the dataset at the site.; p refers to \(\mathrm{P_{\text{de novo}}}\).; min_p refers to the min_p function parameter. HIGH-quality SNV:; (p > 0.99) AND (AB > 0.3) AND (AC == 1); OR; (p > 0.99) AND (AB > 0.3) AND (DR > 0.2); OR; (p > 0.5) AND (AB > 0.3) AND (AC < 10) AND (DP > 10). MEDIUM-quality SNV:; (p > 0.5) AND (AB > 0.3); OR; (AC == 1). LOW-quality SNV:; (AB > 0.2). HIGH-quality indel:; (p > 0.99) AND (AB > 0.3) AND (AC == 1). MEDIUM-quality indel:; (p > 0.5) AND (AB > 0.3) AND (AC < 10). LOW-quality indel:; (AB > 0.2). Additionally, de novo candidates are not considered if the proband GQ is; smaller than the min_gq paramete",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:58965,Modifiability,config,config,58965,"han the min_child_ab parameter, if the depth ratio between the; proband and parents is smaller than the min_depth_ratio parameter, if; the allele balance in a parent is above the max_parent_ab parameter, or; if the posterior probability p is smaller than the min_p parameter. Parameters:. mt (MatrixTable) – High-throughput sequencing dataset.; pedigree (Pedigree) – Sample pedigree.; pop_frequency_prior (Float64Expression) – Expression for population alternate allele frequency prior.; min_gq – Minimum proband GQ to be considered for de novo calling.; min_p – Minimum posterior probability to be considered for de novo calling.; max_parent_ab – Maximum parent allele balance.; min_child_ab – Minimum proband allele balance/; min_dp_ratio – Minimum ratio between proband read depth and parental read depth.; ignore_in_sample_allele_frequency – Ignore in-sample allele frequency in computing site prior. Experimental. Returns:; Table. hail.methods.nirvana(dataset, config, block_size=500000, name='nirvana')[source]; Annotate variants using Nirvana. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). nirvana() runs Nirvana on the current dataset and adds a; new row field in the location specified by name.; Examples; Add Nirvana annotations to the dataset:; >>> result = hl.nirvana(dataset, ""data/nirvana.properties"") . Configuration; nirvana() requires a configuration file. The format is a; .properties file, where each; line defines a property as a key-value pair of the form key = value.; nirvana() supports the following properties:. hail.nirvana.dotnet – Location of dotnet. Optional, default: dotnet.; hail.nirvana.path – Value of the PATH environment variable when; invoking Nirvana. Optional, by default PATH is not set.; hail.nirvana.location – Location of Nirvana.dll. Required.; hail.nirva",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:59554,Modifiability,config,configuration,59554,"alling.; min_p – Minimum posterior probability to be considered for de novo calling.; max_parent_ab – Maximum parent allele balance.; min_child_ab – Minimum proband allele balance/; min_dp_ratio – Minimum ratio between proband read depth and parental read depth.; ignore_in_sample_allele_frequency – Ignore in-sample allele frequency in computing site prior. Experimental. Returns:; Table. hail.methods.nirvana(dataset, config, block_size=500000, name='nirvana')[source]; Annotate variants using Nirvana. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). nirvana() runs Nirvana on the current dataset and adds a; new row field in the location specified by name.; Examples; Add Nirvana annotations to the dataset:; >>> result = hl.nirvana(dataset, ""data/nirvana.properties"") . Configuration; nirvana() requires a configuration file. The format is a; .properties file, where each; line defines a property as a key-value pair of the form key = value.; nirvana() supports the following properties:. hail.nirvana.dotnet – Location of dotnet. Optional, default: dotnet.; hail.nirvana.path – Value of the PATH environment variable when; invoking Nirvana. Optional, by default PATH is not set.; hail.nirvana.location – Location of Nirvana.dll. Required.; hail.nirvana.reference – Location of reference genome. Required.; hail.nirvana.cache – Location of cache. Required.; hail.nirvana.supplementaryAnnotationDirectory – Location of; Supplementary Database. Optional, no supplementary database by default. Here is an example nirvana.properties configuration file:; hail.nirvana.location = /path/to/dotnet/netcoreapp2.0/Nirvana.dll; hail.nirvana.reference = /path/to/nirvana/References/Homo_sapiens.GRCh37.Nirvana.dat; hail.nirvana.cache = /path/to/nirvana/Cache/GRCh37/Ensembl; hail.nirvana.supplementaryAnnotati",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:59857,Modifiability,variab,variable,59857,"-sample allele frequency in computing site prior. Experimental. Returns:; Table. hail.methods.nirvana(dataset, config, block_size=500000, name='nirvana')[source]; Annotate variants using Nirvana. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). nirvana() runs Nirvana on the current dataset and adds a; new row field in the location specified by name.; Examples; Add Nirvana annotations to the dataset:; >>> result = hl.nirvana(dataset, ""data/nirvana.properties"") . Configuration; nirvana() requires a configuration file. The format is a; .properties file, where each; line defines a property as a key-value pair of the form key = value.; nirvana() supports the following properties:. hail.nirvana.dotnet – Location of dotnet. Optional, default: dotnet.; hail.nirvana.path – Value of the PATH environment variable when; invoking Nirvana. Optional, by default PATH is not set.; hail.nirvana.location – Location of Nirvana.dll. Required.; hail.nirvana.reference – Location of reference genome. Required.; hail.nirvana.cache – Location of cache. Required.; hail.nirvana.supplementaryAnnotationDirectory – Location of; Supplementary Database. Optional, no supplementary database by default. Here is an example nirvana.properties configuration file:; hail.nirvana.location = /path/to/dotnet/netcoreapp2.0/Nirvana.dll; hail.nirvana.reference = /path/to/nirvana/References/Homo_sapiens.GRCh37.Nirvana.dat; hail.nirvana.cache = /path/to/nirvana/Cache/GRCh37/Ensembl; hail.nirvana.supplementaryAnnotationDirectory = /path/to/nirvana/SupplementaryDatabase/GRCh37. Annotations; A new row field is added in the location specified by name with the; following schema:; struct {; chromosome: str,; refAllele: str,; position: int32,; altAlleles: array<str>,; cytogeneticBand: str,; quality: float64,; filters: array<str",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:60277,Modifiability,config,configuration,60277,"ay of tstr). nirvana() runs Nirvana on the current dataset and adds a; new row field in the location specified by name.; Examples; Add Nirvana annotations to the dataset:; >>> result = hl.nirvana(dataset, ""data/nirvana.properties"") . Configuration; nirvana() requires a configuration file. The format is a; .properties file, where each; line defines a property as a key-value pair of the form key = value.; nirvana() supports the following properties:. hail.nirvana.dotnet – Location of dotnet. Optional, default: dotnet.; hail.nirvana.path – Value of the PATH environment variable when; invoking Nirvana. Optional, by default PATH is not set.; hail.nirvana.location – Location of Nirvana.dll. Required.; hail.nirvana.reference – Location of reference genome. Required.; hail.nirvana.cache – Location of cache. Required.; hail.nirvana.supplementaryAnnotationDirectory – Location of; Supplementary Database. Optional, no supplementary database by default. Here is an example nirvana.properties configuration file:; hail.nirvana.location = /path/to/dotnet/netcoreapp2.0/Nirvana.dll; hail.nirvana.reference = /path/to/nirvana/References/Homo_sapiens.GRCh37.Nirvana.dat; hail.nirvana.cache = /path/to/nirvana/Cache/GRCh37/Ensembl; hail.nirvana.supplementaryAnnotationDirectory = /path/to/nirvana/SupplementaryDatabase/GRCh37. Annotations; A new row field is added in the location specified by name with the; following schema:; struct {; chromosome: str,; refAllele: str,; position: int32,; altAlleles: array<str>,; cytogeneticBand: str,; quality: float64,; filters: array<str>,; jointSomaticNormalQuality: int32,; copyNumber: int32,; strandBias: float64,; recalibratedQuality: float64,; variants: array<struct {; altAllele: str,; refAllele: str,; chromosome: str,; begin: int32,; end: int32,; phylopScore: float64,; isReferenceMinor: bool,; variantType: str,; vid: str,; hgvsg: str,; isRecomposedVariant: bool,; isDecomposedVariant: bool,; regulatoryRegions: array<struct {; id: str,; type: str,; conseque",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:64845,Modifiability,inherit,inheritance,64845,"Hc: int32,; asjAf: float64,; asjAc: int32,; asjAn: int32,; asjHc: int32,; sasAf: float64,; sasAc: int32,; sasAn: int32,; sasHc: int32,; failedFilter: bool; },; topmed: struct {; failedFilter: bool,; allAc: int32,; allAn: int32,; allAf: float64,; allHc: int32; },; oneKg: struct {; ancestralAllele: str,; allAf: float64,; allAc: int32,; allAn: int32,; afrAf: float64,; afrAc: int32,; afrAn: int32,; amrAf: float64,; amrAc: int32,; amrAn: int32,; easAf: float64,; easAc: int32,; easAn: int32,; eurAf: float64,; eurAc: int32,; eurAn: int32,; sasAf: float64,; sasAc: int32,; sasAn: int32; },; mitomap: array<struct {; refAllele: str,; altAllele: str,; diseases : array<str>,; hasHomoplasmy: bool,; hasHeteroplasmy: bool,; status: str,; clinicalSignificance: str,; scorePercentile: float64,; isAlleleSpecific: bool,; chromosome: str,; begin: int32,; end: int32,; variantType: str; }; transcripts: struct {; refSeq: array<struct {; transcript: str,; bioType: str,; aminoAcids: str,; cdnaPos: str,; codons: str,; cdsPos: str,; exons: str,; introns: str,; geneId: str,; hgnc: str,; consequence: array<str>,; hgvsc: str,; hgvsp: str,; isCanonical: bool,; polyPhenScore: float64,; polyPhenPrediction: str,; proteinId: str,; proteinPos: str,; siftScore: float64,; siftPrediction: str; }>,; ensembl: array<struct {; transcript: str,; bioType: str,; aminoAcids: str,; cdnaPos: str,; codons: str,; cdsPos: str,; exons: str,; introns: str,; geneId: str,; hgnc: str,; consequence: array<str>,; hgvsc: str,; hgvsp: str,; isCanonical: bool,; polyPhenScore: float64,; polyPhenPrediction: str,; proteinId: str,; proteinPos: str,; siftScore: float64,; siftPrediction: str; }>; },; overlappingGenes: array<str>; }>; genes: array<struct {; name: str,; omim: array<struct {; mimNumber: int32,; hgnc: str,; description: str,; phenotypes: array<struct {; mimNumber: int32,; phenotype: str,; mapping: str,; inheritance: array<str>,; comments: str; }>; }>; exac: struct {; pLi: float64,; pRec: float64,; pNull: float64; }; }>; }.",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:65023,Modifiability,config,config,65023,"nce: array<str>,; hgvsc: str,; hgvsp: str,; isCanonical: bool,; polyPhenScore: float64,; polyPhenPrediction: str,; proteinId: str,; proteinPos: str,; siftScore: float64,; siftPrediction: str; }>,; ensembl: array<struct {; transcript: str,; bioType: str,; aminoAcids: str,; cdnaPos: str,; codons: str,; cdsPos: str,; exons: str,; introns: str,; geneId: str,; hgnc: str,; consequence: array<str>,; hgvsc: str,; hgvsp: str,; isCanonical: bool,; polyPhenScore: float64,; polyPhenPrediction: str,; proteinId: str,; proteinPos: str,; siftScore: float64,; siftPrediction: str; }>; },; overlappingGenes: array<str>; }>; genes: array<struct {; name: str,; omim: array<struct {; mimNumber: int32,; hgnc: str,; description: str,; phenotypes: array<struct {; mimNumber: int32,; phenotype: str,; mapping: str,; inheritance: array<str>,; comments: str; }>; }>; exac: struct {; pLi: float64,; pRec: float64,; pNull: float64; }; }>; }. Parameters:. dataset (MatrixTable or Table) – Dataset.; config (str) – Path to Nirvana configuration file.; block_size (int) – Number of rows to process per Nirvana invocation.; name (str) – Name for resulting row field. Returns:; MatrixTable or Table – Dataset with new row-indexed field name containing Nirvana annotations. hail.methods.sample_qc(mt, name='sample_qc')[source]; Compute per-sample metrics useful for quality control. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). Examples; Compute sample QC metrics and remove low-quality samples:; >>> dataset = hl.sample_qc(dataset, name='sample_qc'); >>> filtered_dataset = dataset.filter_cols((dataset.sample_qc.dp_stats.mean > 20) & (dataset.sample_qc.r_ti_tv > 1.5)). Notes; This method computes summary statistics per sample from a genetic matrix and stores; the results as a new column-indexed struct field in the matrix, named based on the; name parameter.; If mt contains an entry field DP of type tint32, then the; field dp_stats is computed. If mt contai",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:65054,Modifiability,config,configuration,65054,"nce: array<str>,; hgvsc: str,; hgvsp: str,; isCanonical: bool,; polyPhenScore: float64,; polyPhenPrediction: str,; proteinId: str,; proteinPos: str,; siftScore: float64,; siftPrediction: str; }>,; ensembl: array<struct {; transcript: str,; bioType: str,; aminoAcids: str,; cdnaPos: str,; codons: str,; cdsPos: str,; exons: str,; introns: str,; geneId: str,; hgnc: str,; consequence: array<str>,; hgvsc: str,; hgvsp: str,; isCanonical: bool,; polyPhenScore: float64,; polyPhenPrediction: str,; proteinId: str,; proteinPos: str,; siftScore: float64,; siftPrediction: str; }>; },; overlappingGenes: array<str>; }>; genes: array<struct {; name: str,; omim: array<struct {; mimNumber: int32,; hgnc: str,; description: str,; phenotypes: array<struct {; mimNumber: int32,; phenotype: str,; mapping: str,; inheritance: array<str>,; comments: str; }>; }>; exac: struct {; pLi: float64,; pRec: float64,; pNull: float64; }; }>; }. Parameters:. dataset (MatrixTable or Table) – Dataset.; config (str) – Path to Nirvana configuration file.; block_size (int) – Number of rows to process per Nirvana invocation.; name (str) – Name for resulting row field. Returns:; MatrixTable or Table – Dataset with new row-indexed field name containing Nirvana annotations. hail.methods.sample_qc(mt, name='sample_qc')[source]; Compute per-sample metrics useful for quality control. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). Examples; Compute sample QC metrics and remove low-quality samples:; >>> dataset = hl.sample_qc(dataset, name='sample_qc'); >>> filtered_dataset = dataset.filter_cols((dataset.sample_qc.dp_stats.mean > 20) & (dataset.sample_qc.r_ti_tv > 1.5)). Notes; This method computes summary statistics per sample from a genetic matrix and stores; the results as a new column-indexed struct field in the matrix, named based on the; name parameter.; If mt contains an entry field DP of type tint32, then the; field dp_stats is computed. If mt contai",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:75673,Modifiability,variab,variable,75673," | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | NA | NA | NA |; | 1 | 9 | 1.39e+02 | 1.82e-03 | 0 |; +-------+-------+----------+----------+-------+. Notes; In the SKAT R package, the “weights” are actually the square root of the weight expression; from the paper. This method uses the definition from the paper.; The paper includes an explicit intercept term but this method expects the user to specify the; intercept as an extra covariate with the value 1.; This method does not perform small sample size correction.; The q_stat return value is not the \(Q\) statistic from the paper. We match the output; of the SKAT R package which returns \(\tilde{Q}\):. \[\tilde{Q} = \frac{Q}{2}\]. Parameters:. group (Expression) – Row-indexed expression indicating to which group a variant belongs. This is typically a gene; name or an interval.; weight (Float64Expression) – Row-indexed expression for weights. Must be non-negative.; y (Float64Expression) – Column-indexed response (dependent variable) expression.; x (Float64Expression) – Entry-indexed expression for input (independent variable).; covariates (list of Float64Expression) – List of column-indexed covariate expressions. You must explicitly provide an intercept term; if desired. You must provide at least one covariate.; max_size (int) – Maximum size of group on which to run the test. Groups which exceed this size will have a; missing p-value and missing q statistic. Defaults to 46340.; null_max_iterations (int) – The maximum number of iterations when fitting the logistic null model. Defaults to 25.; null_tolerance (float) – The null model logisitic regression converges when the errors is less than this. Defaults to; 1e-6.; accuracy (float) – The accuracy of the p-value if fault value is zero. Defaults to 1e-6.; iterations (int) – The maximum number of iterations used to calculate the p-value (which has no closed; form). Defaults to 1e5. Returns:; Table – One row per-group. The key is group. T",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:75768,Modifiability,variab,variable,75768,"A | NA | NA |; | 1 | 9 | 1.39e+02 | 1.82e-03 | 0 |; +-------+-------+----------+----------+-------+. Notes; In the SKAT R package, the “weights” are actually the square root of the weight expression; from the paper. This method uses the definition from the paper.; The paper includes an explicit intercept term but this method expects the user to specify the; intercept as an extra covariate with the value 1.; This method does not perform small sample size correction.; The q_stat return value is not the \(Q\) statistic from the paper. We match the output; of the SKAT R package which returns \(\tilde{Q}\):. \[\tilde{Q} = \frac{Q}{2}\]. Parameters:. group (Expression) – Row-indexed expression indicating to which group a variant belongs. This is typically a gene; name or an interval.; weight (Float64Expression) – Row-indexed expression for weights. Must be non-negative.; y (Float64Expression) – Column-indexed response (dependent variable) expression.; x (Float64Expression) – Entry-indexed expression for input (independent variable).; covariates (list of Float64Expression) – List of column-indexed covariate expressions. You must explicitly provide an intercept term; if desired. You must provide at least one covariate.; max_size (int) – Maximum size of group on which to run the test. Groups which exceed this size will have a; missing p-value and missing q statistic. Defaults to 46340.; null_max_iterations (int) – The maximum number of iterations when fitting the logistic null model. Defaults to 25.; null_tolerance (float) – The null model logisitic regression converges when the errors is less than this. Defaults to; 1e-6.; accuracy (float) – The accuracy of the p-value if fault value is zero. Defaults to 1e-6.; iterations (int) – The maximum number of iterations used to calculate the p-value (which has no closed; form). Defaults to 1e5. Returns:; Table – One row per-group. The key is group. The row fields are:. group : the group parameter.; size : tint64, the number of vari",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:81976,Modifiability,variab,variable,81976," package also applies a; “small-sample adjustment” to the null distribution in the logistic case; when the sample size is less than 2000. Hail does not apply this; adjustment.; The fault flag is an integer indicating whether any issues occurred when; running the Davies algorithm to compute the p-value as the right tail of a; weighted sum of \(\chi^2(1)\) distributions. fault value; Description. 0; no issues. 1; accuracy NOT achieved. 2; round-off error possibly significant. 3; invalid parameters. 4; unable to locate integration parameters. 5; out of memory. Parameters:. key_expr (Expression) – Row-indexed expression for key associated to each row.; weight_expr (Float64Expression) – Row-indexed expression for row weights.; y (Float64Expression) – Column-indexed response expression.; If logistic is True, all non-missing values must evaluate to 0 or; 1. Note that a BooleanExpression will be implicitly converted; to a Float64Expression with this property.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – List of column-indexed covariate expressions.; logistic (bool or tuple of int and float) – If false, use the linear test. If true, use the logistic test with no; more than 25 logistic iterations and a convergence tolerance of 1e-6. If; a tuple is given, use the logistic test with the tuple elements as the; maximum nubmer of iterations and convergence tolerance, respectively.; max_size (int) – Maximum size of group on which to run the test.; accuracy (float) – Accuracy achieved by the Davies algorithm if fault value is zero.; iterations (int) – Maximum number of iterations attempted by the Davies algorithm. Returns:; Table – Table of SKAT results. hail.methods.lambda_gc(p_value, approximate=True)[source]; Compute genomic inflation factor (lambda GC) from an Expression of p-values. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; ",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:91669,Modifiability,variab,variable-length,91669,">>> hl.export_vcf(split_ds, 'output/export.vcf') . The info field AC in data/export.vcf will have Number=1.; New Fields; split_multi_hts() adds the following fields:. was_split (bool) – True if this variant was originally; multiallelic, otherwise False.; a_index (int) – The original index of this alternate allele in the; multiallelic representation (NB: 1 is the first alternate allele or the; only alternate allele in a biallelic variant). For example, 1:100:A:T,C; splits into two variants: 1:100:A:T with a_index = 1 and 1:100:A:C; with a_index = 2. See also; split_multi(). Parameters:. ds (MatrixTable or Table) – An unsplit dataset.; keep_star (bool) – Do not filter out * alleles.; left_aligned (bool) – If True, variants are assumed to be left; aligned and have unique loci. This avoids a shuffle. If the assumption; is violated, an error is generated.; vep_root (str) – Top-level location of vep data. All variable-length VEP fields; (intergenic_consequences, motif_feature_consequences,; regulatory_feature_consequences, and transcript_consequences); will be split properly (i.e. a_index corresponding to the VEP allele_num).; permit_shuffle (bool) – If True, permit a data shuffle to sort out-of-order split results.; This will only be required if input data has duplicate loci, one of; which contains more than one alternate allele. Returns:; MatrixTable or Table – A biallelic variant dataset. hail.methods.summarize_variants(mt, show=True, *, handler=None)[source]; Summarize the variants present in a dataset and print the results.; Examples; >>> hl.summarize_variants(dataset) ; ==============================; Number of variants: 346; ==============================; Alleles per variant; -------------------; 2 alleles: 346 variants; ==============================; Variants per contig; -------------------; 20: 346 variants; ==============================; Allele type distribution; ------------------------; SNP: 301 alleles; Deletion: 27 alleles; Insertion: 18 alleles; ========",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:95604,Modifiability,config,configurations,95604,"7262169 | [""T"",""C""] | NA | NA | NA | NA |; +---------------+------------+-------+-------+----------+----------+. Export variants with p-values below 0.001:; >>> tdt_table = tdt_table.filter(tdt_table.p_value < 0.001); >>> tdt_table.export(f""output/tdt_results.tsv""). Notes; The; transmission disequilibrium test; compares the number of times the alternate allele is transmitted (t) versus; not transmitted (u) from a heterozgyous parent to an affected child. The null; hypothesis holds that each case is equally likely. The TDT statistic is given by. \[(t - u)^2 \over (t + u)\]; and asymptotically follows a chi-squared distribution with one degree of; freedom under the null hypothesis.; transmission_disequilibrium_test() only considers complete trios (two; parents and a proband with defined sex) and only returns results for the; autosome, as defined by in_autosome(), and; chromosome X. Transmissions and non-transmissions are counted only for the; configurations of genotypes and copy state in the table below, in order to; filter out Mendel errors and configurations where transmission is; guaranteed. The copy state of a locus with respect to a trio is defined as; follows:. Auto – in autosome or in PAR of X or female child; HemiX – in non-PAR of X and male child. Here PAR is the pseudoautosomal region; of X and Y defined by ReferenceGenome, which many variant callers; map to chromosome X. Kid; Dad; Mom; Copy State; t; u. HomRef; Het; Het; Auto; 0; 2. HomRef; HomRef; Het; Auto; 0; 1. HomRef; Het; HomRef; Auto; 0; 1. Het; Het; Het; Auto; 1; 1. Het; HomRef; Het; Auto; 1; 0. Het; Het; HomRef; Auto; 1; 0. Het; HomVar; Het; Auto; 0; 1. Het; Het; HomVar; Auto; 0; 1. HomVar; Het; Het; Auto; 2; 0. HomVar; Het; HomVar; Auto; 1; 0. HomVar; HomVar; Het; Auto; 1; 0. HomRef; HomRef; Het; HemiX; 0; 1. HomRef; HomVar; Het; HemiX; 0; 1. HomVar; HomRef; Het; HemiX; 1; 0. HomVar; HomVar; Het; HemiX; 1; 0. transmission_disequilibrium_test() produces a table with the following columns:. locus (tl",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:95709,Modifiability,config,configurations,95709,"7262169 | [""T"",""C""] | NA | NA | NA | NA |; +---------------+------------+-------+-------+----------+----------+. Export variants with p-values below 0.001:; >>> tdt_table = tdt_table.filter(tdt_table.p_value < 0.001); >>> tdt_table.export(f""output/tdt_results.tsv""). Notes; The; transmission disequilibrium test; compares the number of times the alternate allele is transmitted (t) versus; not transmitted (u) from a heterozgyous parent to an affected child. The null; hypothesis holds that each case is equally likely. The TDT statistic is given by. \[(t - u)^2 \over (t + u)\]; and asymptotically follows a chi-squared distribution with one degree of; freedom under the null hypothesis.; transmission_disequilibrium_test() only considers complete trios (two; parents and a proband with defined sex) and only returns results for the; autosome, as defined by in_autosome(), and; chromosome X. Transmissions and non-transmissions are counted only for the; configurations of genotypes and copy state in the table below, in order to; filter out Mendel errors and configurations where transmission is; guaranteed. The copy state of a locus with respect to a trio is defined as; follows:. Auto – in autosome or in PAR of X or female child; HemiX – in non-PAR of X and male child. Here PAR is the pseudoautosomal region; of X and Y defined by ReferenceGenome, which many variant callers; map to chromosome X. Kid; Dad; Mom; Copy State; t; u. HomRef; Het; Het; Auto; 0; 2. HomRef; HomRef; Het; Auto; 0; 1. HomRef; Het; HomRef; Auto; 0; 1. Het; Het; Het; Auto; 1; 1. Het; HomRef; Het; Auto; 1; 0. Het; Het; HomRef; Auto; 1; 0. Het; HomVar; Het; Auto; 0; 1. Het; Het; HomVar; Auto; 0; 1. HomVar; Het; Het; Auto; 2; 0. HomVar; Het; HomVar; Auto; 1; 0. HomVar; HomVar; Het; Auto; 1; 0. HomRef; HomRef; Het; HemiX; 0; 1. HomRef; HomVar; Het; HemiX; 0; 1. HomVar; HomRef; Het; HemiX; 1; 0. HomVar; HomVar; Het; HemiX; 1; 0. transmission_disequilibrium_test() produces a table with the following columns:. locus (tl",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:101180,Modifiability,config,config,101180,"req_hwe (float64) – Expected frequency of heterozygous; samples under Hardy-Weinberg equilibrium. See; functions.hardy_weinberg_test() for details.; p_value_hwe (float64) – p-value from two-sided test of Hardy-Weinberg; equilibrium. See functions.hardy_weinberg_test() for details.; p_value_excess_het (float64) – p-value from one-sided test of; Hardy-Weinberg equilibrium for excess heterozygosity.; See functions.hardy_weinberg_test() for details. Warning; het_freq_hwe and p_value_hwe are calculated as in; functions.hardy_weinberg_test(), with non-diploid calls; (ploidy != 2) ignored in the counts. As this test is only; statistically rigorous in the biallelic setting, variant_qc(); sets both fields to missing for multiallelic variants. Consider using; split_multi() to split multi-allelic variants beforehand. Parameters:. mt (MatrixTable) – Dataset.; name (str) – Name for resulting field. Returns:; MatrixTable. hail.methods.vep(dataset, config=None, block_size=1000, name='vep', csq=False, tolerate_parse_error=False)[source]; Annotate variants with VEP. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). vep() runs Variant Effect Predictor on the; current dataset and adds the result as a row field.; Examples; Add VEP annotations to the dataset:; >>> result = hl.vep(dataset, ""data/vep-configuration.json"") . Notes; Installation; This VEP command only works if you have already installed VEP on your; computing environment. If you use hailctl dataproc to start Hail clusters,; installing VEP is achieved by specifying the –vep flag. For more detailed instructions,; see Variant Effect Predictor (VEP). If you use hailctl hdinsight, see Variant Effect Predictor (VEP).; Spark Configuration; vep() needs a configuration file to tell it how to run VEP. This is the config argument; to the VEP function. If you are using hailctl dataproc as mentioned above, you can just use the; default argument for config and everything will work.",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:101590,Modifiability,config,configuration,101590,"einberg equilibrium for excess heterozygosity.; See functions.hardy_weinberg_test() for details. Warning; het_freq_hwe and p_value_hwe are calculated as in; functions.hardy_weinberg_test(), with non-diploid calls; (ploidy != 2) ignored in the counts. As this test is only; statistically rigorous in the biallelic setting, variant_qc(); sets both fields to missing for multiallelic variants. Consider using; split_multi() to split multi-allelic variants beforehand. Parameters:. mt (MatrixTable) – Dataset.; name (str) – Name for resulting field. Returns:; MatrixTable. hail.methods.vep(dataset, config=None, block_size=1000, name='vep', csq=False, tolerate_parse_error=False)[source]; Annotate variants with VEP. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). vep() runs Variant Effect Predictor on the; current dataset and adds the result as a row field.; Examples; Add VEP annotations to the dataset:; >>> result = hl.vep(dataset, ""data/vep-configuration.json"") . Notes; Installation; This VEP command only works if you have already installed VEP on your; computing environment. If you use hailctl dataproc to start Hail clusters,; installing VEP is achieved by specifying the –vep flag. For more detailed instructions,; see Variant Effect Predictor (VEP). If you use hailctl hdinsight, see Variant Effect Predictor (VEP).; Spark Configuration; vep() needs a configuration file to tell it how to run VEP. This is the config argument; to the VEP function. If you are using hailctl dataproc as mentioned above, you can just use the; default argument for config and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below.; The format of the configuration file is JSON, and vep(); expects a JSON object with three fields:. command (array of string) – The VEP command line to run. The string literal __OUTPUT_FORMAT_FLAG__ is replaced with –json or –vcf depending on csq.; env (ob",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:102008,Modifiability,config,configuration,102008,"it multi-allelic variants beforehand. Parameters:. mt (MatrixTable) – Dataset.; name (str) – Name for resulting field. Returns:; MatrixTable. hail.methods.vep(dataset, config=None, block_size=1000, name='vep', csq=False, tolerate_parse_error=False)[source]; Annotate variants with VEP. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). vep() runs Variant Effect Predictor on the; current dataset and adds the result as a row field.; Examples; Add VEP annotations to the dataset:; >>> result = hl.vep(dataset, ""data/vep-configuration.json"") . Notes; Installation; This VEP command only works if you have already installed VEP on your; computing environment. If you use hailctl dataproc to start Hail clusters,; installing VEP is achieved by specifying the –vep flag. For more detailed instructions,; see Variant Effect Predictor (VEP). If you use hailctl hdinsight, see Variant Effect Predictor (VEP).; Spark Configuration; vep() needs a configuration file to tell it how to run VEP. This is the config argument; to the VEP function. If you are using hailctl dataproc as mentioned above, you can just use the; default argument for config and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below.; The format of the configuration file is JSON, and vep(); expects a JSON object with three fields:. command (array of string) – The VEP command line to run. The string literal __OUTPUT_FORMAT_FLAG__ is replaced with –json or –vcf depending on csq.; env (object) – A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; vep_json_schema (string): The type of the VEP JSON schema (as produced by the VEP when invoked with the –json option). Note: This is the old-style ‘parseable’ Hail type syntax. This will change. Here is an example configuration file for invoking VEP release 85; installed i",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:102066,Modifiability,config,config,102066,") – Dataset.; name (str) – Name for resulting field. Returns:; MatrixTable. hail.methods.vep(dataset, config=None, block_size=1000, name='vep', csq=False, tolerate_parse_error=False)[source]; Annotate variants with VEP. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). vep() runs Variant Effect Predictor on the; current dataset and adds the result as a row field.; Examples; Add VEP annotations to the dataset:; >>> result = hl.vep(dataset, ""data/vep-configuration.json"") . Notes; Installation; This VEP command only works if you have already installed VEP on your; computing environment. If you use hailctl dataproc to start Hail clusters,; installing VEP is achieved by specifying the –vep flag. For more detailed instructions,; see Variant Effect Predictor (VEP). If you use hailctl hdinsight, see Variant Effect Predictor (VEP).; Spark Configuration; vep() needs a configuration file to tell it how to run VEP. This is the config argument; to the VEP function. If you are using hailctl dataproc as mentioned above, you can just use the; default argument for config and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below.; The format of the configuration file is JSON, and vep(); expects a JSON object with three fields:. command (array of string) – The VEP command line to run. The string literal __OUTPUT_FORMAT_FLAG__ is replaced with –json or –vcf depending on csq.; env (object) – A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; vep_json_schema (string): The type of the VEP JSON schema (as produced by the VEP when invoked with the –json option). Note: This is the old-style ‘parseable’ Hail type syntax. This will change. Here is an example configuration file for invoking VEP release 85; installed in /vep with the Loftee plugin:; {; ""command"": [; ""/vep"",; ""--forma",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:102201,Modifiability,config,config,102201,"ep(dataset, config=None, block_size=1000, name='vep', csq=False, tolerate_parse_error=False)[source]; Annotate variants with VEP. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). vep() runs Variant Effect Predictor on the; current dataset and adds the result as a row field.; Examples; Add VEP annotations to the dataset:; >>> result = hl.vep(dataset, ""data/vep-configuration.json"") . Notes; Installation; This VEP command only works if you have already installed VEP on your; computing environment. If you use hailctl dataproc to start Hail clusters,; installing VEP is achieved by specifying the –vep flag. For more detailed instructions,; see Variant Effect Predictor (VEP). If you use hailctl hdinsight, see Variant Effect Predictor (VEP).; Spark Configuration; vep() needs a configuration file to tell it how to run VEP. This is the config argument; to the VEP function. If you are using hailctl dataproc as mentioned above, you can just use the; default argument for config and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below.; The format of the configuration file is JSON, and vep(); expects a JSON object with three fields:. command (array of string) – The VEP command line to run. The string literal __OUTPUT_FORMAT_FLAG__ is replaced with –json or –vcf depending on csq.; env (object) – A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; vep_json_schema (string): The type of the VEP JSON schema (as produced by the VEP when invoked with the –json option). Note: This is the old-style ‘parseable’ Hail type syntax. This will change. Here is an example configuration file for invoking VEP release 85; installed in /vep with the Loftee plugin:; {; ""command"": [; ""/vep"",; ""--format"", ""vcf"",; ""__OUTPUT_FORMAT_FLAG__"",; ""--everything"",; ""--allele_number"",; ""--no_stats"",;",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:102349,Modifiability,config,configuration,102349,"eles (type tarray of tstr). vep() runs Variant Effect Predictor on the; current dataset and adds the result as a row field.; Examples; Add VEP annotations to the dataset:; >>> result = hl.vep(dataset, ""data/vep-configuration.json"") . Notes; Installation; This VEP command only works if you have already installed VEP on your; computing environment. If you use hailctl dataproc to start Hail clusters,; installing VEP is achieved by specifying the –vep flag. For more detailed instructions,; see Variant Effect Predictor (VEP). If you use hailctl hdinsight, see Variant Effect Predictor (VEP).; Spark Configuration; vep() needs a configuration file to tell it how to run VEP. This is the config argument; to the VEP function. If you are using hailctl dataproc as mentioned above, you can just use the; default argument for config and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below.; The format of the configuration file is JSON, and vep(); expects a JSON object with three fields:. command (array of string) – The VEP command line to run. The string literal __OUTPUT_FORMAT_FLAG__ is replaced with –json or –vcf depending on csq.; env (object) – A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; vep_json_schema (string): The type of the VEP JSON schema (as produced by the VEP when invoked with the –json option). Note: This is the old-style ‘parseable’ Hail type syntax. This will change. Here is an example configuration file for invoking VEP release 85; installed in /vep with the Loftee plugin:; {; ""command"": [; ""/vep"",; ""--format"", ""vcf"",; ""__OUTPUT_FORMAT_FLAG__"",; ""--everything"",; ""--allele_number"",; ""--no_stats"",; ""--cache"", ""--offline"",; ""--minimal"",; ""--assembly"", ""GRCh37"",; ""--plugin"", ""LoF,human_ancestor_fa:/root/.vep/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:/root/.ve",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:102615,Modifiability,variab,variables,102615,"; This VEP command only works if you have already installed VEP on your; computing environment. If you use hailctl dataproc to start Hail clusters,; installing VEP is achieved by specifying the –vep flag. For more detailed instructions,; see Variant Effect Predictor (VEP). If you use hailctl hdinsight, see Variant Effect Predictor (VEP).; Spark Configuration; vep() needs a configuration file to tell it how to run VEP. This is the config argument; to the VEP function. If you are using hailctl dataproc as mentioned above, you can just use the; default argument for config and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below.; The format of the configuration file is JSON, and vep(); expects a JSON object with three fields:. command (array of string) – The VEP command line to run. The string literal __OUTPUT_FORMAT_FLAG__ is replaced with –json or –vcf depending on csq.; env (object) – A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; vep_json_schema (string): The type of the VEP JSON schema (as produced by the VEP when invoked with the –json option). Note: This is the old-style ‘parseable’ Hail type syntax. This will change. Here is an example configuration file for invoking VEP release 85; installed in /vep with the Loftee plugin:; {; ""command"": [; ""/vep"",; ""--format"", ""vcf"",; ""__OUTPUT_FORMAT_FLAG__"",; ""--everything"",; ""--allele_number"",; ""--no_stats"",; ""--cache"", ""--offline"",; ""--minimal"",; ""--assembly"", ""GRCh37"",; ""--plugin"", ""LoF,human_ancestor_fa:/root/.vep/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:/root/.vep/loftee_data/phylocsf_gerp.sql,gerp_file:/root/.vep/loftee_data/GERP_scores.final.sorted.txt.gz"",; ""-o"", ""STDOUT""; ],; ""env"": {; ""PERL5LIB"": ""/vep_data/loftee""; },; ""vep_json_schema"": ""Struct{assembly_name:String,allele_string:String,ancestral:String,c",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:102953,Modifiability,config,configuration,102953,"If you are using hailctl dataproc as mentioned above, you can just use the; default argument for config and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below.; The format of the configuration file is JSON, and vep(); expects a JSON object with three fields:. command (array of string) – The VEP command line to run. The string literal __OUTPUT_FORMAT_FLAG__ is replaced with –json or –vcf depending on csq.; env (object) – A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; vep_json_schema (string): The type of the VEP JSON schema (as produced by the VEP when invoked with the –json option). Note: This is the old-style ‘parseable’ Hail type syntax. This will change. Here is an example configuration file for invoking VEP release 85; installed in /vep with the Loftee plugin:; {; ""command"": [; ""/vep"",; ""--format"", ""vcf"",; ""__OUTPUT_FORMAT_FLAG__"",; ""--everything"",; ""--allele_number"",; ""--no_stats"",; ""--cache"", ""--offline"",; ""--minimal"",; ""--assembly"", ""GRCh37"",; ""--plugin"", ""LoF,human_ancestor_fa:/root/.vep/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:/root/.vep/loftee_data/phylocsf_gerp.sql,gerp_file:/root/.vep/loftee_data/GERP_scores.final.sorted.txt.gz"",; ""-o"", ""STDOUT""; ],; ""env"": {; ""PERL5LIB"": ""/vep_data/loftee""; },; ""vep_json_schema"": ""Struct{assembly_name:String,allele_string:String,ancestral:String,colocated_variants:Array[Struct{aa_allele:String,aa_maf:Float64,afr_allele:String,afr_maf:Float64,allele_string:String,amr_allele:String,amr_maf:Float64,clin_sig:Array[String],end:Int32,eas_allele:String,eas_maf:Float64,ea_allele:String,ea_maf:Float64,eur_allele:String,eur_maf:Float64,exac_adj_allele:String,exac_adj_maf:Float64,exac_allele:String,exac_afr_allele:String,exac_afr_maf:Float64,exac_amr_allele:String,exac_amr_maf:Float64,exac_eas_allele:String,exac_eas_maf:",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:103035,Modifiability,plugin,plugin,103035,"If you are using hailctl dataproc as mentioned above, you can just use the; default argument for config and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below.; The format of the configuration file is JSON, and vep(); expects a JSON object with three fields:. command (array of string) – The VEP command line to run. The string literal __OUTPUT_FORMAT_FLAG__ is replaced with –json or –vcf depending on csq.; env (object) – A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; vep_json_schema (string): The type of the VEP JSON schema (as produced by the VEP when invoked with the –json option). Note: This is the old-style ‘parseable’ Hail type syntax. This will change. Here is an example configuration file for invoking VEP release 85; installed in /vep with the Loftee plugin:; {; ""command"": [; ""/vep"",; ""--format"", ""vcf"",; ""__OUTPUT_FORMAT_FLAG__"",; ""--everything"",; ""--allele_number"",; ""--no_stats"",; ""--cache"", ""--offline"",; ""--minimal"",; ""--assembly"", ""GRCh37"",; ""--plugin"", ""LoF,human_ancestor_fa:/root/.vep/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:/root/.vep/loftee_data/phylocsf_gerp.sql,gerp_file:/root/.vep/loftee_data/GERP_scores.final.sorted.txt.gz"",; ""-o"", ""STDOUT""; ],; ""env"": {; ""PERL5LIB"": ""/vep_data/loftee""; },; ""vep_json_schema"": ""Struct{assembly_name:String,allele_string:String,ancestral:String,colocated_variants:Array[Struct{aa_allele:String,aa_maf:Float64,afr_allele:String,afr_maf:Float64,allele_string:String,amr_allele:String,amr_maf:Float64,clin_sig:Array[String],end:Int32,eas_allele:String,eas_maf:Float64,ea_allele:String,ea_maf:Float64,eur_allele:String,eur_maf:Float64,exac_adj_allele:String,exac_adj_maf:Float64,exac_allele:String,exac_afr_allele:String,exac_afr_maf:Float64,exac_amr_allele:String,exac_amr_maf:Float64,exac_eas_allele:String,exac_eas_maf:",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:103236,Modifiability,plugin,plugin,103236,"If you are using hailctl dataproc as mentioned above, you can just use the; default argument for config and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below.; The format of the configuration file is JSON, and vep(); expects a JSON object with three fields:. command (array of string) – The VEP command line to run. The string literal __OUTPUT_FORMAT_FLAG__ is replaced with –json or –vcf depending on csq.; env (object) – A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; vep_json_schema (string): The type of the VEP JSON schema (as produced by the VEP when invoked with the –json option). Note: This is the old-style ‘parseable’ Hail type syntax. This will change. Here is an example configuration file for invoking VEP release 85; installed in /vep with the Loftee plugin:; {; ""command"": [; ""/vep"",; ""--format"", ""vcf"",; ""__OUTPUT_FORMAT_FLAG__"",; ""--everything"",; ""--allele_number"",; ""--no_stats"",; ""--cache"", ""--offline"",; ""--minimal"",; ""--assembly"", ""GRCh37"",; ""--plugin"", ""LoF,human_ancestor_fa:/root/.vep/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:/root/.vep/loftee_data/phylocsf_gerp.sql,gerp_file:/root/.vep/loftee_data/GERP_scores.final.sorted.txt.gz"",; ""-o"", ""STDOUT""; ],; ""env"": {; ""PERL5LIB"": ""/vep_data/loftee""; },; ""vep_json_schema"": ""Struct{assembly_name:String,allele_string:String,ancestral:String,colocated_variants:Array[Struct{aa_allele:String,aa_maf:Float64,afr_allele:String,afr_maf:Float64,allele_string:String,amr_allele:String,amr_maf:Float64,clin_sig:Array[String],end:Int32,eas_allele:String,eas_maf:Float64,ea_allele:String,ea_maf:Float64,eur_allele:String,eur_maf:Float64,exac_adj_allele:String,exac_adj_maf:Float64,exac_allele:String,exac_afr_allele:String,exac_afr_maf:Float64,exac_amr_allele:String,exac_amr_maf:Float64,exac_eas_allele:String,exac_eas_maf:",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:106024,Modifiability,config,configuration,106024,"ring],impact:String,minimised:Int32,regulatory_feature_id:String,variant_allele:String}],seq_region_name:String,start:Int32,strand:Int32,transcript_consequences:Array[Struct{allele_num:Int32,amino_acids:String,biotype:String,canonical:Int32,ccds:String,cdna_start:Int32,cdna_end:Int32,cds_end:Int32,cds_start:Int32,codons:String,consequence_terms:Array[String],distance:Int32,domains:Array[Struct{db:String,name:String}],exon:String,gene_id:String,gene_pheno:Int32,gene_symbol:String,gene_symbol_source:String,hgnc_id:String,hgvsc:String,hgvsp:String,hgvs_offset:Int32,impact:String,intron:String,lof:String,lof_flags:String,lof_filter:String,lof_info:String,minimised:Int32,polyphen_prediction:String,polyphen_score:Float64,protein_end:Int32,protein_start:Int32,protein_id:String,sift_prediction:String,sift_score:Float64,strand:Int32,swissprot:String,transcript_id:String,trembl:String,uniparc:String,variant_allele:String}],variant_class:String}""; }. The configuration files used by``hailctl dataproc`` can be found at the following locations:. GRCh37: gs://hail-us-central1-vep/vep85-loftee-gcloud.json; GRCh38: gs://hail-us-central1-vep/vep95-GRCh38-loftee-gcloud.json. If no config file is specified, this function will check to see if environment variable VEP_CONFIG_URI is set with a path to a config file.; Batch Service Configuration; If no config is specified, Hail will use the user’s Service configuration parameters to find a supported VEP configuration.; However, if you wish to use your own implementation of VEP, then see the documentation for VEPConfig.; Annotations; A new row field is added in the location specified by name with type given; by the type given by the json_vep_schema (if csq is False) or; tarray of tstr (if csq is True).; If csq is True, then the CSQ header string is also added as a global; field with name name + '_csq_header'. Parameters:. dataset (MatrixTable or Table) – Dataset.; config (str or VEPConfig, optional) – Path to VEP configuration file or a VEP",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:106247,Modifiability,config,config,106247,"s:String,cdna_start:Int32,cdna_end:Int32,cds_end:Int32,cds_start:Int32,codons:String,consequence_terms:Array[String],distance:Int32,domains:Array[Struct{db:String,name:String}],exon:String,gene_id:String,gene_pheno:Int32,gene_symbol:String,gene_symbol_source:String,hgnc_id:String,hgvsc:String,hgvsp:String,hgvs_offset:Int32,impact:String,intron:String,lof:String,lof_flags:String,lof_filter:String,lof_info:String,minimised:Int32,polyphen_prediction:String,polyphen_score:Float64,protein_end:Int32,protein_start:Int32,protein_id:String,sift_prediction:String,sift_score:Float64,strand:Int32,swissprot:String,transcript_id:String,trembl:String,uniparc:String,variant_allele:String}],variant_class:String}""; }. The configuration files used by``hailctl dataproc`` can be found at the following locations:. GRCh37: gs://hail-us-central1-vep/vep85-loftee-gcloud.json; GRCh38: gs://hail-us-central1-vep/vep95-GRCh38-loftee-gcloud.json. If no config file is specified, this function will check to see if environment variable VEP_CONFIG_URI is set with a path to a config file.; Batch Service Configuration; If no config is specified, Hail will use the user’s Service configuration parameters to find a supported VEP configuration.; However, if you wish to use your own implementation of VEP, then see the documentation for VEPConfig.; Annotations; A new row field is added in the location specified by name with type given; by the type given by the json_vep_schema (if csq is False) or; tarray of tstr (if csq is True).; If csq is True, then the CSQ header string is also added as a global; field with name name + '_csq_header'. Parameters:. dataset (MatrixTable or Table) – Dataset.; config (str or VEPConfig, optional) – Path to VEP configuration file or a VEPConfig object.; block_size (int) – Number of rows to process per VEP invocation.; name (str) – Name for resulting row field.; csq (bool) – If True, annotates with the VCF CSQ field as a tstr.; If False, annotates as the vep_json_schema.; tolera",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:106320,Modifiability,variab,variable,106320,"s:String,cdna_start:Int32,cdna_end:Int32,cds_end:Int32,cds_start:Int32,codons:String,consequence_terms:Array[String],distance:Int32,domains:Array[Struct{db:String,name:String}],exon:String,gene_id:String,gene_pheno:Int32,gene_symbol:String,gene_symbol_source:String,hgnc_id:String,hgvsc:String,hgvsp:String,hgvs_offset:Int32,impact:String,intron:String,lof:String,lof_flags:String,lof_filter:String,lof_info:String,minimised:Int32,polyphen_prediction:String,polyphen_score:Float64,protein_end:Int32,protein_start:Int32,protein_id:String,sift_prediction:String,sift_score:Float64,strand:Int32,swissprot:String,transcript_id:String,trembl:String,uniparc:String,variant_allele:String}],variant_class:String}""; }. The configuration files used by``hailctl dataproc`` can be found at the following locations:. GRCh37: gs://hail-us-central1-vep/vep85-loftee-gcloud.json; GRCh38: gs://hail-us-central1-vep/vep95-GRCh38-loftee-gcloud.json. If no config file is specified, this function will check to see if environment variable VEP_CONFIG_URI is set with a path to a config file.; Batch Service Configuration; If no config is specified, Hail will use the user’s Service configuration parameters to find a supported VEP configuration.; However, if you wish to use your own implementation of VEP, then see the documentation for VEPConfig.; Annotations; A new row field is added in the location specified by name with type given; by the type given by the json_vep_schema (if csq is False) or; tarray of tstr (if csq is True).; If csq is True, then the CSQ header string is also added as a global; field with name name + '_csq_header'. Parameters:. dataset (MatrixTable or Table) – Dataset.; config (str or VEPConfig, optional) – Path to VEP configuration file or a VEPConfig object.; block_size (int) – Number of rows to process per VEP invocation.; name (str) – Name for resulting row field.; csq (bool) – If True, annotates with the VCF CSQ field as a tstr.; If False, annotates as the vep_json_schema.; tolera",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:106368,Modifiability,config,config,106368,"s:String,cdna_start:Int32,cdna_end:Int32,cds_end:Int32,cds_start:Int32,codons:String,consequence_terms:Array[String],distance:Int32,domains:Array[Struct{db:String,name:String}],exon:String,gene_id:String,gene_pheno:Int32,gene_symbol:String,gene_symbol_source:String,hgnc_id:String,hgvsc:String,hgvsp:String,hgvs_offset:Int32,impact:String,intron:String,lof:String,lof_flags:String,lof_filter:String,lof_info:String,minimised:Int32,polyphen_prediction:String,polyphen_score:Float64,protein_end:Int32,protein_start:Int32,protein_id:String,sift_prediction:String,sift_score:Float64,strand:Int32,swissprot:String,transcript_id:String,trembl:String,uniparc:String,variant_allele:String}],variant_class:String}""; }. The configuration files used by``hailctl dataproc`` can be found at the following locations:. GRCh37: gs://hail-us-central1-vep/vep85-loftee-gcloud.json; GRCh38: gs://hail-us-central1-vep/vep95-GRCh38-loftee-gcloud.json. If no config file is specified, this function will check to see if environment variable VEP_CONFIG_URI is set with a path to a config file.; Batch Service Configuration; If no config is specified, Hail will use the user’s Service configuration parameters to find a supported VEP configuration.; However, if you wish to use your own implementation of VEP, then see the documentation for VEPConfig.; Annotations; A new row field is added in the location specified by name with type given; by the type given by the json_vep_schema (if csq is False) or; tarray of tstr (if csq is True).; If csq is True, then the CSQ header string is also added as a global; field with name name + '_csq_header'. Parameters:. dataset (MatrixTable or Table) – Dataset.; config (str or VEPConfig, optional) – Path to VEP configuration file or a VEPConfig object.; block_size (int) – Number of rows to process per VEP invocation.; name (str) – Name for resulting row field.; csq (bool) – If True, annotates with the VCF CSQ field as a tstr.; If False, annotates as the vep_json_schema.; tolera",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:106417,Modifiability,config,config,106417,"truct{db:String,name:String}],exon:String,gene_id:String,gene_pheno:Int32,gene_symbol:String,gene_symbol_source:String,hgnc_id:String,hgvsc:String,hgvsp:String,hgvs_offset:Int32,impact:String,intron:String,lof:String,lof_flags:String,lof_filter:String,lof_info:String,minimised:Int32,polyphen_prediction:String,polyphen_score:Float64,protein_end:Int32,protein_start:Int32,protein_id:String,sift_prediction:String,sift_score:Float64,strand:Int32,swissprot:String,transcript_id:String,trembl:String,uniparc:String,variant_allele:String}],variant_class:String}""; }. The configuration files used by``hailctl dataproc`` can be found at the following locations:. GRCh37: gs://hail-us-central1-vep/vep85-loftee-gcloud.json; GRCh38: gs://hail-us-central1-vep/vep95-GRCh38-loftee-gcloud.json. If no config file is specified, this function will check to see if environment variable VEP_CONFIG_URI is set with a path to a config file.; Batch Service Configuration; If no config is specified, Hail will use the user’s Service configuration parameters to find a supported VEP configuration.; However, if you wish to use your own implementation of VEP, then see the documentation for VEPConfig.; Annotations; A new row field is added in the location specified by name with type given; by the type given by the json_vep_schema (if csq is False) or; tarray of tstr (if csq is True).; If csq is True, then the CSQ header string is also added as a global; field with name name + '_csq_header'. Parameters:. dataset (MatrixTable or Table) – Dataset.; config (str or VEPConfig, optional) – Path to VEP configuration file or a VEPConfig object.; block_size (int) – Number of rows to process per VEP invocation.; name (str) – Name for resulting row field.; csq (bool) – If True, annotates with the VCF CSQ field as a tstr.; If False, annotates as the vep_json_schema.; tolerate_parse_error (bool) – If True, ignore invalid JSON produced by VEP and return a missing annotation. Returns:; MatrixTable or Table – Dataset with",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:106471,Modifiability,config,configuration,106471,"truct{db:String,name:String}],exon:String,gene_id:String,gene_pheno:Int32,gene_symbol:String,gene_symbol_source:String,hgnc_id:String,hgvsc:String,hgvsp:String,hgvs_offset:Int32,impact:String,intron:String,lof:String,lof_flags:String,lof_filter:String,lof_info:String,minimised:Int32,polyphen_prediction:String,polyphen_score:Float64,protein_end:Int32,protein_start:Int32,protein_id:String,sift_prediction:String,sift_score:Float64,strand:Int32,swissprot:String,transcript_id:String,trembl:String,uniparc:String,variant_allele:String}],variant_class:String}""; }. The configuration files used by``hailctl dataproc`` can be found at the following locations:. GRCh37: gs://hail-us-central1-vep/vep85-loftee-gcloud.json; GRCh38: gs://hail-us-central1-vep/vep95-GRCh38-loftee-gcloud.json. If no config file is specified, this function will check to see if environment variable VEP_CONFIG_URI is set with a path to a config file.; Batch Service Configuration; If no config is specified, Hail will use the user’s Service configuration parameters to find a supported VEP configuration.; However, if you wish to use your own implementation of VEP, then see the documentation for VEPConfig.; Annotations; A new row field is added in the location specified by name with type given; by the type given by the json_vep_schema (if csq is False) or; tarray of tstr (if csq is True).; If csq is True, then the CSQ header string is also added as a global; field with name name + '_csq_header'. Parameters:. dataset (MatrixTable or Table) – Dataset.; config (str or VEPConfig, optional) – Path to VEP configuration file or a VEPConfig object.; block_size (int) – Number of rows to process per VEP invocation.; name (str) – Name for resulting row field.; csq (bool) – If True, annotates with the VCF CSQ field as a tstr.; If False, annotates as the vep_json_schema.; tolerate_parse_error (bool) – If True, ignore invalid JSON produced by VEP and return a missing annotation. Returns:; MatrixTable or Table – Dataset with",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:106520,Modifiability,config,configuration,106520,"truct{db:String,name:String}],exon:String,gene_id:String,gene_pheno:Int32,gene_symbol:String,gene_symbol_source:String,hgnc_id:String,hgvsc:String,hgvsp:String,hgvs_offset:Int32,impact:String,intron:String,lof:String,lof_flags:String,lof_filter:String,lof_info:String,minimised:Int32,polyphen_prediction:String,polyphen_score:Float64,protein_end:Int32,protein_start:Int32,protein_id:String,sift_prediction:String,sift_score:Float64,strand:Int32,swissprot:String,transcript_id:String,trembl:String,uniparc:String,variant_allele:String}],variant_class:String}""; }. The configuration files used by``hailctl dataproc`` can be found at the following locations:. GRCh37: gs://hail-us-central1-vep/vep85-loftee-gcloud.json; GRCh38: gs://hail-us-central1-vep/vep95-GRCh38-loftee-gcloud.json. If no config file is specified, this function will check to see if environment variable VEP_CONFIG_URI is set with a path to a config file.; Batch Service Configuration; If no config is specified, Hail will use the user’s Service configuration parameters to find a supported VEP configuration.; However, if you wish to use your own implementation of VEP, then see the documentation for VEPConfig.; Annotations; A new row field is added in the location specified by name with type given; by the type given by the json_vep_schema (if csq is False) or; tarray of tstr (if csq is True).; If csq is True, then the CSQ header string is also added as a global; field with name name + '_csq_header'. Parameters:. dataset (MatrixTable or Table) – Dataset.; config (str or VEPConfig, optional) – Path to VEP configuration file or a VEPConfig object.; block_size (int) – Number of rows to process per VEP invocation.; name (str) – Name for resulting row field.; csq (bool) – If True, annotates with the VCF CSQ field as a tstr.; If False, annotates as the vep_json_schema.; tolerate_parse_error (bool) – If True, ignore invalid JSON produced by VEP and return a missing annotation. Returns:; MatrixTable or Table – Dataset with",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:106989,Modifiability,config,config,106989,"tring,hgvsp:String,hgvs_offset:Int32,impact:String,intron:String,lof:String,lof_flags:String,lof_filter:String,lof_info:String,minimised:Int32,polyphen_prediction:String,polyphen_score:Float64,protein_end:Int32,protein_start:Int32,protein_id:String,sift_prediction:String,sift_score:Float64,strand:Int32,swissprot:String,transcript_id:String,trembl:String,uniparc:String,variant_allele:String}],variant_class:String}""; }. The configuration files used by``hailctl dataproc`` can be found at the following locations:. GRCh37: gs://hail-us-central1-vep/vep85-loftee-gcloud.json; GRCh38: gs://hail-us-central1-vep/vep95-GRCh38-loftee-gcloud.json. If no config file is specified, this function will check to see if environment variable VEP_CONFIG_URI is set with a path to a config file.; Batch Service Configuration; If no config is specified, Hail will use the user’s Service configuration parameters to find a supported VEP configuration.; However, if you wish to use your own implementation of VEP, then see the documentation for VEPConfig.; Annotations; A new row field is added in the location specified by name with type given; by the type given by the json_vep_schema (if csq is False) or; tarray of tstr (if csq is True).; If csq is True, then the CSQ header string is also added as a global; field with name name + '_csq_header'. Parameters:. dataset (MatrixTable or Table) – Dataset.; config (str or VEPConfig, optional) – Path to VEP configuration file or a VEPConfig object.; block_size (int) – Number of rows to process per VEP invocation.; name (str) – Name for resulting row field.; csq (bool) – If True, annotates with the VCF CSQ field as a tstr.; If False, annotates as the vep_json_schema.; tolerate_parse_error (bool) – If True, ignore invalid JSON produced by VEP and return a missing annotation. Returns:; MatrixTable or Table – Dataset with new row-indexed field name containing VEP annotations. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:107039,Modifiability,config,configuration,107039,"tring,hgvsp:String,hgvs_offset:Int32,impact:String,intron:String,lof:String,lof_flags:String,lof_filter:String,lof_info:String,minimised:Int32,polyphen_prediction:String,polyphen_score:Float64,protein_end:Int32,protein_start:Int32,protein_id:String,sift_prediction:String,sift_score:Float64,strand:Int32,swissprot:String,transcript_id:String,trembl:String,uniparc:String,variant_allele:String}],variant_class:String}""; }. The configuration files used by``hailctl dataproc`` can be found at the following locations:. GRCh37: gs://hail-us-central1-vep/vep85-loftee-gcloud.json; GRCh38: gs://hail-us-central1-vep/vep95-GRCh38-loftee-gcloud.json. If no config file is specified, this function will check to see if environment variable VEP_CONFIG_URI is set with a path to a config file.; Batch Service Configuration; If no config is specified, Hail will use the user’s Service configuration parameters to find a supported VEP configuration.; However, if you wish to use your own implementation of VEP, then see the documentation for VEPConfig.; Annotations; A new row field is added in the location specified by name with type given; by the type given by the json_vep_schema (if csq is False) or; tarray of tstr (if csq is True).; If csq is True, then the CSQ header string is also added as a global; field with name name + '_csq_header'. Parameters:. dataset (MatrixTable or Table) – Dataset.; config (str or VEPConfig, optional) – Path to VEP configuration file or a VEPConfig object.; block_size (int) – Number of rows to process per VEP invocation.; name (str) – Name for resulting row field.; csq (bool) – If True, annotates with the VCF CSQ field as a tstr.; If False, annotates as the vep_json_schema.; tolerate_parse_error (bool) – If True, ignore invalid JSON produced by VEP and return a missing annotation. Returns:; MatrixTable or Table – Dataset with new row-indexed field name containing VEP annotations. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
https://hail.is/docs/0.2/methods/genetics.html:3159,Performance,throughput,throughput,3159,"ated within each window. compute_charr(ds[, min_af, max_af, min_dp, ...]); Compute CHARR, the DNA sample contamination estimator. mendel_errors(call, pedigree); Find Mendel errors; count per variant, individual and nuclear family. de_novo(mt, pedigree, pop_frequency_prior, *); Call putative de novo events from trio data. nirvana(dataset, config[, block_size, name]); Annotate variants using Nirvana. sample_qc(mt[, name]); Compute per-sample metrics useful for quality control. _logistic_skat(group, weight, y, x, covariates); The logistic sequence kernel association test (SKAT). skat(key_expr, weight_expr, y, x, covariates); Test each keyed group of rows for association by linear or logistic SKAT test. lambda_gc(p_value[, approximate]); Compute genomic inflation factor (lambda GC) from an Expression of p-values. split_multi(ds[, keep_star, left_aligned, ...]); Split multiallelic variants. split_multi_hts(ds[, keep_star, ...]); Split multiallelic variants for datasets that contain one or more fields from a standard high-throughput sequencing entry schema. summarize_variants(mt[, show, handler]); Summarize the variants present in a dataset and print the results. transmission_disequilibrium_test(dataset, ...); Performs the transmission disequilibrium test on trios. trio_matrix(dataset, pedigree[, complete_trios]); Builds and returns a matrix where columns correspond to trios and entries contain genotypes for the trio. variant_qc(mt[, name]); Compute common variant statistics (quality control metrics). vep(dataset[, config, block_size, name, ...]); Annotate variants with VEP. class hail.methods.VEPConfig[source]; Base class for configuring VEP.; To define a custom VEP configuration to for Query on Batch, construct a new class that inherits from VEPConfig; and has the following parameters defined:. json_type (HailType): The type of the VEP JSON schema (as produced by VEP when invoked with the –json option).; data_bucket (str) – The location where the VEP data is stored.; da",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
