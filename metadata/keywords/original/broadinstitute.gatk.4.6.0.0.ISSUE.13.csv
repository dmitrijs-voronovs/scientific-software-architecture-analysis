id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/broadinstitute/gatk/pull/8248:28993,Deployability,Integrat,Integration,28993,"ility to tweak sample-every-Nth-variant parameter for SNP model creation (#8019); - add initial notebook copy pasta (#8008); - add sample_table_timestamp to GetNumSamplesLoaded (#8022); - Batched Avro export [VS-630] (#8020); - Updating references to old GATK for VS-620 (#8023); - VS-517 Use standard version of GetBQTableLastModifiedDatetime in GvsValidateVat (#8024); - Fix bug in GvsWithdrawSamples.wdl (#8026); - Ah 617 exposing the drop_state parameter to the GvsJointVariantCalling wdl used for beta (and internal customer) (#8032); - Expose maximum-training-variants VQSR parameter [VS-634] (#8029); - Callset statistics [VS-560] (#8018); - Check for withdrawn before exporting to AVRO files [VS-646] (#8039); - Small updates to GVS Integration WDL [VS-618] (#8042); - Rework Hail script generation [VS-616] (#8034); - Alpine based Variant Store Docker image [VS-648] (#8044); - update warp version (#7906); - Fail Avro extract and callset stats on bad filter name [VS-655] (#8046); - Vs 629 failure to retrieve job information during ingest (#8047); - Restore accidentally removed bcftools [VS-661] (#8051); - Allowing our pipeline to function with a sample size of one (#8055); - Vs 665 re create vcf for cd 68 po 52339 with ad padding fixed (#8057); - VS-665 and VS-620 updating code to use latest docker images containing Rori's AD calculation changes in extract (#8061); - updating the beta workflow to use the latest jar, representing the version of GATK George tested against the workflow (#8062); - VS-637 Address a couple of issues in SampleLoadStatus handling in GVSImportGenomes. (#8052); - Revert Alpinizing of apt dependent task [VS-688] (#8065); - Fix missing vat schema JSONs [VS-699] (#8072); - Fix integration expectations for fixed AD [VS-689] (#8066); - VS-698 Remove unnecessary columns from Call set statistics (#8073); - Fix Dockerfile nits that break 20.10.21 (#8078); - Nirvana 3.18.1 Docker images support [VS-661] (#8082); - Add option to not prepare __REF_DATA or __",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:29139,Deployability,update,update,29139,"ility to tweak sample-every-Nth-variant parameter for SNP model creation (#8019); - add initial notebook copy pasta (#8008); - add sample_table_timestamp to GetNumSamplesLoaded (#8022); - Batched Avro export [VS-630] (#8020); - Updating references to old GATK for VS-620 (#8023); - VS-517 Use standard version of GetBQTableLastModifiedDatetime in GvsValidateVat (#8024); - Fix bug in GvsWithdrawSamples.wdl (#8026); - Ah 617 exposing the drop_state parameter to the GvsJointVariantCalling wdl used for beta (and internal customer) (#8032); - Expose maximum-training-variants VQSR parameter [VS-634] (#8029); - Callset statistics [VS-560] (#8018); - Check for withdrawn before exporting to AVRO files [VS-646] (#8039); - Small updates to GVS Integration WDL [VS-618] (#8042); - Rework Hail script generation [VS-616] (#8034); - Alpine based Variant Store Docker image [VS-648] (#8044); - update warp version (#7906); - Fail Avro extract and callset stats on bad filter name [VS-655] (#8046); - Vs 629 failure to retrieve job information during ingest (#8047); - Restore accidentally removed bcftools [VS-661] (#8051); - Allowing our pipeline to function with a sample size of one (#8055); - Vs 665 re create vcf for cd 68 po 52339 with ad padding fixed (#8057); - VS-665 and VS-620 updating code to use latest docker images containing Rori's AD calculation changes in extract (#8061); - updating the beta workflow to use the latest jar, representing the version of GATK George tested against the workflow (#8062); - VS-637 Address a couple of issues in SampleLoadStatus handling in GVSImportGenomes. (#8052); - Revert Alpinizing of apt dependent task [VS-688] (#8065); - Fix missing vat schema JSONs [VS-699] (#8072); - Fix integration expectations for fixed AD [VS-689] (#8066); - VS-698 Remove unnecessary columns from Call set statistics (#8073); - Fix Dockerfile nits that break 20.10.21 (#8078); - Nirvana 3.18.1 Docker images support [VS-661] (#8082); - Add option to not prepare __REF_DATA or __",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:29384,Deployability,pipeline,pipeline,29384,"ility to tweak sample-every-Nth-variant parameter for SNP model creation (#8019); - add initial notebook copy pasta (#8008); - add sample_table_timestamp to GetNumSamplesLoaded (#8022); - Batched Avro export [VS-630] (#8020); - Updating references to old GATK for VS-620 (#8023); - VS-517 Use standard version of GetBQTableLastModifiedDatetime in GvsValidateVat (#8024); - Fix bug in GvsWithdrawSamples.wdl (#8026); - Ah 617 exposing the drop_state parameter to the GvsJointVariantCalling wdl used for beta (and internal customer) (#8032); - Expose maximum-training-variants VQSR parameter [VS-634] (#8029); - Callset statistics [VS-560] (#8018); - Check for withdrawn before exporting to AVRO files [VS-646] (#8039); - Small updates to GVS Integration WDL [VS-618] (#8042); - Rework Hail script generation [VS-616] (#8034); - Alpine based Variant Store Docker image [VS-648] (#8044); - update warp version (#7906); - Fail Avro extract and callset stats on bad filter name [VS-655] (#8046); - Vs 629 failure to retrieve job information during ingest (#8047); - Restore accidentally removed bcftools [VS-661] (#8051); - Allowing our pipeline to function with a sample size of one (#8055); - Vs 665 re create vcf for cd 68 po 52339 with ad padding fixed (#8057); - VS-665 and VS-620 updating code to use latest docker images containing Rori's AD calculation changes in extract (#8061); - updating the beta workflow to use the latest jar, representing the version of GATK George tested against the workflow (#8062); - VS-637 Address a couple of issues in SampleLoadStatus handling in GVSImportGenomes. (#8052); - Revert Alpinizing of apt dependent task [VS-688] (#8065); - Fix missing vat schema JSONs [VS-699] (#8072); - Fix integration expectations for fixed AD [VS-689] (#8066); - VS-698 Remove unnecessary columns from Call set statistics (#8073); - Fix Dockerfile nits that break 20.10.21 (#8078); - Nirvana 3.18.1 Docker images support [VS-661] (#8082); - Add option to not prepare __REF_DATA or __",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:29975,Deployability,integrat,integration,29975,"Integration WDL [VS-618] (#8042); - Rework Hail script generation [VS-616] (#8034); - Alpine based Variant Store Docker image [VS-648] (#8044); - update warp version (#7906); - Fail Avro extract and callset stats on bad filter name [VS-655] (#8046); - Vs 629 failure to retrieve job information during ingest (#8047); - Restore accidentally removed bcftools [VS-661] (#8051); - Allowing our pipeline to function with a sample size of one (#8055); - Vs 665 re create vcf for cd 68 po 52339 with ad padding fixed (#8057); - VS-665 and VS-620 updating code to use latest docker images containing Rori's AD calculation changes in extract (#8061); - updating the beta workflow to use the latest jar, representing the version of GATK George tested against the workflow (#8062); - VS-637 Address a couple of issues in SampleLoadStatus handling in GVSImportGenomes. (#8052); - Revert Alpinizing of apt dependent task [VS-688] (#8065); - Fix missing vat schema JSONs [VS-699] (#8072); - Fix integration expectations for fixed AD [VS-689] (#8066); - VS-698 Remove unnecessary columns from Call set statistics (#8073); - Fix Dockerfile nits that break 20.10.21 (#8078); - Nirvana 3.18.1 Docker images support [VS-661] (#8082); - Add option to not prepare __REF_DATA or __SAMPLES tables to Prepare [VS-697] (#8079); - ""build-base"" Docker image for faster variantstore image builds [VS-712] (#8085); - GVS / Hail VDS integration test [VS-639] (#8086); - Remove AI/AN from VDS docs [VS-726] (#8096); - Add flag for cost_observability table writing to support sub-cohort use case [VS-521] (#8093); - Document STS delivery process for VDS [VS-727] (#8101); - delete obsolete callset_QC directory and its contents [VS-318] (#8108); - doc link typo and add check for control samples in AVRO export (#8110); - Add defaults for scatter_count in GvsExtractCohortFromSampleNames [VS-496] (#8109); - Escape table names properly in ValidateVat WDL (#8116); - Vs 741 fix indefinite freeze in split intervals task when using ex",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:30397,Deployability,integrat,integration,30397,"ge tested against the workflow (#8062); - VS-637 Address a couple of issues in SampleLoadStatus handling in GVSImportGenomes. (#8052); - Revert Alpinizing of apt dependent task [VS-688] (#8065); - Fix missing vat schema JSONs [VS-699] (#8072); - Fix integration expectations for fixed AD [VS-689] (#8066); - VS-698 Remove unnecessary columns from Call set statistics (#8073); - Fix Dockerfile nits that break 20.10.21 (#8078); - Nirvana 3.18.1 Docker images support [VS-661] (#8082); - Add option to not prepare __REF_DATA or __SAMPLES tables to Prepare [VS-697] (#8079); - ""build-base"" Docker image for faster variantstore image builds [VS-712] (#8085); - GVS / Hail VDS integration test [VS-639] (#8086); - Remove AI/AN from VDS docs [VS-726] (#8096); - Add flag for cost_observability table writing to support sub-cohort use case [VS-521] (#8093); - Document STS delivery process for VDS [VS-727] (#8101); - delete obsolete callset_QC directory and its contents [VS-318] (#8108); - doc link typo and add check for control samples in AVRO export (#8110); - Add defaults for scatter_count in GvsExtractCohortFromSampleNames [VS-496] (#8109); - Escape table names properly in ValidateVat WDL (#8116); - Vs 741 fix indefinite freeze in split intervals task when using exome data (#8113); - VAT Readme updates (#8090); - WDL and python scripts to use the VDS in the VAT (#8077); - VS-757 - Use JASIX to make sub-jsons of annotated output of Nirvana (#8133); - add note about permissions for P&S workflow to work (#8135); - VS-759 (and VS-760) (#8137); - VS-765. Scatter the RemoveDuplicates task. (#8144); - update delivery docs based on latest VDS delivery run [VS-770] (#8150); - Add monitoring to index vcf (#8151); - Make some noise when VDS validation succeeds (#8155); - Handle empty genes annotation file. (#8153); - Add escapes for otherwise problematic dataset / table names. (#8162); - New WDL to create VAT tsvs from previously generated BigQuery table. (#8165); - Treat withdrawn samples in ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:31025,Deployability,update,updates,31025,"ge tested against the workflow (#8062); - VS-637 Address a couple of issues in SampleLoadStatus handling in GVSImportGenomes. (#8052); - Revert Alpinizing of apt dependent task [VS-688] (#8065); - Fix missing vat schema JSONs [VS-699] (#8072); - Fix integration expectations for fixed AD [VS-689] (#8066); - VS-698 Remove unnecessary columns from Call set statistics (#8073); - Fix Dockerfile nits that break 20.10.21 (#8078); - Nirvana 3.18.1 Docker images support [VS-661] (#8082); - Add option to not prepare __REF_DATA or __SAMPLES tables to Prepare [VS-697] (#8079); - ""build-base"" Docker image for faster variantstore image builds [VS-712] (#8085); - GVS / Hail VDS integration test [VS-639] (#8086); - Remove AI/AN from VDS docs [VS-726] (#8096); - Add flag for cost_observability table writing to support sub-cohort use case [VS-521] (#8093); - Document STS delivery process for VDS [VS-727] (#8101); - delete obsolete callset_QC directory and its contents [VS-318] (#8108); - doc link typo and add check for control samples in AVRO export (#8110); - Add defaults for scatter_count in GvsExtractCohortFromSampleNames [VS-496] (#8109); - Escape table names properly in ValidateVat WDL (#8116); - Vs 741 fix indefinite freeze in split intervals task when using exome data (#8113); - VAT Readme updates (#8090); - WDL and python scripts to use the VDS in the VAT (#8077); - VS-757 - Use JASIX to make sub-jsons of annotated output of Nirvana (#8133); - add note about permissions for P&S workflow to work (#8135); - VS-759 (and VS-760) (#8137); - VS-765. Scatter the RemoveDuplicates task. (#8144); - update delivery docs based on latest VDS delivery run [VS-770] (#8150); - Add monitoring to index vcf (#8151); - Make some noise when VDS validation succeeds (#8155); - Handle empty genes annotation file. (#8153); - Add escapes for otherwise problematic dataset / table names. (#8162); - New WDL to create VAT tsvs from previously generated BigQuery table. (#8165); - Treat withdrawn samples in ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:31331,Deployability,update,update,31331,"86); - Remove AI/AN from VDS docs [VS-726] (#8096); - Add flag for cost_observability table writing to support sub-cohort use case [VS-521] (#8093); - Document STS delivery process for VDS [VS-727] (#8101); - delete obsolete callset_QC directory and its contents [VS-318] (#8108); - doc link typo and add check for control samples in AVRO export (#8110); - Add defaults for scatter_count in GvsExtractCohortFromSampleNames [VS-496] (#8109); - Escape table names properly in ValidateVat WDL (#8116); - Vs 741 fix indefinite freeze in split intervals task when using exome data (#8113); - VAT Readme updates (#8090); - WDL and python scripts to use the VDS in the VAT (#8077); - VS-757 - Use JASIX to make sub-jsons of annotated output of Nirvana (#8133); - add note about permissions for P&S workflow to work (#8135); - VS-759 (and VS-760) (#8137); - VS-765. Scatter the RemoveDuplicates task. (#8144); - update delivery docs based on latest VDS delivery run [VS-770] (#8150); - Add monitoring to index vcf (#8151); - Make some noise when VDS validation succeeds (#8155); - Handle empty genes annotation file. (#8153); - Add escapes for otherwise problematic dataset / table names. (#8162); - New WDL to create VAT tsvs from previously generated BigQuery table. (#8165); - Treat withdrawn samples in sub-cohort prepare correctly [VS-772] (#8156); - Remove unused VAT Creation WDL (#8172); - Gg consistently use dataset name as input parameter (#8173); - AoU cleanup docs, round 1 [VS-671] (#8104); - VDS docs remove samples and correct GT [VS-807] (#8178); - [VS-693] Add support for VQSR Lite to GvsCreateFilterSet (#8157); - VAT Documentation Update Round 1 [VS-531]; - VS-530 VDS creation documentation for AoU (#8169); - Update beta docs to tell people not to use free credits (#8184); - VS-816 Keeping ingestion under quota (#8193); - CromwellOnAzure + Azure SQL DB + AAD first steps doc [VS-805] (#8191); - Edit and re-format VDS -> VAT doc [VS-821] (#8187); - VS-820 Incorporate code to stay un",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:32071,Deployability,Update,Update,32071,"run [VS-770] (#8150); - Add monitoring to index vcf (#8151); - Make some noise when VDS validation succeeds (#8155); - Handle empty genes annotation file. (#8153); - Add escapes for otherwise problematic dataset / table names. (#8162); - New WDL to create VAT tsvs from previously generated BigQuery table. (#8165); - Treat withdrawn samples in sub-cohort prepare correctly [VS-772] (#8156); - Remove unused VAT Creation WDL (#8172); - Gg consistently use dataset name as input parameter (#8173); - AoU cleanup docs, round 1 [VS-671] (#8104); - VDS docs remove samples and correct GT [VS-807] (#8178); - [VS-693] Add support for VQSR Lite to GvsCreateFilterSet (#8157); - VAT Documentation Update Round 1 [VS-531]; - VS-530 VDS creation documentation for AoU (#8169); - Update beta docs to tell people not to use free credits (#8184); - VS-816 Keeping ingestion under quota (#8193); - CromwellOnAzure + Azure SQL DB + AAD first steps doc [VS-805] (#8191); - Edit and re-format VDS -> VAT doc [VS-821] (#8187); - VS-820 Incorporate code to stay under Google quotas for new accounts into beta workflow (#8200); - Update docs for Nirvana reference disk [VS-531] [VS-796] (#8170); - VS-694 - Extract Callset for VQSR Lite (#8182); - Updating docker image (#8210); - Document VCF generation [VS-795] (#8202); - Variants GATK Docker image building docs + script [VS-827] (#8207); - Update GATK jar used in GvsJointVariantCalling WDL (#8216); - Hello Azure SQL Database from Cromwell on Azure [VS-812] (#8220); - Remove what appear to be accidentally added files [VS-834] (#8225); - VS-815: Add Support for YNG to VQSR Lite (#8206); - Disentangle non-GVS code from GVS code [VS-834] (#8229); - VS-695. Updates to run Precision and Sensitivity on VQSR Lite (#8230); - Track avro export costs [VS-769] (#8236); - Add note that we deleted a VDS! (#8214); - Vs 822 Add documentation for the work that we did on the latest iteration of Delta (#8205); - Rc vs 822 gq0 documentation (#8240). [VS-16]: https://broad",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:32151,Deployability,Update,Update,32151,"run [VS-770] (#8150); - Add monitoring to index vcf (#8151); - Make some noise when VDS validation succeeds (#8155); - Handle empty genes annotation file. (#8153); - Add escapes for otherwise problematic dataset / table names. (#8162); - New WDL to create VAT tsvs from previously generated BigQuery table. (#8165); - Treat withdrawn samples in sub-cohort prepare correctly [VS-772] (#8156); - Remove unused VAT Creation WDL (#8172); - Gg consistently use dataset name as input parameter (#8173); - AoU cleanup docs, round 1 [VS-671] (#8104); - VDS docs remove samples and correct GT [VS-807] (#8178); - [VS-693] Add support for VQSR Lite to GvsCreateFilterSet (#8157); - VAT Documentation Update Round 1 [VS-531]; - VS-530 VDS creation documentation for AoU (#8169); - Update beta docs to tell people not to use free credits (#8184); - VS-816 Keeping ingestion under quota (#8193); - CromwellOnAzure + Azure SQL DB + AAD first steps doc [VS-805] (#8191); - Edit and re-format VDS -> VAT doc [VS-821] (#8187); - VS-820 Incorporate code to stay under Google quotas for new accounts into beta workflow (#8200); - Update docs for Nirvana reference disk [VS-531] [VS-796] (#8170); - VS-694 - Extract Callset for VQSR Lite (#8182); - Updating docker image (#8210); - Document VCF generation [VS-795] (#8202); - Variants GATK Docker image building docs + script [VS-827] (#8207); - Update GATK jar used in GvsJointVariantCalling WDL (#8216); - Hello Azure SQL Database from Cromwell on Azure [VS-812] (#8220); - Remove what appear to be accidentally added files [VS-834] (#8225); - VS-815: Add Support for YNG to VQSR Lite (#8206); - Disentangle non-GVS code from GVS code [VS-834] (#8229); - VS-695. Updates to run Precision and Sensitivity on VQSR Lite (#8230); - Track avro export costs [VS-769] (#8236); - Add note that we deleted a VDS! (#8214); - Vs 822 Add documentation for the work that we did on the latest iteration of Delta (#8205); - Rc vs 822 gq0 documentation (#8240). [VS-16]: https://broad",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:32492,Deployability,Update,Update,32492,"run [VS-770] (#8150); - Add monitoring to index vcf (#8151); - Make some noise when VDS validation succeeds (#8155); - Handle empty genes annotation file. (#8153); - Add escapes for otherwise problematic dataset / table names. (#8162); - New WDL to create VAT tsvs from previously generated BigQuery table. (#8165); - Treat withdrawn samples in sub-cohort prepare correctly [VS-772] (#8156); - Remove unused VAT Creation WDL (#8172); - Gg consistently use dataset name as input parameter (#8173); - AoU cleanup docs, round 1 [VS-671] (#8104); - VDS docs remove samples and correct GT [VS-807] (#8178); - [VS-693] Add support for VQSR Lite to GvsCreateFilterSet (#8157); - VAT Documentation Update Round 1 [VS-531]; - VS-530 VDS creation documentation for AoU (#8169); - Update beta docs to tell people not to use free credits (#8184); - VS-816 Keeping ingestion under quota (#8193); - CromwellOnAzure + Azure SQL DB + AAD first steps doc [VS-805] (#8191); - Edit and re-format VDS -> VAT doc [VS-821] (#8187); - VS-820 Incorporate code to stay under Google quotas for new accounts into beta workflow (#8200); - Update docs for Nirvana reference disk [VS-531] [VS-796] (#8170); - VS-694 - Extract Callset for VQSR Lite (#8182); - Updating docker image (#8210); - Document VCF generation [VS-795] (#8202); - Variants GATK Docker image building docs + script [VS-827] (#8207); - Update GATK jar used in GvsJointVariantCalling WDL (#8216); - Hello Azure SQL Database from Cromwell on Azure [VS-812] (#8220); - Remove what appear to be accidentally added files [VS-834] (#8225); - VS-815: Add Support for YNG to VQSR Lite (#8206); - Disentangle non-GVS code from GVS code [VS-834] (#8229); - VS-695. Updates to run Precision and Sensitivity on VQSR Lite (#8230); - Track avro export costs [VS-769] (#8236); - Add note that we deleted a VDS! (#8214); - Vs 822 Add documentation for the work that we did on the latest iteration of Delta (#8205); - Rc vs 822 gq0 documentation (#8240). [VS-16]: https://broad",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:32757,Deployability,Update,Update,32757,"run [VS-770] (#8150); - Add monitoring to index vcf (#8151); - Make some noise when VDS validation succeeds (#8155); - Handle empty genes annotation file. (#8153); - Add escapes for otherwise problematic dataset / table names. (#8162); - New WDL to create VAT tsvs from previously generated BigQuery table. (#8165); - Treat withdrawn samples in sub-cohort prepare correctly [VS-772] (#8156); - Remove unused VAT Creation WDL (#8172); - Gg consistently use dataset name as input parameter (#8173); - AoU cleanup docs, round 1 [VS-671] (#8104); - VDS docs remove samples and correct GT [VS-807] (#8178); - [VS-693] Add support for VQSR Lite to GvsCreateFilterSet (#8157); - VAT Documentation Update Round 1 [VS-531]; - VS-530 VDS creation documentation for AoU (#8169); - Update beta docs to tell people not to use free credits (#8184); - VS-816 Keeping ingestion under quota (#8193); - CromwellOnAzure + Azure SQL DB + AAD first steps doc [VS-805] (#8191); - Edit and re-format VDS -> VAT doc [VS-821] (#8187); - VS-820 Incorporate code to stay under Google quotas for new accounts into beta workflow (#8200); - Update docs for Nirvana reference disk [VS-531] [VS-796] (#8170); - VS-694 - Extract Callset for VQSR Lite (#8182); - Updating docker image (#8210); - Document VCF generation [VS-795] (#8202); - Variants GATK Docker image building docs + script [VS-827] (#8207); - Update GATK jar used in GvsJointVariantCalling WDL (#8216); - Hello Azure SQL Database from Cromwell on Azure [VS-812] (#8220); - Remove what appear to be accidentally added files [VS-834] (#8225); - VS-815: Add Support for YNG to VQSR Lite (#8206); - Disentangle non-GVS code from GVS code [VS-834] (#8229); - VS-695. Updates to run Precision and Sensitivity on VQSR Lite (#8230); - Track avro export costs [VS-769] (#8236); - Add note that we deleted a VDS! (#8214); - Vs 822 Add documentation for the work that we did on the latest iteration of Delta (#8205); - Rc vs 822 gq0 documentation (#8240). [VS-16]: https://broad",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:33076,Deployability,Update,Updates,33076,"ty genes annotation file. (#8153); - Add escapes for otherwise problematic dataset / table names. (#8162); - New WDL to create VAT tsvs from previously generated BigQuery table. (#8165); - Treat withdrawn samples in sub-cohort prepare correctly [VS-772] (#8156); - Remove unused VAT Creation WDL (#8172); - Gg consistently use dataset name as input parameter (#8173); - AoU cleanup docs, round 1 [VS-671] (#8104); - VDS docs remove samples and correct GT [VS-807] (#8178); - [VS-693] Add support for VQSR Lite to GvsCreateFilterSet (#8157); - VAT Documentation Update Round 1 [VS-531]; - VS-530 VDS creation documentation for AoU (#8169); - Update beta docs to tell people not to use free credits (#8184); - VS-816 Keeping ingestion under quota (#8193); - CromwellOnAzure + Azure SQL DB + AAD first steps doc [VS-805] (#8191); - Edit and re-format VDS -> VAT doc [VS-821] (#8187); - VS-820 Incorporate code to stay under Google quotas for new accounts into beta workflow (#8200); - Update docs for Nirvana reference disk [VS-531] [VS-796] (#8170); - VS-694 - Extract Callset for VQSR Lite (#8182); - Updating docker image (#8210); - Document VCF generation [VS-795] (#8202); - Variants GATK Docker image building docs + script [VS-827] (#8207); - Update GATK jar used in GvsJointVariantCalling WDL (#8216); - Hello Azure SQL Database from Cromwell on Azure [VS-812] (#8220); - Remove what appear to be accidentally added files [VS-834] (#8225); - VS-815: Add Support for YNG to VQSR Lite (#8206); - Disentangle non-GVS code from GVS code [VS-834] (#8229); - VS-695. Updates to run Precision and Sensitivity on VQSR Lite (#8230); - Track avro export costs [VS-769] (#8236); - Add note that we deleted a VDS! (#8214); - Vs 822 Add documentation for the work that we did on the latest iteration of Delta (#8205); - Rc vs 822 gq0 documentation (#8240). [VS-16]: https://broadworkbench.atlassian.net/browse/VS-16?atlOrigin=eyJpIjoiNWRkNTljNzYxNjVmNDY3MDlhMDU5Y2ZhYzA5YTRkZjUiLCJwIjoiZ2l0aHViLWNvbS1KU1cifQ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:5765,Energy Efficiency,Reduce,Reduce,5765," file for warp pipeline; - reverting logging; - included top level WDL; - use gnarly with BQ extract cohort; - remove unused file; - cleaning up; - tidy; - tidy up before PR; - tidy up before PR; - PR comments; - merge conflict misfires; - added example SQL to create alt allele table from VET; - option to remove PLs; - fixed and enhanced unit test; - removing unused config, causing travis to fail; - add CreateVariantIngestFiles integration test (#7071); - add sampleName (instead of NULL) to error message (#7074); - Update To handle if no data error (#7084); - Memory improvement when writing missing positions to pet (#7098); - added support for loading QUALapprox into VET (#7101); - Add -m flag to gsutil step; add dockstore branch filters to facilitate development (#7104); - updates to ImportGenomes and LoadBigQueryData (#7112); - Add ngs to cohort extract Dockerfile; remove exception catching in extract python script (#7113); - remove problematic storage_location imports (#7119); - Reduce memory and CPU for CreateImportTsvs task, check for files before attempting load (#7121); - add -m flag to gsutil mv step (#7129); - ah_var_store : Add sample file argument to cohort extract (#7117); - wip; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - forgot this file; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691);",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:11737,Energy Efficiency,Reduce,Reduce,11737," table creation and data loading in LoadData (#7056); - WIP; - tieout scripts; - notes files; - updated diff scripts; - fixed bug...; - add wdl and inputs file for warp pipeline; - reverting logging; - included top level WDL; - use gnarly with BQ extract cohort; - remove unused file; - cleaning up; - tidy; - tidy up before PR; - tidy up before PR; - PR comments; - merge conflict misfires; - added example SQL to create alt allele table from VET; - option to remove PLs; - fixed and enhanced unit test; - removing unused config, causing travis to fail; - add CreateVariantIngestFiles integration test (#7071); - add sampleName (instead of NULL) to error message (#7074); - Update To handle if no data error (#7084); - Memory improvement when writing missing positions to pet (#7098); - added support for loading QUALapprox into VET (#7101); - Add -m flag to gsutil step; add dockstore branch filters to facilitate development (#7104); - updates to ImportGenomes and LoadBigQueryData (#7112); - Add ngs to cohort extract Dockerfile; remove exception catching in extract python script (#7113); - remove problematic storage_location imports (#7119); - Reduce memory and CPU for CreateImportTsvs task, check for files before attempting load (#7121); - add -m flag to gsutil mv step (#7129); - ah_var_store : Add sample file argument to cohort extract (#7117); - Perform full WGS cohort extract scientific tieout for 35 ACMG59 samples (#7106); - Enable Read/Execution Project for BQ Queries (#7136); - ah - optional service account (#7140); - Add load lock file to prevent accidental re-loading of data to BQ (#7138); - #251 Address gvcf no-calls missing QUALapprox and other features (#7146); - Job Add labels to BQ operations from GATK (Issues-199) (#7115); - parse map to list to avoid brackets and spaces in vcf output (#7168); - #259 Inline schema for importgenomes.wdl (#7171); - Created AvroFileReader and unittest, Update ExtractCohort and ExtractCohortEngine (#7174); - #224 Import WDL: handle ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:14125,Energy Efficiency,reduce,reduce,14125,"ut allow NONE as input (#7206); - SA support and consistent naming for all GVS WDLs (#7205); - fix GvsExtractCallset inputs file (#7210); - add clustering to tables (#7207); - add vqsr cutoffs to GvsExtractCallset wdl; clean up dockstore yml (#7209); - Avro test (#7192); - Enable call caching of TSV generation in GvsImportGenomes (#7226); - 266 Clean up ExtractCohort -- remove query mode param (#7227); - 288 Add an excess alleles param (#7221); - take sample name as a param (#7236); - How to run GIAB comparisons (#7237); - Update GvsCreateFilterSet.wdl (#7239); - Use GatherVcfsCloud in GvsCreateFilterSet.wdl (#7241); - parameterize TTL with defaults, reduce memory allocation (#7244); - Addressing OOM in CohortExtract (#7245); - make outputs optional, change case in output (#7252); - Support for FORMAT/FT VQSLod Filtering and cohort-wide LowQual filter (#7248); - removed arrays code, renamed packages (#7260); - 279 labels (#7233); - add conda commands to GIAB readme (#7268); - remove gvs branch (#7263); - remove gvs branch (#7263); - upgrade bq libraries (#7264); - #299 - Sample list ease of use for cohort extracts (#7272); - check for duplicate ids (#7273); - Rc 274 passing sites only (#7275); - added default value to drop_state; broadinstitute/dsp-spec-ops#310 (#7278); - version bump for reliability (#7284); - add timestamp check to ExtractTask call https://github.com/broadinstitute/dsp-spec-ops/issues/320; - serial inserts for scaling prepare, factored out sample name (#7288); - Remove training sites only param from ExtractFeatures broadinstitute/dsp-spec-ops#261; - add param for mem for indels (#7282); - Ah prepare localize option (#7299); - Export sites only vcf STEP 1-- 317 add AC, AN, AF to the final VCF (#7279); - AoU GVS Cohort Extract wdl (#7242); - reliability (#7310); - bump to include FT tag filtering (#7316); - First pass at a Terra QuickStart (#7267); - Ah fix timestamp query (#7319); - 313 Cleanup Extract Cohort params (#7293); - bump bq storage versi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:31409,Energy Efficiency,monitor,monitoring,31409,"86); - Remove AI/AN from VDS docs [VS-726] (#8096); - Add flag for cost_observability table writing to support sub-cohort use case [VS-521] (#8093); - Document STS delivery process for VDS [VS-727] (#8101); - delete obsolete callset_QC directory and its contents [VS-318] (#8108); - doc link typo and add check for control samples in AVRO export (#8110); - Add defaults for scatter_count in GvsExtractCohortFromSampleNames [VS-496] (#8109); - Escape table names properly in ValidateVat WDL (#8116); - Vs 741 fix indefinite freeze in split intervals task when using exome data (#8113); - VAT Readme updates (#8090); - WDL and python scripts to use the VDS in the VAT (#8077); - VS-757 - Use JASIX to make sub-jsons of annotated output of Nirvana (#8133); - add note about permissions for P&S workflow to work (#8135); - VS-759 (and VS-760) (#8137); - VS-765. Scatter the RemoveDuplicates task. (#8144); - update delivery docs based on latest VDS delivery run [VS-770] (#8150); - Add monitoring to index vcf (#8151); - Make some noise when VDS validation succeeds (#8155); - Handle empty genes annotation file. (#8153); - Add escapes for otherwise problematic dataset / table names. (#8162); - New WDL to create VAT tsvs from previously generated BigQuery table. (#8165); - Treat withdrawn samples in sub-cohort prepare correctly [VS-772] (#8156); - Remove unused VAT Creation WDL (#8172); - Gg consistently use dataset name as input parameter (#8173); - AoU cleanup docs, round 1 [VS-671] (#8104); - VDS docs remove samples and correct GT [VS-807] (#8178); - [VS-693] Add support for VQSR Lite to GvsCreateFilterSet (#8157); - VAT Documentation Update Round 1 [VS-531]; - VS-530 VDS creation documentation for AoU (#8169); - Update beta docs to tell people not to use free credits (#8184); - VS-816 Keeping ingestion under quota (#8193); - CromwellOnAzure + Azure SQL DB + AAD first steps doc [VS-805] (#8191); - Edit and re-format VDS -> VAT doc [VS-821] (#8187); - VS-820 Incorporate code to stay un",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:5200,Integrability,integrat,integration,5200,,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:5270,Integrability,message,message,5270,,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:11172,Integrability,integrat,integration,11172," table creation and data loading in LoadData (#7056); - WIP; - tieout scripts; - notes files; - updated diff scripts; - fixed bug...; - add wdl and inputs file for warp pipeline; - reverting logging; - included top level WDL; - use gnarly with BQ extract cohort; - remove unused file; - cleaning up; - tidy; - tidy up before PR; - tidy up before PR; - PR comments; - merge conflict misfires; - added example SQL to create alt allele table from VET; - option to remove PLs; - fixed and enhanced unit test; - removing unused config, causing travis to fail; - add CreateVariantIngestFiles integration test (#7071); - add sampleName (instead of NULL) to error message (#7074); - Update To handle if no data error (#7084); - Memory improvement when writing missing positions to pet (#7098); - added support for loading QUALapprox into VET (#7101); - Add -m flag to gsutil step; add dockstore branch filters to facilitate development (#7104); - updates to ImportGenomes and LoadBigQueryData (#7112); - Add ngs to cohort extract Dockerfile; remove exception catching in extract python script (#7113); - remove problematic storage_location imports (#7119); - Reduce memory and CPU for CreateImportTsvs task, check for files before attempting load (#7121); - add -m flag to gsutil mv step (#7129); - ah_var_store : Add sample file argument to cohort extract (#7117); - Perform full WGS cohort extract scientific tieout for 35 ACMG59 samples (#7106); - Enable Read/Execution Project for BQ Queries (#7136); - ah - optional service account (#7140); - Add load lock file to prevent accidental re-loading of data to BQ (#7138); - #251 Address gvcf no-calls missing QUALapprox and other features (#7146); - Job Add labels to BQ operations from GATK (Issues-199) (#7115); - parse map to list to avoid brackets and spaces in vcf output (#7168); - #259 Inline schema for importgenomes.wdl (#7171); - Created AvroFileReader and unittest, Update ExtractCohort and ExtractCohortEngine (#7174); - #224 Import WDL: handle ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:11242,Integrability,message,message,11242," table creation and data loading in LoadData (#7056); - WIP; - tieout scripts; - notes files; - updated diff scripts; - fixed bug...; - add wdl and inputs file for warp pipeline; - reverting logging; - included top level WDL; - use gnarly with BQ extract cohort; - remove unused file; - cleaning up; - tidy; - tidy up before PR; - tidy up before PR; - PR comments; - merge conflict misfires; - added example SQL to create alt allele table from VET; - option to remove PLs; - fixed and enhanced unit test; - removing unused config, causing travis to fail; - add CreateVariantIngestFiles integration test (#7071); - add sampleName (instead of NULL) to error message (#7074); - Update To handle if no data error (#7084); - Memory improvement when writing missing positions to pet (#7098); - added support for loading QUALapprox into VET (#7101); - Add -m flag to gsutil step; add dockstore branch filters to facilitate development (#7104); - updates to ImportGenomes and LoadBigQueryData (#7112); - Add ngs to cohort extract Dockerfile; remove exception catching in extract python script (#7113); - remove problematic storage_location imports (#7119); - Reduce memory and CPU for CreateImportTsvs task, check for files before attempting load (#7121); - add -m flag to gsutil mv step (#7129); - ah_var_store : Add sample file argument to cohort extract (#7117); - Perform full WGS cohort extract scientific tieout for 35 ACMG59 samples (#7106); - Enable Read/Execution Project for BQ Queries (#7136); - ah - optional service account (#7140); - Add load lock file to prevent accidental re-loading of data to BQ (#7138); - #251 Address gvcf no-calls missing QUALapprox and other features (#7146); - Job Add labels to BQ operations from GATK (Issues-199) (#7115); - parse map to list to avoid brackets and spaces in vcf output (#7168); - #259 Inline schema for importgenomes.wdl (#7171); - Created AvroFileReader and unittest, Update ExtractCohort and ExtractCohortEngine (#7174); - #224 Import WDL: handle ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:23188,Integrability,wrap,wrap,23188,"res (#7735); - Add withdrawn and is_control columns [VS-70] [VS-213] (#7736); - Allow interval lists that require the SA to see (#7743); - allow for gatk to be overridden, update with known good jar (#7758); - VS-361 Add GvsWithdrawSamples wdl (#7765); - Extract Performance Improvements (#7686); - Don't put withdrawn sample data in alt_allele table [VS-369] (#7762); - remove PET code (#7768); - Adding AD for scale testing VS 225 add AD (#7713); - Deterministic Sample ID assignments [VS-371] (#7770); - remove R scripts from filtering (#7781); - Remove an old ""temp table"" dataset (#7780); - Clean up LocalizeFile [VS-314] (#7771); - Remove pet code from CreateVariantIngestFiles and friends [VS-375] (#7773); - 317 remove excess header values in VCF extract (#7786); - correct auth in split intervals (#7790); - Add code to (optionally) zero pad the vcf filename. (#7783); - LoadData `maxRetries` parameterized, default increased [VS-383] (#7791); - Update to latest version of ah_var_store gatk override jar (#7793); - GvsUnified WDL to wrap the 6 core GVS WDLs [VS-382] (#7789); - Pinned typing_extensions python package to 4.1.1 to fix conda environment. (#7802); - WeightedSplitInterval fixes [VS-384] [VS-332] (#7795); - Replace Travis with GithubActions (#7754); - Docker build only lfs pulls main/src/resources/large (#7727); - Clean up gatk jars -- looks like we are not passing them properly in the extract (#7788); - Fix typo that broke git lfs pull (#7806); - Document AoU SOP (up to the VAT) [VS-63] (#7807); - Incident VS 365 clinvar classification fix (#7769); - VS-390. Add precision and sensitivity wdl (#7813); - Quickstart based integration test [VS-357] (#7812); - 365 vat python testing additions (#7756); - VS 396 clinvar grabs too many values (#7823); - Added a test to validate WDLs in the scripts directory. (#7826) (#7829); - VAT Performance / Reliability Improvements (#7828); - VAT naming conventions [VS-410] (#7827); - Rc remove ad from vat (#7832); - bugfix, we were",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:23797,Integrability,integrat,integration,23797,"; - 317 remove excess header values in VCF extract (#7786); - correct auth in split intervals (#7790); - Add code to (optionally) zero pad the vcf filename. (#7783); - LoadData `maxRetries` parameterized, default increased [VS-383] (#7791); - Update to latest version of ah_var_store gatk override jar (#7793); - GvsUnified WDL to wrap the 6 core GVS WDLs [VS-382] (#7789); - Pinned typing_extensions python package to 4.1.1 to fix conda environment. (#7802); - WeightedSplitInterval fixes [VS-384] [VS-332] (#7795); - Replace Travis with GithubActions (#7754); - Docker build only lfs pulls main/src/resources/large (#7727); - Clean up gatk jars -- looks like we are not passing them properly in the extract (#7788); - Fix typo that broke git lfs pull (#7806); - Document AoU SOP (up to the VAT) [VS-63] (#7807); - Incident VS 365 clinvar classification fix (#7769); - VS-390. Add precision and sensitivity wdl (#7813); - Quickstart based integration test [VS-357] (#7812); - 365 vat python testing additions (#7756); - VS 396 clinvar grabs too many values (#7823); - Added a test to validate WDLs in the scripts directory. (#7826) (#7829); - VAT Performance / Reliability Improvements (#7828); - VAT naming conventions [VS-410] (#7827); - Rc remove ad from vat (#7832); - bugfix, we were trying to grep a binary file (#7837); - Cleanup scripts/variantstore [VS-414] (#7834); - Merge VAT TSV files into single bgzipped file [VS-304] (#7848); - Handle fully and partially loaded samples [VS-262] [VS-258] (#7843); - Ingest Error Handling Fixes [VS-261] (#7841); - First cut at a python notebook to validate inputs. (#7845); - Compute filter scatter [VS-392] (#7852); - remove withdrawn req (#7844); - Improve import error message [VS-437] (#7855); - Fix Input Validation python notebook (#7853); - Add VAT Validation check that aa_change and exon_number are consistently set. (#7850); - Ingest 10K [VS-344] (#7860); - X/Y chromosome reweighting for better extract shard runtime balance [VS-389] (#7868",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:24579,Integrability,message,message,24579,"lfs pull (#7806); - Document AoU SOP (up to the VAT) [VS-63] (#7807); - Incident VS 365 clinvar classification fix (#7769); - VS-390. Add precision and sensitivity wdl (#7813); - Quickstart based integration test [VS-357] (#7812); - 365 vat python testing additions (#7756); - VS 396 clinvar grabs too many values (#7823); - Added a test to validate WDLs in the scripts directory. (#7826) (#7829); - VAT Performance / Reliability Improvements (#7828); - VAT naming conventions [VS-410] (#7827); - Rc remove ad from vat (#7832); - bugfix, we were trying to grep a binary file (#7837); - Cleanup scripts/variantstore [VS-414] (#7834); - Merge VAT TSV files into single bgzipped file [VS-304] (#7848); - Handle fully and partially loaded samples [VS-262] [VS-258] (#7843); - Ingest Error Handling Fixes [VS-261] (#7841); - First cut at a python notebook to validate inputs. (#7845); - Compute filter scatter [VS-392] (#7852); - remove withdrawn req (#7844); - Improve import error message [VS-437] (#7855); - Fix Input Validation python notebook (#7853); - Add VAT Validation check that aa_change and exon_number are consistently set. (#7850); - Ingest 10K [VS-344] (#7860); - X/Y chromosome reweighting for better extract shard runtime balance [VS-389] (#7868); - VET Ingest Validation / Allow Ingest of non-VQSR'ed data (#7870); - Fix AoU workflow bugs (#7874); - Curate input arrays to skip already ingested sample data [VS-246] (#7862); - KM upload GVS product sheet (#7883); - Default extract scatter width [VS-415] (#7878); - Volatile tasks review [VS-447] (#7880); - Update Quickstart Integration for X/Y scaling changes [VS-464] (#7881); - clean up dockstore; - Rc vs 63 vat sop documentation (#7879); - Fix up FQ and race condition issues with volatile tasks work [VS-478] (#7888); - Use gvs-internal project in integration test (#7901); - Add cost observability BQ table [VS-441] (#7891); - Add preliminary labels to queries [VS-381] (#7902); - Workflow compute costs [VS-472] (#7905); - Fix bu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:25190,Integrability,Integrat,Integration,25190,"to validate inputs. (#7845); - Compute filter scatter [VS-392] (#7852); - remove withdrawn req (#7844); - Improve import error message [VS-437] (#7855); - Fix Input Validation python notebook (#7853); - Add VAT Validation check that aa_change and exon_number are consistently set. (#7850); - Ingest 10K [VS-344] (#7860); - X/Y chromosome reweighting for better extract shard runtime balance [VS-389] (#7868); - VET Ingest Validation / Allow Ingest of non-VQSR'ed data (#7870); - Fix AoU workflow bugs (#7874); - Curate input arrays to skip already ingested sample data [VS-246] (#7862); - KM upload GVS product sheet (#7883); - Default extract scatter width [VS-415] (#7878); - Volatile tasks review [VS-447] (#7880); - Update Quickstart Integration for X/Y scaling changes [VS-464] (#7881); - clean up dockstore; - Rc vs 63 vat sop documentation (#7879); - Fix up FQ and race condition issues with volatile tasks work [VS-478] (#7888); - Use gvs-internal project in integration test (#7901); - Add cost observability BQ table [VS-441] (#7891); - Add preliminary labels to queries [VS-381] (#7902); - Workflow compute costs [VS-472] (#7905); - Fix bug and update images (#7912); - VS 483 Beta user wdl (#7894); - Core storage model cost [VS-473] (#7913); - Update Quickstart & Integration to use re-blocked v2 gVCFs [VS-491] (#7924); - KM GVS documentation (#7903); - Track BigQuery costs of GVS python VS-480 (#7915); - Read cost observability table [VS-475] (#7923); - Fix Race Condition, Add Support for Extract by Array of Sample Names (ie from a Sample Set) (#7917); - Rightsize import batches [VS-486] (#7925); - [AoU DRC] Support uppercase site_ids for reblocking (#7929); - Populate cost metadata for GATK tasks. (#7919); - remove accidentally added input (#7931); - VS_492 - Beta User Jar release (#7934); - Cost WDL should throw on FISS API errors [VS-518] (#7942); - Fix bad check for missing workflow name [VS-520] (#7943); - Remove usage of service account from GvsValidateVAT.wdl (#7937",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:25419,Integrability,integrat,integration,25419,"to validate inputs. (#7845); - Compute filter scatter [VS-392] (#7852); - remove withdrawn req (#7844); - Improve import error message [VS-437] (#7855); - Fix Input Validation python notebook (#7853); - Add VAT Validation check that aa_change and exon_number are consistently set. (#7850); - Ingest 10K [VS-344] (#7860); - X/Y chromosome reweighting for better extract shard runtime balance [VS-389] (#7868); - VET Ingest Validation / Allow Ingest of non-VQSR'ed data (#7870); - Fix AoU workflow bugs (#7874); - Curate input arrays to skip already ingested sample data [VS-246] (#7862); - KM upload GVS product sheet (#7883); - Default extract scatter width [VS-415] (#7878); - Volatile tasks review [VS-447] (#7880); - Update Quickstart Integration for X/Y scaling changes [VS-464] (#7881); - clean up dockstore; - Rc vs 63 vat sop documentation (#7879); - Fix up FQ and race condition issues with volatile tasks work [VS-478] (#7888); - Use gvs-internal project in integration test (#7901); - Add cost observability BQ table [VS-441] (#7891); - Add preliminary labels to queries [VS-381] (#7902); - Workflow compute costs [VS-472] (#7905); - Fix bug and update images (#7912); - VS 483 Beta user wdl (#7894); - Core storage model cost [VS-473] (#7913); - Update Quickstart & Integration to use re-blocked v2 gVCFs [VS-491] (#7924); - KM GVS documentation (#7903); - Track BigQuery costs of GVS python VS-480 (#7915); - Read cost observability table [VS-475] (#7923); - Fix Race Condition, Add Support for Extract by Array of Sample Names (ie from a Sample Set) (#7917); - Rightsize import batches [VS-486] (#7925); - [AoU DRC] Support uppercase site_ids for reblocking (#7929); - Populate cost metadata for GATK tasks. (#7919); - remove accidentally added input (#7931); - VS_492 - Beta User Jar release (#7934); - Cost WDL should throw on FISS API errors [VS-518] (#7942); - Fix bad check for missing workflow name [VS-520] (#7943); - Remove usage of service account from GvsValidateVAT.wdl (#7937",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:25729,Integrability,Integrat,Integration,25729,"to validate inputs. (#7845); - Compute filter scatter [VS-392] (#7852); - remove withdrawn req (#7844); - Improve import error message [VS-437] (#7855); - Fix Input Validation python notebook (#7853); - Add VAT Validation check that aa_change and exon_number are consistently set. (#7850); - Ingest 10K [VS-344] (#7860); - X/Y chromosome reweighting for better extract shard runtime balance [VS-389] (#7868); - VET Ingest Validation / Allow Ingest of non-VQSR'ed data (#7870); - Fix AoU workflow bugs (#7874); - Curate input arrays to skip already ingested sample data [VS-246] (#7862); - KM upload GVS product sheet (#7883); - Default extract scatter width [VS-415] (#7878); - Volatile tasks review [VS-447] (#7880); - Update Quickstart Integration for X/Y scaling changes [VS-464] (#7881); - clean up dockstore; - Rc vs 63 vat sop documentation (#7879); - Fix up FQ and race condition issues with volatile tasks work [VS-478] (#7888); - Use gvs-internal project in integration test (#7901); - Add cost observability BQ table [VS-441] (#7891); - Add preliminary labels to queries [VS-381] (#7902); - Workflow compute costs [VS-472] (#7905); - Fix bug and update images (#7912); - VS 483 Beta user wdl (#7894); - Core storage model cost [VS-473] (#7913); - Update Quickstart & Integration to use re-blocked v2 gVCFs [VS-491] (#7924); - KM GVS documentation (#7903); - Track BigQuery costs of GVS python VS-480 (#7915); - Read cost observability table [VS-475] (#7923); - Fix Race Condition, Add Support for Extract by Array of Sample Names (ie from a Sample Set) (#7917); - Rightsize import batches [VS-486] (#7925); - [AoU DRC] Support uppercase site_ids for reblocking (#7929); - Populate cost metadata for GATK tasks. (#7919); - remove accidentally added input (#7931); - VS_492 - Beta User Jar release (#7934); - Cost WDL should throw on FISS API errors [VS-518] (#7942); - Fix bad check for missing workflow name [VS-520] (#7943); - Remove usage of service account from GvsValidateVAT.wdl (#7937",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:26897,Integrability,integrat,integration,26897,"x Race Condition, Add Support for Extract by Array of Sample Names (ie from a Sample Set) (#7917); - Rightsize import batches [VS-486] (#7925); - [AoU DRC] Support uppercase site_ids for reblocking (#7929); - Populate cost metadata for GATK tasks. (#7919); - remove accidentally added input (#7931); - VS_492 - Beta User Jar release (#7934); - Cost WDL should throw on FISS API errors [VS-518] (#7942); - Fix bad check for missing workflow name [VS-520] (#7943); - Remove usage of service account from GvsValidateVAT.wdl (#7937); - refactoring for testablity (#7946); - More import retries [VS-532] (#7953); - A few last doc changes (#7927); - WDL to extract a single callset cost (BQ only, not Terra) (#7940); - Temporarily swap in Corretto for Temurin as we can't download Temurin. (#7969); - GL-548 - Update CreateVat code to handle samples that do not contain all population groups. (#7965); - Restore Temurin 11 [VS-570] (#7972); - Add table size check to quickstart integration test [VS-501] (#7970); - Consolidate various docs for AoU callset generation into one to rule them all [VS-553] (#7971); - VS-567. Removing usage of ServiceAccount from CreateVat related WDLs (#7974); - WDL to extract Avro files for Hail import [VS-579] (#7981); - Removed usage of service account from WDLs (#7985); - Document steps for GVS cleanup for base use case [VS-586] (#7989); - Change backticks to single quotes in several error messages - causing shell to attempt to execute. (#7995); - VS-598 - Minor update to AoU Documentation. (#7994); - Allow for incremental addition of data to alt_allele [VS-52] (#7993); - Minor AoU Documentation Update (#7999); - Batch population of alt_allele table from vet_ tables [VS-265] (#7998); - Change drop_state to NONE for Ingest/Extract [VS-607] (#8000); - python -> python3 (#8001); - Generate Hail import/export script [VS-605] (#8002); - clearer error when values are missing (#7939); - Ah [VS-565] output intervals and sample list (#8010); - make CreateAltAlleleT",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:27348,Integrability,message,messages,27348,"#7931); - VS_492 - Beta User Jar release (#7934); - Cost WDL should throw on FISS API errors [VS-518] (#7942); - Fix bad check for missing workflow name [VS-520] (#7943); - Remove usage of service account from GvsValidateVAT.wdl (#7937); - refactoring for testablity (#7946); - More import retries [VS-532] (#7953); - A few last doc changes (#7927); - WDL to extract a single callset cost (BQ only, not Terra) (#7940); - Temporarily swap in Corretto for Temurin as we can't download Temurin. (#7969); - GL-548 - Update CreateVat code to handle samples that do not contain all population groups. (#7965); - Restore Temurin 11 [VS-570] (#7972); - Add table size check to quickstart integration test [VS-501] (#7970); - Consolidate various docs for AoU callset generation into one to rule them all [VS-553] (#7971); - VS-567. Removing usage of ServiceAccount from CreateVat related WDLs (#7974); - WDL to extract Avro files for Hail import [VS-579] (#7981); - Removed usage of service account from WDLs (#7985); - Document steps for GVS cleanup for base use case [VS-586] (#7989); - Change backticks to single quotes in several error messages - causing shell to attempt to execute. (#7995); - VS-598 - Minor update to AoU Documentation. (#7994); - Allow for incremental addition of data to alt_allele [VS-52] (#7993); - Minor AoU Documentation Update (#7999); - Batch population of alt_allele table from vet_ tables [VS-265] (#7998); - Change drop_state to NONE for Ingest/Extract [VS-607] (#8000); - python -> python3 (#8001); - Generate Hail import/export script [VS-605] (#8002); - clearer error when values are missing (#7939); - Ah [VS-565] output intervals and sample list (#8010); - make CreateAltAlleleTable task volatile (#8011); - Restore withdrawn [VS-581] (#8006); - Km gvs add storage cost and cleanup doc (#8012); - Updating documentation to reflect the changed outputs [VS-565] (#8014); - File of callset samples -> samples marked as 'withdrawn' in GVS [VS-436] (#8009); - fix quota guide",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:28993,Integrability,Integrat,Integration,28993,"ility to tweak sample-every-Nth-variant parameter for SNP model creation (#8019); - add initial notebook copy pasta (#8008); - add sample_table_timestamp to GetNumSamplesLoaded (#8022); - Batched Avro export [VS-630] (#8020); - Updating references to old GATK for VS-620 (#8023); - VS-517 Use standard version of GetBQTableLastModifiedDatetime in GvsValidateVat (#8024); - Fix bug in GvsWithdrawSamples.wdl (#8026); - Ah 617 exposing the drop_state parameter to the GvsJointVariantCalling wdl used for beta (and internal customer) (#8032); - Expose maximum-training-variants VQSR parameter [VS-634] (#8029); - Callset statistics [VS-560] (#8018); - Check for withdrawn before exporting to AVRO files [VS-646] (#8039); - Small updates to GVS Integration WDL [VS-618] (#8042); - Rework Hail script generation [VS-616] (#8034); - Alpine based Variant Store Docker image [VS-648] (#8044); - update warp version (#7906); - Fail Avro extract and callset stats on bad filter name [VS-655] (#8046); - Vs 629 failure to retrieve job information during ingest (#8047); - Restore accidentally removed bcftools [VS-661] (#8051); - Allowing our pipeline to function with a sample size of one (#8055); - Vs 665 re create vcf for cd 68 po 52339 with ad padding fixed (#8057); - VS-665 and VS-620 updating code to use latest docker images containing Rori's AD calculation changes in extract (#8061); - updating the beta workflow to use the latest jar, representing the version of GATK George tested against the workflow (#8062); - VS-637 Address a couple of issues in SampleLoadStatus handling in GVSImportGenomes. (#8052); - Revert Alpinizing of apt dependent task [VS-688] (#8065); - Fix missing vat schema JSONs [VS-699] (#8072); - Fix integration expectations for fixed AD [VS-689] (#8066); - VS-698 Remove unnecessary columns from Call set statistics (#8073); - Fix Dockerfile nits that break 20.10.21 (#8078); - Nirvana 3.18.1 Docker images support [VS-661] (#8082); - Add option to not prepare __REF_DATA or __",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:29887,Integrability,depend,dependent,29887,"Integration WDL [VS-618] (#8042); - Rework Hail script generation [VS-616] (#8034); - Alpine based Variant Store Docker image [VS-648] (#8044); - update warp version (#7906); - Fail Avro extract and callset stats on bad filter name [VS-655] (#8046); - Vs 629 failure to retrieve job information during ingest (#8047); - Restore accidentally removed bcftools [VS-661] (#8051); - Allowing our pipeline to function with a sample size of one (#8055); - Vs 665 re create vcf for cd 68 po 52339 with ad padding fixed (#8057); - VS-665 and VS-620 updating code to use latest docker images containing Rori's AD calculation changes in extract (#8061); - updating the beta workflow to use the latest jar, representing the version of GATK George tested against the workflow (#8062); - VS-637 Address a couple of issues in SampleLoadStatus handling in GVSImportGenomes. (#8052); - Revert Alpinizing of apt dependent task [VS-688] (#8065); - Fix missing vat schema JSONs [VS-699] (#8072); - Fix integration expectations for fixed AD [VS-689] (#8066); - VS-698 Remove unnecessary columns from Call set statistics (#8073); - Fix Dockerfile nits that break 20.10.21 (#8078); - Nirvana 3.18.1 Docker images support [VS-661] (#8082); - Add option to not prepare __REF_DATA or __SAMPLES tables to Prepare [VS-697] (#8079); - ""build-base"" Docker image for faster variantstore image builds [VS-712] (#8085); - GVS / Hail VDS integration test [VS-639] (#8086); - Remove AI/AN from VDS docs [VS-726] (#8096); - Add flag for cost_observability table writing to support sub-cohort use case [VS-521] (#8093); - Document STS delivery process for VDS [VS-727] (#8101); - delete obsolete callset_QC directory and its contents [VS-318] (#8108); - doc link typo and add check for control samples in AVRO export (#8110); - Add defaults for scatter_count in GvsExtractCohortFromSampleNames [VS-496] (#8109); - Escape table names properly in ValidateVat WDL (#8116); - Vs 741 fix indefinite freeze in split intervals task when using ex",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:29975,Integrability,integrat,integration,29975,"Integration WDL [VS-618] (#8042); - Rework Hail script generation [VS-616] (#8034); - Alpine based Variant Store Docker image [VS-648] (#8044); - update warp version (#7906); - Fail Avro extract and callset stats on bad filter name [VS-655] (#8046); - Vs 629 failure to retrieve job information during ingest (#8047); - Restore accidentally removed bcftools [VS-661] (#8051); - Allowing our pipeline to function with a sample size of one (#8055); - Vs 665 re create vcf for cd 68 po 52339 with ad padding fixed (#8057); - VS-665 and VS-620 updating code to use latest docker images containing Rori's AD calculation changes in extract (#8061); - updating the beta workflow to use the latest jar, representing the version of GATK George tested against the workflow (#8062); - VS-637 Address a couple of issues in SampleLoadStatus handling in GVSImportGenomes. (#8052); - Revert Alpinizing of apt dependent task [VS-688] (#8065); - Fix missing vat schema JSONs [VS-699] (#8072); - Fix integration expectations for fixed AD [VS-689] (#8066); - VS-698 Remove unnecessary columns from Call set statistics (#8073); - Fix Dockerfile nits that break 20.10.21 (#8078); - Nirvana 3.18.1 Docker images support [VS-661] (#8082); - Add option to not prepare __REF_DATA or __SAMPLES tables to Prepare [VS-697] (#8079); - ""build-base"" Docker image for faster variantstore image builds [VS-712] (#8085); - GVS / Hail VDS integration test [VS-639] (#8086); - Remove AI/AN from VDS docs [VS-726] (#8096); - Add flag for cost_observability table writing to support sub-cohort use case [VS-521] (#8093); - Document STS delivery process for VDS [VS-727] (#8101); - delete obsolete callset_QC directory and its contents [VS-318] (#8108); - doc link typo and add check for control samples in AVRO export (#8110); - Add defaults for scatter_count in GvsExtractCohortFromSampleNames [VS-496] (#8109); - Escape table names properly in ValidateVat WDL (#8116); - Vs 741 fix indefinite freeze in split intervals task when using ex",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:30397,Integrability,integrat,integration,30397,"ge tested against the workflow (#8062); - VS-637 Address a couple of issues in SampleLoadStatus handling in GVSImportGenomes. (#8052); - Revert Alpinizing of apt dependent task [VS-688] (#8065); - Fix missing vat schema JSONs [VS-699] (#8072); - Fix integration expectations for fixed AD [VS-689] (#8066); - VS-698 Remove unnecessary columns from Call set statistics (#8073); - Fix Dockerfile nits that break 20.10.21 (#8078); - Nirvana 3.18.1 Docker images support [VS-661] (#8082); - Add option to not prepare __REF_DATA or __SAMPLES tables to Prepare [VS-697] (#8079); - ""build-base"" Docker image for faster variantstore image builds [VS-712] (#8085); - GVS / Hail VDS integration test [VS-639] (#8086); - Remove AI/AN from VDS docs [VS-726] (#8096); - Add flag for cost_observability table writing to support sub-cohort use case [VS-521] (#8093); - Document STS delivery process for VDS [VS-727] (#8101); - delete obsolete callset_QC directory and its contents [VS-318] (#8108); - doc link typo and add check for control samples in AVRO export (#8110); - Add defaults for scatter_count in GvsExtractCohortFromSampleNames [VS-496] (#8109); - Escape table names properly in ValidateVat WDL (#8116); - Vs 741 fix indefinite freeze in split intervals task when using exome data (#8113); - VAT Readme updates (#8090); - WDL and python scripts to use the VDS in the VAT (#8077); - VS-757 - Use JASIX to make sub-jsons of annotated output of Nirvana (#8133); - add note about permissions for P&S workflow to work (#8135); - VS-759 (and VS-760) (#8137); - VS-765. Scatter the RemoveDuplicates task. (#8144); - update delivery docs based on latest VDS delivery run [VS-770] (#8150); - Add monitoring to index vcf (#8151); - Make some noise when VDS validation succeeds (#8155); - Handle empty genes annotation file. (#8153); - Add escapes for otherwise problematic dataset / table names. (#8162); - New WDL to create VAT tsvs from previously generated BigQuery table. (#8165); - Treat withdrawn samples in ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:329,Modifiability,refactor,refactored,329,,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:410,Modifiability,refactor,refactor,410,,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:800,Modifiability,refactor,refactored,800,,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:1017,Modifiability,refactor,refactor,1017,"rt extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - forgot this file; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - add fields for uncompressed imputed data; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - Tool for arrays QC metrics calculations (#6812); - ah update array extract tool (#6827); - fix enum (#6834); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - resolved rebase conflicts; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to samp",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:1975,Modifiability,refactor,refactored,1975,"ngest; - add imputed tsv creator and refactor; - add fields for uncompressed imputed data; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - Tool for arrays QC metrics calculations (#6812); - ah update array extract tool (#6827); - fix enum (#6834); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - resolved rebase conflicts; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - updating ArrayCalculateMetrics fo",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:2056,Modifiability,refactor,refactor,2056,"uted data; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - Tool for arrays QC metrics calculations (#6812); - ah update array extract tool (#6827); - fix enum (#6834); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - resolved rebase conflicts; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:2446,Modifiability,refactor,refactored,2446,"4); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - resolved rebase conflicts; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - Moving the WDL for importing array manifest to BQ (#6860); - fix up after rebase; - Moving and testing ingest scripts from variantstore (#6881); - optionally provide sample-map-file instead of sample-map-table (#6872); - Moving extract wdls from variantstore repo (#6902); - update for genomes (#6918); - update pat",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:2663,Modifiability,refactor,refactor,2663,"sed on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - resolved rebase conflicts; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - Moving the WDL for importing array manifest to BQ (#6860); - fix up after rebase; - Moving and testing ingest scripts from variantstore (#6881); - optionally provide sample-map-file instead of sample-map-table (#6872); - Moving extract wdls from variantstore repo (#6902); - update for genomes (#6918); - update paths; - update field name; - consolidate exome and genome code; - missing comma; - allow null for drop state.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:5099,Modifiability,enhance,enhanced,5099,,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:5137,Modifiability,config,config,5137,,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:6301,Modifiability,refactor,refactored,6301,"if no data error (#7084); - Memory improvement when writing missing positions to pet (#7098); - added support for loading QUALapprox into VET (#7101); - Add -m flag to gsutil step; add dockstore branch filters to facilitate development (#7104); - updates to ImportGenomes and LoadBigQueryData (#7112); - Add ngs to cohort extract Dockerfile; remove exception catching in extract python script (#7113); - remove problematic storage_location imports (#7119); - Reduce memory and CPU for CreateImportTsvs task, check for files before attempting load (#7121); - add -m flag to gsutil mv step (#7129); - ah_var_store : Add sample file argument to cohort extract (#7117); - wip; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - forgot this file; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - add fields for uncompressed imputed data; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:6382,Modifiability,refactor,refactor,6382," pet (#7098); - added support for loading QUALapprox into VET (#7101); - Add -m flag to gsutil step; add dockstore branch filters to facilitate development (#7104); - updates to ImportGenomes and LoadBigQueryData (#7112); - Add ngs to cohort extract Dockerfile; remove exception catching in extract python script (#7113); - remove problematic storage_location imports (#7119); - Reduce memory and CPU for CreateImportTsvs task, check for files before attempting load (#7121); - add -m flag to gsutil mv step (#7129); - ah_var_store : Add sample file argument to cohort extract (#7117); - wip; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - forgot this file; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - add fields for uncompressed imputed data; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - Tool for arrays QC metrics calculations (#6812); - ah update array ex",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:6772,Modifiability,refactor,refactored,6772,"y and CPU for CreateImportTsvs task, check for files before attempting load (#7121); - add -m flag to gsutil mv step (#7129); - ah_var_store : Add sample file argument to cohort extract (#7117); - wip; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - forgot this file; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - add fields for uncompressed imputed data; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - Tool for arrays QC metrics calculations (#6812); - ah update array extract tool (#6827); - fix enum (#6834); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - resolved rebase conflicts; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is manda",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:6989,Modifiability,refactor,refactor,6989,"rt extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - forgot this file; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - add fields for uncompressed imputed data; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - Tool for arrays QC metrics calculations (#6812); - ah update array extract tool (#6827); - fix enum (#6834); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - resolved rebase conflicts; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to samp",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:7947,Modifiability,refactor,refactored,7947,"ngest; - add imputed tsv creator and refactor; - add fields for uncompressed imputed data; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - Tool for arrays QC metrics calculations (#6812); - ah update array extract tool (#6827); - fix enum (#6834); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - resolved rebase conflicts; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - updating ArrayCalculateMetrics fo",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:8028,Modifiability,refactor,refactor,8028,"uted data; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - Tool for arrays QC metrics calculations (#6812); - ah update array extract tool (#6827); - fix enum (#6834); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - resolved rebase conflicts; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:8418,Modifiability,refactor,refactored,8418,"4); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - resolved rebase conflicts; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - Moving the WDL for importing array manifest to BQ (#6860); - fix up after rebase; - Moving and testing ingest scripts from variantstore (#6881); - optionally provide sample-map-file instead of sample-map-table (#6872); - Moving extract wdls from variantstore repo (#6902); - update for genomes (#6918); - update pat",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:8635,Modifiability,refactor,refactor,8635,"sed on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - resolved rebase conflicts; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - Moving the WDL for importing array manifest to BQ (#6860); - fix up after rebase; - Moving and testing ingest scripts from variantstore (#6881); - optionally provide sample-map-file instead of sample-map-table (#6872); - Moving extract wdls from variantstore repo (#6902); - update for genomes (#6918); - update paths; - update field name; - consolidate exome and genome code; - missing comma; - allow null for drop state.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:11071,Modifiability,enhance,enhanced,11071," table creation and data loading in LoadData (#7056); - WIP; - tieout scripts; - notes files; - updated diff scripts; - fixed bug...; - add wdl and inputs file for warp pipeline; - reverting logging; - included top level WDL; - use gnarly with BQ extract cohort; - remove unused file; - cleaning up; - tidy; - tidy up before PR; - tidy up before PR; - PR comments; - merge conflict misfires; - added example SQL to create alt allele table from VET; - option to remove PLs; - fixed and enhanced unit test; - removing unused config, causing travis to fail; - add CreateVariantIngestFiles integration test (#7071); - add sampleName (instead of NULL) to error message (#7074); - Update To handle if no data error (#7084); - Memory improvement when writing missing positions to pet (#7098); - added support for loading QUALapprox into VET (#7101); - Add -m flag to gsutil step; add dockstore branch filters to facilitate development (#7104); - updates to ImportGenomes and LoadBigQueryData (#7112); - Add ngs to cohort extract Dockerfile; remove exception catching in extract python script (#7113); - remove problematic storage_location imports (#7119); - Reduce memory and CPU for CreateImportTsvs task, check for files before attempting load (#7121); - add -m flag to gsutil mv step (#7129); - ah_var_store : Add sample file argument to cohort extract (#7117); - Perform full WGS cohort extract scientific tieout for 35 ACMG59 samples (#7106); - Enable Read/Execution Project for BQ Queries (#7136); - ah - optional service account (#7140); - Add load lock file to prevent accidental re-loading of data to BQ (#7138); - #251 Address gvcf no-calls missing QUALapprox and other features (#7146); - Job Add labels to BQ operations from GATK (Issues-199) (#7115); - parse map to list to avoid brackets and spaces in vcf output (#7168); - #259 Inline schema for importgenomes.wdl (#7171); - Created AvroFileReader and unittest, Update ExtractCohort and ExtractCohortEngine (#7174); - #224 Import WDL: handle ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:11109,Modifiability,config,config,11109," table creation and data loading in LoadData (#7056); - WIP; - tieout scripts; - notes files; - updated diff scripts; - fixed bug...; - add wdl and inputs file for warp pipeline; - reverting logging; - included top level WDL; - use gnarly with BQ extract cohort; - remove unused file; - cleaning up; - tidy; - tidy up before PR; - tidy up before PR; - PR comments; - merge conflict misfires; - added example SQL to create alt allele table from VET; - option to remove PLs; - fixed and enhanced unit test; - removing unused config, causing travis to fail; - add CreateVariantIngestFiles integration test (#7071); - add sampleName (instead of NULL) to error message (#7074); - Update To handle if no data error (#7084); - Memory improvement when writing missing positions to pet (#7098); - added support for loading QUALapprox into VET (#7101); - Add -m flag to gsutil step; add dockstore branch filters to facilitate development (#7104); - updates to ImportGenomes and LoadBigQueryData (#7112); - Add ngs to cohort extract Dockerfile; remove exception catching in extract python script (#7113); - remove problematic storage_location imports (#7119); - Reduce memory and CPU for CreateImportTsvs task, check for files before attempting load (#7121); - add -m flag to gsutil mv step (#7129); - ah_var_store : Add sample file argument to cohort extract (#7117); - Perform full WGS cohort extract scientific tieout for 35 ACMG59 samples (#7106); - Enable Read/Execution Project for BQ Queries (#7136); - ah - optional service account (#7140); - Add load lock file to prevent accidental re-loading of data to BQ (#7138); - #251 Address gvcf no-calls missing QUALapprox and other features (#7146); - Job Add labels to BQ operations from GATK (Issues-199) (#7115); - parse map to list to avoid brackets and spaces in vcf output (#7168); - #259 Inline schema for importgenomes.wdl (#7171); - Created AvroFileReader and unittest, Update ExtractCohort and ExtractCohortEngine (#7174); - #224 Import WDL: handle ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:14093,Modifiability,parameteriz,parameterize,14093,"ut allow NONE as input (#7206); - SA support and consistent naming for all GVS WDLs (#7205); - fix GvsExtractCallset inputs file (#7210); - add clustering to tables (#7207); - add vqsr cutoffs to GvsExtractCallset wdl; clean up dockstore yml (#7209); - Avro test (#7192); - Enable call caching of TSV generation in GvsImportGenomes (#7226); - 266 Clean up ExtractCohort -- remove query mode param (#7227); - 288 Add an excess alleles param (#7221); - take sample name as a param (#7236); - How to run GIAB comparisons (#7237); - Update GvsCreateFilterSet.wdl (#7239); - Use GatherVcfsCloud in GvsCreateFilterSet.wdl (#7241); - parameterize TTL with defaults, reduce memory allocation (#7244); - Addressing OOM in CohortExtract (#7245); - make outputs optional, change case in output (#7252); - Support for FORMAT/FT VQSLod Filtering and cohort-wide LowQual filter (#7248); - removed arrays code, renamed packages (#7260); - 279 labels (#7233); - add conda commands to GIAB readme (#7268); - remove gvs branch (#7263); - remove gvs branch (#7263); - upgrade bq libraries (#7264); - #299 - Sample list ease of use for cohort extracts (#7272); - check for duplicate ids (#7273); - Rc 274 passing sites only (#7275); - added default value to drop_state; broadinstitute/dsp-spec-ops#310 (#7278); - version bump for reliability (#7284); - add timestamp check to ExtractTask call https://github.com/broadinstitute/dsp-spec-ops/issues/320; - serial inserts for scaling prepare, factored out sample name (#7288); - Remove training sites only param from ExtractFeatures broadinstitute/dsp-spec-ops#261; - add param for mem for indels (#7282); - Ah prepare localize option (#7299); - Export sites only vcf STEP 1-- 317 add AC, AN, AF to the final VCF (#7279); - AoU GVS Cohort Extract wdl (#7242); - reliability (#7310); - bump to include FT tag filtering (#7316); - First pass at a Terra QuickStart (#7267); - Ah fix timestamp query (#7319); - 313 Cleanup Extract Cohort params (#7293); - bump bq storage versi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:19005,Modifiability,refactor,refactor,19005,"463); - formatting on sample QC README; - formatting change #2 to sample QC README; - address VS-152, remove extra headers from extract (#7466); - Update GvsExtractCallset.example.inputs.json (#7469); - Add ability to copy interval list files to gs directory [VS-191] (#7467); - add an expiration date to the temp tables (#7455); - fix the check for duplicates in import genomes (#7470); - added job ID to alt_allele population call output [VS-194] (#7473); - added steps and deliverables to GVS README [VS-181] (#7452); - Ah check the is loaded field in feature extract (#7475); - changes to put pet data directly into data table (#7478); - added override for ExtractTasks' preemptible value (#7477); - bcftools to the rescue (#7456); - execute_with_retry() refactor and error handling improvements [VS-159] (#7480); - Small updates to GvsExtractCallset from beta callset, new workflow for re-scattered shards (#7493); - add flag in prepare to print out sql instead of executing (#7501); - Workflow to re-scatter and then merge ""problematic"" intervals from ExtractCallset [VS-209] (#7495); - changed README to reflect comments from Lee [VS-210] (#7502); - Export the VAT into GCS (#7472); - addresses VS-219 (#7508); - small fix to MergeVCFs (#7517); - small fixes to GVS pipeline (#7522); - make sure ExtractTask is run on all interval files; - Revert ""make sure ExtractTask is run on all interval files""; - make sure ExtractTask is run on all interval files (#7527); - Remove Sites only step from the VAT creation WDL (#7510); - fix bad argument processing for bool (#7529); - Support for TDR DRS URIs in Import (#7528); - Match format of filename output in GvsRescatterCallsetInterval (#7539); - Reference block storage and query support (#7498); - update docs (#7540); - Kc fix rr load bug (#7550); - Update .dockstore.yml (#7553); - Ah add reblocking wdl (#7544); - Scatter over all interval files, not just scatter count (#7551); - fixed docker (#7558); - take advantage of fixed version of Spl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:23047,Modifiability,parameteriz,parameterized,23047,"res (#7735); - Add withdrawn and is_control columns [VS-70] [VS-213] (#7736); - Allow interval lists that require the SA to see (#7743); - allow for gatk to be overridden, update with known good jar (#7758); - VS-361 Add GvsWithdrawSamples wdl (#7765); - Extract Performance Improvements (#7686); - Don't put withdrawn sample data in alt_allele table [VS-369] (#7762); - remove PET code (#7768); - Adding AD for scale testing VS 225 add AD (#7713); - Deterministic Sample ID assignments [VS-371] (#7770); - remove R scripts from filtering (#7781); - Remove an old ""temp table"" dataset (#7780); - Clean up LocalizeFile [VS-314] (#7771); - Remove pet code from CreateVariantIngestFiles and friends [VS-375] (#7773); - 317 remove excess header values in VCF extract (#7786); - correct auth in split intervals (#7790); - Add code to (optionally) zero pad the vcf filename. (#7783); - LoadData `maxRetries` parameterized, default increased [VS-383] (#7791); - Update to latest version of ah_var_store gatk override jar (#7793); - GvsUnified WDL to wrap the 6 core GVS WDLs [VS-382] (#7789); - Pinned typing_extensions python package to 4.1.1 to fix conda environment. (#7802); - WeightedSplitInterval fixes [VS-384] [VS-332] (#7795); - Replace Travis with GithubActions (#7754); - Docker build only lfs pulls main/src/resources/large (#7727); - Clean up gatk jars -- looks like we are not passing them properly in the extract (#7788); - Fix typo that broke git lfs pull (#7806); - Document AoU SOP (up to the VAT) [VS-63] (#7807); - Incident VS 365 clinvar classification fix (#7769); - VS-390. Add precision and sensitivity wdl (#7813); - Quickstart based integration test [VS-357] (#7812); - 365 vat python testing additions (#7756); - VS 396 clinvar grabs too many values (#7823); - Added a test to validate WDLs in the scripts directory. (#7826) (#7829); - VAT Performance / Reliability Improvements (#7828); - VAT naming conventions [VS-410] (#7827); - Rc remove ad from vat (#7832); - bugfix, we were",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:26457,Modifiability,refactor,refactoring,26457," [VS-472] (#7905); - Fix bug and update images (#7912); - VS 483 Beta user wdl (#7894); - Core storage model cost [VS-473] (#7913); - Update Quickstart & Integration to use re-blocked v2 gVCFs [VS-491] (#7924); - KM GVS documentation (#7903); - Track BigQuery costs of GVS python VS-480 (#7915); - Read cost observability table [VS-475] (#7923); - Fix Race Condition, Add Support for Extract by Array of Sample Names (ie from a Sample Set) (#7917); - Rightsize import batches [VS-486] (#7925); - [AoU DRC] Support uppercase site_ids for reblocking (#7929); - Populate cost metadata for GATK tasks. (#7919); - remove accidentally added input (#7931); - VS_492 - Beta User Jar release (#7934); - Cost WDL should throw on FISS API errors [VS-518] (#7942); - Fix bad check for missing workflow name [VS-520] (#7943); - Remove usage of service account from GvsValidateVAT.wdl (#7937); - refactoring for testablity (#7946); - More import retries [VS-532] (#7953); - A few last doc changes (#7927); - WDL to extract a single callset cost (BQ only, not Terra) (#7940); - Temporarily swap in Corretto for Temurin as we can't download Temurin. (#7969); - GL-548 - Update CreateVat code to handle samples that do not contain all population groups. (#7965); - Restore Temurin 11 [VS-570] (#7972); - Add table size check to quickstart integration test [VS-501] (#7970); - Consolidate various docs for AoU callset generation into one to rule them all [VS-553] (#7971); - VS-567. Removing usage of ServiceAccount from CreateVat related WDLs (#7974); - WDL to extract Avro files for Hail import [VS-579] (#7981); - Removed usage of service account from WDLs (#7985); - Document steps for GVS cleanup for base use case [VS-586] (#7989); - Change backticks to single quotes in several error messages - causing shell to attempt to execute. (#7995); - VS-598 - Minor update to AoU Documentation. (#7994); - Allow for incremental addition of data to alt_allele [VS-52] (#7993); - Minor AoU Documentation Update (#7999); -",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:4470,Performance,optimiz,optimized,4470,"e-map-file instead of sample-map-table (#6872); - Moving extract wdls from variantstore repo (#6902); - update for genomes (#6918); - update paths; - update field name; - consolidate exome and genome code; - missing comma; - allow null for drop state. add to wdl; - fix when NON-REF has value (drop it anyway). add memory to wdl; - fix order of columns in vet; - Making ingest work on gvcfs without allele specific annotations (#6934); - fix partitioning (#6939); - more fixes for non-allele-specific gvcfs (#6945); - fix GT for ./.; - add in feature extract code. resulting vcf still has bugs (#6947); - Rudimentary support for other output types, support for scattering extract cohort (#6949); - added batch/interactive flag, removed samples (#6953); - WIP for Feature Extract code (#6958); - move wdl and cromwell_tests dirs under variantstore (#6961); - WDL to run feature extract, VQSR, and upload (#6966); - support for multiple PET/VETs (#6969); - Add filtering to ExtractCohort (#6971); - support for non-AS called data (#6975); - fixing bug in filtering (#6976); - Updating extract wdls to apply filtering (#6977); - moving from specops repo (#6983); - optimized TSV experiment, and range GQ dropping (#6987); - back to 30x defaults; - allow no filtering to be applied (#7004); - Separate bigquery table creation and data loading in LoadData (#7056); - WIP; - tieout scripts; - notes files; - updated diff scripts; - fixed bug...; - add wdl and inputs file for warp pipeline; - reverting logging; - included top level WDL; - use gnarly with BQ extract cohort; - remove unused file; - cleaning up; - tidy; - tidy up before PR; - tidy up before PR; - PR comments; - merge conflict misfires; - added example SQL to create alt allele table from VET; - option to remove PLs; - fixed and enhanced unit test; - removing unused config, causing travis to fail; - add CreateVariantIngestFiles integration test (#7071); - add sampleName (instead of NULL) to error message (#7074); - Update To handle if",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:4639,Performance,load,loading,4639,"e-map-file instead of sample-map-table (#6872); - Moving extract wdls from variantstore repo (#6902); - update for genomes (#6918); - update paths; - update field name; - consolidate exome and genome code; - missing comma; - allow null for drop state. add to wdl; - fix when NON-REF has value (drop it anyway). add memory to wdl; - fix order of columns in vet; - Making ingest work on gvcfs without allele specific annotations (#6934); - fix partitioning (#6939); - more fixes for non-allele-specific gvcfs (#6945); - fix GT for ./.; - add in feature extract code. resulting vcf still has bugs (#6947); - Rudimentary support for other output types, support for scattering extract cohort (#6949); - added batch/interactive flag, removed samples (#6953); - WIP for Feature Extract code (#6958); - move wdl and cromwell_tests dirs under variantstore (#6961); - WDL to run feature extract, VQSR, and upload (#6966); - support for multiple PET/VETs (#6969); - Add filtering to ExtractCohort (#6971); - support for non-AS called data (#6975); - fixing bug in filtering (#6976); - Updating extract wdls to apply filtering (#6977); - moving from specops repo (#6983); - optimized TSV experiment, and range GQ dropping (#6987); - back to 30x defaults; - allow no filtering to be applied (#7004); - Separate bigquery table creation and data loading in LoadData (#7056); - WIP; - tieout scripts; - notes files; - updated diff scripts; - fixed bug...; - add wdl and inputs file for warp pipeline; - reverting logging; - included top level WDL; - use gnarly with BQ extract cohort; - remove unused file; - cleaning up; - tidy; - tidy up before PR; - tidy up before PR; - PR comments; - merge conflict misfires; - added example SQL to create alt allele table from VET; - option to remove PLs; - fixed and enhanced unit test; - removing unused config, causing travis to fail; - add CreateVariantIngestFiles integration test (#7071); - add sampleName (instead of NULL) to error message (#7074); - Update To handle if",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:4650,Performance,Load,LoadData,4650,"e-map-file instead of sample-map-table (#6872); - Moving extract wdls from variantstore repo (#6902); - update for genomes (#6918); - update paths; - update field name; - consolidate exome and genome code; - missing comma; - allow null for drop state. add to wdl; - fix when NON-REF has value (drop it anyway). add memory to wdl; - fix order of columns in vet; - Making ingest work on gvcfs without allele specific annotations (#6934); - fix partitioning (#6939); - more fixes for non-allele-specific gvcfs (#6945); - fix GT for ./.; - add in feature extract code. resulting vcf still has bugs (#6947); - Rudimentary support for other output types, support for scattering extract cohort (#6949); - added batch/interactive flag, removed samples (#6953); - WIP for Feature Extract code (#6958); - move wdl and cromwell_tests dirs under variantstore (#6961); - WDL to run feature extract, VQSR, and upload (#6966); - support for multiple PET/VETs (#6969); - Add filtering to ExtractCohort (#6971); - support for non-AS called data (#6975); - fixing bug in filtering (#6976); - Updating extract wdls to apply filtering (#6977); - moving from specops repo (#6983); - optimized TSV experiment, and range GQ dropping (#6987); - back to 30x defaults; - allow no filtering to be applied (#7004); - Separate bigquery table creation and data loading in LoadData (#7056); - WIP; - tieout scripts; - notes files; - updated diff scripts; - fixed bug...; - add wdl and inputs file for warp pipeline; - reverting logging; - included top level WDL; - use gnarly with BQ extract cohort; - remove unused file; - cleaning up; - tidy; - tidy up before PR; - tidy up before PR; - PR comments; - merge conflict misfires; - added example SQL to create alt allele table from VET; - option to remove PLs; - fixed and enhanced unit test; - removing unused config, causing travis to fail; - add CreateVariantIngestFiles integration test (#7071); - add sampleName (instead of NULL) to error message (#7074); - Update To handle if",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:5420,Performance,load,loading,5420,,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:5582,Performance,Load,LoadBigQueryData,5582,,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:5848,Performance,load,load,5848,"ly with BQ extract cohort; - remove unused file; - cleaning up; - tidy; - tidy up before PR; - tidy up before PR; - PR comments; - merge conflict misfires; - added example SQL to create alt allele table from VET; - option to remove PLs; - fixed and enhanced unit test; - removing unused config, causing travis to fail; - add CreateVariantIngestFiles integration test (#7071); - add sampleName (instead of NULL) to error message (#7074); - Update To handle if no data error (#7084); - Memory improvement when writing missing positions to pet (#7098); - added support for loading QUALapprox into VET (#7101); - Add -m flag to gsutil step; add dockstore branch filters to facilitate development (#7104); - updates to ImportGenomes and LoadBigQueryData (#7112); - Add ngs to cohort extract Dockerfile; remove exception catching in extract python script (#7113); - remove problematic storage_location imports (#7119); - Reduce memory and CPU for CreateImportTsvs task, check for files before attempting load (#7121); - add -m flag to gsutil mv step (#7129); - ah_var_store : Add sample file argument to cohort extract (#7117); - wip; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - forgot this file; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support loca",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:10442,Performance,optimiz,optimized,10442,"e-map-file instead of sample-map-table (#6872); - Moving extract wdls from variantstore repo (#6902); - update for genomes (#6918); - update paths; - update field name; - consolidate exome and genome code; - missing comma; - allow null for drop state. add to wdl; - fix when NON-REF has value (drop it anyway). add memory to wdl; - fix order of columns in vet; - Making ingest work on gvcfs without allele specific annotations (#6934); - fix partitioning (#6939); - more fixes for non-allele-specific gvcfs (#6945); - fix GT for ./.; - add in feature extract code. resulting vcf still has bugs (#6947); - Rudimentary support for other output types, support for scattering extract cohort (#6949); - added batch/interactive flag, removed samples (#6953); - WIP for Feature Extract code (#6958); - move wdl and cromwell_tests dirs under variantstore (#6961); - WDL to run feature extract, VQSR, and upload (#6966); - support for multiple PET/VETs (#6969); - Add filtering to ExtractCohort (#6971); - support for non-AS called data (#6975); - fixing bug in filtering (#6976); - Updating extract wdls to apply filtering (#6977); - moving from specops repo (#6983); - optimized TSV experiment, and range GQ dropping (#6987); - back to 30x defaults; - allow no filtering to be applied (#7004); - Separate bigquery table creation and data loading in LoadData (#7056); - WIP; - tieout scripts; - notes files; - updated diff scripts; - fixed bug...; - add wdl and inputs file for warp pipeline; - reverting logging; - included top level WDL; - use gnarly with BQ extract cohort; - remove unused file; - cleaning up; - tidy; - tidy up before PR; - tidy up before PR; - PR comments; - merge conflict misfires; - added example SQL to create alt allele table from VET; - option to remove PLs; - fixed and enhanced unit test; - removing unused config, causing travis to fail; - add CreateVariantIngestFiles integration test (#7071); - add sampleName (instead of NULL) to error message (#7074); - Update To handle if",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:10611,Performance,load,loading,10611,"e-map-file instead of sample-map-table (#6872); - Moving extract wdls from variantstore repo (#6902); - update for genomes (#6918); - update paths; - update field name; - consolidate exome and genome code; - missing comma; - allow null for drop state. add to wdl; - fix when NON-REF has value (drop it anyway). add memory to wdl; - fix order of columns in vet; - Making ingest work on gvcfs without allele specific annotations (#6934); - fix partitioning (#6939); - more fixes for non-allele-specific gvcfs (#6945); - fix GT for ./.; - add in feature extract code. resulting vcf still has bugs (#6947); - Rudimentary support for other output types, support for scattering extract cohort (#6949); - added batch/interactive flag, removed samples (#6953); - WIP for Feature Extract code (#6958); - move wdl and cromwell_tests dirs under variantstore (#6961); - WDL to run feature extract, VQSR, and upload (#6966); - support for multiple PET/VETs (#6969); - Add filtering to ExtractCohort (#6971); - support for non-AS called data (#6975); - fixing bug in filtering (#6976); - Updating extract wdls to apply filtering (#6977); - moving from specops repo (#6983); - optimized TSV experiment, and range GQ dropping (#6987); - back to 30x defaults; - allow no filtering to be applied (#7004); - Separate bigquery table creation and data loading in LoadData (#7056); - WIP; - tieout scripts; - notes files; - updated diff scripts; - fixed bug...; - add wdl and inputs file for warp pipeline; - reverting logging; - included top level WDL; - use gnarly with BQ extract cohort; - remove unused file; - cleaning up; - tidy; - tidy up before PR; - tidy up before PR; - PR comments; - merge conflict misfires; - added example SQL to create alt allele table from VET; - option to remove PLs; - fixed and enhanced unit test; - removing unused config, causing travis to fail; - add CreateVariantIngestFiles integration test (#7071); - add sampleName (instead of NULL) to error message (#7074); - Update To handle if",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:10622,Performance,Load,LoadData,10622,"e-map-file instead of sample-map-table (#6872); - Moving extract wdls from variantstore repo (#6902); - update for genomes (#6918); - update paths; - update field name; - consolidate exome and genome code; - missing comma; - allow null for drop state. add to wdl; - fix when NON-REF has value (drop it anyway). add memory to wdl; - fix order of columns in vet; - Making ingest work on gvcfs without allele specific annotations (#6934); - fix partitioning (#6939); - more fixes for non-allele-specific gvcfs (#6945); - fix GT for ./.; - add in feature extract code. resulting vcf still has bugs (#6947); - Rudimentary support for other output types, support for scattering extract cohort (#6949); - added batch/interactive flag, removed samples (#6953); - WIP for Feature Extract code (#6958); - move wdl and cromwell_tests dirs under variantstore (#6961); - WDL to run feature extract, VQSR, and upload (#6966); - support for multiple PET/VETs (#6969); - Add filtering to ExtractCohort (#6971); - support for non-AS called data (#6975); - fixing bug in filtering (#6976); - Updating extract wdls to apply filtering (#6977); - moving from specops repo (#6983); - optimized TSV experiment, and range GQ dropping (#6987); - back to 30x defaults; - allow no filtering to be applied (#7004); - Separate bigquery table creation and data loading in LoadData (#7056); - WIP; - tieout scripts; - notes files; - updated diff scripts; - fixed bug...; - add wdl and inputs file for warp pipeline; - reverting logging; - included top level WDL; - use gnarly with BQ extract cohort; - remove unused file; - cleaning up; - tidy; - tidy up before PR; - tidy up before PR; - PR comments; - merge conflict misfires; - added example SQL to create alt allele table from VET; - option to remove PLs; - fixed and enhanced unit test; - removing unused config, causing travis to fail; - add CreateVariantIngestFiles integration test (#7071); - add sampleName (instead of NULL) to error message (#7074); - Update To handle if",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:11392,Performance,load,loading,11392," table creation and data loading in LoadData (#7056); - WIP; - tieout scripts; - notes files; - updated diff scripts; - fixed bug...; - add wdl and inputs file for warp pipeline; - reverting logging; - included top level WDL; - use gnarly with BQ extract cohort; - remove unused file; - cleaning up; - tidy; - tidy up before PR; - tidy up before PR; - PR comments; - merge conflict misfires; - added example SQL to create alt allele table from VET; - option to remove PLs; - fixed and enhanced unit test; - removing unused config, causing travis to fail; - add CreateVariantIngestFiles integration test (#7071); - add sampleName (instead of NULL) to error message (#7074); - Update To handle if no data error (#7084); - Memory improvement when writing missing positions to pet (#7098); - added support for loading QUALapprox into VET (#7101); - Add -m flag to gsutil step; add dockstore branch filters to facilitate development (#7104); - updates to ImportGenomes and LoadBigQueryData (#7112); - Add ngs to cohort extract Dockerfile; remove exception catching in extract python script (#7113); - remove problematic storage_location imports (#7119); - Reduce memory and CPU for CreateImportTsvs task, check for files before attempting load (#7121); - add -m flag to gsutil mv step (#7129); - ah_var_store : Add sample file argument to cohort extract (#7117); - Perform full WGS cohort extract scientific tieout for 35 ACMG59 samples (#7106); - Enable Read/Execution Project for BQ Queries (#7136); - ah - optional service account (#7140); - Add load lock file to prevent accidental re-loading of data to BQ (#7138); - #251 Address gvcf no-calls missing QUALapprox and other features (#7146); - Job Add labels to BQ operations from GATK (Issues-199) (#7115); - parse map to list to avoid brackets and spaces in vcf output (#7168); - #259 Inline schema for importgenomes.wdl (#7171); - Created AvroFileReader and unittest, Update ExtractCohort and ExtractCohortEngine (#7174); - #224 Import WDL: handle ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:11554,Performance,Load,LoadBigQueryData,11554," table creation and data loading in LoadData (#7056); - WIP; - tieout scripts; - notes files; - updated diff scripts; - fixed bug...; - add wdl and inputs file for warp pipeline; - reverting logging; - included top level WDL; - use gnarly with BQ extract cohort; - remove unused file; - cleaning up; - tidy; - tidy up before PR; - tidy up before PR; - PR comments; - merge conflict misfires; - added example SQL to create alt allele table from VET; - option to remove PLs; - fixed and enhanced unit test; - removing unused config, causing travis to fail; - add CreateVariantIngestFiles integration test (#7071); - add sampleName (instead of NULL) to error message (#7074); - Update To handle if no data error (#7084); - Memory improvement when writing missing positions to pet (#7098); - added support for loading QUALapprox into VET (#7101); - Add -m flag to gsutil step; add dockstore branch filters to facilitate development (#7104); - updates to ImportGenomes and LoadBigQueryData (#7112); - Add ngs to cohort extract Dockerfile; remove exception catching in extract python script (#7113); - remove problematic storage_location imports (#7119); - Reduce memory and CPU for CreateImportTsvs task, check for files before attempting load (#7121); - add -m flag to gsutil mv step (#7129); - ah_var_store : Add sample file argument to cohort extract (#7117); - Perform full WGS cohort extract scientific tieout for 35 ACMG59 samples (#7106); - Enable Read/Execution Project for BQ Queries (#7136); - ah - optional service account (#7140); - Add load lock file to prevent accidental re-loading of data to BQ (#7138); - #251 Address gvcf no-calls missing QUALapprox and other features (#7146); - Job Add labels to BQ operations from GATK (Issues-199) (#7115); - parse map to list to avoid brackets and spaces in vcf output (#7168); - #259 Inline schema for importgenomes.wdl (#7171); - Created AvroFileReader and unittest, Update ExtractCohort and ExtractCohortEngine (#7174); - #224 Import WDL: handle ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:11820,Performance,load,load,11820," table creation and data loading in LoadData (#7056); - WIP; - tieout scripts; - notes files; - updated diff scripts; - fixed bug...; - add wdl and inputs file for warp pipeline; - reverting logging; - included top level WDL; - use gnarly with BQ extract cohort; - remove unused file; - cleaning up; - tidy; - tidy up before PR; - tidy up before PR; - PR comments; - merge conflict misfires; - added example SQL to create alt allele table from VET; - option to remove PLs; - fixed and enhanced unit test; - removing unused config, causing travis to fail; - add CreateVariantIngestFiles integration test (#7071); - add sampleName (instead of NULL) to error message (#7074); - Update To handle if no data error (#7084); - Memory improvement when writing missing positions to pet (#7098); - added support for loading QUALapprox into VET (#7101); - Add -m flag to gsutil step; add dockstore branch filters to facilitate development (#7104); - updates to ImportGenomes and LoadBigQueryData (#7112); - Add ngs to cohort extract Dockerfile; remove exception catching in extract python script (#7113); - remove problematic storage_location imports (#7119); - Reduce memory and CPU for CreateImportTsvs task, check for files before attempting load (#7121); - add -m flag to gsutil mv step (#7129); - ah_var_store : Add sample file argument to cohort extract (#7117); - Perform full WGS cohort extract scientific tieout for 35 ACMG59 samples (#7106); - Enable Read/Execution Project for BQ Queries (#7136); - ah - optional service account (#7140); - Add load lock file to prevent accidental re-loading of data to BQ (#7138); - #251 Address gvcf no-calls missing QUALapprox and other features (#7146); - Job Add labels to BQ operations from GATK (Issues-199) (#7115); - parse map to list to avoid brackets and spaces in vcf output (#7168); - #259 Inline schema for importgenomes.wdl (#7171); - Created AvroFileReader and unittest, Update ExtractCohort and ExtractCohortEngine (#7174); - #224 Import WDL: handle ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:11946,Performance,Perform,Perform,11946," table creation and data loading in LoadData (#7056); - WIP; - tieout scripts; - notes files; - updated diff scripts; - fixed bug...; - add wdl and inputs file for warp pipeline; - reverting logging; - included top level WDL; - use gnarly with BQ extract cohort; - remove unused file; - cleaning up; - tidy; - tidy up before PR; - tidy up before PR; - PR comments; - merge conflict misfires; - added example SQL to create alt allele table from VET; - option to remove PLs; - fixed and enhanced unit test; - removing unused config, causing travis to fail; - add CreateVariantIngestFiles integration test (#7071); - add sampleName (instead of NULL) to error message (#7074); - Update To handle if no data error (#7084); - Memory improvement when writing missing positions to pet (#7098); - added support for loading QUALapprox into VET (#7101); - Add -m flag to gsutil step; add dockstore branch filters to facilitate development (#7104); - updates to ImportGenomes and LoadBigQueryData (#7112); - Add ngs to cohort extract Dockerfile; remove exception catching in extract python script (#7113); - remove problematic storage_location imports (#7119); - Reduce memory and CPU for CreateImportTsvs task, check for files before attempting load (#7121); - add -m flag to gsutil mv step (#7129); - ah_var_store : Add sample file argument to cohort extract (#7117); - Perform full WGS cohort extract scientific tieout for 35 ACMG59 samples (#7106); - Enable Read/Execution Project for BQ Queries (#7136); - ah - optional service account (#7140); - Add load lock file to prevent accidental re-loading of data to BQ (#7138); - #251 Address gvcf no-calls missing QUALapprox and other features (#7146); - Job Add labels to BQ operations from GATK (Issues-199) (#7115); - parse map to list to avoid brackets and spaces in vcf output (#7168); - #259 Inline schema for importgenomes.wdl (#7171); - Created AvroFileReader and unittest, Update ExtractCohort and ExtractCohortEngine (#7174); - #224 Import WDL: handle ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:12130,Performance,load,load,12130," table creation and data loading in LoadData (#7056); - WIP; - tieout scripts; - notes files; - updated diff scripts; - fixed bug...; - add wdl and inputs file for warp pipeline; - reverting logging; - included top level WDL; - use gnarly with BQ extract cohort; - remove unused file; - cleaning up; - tidy; - tidy up before PR; - tidy up before PR; - PR comments; - merge conflict misfires; - added example SQL to create alt allele table from VET; - option to remove PLs; - fixed and enhanced unit test; - removing unused config, causing travis to fail; - add CreateVariantIngestFiles integration test (#7071); - add sampleName (instead of NULL) to error message (#7074); - Update To handle if no data error (#7084); - Memory improvement when writing missing positions to pet (#7098); - added support for loading QUALapprox into VET (#7101); - Add -m flag to gsutil step; add dockstore branch filters to facilitate development (#7104); - updates to ImportGenomes and LoadBigQueryData (#7112); - Add ngs to cohort extract Dockerfile; remove exception catching in extract python script (#7113); - remove problematic storage_location imports (#7119); - Reduce memory and CPU for CreateImportTsvs task, check for files before attempting load (#7121); - add -m flag to gsutil mv step (#7129); - ah_var_store : Add sample file argument to cohort extract (#7117); - Perform full WGS cohort extract scientific tieout for 35 ACMG59 samples (#7106); - Enable Read/Execution Project for BQ Queries (#7136); - ah - optional service account (#7140); - Add load lock file to prevent accidental re-loading of data to BQ (#7138); - #251 Address gvcf no-calls missing QUALapprox and other features (#7146); - Job Add labels to BQ operations from GATK (Issues-199) (#7115); - parse map to list to avoid brackets and spaces in vcf output (#7168); - #259 Inline schema for importgenomes.wdl (#7171); - Created AvroFileReader and unittest, Update ExtractCohort and ExtractCohortEngine (#7174); - #224 Import WDL: handle ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:12170,Performance,load,loading,12170," table creation and data loading in LoadData (#7056); - WIP; - tieout scripts; - notes files; - updated diff scripts; - fixed bug...; - add wdl and inputs file for warp pipeline; - reverting logging; - included top level WDL; - use gnarly with BQ extract cohort; - remove unused file; - cleaning up; - tidy; - tidy up before PR; - tidy up before PR; - PR comments; - merge conflict misfires; - added example SQL to create alt allele table from VET; - option to remove PLs; - fixed and enhanced unit test; - removing unused config, causing travis to fail; - add CreateVariantIngestFiles integration test (#7071); - add sampleName (instead of NULL) to error message (#7074); - Update To handle if no data error (#7084); - Memory improvement when writing missing positions to pet (#7098); - added support for loading QUALapprox into VET (#7101); - Add -m flag to gsutil step; add dockstore branch filters to facilitate development (#7104); - updates to ImportGenomes and LoadBigQueryData (#7112); - Add ngs to cohort extract Dockerfile; remove exception catching in extract python script (#7113); - remove problematic storage_location imports (#7119); - Reduce memory and CPU for CreateImportTsvs task, check for files before attempting load (#7121); - add -m flag to gsutil mv step (#7129); - ah_var_store : Add sample file argument to cohort extract (#7117); - Perform full WGS cohort extract scientific tieout for 35 ACMG59 samples (#7106); - Enable Read/Execution Project for BQ Queries (#7136); - ah - optional service account (#7140); - Add load lock file to prevent accidental re-loading of data to BQ (#7138); - #251 Address gvcf no-calls missing QUALapprox and other features (#7146); - Job Add labels to BQ operations from GATK (Issues-199) (#7115); - parse map to list to avoid brackets and spaces in vcf output (#7168); - #259 Inline schema for importgenomes.wdl (#7171); - Created AvroFileReader and unittest, Update ExtractCohort and ExtractCohortEngine (#7174); - #224 Import WDL: handle ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:12928,Performance,optimiz,optimizations,12928," QUALapprox and other features (#7146); - Job Add labels to BQ operations from GATK (Issues-199) (#7115); - parse map to list to avoid brackets and spaces in vcf output (#7168); - #259 Inline schema for importgenomes.wdl (#7171); - Created AvroFileReader and unittest, Update ExtractCohort and ExtractCohortEngine (#7174); - #224 Import WDL: handle 15 TB /table /day import limit (#7167); - #260 filter out AS_QD, SOR, FS from cohort extract VCF (#7173); - Full scientific validation via end to end comparison of filtered results between WARP and BQ (#7179); - Cherry pick of commits to fix GATK tests from master (#7183); - ExtractCohort supports -XL exclusion and follows intervals, other optimizations (#7181); - ExtractFeatures supports -XL exclusion and follows intervals, other optimizations (#7184); - change 0/0 GQ0 sites to nocalls (#7190); - updated (#7195); - Rename ""metadata"" table to ""sample_info"" table, fix vet schema (#7196); - Allow users to specify VQSLOD sensitivity and apply threshold in ExtractCohort (#7194); - Calculate and Store site-level QCs (#7197); - Filter Failing QC Sites from Extract (#7201); - WDLize GvsPrepareCallset (briefly known as CreateCohortTable) (#7200); - default drop_state to 60, but allow NONE as input (#7206); - SA support and consistent naming for all GVS WDLs (#7205); - fix GvsExtractCallset inputs file (#7210); - add clustering to tables (#7207); - add vqsr cutoffs to GvsExtractCallset wdl; clean up dockstore yml (#7209); - Avro test (#7192); - Enable call caching of TSV generation in GvsImportGenomes (#7226); - 266 Clean up ExtractCohort -- remove query mode param (#7227); - 288 Add an excess alleles param (#7221); - take sample name as a param (#7236); - How to run GIAB comparisons (#7237); - Update GvsCreateFilterSet.wdl (#7239); - Use GatherVcfsCloud in GvsCreateFilterSet.wdl (#7241); - parameterize TTL with defaults, reduce memory allocation (#7244); - Addressing OOM in CohortExtract (#7245); - make outputs optional, change case",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:13021,Performance,optimiz,optimizations,13021," QUALapprox and other features (#7146); - Job Add labels to BQ operations from GATK (Issues-199) (#7115); - parse map to list to avoid brackets and spaces in vcf output (#7168); - #259 Inline schema for importgenomes.wdl (#7171); - Created AvroFileReader and unittest, Update ExtractCohort and ExtractCohortEngine (#7174); - #224 Import WDL: handle 15 TB /table /day import limit (#7167); - #260 filter out AS_QD, SOR, FS from cohort extract VCF (#7173); - Full scientific validation via end to end comparison of filtered results between WARP and BQ (#7179); - Cherry pick of commits to fix GATK tests from master (#7183); - ExtractCohort supports -XL exclusion and follows intervals, other optimizations (#7181); - ExtractFeatures supports -XL exclusion and follows intervals, other optimizations (#7184); - change 0/0 GQ0 sites to nocalls (#7190); - updated (#7195); - Rename ""metadata"" table to ""sample_info"" table, fix vet schema (#7196); - Allow users to specify VQSLOD sensitivity and apply threshold in ExtractCohort (#7194); - Calculate and Store site-level QCs (#7197); - Filter Failing QC Sites from Extract (#7201); - WDLize GvsPrepareCallset (briefly known as CreateCohortTable) (#7200); - default drop_state to 60, but allow NONE as input (#7206); - SA support and consistent naming for all GVS WDLs (#7205); - fix GvsExtractCallset inputs file (#7210); - add clustering to tables (#7207); - add vqsr cutoffs to GvsExtractCallset wdl; clean up dockstore yml (#7209); - Avro test (#7192); - Enable call caching of TSV generation in GvsImportGenomes (#7226); - 266 Clean up ExtractCohort -- remove query mode param (#7227); - 288 Add an excess alleles param (#7221); - take sample name as a param (#7236); - How to run GIAB comparisons (#7237); - Update GvsCreateFilterSet.wdl (#7239); - Use GatherVcfsCloud in GvsCreateFilterSet.wdl (#7241); - parameterize TTL with defaults, reduce memory allocation (#7244); - Addressing OOM in CohortExtract (#7245); - make outputs optional, change case",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:15918,Performance,load,loads,15918,"epare, factored out sample name (#7288); - Remove training sites only param from ExtractFeatures broadinstitute/dsp-spec-ops#261; - add param for mem for indels (#7282); - Ah prepare localize option (#7299); - Export sites only vcf STEP 1-- 317 add AC, AN, AF to the final VCF (#7279); - AoU GVS Cohort Extract wdl (#7242); - reliability (#7310); - bump to include FT tag filtering (#7316); - First pass at a Terra QuickStart (#7267); - Ah fix timestamp query (#7319); - 313 Cleanup Extract Cohort params (#7293); - bump bq storage version. See GVS-332 (#7330); - Variant Store extraction - Add VCF size to output (#7329); - add WARP-style scattering to SNPsVariantRecalibrator in GvsCreateFilterSet (#7320); - added ref ranges support (#7337); - 318 Sites only filtered vcf then annotate wdl (#7305); - Replace service_account_json (file) with service_account_json_path (string) to allow call-caching (#7347); - Parallelize create filterset by breaking out the 3 filter set file creation/loads into separate tasks (#7342); - Create WDL to validate VAT and add first test (#7352); - Add task for VAT validation #3 (#7360); - Add task for VAT validation #4 (#7363); - Instructions on how to download BQ Metadata and visualize results (#7359); - don't mix contigs, rightsize memory (#7361); - Add custom annotations as ac an af (#7351); - Add task for VAT validation #8 & 9 (#7364); - added bcftools, upgraded gcloud version (#7369); - fix wdl (#7378); - Update .dockstore.yml; - Add VAT validation rule #5 [VS-16] (#7365); - Add VAT validation rule #7 [VS-14] and validation rule #6 [VS-15] (#7379); - Batching of samples for create import TSVs (#7382); - Add VAT validation rule #2 [VS-19] (#7374); - Create VAT scripts directory (#7386); - fixing SA change from file to string (#7371); - add extract_subpop script (#7387); - Add is_loaded column to sample_info and logic to populate after ingest [VS-158] (#7389); - Add Gnomad subpopulation info into the VAT (#7381); - implement GVS ID assignment (#",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:16949,Performance,load,loading,16949,"#7359); - don't mix contigs, rightsize memory (#7361); - Add custom annotations as ac an af (#7351); - Add task for VAT validation #8 & 9 (#7364); - added bcftools, upgraded gcloud version (#7369); - fix wdl (#7378); - Update .dockstore.yml; - Add VAT validation rule #5 [VS-16] (#7365); - Add VAT validation rule #7 [VS-14] and validation rule #6 [VS-15] (#7379); - Batching of samples for create import TSVs (#7382); - Add VAT validation rule #2 [VS-19] (#7374); - Create VAT scripts directory (#7386); - fixing SA change from file to string (#7371); - add extract_subpop script (#7387); - Add is_loaded column to sample_info and logic to populate after ingest [VS-158] (#7389); - Add Gnomad subpopulation info into the VAT (#7381); - implement GVS ID assignment (#7355); - no longer loading sample info table in this wdl (#7407); - divide up creation/population of temp pet table [VS-48] (#7395); - Sample QC metrics (#7396); - update import for is_loaded (#7416); - fix partition end, add 1 (#7420); - Fixes to CreateFilterSet and ExtractCallset from 30K run (#7423); - Also changed file size from Int to Float in SumBytes task python (#7429); - Adding the subpopulation calculations to the VAT creation WDL (#7399); - 154 De obfuscate (#7435); - filter on gvs_ids for workflow (#7428); - update for assign ids and changes in import (#7439); - need to loop through sets when moving to done (#7440); - add option for create filter set to use sample_info with is_loaded (#7434); - remove dead branch (#7443); - Scaling the VAT -- switch the input to take in a file of vcf shard file names (#7446); - dockstore testing: move validate vat inputs (#7449); - Update GVS sample QC to support multiple callsets per datasset [VS-177] (#7451); - Update GvsImportGenomes.wdl (#7462); - Add extraction uuid BQ label to GvsPrepareCallstep from GvsExtractCohortFromSampleNames (#7458); - Add manifest summary file to GvsExtractCallset (#7457); - Create workflow to create and populate alt_allele table [VS-51] (",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:18785,Performance,load,loaded,18785,"463); - formatting on sample QC README; - formatting change #2 to sample QC README; - address VS-152, remove extra headers from extract (#7466); - Update GvsExtractCallset.example.inputs.json (#7469); - Add ability to copy interval list files to gs directory [VS-191] (#7467); - add an expiration date to the temp tables (#7455); - fix the check for duplicates in import genomes (#7470); - added job ID to alt_allele population call output [VS-194] (#7473); - added steps and deliverables to GVS README [VS-181] (#7452); - Ah check the is loaded field in feature extract (#7475); - changes to put pet data directly into data table (#7478); - added override for ExtractTasks' preemptible value (#7477); - bcftools to the rescue (#7456); - execute_with_retry() refactor and error handling improvements [VS-159] (#7480); - Small updates to GvsExtractCallset from beta callset, new workflow for re-scattered shards (#7493); - add flag in prepare to print out sql instead of executing (#7501); - Workflow to re-scatter and then merge ""problematic"" intervals from ExtractCallset [VS-209] (#7495); - changed README to reflect comments from Lee [VS-210] (#7502); - Export the VAT into GCS (#7472); - addresses VS-219 (#7508); - small fix to MergeVCFs (#7517); - small fixes to GVS pipeline (#7522); - make sure ExtractTask is run on all interval files; - Revert ""make sure ExtractTask is run on all interval files""; - make sure ExtractTask is run on all interval files (#7527); - Remove Sites only step from the VAT creation WDL (#7510); - fix bad argument processing for bool (#7529); - Support for TDR DRS URIs in Import (#7528); - Match format of filename output in GvsRescatterCallsetInterval (#7539); - Reference block storage and query support (#7498); - update docs (#7540); - Kc fix rr load bug (#7550); - Update .dockstore.yml (#7553); - Ah add reblocking wdl (#7544); - Scatter over all interval files, not just scatter count (#7551); - fixed docker (#7558); - take advantage of fixed version of Spl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:20032,Performance,load,load,20032,"463); - formatting on sample QC README; - formatting change #2 to sample QC README; - address VS-152, remove extra headers from extract (#7466); - Update GvsExtractCallset.example.inputs.json (#7469); - Add ability to copy interval list files to gs directory [VS-191] (#7467); - add an expiration date to the temp tables (#7455); - fix the check for duplicates in import genomes (#7470); - added job ID to alt_allele population call output [VS-194] (#7473); - added steps and deliverables to GVS README [VS-181] (#7452); - Ah check the is loaded field in feature extract (#7475); - changes to put pet data directly into data table (#7478); - added override for ExtractTasks' preemptible value (#7477); - bcftools to the rescue (#7456); - execute_with_retry() refactor and error handling improvements [VS-159] (#7480); - Small updates to GvsExtractCallset from beta callset, new workflow for re-scattered shards (#7493); - add flag in prepare to print out sql instead of executing (#7501); - Workflow to re-scatter and then merge ""problematic"" intervals from ExtractCallset [VS-209] (#7495); - changed README to reflect comments from Lee [VS-210] (#7502); - Export the VAT into GCS (#7472); - addresses VS-219 (#7508); - small fix to MergeVCFs (#7517); - small fixes to GVS pipeline (#7522); - make sure ExtractTask is run on all interval files; - Revert ""make sure ExtractTask is run on all interval files""; - make sure ExtractTask is run on all interval files (#7527); - Remove Sites only step from the VAT creation WDL (#7510); - fix bad argument processing for bool (#7529); - Support for TDR DRS URIs in Import (#7528); - Match format of filename output in GvsRescatterCallsetInterval (#7539); - Reference block storage and query support (#7498); - update docs (#7540); - Kc fix rr load bug (#7550); - Update .dockstore.yml (#7553); - Ah add reblocking wdl (#7544); - Scatter over all interval files, not just scatter count (#7551); - fixed docker (#7558); - take advantage of fixed version of Spl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:20865,Performance,perform,performance,20865,"filename output in GvsRescatterCallsetInterval (#7539); - Reference block storage and query support (#7498); - update docs (#7540); - Kc fix rr load bug (#7550); - Update .dockstore.yml (#7553); - Ah add reblocking wdl (#7544); - Scatter over all interval files, not just scatter count (#7551); - fixed docker (#7558); - take advantage of fixed version of SplitIntervals (#7566); - Document AoU-specific tieout [VS-233] (#7552); - bad param assignment in aou reblocking (#7572); - Small fixes to ImportGenomes (non-write api version) (#7574); - Ah change output of reblocking wdl to external path (#7575); - close BQ Readers (#7583); - Ah spike writeapi (#7530); - bump WDL jar (#7593); - read api bytes logging, upgrade bigquery client versions (#7601); - bump (#7610); - upgrade log4j to 2.17 (#7616); - Add drop_state default of Forty to extract (#7619); - Kc fix type (#7620); - VAT cleanup and documentation (#7531); - fix empty flush (#7627); - presorted avro files, fix performance issue (#7635); - WIP extract for ranges (#7640); - VS-268 import more samples at once (#7629); - clustering vqsr tables by location (#7656); - First Version of a weight-based splitter (#7643); - Update GvsExtractCallset.wdl; - Quoting of table names (#7666); - docs for analysis of shard runtimes for balanced sharding (#7645); - Wire through GvsExtractCohortFromSampleNames with new prepare/extract [VS-283] (#7654); - Update GvsExtractCallset.wdl (#7678); - cherry pick lb_lfs_force change (#7683); - Tweak ingest messaging and failure mode [VS-267] (#7680); - Additional tweaks for GvsExtractCohortFromSampleNames [VS-283] (#7698); - VS-280 Create a VAT intermediary (#7657); - There something about split intervals [VS-306] (#7694); - VS 284 Add prepare step to Quick Start (#7685); - VS-222 dont hard code the dataset name! (#7704); - fixed bug; added tests (#7717); - Clean up optional and inconsistently named inputs [VS-294] [VS-218] (#7715); - VS-263 notes on ingest and beyond (#7618); - Add task to Ex",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:22408,Performance,Perform,Performance,22408,"ntimes for balanced sharding (#7645); - Wire through GvsExtractCohortFromSampleNames with new prepare/extract [VS-283] (#7654); - Update GvsExtractCallset.wdl (#7678); - cherry pick lb_lfs_force change (#7683); - Tweak ingest messaging and failure mode [VS-267] (#7680); - Additional tweaks for GvsExtractCohortFromSampleNames [VS-283] (#7698); - VS-280 Create a VAT intermediary (#7657); - There something about split intervals [VS-306] (#7694); - VS 284 Add prepare step to Quick Start (#7685); - VS-222 dont hard code the dataset name! (#7704); - fixed bug; added tests (#7717); - Clean up optional and inconsistently named inputs [VS-294] [VS-218] (#7715); - VS-263 notes on ingest and beyond (#7618); - Add task to ExtractCallset that verifies filter_set_name exists in GVS dataset [VS-335] (#7734); - Clean up input json files to reflect changes inputs [VS-337] (#7733); - used constants; implemented non-AS transformation (#7718); - Pass dataset name to gatk ExtractFeatures (#7735); - Add withdrawn and is_control columns [VS-70] [VS-213] (#7736); - Allow interval lists that require the SA to see (#7743); - allow for gatk to be overridden, update with known good jar (#7758); - VS-361 Add GvsWithdrawSamples wdl (#7765); - Extract Performance Improvements (#7686); - Don't put withdrawn sample data in alt_allele table [VS-369] (#7762); - remove PET code (#7768); - Adding AD for scale testing VS 225 add AD (#7713); - Deterministic Sample ID assignments [VS-371] (#7770); - remove R scripts from filtering (#7781); - Remove an old ""temp table"" dataset (#7780); - Clean up LocalizeFile [VS-314] (#7771); - Remove pet code from CreateVariantIngestFiles and friends [VS-375] (#7773); - 317 remove excess header values in VCF extract (#7786); - correct auth in split intervals (#7790); - Add code to (optionally) zero pad the vcf filename. (#7783); - LoadData `maxRetries` parameterized, default increased [VS-383] (#7791); - Update to latest version of ah_var_store gatk override jar (#7793); ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:23025,Performance,Load,LoadData,23025,"res (#7735); - Add withdrawn and is_control columns [VS-70] [VS-213] (#7736); - Allow interval lists that require the SA to see (#7743); - allow for gatk to be overridden, update with known good jar (#7758); - VS-361 Add GvsWithdrawSamples wdl (#7765); - Extract Performance Improvements (#7686); - Don't put withdrawn sample data in alt_allele table [VS-369] (#7762); - remove PET code (#7768); - Adding AD for scale testing VS 225 add AD (#7713); - Deterministic Sample ID assignments [VS-371] (#7770); - remove R scripts from filtering (#7781); - Remove an old ""temp table"" dataset (#7780); - Clean up LocalizeFile [VS-314] (#7771); - Remove pet code from CreateVariantIngestFiles and friends [VS-375] (#7773); - 317 remove excess header values in VCF extract (#7786); - correct auth in split intervals (#7790); - Add code to (optionally) zero pad the vcf filename. (#7783); - LoadData `maxRetries` parameterized, default increased [VS-383] (#7791); - Update to latest version of ah_var_store gatk override jar (#7793); - GvsUnified WDL to wrap the 6 core GVS WDLs [VS-382] (#7789); - Pinned typing_extensions python package to 4.1.1 to fix conda environment. (#7802); - WeightedSplitInterval fixes [VS-384] [VS-332] (#7795); - Replace Travis with GithubActions (#7754); - Docker build only lfs pulls main/src/resources/large (#7727); - Clean up gatk jars -- looks like we are not passing them properly in the extract (#7788); - Fix typo that broke git lfs pull (#7806); - Document AoU SOP (up to the VAT) [VS-63] (#7807); - Incident VS 365 clinvar classification fix (#7769); - VS-390. Add precision and sensitivity wdl (#7813); - Quickstart based integration test [VS-357] (#7812); - 365 vat python testing additions (#7756); - VS 396 clinvar grabs too many values (#7823); - Added a test to validate WDLs in the scripts directory. (#7826) (#7829); - VAT Performance / Reliability Improvements (#7828); - VAT naming conventions [VS-410] (#7827); - Rc remove ad from vat (#7832); - bugfix, we were",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:24005,Performance,Perform,Performance,24005,"89); - Pinned typing_extensions python package to 4.1.1 to fix conda environment. (#7802); - WeightedSplitInterval fixes [VS-384] [VS-332] (#7795); - Replace Travis with GithubActions (#7754); - Docker build only lfs pulls main/src/resources/large (#7727); - Clean up gatk jars -- looks like we are not passing them properly in the extract (#7788); - Fix typo that broke git lfs pull (#7806); - Document AoU SOP (up to the VAT) [VS-63] (#7807); - Incident VS 365 clinvar classification fix (#7769); - VS-390. Add precision and sensitivity wdl (#7813); - Quickstart based integration test [VS-357] (#7812); - 365 vat python testing additions (#7756); - VS 396 clinvar grabs too many values (#7823); - Added a test to validate WDLs in the scripts directory. (#7826) (#7829); - VAT Performance / Reliability Improvements (#7828); - VAT naming conventions [VS-410] (#7827); - Rc remove ad from vat (#7832); - bugfix, we were trying to grep a binary file (#7837); - Cleanup scripts/variantstore [VS-414] (#7834); - Merge VAT TSV files into single bgzipped file [VS-304] (#7848); - Handle fully and partially loaded samples [VS-262] [VS-258] (#7843); - Ingest Error Handling Fixes [VS-261] (#7841); - First cut at a python notebook to validate inputs. (#7845); - Compute filter scatter [VS-392] (#7852); - remove withdrawn req (#7844); - Improve import error message [VS-437] (#7855); - Fix Input Validation python notebook (#7853); - Add VAT Validation check that aa_change and exon_number are consistently set. (#7850); - Ingest 10K [VS-344] (#7860); - X/Y chromosome reweighting for better extract shard runtime balance [VS-389] (#7868); - VET Ingest Validation / Allow Ingest of non-VQSR'ed data (#7870); - Fix AoU workflow bugs (#7874); - Curate input arrays to skip already ingested sample data [VS-246] (#7862); - KM upload GVS product sheet (#7883); - Default extract scatter width [VS-415] (#7878); - Volatile tasks review [VS-447] (#7880); - Update Quickstart Integration for X/Y scaling changes ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:24329,Performance,load,loaded,24329,"89); - Pinned typing_extensions python package to 4.1.1 to fix conda environment. (#7802); - WeightedSplitInterval fixes [VS-384] [VS-332] (#7795); - Replace Travis with GithubActions (#7754); - Docker build only lfs pulls main/src/resources/large (#7727); - Clean up gatk jars -- looks like we are not passing them properly in the extract (#7788); - Fix typo that broke git lfs pull (#7806); - Document AoU SOP (up to the VAT) [VS-63] (#7807); - Incident VS 365 clinvar classification fix (#7769); - VS-390. Add precision and sensitivity wdl (#7813); - Quickstart based integration test [VS-357] (#7812); - 365 vat python testing additions (#7756); - VS 396 clinvar grabs too many values (#7823); - Added a test to validate WDLs in the scripts directory. (#7826) (#7829); - VAT Performance / Reliability Improvements (#7828); - VAT naming conventions [VS-410] (#7827); - Rc remove ad from vat (#7832); - bugfix, we were trying to grep a binary file (#7837); - Cleanup scripts/variantstore [VS-414] (#7834); - Merge VAT TSV files into single bgzipped file [VS-304] (#7848); - Handle fully and partially loaded samples [VS-262] [VS-258] (#7843); - Ingest Error Handling Fixes [VS-261] (#7841); - First cut at a python notebook to validate inputs. (#7845); - Compute filter scatter [VS-392] (#7852); - remove withdrawn req (#7844); - Improve import error message [VS-437] (#7855); - Fix Input Validation python notebook (#7853); - Add VAT Validation check that aa_change and exon_number are consistently set. (#7850); - Ingest 10K [VS-344] (#7860); - X/Y chromosome reweighting for better extract shard runtime balance [VS-389] (#7868); - VET Ingest Validation / Allow Ingest of non-VQSR'ed data (#7870); - Fix AoU workflow bugs (#7874); - Curate input arrays to skip already ingested sample data [VS-246] (#7862); - KM upload GVS product sheet (#7883); - Default extract scatter width [VS-415] (#7878); - Volatile tasks review [VS-447] (#7880); - Update Quickstart Integration for X/Y scaling changes ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:25324,Performance,race condition,race condition,25324,"to validate inputs. (#7845); - Compute filter scatter [VS-392] (#7852); - remove withdrawn req (#7844); - Improve import error message [VS-437] (#7855); - Fix Input Validation python notebook (#7853); - Add VAT Validation check that aa_change and exon_number are consistently set. (#7850); - Ingest 10K [VS-344] (#7860); - X/Y chromosome reweighting for better extract shard runtime balance [VS-389] (#7868); - VET Ingest Validation / Allow Ingest of non-VQSR'ed data (#7870); - Fix AoU workflow bugs (#7874); - Curate input arrays to skip already ingested sample data [VS-246] (#7862); - KM upload GVS product sheet (#7883); - Default extract scatter width [VS-415] (#7878); - Volatile tasks review [VS-447] (#7880); - Update Quickstart Integration for X/Y scaling changes [VS-464] (#7881); - clean up dockstore; - Rc vs 63 vat sop documentation (#7879); - Fix up FQ and race condition issues with volatile tasks work [VS-478] (#7888); - Use gvs-internal project in integration test (#7901); - Add cost observability BQ table [VS-441] (#7891); - Add preliminary labels to queries [VS-381] (#7902); - Workflow compute costs [VS-472] (#7905); - Fix bug and update images (#7912); - VS 483 Beta user wdl (#7894); - Core storage model cost [VS-473] (#7913); - Update Quickstart & Integration to use re-blocked v2 gVCFs [VS-491] (#7924); - KM GVS documentation (#7903); - Track BigQuery costs of GVS python VS-480 (#7915); - Read cost observability table [VS-475] (#7923); - Fix Race Condition, Add Support for Extract by Array of Sample Names (ie from a Sample Set) (#7917); - Rightsize import batches [VS-486] (#7925); - [AoU DRC] Support uppercase site_ids for reblocking (#7929); - Populate cost metadata for GATK tasks. (#7919); - remove accidentally added input (#7931); - VS_492 - Beta User Jar release (#7934); - Cost WDL should throw on FISS API errors [VS-518] (#7942); - Fix bad check for missing workflow name [VS-520] (#7943); - Remove usage of service account from GvsValidateVAT.wdl (#7937",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:25927,Performance,Race Condition,Race Condition,25927,"to validate inputs. (#7845); - Compute filter scatter [VS-392] (#7852); - remove withdrawn req (#7844); - Improve import error message [VS-437] (#7855); - Fix Input Validation python notebook (#7853); - Add VAT Validation check that aa_change and exon_number are consistently set. (#7850); - Ingest 10K [VS-344] (#7860); - X/Y chromosome reweighting for better extract shard runtime balance [VS-389] (#7868); - VET Ingest Validation / Allow Ingest of non-VQSR'ed data (#7870); - Fix AoU workflow bugs (#7874); - Curate input arrays to skip already ingested sample data [VS-246] (#7862); - KM upload GVS product sheet (#7883); - Default extract scatter width [VS-415] (#7878); - Volatile tasks review [VS-447] (#7880); - Update Quickstart Integration for X/Y scaling changes [VS-464] (#7881); - clean up dockstore; - Rc vs 63 vat sop documentation (#7879); - Fix up FQ and race condition issues with volatile tasks work [VS-478] (#7888); - Use gvs-internal project in integration test (#7901); - Add cost observability BQ table [VS-441] (#7891); - Add preliminary labels to queries [VS-381] (#7902); - Workflow compute costs [VS-472] (#7905); - Fix bug and update images (#7912); - VS 483 Beta user wdl (#7894); - Core storage model cost [VS-473] (#7913); - Update Quickstart & Integration to use re-blocked v2 gVCFs [VS-491] (#7924); - KM GVS documentation (#7903); - Track BigQuery costs of GVS python VS-480 (#7915); - Read cost observability table [VS-475] (#7923); - Fix Race Condition, Add Support for Extract by Array of Sample Names (ie from a Sample Set) (#7917); - Rightsize import batches [VS-486] (#7925); - [AoU DRC] Support uppercase site_ids for reblocking (#7929); - Populate cost metadata for GATK tasks. (#7919); - remove accidentally added input (#7931); - VS_492 - Beta User Jar release (#7934); - Cost WDL should throw on FISS API errors [VS-518] (#7942); - Fix bad check for missing workflow name [VS-520] (#7943); - Remove usage of service account from GvsValidateVAT.wdl (#7937",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:12366,Safety,avoid,avoid,12366," table creation and data loading in LoadData (#7056); - WIP; - tieout scripts; - notes files; - updated diff scripts; - fixed bug...; - add wdl and inputs file for warp pipeline; - reverting logging; - included top level WDL; - use gnarly with BQ extract cohort; - remove unused file; - cleaning up; - tidy; - tidy up before PR; - tidy up before PR; - PR comments; - merge conflict misfires; - added example SQL to create alt allele table from VET; - option to remove PLs; - fixed and enhanced unit test; - removing unused config, causing travis to fail; - add CreateVariantIngestFiles integration test (#7071); - add sampleName (instead of NULL) to error message (#7074); - Update To handle if no data error (#7084); - Memory improvement when writing missing positions to pet (#7098); - added support for loading QUALapprox into VET (#7101); - Add -m flag to gsutil step; add dockstore branch filters to facilitate development (#7104); - updates to ImportGenomes and LoadBigQueryData (#7112); - Add ngs to cohort extract Dockerfile; remove exception catching in extract python script (#7113); - remove problematic storage_location imports (#7119); - Reduce memory and CPU for CreateImportTsvs task, check for files before attempting load (#7121); - add -m flag to gsutil mv step (#7129); - ah_var_store : Add sample file argument to cohort extract (#7117); - Perform full WGS cohort extract scientific tieout for 35 ACMG59 samples (#7106); - Enable Read/Execution Project for BQ Queries (#7136); - ah - optional service account (#7140); - Add load lock file to prevent accidental re-loading of data to BQ (#7138); - #251 Address gvcf no-calls missing QUALapprox and other features (#7146); - Job Add labels to BQ operations from GATK (Issues-199) (#7115); - parse map to list to avoid brackets and spaces in vcf output (#7168); - #259 Inline schema for importgenomes.wdl (#7171); - Created AvroFileReader and unittest, Update ExtractCohort and ExtractCohortEngine (#7174); - #224 Import WDL: handle ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:12710,Security,validat,validation,12710," QUALapprox and other features (#7146); - Job Add labels to BQ operations from GATK (Issues-199) (#7115); - parse map to list to avoid brackets and spaces in vcf output (#7168); - #259 Inline schema for importgenomes.wdl (#7171); - Created AvroFileReader and unittest, Update ExtractCohort and ExtractCohortEngine (#7174); - #224 Import WDL: handle 15 TB /table /day import limit (#7167); - #260 filter out AS_QD, SOR, FS from cohort extract VCF (#7173); - Full scientific validation via end to end comparison of filtered results between WARP and BQ (#7179); - Cherry pick of commits to fix GATK tests from master (#7183); - ExtractCohort supports -XL exclusion and follows intervals, other optimizations (#7181); - ExtractFeatures supports -XL exclusion and follows intervals, other optimizations (#7184); - change 0/0 GQ0 sites to nocalls (#7190); - updated (#7195); - Rename ""metadata"" table to ""sample_info"" table, fix vet schema (#7196); - Allow users to specify VQSLOD sensitivity and apply threshold in ExtractCohort (#7194); - Calculate and Store site-level QCs (#7197); - Filter Failing QC Sites from Extract (#7201); - WDLize GvsPrepareCallset (briefly known as CreateCohortTable) (#7200); - default drop_state to 60, but allow NONE as input (#7206); - SA support and consistent naming for all GVS WDLs (#7205); - fix GvsExtractCallset inputs file (#7210); - add clustering to tables (#7207); - add vqsr cutoffs to GvsExtractCallset wdl; clean up dockstore yml (#7209); - Avro test (#7192); - Enable call caching of TSV generation in GvsImportGenomes (#7226); - 266 Clean up ExtractCohort -- remove query mode param (#7227); - 288 Add an excess alleles param (#7221); - take sample name as a param (#7236); - How to run GIAB comparisons (#7237); - Update GvsCreateFilterSet.wdl (#7239); - Use GatherVcfsCloud in GvsCreateFilterSet.wdl (#7241); - parameterize TTL with defaults, reduce memory allocation (#7244); - Addressing OOM in CohortExtract (#7245); - make outputs optional, change case",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:15969,Security,validat,validate,15969,"epare, factored out sample name (#7288); - Remove training sites only param from ExtractFeatures broadinstitute/dsp-spec-ops#261; - add param for mem for indels (#7282); - Ah prepare localize option (#7299); - Export sites only vcf STEP 1-- 317 add AC, AN, AF to the final VCF (#7279); - AoU GVS Cohort Extract wdl (#7242); - reliability (#7310); - bump to include FT tag filtering (#7316); - First pass at a Terra QuickStart (#7267); - Ah fix timestamp query (#7319); - 313 Cleanup Extract Cohort params (#7293); - bump bq storage version. See GVS-332 (#7330); - Variant Store extraction - Add VCF size to output (#7329); - add WARP-style scattering to SNPsVariantRecalibrator in GvsCreateFilterSet (#7320); - added ref ranges support (#7337); - 318 Sites only filtered vcf then annotate wdl (#7305); - Replace service_account_json (file) with service_account_json_path (string) to allow call-caching (#7347); - Parallelize create filterset by breaking out the 3 filter set file creation/loads into separate tasks (#7342); - Create WDL to validate VAT and add first test (#7352); - Add task for VAT validation #3 (#7360); - Add task for VAT validation #4 (#7363); - Instructions on how to download BQ Metadata and visualize results (#7359); - don't mix contigs, rightsize memory (#7361); - Add custom annotations as ac an af (#7351); - Add task for VAT validation #8 & 9 (#7364); - added bcftools, upgraded gcloud version (#7369); - fix wdl (#7378); - Update .dockstore.yml; - Add VAT validation rule #5 [VS-16] (#7365); - Add VAT validation rule #7 [VS-14] and validation rule #6 [VS-15] (#7379); - Batching of samples for create import TSVs (#7382); - Add VAT validation rule #2 [VS-19] (#7374); - Create VAT scripts directory (#7386); - fixing SA change from file to string (#7371); - add extract_subpop script (#7387); - Add is_loaded column to sample_info and logic to populate after ingest [VS-158] (#7389); - Add Gnomad subpopulation info into the VAT (#7381); - implement GVS ID assignment (#",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:16029,Security,validat,validation,16029,"epare, factored out sample name (#7288); - Remove training sites only param from ExtractFeatures broadinstitute/dsp-spec-ops#261; - add param for mem for indels (#7282); - Ah prepare localize option (#7299); - Export sites only vcf STEP 1-- 317 add AC, AN, AF to the final VCF (#7279); - AoU GVS Cohort Extract wdl (#7242); - reliability (#7310); - bump to include FT tag filtering (#7316); - First pass at a Terra QuickStart (#7267); - Ah fix timestamp query (#7319); - 313 Cleanup Extract Cohort params (#7293); - bump bq storage version. See GVS-332 (#7330); - Variant Store extraction - Add VCF size to output (#7329); - add WARP-style scattering to SNPsVariantRecalibrator in GvsCreateFilterSet (#7320); - added ref ranges support (#7337); - 318 Sites only filtered vcf then annotate wdl (#7305); - Replace service_account_json (file) with service_account_json_path (string) to allow call-caching (#7347); - Parallelize create filterset by breaking out the 3 filter set file creation/loads into separate tasks (#7342); - Create WDL to validate VAT and add first test (#7352); - Add task for VAT validation #3 (#7360); - Add task for VAT validation #4 (#7363); - Instructions on how to download BQ Metadata and visualize results (#7359); - don't mix contigs, rightsize memory (#7361); - Add custom annotations as ac an af (#7351); - Add task for VAT validation #8 & 9 (#7364); - added bcftools, upgraded gcloud version (#7369); - fix wdl (#7378); - Update .dockstore.yml; - Add VAT validation rule #5 [VS-16] (#7365); - Add VAT validation rule #7 [VS-14] and validation rule #6 [VS-15] (#7379); - Batching of samples for create import TSVs (#7382); - Add VAT validation rule #2 [VS-19] (#7374); - Create VAT scripts directory (#7386); - fixing SA change from file to string (#7371); - add extract_subpop script (#7387); - Add is_loaded column to sample_info and logic to populate after ingest [VS-158] (#7389); - Add Gnomad subpopulation info into the VAT (#7381); - implement GVS ID assignment (#",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:16071,Security,validat,validation,16071,"epare, factored out sample name (#7288); - Remove training sites only param from ExtractFeatures broadinstitute/dsp-spec-ops#261; - add param for mem for indels (#7282); - Ah prepare localize option (#7299); - Export sites only vcf STEP 1-- 317 add AC, AN, AF to the final VCF (#7279); - AoU GVS Cohort Extract wdl (#7242); - reliability (#7310); - bump to include FT tag filtering (#7316); - First pass at a Terra QuickStart (#7267); - Ah fix timestamp query (#7319); - 313 Cleanup Extract Cohort params (#7293); - bump bq storage version. See GVS-332 (#7330); - Variant Store extraction - Add VCF size to output (#7329); - add WARP-style scattering to SNPsVariantRecalibrator in GvsCreateFilterSet (#7320); - added ref ranges support (#7337); - 318 Sites only filtered vcf then annotate wdl (#7305); - Replace service_account_json (file) with service_account_json_path (string) to allow call-caching (#7347); - Parallelize create filterset by breaking out the 3 filter set file creation/loads into separate tasks (#7342); - Create WDL to validate VAT and add first test (#7352); - Add task for VAT validation #3 (#7360); - Add task for VAT validation #4 (#7363); - Instructions on how to download BQ Metadata and visualize results (#7359); - don't mix contigs, rightsize memory (#7361); - Add custom annotations as ac an af (#7351); - Add task for VAT validation #8 & 9 (#7364); - added bcftools, upgraded gcloud version (#7369); - fix wdl (#7378); - Update .dockstore.yml; - Add VAT validation rule #5 [VS-16] (#7365); - Add VAT validation rule #7 [VS-14] and validation rule #6 [VS-15] (#7379); - Batching of samples for create import TSVs (#7382); - Add VAT validation rule #2 [VS-19] (#7374); - Create VAT scripts directory (#7386); - fixing SA change from file to string (#7371); - add extract_subpop script (#7387); - Add is_loaded column to sample_info and logic to populate after ingest [VS-158] (#7389); - Add Gnomad subpopulation info into the VAT (#7381); - implement GVS ID assignment (#",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:16283,Security,validat,validation,16283,"epare, factored out sample name (#7288); - Remove training sites only param from ExtractFeatures broadinstitute/dsp-spec-ops#261; - add param for mem for indels (#7282); - Ah prepare localize option (#7299); - Export sites only vcf STEP 1-- 317 add AC, AN, AF to the final VCF (#7279); - AoU GVS Cohort Extract wdl (#7242); - reliability (#7310); - bump to include FT tag filtering (#7316); - First pass at a Terra QuickStart (#7267); - Ah fix timestamp query (#7319); - 313 Cleanup Extract Cohort params (#7293); - bump bq storage version. See GVS-332 (#7330); - Variant Store extraction - Add VCF size to output (#7329); - add WARP-style scattering to SNPsVariantRecalibrator in GvsCreateFilterSet (#7320); - added ref ranges support (#7337); - 318 Sites only filtered vcf then annotate wdl (#7305); - Replace service_account_json (file) with service_account_json_path (string) to allow call-caching (#7347); - Parallelize create filterset by breaking out the 3 filter set file creation/loads into separate tasks (#7342); - Create WDL to validate VAT and add first test (#7352); - Add task for VAT validation #3 (#7360); - Add task for VAT validation #4 (#7363); - Instructions on how to download BQ Metadata and visualize results (#7359); - don't mix contigs, rightsize memory (#7361); - Add custom annotations as ac an af (#7351); - Add task for VAT validation #8 & 9 (#7364); - added bcftools, upgraded gcloud version (#7369); - fix wdl (#7378); - Update .dockstore.yml; - Add VAT validation rule #5 [VS-16] (#7365); - Add VAT validation rule #7 [VS-14] and validation rule #6 [VS-15] (#7379); - Batching of samples for create import TSVs (#7382); - Add VAT validation rule #2 [VS-19] (#7374); - Create VAT scripts directory (#7386); - fixing SA change from file to string (#7371); - add extract_subpop script (#7387); - Add is_loaded column to sample_info and logic to populate after ingest [VS-158] (#7389); - Add Gnomad subpopulation info into the VAT (#7381); - implement GVS ID assignment (#",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:16415,Security,validat,validation,16415,"#7359); - don't mix contigs, rightsize memory (#7361); - Add custom annotations as ac an af (#7351); - Add task for VAT validation #8 & 9 (#7364); - added bcftools, upgraded gcloud version (#7369); - fix wdl (#7378); - Update .dockstore.yml; - Add VAT validation rule #5 [VS-16] (#7365); - Add VAT validation rule #7 [VS-14] and validation rule #6 [VS-15] (#7379); - Batching of samples for create import TSVs (#7382); - Add VAT validation rule #2 [VS-19] (#7374); - Create VAT scripts directory (#7386); - fixing SA change from file to string (#7371); - add extract_subpop script (#7387); - Add is_loaded column to sample_info and logic to populate after ingest [VS-158] (#7389); - Add Gnomad subpopulation info into the VAT (#7381); - implement GVS ID assignment (#7355); - no longer loading sample info table in this wdl (#7407); - divide up creation/population of temp pet table [VS-48] (#7395); - Sample QC metrics (#7396); - update import for is_loaded (#7416); - fix partition end, add 1 (#7420); - Fixes to CreateFilterSet and ExtractCallset from 30K run (#7423); - Also changed file size from Int to Float in SumBytes task python (#7429); - Adding the subpopulation calculations to the VAT creation WDL (#7399); - 154 De obfuscate (#7435); - filter on gvs_ids for workflow (#7428); - update for assign ids and changes in import (#7439); - need to loop through sets when moving to done (#7440); - add option for create filter set to use sample_info with is_loaded (#7434); - remove dead branch (#7443); - Scaling the VAT -- switch the input to take in a file of vcf shard file names (#7446); - dockstore testing: move validate vat inputs (#7449); - Update GVS sample QC to support multiple callsets per datasset [VS-177] (#7451); - Update GvsImportGenomes.wdl (#7462); - Add extraction uuid BQ label to GvsPrepareCallstep from GvsExtractCohortFromSampleNames (#7458); - Add manifest summary file to GvsExtractCallset (#7457); - Create workflow to create and populate alt_allele table [VS-51] (",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:16461,Security,validat,validation,16461,"#7359); - don't mix contigs, rightsize memory (#7361); - Add custom annotations as ac an af (#7351); - Add task for VAT validation #8 & 9 (#7364); - added bcftools, upgraded gcloud version (#7369); - fix wdl (#7378); - Update .dockstore.yml; - Add VAT validation rule #5 [VS-16] (#7365); - Add VAT validation rule #7 [VS-14] and validation rule #6 [VS-15] (#7379); - Batching of samples for create import TSVs (#7382); - Add VAT validation rule #2 [VS-19] (#7374); - Create VAT scripts directory (#7386); - fixing SA change from file to string (#7371); - add extract_subpop script (#7387); - Add is_loaded column to sample_info and logic to populate after ingest [VS-158] (#7389); - Add Gnomad subpopulation info into the VAT (#7381); - implement GVS ID assignment (#7355); - no longer loading sample info table in this wdl (#7407); - divide up creation/population of temp pet table [VS-48] (#7395); - Sample QC metrics (#7396); - update import for is_loaded (#7416); - fix partition end, add 1 (#7420); - Fixes to CreateFilterSet and ExtractCallset from 30K run (#7423); - Also changed file size from Int to Float in SumBytes task python (#7429); - Adding the subpopulation calculations to the VAT creation WDL (#7399); - 154 De obfuscate (#7435); - filter on gvs_ids for workflow (#7428); - update for assign ids and changes in import (#7439); - need to loop through sets when moving to done (#7440); - add option for create filter set to use sample_info with is_loaded (#7434); - remove dead branch (#7443); - Scaling the VAT -- switch the input to take in a file of vcf shard file names (#7446); - dockstore testing: move validate vat inputs (#7449); - Update GVS sample QC to support multiple callsets per datasset [VS-177] (#7451); - Update GvsImportGenomes.wdl (#7462); - Add extraction uuid BQ label to GvsPrepareCallstep from GvsExtractCohortFromSampleNames (#7458); - Add manifest summary file to GvsExtractCallset (#7457); - Create workflow to create and populate alt_allele table [VS-51] (",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:16492,Security,validat,validation,16492,"#7359); - don't mix contigs, rightsize memory (#7361); - Add custom annotations as ac an af (#7351); - Add task for VAT validation #8 & 9 (#7364); - added bcftools, upgraded gcloud version (#7369); - fix wdl (#7378); - Update .dockstore.yml; - Add VAT validation rule #5 [VS-16] (#7365); - Add VAT validation rule #7 [VS-14] and validation rule #6 [VS-15] (#7379); - Batching of samples for create import TSVs (#7382); - Add VAT validation rule #2 [VS-19] (#7374); - Create VAT scripts directory (#7386); - fixing SA change from file to string (#7371); - add extract_subpop script (#7387); - Add is_loaded column to sample_info and logic to populate after ingest [VS-158] (#7389); - Add Gnomad subpopulation info into the VAT (#7381); - implement GVS ID assignment (#7355); - no longer loading sample info table in this wdl (#7407); - divide up creation/population of temp pet table [VS-48] (#7395); - Sample QC metrics (#7396); - update import for is_loaded (#7416); - fix partition end, add 1 (#7420); - Fixes to CreateFilterSet and ExtractCallset from 30K run (#7423); - Also changed file size from Int to Float in SumBytes task python (#7429); - Adding the subpopulation calculations to the VAT creation WDL (#7399); - 154 De obfuscate (#7435); - filter on gvs_ids for workflow (#7428); - update for assign ids and changes in import (#7439); - need to loop through sets when moving to done (#7440); - add option for create filter set to use sample_info with is_loaded (#7434); - remove dead branch (#7443); - Scaling the VAT -- switch the input to take in a file of vcf shard file names (#7446); - dockstore testing: move validate vat inputs (#7449); - Update GVS sample QC to support multiple callsets per datasset [VS-177] (#7451); - Update GvsImportGenomes.wdl (#7462); - Add extraction uuid BQ label to GvsPrepareCallstep from GvsExtractCohortFromSampleNames (#7458); - Add manifest summary file to GvsExtractCallset (#7457); - Create workflow to create and populate alt_allele table [VS-51] (",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:16592,Security,validat,validation,16592,"#7359); - don't mix contigs, rightsize memory (#7361); - Add custom annotations as ac an af (#7351); - Add task for VAT validation #8 & 9 (#7364); - added bcftools, upgraded gcloud version (#7369); - fix wdl (#7378); - Update .dockstore.yml; - Add VAT validation rule #5 [VS-16] (#7365); - Add VAT validation rule #7 [VS-14] and validation rule #6 [VS-15] (#7379); - Batching of samples for create import TSVs (#7382); - Add VAT validation rule #2 [VS-19] (#7374); - Create VAT scripts directory (#7386); - fixing SA change from file to string (#7371); - add extract_subpop script (#7387); - Add is_loaded column to sample_info and logic to populate after ingest [VS-158] (#7389); - Add Gnomad subpopulation info into the VAT (#7381); - implement GVS ID assignment (#7355); - no longer loading sample info table in this wdl (#7407); - divide up creation/population of temp pet table [VS-48] (#7395); - Sample QC metrics (#7396); - update import for is_loaded (#7416); - fix partition end, add 1 (#7420); - Fixes to CreateFilterSet and ExtractCallset from 30K run (#7423); - Also changed file size from Int to Float in SumBytes task python (#7429); - Adding the subpopulation calculations to the VAT creation WDL (#7399); - 154 De obfuscate (#7435); - filter on gvs_ids for workflow (#7428); - update for assign ids and changes in import (#7439); - need to loop through sets when moving to done (#7440); - add option for create filter set to use sample_info with is_loaded (#7434); - remove dead branch (#7443); - Scaling the VAT -- switch the input to take in a file of vcf shard file names (#7446); - dockstore testing: move validate vat inputs (#7449); - Update GVS sample QC to support multiple callsets per datasset [VS-177] (#7451); - Update GvsImportGenomes.wdl (#7462); - Add extraction uuid BQ label to GvsPrepareCallstep from GvsExtractCohortFromSampleNames (#7458); - Add manifest summary file to GvsExtractCallset (#7457); - Create workflow to create and populate alt_allele table [VS-51] (",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:17789,Security,validat,validate,17789,"#7359); - don't mix contigs, rightsize memory (#7361); - Add custom annotations as ac an af (#7351); - Add task for VAT validation #8 & 9 (#7364); - added bcftools, upgraded gcloud version (#7369); - fix wdl (#7378); - Update .dockstore.yml; - Add VAT validation rule #5 [VS-16] (#7365); - Add VAT validation rule #7 [VS-14] and validation rule #6 [VS-15] (#7379); - Batching of samples for create import TSVs (#7382); - Add VAT validation rule #2 [VS-19] (#7374); - Create VAT scripts directory (#7386); - fixing SA change from file to string (#7371); - add extract_subpop script (#7387); - Add is_loaded column to sample_info and logic to populate after ingest [VS-158] (#7389); - Add Gnomad subpopulation info into the VAT (#7381); - implement GVS ID assignment (#7355); - no longer loading sample info table in this wdl (#7407); - divide up creation/population of temp pet table [VS-48] (#7395); - Sample QC metrics (#7396); - update import for is_loaded (#7416); - fix partition end, add 1 (#7420); - Fixes to CreateFilterSet and ExtractCallset from 30K run (#7423); - Also changed file size from Int to Float in SumBytes task python (#7429); - Adding the subpopulation calculations to the VAT creation WDL (#7399); - 154 De obfuscate (#7435); - filter on gvs_ids for workflow (#7428); - update for assign ids and changes in import (#7439); - need to loop through sets when moving to done (#7440); - add option for create filter set to use sample_info with is_loaded (#7434); - remove dead branch (#7443); - Scaling the VAT -- switch the input to take in a file of vcf shard file names (#7446); - dockstore testing: move validate vat inputs (#7449); - Update GVS sample QC to support multiple callsets per datasset [VS-177] (#7451); - Update GvsImportGenomes.wdl (#7462); - Add extraction uuid BQ label to GvsPrepareCallstep from GvsExtractCohortFromSampleNames (#7458); - Add manifest summary file to GvsExtractCallset (#7457); - Create workflow to create and populate alt_allele table [VS-51] (",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:23942,Security,validat,validate,23942,"; - 317 remove excess header values in VCF extract (#7786); - correct auth in split intervals (#7790); - Add code to (optionally) zero pad the vcf filename. (#7783); - LoadData `maxRetries` parameterized, default increased [VS-383] (#7791); - Update to latest version of ah_var_store gatk override jar (#7793); - GvsUnified WDL to wrap the 6 core GVS WDLs [VS-382] (#7789); - Pinned typing_extensions python package to 4.1.1 to fix conda environment. (#7802); - WeightedSplitInterval fixes [VS-384] [VS-332] (#7795); - Replace Travis with GithubActions (#7754); - Docker build only lfs pulls main/src/resources/large (#7727); - Clean up gatk jars -- looks like we are not passing them properly in the extract (#7788); - Fix typo that broke git lfs pull (#7806); - Document AoU SOP (up to the VAT) [VS-63] (#7807); - Incident VS 365 clinvar classification fix (#7769); - VS-390. Add precision and sensitivity wdl (#7813); - Quickstart based integration test [VS-357] (#7812); - 365 vat python testing additions (#7756); - VS 396 clinvar grabs too many values (#7823); - Added a test to validate WDLs in the scripts directory. (#7826) (#7829); - VAT Performance / Reliability Improvements (#7828); - VAT naming conventions [VS-410] (#7827); - Rc remove ad from vat (#7832); - bugfix, we were trying to grep a binary file (#7837); - Cleanup scripts/variantstore [VS-414] (#7834); - Merge VAT TSV files into single bgzipped file [VS-304] (#7848); - Handle fully and partially loaded samples [VS-262] [VS-258] (#7843); - Ingest Error Handling Fixes [VS-261] (#7841); - First cut at a python notebook to validate inputs. (#7845); - Compute filter scatter [VS-392] (#7852); - remove withdrawn req (#7844); - Improve import error message [VS-437] (#7855); - Fix Input Validation python notebook (#7853); - Add VAT Validation check that aa_change and exon_number are consistently set. (#7850); - Ingest 10K [VS-344] (#7860); - X/Y chromosome reweighting for better extract shard runtime balance [VS-389] (#7868",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:24455,Security,validat,validate,24455,"89); - Pinned typing_extensions python package to 4.1.1 to fix conda environment. (#7802); - WeightedSplitInterval fixes [VS-384] [VS-332] (#7795); - Replace Travis with GithubActions (#7754); - Docker build only lfs pulls main/src/resources/large (#7727); - Clean up gatk jars -- looks like we are not passing them properly in the extract (#7788); - Fix typo that broke git lfs pull (#7806); - Document AoU SOP (up to the VAT) [VS-63] (#7807); - Incident VS 365 clinvar classification fix (#7769); - VS-390. Add precision and sensitivity wdl (#7813); - Quickstart based integration test [VS-357] (#7812); - 365 vat python testing additions (#7756); - VS 396 clinvar grabs too many values (#7823); - Added a test to validate WDLs in the scripts directory. (#7826) (#7829); - VAT Performance / Reliability Improvements (#7828); - VAT naming conventions [VS-410] (#7827); - Rc remove ad from vat (#7832); - bugfix, we were trying to grep a binary file (#7837); - Cleanup scripts/variantstore [VS-414] (#7834); - Merge VAT TSV files into single bgzipped file [VS-304] (#7848); - Handle fully and partially loaded samples [VS-262] [VS-258] (#7843); - Ingest Error Handling Fixes [VS-261] (#7841); - First cut at a python notebook to validate inputs. (#7845); - Compute filter scatter [VS-392] (#7852); - remove withdrawn req (#7844); - Improve import error message [VS-437] (#7855); - Fix Input Validation python notebook (#7853); - Add VAT Validation check that aa_change and exon_number are consistently set. (#7850); - Ingest 10K [VS-344] (#7860); - X/Y chromosome reweighting for better extract shard runtime balance [VS-389] (#7868); - VET Ingest Validation / Allow Ingest of non-VQSR'ed data (#7870); - Fix AoU workflow bugs (#7874); - Curate input arrays to skip already ingested sample data [VS-246] (#7862); - KM upload GVS product sheet (#7883); - Default extract scatter width [VS-415] (#7878); - Volatile tasks review [VS-447] (#7880); - Update Quickstart Integration for X/Y scaling changes ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:24617,Security,Validat,Validation,24617,"lfs pull (#7806); - Document AoU SOP (up to the VAT) [VS-63] (#7807); - Incident VS 365 clinvar classification fix (#7769); - VS-390. Add precision and sensitivity wdl (#7813); - Quickstart based integration test [VS-357] (#7812); - 365 vat python testing additions (#7756); - VS 396 clinvar grabs too many values (#7823); - Added a test to validate WDLs in the scripts directory. (#7826) (#7829); - VAT Performance / Reliability Improvements (#7828); - VAT naming conventions [VS-410] (#7827); - Rc remove ad from vat (#7832); - bugfix, we were trying to grep a binary file (#7837); - Cleanup scripts/variantstore [VS-414] (#7834); - Merge VAT TSV files into single bgzipped file [VS-304] (#7848); - Handle fully and partially loaded samples [VS-262] [VS-258] (#7843); - Ingest Error Handling Fixes [VS-261] (#7841); - First cut at a python notebook to validate inputs. (#7845); - Compute filter scatter [VS-392] (#7852); - remove withdrawn req (#7844); - Improve import error message [VS-437] (#7855); - Fix Input Validation python notebook (#7853); - Add VAT Validation check that aa_change and exon_number are consistently set. (#7850); - Ingest 10K [VS-344] (#7860); - X/Y chromosome reweighting for better extract shard runtime balance [VS-389] (#7868); - VET Ingest Validation / Allow Ingest of non-VQSR'ed data (#7870); - Fix AoU workflow bugs (#7874); - Curate input arrays to skip already ingested sample data [VS-246] (#7862); - KM upload GVS product sheet (#7883); - Default extract scatter width [VS-415] (#7878); - Volatile tasks review [VS-447] (#7880); - Update Quickstart Integration for X/Y scaling changes [VS-464] (#7881); - clean up dockstore; - Rc vs 63 vat sop documentation (#7879); - Fix up FQ and race condition issues with volatile tasks work [VS-478] (#7888); - Use gvs-internal project in integration test (#7901); - Add cost observability BQ table [VS-441] (#7891); - Add preliminary labels to queries [VS-381] (#7902); - Workflow compute costs [VS-472] (#7905); - Fix bu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:24663,Security,Validat,Validation,24663,"lfs pull (#7806); - Document AoU SOP (up to the VAT) [VS-63] (#7807); - Incident VS 365 clinvar classification fix (#7769); - VS-390. Add precision and sensitivity wdl (#7813); - Quickstart based integration test [VS-357] (#7812); - 365 vat python testing additions (#7756); - VS 396 clinvar grabs too many values (#7823); - Added a test to validate WDLs in the scripts directory. (#7826) (#7829); - VAT Performance / Reliability Improvements (#7828); - VAT naming conventions [VS-410] (#7827); - Rc remove ad from vat (#7832); - bugfix, we were trying to grep a binary file (#7837); - Cleanup scripts/variantstore [VS-414] (#7834); - Merge VAT TSV files into single bgzipped file [VS-304] (#7848); - Handle fully and partially loaded samples [VS-262] [VS-258] (#7843); - Ingest Error Handling Fixes [VS-261] (#7841); - First cut at a python notebook to validate inputs. (#7845); - Compute filter scatter [VS-392] (#7852); - remove withdrawn req (#7844); - Improve import error message [VS-437] (#7855); - Fix Input Validation python notebook (#7853); - Add VAT Validation check that aa_change and exon_number are consistently set. (#7850); - Ingest 10K [VS-344] (#7860); - X/Y chromosome reweighting for better extract shard runtime balance [VS-389] (#7868); - VET Ingest Validation / Allow Ingest of non-VQSR'ed data (#7870); - Fix AoU workflow bugs (#7874); - Curate input arrays to skip already ingested sample data [VS-246] (#7862); - KM upload GVS product sheet (#7883); - Default extract scatter width [VS-415] (#7878); - Volatile tasks review [VS-447] (#7880); - Update Quickstart Integration for X/Y scaling changes [VS-464] (#7881); - clean up dockstore; - Rc vs 63 vat sop documentation (#7879); - Fix up FQ and race condition issues with volatile tasks work [VS-478] (#7888); - Use gvs-internal project in integration test (#7901); - Add cost observability BQ table [VS-441] (#7891); - Add preliminary labels to queries [VS-381] (#7902); - Workflow compute costs [VS-472] (#7905); - Fix bu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:24874,Security,Validat,Validation,24874,"to validate inputs. (#7845); - Compute filter scatter [VS-392] (#7852); - remove withdrawn req (#7844); - Improve import error message [VS-437] (#7855); - Fix Input Validation python notebook (#7853); - Add VAT Validation check that aa_change and exon_number are consistently set. (#7850); - Ingest 10K [VS-344] (#7860); - X/Y chromosome reweighting for better extract shard runtime balance [VS-389] (#7868); - VET Ingest Validation / Allow Ingest of non-VQSR'ed data (#7870); - Fix AoU workflow bugs (#7874); - Curate input arrays to skip already ingested sample data [VS-246] (#7862); - KM upload GVS product sheet (#7883); - Default extract scatter width [VS-415] (#7878); - Volatile tasks review [VS-447] (#7880); - Update Quickstart Integration for X/Y scaling changes [VS-464] (#7881); - clean up dockstore; - Rc vs 63 vat sop documentation (#7879); - Fix up FQ and race condition issues with volatile tasks work [VS-478] (#7888); - Use gvs-internal project in integration test (#7901); - Add cost observability BQ table [VS-441] (#7891); - Add preliminary labels to queries [VS-381] (#7902); - Workflow compute costs [VS-472] (#7905); - Fix bug and update images (#7912); - VS 483 Beta user wdl (#7894); - Core storage model cost [VS-473] (#7913); - Update Quickstart & Integration to use re-blocked v2 gVCFs [VS-491] (#7924); - KM GVS documentation (#7903); - Track BigQuery costs of GVS python VS-480 (#7915); - Read cost observability table [VS-475] (#7923); - Fix Race Condition, Add Support for Extract by Array of Sample Names (ie from a Sample Set) (#7917); - Rightsize import batches [VS-486] (#7925); - [AoU DRC] Support uppercase site_ids for reblocking (#7929); - Populate cost metadata for GATK tasks. (#7919); - remove accidentally added input (#7931); - VS_492 - Beta User Jar release (#7934); - Cost WDL should throw on FISS API errors [VS-518] (#7942); - Fix bad check for missing workflow name [VS-520] (#7943); - Remove usage of service account from GvsValidateVAT.wdl (#7937",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:28794,Security,Expose,Expose,28794,"ility to tweak sample-every-Nth-variant parameter for SNP model creation (#8019); - add initial notebook copy pasta (#8008); - add sample_table_timestamp to GetNumSamplesLoaded (#8022); - Batched Avro export [VS-630] (#8020); - Updating references to old GATK for VS-620 (#8023); - VS-517 Use standard version of GetBQTableLastModifiedDatetime in GvsValidateVat (#8024); - Fix bug in GvsWithdrawSamples.wdl (#8026); - Ah 617 exposing the drop_state parameter to the GvsJointVariantCalling wdl used for beta (and internal customer) (#8032); - Expose maximum-training-variants VQSR parameter [VS-634] (#8029); - Callset statistics [VS-560] (#8018); - Check for withdrawn before exporting to AVRO files [VS-646] (#8039); - Small updates to GVS Integration WDL [VS-618] (#8042); - Rework Hail script generation [VS-616] (#8034); - Alpine based Variant Store Docker image [VS-648] (#8044); - update warp version (#7906); - Fail Avro extract and callset stats on bad filter name [VS-655] (#8046); - Vs 629 failure to retrieve job information during ingest (#8047); - Restore accidentally removed bcftools [VS-661] (#8051); - Allowing our pipeline to function with a sample size of one (#8055); - Vs 665 re create vcf for cd 68 po 52339 with ad padding fixed (#8057); - VS-665 and VS-620 updating code to use latest docker images containing Rori's AD calculation changes in extract (#8061); - updating the beta workflow to use the latest jar, representing the version of GATK George tested against the workflow (#8062); - VS-637 Address a couple of issues in SampleLoadStatus handling in GVSImportGenomes. (#8052); - Revert Alpinizing of apt dependent task [VS-688] (#8065); - Fix missing vat schema JSONs [VS-699] (#8072); - Fix integration expectations for fixed AD [VS-689] (#8066); - VS-698 Remove unnecessary columns from Call set statistics (#8073); - Fix Dockerfile nits that break 20.10.21 (#8078); - Nirvana 3.18.1 Docker images support [VS-661] (#8082); - Add option to not prepare __REF_DATA or __",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:30901,Security,Validat,ValidateVat,30901,"ge tested against the workflow (#8062); - VS-637 Address a couple of issues in SampleLoadStatus handling in GVSImportGenomes. (#8052); - Revert Alpinizing of apt dependent task [VS-688] (#8065); - Fix missing vat schema JSONs [VS-699] (#8072); - Fix integration expectations for fixed AD [VS-689] (#8066); - VS-698 Remove unnecessary columns from Call set statistics (#8073); - Fix Dockerfile nits that break 20.10.21 (#8078); - Nirvana 3.18.1 Docker images support [VS-661] (#8082); - Add option to not prepare __REF_DATA or __SAMPLES tables to Prepare [VS-697] (#8079); - ""build-base"" Docker image for faster variantstore image builds [VS-712] (#8085); - GVS / Hail VDS integration test [VS-639] (#8086); - Remove AI/AN from VDS docs [VS-726] (#8096); - Add flag for cost_observability table writing to support sub-cohort use case [VS-521] (#8093); - Document STS delivery process for VDS [VS-727] (#8101); - delete obsolete callset_QC directory and its contents [VS-318] (#8108); - doc link typo and add check for control samples in AVRO export (#8110); - Add defaults for scatter_count in GvsExtractCohortFromSampleNames [VS-496] (#8109); - Escape table names properly in ValidateVat WDL (#8116); - Vs 741 fix indefinite freeze in split intervals task when using exome data (#8113); - VAT Readme updates (#8090); - WDL and python scripts to use the VDS in the VAT (#8077); - VS-757 - Use JASIX to make sub-jsons of annotated output of Nirvana (#8133); - add note about permissions for P&S workflow to work (#8135); - VS-759 (and VS-760) (#8137); - VS-765. Scatter the RemoveDuplicates task. (#8144); - update delivery docs based on latest VDS delivery run [VS-770] (#8150); - Add monitoring to index vcf (#8151); - Make some noise when VDS validation succeeds (#8155); - Handle empty genes annotation file. (#8153); - Add escapes for otherwise problematic dataset / table names. (#8162); - New WDL to create VAT tsvs from previously generated BigQuery table. (#8165); - Treat withdrawn samples in ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:31469,Security,validat,validation,31469,"86); - Remove AI/AN from VDS docs [VS-726] (#8096); - Add flag for cost_observability table writing to support sub-cohort use case [VS-521] (#8093); - Document STS delivery process for VDS [VS-727] (#8101); - delete obsolete callset_QC directory and its contents [VS-318] (#8108); - doc link typo and add check for control samples in AVRO export (#8110); - Add defaults for scatter_count in GvsExtractCohortFromSampleNames [VS-496] (#8109); - Escape table names properly in ValidateVat WDL (#8116); - Vs 741 fix indefinite freeze in split intervals task when using exome data (#8113); - VAT Readme updates (#8090); - WDL and python scripts to use the VDS in the VAT (#8077); - VS-757 - Use JASIX to make sub-jsons of annotated output of Nirvana (#8133); - add note about permissions for P&S workflow to work (#8135); - VS-759 (and VS-760) (#8137); - VS-765. Scatter the RemoveDuplicates task. (#8144); - update delivery docs based on latest VDS delivery run [VS-770] (#8150); - Add monitoring to index vcf (#8151); - Make some noise when VDS validation succeeds (#8155); - Handle empty genes annotation file. (#8153); - Add escapes for otherwise problematic dataset / table names. (#8162); - New WDL to create VAT tsvs from previously generated BigQuery table. (#8165); - Treat withdrawn samples in sub-cohort prepare correctly [VS-772] (#8156); - Remove unused VAT Creation WDL (#8172); - Gg consistently use dataset name as input parameter (#8173); - AoU cleanup docs, round 1 [VS-671] (#8104); - VDS docs remove samples and correct GT [VS-807] (#8178); - [VS-693] Add support for VQSR Lite to GvsCreateFilterSet (#8157); - VAT Documentation Update Round 1 [VS-531]; - VS-530 VDS creation documentation for AoU (#8169); - Update beta docs to tell people not to use free credits (#8184); - VS-816 Keeping ingestion under quota (#8193); - CromwellOnAzure + Azure SQL DB + AAD first steps doc [VS-805] (#8191); - Edit and re-format VDS -> VAT doc [VS-821] (#8187); - VS-820 Incorporate code to stay un",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:1082,Testability,test,test,1082,"clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - forgot this file; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - add fields for uncompressed imputed data; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - Tool for arrays QC metrics calculations (#6812); - ah update array extract tool (#6827); - fix enum (#6834); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - resolved rebase conflicts; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:2684,Testability,test,test,2684,"sed on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - resolved rebase conflicts; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - Moving the WDL for importing array manifest to BQ (#6860); - fix up after rebase; - Moving and testing ingest scripts from variantstore (#6881); - optionally provide sample-map-file instead of sample-map-table (#6872); - Moving extract wdls from variantstore repo (#6902); - update for genomes (#6918); - update paths; - update field name; - consolidate exome and genome code; - missing comma; - allow null for drop state.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:3232,Testability,test,testing,3232,"sed on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - resolved rebase conflicts; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - Moving the WDL for importing array manifest to BQ (#6860); - fix up after rebase; - Moving and testing ingest scripts from variantstore (#6881); - optionally provide sample-map-file instead of sample-map-table (#6872); - Moving extract wdls from variantstore repo (#6902); - update for genomes (#6918); - update paths; - update field name; - consolidate exome and genome code; - missing comma; - allow null for drop state.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:4805,Testability,log,logging,4805,,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:5113,Testability,test,test,5113,,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:5212,Testability,test,test,5212,,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:7054,Testability,test,test,7054,"clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - forgot this file; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - add fields for uncompressed imputed data; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - Tool for arrays QC metrics calculations (#6812); - ah update array extract tool (#6827); - fix enum (#6834); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - resolved rebase conflicts; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:8656,Testability,test,test,8656,"sed on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - resolved rebase conflicts; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - Moving the WDL for importing array manifest to BQ (#6860); - fix up after rebase; - Moving and testing ingest scripts from variantstore (#6881); - optionally provide sample-map-file instead of sample-map-table (#6872); - Moving extract wdls from variantstore repo (#6902); - update for genomes (#6918); - update paths; - update field name; - consolidate exome and genome code; - missing comma; - allow null for drop state.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:9204,Testability,test,testing,9204,"sed on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - resolved rebase conflicts; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - Moving the WDL for importing array manifest to BQ (#6860); - fix up after rebase; - Moving and testing ingest scripts from variantstore (#6881); - optionally provide sample-map-file instead of sample-map-table (#6872); - Moving extract wdls from variantstore repo (#6902); - update for genomes (#6918); - update paths; - update field name; - consolidate exome and genome code; - missing comma; - allow null for drop state.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:10777,Testability,log,logging,10777," table creation and data loading in LoadData (#7056); - WIP; - tieout scripts; - notes files; - updated diff scripts; - fixed bug...; - add wdl and inputs file for warp pipeline; - reverting logging; - included top level WDL; - use gnarly with BQ extract cohort; - remove unused file; - cleaning up; - tidy; - tidy up before PR; - tidy up before PR; - PR comments; - merge conflict misfires; - added example SQL to create alt allele table from VET; - option to remove PLs; - fixed and enhanced unit test; - removing unused config, causing travis to fail; - add CreateVariantIngestFiles integration test (#7071); - add sampleName (instead of NULL) to error message (#7074); - Update To handle if no data error (#7084); - Memory improvement when writing missing positions to pet (#7098); - added support for loading QUALapprox into VET (#7101); - Add -m flag to gsutil step; add dockstore branch filters to facilitate development (#7104); - updates to ImportGenomes and LoadBigQueryData (#7112); - Add ngs to cohort extract Dockerfile; remove exception catching in extract python script (#7113); - remove problematic storage_location imports (#7119); - Reduce memory and CPU for CreateImportTsvs task, check for files before attempting load (#7121); - add -m flag to gsutil mv step (#7129); - ah_var_store : Add sample file argument to cohort extract (#7117); - Perform full WGS cohort extract scientific tieout for 35 ACMG59 samples (#7106); - Enable Read/Execution Project for BQ Queries (#7136); - ah - optional service account (#7140); - Add load lock file to prevent accidental re-loading of data to BQ (#7138); - #251 Address gvcf no-calls missing QUALapprox and other features (#7146); - Job Add labels to BQ operations from GATK (Issues-199) (#7115); - parse map to list to avoid brackets and spaces in vcf output (#7168); - #259 Inline schema for importgenomes.wdl (#7171); - Created AvroFileReader and unittest, Update ExtractCohort and ExtractCohortEngine (#7174); - #224 Import WDL: handle ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:11085,Testability,test,test,11085," table creation and data loading in LoadData (#7056); - WIP; - tieout scripts; - notes files; - updated diff scripts; - fixed bug...; - add wdl and inputs file for warp pipeline; - reverting logging; - included top level WDL; - use gnarly with BQ extract cohort; - remove unused file; - cleaning up; - tidy; - tidy up before PR; - tidy up before PR; - PR comments; - merge conflict misfires; - added example SQL to create alt allele table from VET; - option to remove PLs; - fixed and enhanced unit test; - removing unused config, causing travis to fail; - add CreateVariantIngestFiles integration test (#7071); - add sampleName (instead of NULL) to error message (#7074); - Update To handle if no data error (#7084); - Memory improvement when writing missing positions to pet (#7098); - added support for loading QUALapprox into VET (#7101); - Add -m flag to gsutil step; add dockstore branch filters to facilitate development (#7104); - updates to ImportGenomes and LoadBigQueryData (#7112); - Add ngs to cohort extract Dockerfile; remove exception catching in extract python script (#7113); - remove problematic storage_location imports (#7119); - Reduce memory and CPU for CreateImportTsvs task, check for files before attempting load (#7121); - add -m flag to gsutil mv step (#7129); - ah_var_store : Add sample file argument to cohort extract (#7117); - Perform full WGS cohort extract scientific tieout for 35 ACMG59 samples (#7106); - Enable Read/Execution Project for BQ Queries (#7136); - ah - optional service account (#7140); - Add load lock file to prevent accidental re-loading of data to BQ (#7138); - #251 Address gvcf no-calls missing QUALapprox and other features (#7146); - Job Add labels to BQ operations from GATK (Issues-199) (#7115); - parse map to list to avoid brackets and spaces in vcf output (#7168); - #259 Inline schema for importgenomes.wdl (#7171); - Created AvroFileReader and unittest, Update ExtractCohort and ExtractCohortEngine (#7174); - #224 Import WDL: handle ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:11184,Testability,test,test,11184," table creation and data loading in LoadData (#7056); - WIP; - tieout scripts; - notes files; - updated diff scripts; - fixed bug...; - add wdl and inputs file for warp pipeline; - reverting logging; - included top level WDL; - use gnarly with BQ extract cohort; - remove unused file; - cleaning up; - tidy; - tidy up before PR; - tidy up before PR; - PR comments; - merge conflict misfires; - added example SQL to create alt allele table from VET; - option to remove PLs; - fixed and enhanced unit test; - removing unused config, causing travis to fail; - add CreateVariantIngestFiles integration test (#7071); - add sampleName (instead of NULL) to error message (#7074); - Update To handle if no data error (#7084); - Memory improvement when writing missing positions to pet (#7098); - added support for loading QUALapprox into VET (#7101); - Add -m flag to gsutil step; add dockstore branch filters to facilitate development (#7104); - updates to ImportGenomes and LoadBigQueryData (#7112); - Add ngs to cohort extract Dockerfile; remove exception catching in extract python script (#7113); - remove problematic storage_location imports (#7119); - Reduce memory and CPU for CreateImportTsvs task, check for files before attempting load (#7121); - add -m flag to gsutil mv step (#7129); - ah_var_store : Add sample file argument to cohort extract (#7117); - Perform full WGS cohort extract scientific tieout for 35 ACMG59 samples (#7106); - Enable Read/Execution Project for BQ Queries (#7136); - ah - optional service account (#7140); - Add load lock file to prevent accidental re-loading of data to BQ (#7138); - #251 Address gvcf no-calls missing QUALapprox and other features (#7146); - Job Add labels to BQ operations from GATK (Issues-199) (#7115); - parse map to list to avoid brackets and spaces in vcf output (#7168); - #259 Inline schema for importgenomes.wdl (#7171); - Created AvroFileReader and unittest, Update ExtractCohort and ExtractCohortEngine (#7174); - #224 Import WDL: handle ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:12833,Testability,test,tests,12833," QUALapprox and other features (#7146); - Job Add labels to BQ operations from GATK (Issues-199) (#7115); - parse map to list to avoid brackets and spaces in vcf output (#7168); - #259 Inline schema for importgenomes.wdl (#7171); - Created AvroFileReader and unittest, Update ExtractCohort and ExtractCohortEngine (#7174); - #224 Import WDL: handle 15 TB /table /day import limit (#7167); - #260 filter out AS_QD, SOR, FS from cohort extract VCF (#7173); - Full scientific validation via end to end comparison of filtered results between WARP and BQ (#7179); - Cherry pick of commits to fix GATK tests from master (#7183); - ExtractCohort supports -XL exclusion and follows intervals, other optimizations (#7181); - ExtractFeatures supports -XL exclusion and follows intervals, other optimizations (#7184); - change 0/0 GQ0 sites to nocalls (#7190); - updated (#7195); - Rename ""metadata"" table to ""sample_info"" table, fix vet schema (#7196); - Allow users to specify VQSLOD sensitivity and apply threshold in ExtractCohort (#7194); - Calculate and Store site-level QCs (#7197); - Filter Failing QC Sites from Extract (#7201); - WDLize GvsPrepareCallset (briefly known as CreateCohortTable) (#7200); - default drop_state to 60, but allow NONE as input (#7206); - SA support and consistent naming for all GVS WDLs (#7205); - fix GvsExtractCallset inputs file (#7210); - add clustering to tables (#7207); - add vqsr cutoffs to GvsExtractCallset wdl; clean up dockstore yml (#7209); - Avro test (#7192); - Enable call caching of TSV generation in GvsImportGenomes (#7226); - 266 Clean up ExtractCohort -- remove query mode param (#7227); - 288 Add an excess alleles param (#7221); - take sample name as a param (#7236); - How to run GIAB comparisons (#7237); - Update GvsCreateFilterSet.wdl (#7239); - Use GatherVcfsCloud in GvsCreateFilterSet.wdl (#7241); - parameterize TTL with defaults, reduce memory allocation (#7244); - Addressing OOM in CohortExtract (#7245); - make outputs optional, change case",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:13724,Testability,test,test,13724," QUALapprox and other features (#7146); - Job Add labels to BQ operations from GATK (Issues-199) (#7115); - parse map to list to avoid brackets and spaces in vcf output (#7168); - #259 Inline schema for importgenomes.wdl (#7171); - Created AvroFileReader and unittest, Update ExtractCohort and ExtractCohortEngine (#7174); - #224 Import WDL: handle 15 TB /table /day import limit (#7167); - #260 filter out AS_QD, SOR, FS from cohort extract VCF (#7173); - Full scientific validation via end to end comparison of filtered results between WARP and BQ (#7179); - Cherry pick of commits to fix GATK tests from master (#7183); - ExtractCohort supports -XL exclusion and follows intervals, other optimizations (#7181); - ExtractFeatures supports -XL exclusion and follows intervals, other optimizations (#7184); - change 0/0 GQ0 sites to nocalls (#7190); - updated (#7195); - Rename ""metadata"" table to ""sample_info"" table, fix vet schema (#7196); - Allow users to specify VQSLOD sensitivity and apply threshold in ExtractCohort (#7194); - Calculate and Store site-level QCs (#7197); - Filter Failing QC Sites from Extract (#7201); - WDLize GvsPrepareCallset (briefly known as CreateCohortTable) (#7200); - default drop_state to 60, but allow NONE as input (#7206); - SA support and consistent naming for all GVS WDLs (#7205); - fix GvsExtractCallset inputs file (#7210); - add clustering to tables (#7207); - add vqsr cutoffs to GvsExtractCallset wdl; clean up dockstore yml (#7209); - Avro test (#7192); - Enable call caching of TSV generation in GvsImportGenomes (#7226); - 266 Clean up ExtractCohort -- remove query mode param (#7227); - 288 Add an excess alleles param (#7221); - take sample name as a param (#7236); - How to run GIAB comparisons (#7237); - Update GvsCreateFilterSet.wdl (#7239); - Use GatherVcfsCloud in GvsCreateFilterSet.wdl (#7241); - parameterize TTL with defaults, reduce memory allocation (#7244); - Addressing OOM in CohortExtract (#7245); - make outputs optional, change case",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:15996,Testability,test,test,15996,"epare, factored out sample name (#7288); - Remove training sites only param from ExtractFeatures broadinstitute/dsp-spec-ops#261; - add param for mem for indels (#7282); - Ah prepare localize option (#7299); - Export sites only vcf STEP 1-- 317 add AC, AN, AF to the final VCF (#7279); - AoU GVS Cohort Extract wdl (#7242); - reliability (#7310); - bump to include FT tag filtering (#7316); - First pass at a Terra QuickStart (#7267); - Ah fix timestamp query (#7319); - 313 Cleanup Extract Cohort params (#7293); - bump bq storage version. See GVS-332 (#7330); - Variant Store extraction - Add VCF size to output (#7329); - add WARP-style scattering to SNPsVariantRecalibrator in GvsCreateFilterSet (#7320); - added ref ranges support (#7337); - 318 Sites only filtered vcf then annotate wdl (#7305); - Replace service_account_json (file) with service_account_json_path (string) to allow call-caching (#7347); - Parallelize create filterset by breaking out the 3 filter set file creation/loads into separate tasks (#7342); - Create WDL to validate VAT and add first test (#7352); - Add task for VAT validation #3 (#7360); - Add task for VAT validation #4 (#7363); - Instructions on how to download BQ Metadata and visualize results (#7359); - don't mix contigs, rightsize memory (#7361); - Add custom annotations as ac an af (#7351); - Add task for VAT validation #8 & 9 (#7364); - added bcftools, upgraded gcloud version (#7369); - fix wdl (#7378); - Update .dockstore.yml; - Add VAT validation rule #5 [VS-16] (#7365); - Add VAT validation rule #7 [VS-14] and validation rule #6 [VS-15] (#7379); - Batching of samples for create import TSVs (#7382); - Add VAT validation rule #2 [VS-19] (#7374); - Create VAT scripts directory (#7386); - fixing SA change from file to string (#7371); - add extract_subpop script (#7387); - Add is_loaded column to sample_info and logic to populate after ingest [VS-158] (#7389); - Add Gnomad subpopulation info into the VAT (#7381); - implement GVS ID assignment (#",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:16795,Testability,log,logic,16795,"#7359); - don't mix contigs, rightsize memory (#7361); - Add custom annotations as ac an af (#7351); - Add task for VAT validation #8 & 9 (#7364); - added bcftools, upgraded gcloud version (#7369); - fix wdl (#7378); - Update .dockstore.yml; - Add VAT validation rule #5 [VS-16] (#7365); - Add VAT validation rule #7 [VS-14] and validation rule #6 [VS-15] (#7379); - Batching of samples for create import TSVs (#7382); - Add VAT validation rule #2 [VS-19] (#7374); - Create VAT scripts directory (#7386); - fixing SA change from file to string (#7371); - add extract_subpop script (#7387); - Add is_loaded column to sample_info and logic to populate after ingest [VS-158] (#7389); - Add Gnomad subpopulation info into the VAT (#7381); - implement GVS ID assignment (#7355); - no longer loading sample info table in this wdl (#7407); - divide up creation/population of temp pet table [VS-48] (#7395); - Sample QC metrics (#7396); - update import for is_loaded (#7416); - fix partition end, add 1 (#7420); - Fixes to CreateFilterSet and ExtractCallset from 30K run (#7423); - Also changed file size from Int to Float in SumBytes task python (#7429); - Adding the subpopulation calculations to the VAT creation WDL (#7399); - 154 De obfuscate (#7435); - filter on gvs_ids for workflow (#7428); - update for assign ids and changes in import (#7439); - need to loop through sets when moving to done (#7440); - add option for create filter set to use sample_info with is_loaded (#7434); - remove dead branch (#7443); - Scaling the VAT -- switch the input to take in a file of vcf shard file names (#7446); - dockstore testing: move validate vat inputs (#7449); - Update GVS sample QC to support multiple callsets per datasset [VS-177] (#7451); - Update GvsImportGenomes.wdl (#7462); - Add extraction uuid BQ label to GvsPrepareCallstep from GvsExtractCohortFromSampleNames (#7458); - Add manifest summary file to GvsExtractCallset (#7457); - Create workflow to create and populate alt_allele table [VS-51] (",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:17775,Testability,test,testing,17775,"#7359); - don't mix contigs, rightsize memory (#7361); - Add custom annotations as ac an af (#7351); - Add task for VAT validation #8 & 9 (#7364); - added bcftools, upgraded gcloud version (#7369); - fix wdl (#7378); - Update .dockstore.yml; - Add VAT validation rule #5 [VS-16] (#7365); - Add VAT validation rule #7 [VS-14] and validation rule #6 [VS-15] (#7379); - Batching of samples for create import TSVs (#7382); - Add VAT validation rule #2 [VS-19] (#7374); - Create VAT scripts directory (#7386); - fixing SA change from file to string (#7371); - add extract_subpop script (#7387); - Add is_loaded column to sample_info and logic to populate after ingest [VS-158] (#7389); - Add Gnomad subpopulation info into the VAT (#7381); - implement GVS ID assignment (#7355); - no longer loading sample info table in this wdl (#7407); - divide up creation/population of temp pet table [VS-48] (#7395); - Sample QC metrics (#7396); - update import for is_loaded (#7416); - fix partition end, add 1 (#7420); - Fixes to CreateFilterSet and ExtractCallset from 30K run (#7423); - Also changed file size from Int to Float in SumBytes task python (#7429); - Adding the subpopulation calculations to the VAT creation WDL (#7399); - 154 De obfuscate (#7435); - filter on gvs_ids for workflow (#7428); - update for assign ids and changes in import (#7439); - need to loop through sets when moving to done (#7440); - add option for create filter set to use sample_info with is_loaded (#7434); - remove dead branch (#7443); - Scaling the VAT -- switch the input to take in a file of vcf shard file names (#7446); - dockstore testing: move validate vat inputs (#7449); - Update GVS sample QC to support multiple callsets per datasset [VS-177] (#7451); - Update GvsImportGenomes.wdl (#7462); - Add extraction uuid BQ label to GvsPrepareCallstep from GvsExtractCohortFromSampleNames (#7458); - Add manifest summary file to GvsExtractCallset (#7457); - Create workflow to create and populate alt_allele table [VS-51] (",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:20592,Testability,log,logging,20592,"rom Lee [VS-210] (#7502); - Export the VAT into GCS (#7472); - addresses VS-219 (#7508); - small fix to MergeVCFs (#7517); - small fixes to GVS pipeline (#7522); - make sure ExtractTask is run on all interval files; - Revert ""make sure ExtractTask is run on all interval files""; - make sure ExtractTask is run on all interval files (#7527); - Remove Sites only step from the VAT creation WDL (#7510); - fix bad argument processing for bool (#7529); - Support for TDR DRS URIs in Import (#7528); - Match format of filename output in GvsRescatterCallsetInterval (#7539); - Reference block storage and query support (#7498); - update docs (#7540); - Kc fix rr load bug (#7550); - Update .dockstore.yml (#7553); - Ah add reblocking wdl (#7544); - Scatter over all interval files, not just scatter count (#7551); - fixed docker (#7558); - take advantage of fixed version of SplitIntervals (#7566); - Document AoU-specific tieout [VS-233] (#7552); - bad param assignment in aou reblocking (#7572); - Small fixes to ImportGenomes (non-write api version) (#7574); - Ah change output of reblocking wdl to external path (#7575); - close BQ Readers (#7583); - Ah spike writeapi (#7530); - bump WDL jar (#7593); - read api bytes logging, upgrade bigquery client versions (#7601); - bump (#7610); - upgrade log4j to 2.17 (#7616); - Add drop_state default of Forty to extract (#7619); - Kc fix type (#7620); - VAT cleanup and documentation (#7531); - fix empty flush (#7627); - presorted avro files, fix performance issue (#7635); - WIP extract for ranges (#7640); - VS-268 import more samples at once (#7629); - clustering vqsr tables by location (#7656); - First Version of a weight-based splitter (#7643); - Update GvsExtractCallset.wdl; - Quoting of table names (#7666); - docs for analysis of shard runtimes for balanced sharding (#7645); - Wire through GvsExtractCohortFromSampleNames with new prepare/extract [VS-283] (#7654); - Update GvsExtractCallset.wdl (#7678); - cherry pick lb_lfs_force change (#7683",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:21734,Testability,test,tests,21734,"ntimes for balanced sharding (#7645); - Wire through GvsExtractCohortFromSampleNames with new prepare/extract [VS-283] (#7654); - Update GvsExtractCallset.wdl (#7678); - cherry pick lb_lfs_force change (#7683); - Tweak ingest messaging and failure mode [VS-267] (#7680); - Additional tweaks for GvsExtractCohortFromSampleNames [VS-283] (#7698); - VS-280 Create a VAT intermediary (#7657); - There something about split intervals [VS-306] (#7694); - VS 284 Add prepare step to Quick Start (#7685); - VS-222 dont hard code the dataset name! (#7704); - fixed bug; added tests (#7717); - Clean up optional and inconsistently named inputs [VS-294] [VS-218] (#7715); - VS-263 notes on ingest and beyond (#7618); - Add task to ExtractCallset that verifies filter_set_name exists in GVS dataset [VS-335] (#7734); - Clean up input json files to reflect changes inputs [VS-337] (#7733); - used constants; implemented non-AS transformation (#7718); - Pass dataset name to gatk ExtractFeatures (#7735); - Add withdrawn and is_control columns [VS-70] [VS-213] (#7736); - Allow interval lists that require the SA to see (#7743); - allow for gatk to be overridden, update with known good jar (#7758); - VS-361 Add GvsWithdrawSamples wdl (#7765); - Extract Performance Improvements (#7686); - Don't put withdrawn sample data in alt_allele table [VS-369] (#7762); - remove PET code (#7768); - Adding AD for scale testing VS 225 add AD (#7713); - Deterministic Sample ID assignments [VS-371] (#7770); - remove R scripts from filtering (#7781); - Remove an old ""temp table"" dataset (#7780); - Clean up LocalizeFile [VS-314] (#7771); - Remove pet code from CreateVariantIngestFiles and friends [VS-375] (#7773); - 317 remove excess header values in VCF extract (#7786); - correct auth in split intervals (#7790); - Add code to (optionally) zero pad the vcf filename. (#7783); - LoadData `maxRetries` parameterized, default increased [VS-383] (#7791); - Update to latest version of ah_var_store gatk override jar (#7793); ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:22563,Testability,test,testing,22563,"ntimes for balanced sharding (#7645); - Wire through GvsExtractCohortFromSampleNames with new prepare/extract [VS-283] (#7654); - Update GvsExtractCallset.wdl (#7678); - cherry pick lb_lfs_force change (#7683); - Tweak ingest messaging and failure mode [VS-267] (#7680); - Additional tweaks for GvsExtractCohortFromSampleNames [VS-283] (#7698); - VS-280 Create a VAT intermediary (#7657); - There something about split intervals [VS-306] (#7694); - VS 284 Add prepare step to Quick Start (#7685); - VS-222 dont hard code the dataset name! (#7704); - fixed bug; added tests (#7717); - Clean up optional and inconsistently named inputs [VS-294] [VS-218] (#7715); - VS-263 notes on ingest and beyond (#7618); - Add task to ExtractCallset that verifies filter_set_name exists in GVS dataset [VS-335] (#7734); - Clean up input json files to reflect changes inputs [VS-337] (#7733); - used constants; implemented non-AS transformation (#7718); - Pass dataset name to gatk ExtractFeatures (#7735); - Add withdrawn and is_control columns [VS-70] [VS-213] (#7736); - Allow interval lists that require the SA to see (#7743); - allow for gatk to be overridden, update with known good jar (#7758); - VS-361 Add GvsWithdrawSamples wdl (#7765); - Extract Performance Improvements (#7686); - Don't put withdrawn sample data in alt_allele table [VS-369] (#7762); - remove PET code (#7768); - Adding AD for scale testing VS 225 add AD (#7713); - Deterministic Sample ID assignments [VS-371] (#7770); - remove R scripts from filtering (#7781); - Remove an old ""temp table"" dataset (#7780); - Clean up LocalizeFile [VS-314] (#7771); - Remove pet code from CreateVariantIngestFiles and friends [VS-375] (#7773); - 317 remove excess header values in VCF extract (#7786); - correct auth in split intervals (#7790); - Add code to (optionally) zero pad the vcf filename. (#7783); - LoadData `maxRetries` parameterized, default increased [VS-383] (#7791); - Update to latest version of ah_var_store gatk override jar (#7793); ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:23809,Testability,test,test,23809,"; - 317 remove excess header values in VCF extract (#7786); - correct auth in split intervals (#7790); - Add code to (optionally) zero pad the vcf filename. (#7783); - LoadData `maxRetries` parameterized, default increased [VS-383] (#7791); - Update to latest version of ah_var_store gatk override jar (#7793); - GvsUnified WDL to wrap the 6 core GVS WDLs [VS-382] (#7789); - Pinned typing_extensions python package to 4.1.1 to fix conda environment. (#7802); - WeightedSplitInterval fixes [VS-384] [VS-332] (#7795); - Replace Travis with GithubActions (#7754); - Docker build only lfs pulls main/src/resources/large (#7727); - Clean up gatk jars -- looks like we are not passing them properly in the extract (#7788); - Fix typo that broke git lfs pull (#7806); - Document AoU SOP (up to the VAT) [VS-63] (#7807); - Incident VS 365 clinvar classification fix (#7769); - VS-390. Add precision and sensitivity wdl (#7813); - Quickstart based integration test [VS-357] (#7812); - 365 vat python testing additions (#7756); - VS 396 clinvar grabs too many values (#7823); - Added a test to validate WDLs in the scripts directory. (#7826) (#7829); - VAT Performance / Reliability Improvements (#7828); - VAT naming conventions [VS-410] (#7827); - Rc remove ad from vat (#7832); - bugfix, we were trying to grep a binary file (#7837); - Cleanup scripts/variantstore [VS-414] (#7834); - Merge VAT TSV files into single bgzipped file [VS-304] (#7848); - Handle fully and partially loaded samples [VS-262] [VS-258] (#7843); - Ingest Error Handling Fixes [VS-261] (#7841); - First cut at a python notebook to validate inputs. (#7845); - Compute filter scatter [VS-392] (#7852); - remove withdrawn req (#7844); - Improve import error message [VS-437] (#7855); - Fix Input Validation python notebook (#7853); - Add VAT Validation check that aa_change and exon_number are consistently set. (#7850); - Ingest 10K [VS-344] (#7860); - X/Y chromosome reweighting for better extract shard runtime balance [VS-389] (#7868",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:23849,Testability,test,testing,23849,"; - 317 remove excess header values in VCF extract (#7786); - correct auth in split intervals (#7790); - Add code to (optionally) zero pad the vcf filename. (#7783); - LoadData `maxRetries` parameterized, default increased [VS-383] (#7791); - Update to latest version of ah_var_store gatk override jar (#7793); - GvsUnified WDL to wrap the 6 core GVS WDLs [VS-382] (#7789); - Pinned typing_extensions python package to 4.1.1 to fix conda environment. (#7802); - WeightedSplitInterval fixes [VS-384] [VS-332] (#7795); - Replace Travis with GithubActions (#7754); - Docker build only lfs pulls main/src/resources/large (#7727); - Clean up gatk jars -- looks like we are not passing them properly in the extract (#7788); - Fix typo that broke git lfs pull (#7806); - Document AoU SOP (up to the VAT) [VS-63] (#7807); - Incident VS 365 clinvar classification fix (#7769); - VS-390. Add precision and sensitivity wdl (#7813); - Quickstart based integration test [VS-357] (#7812); - 365 vat python testing additions (#7756); - VS 396 clinvar grabs too many values (#7823); - Added a test to validate WDLs in the scripts directory. (#7826) (#7829); - VAT Performance / Reliability Improvements (#7828); - VAT naming conventions [VS-410] (#7827); - Rc remove ad from vat (#7832); - bugfix, we were trying to grep a binary file (#7837); - Cleanup scripts/variantstore [VS-414] (#7834); - Merge VAT TSV files into single bgzipped file [VS-304] (#7848); - Handle fully and partially loaded samples [VS-262] [VS-258] (#7843); - Ingest Error Handling Fixes [VS-261] (#7841); - First cut at a python notebook to validate inputs. (#7845); - Compute filter scatter [VS-392] (#7852); - remove withdrawn req (#7844); - Improve import error message [VS-437] (#7855); - Fix Input Validation python notebook (#7853); - Add VAT Validation check that aa_change and exon_number are consistently set. (#7850); - Ingest 10K [VS-344] (#7860); - X/Y chromosome reweighting for better extract shard runtime balance [VS-389] (#7868",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:23934,Testability,test,test,23934,"; - 317 remove excess header values in VCF extract (#7786); - correct auth in split intervals (#7790); - Add code to (optionally) zero pad the vcf filename. (#7783); - LoadData `maxRetries` parameterized, default increased [VS-383] (#7791); - Update to latest version of ah_var_store gatk override jar (#7793); - GvsUnified WDL to wrap the 6 core GVS WDLs [VS-382] (#7789); - Pinned typing_extensions python package to 4.1.1 to fix conda environment. (#7802); - WeightedSplitInterval fixes [VS-384] [VS-332] (#7795); - Replace Travis with GithubActions (#7754); - Docker build only lfs pulls main/src/resources/large (#7727); - Clean up gatk jars -- looks like we are not passing them properly in the extract (#7788); - Fix typo that broke git lfs pull (#7806); - Document AoU SOP (up to the VAT) [VS-63] (#7807); - Incident VS 365 clinvar classification fix (#7769); - VS-390. Add precision and sensitivity wdl (#7813); - Quickstart based integration test [VS-357] (#7812); - 365 vat python testing additions (#7756); - VS 396 clinvar grabs too many values (#7823); - Added a test to validate WDLs in the scripts directory. (#7826) (#7829); - VAT Performance / Reliability Improvements (#7828); - VAT naming conventions [VS-410] (#7827); - Rc remove ad from vat (#7832); - bugfix, we were trying to grep a binary file (#7837); - Cleanup scripts/variantstore [VS-414] (#7834); - Merge VAT TSV files into single bgzipped file [VS-304] (#7848); - Handle fully and partially loaded samples [VS-262] [VS-258] (#7843); - Ingest Error Handling Fixes [VS-261] (#7841); - First cut at a python notebook to validate inputs. (#7845); - Compute filter scatter [VS-392] (#7852); - remove withdrawn req (#7844); - Improve import error message [VS-437] (#7855); - Fix Input Validation python notebook (#7853); - Add VAT Validation check that aa_change and exon_number are consistently set. (#7850); - Ingest 10K [VS-344] (#7860); - X/Y chromosome reweighting for better extract shard runtime balance [VS-389] (#7868",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:25431,Testability,test,test,25431,"to validate inputs. (#7845); - Compute filter scatter [VS-392] (#7852); - remove withdrawn req (#7844); - Improve import error message [VS-437] (#7855); - Fix Input Validation python notebook (#7853); - Add VAT Validation check that aa_change and exon_number are consistently set. (#7850); - Ingest 10K [VS-344] (#7860); - X/Y chromosome reweighting for better extract shard runtime balance [VS-389] (#7868); - VET Ingest Validation / Allow Ingest of non-VQSR'ed data (#7870); - Fix AoU workflow bugs (#7874); - Curate input arrays to skip already ingested sample data [VS-246] (#7862); - KM upload GVS product sheet (#7883); - Default extract scatter width [VS-415] (#7878); - Volatile tasks review [VS-447] (#7880); - Update Quickstart Integration for X/Y scaling changes [VS-464] (#7881); - clean up dockstore; - Rc vs 63 vat sop documentation (#7879); - Fix up FQ and race condition issues with volatile tasks work [VS-478] (#7888); - Use gvs-internal project in integration test (#7901); - Add cost observability BQ table [VS-441] (#7891); - Add preliminary labels to queries [VS-381] (#7902); - Workflow compute costs [VS-472] (#7905); - Fix bug and update images (#7912); - VS 483 Beta user wdl (#7894); - Core storage model cost [VS-473] (#7913); - Update Quickstart & Integration to use re-blocked v2 gVCFs [VS-491] (#7924); - KM GVS documentation (#7903); - Track BigQuery costs of GVS python VS-480 (#7915); - Read cost observability table [VS-475] (#7923); - Fix Race Condition, Add Support for Extract by Array of Sample Names (ie from a Sample Set) (#7917); - Rightsize import batches [VS-486] (#7925); - [AoU DRC] Support uppercase site_ids for reblocking (#7929); - Populate cost metadata for GATK tasks. (#7919); - remove accidentally added input (#7931); - VS_492 - Beta User Jar release (#7934); - Cost WDL should throw on FISS API errors [VS-518] (#7942); - Fix bad check for missing workflow name [VS-520] (#7943); - Remove usage of service account from GvsValidateVAT.wdl (#7937",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:26473,Testability,test,testablity,26473," [VS-472] (#7905); - Fix bug and update images (#7912); - VS 483 Beta user wdl (#7894); - Core storage model cost [VS-473] (#7913); - Update Quickstart & Integration to use re-blocked v2 gVCFs [VS-491] (#7924); - KM GVS documentation (#7903); - Track BigQuery costs of GVS python VS-480 (#7915); - Read cost observability table [VS-475] (#7923); - Fix Race Condition, Add Support for Extract by Array of Sample Names (ie from a Sample Set) (#7917); - Rightsize import batches [VS-486] (#7925); - [AoU DRC] Support uppercase site_ids for reblocking (#7929); - Populate cost metadata for GATK tasks. (#7919); - remove accidentally added input (#7931); - VS_492 - Beta User Jar release (#7934); - Cost WDL should throw on FISS API errors [VS-518] (#7942); - Fix bad check for missing workflow name [VS-520] (#7943); - Remove usage of service account from GvsValidateVAT.wdl (#7937); - refactoring for testablity (#7946); - More import retries [VS-532] (#7953); - A few last doc changes (#7927); - WDL to extract a single callset cost (BQ only, not Terra) (#7940); - Temporarily swap in Corretto for Temurin as we can't download Temurin. (#7969); - GL-548 - Update CreateVat code to handle samples that do not contain all population groups. (#7965); - Restore Temurin 11 [VS-570] (#7972); - Add table size check to quickstart integration test [VS-501] (#7970); - Consolidate various docs for AoU callset generation into one to rule them all [VS-553] (#7971); - VS-567. Removing usage of ServiceAccount from CreateVat related WDLs (#7974); - WDL to extract Avro files for Hail import [VS-579] (#7981); - Removed usage of service account from WDLs (#7985); - Document steps for GVS cleanup for base use case [VS-586] (#7989); - Change backticks to single quotes in several error messages - causing shell to attempt to execute. (#7995); - VS-598 - Minor update to AoU Documentation. (#7994); - Allow for incremental addition of data to alt_allele [VS-52] (#7993); - Minor AoU Documentation Update (#7999); -",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:26909,Testability,test,test,26909,"x Race Condition, Add Support for Extract by Array of Sample Names (ie from a Sample Set) (#7917); - Rightsize import batches [VS-486] (#7925); - [AoU DRC] Support uppercase site_ids for reblocking (#7929); - Populate cost metadata for GATK tasks. (#7919); - remove accidentally added input (#7931); - VS_492 - Beta User Jar release (#7934); - Cost WDL should throw on FISS API errors [VS-518] (#7942); - Fix bad check for missing workflow name [VS-520] (#7943); - Remove usage of service account from GvsValidateVAT.wdl (#7937); - refactoring for testablity (#7946); - More import retries [VS-532] (#7953); - A few last doc changes (#7927); - WDL to extract a single callset cost (BQ only, not Terra) (#7940); - Temporarily swap in Corretto for Temurin as we can't download Temurin. (#7969); - GL-548 - Update CreateVat code to handle samples that do not contain all population groups. (#7965); - Restore Temurin 11 [VS-570] (#7972); - Add table size check to quickstart integration test [VS-501] (#7970); - Consolidate various docs for AoU callset generation into one to rule them all [VS-553] (#7971); - VS-567. Removing usage of ServiceAccount from CreateVat related WDLs (#7974); - WDL to extract Avro files for Hail import [VS-579] (#7981); - Removed usage of service account from WDLs (#7985); - Document steps for GVS cleanup for base use case [VS-586] (#7989); - Change backticks to single quotes in several error messages - causing shell to attempt to execute. (#7995); - VS-598 - Minor update to AoU Documentation. (#7994); - Allow for incremental addition of data to alt_allele [VS-52] (#7993); - Minor AoU Documentation Update (#7999); - Batch population of alt_allele table from vet_ tables [VS-265] (#7998); - Change drop_state to NONE for Ingest/Extract [VS-607] (#8000); - python -> python3 (#8001); - Generate Hail import/export script [VS-605] (#8002); - clearer error when values are missing (#7939); - Ah [VS-565] output intervals and sample list (#8010); - make CreateAltAlleleT",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:29728,Testability,test,tested,29728,"ility to tweak sample-every-Nth-variant parameter for SNP model creation (#8019); - add initial notebook copy pasta (#8008); - add sample_table_timestamp to GetNumSamplesLoaded (#8022); - Batched Avro export [VS-630] (#8020); - Updating references to old GATK for VS-620 (#8023); - VS-517 Use standard version of GetBQTableLastModifiedDatetime in GvsValidateVat (#8024); - Fix bug in GvsWithdrawSamples.wdl (#8026); - Ah 617 exposing the drop_state parameter to the GvsJointVariantCalling wdl used for beta (and internal customer) (#8032); - Expose maximum-training-variants VQSR parameter [VS-634] (#8029); - Callset statistics [VS-560] (#8018); - Check for withdrawn before exporting to AVRO files [VS-646] (#8039); - Small updates to GVS Integration WDL [VS-618] (#8042); - Rework Hail script generation [VS-616] (#8034); - Alpine based Variant Store Docker image [VS-648] (#8044); - update warp version (#7906); - Fail Avro extract and callset stats on bad filter name [VS-655] (#8046); - Vs 629 failure to retrieve job information during ingest (#8047); - Restore accidentally removed bcftools [VS-661] (#8051); - Allowing our pipeline to function with a sample size of one (#8055); - Vs 665 re create vcf for cd 68 po 52339 with ad padding fixed (#8057); - VS-665 and VS-620 updating code to use latest docker images containing Rori's AD calculation changes in extract (#8061); - updating the beta workflow to use the latest jar, representing the version of GATK George tested against the workflow (#8062); - VS-637 Address a couple of issues in SampleLoadStatus handling in GVSImportGenomes. (#8052); - Revert Alpinizing of apt dependent task [VS-688] (#8065); - Fix missing vat schema JSONs [VS-699] (#8072); - Fix integration expectations for fixed AD [VS-689] (#8066); - VS-698 Remove unnecessary columns from Call set statistics (#8073); - Fix Dockerfile nits that break 20.10.21 (#8078); - Nirvana 3.18.1 Docker images support [VS-661] (#8082); - Add option to not prepare __REF_DATA or __",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:30409,Testability,test,test,30409,"ge tested against the workflow (#8062); - VS-637 Address a couple of issues in SampleLoadStatus handling in GVSImportGenomes. (#8052); - Revert Alpinizing of apt dependent task [VS-688] (#8065); - Fix missing vat schema JSONs [VS-699] (#8072); - Fix integration expectations for fixed AD [VS-689] (#8066); - VS-698 Remove unnecessary columns from Call set statistics (#8073); - Fix Dockerfile nits that break 20.10.21 (#8078); - Nirvana 3.18.1 Docker images support [VS-661] (#8082); - Add option to not prepare __REF_DATA or __SAMPLES tables to Prepare [VS-697] (#8079); - ""build-base"" Docker image for faster variantstore image builds [VS-712] (#8085); - GVS / Hail VDS integration test [VS-639] (#8086); - Remove AI/AN from VDS docs [VS-726] (#8096); - Add flag for cost_observability table writing to support sub-cohort use case [VS-521] (#8093); - Document STS delivery process for VDS [VS-727] (#8101); - delete obsolete callset_QC directory and its contents [VS-318] (#8108); - doc link typo and add check for control samples in AVRO export (#8110); - Add defaults for scatter_count in GvsExtractCohortFromSampleNames [VS-496] (#8109); - Escape table names properly in ValidateVat WDL (#8116); - Vs 741 fix indefinite freeze in split intervals task when using exome data (#8113); - VAT Readme updates (#8090); - WDL and python scripts to use the VDS in the VAT (#8077); - VS-757 - Use JASIX to make sub-jsons of annotated output of Nirvana (#8133); - add note about permissions for P&S workflow to work (#8135); - VS-759 (and VS-760) (#8137); - VS-765. Scatter the RemoveDuplicates task. (#8144); - update delivery docs based on latest VDS delivery run [VS-770] (#8150); - Add monitoring to index vcf (#8151); - Make some noise when VDS validation succeeds (#8155); - Handle empty genes annotation file. (#8153); - Add escapes for otherwise problematic dataset / table names. (#8162); - New WDL to create VAT tsvs from previously generated BigQuery table. (#8165); - Treat withdrawn samples in ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:27799,Usability,clear,clearer,27799,ge of ServiceAccount from CreateVat related WDLs (#7974); - WDL to extract Avro files for Hail import [VS-579] (#7981); - Removed usage of service account from WDLs (#7985); - Document steps for GVS cleanup for base use case [VS-586] (#7989); - Change backticks to single quotes in several error messages - causing shell to attempt to execute. (#7995); - VS-598 - Minor update to AoU Documentation. (#7994); - Allow for incremental addition of data to alt_allele [VS-52] (#7993); - Minor AoU Documentation Update (#7999); - Batch population of alt_allele table from vet_ tables [VS-265] (#7998); - Change drop_state to NONE for Ingest/Extract [VS-607] (#8000); - python -> python3 (#8001); - Generate Hail import/export script [VS-605] (#8002); - clearer error when values are missing (#7939); - Ah [VS-565] output intervals and sample list (#8010); - make CreateAltAlleleTable task volatile (#8011); - Restore withdrawn [VS-581] (#8006); - Km gvs add storage cost and cleanup doc (#8012); - Updating documentation to reflect the changed outputs [VS-565] (#8014); - File of callset samples -> samples marked as 'withdrawn' in GVS [VS-436] (#8009); - fix quota guidelines for CPUs (#8016); - Add in ability to tweak sample-every-Nth-variant parameter for SNP model creation (#8019); - add initial notebook copy pasta (#8008); - add sample_table_timestamp to GetNumSamplesLoaded (#8022); - Batched Avro export [VS-630] (#8020); - Updating references to old GATK for VS-620 (#8023); - VS-517 Use standard version of GetBQTableLastModifiedDatetime in GvsValidateVat (#8024); - Fix bug in GvsWithdrawSamples.wdl (#8026); - Ah 617 exposing the drop_state parameter to the GvsJointVariantCalling wdl used for beta (and internal customer) (#8032); - Expose maximum-training-variants VQSR parameter [VS-634] (#8029); - Callset statistics [VS-560] (#8018); - Check for withdrawn before exporting to AVRO files [VS-646] (#8039); - Small updates to GVS Integration WDL [VS-618] (#8042); - Rework Hail script gener,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:28212,Usability,guid,guidelines,28212,ge of ServiceAccount from CreateVat related WDLs (#7974); - WDL to extract Avro files for Hail import [VS-579] (#7981); - Removed usage of service account from WDLs (#7985); - Document steps for GVS cleanup for base use case [VS-586] (#7989); - Change backticks to single quotes in several error messages - causing shell to attempt to execute. (#7995); - VS-598 - Minor update to AoU Documentation. (#7994); - Allow for incremental addition of data to alt_allele [VS-52] (#7993); - Minor AoU Documentation Update (#7999); - Batch population of alt_allele table from vet_ tables [VS-265] (#7998); - Change drop_state to NONE for Ingest/Extract [VS-607] (#8000); - python -> python3 (#8001); - Generate Hail import/export script [VS-605] (#8002); - clearer error when values are missing (#7939); - Ah [VS-565] output intervals and sample list (#8010); - make CreateAltAlleleTable task volatile (#8011); - Restore withdrawn [VS-581] (#8006); - Km gvs add storage cost and cleanup doc (#8012); - Updating documentation to reflect the changed outputs [VS-565] (#8014); - File of callset samples -> samples marked as 'withdrawn' in GVS [VS-436] (#8009); - fix quota guidelines for CPUs (#8016); - Add in ability to tweak sample-every-Nth-variant parameter for SNP model creation (#8019); - add initial notebook copy pasta (#8008); - add sample_table_timestamp to GetNumSamplesLoaded (#8022); - Batched Avro export [VS-630] (#8020); - Updating references to old GATK for VS-620 (#8023); - VS-517 Use standard version of GetBQTableLastModifiedDatetime in GvsValidateVat (#8024); - Fix bug in GvsWithdrawSamples.wdl (#8026); - Ah 617 exposing the drop_state parameter to the GvsJointVariantCalling wdl used for beta (and internal customer) (#8032); - Expose maximum-training-variants VQSR parameter [VS-634] (#8029); - Callset statistics [VS-560] (#8018); - Check for withdrawn before exporting to AVRO files [VS-646] (#8039); - Small updates to GVS Integration WDL [VS-618] (#8042); - Rework Hail script gener,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8250:33,Testability,test,tests,33,This should add an exclusion for tests so they don't run when only the var store scripts are changed.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8250
https://github.com/broadinstitute/gatk/pull/8251:5264,Availability,error,error,5264,,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:5317,Availability,error,error,5317,,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:11236,Availability,error,error,11236," table creation and data loading in LoadData (#7056); - WIP; - tieout scripts; - notes files; - updated diff scripts; - fixed bug...; - add wdl and inputs file for warp pipeline; - reverting logging; - included top level WDL; - use gnarly with BQ extract cohort; - remove unused file; - cleaning up; - tidy; - tidy up before PR; - tidy up before PR; - PR comments; - merge conflict misfires; - added example SQL to create alt allele table from VET; - option to remove PLs; - fixed and enhanced unit test; - removing unused config, causing travis to fail; - add CreateVariantIngestFiles integration test (#7071); - add sampleName (instead of NULL) to error message (#7074); - Update To handle if no data error (#7084); - Memory improvement when writing missing positions to pet (#7098); - added support for loading QUALapprox into VET (#7101); - Add -m flag to gsutil step; add dockstore branch filters to facilitate development (#7104); - updates to ImportGenomes and LoadBigQueryData (#7112); - Add ngs to cohort extract Dockerfile; remove exception catching in extract python script (#7113); - remove problematic storage_location imports (#7119); - Reduce memory and CPU for CreateImportTsvs task, check for files before attempting load (#7121); - add -m flag to gsutil mv step (#7129); - ah_var_store : Add sample file argument to cohort extract (#7117); - Perform full WGS cohort extract scientific tieout for 35 ACMG59 samples (#7106); - Enable Read/Execution Project for BQ Queries (#7136); - ah - optional service account (#7140); - Add load lock file to prevent accidental re-loading of data to BQ (#7138); - #251 Address gvcf no-calls missing QUALapprox and other features (#7146); - Job Add labels to BQ operations from GATK (Issues-199) (#7115); - parse map to list to avoid brackets and spaces in vcf output (#7168); - #259 Inline schema for importgenomes.wdl (#7171); - Created AvroFileReader and unittest, Update ExtractCohort and ExtractCohortEngine (#7174); - #224 Import WDL: handle ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:11289,Availability,error,error,11289," table creation and data loading in LoadData (#7056); - WIP; - tieout scripts; - notes files; - updated diff scripts; - fixed bug...; - add wdl and inputs file for warp pipeline; - reverting logging; - included top level WDL; - use gnarly with BQ extract cohort; - remove unused file; - cleaning up; - tidy; - tidy up before PR; - tidy up before PR; - PR comments; - merge conflict misfires; - added example SQL to create alt allele table from VET; - option to remove PLs; - fixed and enhanced unit test; - removing unused config, causing travis to fail; - add CreateVariantIngestFiles integration test (#7071); - add sampleName (instead of NULL) to error message (#7074); - Update To handle if no data error (#7084); - Memory improvement when writing missing positions to pet (#7098); - added support for loading QUALapprox into VET (#7101); - Add -m flag to gsutil step; add dockstore branch filters to facilitate development (#7104); - updates to ImportGenomes and LoadBigQueryData (#7112); - Add ngs to cohort extract Dockerfile; remove exception catching in extract python script (#7113); - remove problematic storage_location imports (#7119); - Reduce memory and CPU for CreateImportTsvs task, check for files before attempting load (#7121); - add -m flag to gsutil mv step (#7129); - ah_var_store : Add sample file argument to cohort extract (#7117); - Perform full WGS cohort extract scientific tieout for 35 ACMG59 samples (#7106); - Enable Read/Execution Project for BQ Queries (#7136); - ah - optional service account (#7140); - Add load lock file to prevent accidental re-loading of data to BQ (#7138); - #251 Address gvcf no-calls missing QUALapprox and other features (#7146); - Job Add labels to BQ operations from GATK (Issues-199) (#7115); - parse map to list to avoid brackets and spaces in vcf output (#7168); - #259 Inline schema for importgenomes.wdl (#7171); - Created AvroFileReader and unittest, Update ExtractCohort and ExtractCohortEngine (#7174); - #224 Import WDL: handle ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:14776,Availability,reliab,reliability,14776,"ut allow NONE as input (#7206); - SA support and consistent naming for all GVS WDLs (#7205); - fix GvsExtractCallset inputs file (#7210); - add clustering to tables (#7207); - add vqsr cutoffs to GvsExtractCallset wdl; clean up dockstore yml (#7209); - Avro test (#7192); - Enable call caching of TSV generation in GvsImportGenomes (#7226); - 266 Clean up ExtractCohort -- remove query mode param (#7227); - 288 Add an excess alleles param (#7221); - take sample name as a param (#7236); - How to run GIAB comparisons (#7237); - Update GvsCreateFilterSet.wdl (#7239); - Use GatherVcfsCloud in GvsCreateFilterSet.wdl (#7241); - parameterize TTL with defaults, reduce memory allocation (#7244); - Addressing OOM in CohortExtract (#7245); - make outputs optional, change case in output (#7252); - Support for FORMAT/FT VQSLod Filtering and cohort-wide LowQual filter (#7248); - removed arrays code, renamed packages (#7260); - 279 labels (#7233); - add conda commands to GIAB readme (#7268); - remove gvs branch (#7263); - remove gvs branch (#7263); - upgrade bq libraries (#7264); - #299 - Sample list ease of use for cohort extracts (#7272); - check for duplicate ids (#7273); - Rc 274 passing sites only (#7275); - added default value to drop_state; broadinstitute/dsp-spec-ops#310 (#7278); - version bump for reliability (#7284); - add timestamp check to ExtractTask call https://github.com/broadinstitute/dsp-spec-ops/issues/320; - serial inserts for scaling prepare, factored out sample name (#7288); - Remove training sites only param from ExtractFeatures broadinstitute/dsp-spec-ops#261; - add param for mem for indels (#7282); - Ah prepare localize option (#7299); - Export sites only vcf STEP 1-- 317 add AC, AN, AF to the final VCF (#7279); - AoU GVS Cohort Extract wdl (#7242); - reliability (#7310); - bump to include FT tag filtering (#7316); - First pass at a Terra QuickStart (#7267); - Ah fix timestamp query (#7319); - 313 Cleanup Extract Cohort params (#7293); - bump bq storage versi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:15255,Availability,reliab,reliability,15255,"Addressing OOM in CohortExtract (#7245); - make outputs optional, change case in output (#7252); - Support for FORMAT/FT VQSLod Filtering and cohort-wide LowQual filter (#7248); - removed arrays code, renamed packages (#7260); - 279 labels (#7233); - add conda commands to GIAB readme (#7268); - remove gvs branch (#7263); - remove gvs branch (#7263); - upgrade bq libraries (#7264); - #299 - Sample list ease of use for cohort extracts (#7272); - check for duplicate ids (#7273); - Rc 274 passing sites only (#7275); - added default value to drop_state; broadinstitute/dsp-spec-ops#310 (#7278); - version bump for reliability (#7284); - add timestamp check to ExtractTask call https://github.com/broadinstitute/dsp-spec-ops/issues/320; - serial inserts for scaling prepare, factored out sample name (#7288); - Remove training sites only param from ExtractFeatures broadinstitute/dsp-spec-ops#261; - add param for mem for indels (#7282); - Ah prepare localize option (#7299); - Export sites only vcf STEP 1-- 317 add AC, AN, AF to the final VCF (#7279); - AoU GVS Cohort Extract wdl (#7242); - reliability (#7310); - bump to include FT tag filtering (#7316); - First pass at a Terra QuickStart (#7267); - Ah fix timestamp query (#7319); - 313 Cleanup Extract Cohort params (#7293); - bump bq storage version. See GVS-332 (#7330); - Variant Store extraction - Add VCF size to output (#7329); - add WARP-style scattering to SNPsVariantRecalibrator in GvsCreateFilterSet (#7320); - added ref ranges support (#7337); - 318 Sites only filtered vcf then annotate wdl (#7305); - Replace service_account_json (file) with service_account_json_path (string) to allow call-caching (#7347); - Parallelize create filterset by breaking out the 3 filter set file creation/loads into separate tasks (#7342); - Create WDL to validate VAT and add first test (#7352); - Add task for VAT validation #3 (#7360); - Add task for VAT validation #4 (#7363); - Instructions on how to download BQ Metadata and visualize results ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:16119,Availability,down,download,16119,"epare, factored out sample name (#7288); - Remove training sites only param from ExtractFeatures broadinstitute/dsp-spec-ops#261; - add param for mem for indels (#7282); - Ah prepare localize option (#7299); - Export sites only vcf STEP 1-- 317 add AC, AN, AF to the final VCF (#7279); - AoU GVS Cohort Extract wdl (#7242); - reliability (#7310); - bump to include FT tag filtering (#7316); - First pass at a Terra QuickStart (#7267); - Ah fix timestamp query (#7319); - 313 Cleanup Extract Cohort params (#7293); - bump bq storage version. See GVS-332 (#7330); - Variant Store extraction - Add VCF size to output (#7329); - add WARP-style scattering to SNPsVariantRecalibrator in GvsCreateFilterSet (#7320); - added ref ranges support (#7337); - 318 Sites only filtered vcf then annotate wdl (#7305); - Replace service_account_json (file) with service_account_json_path (string) to allow call-caching (#7347); - Parallelize create filterset by breaking out the 3 filter set file creation/loads into separate tasks (#7342); - Create WDL to validate VAT and add first test (#7352); - Add task for VAT validation #3 (#7360); - Add task for VAT validation #4 (#7363); - Instructions on how to download BQ Metadata and visualize results (#7359); - don't mix contigs, rightsize memory (#7361); - Add custom annotations as ac an af (#7351); - Add task for VAT validation #8 & 9 (#7364); - added bcftools, upgraded gcloud version (#7369); - fix wdl (#7378); - Update .dockstore.yml; - Add VAT validation rule #5 [VS-16] (#7365); - Add VAT validation rule #7 [VS-14] and validation rule #6 [VS-15] (#7379); - Batching of samples for create import TSVs (#7382); - Add VAT validation rule #2 [VS-19] (#7374); - Create VAT scripts directory (#7386); - fixing SA change from file to string (#7371); - add extract_subpop script (#7387); - Add is_loaded column to sample_info and logic to populate after ingest [VS-158] (#7389); - Add Gnomad subpopulation info into the VAT (#7381); - implement GVS ID assignment (#",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:19018,Availability,error,error,19018,"463); - formatting on sample QC README; - formatting change #2 to sample QC README; - address VS-152, remove extra headers from extract (#7466); - Update GvsExtractCallset.example.inputs.json (#7469); - Add ability to copy interval list files to gs directory [VS-191] (#7467); - add an expiration date to the temp tables (#7455); - fix the check for duplicates in import genomes (#7470); - added job ID to alt_allele population call output [VS-194] (#7473); - added steps and deliverables to GVS README [VS-181] (#7452); - Ah check the is loaded field in feature extract (#7475); - changes to put pet data directly into data table (#7478); - added override for ExtractTasks' preemptible value (#7477); - bcftools to the rescue (#7456); - execute_with_retry() refactor and error handling improvements [VS-159] (#7480); - Small updates to GvsExtractCallset from beta callset, new workflow for re-scattered shards (#7493); - add flag in prepare to print out sql instead of executing (#7501); - Workflow to re-scatter and then merge ""problematic"" intervals from ExtractCallset [VS-209] (#7495); - changed README to reflect comments from Lee [VS-210] (#7502); - Export the VAT into GCS (#7472); - addresses VS-219 (#7508); - small fix to MergeVCFs (#7517); - small fixes to GVS pipeline (#7522); - make sure ExtractTask is run on all interval files; - Revert ""make sure ExtractTask is run on all interval files""; - make sure ExtractTask is run on all interval files (#7527); - Remove Sites only step from the VAT creation WDL (#7510); - fix bad argument processing for bool (#7529); - Support for TDR DRS URIs in Import (#7528); - Match format of filename output in GvsRescatterCallsetInterval (#7539); - Reference block storage and query support (#7498); - update docs (#7540); - Kc fix rr load bug (#7550); - Update .dockstore.yml (#7553); - Ah add reblocking wdl (#7544); - Scatter over all interval files, not just scatter count (#7551); - fixed docker (#7558); - take advantage of fixed version of Spl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:21407,Availability,failure,failure,21407,"ntimes for balanced sharding (#7645); - Wire through GvsExtractCohortFromSampleNames with new prepare/extract [VS-283] (#7654); - Update GvsExtractCallset.wdl (#7678); - cherry pick lb_lfs_force change (#7683); - Tweak ingest messaging and failure mode [VS-267] (#7680); - Additional tweaks for GvsExtractCohortFromSampleNames [VS-283] (#7698); - VS-280 Create a VAT intermediary (#7657); - There something about split intervals [VS-306] (#7694); - VS 284 Add prepare step to Quick Start (#7685); - VS-222 dont hard code the dataset name! (#7704); - fixed bug; added tests (#7717); - Clean up optional and inconsistently named inputs [VS-294] [VS-218] (#7715); - VS-263 notes on ingest and beyond (#7618); - Add task to ExtractCallset that verifies filter_set_name exists in GVS dataset [VS-335] (#7734); - Clean up input json files to reflect changes inputs [VS-337] (#7733); - used constants; implemented non-AS transformation (#7718); - Pass dataset name to gatk ExtractFeatures (#7735); - Add withdrawn and is_control columns [VS-70] [VS-213] (#7736); - Allow interval lists that require the SA to see (#7743); - allow for gatk to be overridden, update with known good jar (#7758); - VS-361 Add GvsWithdrawSamples wdl (#7765); - Extract Performance Improvements (#7686); - Don't put withdrawn sample data in alt_allele table [VS-369] (#7762); - remove PET code (#7768); - Adding AD for scale testing VS 225 add AD (#7713); - Deterministic Sample ID assignments [VS-371] (#7770); - remove R scripts from filtering (#7781); - Remove an old ""temp table"" dataset (#7780); - Clean up LocalizeFile [VS-314] (#7771); - Remove pet code from CreateVariantIngestFiles and friends [VS-375] (#7773); - 317 remove excess header values in VCF extract (#7786); - correct auth in split intervals (#7790); - Add code to (optionally) zero pad the vcf filename. (#7783); - LoadData `maxRetries` parameterized, default increased [VS-383] (#7791); - Update to latest version of ah_var_store gatk override jar (#7793); ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:24019,Availability,Reliab,Reliability,24019,"89); - Pinned typing_extensions python package to 4.1.1 to fix conda environment. (#7802); - WeightedSplitInterval fixes [VS-384] [VS-332] (#7795); - Replace Travis with GithubActions (#7754); - Docker build only lfs pulls main/src/resources/large (#7727); - Clean up gatk jars -- looks like we are not passing them properly in the extract (#7788); - Fix typo that broke git lfs pull (#7806); - Document AoU SOP (up to the VAT) [VS-63] (#7807); - Incident VS 365 clinvar classification fix (#7769); - VS-390. Add precision and sensitivity wdl (#7813); - Quickstart based integration test [VS-357] (#7812); - 365 vat python testing additions (#7756); - VS 396 clinvar grabs too many values (#7823); - Added a test to validate WDLs in the scripts directory. (#7826) (#7829); - VAT Performance / Reliability Improvements (#7828); - VAT naming conventions [VS-410] (#7827); - Rc remove ad from vat (#7832); - bugfix, we were trying to grep a binary file (#7837); - Cleanup scripts/variantstore [VS-414] (#7834); - Merge VAT TSV files into single bgzipped file [VS-304] (#7848); - Handle fully and partially loaded samples [VS-262] [VS-258] (#7843); - Ingest Error Handling Fixes [VS-261] (#7841); - First cut at a python notebook to validate inputs. (#7845); - Compute filter scatter [VS-392] (#7852); - remove withdrawn req (#7844); - Improve import error message [VS-437] (#7855); - Fix Input Validation python notebook (#7853); - Add VAT Validation check that aa_change and exon_number are consistently set. (#7850); - Ingest 10K [VS-344] (#7860); - X/Y chromosome reweighting for better extract shard runtime balance [VS-389] (#7868); - VET Ingest Validation / Allow Ingest of non-VQSR'ed data (#7870); - Fix AoU workflow bugs (#7874); - Curate input arrays to skip already ingested sample data [VS-246] (#7862); - KM upload GVS product sheet (#7883); - Default extract scatter width [VS-415] (#7878); - Volatile tasks review [VS-447] (#7880); - Update Quickstart Integration for X/Y scaling changes ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:24380,Availability,Error,Error,24380,"89); - Pinned typing_extensions python package to 4.1.1 to fix conda environment. (#7802); - WeightedSplitInterval fixes [VS-384] [VS-332] (#7795); - Replace Travis with GithubActions (#7754); - Docker build only lfs pulls main/src/resources/large (#7727); - Clean up gatk jars -- looks like we are not passing them properly in the extract (#7788); - Fix typo that broke git lfs pull (#7806); - Document AoU SOP (up to the VAT) [VS-63] (#7807); - Incident VS 365 clinvar classification fix (#7769); - VS-390. Add precision and sensitivity wdl (#7813); - Quickstart based integration test [VS-357] (#7812); - 365 vat python testing additions (#7756); - VS 396 clinvar grabs too many values (#7823); - Added a test to validate WDLs in the scripts directory. (#7826) (#7829); - VAT Performance / Reliability Improvements (#7828); - VAT naming conventions [VS-410] (#7827); - Rc remove ad from vat (#7832); - bugfix, we were trying to grep a binary file (#7837); - Cleanup scripts/variantstore [VS-414] (#7834); - Merge VAT TSV files into single bgzipped file [VS-304] (#7848); - Handle fully and partially loaded samples [VS-262] [VS-258] (#7843); - Ingest Error Handling Fixes [VS-261] (#7841); - First cut at a python notebook to validate inputs. (#7845); - Compute filter scatter [VS-392] (#7852); - remove withdrawn req (#7844); - Improve import error message [VS-437] (#7855); - Fix Input Validation python notebook (#7853); - Add VAT Validation check that aa_change and exon_number are consistently set. (#7850); - Ingest 10K [VS-344] (#7860); - X/Y chromosome reweighting for better extract shard runtime balance [VS-389] (#7868); - VET Ingest Validation / Allow Ingest of non-VQSR'ed data (#7870); - Fix AoU workflow bugs (#7874); - Curate input arrays to skip already ingested sample data [VS-246] (#7862); - KM upload GVS product sheet (#7883); - Default extract scatter width [VS-415] (#7878); - Volatile tasks review [VS-447] (#7880); - Update Quickstart Integration for X/Y scaling changes ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:24573,Availability,error,error,24573,"lfs pull (#7806); - Document AoU SOP (up to the VAT) [VS-63] (#7807); - Incident VS 365 clinvar classification fix (#7769); - VS-390. Add precision and sensitivity wdl (#7813); - Quickstart based integration test [VS-357] (#7812); - 365 vat python testing additions (#7756); - VS 396 clinvar grabs too many values (#7823); - Added a test to validate WDLs in the scripts directory. (#7826) (#7829); - VAT Performance / Reliability Improvements (#7828); - VAT naming conventions [VS-410] (#7827); - Rc remove ad from vat (#7832); - bugfix, we were trying to grep a binary file (#7837); - Cleanup scripts/variantstore [VS-414] (#7834); - Merge VAT TSV files into single bgzipped file [VS-304] (#7848); - Handle fully and partially loaded samples [VS-262] [VS-258] (#7843); - Ingest Error Handling Fixes [VS-261] (#7841); - First cut at a python notebook to validate inputs. (#7845); - Compute filter scatter [VS-392] (#7852); - remove withdrawn req (#7844); - Improve import error message [VS-437] (#7855); - Fix Input Validation python notebook (#7853); - Add VAT Validation check that aa_change and exon_number are consistently set. (#7850); - Ingest 10K [VS-344] (#7860); - X/Y chromosome reweighting for better extract shard runtime balance [VS-389] (#7868); - VET Ingest Validation / Allow Ingest of non-VQSR'ed data (#7870); - Fix AoU workflow bugs (#7874); - Curate input arrays to skip already ingested sample data [VS-246] (#7862); - KM upload GVS product sheet (#7883); - Default extract scatter width [VS-415] (#7878); - Volatile tasks review [VS-447] (#7880); - Update Quickstart Integration for X/Y scaling changes [VS-464] (#7881); - clean up dockstore; - Rc vs 63 vat sop documentation (#7879); - Fix up FQ and race condition issues with volatile tasks work [VS-478] (#7888); - Use gvs-internal project in integration test (#7901); - Add cost observability BQ table [VS-441] (#7891); - Add preliminary labels to queries [VS-381] (#7902); - Workflow compute costs [VS-472] (#7905); - Fix bu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:26303,Availability,error,errors,26303," - Fix up FQ and race condition issues with volatile tasks work [VS-478] (#7888); - Use gvs-internal project in integration test (#7901); - Add cost observability BQ table [VS-441] (#7891); - Add preliminary labels to queries [VS-381] (#7902); - Workflow compute costs [VS-472] (#7905); - Fix bug and update images (#7912); - VS 483 Beta user wdl (#7894); - Core storage model cost [VS-473] (#7913); - Update Quickstart & Integration to use re-blocked v2 gVCFs [VS-491] (#7924); - KM GVS documentation (#7903); - Track BigQuery costs of GVS python VS-480 (#7915); - Read cost observability table [VS-475] (#7923); - Fix Race Condition, Add Support for Extract by Array of Sample Names (ie from a Sample Set) (#7917); - Rightsize import batches [VS-486] (#7925); - [AoU DRC] Support uppercase site_ids for reblocking (#7929); - Populate cost metadata for GATK tasks. (#7919); - remove accidentally added input (#7931); - VS_492 - Beta User Jar release (#7934); - Cost WDL should throw on FISS API errors [VS-518] (#7942); - Fix bad check for missing workflow name [VS-520] (#7943); - Remove usage of service account from GvsValidateVAT.wdl (#7937); - refactoring for testablity (#7946); - More import retries [VS-532] (#7953); - A few last doc changes (#7927); - WDL to extract a single callset cost (BQ only, not Terra) (#7940); - Temporarily swap in Corretto for Temurin as we can't download Temurin. (#7969); - GL-548 - Update CreateVat code to handle samples that do not contain all population groups. (#7965); - Restore Temurin 11 [VS-570] (#7972); - Add table size check to quickstart integration test [VS-501] (#7970); - Consolidate various docs for AoU callset generation into one to rule them all [VS-553] (#7971); - VS-567. Removing usage of ServiceAccount from CreateVat related WDLs (#7974); - WDL to extract Avro files for Hail import [VS-579] (#7981); - Removed usage of service account from WDLs (#7985); - Document steps for GVS cleanup for base use case [VS-586] (#7989); - Change bac",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:26691,Availability,down,download,26691," [VS-472] (#7905); - Fix bug and update images (#7912); - VS 483 Beta user wdl (#7894); - Core storage model cost [VS-473] (#7913); - Update Quickstart & Integration to use re-blocked v2 gVCFs [VS-491] (#7924); - KM GVS documentation (#7903); - Track BigQuery costs of GVS python VS-480 (#7915); - Read cost observability table [VS-475] (#7923); - Fix Race Condition, Add Support for Extract by Array of Sample Names (ie from a Sample Set) (#7917); - Rightsize import batches [VS-486] (#7925); - [AoU DRC] Support uppercase site_ids for reblocking (#7929); - Populate cost metadata for GATK tasks. (#7919); - remove accidentally added input (#7931); - VS_492 - Beta User Jar release (#7934); - Cost WDL should throw on FISS API errors [VS-518] (#7942); - Fix bad check for missing workflow name [VS-520] (#7943); - Remove usage of service account from GvsValidateVAT.wdl (#7937); - refactoring for testablity (#7946); - More import retries [VS-532] (#7953); - A few last doc changes (#7927); - WDL to extract a single callset cost (BQ only, not Terra) (#7940); - Temporarily swap in Corretto for Temurin as we can't download Temurin. (#7969); - GL-548 - Update CreateVat code to handle samples that do not contain all population groups. (#7965); - Restore Temurin 11 [VS-570] (#7972); - Add table size check to quickstart integration test [VS-501] (#7970); - Consolidate various docs for AoU callset generation into one to rule them all [VS-553] (#7971); - VS-567. Removing usage of ServiceAccount from CreateVat related WDLs (#7974); - WDL to extract Avro files for Hail import [VS-579] (#7981); - Removed usage of service account from WDLs (#7985); - Document steps for GVS cleanup for base use case [VS-586] (#7989); - Change backticks to single quotes in several error messages - causing shell to attempt to execute. (#7995); - VS-598 - Minor update to AoU Documentation. (#7994); - Allow for incremental addition of data to alt_allele [VS-52] (#7993); - Minor AoU Documentation Update (#7999); -",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:27342,Availability,error,error,27342,"#7931); - VS_492 - Beta User Jar release (#7934); - Cost WDL should throw on FISS API errors [VS-518] (#7942); - Fix bad check for missing workflow name [VS-520] (#7943); - Remove usage of service account from GvsValidateVAT.wdl (#7937); - refactoring for testablity (#7946); - More import retries [VS-532] (#7953); - A few last doc changes (#7927); - WDL to extract a single callset cost (BQ only, not Terra) (#7940); - Temporarily swap in Corretto for Temurin as we can't download Temurin. (#7969); - GL-548 - Update CreateVat code to handle samples that do not contain all population groups. (#7965); - Restore Temurin 11 [VS-570] (#7972); - Add table size check to quickstart integration test [VS-501] (#7970); - Consolidate various docs for AoU callset generation into one to rule them all [VS-553] (#7971); - VS-567. Removing usage of ServiceAccount from CreateVat related WDLs (#7974); - WDL to extract Avro files for Hail import [VS-579] (#7981); - Removed usage of service account from WDLs (#7985); - Document steps for GVS cleanup for base use case [VS-586] (#7989); - Change backticks to single quotes in several error messages - causing shell to attempt to execute. (#7995); - VS-598 - Minor update to AoU Documentation. (#7994); - Allow for incremental addition of data to alt_allele [VS-52] (#7993); - Minor AoU Documentation Update (#7999); - Batch population of alt_allele table from vet_ tables [VS-265] (#7998); - Change drop_state to NONE for Ingest/Extract [VS-607] (#8000); - python -> python3 (#8001); - Generate Hail import/export script [VS-605] (#8002); - clearer error when values are missing (#7939); - Ah [VS-565] output intervals and sample list (#8010); - make CreateAltAlleleTable task volatile (#8011); - Restore withdrawn [VS-581] (#8006); - Km gvs add storage cost and cleanup doc (#8012); - Updating documentation to reflect the changed outputs [VS-565] (#8014); - File of callset samples -> samples marked as 'withdrawn' in GVS [VS-436] (#8009); - fix quota guide",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:27807,Availability,error,error,27807,ge of ServiceAccount from CreateVat related WDLs (#7974); - WDL to extract Avro files for Hail import [VS-579] (#7981); - Removed usage of service account from WDLs (#7985); - Document steps for GVS cleanup for base use case [VS-586] (#7989); - Change backticks to single quotes in several error messages - causing shell to attempt to execute. (#7995); - VS-598 - Minor update to AoU Documentation. (#7994); - Allow for incremental addition of data to alt_allele [VS-52] (#7993); - Minor AoU Documentation Update (#7999); - Batch population of alt_allele table from vet_ tables [VS-265] (#7998); - Change drop_state to NONE for Ingest/Extract [VS-607] (#8000); - python -> python3 (#8001); - Generate Hail import/export script [VS-605] (#8002); - clearer error when values are missing (#7939); - Ah [VS-565] output intervals and sample list (#8010); - make CreateAltAlleleTable task volatile (#8011); - Restore withdrawn [VS-581] (#8006); - Km gvs add storage cost and cleanup doc (#8012); - Updating documentation to reflect the changed outputs [VS-565] (#8014); - File of callset samples -> samples marked as 'withdrawn' in GVS [VS-436] (#8009); - fix quota guidelines for CPUs (#8016); - Add in ability to tweak sample-every-Nth-variant parameter for SNP model creation (#8019); - add initial notebook copy pasta (#8008); - add sample_table_timestamp to GetNumSamplesLoaded (#8022); - Batched Avro export [VS-630] (#8020); - Updating references to old GATK for VS-620 (#8023); - VS-517 Use standard version of GetBQTableLastModifiedDatetime in GvsValidateVat (#8024); - Fix bug in GvsWithdrawSamples.wdl (#8026); - Ah 617 exposing the drop_state parameter to the GvsJointVariantCalling wdl used for beta (and internal customer) (#8032); - Expose maximum-training-variants VQSR parameter [VS-634] (#8029); - Callset statistics [VS-560] (#8018); - Check for withdrawn before exporting to AVRO files [VS-646] (#8039); - Small updates to GVS Integration WDL [VS-618] (#8042); - Rework Hail script gener,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:29252,Availability,failure,failure,29252,"ility to tweak sample-every-Nth-variant parameter for SNP model creation (#8019); - add initial notebook copy pasta (#8008); - add sample_table_timestamp to GetNumSamplesLoaded (#8022); - Batched Avro export [VS-630] (#8020); - Updating references to old GATK for VS-620 (#8023); - VS-517 Use standard version of GetBQTableLastModifiedDatetime in GvsValidateVat (#8024); - Fix bug in GvsWithdrawSamples.wdl (#8026); - Ah 617 exposing the drop_state parameter to the GvsJointVariantCalling wdl used for beta (and internal customer) (#8032); - Expose maximum-training-variants VQSR parameter [VS-634] (#8029); - Callset statistics [VS-560] (#8018); - Check for withdrawn before exporting to AVRO files [VS-646] (#8039); - Small updates to GVS Integration WDL [VS-618] (#8042); - Rework Hail script generation [VS-616] (#8034); - Alpine based Variant Store Docker image [VS-648] (#8044); - update warp version (#7906); - Fail Avro extract and callset stats on bad filter name [VS-655] (#8046); - Vs 629 failure to retrieve job information during ingest (#8047); - Restore accidentally removed bcftools [VS-661] (#8051); - Allowing our pipeline to function with a sample size of one (#8055); - Vs 665 re create vcf for cd 68 po 52339 with ad padding fixed (#8057); - VS-665 and VS-620 updating code to use latest docker images containing Rori's AD calculation changes in extract (#8061); - updating the beta workflow to use the latest jar, representing the version of GATK George tested against the workflow (#8062); - VS-637 Address a couple of issues in SampleLoadStatus handling in GVSImportGenomes. (#8052); - Revert Alpinizing of apt dependent task [VS-688] (#8065); - Fix missing vat schema JSONs [VS-699] (#8072); - Fix integration expectations for fixed AD [VS-689] (#8066); - VS-698 Remove unnecessary columns from Call set statistics (#8073); - Fix Dockerfile nits that break 20.10.21 (#8078); - Nirvana 3.18.1 Docker images support [VS-661] (#8082); - Add option to not prepare __REF_DATA or __",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:656,Deployability,update,update,656,,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:899,Deployability,update,update,899,,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:1137,Deployability,upgrade,upgraded,1137,"ory, uses location instead of position; - add query mode; - fix contig name; - forgot this file; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - add fields for uncompressed imputed data; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - Tool for arrays QC metrics calculations (#6812); - ah update array extract tool (#6827); - fix enum (#6834); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - resolved rebase conflicts; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for ac",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:1400,Deployability,update,update,1400," major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - add fields for uncompressed imputed data; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - Tool for arrays QC metrics calculations (#6812); - ah update array extract tool (#6827); - fix enum (#6834); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - resolved rebase conflicts; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:2302,Deployability,update,update,2302,"); - ah - use new GT encoding (#6822); - Tool for arrays QC metrics calculations (#6812); - ah update array extract tool (#6827); - fix enum (#6834); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - resolved rebase conflicts; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - Moving the WDL for importing array manifest to BQ (#6860); - fix up after rebase; - Moving and testing ingest scripts from variantstore (#6881); - optionally provide sam",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:2545,Deployability,update,update,2545,"ariants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - resolved rebase conflicts; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - Moving the WDL for importing array manifest to BQ (#6860); - fix up after rebase; - Moving and testing ingest scripts from variantstore (#6881); - optionally provide sample-map-file instead of sample-map-table (#6872); - Moving extract wdls from variantstore repo (#6902); - update for genomes (#6918); - update paths; - update field name; - consolidate exome and genome code; - missing comma; - allow null for d",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:2739,Deployability,upgrade,upgraded,2739,"sed on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - resolved rebase conflicts; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - Moving the WDL for importing array manifest to BQ (#6860); - fix up after rebase; - Moving and testing ingest scripts from variantstore (#6881); - optionally provide sample-map-file instead of sample-map-table (#6872); - Moving extract wdls from variantstore repo (#6902); - update for genomes (#6918); - update paths; - update field name; - consolidate exome and genome code; - missing comma; - allow null for drop state.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:3412,Deployability,update,update,3412,"sed on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - resolved rebase conflicts; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - Moving the WDL for importing array manifest to BQ (#6860); - fix up after rebase; - Moving and testing ingest scripts from variantstore (#6881); - optionally provide sample-map-file instead of sample-map-table (#6872); - Moving extract wdls from variantstore repo (#6902); - update for genomes (#6918); - update paths; - update field name; - consolidate exome and genome code; - missing comma; - allow null for drop state.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:3442,Deployability,update,update,3442,"sed on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - resolved rebase conflicts; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - Moving the WDL for importing array manifest to BQ (#6860); - fix up after rebase; - Moving and testing ingest scripts from variantstore (#6881); - optionally provide sample-map-file instead of sample-map-table (#6872); - Moving extract wdls from variantstore repo (#6902); - update for genomes (#6918); - update paths; - update field name; - consolidate exome and genome code; - missing comma; - allow null for drop state.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:3458,Deployability,update,update,3458,"sed on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - resolved rebase conflicts; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - Moving the WDL for importing array manifest to BQ (#6860); - fix up after rebase; - Moving and testing ingest scripts from variantstore (#6881); - optionally provide sample-map-file instead of sample-map-table (#6872); - Moving extract wdls from variantstore repo (#6902); - update for genomes (#6918); - update paths; - update field name; - consolidate exome and genome code; - missing comma; - allow null for drop state.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:4710,Deployability,update,updated,4710,"e-map-file instead of sample-map-table (#6872); - Moving extract wdls from variantstore repo (#6902); - update for genomes (#6918); - update paths; - update field name; - consolidate exome and genome code; - missing comma; - allow null for drop state. add to wdl; - fix when NON-REF has value (drop it anyway). add memory to wdl; - fix order of columns in vet; - Making ingest work on gvcfs without allele specific annotations (#6934); - fix partitioning (#6939); - more fixes for non-allele-specific gvcfs (#6945); - fix GT for ./.; - add in feature extract code. resulting vcf still has bugs (#6947); - Rudimentary support for other output types, support for scattering extract cohort (#6949); - added batch/interactive flag, removed samples (#6953); - WIP for Feature Extract code (#6958); - move wdl and cromwell_tests dirs under variantstore (#6961); - WDL to run feature extract, VQSR, and upload (#6966); - support for multiple PET/VETs (#6969); - Add filtering to ExtractCohort (#6971); - support for non-AS called data (#6975); - fixing bug in filtering (#6976); - Updating extract wdls to apply filtering (#6977); - moving from specops repo (#6983); - optimized TSV experiment, and range GQ dropping (#6987); - back to 30x defaults; - allow no filtering to be applied (#7004); - Separate bigquery table creation and data loading in LoadData (#7056); - WIP; - tieout scripts; - notes files; - updated diff scripts; - fixed bug...; - add wdl and inputs file for warp pipeline; - reverting logging; - included top level WDL; - use gnarly with BQ extract cohort; - remove unused file; - cleaning up; - tidy; - tidy up before PR; - tidy up before PR; - PR comments; - merge conflict misfires; - added example SQL to create alt allele table from VET; - option to remove PLs; - fixed and enhanced unit test; - removing unused config, causing travis to fail; - add CreateVariantIngestFiles integration test (#7071); - add sampleName (instead of NULL) to error message (#7074); - Update To handle if",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:4783,Deployability,pipeline,pipeline,4783,,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:5200,Deployability,integrat,integration,5200,,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:5289,Deployability,Update,Update,5289,,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:5553,Deployability,update,updates,5553,,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:6628,Deployability,update,update,6628,"ract Dockerfile; remove exception catching in extract python script (#7113); - remove problematic storage_location imports (#7119); - Reduce memory and CPU for CreateImportTsvs task, check for files before attempting load (#7121); - add -m flag to gsutil mv step (#7129); - ah_var_store : Add sample file argument to cohort extract (#7117); - wip; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - forgot this file; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - add fields for uncompressed imputed data; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - Tool for arrays QC metrics calculations (#6812); - ah update array extract tool (#6827); - fix enum (#6834); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - resolved rebas",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:6871,Deployability,update,update,6871,"g to gsutil mv step (#7129); - ah_var_store : Add sample file argument to cohort extract (#7117); - wip; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - forgot this file; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - add fields for uncompressed imputed data; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - Tool for arrays QC metrics calculations (#6812); - ah update array extract tool (#6827); - fix enum (#6834); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - resolved rebase conflicts; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - fix location bug;",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:7109,Deployability,upgrade,upgraded,7109,"ory, uses location instead of position; - add query mode; - fix contig name; - forgot this file; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - add fields for uncompressed imputed data; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - Tool for arrays QC metrics calculations (#6812); - ah update array extract tool (#6827); - fix enum (#6834); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - resolved rebase conflicts; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for ac",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:7372,Deployability,update,update,7372," major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - add fields for uncompressed imputed data; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - Tool for arrays QC metrics calculations (#6812); - ah update array extract tool (#6827); - fix enum (#6834); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - resolved rebase conflicts; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:8274,Deployability,update,update,8274,"); - ah - use new GT encoding (#6822); - Tool for arrays QC metrics calculations (#6812); - ah update array extract tool (#6827); - fix enum (#6834); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - resolved rebase conflicts; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - Moving the WDL for importing array manifest to BQ (#6860); - fix up after rebase; - Moving and testing ingest scripts from variantstore (#6881); - optionally provide sam",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:8517,Deployability,update,update,8517,"ariants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - resolved rebase conflicts; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - Moving the WDL for importing array manifest to BQ (#6860); - fix up after rebase; - Moving and testing ingest scripts from variantstore (#6881); - optionally provide sample-map-file instead of sample-map-table (#6872); - Moving extract wdls from variantstore repo (#6902); - update for genomes (#6918); - update paths; - update field name; - consolidate exome and genome code; - missing comma; - allow null for d",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:8711,Deployability,upgrade,upgraded,8711,"sed on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - resolved rebase conflicts; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - Moving the WDL for importing array manifest to BQ (#6860); - fix up after rebase; - Moving and testing ingest scripts from variantstore (#6881); - optionally provide sample-map-file instead of sample-map-table (#6872); - Moving extract wdls from variantstore repo (#6902); - update for genomes (#6918); - update paths; - update field name; - consolidate exome and genome code; - missing comma; - allow null for drop state.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:9384,Deployability,update,update,9384,"sed on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - resolved rebase conflicts; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - Moving the WDL for importing array manifest to BQ (#6860); - fix up after rebase; - Moving and testing ingest scripts from variantstore (#6881); - optionally provide sample-map-file instead of sample-map-table (#6872); - Moving extract wdls from variantstore repo (#6902); - update for genomes (#6918); - update paths; - update field name; - consolidate exome and genome code; - missing comma; - allow null for drop state.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:9414,Deployability,update,update,9414,"sed on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - resolved rebase conflicts; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - Moving the WDL for importing array manifest to BQ (#6860); - fix up after rebase; - Moving and testing ingest scripts from variantstore (#6881); - optionally provide sample-map-file instead of sample-map-table (#6872); - Moving extract wdls from variantstore repo (#6902); - update for genomes (#6918); - update paths; - update field name; - consolidate exome and genome code; - missing comma; - allow null for drop state.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:9430,Deployability,update,update,9430,"sed on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - resolved rebase conflicts; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - Moving the WDL for importing array manifest to BQ (#6860); - fix up after rebase; - Moving and testing ingest scripts from variantstore (#6881); - optionally provide sample-map-file instead of sample-map-table (#6872); - Moving extract wdls from variantstore repo (#6902); - update for genomes (#6918); - update paths; - update field name; - consolidate exome and genome code; - missing comma; - allow null for drop state.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:10682,Deployability,update,updated,10682,"e-map-file instead of sample-map-table (#6872); - Moving extract wdls from variantstore repo (#6902); - update for genomes (#6918); - update paths; - update field name; - consolidate exome and genome code; - missing comma; - allow null for drop state. add to wdl; - fix when NON-REF has value (drop it anyway). add memory to wdl; - fix order of columns in vet; - Making ingest work on gvcfs without allele specific annotations (#6934); - fix partitioning (#6939); - more fixes for non-allele-specific gvcfs (#6945); - fix GT for ./.; - add in feature extract code. resulting vcf still has bugs (#6947); - Rudimentary support for other output types, support for scattering extract cohort (#6949); - added batch/interactive flag, removed samples (#6953); - WIP for Feature Extract code (#6958); - move wdl and cromwell_tests dirs under variantstore (#6961); - WDL to run feature extract, VQSR, and upload (#6966); - support for multiple PET/VETs (#6969); - Add filtering to ExtractCohort (#6971); - support for non-AS called data (#6975); - fixing bug in filtering (#6976); - Updating extract wdls to apply filtering (#6977); - moving from specops repo (#6983); - optimized TSV experiment, and range GQ dropping (#6987); - back to 30x defaults; - allow no filtering to be applied (#7004); - Separate bigquery table creation and data loading in LoadData (#7056); - WIP; - tieout scripts; - notes files; - updated diff scripts; - fixed bug...; - add wdl and inputs file for warp pipeline; - reverting logging; - included top level WDL; - use gnarly with BQ extract cohort; - remove unused file; - cleaning up; - tidy; - tidy up before PR; - tidy up before PR; - PR comments; - merge conflict misfires; - added example SQL to create alt allele table from VET; - option to remove PLs; - fixed and enhanced unit test; - removing unused config, causing travis to fail; - add CreateVariantIngestFiles integration test (#7071); - add sampleName (instead of NULL) to error message (#7074); - Update To handle if",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:10755,Deployability,pipeline,pipeline,10755," table creation and data loading in LoadData (#7056); - WIP; - tieout scripts; - notes files; - updated diff scripts; - fixed bug...; - add wdl and inputs file for warp pipeline; - reverting logging; - included top level WDL; - use gnarly with BQ extract cohort; - remove unused file; - cleaning up; - tidy; - tidy up before PR; - tidy up before PR; - PR comments; - merge conflict misfires; - added example SQL to create alt allele table from VET; - option to remove PLs; - fixed and enhanced unit test; - removing unused config, causing travis to fail; - add CreateVariantIngestFiles integration test (#7071); - add sampleName (instead of NULL) to error message (#7074); - Update To handle if no data error (#7084); - Memory improvement when writing missing positions to pet (#7098); - added support for loading QUALapprox into VET (#7101); - Add -m flag to gsutil step; add dockstore branch filters to facilitate development (#7104); - updates to ImportGenomes and LoadBigQueryData (#7112); - Add ngs to cohort extract Dockerfile; remove exception catching in extract python script (#7113); - remove problematic storage_location imports (#7119); - Reduce memory and CPU for CreateImportTsvs task, check for files before attempting load (#7121); - add -m flag to gsutil mv step (#7129); - ah_var_store : Add sample file argument to cohort extract (#7117); - Perform full WGS cohort extract scientific tieout for 35 ACMG59 samples (#7106); - Enable Read/Execution Project for BQ Queries (#7136); - ah - optional service account (#7140); - Add load lock file to prevent accidental re-loading of data to BQ (#7138); - #251 Address gvcf no-calls missing QUALapprox and other features (#7146); - Job Add labels to BQ operations from GATK (Issues-199) (#7115); - parse map to list to avoid brackets and spaces in vcf output (#7168); - #259 Inline schema for importgenomes.wdl (#7171); - Created AvroFileReader and unittest, Update ExtractCohort and ExtractCohortEngine (#7174); - #224 Import WDL: handle ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:11172,Deployability,integrat,integration,11172," table creation and data loading in LoadData (#7056); - WIP; - tieout scripts; - notes files; - updated diff scripts; - fixed bug...; - add wdl and inputs file for warp pipeline; - reverting logging; - included top level WDL; - use gnarly with BQ extract cohort; - remove unused file; - cleaning up; - tidy; - tidy up before PR; - tidy up before PR; - PR comments; - merge conflict misfires; - added example SQL to create alt allele table from VET; - option to remove PLs; - fixed and enhanced unit test; - removing unused config, causing travis to fail; - add CreateVariantIngestFiles integration test (#7071); - add sampleName (instead of NULL) to error message (#7074); - Update To handle if no data error (#7084); - Memory improvement when writing missing positions to pet (#7098); - added support for loading QUALapprox into VET (#7101); - Add -m flag to gsutil step; add dockstore branch filters to facilitate development (#7104); - updates to ImportGenomes and LoadBigQueryData (#7112); - Add ngs to cohort extract Dockerfile; remove exception catching in extract python script (#7113); - remove problematic storage_location imports (#7119); - Reduce memory and CPU for CreateImportTsvs task, check for files before attempting load (#7121); - add -m flag to gsutil mv step (#7129); - ah_var_store : Add sample file argument to cohort extract (#7117); - Perform full WGS cohort extract scientific tieout for 35 ACMG59 samples (#7106); - Enable Read/Execution Project for BQ Queries (#7136); - ah - optional service account (#7140); - Add load lock file to prevent accidental re-loading of data to BQ (#7138); - #251 Address gvcf no-calls missing QUALapprox and other features (#7146); - Job Add labels to BQ operations from GATK (Issues-199) (#7115); - parse map to list to avoid brackets and spaces in vcf output (#7168); - #259 Inline schema for importgenomes.wdl (#7171); - Created AvroFileReader and unittest, Update ExtractCohort and ExtractCohortEngine (#7174); - #224 Import WDL: handle ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:11261,Deployability,Update,Update,11261," table creation and data loading in LoadData (#7056); - WIP; - tieout scripts; - notes files; - updated diff scripts; - fixed bug...; - add wdl and inputs file for warp pipeline; - reverting logging; - included top level WDL; - use gnarly with BQ extract cohort; - remove unused file; - cleaning up; - tidy; - tidy up before PR; - tidy up before PR; - PR comments; - merge conflict misfires; - added example SQL to create alt allele table from VET; - option to remove PLs; - fixed and enhanced unit test; - removing unused config, causing travis to fail; - add CreateVariantIngestFiles integration test (#7071); - add sampleName (instead of NULL) to error message (#7074); - Update To handle if no data error (#7084); - Memory improvement when writing missing positions to pet (#7098); - added support for loading QUALapprox into VET (#7101); - Add -m flag to gsutil step; add dockstore branch filters to facilitate development (#7104); - updates to ImportGenomes and LoadBigQueryData (#7112); - Add ngs to cohort extract Dockerfile; remove exception catching in extract python script (#7113); - remove problematic storage_location imports (#7119); - Reduce memory and CPU for CreateImportTsvs task, check for files before attempting load (#7121); - add -m flag to gsutil mv step (#7129); - ah_var_store : Add sample file argument to cohort extract (#7117); - Perform full WGS cohort extract scientific tieout for 35 ACMG59 samples (#7106); - Enable Read/Execution Project for BQ Queries (#7136); - ah - optional service account (#7140); - Add load lock file to prevent accidental re-loading of data to BQ (#7138); - #251 Address gvcf no-calls missing QUALapprox and other features (#7146); - Job Add labels to BQ operations from GATK (Issues-199) (#7115); - parse map to list to avoid brackets and spaces in vcf output (#7168); - #259 Inline schema for importgenomes.wdl (#7171); - Created AvroFileReader and unittest, Update ExtractCohort and ExtractCohortEngine (#7174); - #224 Import WDL: handle ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:11525,Deployability,update,updates,11525," table creation and data loading in LoadData (#7056); - WIP; - tieout scripts; - notes files; - updated diff scripts; - fixed bug...; - add wdl and inputs file for warp pipeline; - reverting logging; - included top level WDL; - use gnarly with BQ extract cohort; - remove unused file; - cleaning up; - tidy; - tidy up before PR; - tidy up before PR; - PR comments; - merge conflict misfires; - added example SQL to create alt allele table from VET; - option to remove PLs; - fixed and enhanced unit test; - removing unused config, causing travis to fail; - add CreateVariantIngestFiles integration test (#7071); - add sampleName (instead of NULL) to error message (#7074); - Update To handle if no data error (#7084); - Memory improvement when writing missing positions to pet (#7098); - added support for loading QUALapprox into VET (#7101); - Add -m flag to gsutil step; add dockstore branch filters to facilitate development (#7104); - updates to ImportGenomes and LoadBigQueryData (#7112); - Add ngs to cohort extract Dockerfile; remove exception catching in extract python script (#7113); - remove problematic storage_location imports (#7119); - Reduce memory and CPU for CreateImportTsvs task, check for files before attempting load (#7121); - add -m flag to gsutil mv step (#7129); - ah_var_store : Add sample file argument to cohort extract (#7117); - Perform full WGS cohort extract scientific tieout for 35 ACMG59 samples (#7106); - Enable Read/Execution Project for BQ Queries (#7136); - ah - optional service account (#7140); - Add load lock file to prevent accidental re-loading of data to BQ (#7138); - #251 Address gvcf no-calls missing QUALapprox and other features (#7146); - Job Add labels to BQ operations from GATK (Issues-199) (#7115); - parse map to list to avoid brackets and spaces in vcf output (#7168); - #259 Inline schema for importgenomes.wdl (#7171); - Created AvroFileReader and unittest, Update ExtractCohort and ExtractCohortEngine (#7174); - #224 Import WDL: handle ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:12506,Deployability,Update,Update,12506," QUALapprox and other features (#7146); - Job Add labels to BQ operations from GATK (Issues-199) (#7115); - parse map to list to avoid brackets and spaces in vcf output (#7168); - #259 Inline schema for importgenomes.wdl (#7171); - Created AvroFileReader and unittest, Update ExtractCohort and ExtractCohortEngine (#7174); - #224 Import WDL: handle 15 TB /table /day import limit (#7167); - #260 filter out AS_QD, SOR, FS from cohort extract VCF (#7173); - Full scientific validation via end to end comparison of filtered results between WARP and BQ (#7179); - Cherry pick of commits to fix GATK tests from master (#7183); - ExtractCohort supports -XL exclusion and follows intervals, other optimizations (#7181); - ExtractFeatures supports -XL exclusion and follows intervals, other optimizations (#7184); - change 0/0 GQ0 sites to nocalls (#7190); - updated (#7195); - Rename ""metadata"" table to ""sample_info"" table, fix vet schema (#7196); - Allow users to specify VQSLOD sensitivity and apply threshold in ExtractCohort (#7194); - Calculate and Store site-level QCs (#7197); - Filter Failing QC Sites from Extract (#7201); - WDLize GvsPrepareCallset (briefly known as CreateCohortTable) (#7200); - default drop_state to 60, but allow NONE as input (#7206); - SA support and consistent naming for all GVS WDLs (#7205); - fix GvsExtractCallset inputs file (#7210); - add clustering to tables (#7207); - add vqsr cutoffs to GvsExtractCallset wdl; clean up dockstore yml (#7209); - Avro test (#7192); - Enable call caching of TSV generation in GvsImportGenomes (#7226); - 266 Clean up ExtractCohort -- remove query mode param (#7227); - 288 Add an excess alleles param (#7221); - take sample name as a param (#7236); - How to run GIAB comparisons (#7237); - Update GvsCreateFilterSet.wdl (#7239); - Use GatherVcfsCloud in GvsCreateFilterSet.wdl (#7241); - parameterize TTL with defaults, reduce memory allocation (#7244); - Addressing OOM in CohortExtract (#7245); - make outputs optional, change case",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:13089,Deployability,update,updated,13089," QUALapprox and other features (#7146); - Job Add labels to BQ operations from GATK (Issues-199) (#7115); - parse map to list to avoid brackets and spaces in vcf output (#7168); - #259 Inline schema for importgenomes.wdl (#7171); - Created AvroFileReader and unittest, Update ExtractCohort and ExtractCohortEngine (#7174); - #224 Import WDL: handle 15 TB /table /day import limit (#7167); - #260 filter out AS_QD, SOR, FS from cohort extract VCF (#7173); - Full scientific validation via end to end comparison of filtered results between WARP and BQ (#7179); - Cherry pick of commits to fix GATK tests from master (#7183); - ExtractCohort supports -XL exclusion and follows intervals, other optimizations (#7181); - ExtractFeatures supports -XL exclusion and follows intervals, other optimizations (#7184); - change 0/0 GQ0 sites to nocalls (#7190); - updated (#7195); - Rename ""metadata"" table to ""sample_info"" table, fix vet schema (#7196); - Allow users to specify VQSLOD sensitivity and apply threshold in ExtractCohort (#7194); - Calculate and Store site-level QCs (#7197); - Filter Failing QC Sites from Extract (#7201); - WDLize GvsPrepareCallset (briefly known as CreateCohortTable) (#7200); - default drop_state to 60, but allow NONE as input (#7206); - SA support and consistent naming for all GVS WDLs (#7205); - fix GvsExtractCallset inputs file (#7210); - add clustering to tables (#7207); - add vqsr cutoffs to GvsExtractCallset wdl; clean up dockstore yml (#7209); - Avro test (#7192); - Enable call caching of TSV generation in GvsImportGenomes (#7226); - 266 Clean up ExtractCohort -- remove query mode param (#7227); - 288 Add an excess alleles param (#7221); - take sample name as a param (#7236); - How to run GIAB comparisons (#7237); - Update GvsCreateFilterSet.wdl (#7239); - Use GatherVcfsCloud in GvsCreateFilterSet.wdl (#7241); - parameterize TTL with defaults, reduce memory allocation (#7244); - Addressing OOM in CohortExtract (#7245); - make outputs optional, change case",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:13995,Deployability,Update,Update,13995," QUALapprox and other features (#7146); - Job Add labels to BQ operations from GATK (Issues-199) (#7115); - parse map to list to avoid brackets and spaces in vcf output (#7168); - #259 Inline schema for importgenomes.wdl (#7171); - Created AvroFileReader and unittest, Update ExtractCohort and ExtractCohortEngine (#7174); - #224 Import WDL: handle 15 TB /table /day import limit (#7167); - #260 filter out AS_QD, SOR, FS from cohort extract VCF (#7173); - Full scientific validation via end to end comparison of filtered results between WARP and BQ (#7179); - Cherry pick of commits to fix GATK tests from master (#7183); - ExtractCohort supports -XL exclusion and follows intervals, other optimizations (#7181); - ExtractFeatures supports -XL exclusion and follows intervals, other optimizations (#7184); - change 0/0 GQ0 sites to nocalls (#7190); - updated (#7195); - Rename ""metadata"" table to ""sample_info"" table, fix vet schema (#7196); - Allow users to specify VQSLOD sensitivity and apply threshold in ExtractCohort (#7194); - Calculate and Store site-level QCs (#7197); - Filter Failing QC Sites from Extract (#7201); - WDLize GvsPrepareCallset (briefly known as CreateCohortTable) (#7200); - default drop_state to 60, but allow NONE as input (#7206); - SA support and consistent naming for all GVS WDLs (#7205); - fix GvsExtractCallset inputs file (#7210); - add clustering to tables (#7207); - add vqsr cutoffs to GvsExtractCallset wdl; clean up dockstore yml (#7209); - Avro test (#7192); - Enable call caching of TSV generation in GvsImportGenomes (#7226); - 266 Clean up ExtractCohort -- remove query mode param (#7227); - 288 Add an excess alleles param (#7221); - take sample name as a param (#7236); - How to run GIAB comparisons (#7237); - Update GvsCreateFilterSet.wdl (#7239); - Use GatherVcfsCloud in GvsCreateFilterSet.wdl (#7241); - parameterize TTL with defaults, reduce memory allocation (#7244); - Addressing OOM in CohortExtract (#7245); - make outputs optional, change case",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:14515,Deployability,upgrade,upgrade,14515,"ut allow NONE as input (#7206); - SA support and consistent naming for all GVS WDLs (#7205); - fix GvsExtractCallset inputs file (#7210); - add clustering to tables (#7207); - add vqsr cutoffs to GvsExtractCallset wdl; clean up dockstore yml (#7209); - Avro test (#7192); - Enable call caching of TSV generation in GvsImportGenomes (#7226); - 266 Clean up ExtractCohort -- remove query mode param (#7227); - 288 Add an excess alleles param (#7221); - take sample name as a param (#7236); - How to run GIAB comparisons (#7237); - Update GvsCreateFilterSet.wdl (#7239); - Use GatherVcfsCloud in GvsCreateFilterSet.wdl (#7241); - parameterize TTL with defaults, reduce memory allocation (#7244); - Addressing OOM in CohortExtract (#7245); - make outputs optional, change case in output (#7252); - Support for FORMAT/FT VQSLod Filtering and cohort-wide LowQual filter (#7248); - removed arrays code, renamed packages (#7260); - 279 labels (#7233); - add conda commands to GIAB readme (#7268); - remove gvs branch (#7263); - remove gvs branch (#7263); - upgrade bq libraries (#7264); - #299 - Sample list ease of use for cohort extracts (#7272); - check for duplicate ids (#7273); - Rc 274 passing sites only (#7275); - added default value to drop_state; broadinstitute/dsp-spec-ops#310 (#7278); - version bump for reliability (#7284); - add timestamp check to ExtractTask call https://github.com/broadinstitute/dsp-spec-ops/issues/320; - serial inserts for scaling prepare, factored out sample name (#7288); - Remove training sites only param from ExtractFeatures broadinstitute/dsp-spec-ops#261; - add param for mem for indels (#7282); - Ah prepare localize option (#7299); - Export sites only vcf STEP 1-- 317 add AC, AN, AF to the final VCF (#7279); - AoU GVS Cohort Extract wdl (#7242); - reliability (#7310); - bump to include FT tag filtering (#7316); - First pass at a Terra QuickStart (#7267); - Ah fix timestamp query (#7319); - 313 Cleanup Extract Cohort params (#7293); - bump bq storage versi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:16328,Deployability,upgrade,upgraded,16328,"epare, factored out sample name (#7288); - Remove training sites only param from ExtractFeatures broadinstitute/dsp-spec-ops#261; - add param for mem for indels (#7282); - Ah prepare localize option (#7299); - Export sites only vcf STEP 1-- 317 add AC, AN, AF to the final VCF (#7279); - AoU GVS Cohort Extract wdl (#7242); - reliability (#7310); - bump to include FT tag filtering (#7316); - First pass at a Terra QuickStart (#7267); - Ah fix timestamp query (#7319); - 313 Cleanup Extract Cohort params (#7293); - bump bq storage version. See GVS-332 (#7330); - Variant Store extraction - Add VCF size to output (#7329); - add WARP-style scattering to SNPsVariantRecalibrator in GvsCreateFilterSet (#7320); - added ref ranges support (#7337); - 318 Sites only filtered vcf then annotate wdl (#7305); - Replace service_account_json (file) with service_account_json_path (string) to allow call-caching (#7347); - Parallelize create filterset by breaking out the 3 filter set file creation/loads into separate tasks (#7342); - Create WDL to validate VAT and add first test (#7352); - Add task for VAT validation #3 (#7360); - Add task for VAT validation #4 (#7363); - Instructions on how to download BQ Metadata and visualize results (#7359); - don't mix contigs, rightsize memory (#7361); - Add custom annotations as ac an af (#7351); - Add task for VAT validation #8 & 9 (#7364); - added bcftools, upgraded gcloud version (#7369); - fix wdl (#7378); - Update .dockstore.yml; - Add VAT validation rule #5 [VS-16] (#7365); - Add VAT validation rule #7 [VS-14] and validation rule #6 [VS-15] (#7379); - Batching of samples for create import TSVs (#7382); - Add VAT validation rule #2 [VS-19] (#7374); - Create VAT scripts directory (#7386); - fixing SA change from file to string (#7371); - add extract_subpop script (#7387); - Add is_loaded column to sample_info and logic to populate after ingest [VS-158] (#7389); - Add Gnomad subpopulation info into the VAT (#7381); - implement GVS ID assignment (#",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:16382,Deployability,Update,Update,16382,"epare, factored out sample name (#7288); - Remove training sites only param from ExtractFeatures broadinstitute/dsp-spec-ops#261; - add param for mem for indels (#7282); - Ah prepare localize option (#7299); - Export sites only vcf STEP 1-- 317 add AC, AN, AF to the final VCF (#7279); - AoU GVS Cohort Extract wdl (#7242); - reliability (#7310); - bump to include FT tag filtering (#7316); - First pass at a Terra QuickStart (#7267); - Ah fix timestamp query (#7319); - 313 Cleanup Extract Cohort params (#7293); - bump bq storage version. See GVS-332 (#7330); - Variant Store extraction - Add VCF size to output (#7329); - add WARP-style scattering to SNPsVariantRecalibrator in GvsCreateFilterSet (#7320); - added ref ranges support (#7337); - 318 Sites only filtered vcf then annotate wdl (#7305); - Replace service_account_json (file) with service_account_json_path (string) to allow call-caching (#7347); - Parallelize create filterset by breaking out the 3 filter set file creation/loads into separate tasks (#7342); - Create WDL to validate VAT and add first test (#7352); - Add task for VAT validation #3 (#7360); - Add task for VAT validation #4 (#7363); - Instructions on how to download BQ Metadata and visualize results (#7359); - don't mix contigs, rightsize memory (#7361); - Add custom annotations as ac an af (#7351); - Add task for VAT validation #8 & 9 (#7364); - added bcftools, upgraded gcloud version (#7369); - fix wdl (#7378); - Update .dockstore.yml; - Add VAT validation rule #5 [VS-16] (#7365); - Add VAT validation rule #7 [VS-14] and validation rule #6 [VS-15] (#7379); - Batching of samples for create import TSVs (#7382); - Add VAT validation rule #2 [VS-19] (#7374); - Create VAT scripts directory (#7386); - fixing SA change from file to string (#7371); - add extract_subpop script (#7387); - Add is_loaded column to sample_info and logic to populate after ingest [VS-158] (#7389); - Add Gnomad subpopulation info into the VAT (#7381); - implement GVS ID assignment (#",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:17094,Deployability,update,update,17094,"#7359); - don't mix contigs, rightsize memory (#7361); - Add custom annotations as ac an af (#7351); - Add task for VAT validation #8 & 9 (#7364); - added bcftools, upgraded gcloud version (#7369); - fix wdl (#7378); - Update .dockstore.yml; - Add VAT validation rule #5 [VS-16] (#7365); - Add VAT validation rule #7 [VS-14] and validation rule #6 [VS-15] (#7379); - Batching of samples for create import TSVs (#7382); - Add VAT validation rule #2 [VS-19] (#7374); - Create VAT scripts directory (#7386); - fixing SA change from file to string (#7371); - add extract_subpop script (#7387); - Add is_loaded column to sample_info and logic to populate after ingest [VS-158] (#7389); - Add Gnomad subpopulation info into the VAT (#7381); - implement GVS ID assignment (#7355); - no longer loading sample info table in this wdl (#7407); - divide up creation/population of temp pet table [VS-48] (#7395); - Sample QC metrics (#7396); - update import for is_loaded (#7416); - fix partition end, add 1 (#7420); - Fixes to CreateFilterSet and ExtractCallset from 30K run (#7423); - Also changed file size from Int to Float in SumBytes task python (#7429); - Adding the subpopulation calculations to the VAT creation WDL (#7399); - 154 De obfuscate (#7435); - filter on gvs_ids for workflow (#7428); - update for assign ids and changes in import (#7439); - need to loop through sets when moving to done (#7440); - add option for create filter set to use sample_info with is_loaded (#7434); - remove dead branch (#7443); - Scaling the VAT -- switch the input to take in a file of vcf shard file names (#7446); - dockstore testing: move validate vat inputs (#7449); - Update GVS sample QC to support multiple callsets per datasset [VS-177] (#7451); - Update GvsImportGenomes.wdl (#7462); - Add extraction uuid BQ label to GvsPrepareCallstep from GvsExtractCohortFromSampleNames (#7458); - Add manifest summary file to GvsExtractCallset (#7457); - Create workflow to create and populate alt_allele table [VS-51] (",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:17456,Deployability,update,update,17456,"#7359); - don't mix contigs, rightsize memory (#7361); - Add custom annotations as ac an af (#7351); - Add task for VAT validation #8 & 9 (#7364); - added bcftools, upgraded gcloud version (#7369); - fix wdl (#7378); - Update .dockstore.yml; - Add VAT validation rule #5 [VS-16] (#7365); - Add VAT validation rule #7 [VS-14] and validation rule #6 [VS-15] (#7379); - Batching of samples for create import TSVs (#7382); - Add VAT validation rule #2 [VS-19] (#7374); - Create VAT scripts directory (#7386); - fixing SA change from file to string (#7371); - add extract_subpop script (#7387); - Add is_loaded column to sample_info and logic to populate after ingest [VS-158] (#7389); - Add Gnomad subpopulation info into the VAT (#7381); - implement GVS ID assignment (#7355); - no longer loading sample info table in this wdl (#7407); - divide up creation/population of temp pet table [VS-48] (#7395); - Sample QC metrics (#7396); - update import for is_loaded (#7416); - fix partition end, add 1 (#7420); - Fixes to CreateFilterSet and ExtractCallset from 30K run (#7423); - Also changed file size from Int to Float in SumBytes task python (#7429); - Adding the subpopulation calculations to the VAT creation WDL (#7399); - 154 De obfuscate (#7435); - filter on gvs_ids for workflow (#7428); - update for assign ids and changes in import (#7439); - need to loop through sets when moving to done (#7440); - add option for create filter set to use sample_info with is_loaded (#7434); - remove dead branch (#7443); - Scaling the VAT -- switch the input to take in a file of vcf shard file names (#7446); - dockstore testing: move validate vat inputs (#7449); - Update GVS sample QC to support multiple callsets per datasset [VS-177] (#7451); - Update GvsImportGenomes.wdl (#7462); - Add extraction uuid BQ label to GvsPrepareCallstep from GvsExtractCohortFromSampleNames (#7458); - Add manifest summary file to GvsExtractCallset (#7457); - Create workflow to create and populate alt_allele table [VS-51] (",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:17820,Deployability,Update,Update,17820,"#7359); - don't mix contigs, rightsize memory (#7361); - Add custom annotations as ac an af (#7351); - Add task for VAT validation #8 & 9 (#7364); - added bcftools, upgraded gcloud version (#7369); - fix wdl (#7378); - Update .dockstore.yml; - Add VAT validation rule #5 [VS-16] (#7365); - Add VAT validation rule #7 [VS-14] and validation rule #6 [VS-15] (#7379); - Batching of samples for create import TSVs (#7382); - Add VAT validation rule #2 [VS-19] (#7374); - Create VAT scripts directory (#7386); - fixing SA change from file to string (#7371); - add extract_subpop script (#7387); - Add is_loaded column to sample_info and logic to populate after ingest [VS-158] (#7389); - Add Gnomad subpopulation info into the VAT (#7381); - implement GVS ID assignment (#7355); - no longer loading sample info table in this wdl (#7407); - divide up creation/population of temp pet table [VS-48] (#7395); - Sample QC metrics (#7396); - update import for is_loaded (#7416); - fix partition end, add 1 (#7420); - Fixes to CreateFilterSet and ExtractCallset from 30K run (#7423); - Also changed file size from Int to Float in SumBytes task python (#7429); - Adding the subpopulation calculations to the VAT creation WDL (#7399); - 154 De obfuscate (#7435); - filter on gvs_ids for workflow (#7428); - update for assign ids and changes in import (#7439); - need to loop through sets when moving to done (#7440); - add option for create filter set to use sample_info with is_loaded (#7434); - remove dead branch (#7443); - Scaling the VAT -- switch the input to take in a file of vcf shard file names (#7446); - dockstore testing: move validate vat inputs (#7449); - Update GVS sample QC to support multiple callsets per datasset [VS-177] (#7451); - Update GvsImportGenomes.wdl (#7462); - Add extraction uuid BQ label to GvsPrepareCallstep from GvsExtractCohortFromSampleNames (#7458); - Add manifest summary file to GvsExtractCallset (#7457); - Create workflow to create and populate alt_allele table [VS-51] (",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:17903,Deployability,Update,Update,17903,"#7359); - don't mix contigs, rightsize memory (#7361); - Add custom annotations as ac an af (#7351); - Add task for VAT validation #8 & 9 (#7364); - added bcftools, upgraded gcloud version (#7369); - fix wdl (#7378); - Update .dockstore.yml; - Add VAT validation rule #5 [VS-16] (#7365); - Add VAT validation rule #7 [VS-14] and validation rule #6 [VS-15] (#7379); - Batching of samples for create import TSVs (#7382); - Add VAT validation rule #2 [VS-19] (#7374); - Create VAT scripts directory (#7386); - fixing SA change from file to string (#7371); - add extract_subpop script (#7387); - Add is_loaded column to sample_info and logic to populate after ingest [VS-158] (#7389); - Add Gnomad subpopulation info into the VAT (#7381); - implement GVS ID assignment (#7355); - no longer loading sample info table in this wdl (#7407); - divide up creation/population of temp pet table [VS-48] (#7395); - Sample QC metrics (#7396); - update import for is_loaded (#7416); - fix partition end, add 1 (#7420); - Fixes to CreateFilterSet and ExtractCallset from 30K run (#7423); - Also changed file size from Int to Float in SumBytes task python (#7429); - Adding the subpopulation calculations to the VAT creation WDL (#7399); - 154 De obfuscate (#7435); - filter on gvs_ids for workflow (#7428); - update for assign ids and changes in import (#7439); - need to loop through sets when moving to done (#7440); - add option for create filter set to use sample_info with is_loaded (#7434); - remove dead branch (#7443); - Scaling the VAT -- switch the input to take in a file of vcf shard file names (#7446); - dockstore testing: move validate vat inputs (#7449); - Update GVS sample QC to support multiple callsets per datasset [VS-177] (#7451); - Update GvsImportGenomes.wdl (#7462); - Add extraction uuid BQ label to GvsPrepareCallstep from GvsExtractCohortFromSampleNames (#7458); - Add manifest summary file to GvsExtractCallset (#7457); - Create workflow to create and populate alt_allele table [VS-51] (",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:18211,Deployability,update,updates,18211,"es to CreateFilterSet and ExtractCallset from 30K run (#7423); - Also changed file size from Int to Float in SumBytes task python (#7429); - Adding the subpopulation calculations to the VAT creation WDL (#7399); - 154 De obfuscate (#7435); - filter on gvs_ids for workflow (#7428); - update for assign ids and changes in import (#7439); - need to loop through sets when moving to done (#7440); - add option for create filter set to use sample_info with is_loaded (#7434); - remove dead branch (#7443); - Scaling the VAT -- switch the input to take in a file of vcf shard file names (#7446); - dockstore testing: move validate vat inputs (#7449); - Update GVS sample QC to support multiple callsets per datasset [VS-177] (#7451); - Update GvsImportGenomes.wdl (#7462); - Add extraction uuid BQ label to GvsPrepareCallstep from GvsExtractCohortFromSampleNames (#7458); - Add manifest summary file to GvsExtractCallset (#7457); - Create workflow to create and populate alt_allele table [VS-51] (#7426); - Added additional workflow and README updates for Quickstart [VS-183] (#7463); - formatting on sample QC README; - formatting change #2 to sample QC README; - address VS-152, remove extra headers from extract (#7466); - Update GvsExtractCallset.example.inputs.json (#7469); - Add ability to copy interval list files to gs directory [VS-191] (#7467); - add an expiration date to the temp tables (#7455); - fix the check for duplicates in import genomes (#7470); - added job ID to alt_allele population call output [VS-194] (#7473); - added steps and deliverables to GVS README [VS-181] (#7452); - Ah check the is loaded field in feature extract (#7475); - changes to put pet data directly into data table (#7478); - added override for ExtractTasks' preemptible value (#7477); - bcftools to the rescue (#7456); - execute_with_retry() refactor and error handling improvements [VS-159] (#7480); - Small updates to GvsExtractCallset from beta callset, new workflow for re-scattered shards (#7493); - add f",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:18393,Deployability,Update,Update,18393,"es to CreateFilterSet and ExtractCallset from 30K run (#7423); - Also changed file size from Int to Float in SumBytes task python (#7429); - Adding the subpopulation calculations to the VAT creation WDL (#7399); - 154 De obfuscate (#7435); - filter on gvs_ids for workflow (#7428); - update for assign ids and changes in import (#7439); - need to loop through sets when moving to done (#7440); - add option for create filter set to use sample_info with is_loaded (#7434); - remove dead branch (#7443); - Scaling the VAT -- switch the input to take in a file of vcf shard file names (#7446); - dockstore testing: move validate vat inputs (#7449); - Update GVS sample QC to support multiple callsets per datasset [VS-177] (#7451); - Update GvsImportGenomes.wdl (#7462); - Add extraction uuid BQ label to GvsPrepareCallstep from GvsExtractCohortFromSampleNames (#7458); - Add manifest summary file to GvsExtractCallset (#7457); - Create workflow to create and populate alt_allele table [VS-51] (#7426); - Added additional workflow and README updates for Quickstart [VS-183] (#7463); - formatting on sample QC README; - formatting change #2 to sample QC README; - address VS-152, remove extra headers from extract (#7466); - Update GvsExtractCallset.example.inputs.json (#7469); - Add ability to copy interval list files to gs directory [VS-191] (#7467); - add an expiration date to the temp tables (#7455); - fix the check for duplicates in import genomes (#7470); - added job ID to alt_allele population call output [VS-194] (#7473); - added steps and deliverables to GVS README [VS-181] (#7452); - Ah check the is loaded field in feature extract (#7475); - changes to put pet data directly into data table (#7478); - added override for ExtractTasks' preemptible value (#7477); - bcftools to the rescue (#7456); - execute_with_retry() refactor and error handling improvements [VS-159] (#7480); - Small updates to GvsExtractCallset from beta callset, new workflow for re-scattered shards (#7493); - add f",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:19072,Deployability,update,updates,19072,"463); - formatting on sample QC README; - formatting change #2 to sample QC README; - address VS-152, remove extra headers from extract (#7466); - Update GvsExtractCallset.example.inputs.json (#7469); - Add ability to copy interval list files to gs directory [VS-191] (#7467); - add an expiration date to the temp tables (#7455); - fix the check for duplicates in import genomes (#7470); - added job ID to alt_allele population call output [VS-194] (#7473); - added steps and deliverables to GVS README [VS-181] (#7452); - Ah check the is loaded field in feature extract (#7475); - changes to put pet data directly into data table (#7478); - added override for ExtractTasks' preemptible value (#7477); - bcftools to the rescue (#7456); - execute_with_retry() refactor and error handling improvements [VS-159] (#7480); - Small updates to GvsExtractCallset from beta callset, new workflow for re-scattered shards (#7493); - add flag in prepare to print out sql instead of executing (#7501); - Workflow to re-scatter and then merge ""problematic"" intervals from ExtractCallset [VS-209] (#7495); - changed README to reflect comments from Lee [VS-210] (#7502); - Export the VAT into GCS (#7472); - addresses VS-219 (#7508); - small fix to MergeVCFs (#7517); - small fixes to GVS pipeline (#7522); - make sure ExtractTask is run on all interval files; - Revert ""make sure ExtractTask is run on all interval files""; - make sure ExtractTask is run on all interval files (#7527); - Remove Sites only step from the VAT creation WDL (#7510); - fix bad argument processing for bool (#7529); - Support for TDR DRS URIs in Import (#7528); - Match format of filename output in GvsRescatterCallsetInterval (#7539); - Reference block storage and query support (#7498); - update docs (#7540); - Kc fix rr load bug (#7550); - Update .dockstore.yml (#7553); - Ah add reblocking wdl (#7544); - Scatter over all interval files, not just scatter count (#7551); - fixed docker (#7558); - take advantage of fixed version of Spl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:19519,Deployability,pipeline,pipeline,19519,"463); - formatting on sample QC README; - formatting change #2 to sample QC README; - address VS-152, remove extra headers from extract (#7466); - Update GvsExtractCallset.example.inputs.json (#7469); - Add ability to copy interval list files to gs directory [VS-191] (#7467); - add an expiration date to the temp tables (#7455); - fix the check for duplicates in import genomes (#7470); - added job ID to alt_allele population call output [VS-194] (#7473); - added steps and deliverables to GVS README [VS-181] (#7452); - Ah check the is loaded field in feature extract (#7475); - changes to put pet data directly into data table (#7478); - added override for ExtractTasks' preemptible value (#7477); - bcftools to the rescue (#7456); - execute_with_retry() refactor and error handling improvements [VS-159] (#7480); - Small updates to GvsExtractCallset from beta callset, new workflow for re-scattered shards (#7493); - add flag in prepare to print out sql instead of executing (#7501); - Workflow to re-scatter and then merge ""problematic"" intervals from ExtractCallset [VS-209] (#7495); - changed README to reflect comments from Lee [VS-210] (#7502); - Export the VAT into GCS (#7472); - addresses VS-219 (#7508); - small fix to MergeVCFs (#7517); - small fixes to GVS pipeline (#7522); - make sure ExtractTask is run on all interval files; - Revert ""make sure ExtractTask is run on all interval files""; - make sure ExtractTask is run on all interval files (#7527); - Remove Sites only step from the VAT creation WDL (#7510); - fix bad argument processing for bool (#7529); - Support for TDR DRS URIs in Import (#7528); - Match format of filename output in GvsRescatterCallsetInterval (#7539); - Reference block storage and query support (#7498); - update docs (#7540); - Kc fix rr load bug (#7550); - Update .dockstore.yml (#7553); - Ah add reblocking wdl (#7544); - Scatter over all interval files, not just scatter count (#7551); - fixed docker (#7558); - take advantage of fixed version of Spl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:19999,Deployability,update,update,19999,"463); - formatting on sample QC README; - formatting change #2 to sample QC README; - address VS-152, remove extra headers from extract (#7466); - Update GvsExtractCallset.example.inputs.json (#7469); - Add ability to copy interval list files to gs directory [VS-191] (#7467); - add an expiration date to the temp tables (#7455); - fix the check for duplicates in import genomes (#7470); - added job ID to alt_allele population call output [VS-194] (#7473); - added steps and deliverables to GVS README [VS-181] (#7452); - Ah check the is loaded field in feature extract (#7475); - changes to put pet data directly into data table (#7478); - added override for ExtractTasks' preemptible value (#7477); - bcftools to the rescue (#7456); - execute_with_retry() refactor and error handling improvements [VS-159] (#7480); - Small updates to GvsExtractCallset from beta callset, new workflow for re-scattered shards (#7493); - add flag in prepare to print out sql instead of executing (#7501); - Workflow to re-scatter and then merge ""problematic"" intervals from ExtractCallset [VS-209] (#7495); - changed README to reflect comments from Lee [VS-210] (#7502); - Export the VAT into GCS (#7472); - addresses VS-219 (#7508); - small fix to MergeVCFs (#7517); - small fixes to GVS pipeline (#7522); - make sure ExtractTask is run on all interval files; - Revert ""make sure ExtractTask is run on all interval files""; - make sure ExtractTask is run on all interval files (#7527); - Remove Sites only step from the VAT creation WDL (#7510); - fix bad argument processing for bool (#7529); - Support for TDR DRS URIs in Import (#7528); - Match format of filename output in GvsRescatterCallsetInterval (#7539); - Reference block storage and query support (#7498); - update docs (#7540); - Kc fix rr load bug (#7550); - Update .dockstore.yml (#7553); - Ah add reblocking wdl (#7544); - Scatter over all interval files, not just scatter count (#7551); - fixed docker (#7558); - take advantage of fixed version of Spl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:20052,Deployability,Update,Update,20052,"463); - formatting on sample QC README; - formatting change #2 to sample QC README; - address VS-152, remove extra headers from extract (#7466); - Update GvsExtractCallset.example.inputs.json (#7469); - Add ability to copy interval list files to gs directory [VS-191] (#7467); - add an expiration date to the temp tables (#7455); - fix the check for duplicates in import genomes (#7470); - added job ID to alt_allele population call output [VS-194] (#7473); - added steps and deliverables to GVS README [VS-181] (#7452); - Ah check the is loaded field in feature extract (#7475); - changes to put pet data directly into data table (#7478); - added override for ExtractTasks' preemptible value (#7477); - bcftools to the rescue (#7456); - execute_with_retry() refactor and error handling improvements [VS-159] (#7480); - Small updates to GvsExtractCallset from beta callset, new workflow for re-scattered shards (#7493); - add flag in prepare to print out sql instead of executing (#7501); - Workflow to re-scatter and then merge ""problematic"" intervals from ExtractCallset [VS-209] (#7495); - changed README to reflect comments from Lee [VS-210] (#7502); - Export the VAT into GCS (#7472); - addresses VS-219 (#7508); - small fix to MergeVCFs (#7517); - small fixes to GVS pipeline (#7522); - make sure ExtractTask is run on all interval files; - Revert ""make sure ExtractTask is run on all interval files""; - make sure ExtractTask is run on all interval files (#7527); - Remove Sites only step from the VAT creation WDL (#7510); - fix bad argument processing for bool (#7529); - Support for TDR DRS URIs in Import (#7528); - Match format of filename output in GvsRescatterCallsetInterval (#7539); - Reference block storage and query support (#7498); - update docs (#7540); - Kc fix rr load bug (#7550); - Update .dockstore.yml (#7553); - Ah add reblocking wdl (#7544); - Scatter over all interval files, not just scatter count (#7551); - fixed docker (#7558); - take advantage of fixed version of Spl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:20601,Deployability,upgrade,upgrade,20601,"rom Lee [VS-210] (#7502); - Export the VAT into GCS (#7472); - addresses VS-219 (#7508); - small fix to MergeVCFs (#7517); - small fixes to GVS pipeline (#7522); - make sure ExtractTask is run on all interval files; - Revert ""make sure ExtractTask is run on all interval files""; - make sure ExtractTask is run on all interval files (#7527); - Remove Sites only step from the VAT creation WDL (#7510); - fix bad argument processing for bool (#7529); - Support for TDR DRS URIs in Import (#7528); - Match format of filename output in GvsRescatterCallsetInterval (#7539); - Reference block storage and query support (#7498); - update docs (#7540); - Kc fix rr load bug (#7550); - Update .dockstore.yml (#7553); - Ah add reblocking wdl (#7544); - Scatter over all interval files, not just scatter count (#7551); - fixed docker (#7558); - take advantage of fixed version of SplitIntervals (#7566); - Document AoU-specific tieout [VS-233] (#7552); - bad param assignment in aou reblocking (#7572); - Small fixes to ImportGenomes (non-write api version) (#7574); - Ah change output of reblocking wdl to external path (#7575); - close BQ Readers (#7583); - Ah spike writeapi (#7530); - bump WDL jar (#7593); - read api bytes logging, upgrade bigquery client versions (#7601); - bump (#7610); - upgrade log4j to 2.17 (#7616); - Add drop_state default of Forty to extract (#7619); - Kc fix type (#7620); - VAT cleanup and documentation (#7531); - fix empty flush (#7627); - presorted avro files, fix performance issue (#7635); - WIP extract for ranges (#7640); - VS-268 import more samples at once (#7629); - clustering vqsr tables by location (#7656); - First Version of a weight-based splitter (#7643); - Update GvsExtractCallset.wdl; - Quoting of table names (#7666); - docs for analysis of shard runtimes for balanced sharding (#7645); - Wire through GvsExtractCohortFromSampleNames with new prepare/extract [VS-283] (#7654); - Update GvsExtractCallset.wdl (#7678); - cherry pick lb_lfs_force change (#7683",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:20661,Deployability,upgrade,upgrade,20661,"rom Lee [VS-210] (#7502); - Export the VAT into GCS (#7472); - addresses VS-219 (#7508); - small fix to MergeVCFs (#7517); - small fixes to GVS pipeline (#7522); - make sure ExtractTask is run on all interval files; - Revert ""make sure ExtractTask is run on all interval files""; - make sure ExtractTask is run on all interval files (#7527); - Remove Sites only step from the VAT creation WDL (#7510); - fix bad argument processing for bool (#7529); - Support for TDR DRS URIs in Import (#7528); - Match format of filename output in GvsRescatterCallsetInterval (#7539); - Reference block storage and query support (#7498); - update docs (#7540); - Kc fix rr load bug (#7550); - Update .dockstore.yml (#7553); - Ah add reblocking wdl (#7544); - Scatter over all interval files, not just scatter count (#7551); - fixed docker (#7558); - take advantage of fixed version of SplitIntervals (#7566); - Document AoU-specific tieout [VS-233] (#7552); - bad param assignment in aou reblocking (#7572); - Small fixes to ImportGenomes (non-write api version) (#7574); - Ah change output of reblocking wdl to external path (#7575); - close BQ Readers (#7583); - Ah spike writeapi (#7530); - bump WDL jar (#7593); - read api bytes logging, upgrade bigquery client versions (#7601); - bump (#7610); - upgrade log4j to 2.17 (#7616); - Add drop_state default of Forty to extract (#7619); - Kc fix type (#7620); - VAT cleanup and documentation (#7531); - fix empty flush (#7627); - presorted avro files, fix performance issue (#7635); - WIP extract for ranges (#7640); - VS-268 import more samples at once (#7629); - clustering vqsr tables by location (#7656); - First Version of a weight-based splitter (#7643); - Update GvsExtractCallset.wdl; - Quoting of table names (#7666); - docs for analysis of shard runtimes for balanced sharding (#7645); - Wire through GvsExtractCohortFromSampleNames with new prepare/extract [VS-283] (#7654); - Update GvsExtractCallset.wdl (#7678); - cherry pick lb_lfs_force change (#7683",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:21072,Deployability,Update,Update,21072,"filename output in GvsRescatterCallsetInterval (#7539); - Reference block storage and query support (#7498); - update docs (#7540); - Kc fix rr load bug (#7550); - Update .dockstore.yml (#7553); - Ah add reblocking wdl (#7544); - Scatter over all interval files, not just scatter count (#7551); - fixed docker (#7558); - take advantage of fixed version of SplitIntervals (#7566); - Document AoU-specific tieout [VS-233] (#7552); - bad param assignment in aou reblocking (#7572); - Small fixes to ImportGenomes (non-write api version) (#7574); - Ah change output of reblocking wdl to external path (#7575); - close BQ Readers (#7583); - Ah spike writeapi (#7530); - bump WDL jar (#7593); - read api bytes logging, upgrade bigquery client versions (#7601); - bump (#7610); - upgrade log4j to 2.17 (#7616); - Add drop_state default of Forty to extract (#7619); - Kc fix type (#7620); - VAT cleanup and documentation (#7531); - fix empty flush (#7627); - presorted avro files, fix performance issue (#7635); - WIP extract for ranges (#7640); - VS-268 import more samples at once (#7629); - clustering vqsr tables by location (#7656); - First Version of a weight-based splitter (#7643); - Update GvsExtractCallset.wdl; - Quoting of table names (#7666); - docs for analysis of shard runtimes for balanced sharding (#7645); - Wire through GvsExtractCohortFromSampleNames with new prepare/extract [VS-283] (#7654); - Update GvsExtractCallset.wdl (#7678); - cherry pick lb_lfs_force change (#7683); - Tweak ingest messaging and failure mode [VS-267] (#7680); - Additional tweaks for GvsExtractCohortFromSampleNames [VS-283] (#7698); - VS-280 Create a VAT intermediary (#7657); - There something about split intervals [VS-306] (#7694); - VS 284 Add prepare step to Quick Start (#7685); - VS-222 dont hard code the dataset name! (#7704); - fixed bug; added tests (#7717); - Clean up optional and inconsistently named inputs [VS-294] [VS-218] (#7715); - VS-263 notes on ingest and beyond (#7618); - Add task to Ex",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:21297,Deployability,Update,Update,21297,"take advantage of fixed version of SplitIntervals (#7566); - Document AoU-specific tieout [VS-233] (#7552); - bad param assignment in aou reblocking (#7572); - Small fixes to ImportGenomes (non-write api version) (#7574); - Ah change output of reblocking wdl to external path (#7575); - close BQ Readers (#7583); - Ah spike writeapi (#7530); - bump WDL jar (#7593); - read api bytes logging, upgrade bigquery client versions (#7601); - bump (#7610); - upgrade log4j to 2.17 (#7616); - Add drop_state default of Forty to extract (#7619); - Kc fix type (#7620); - VAT cleanup and documentation (#7531); - fix empty flush (#7627); - presorted avro files, fix performance issue (#7635); - WIP extract for ranges (#7640); - VS-268 import more samples at once (#7629); - clustering vqsr tables by location (#7656); - First Version of a weight-based splitter (#7643); - Update GvsExtractCallset.wdl; - Quoting of table names (#7666); - docs for analysis of shard runtimes for balanced sharding (#7645); - Wire through GvsExtractCohortFromSampleNames with new prepare/extract [VS-283] (#7654); - Update GvsExtractCallset.wdl (#7678); - cherry pick lb_lfs_force change (#7683); - Tweak ingest messaging and failure mode [VS-267] (#7680); - Additional tweaks for GvsExtractCohortFromSampleNames [VS-283] (#7698); - VS-280 Create a VAT intermediary (#7657); - There something about split intervals [VS-306] (#7694); - VS 284 Add prepare step to Quick Start (#7685); - VS-222 dont hard code the dataset name! (#7704); - fixed bug; added tests (#7717); - Clean up optional and inconsistently named inputs [VS-294] [VS-218] (#7715); - VS-263 notes on ingest and beyond (#7618); - Add task to ExtractCallset that verifies filter_set_name exists in GVS dataset [VS-335] (#7734); - Clean up input json files to reflect changes inputs [VS-337] (#7733); - used constants; implemented non-AS transformation (#7718); - Pass dataset name to gatk ExtractFeatures (#7735); - Add withdrawn and is_control columns [VS-70] [VS-2",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:22317,Deployability,update,update,22317,"ntimes for balanced sharding (#7645); - Wire through GvsExtractCohortFromSampleNames with new prepare/extract [VS-283] (#7654); - Update GvsExtractCallset.wdl (#7678); - cherry pick lb_lfs_force change (#7683); - Tweak ingest messaging and failure mode [VS-267] (#7680); - Additional tweaks for GvsExtractCohortFromSampleNames [VS-283] (#7698); - VS-280 Create a VAT intermediary (#7657); - There something about split intervals [VS-306] (#7694); - VS 284 Add prepare step to Quick Start (#7685); - VS-222 dont hard code the dataset name! (#7704); - fixed bug; added tests (#7717); - Clean up optional and inconsistently named inputs [VS-294] [VS-218] (#7715); - VS-263 notes on ingest and beyond (#7618); - Add task to ExtractCallset that verifies filter_set_name exists in GVS dataset [VS-335] (#7734); - Clean up input json files to reflect changes inputs [VS-337] (#7733); - used constants; implemented non-AS transformation (#7718); - Pass dataset name to gatk ExtractFeatures (#7735); - Add withdrawn and is_control columns [VS-70] [VS-213] (#7736); - Allow interval lists that require the SA to see (#7743); - allow for gatk to be overridden, update with known good jar (#7758); - VS-361 Add GvsWithdrawSamples wdl (#7765); - Extract Performance Improvements (#7686); - Don't put withdrawn sample data in alt_allele table [VS-369] (#7762); - remove PET code (#7768); - Adding AD for scale testing VS 225 add AD (#7713); - Deterministic Sample ID assignments [VS-371] (#7770); - remove R scripts from filtering (#7781); - Remove an old ""temp table"" dataset (#7780); - Clean up LocalizeFile [VS-314] (#7771); - Remove pet code from CreateVariantIngestFiles and friends [VS-375] (#7773); - 317 remove excess header values in VCF extract (#7786); - correct auth in split intervals (#7790); - Add code to (optionally) zero pad the vcf filename. (#7783); - LoadData `maxRetries` parameterized, default increased [VS-383] (#7791); - Update to latest version of ah_var_store gatk override jar (#7793); ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:23100,Deployability,Update,Update,23100,"res (#7735); - Add withdrawn and is_control columns [VS-70] [VS-213] (#7736); - Allow interval lists that require the SA to see (#7743); - allow for gatk to be overridden, update with known good jar (#7758); - VS-361 Add GvsWithdrawSamples wdl (#7765); - Extract Performance Improvements (#7686); - Don't put withdrawn sample data in alt_allele table [VS-369] (#7762); - remove PET code (#7768); - Adding AD for scale testing VS 225 add AD (#7713); - Deterministic Sample ID assignments [VS-371] (#7770); - remove R scripts from filtering (#7781); - Remove an old ""temp table"" dataset (#7780); - Clean up LocalizeFile [VS-314] (#7771); - Remove pet code from CreateVariantIngestFiles and friends [VS-375] (#7773); - 317 remove excess header values in VCF extract (#7786); - correct auth in split intervals (#7790); - Add code to (optionally) zero pad the vcf filename. (#7783); - LoadData `maxRetries` parameterized, default increased [VS-383] (#7791); - Update to latest version of ah_var_store gatk override jar (#7793); - GvsUnified WDL to wrap the 6 core GVS WDLs [VS-382] (#7789); - Pinned typing_extensions python package to 4.1.1 to fix conda environment. (#7802); - WeightedSplitInterval fixes [VS-384] [VS-332] (#7795); - Replace Travis with GithubActions (#7754); - Docker build only lfs pulls main/src/resources/large (#7727); - Clean up gatk jars -- looks like we are not passing them properly in the extract (#7788); - Fix typo that broke git lfs pull (#7806); - Document AoU SOP (up to the VAT) [VS-63] (#7807); - Incident VS 365 clinvar classification fix (#7769); - VS-390. Add precision and sensitivity wdl (#7813); - Quickstart based integration test [VS-357] (#7812); - 365 vat python testing additions (#7756); - VS 396 clinvar grabs too many values (#7823); - Added a test to validate WDLs in the scripts directory. (#7826) (#7829); - VAT Performance / Reliability Improvements (#7828); - VAT naming conventions [VS-410] (#7827); - Rc remove ad from vat (#7832); - bugfix, we were",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:23797,Deployability,integrat,integration,23797,"; - 317 remove excess header values in VCF extract (#7786); - correct auth in split intervals (#7790); - Add code to (optionally) zero pad the vcf filename. (#7783); - LoadData `maxRetries` parameterized, default increased [VS-383] (#7791); - Update to latest version of ah_var_store gatk override jar (#7793); - GvsUnified WDL to wrap the 6 core GVS WDLs [VS-382] (#7789); - Pinned typing_extensions python package to 4.1.1 to fix conda environment. (#7802); - WeightedSplitInterval fixes [VS-384] [VS-332] (#7795); - Replace Travis with GithubActions (#7754); - Docker build only lfs pulls main/src/resources/large (#7727); - Clean up gatk jars -- looks like we are not passing them properly in the extract (#7788); - Fix typo that broke git lfs pull (#7806); - Document AoU SOP (up to the VAT) [VS-63] (#7807); - Incident VS 365 clinvar classification fix (#7769); - VS-390. Add precision and sensitivity wdl (#7813); - Quickstart based integration test [VS-357] (#7812); - 365 vat python testing additions (#7756); - VS 396 clinvar grabs too many values (#7823); - Added a test to validate WDLs in the scripts directory. (#7826) (#7829); - VAT Performance / Reliability Improvements (#7828); - VAT naming conventions [VS-410] (#7827); - Rc remove ad from vat (#7832); - bugfix, we were trying to grep a binary file (#7837); - Cleanup scripts/variantstore [VS-414] (#7834); - Merge VAT TSV files into single bgzipped file [VS-304] (#7848); - Handle fully and partially loaded samples [VS-262] [VS-258] (#7843); - Ingest Error Handling Fixes [VS-261] (#7841); - First cut at a python notebook to validate inputs. (#7845); - Compute filter scatter [VS-392] (#7852); - remove withdrawn req (#7844); - Improve import error message [VS-437] (#7855); - Fix Input Validation python notebook (#7853); - Add VAT Validation check that aa_change and exon_number are consistently set. (#7850); - Ingest 10K [VS-344] (#7860); - X/Y chromosome reweighting for better extract shard runtime balance [VS-389] (#7868",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:25172,Deployability,Update,Update,25172,"to validate inputs. (#7845); - Compute filter scatter [VS-392] (#7852); - remove withdrawn req (#7844); - Improve import error message [VS-437] (#7855); - Fix Input Validation python notebook (#7853); - Add VAT Validation check that aa_change and exon_number are consistently set. (#7850); - Ingest 10K [VS-344] (#7860); - X/Y chromosome reweighting for better extract shard runtime balance [VS-389] (#7868); - VET Ingest Validation / Allow Ingest of non-VQSR'ed data (#7870); - Fix AoU workflow bugs (#7874); - Curate input arrays to skip already ingested sample data [VS-246] (#7862); - KM upload GVS product sheet (#7883); - Default extract scatter width [VS-415] (#7878); - Volatile tasks review [VS-447] (#7880); - Update Quickstart Integration for X/Y scaling changes [VS-464] (#7881); - clean up dockstore; - Rc vs 63 vat sop documentation (#7879); - Fix up FQ and race condition issues with volatile tasks work [VS-478] (#7888); - Use gvs-internal project in integration test (#7901); - Add cost observability BQ table [VS-441] (#7891); - Add preliminary labels to queries [VS-381] (#7902); - Workflow compute costs [VS-472] (#7905); - Fix bug and update images (#7912); - VS 483 Beta user wdl (#7894); - Core storage model cost [VS-473] (#7913); - Update Quickstart & Integration to use re-blocked v2 gVCFs [VS-491] (#7924); - KM GVS documentation (#7903); - Track BigQuery costs of GVS python VS-480 (#7915); - Read cost observability table [VS-475] (#7923); - Fix Race Condition, Add Support for Extract by Array of Sample Names (ie from a Sample Set) (#7917); - Rightsize import batches [VS-486] (#7925); - [AoU DRC] Support uppercase site_ids for reblocking (#7929); - Populate cost metadata for GATK tasks. (#7919); - remove accidentally added input (#7931); - VS_492 - Beta User Jar release (#7934); - Cost WDL should throw on FISS API errors [VS-518] (#7942); - Fix bad check for missing workflow name [VS-520] (#7943); - Remove usage of service account from GvsValidateVAT.wdl (#7937",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:25190,Deployability,Integrat,Integration,25190,"to validate inputs. (#7845); - Compute filter scatter [VS-392] (#7852); - remove withdrawn req (#7844); - Improve import error message [VS-437] (#7855); - Fix Input Validation python notebook (#7853); - Add VAT Validation check that aa_change and exon_number are consistently set. (#7850); - Ingest 10K [VS-344] (#7860); - X/Y chromosome reweighting for better extract shard runtime balance [VS-389] (#7868); - VET Ingest Validation / Allow Ingest of non-VQSR'ed data (#7870); - Fix AoU workflow bugs (#7874); - Curate input arrays to skip already ingested sample data [VS-246] (#7862); - KM upload GVS product sheet (#7883); - Default extract scatter width [VS-415] (#7878); - Volatile tasks review [VS-447] (#7880); - Update Quickstart Integration for X/Y scaling changes [VS-464] (#7881); - clean up dockstore; - Rc vs 63 vat sop documentation (#7879); - Fix up FQ and race condition issues with volatile tasks work [VS-478] (#7888); - Use gvs-internal project in integration test (#7901); - Add cost observability BQ table [VS-441] (#7891); - Add preliminary labels to queries [VS-381] (#7902); - Workflow compute costs [VS-472] (#7905); - Fix bug and update images (#7912); - VS 483 Beta user wdl (#7894); - Core storage model cost [VS-473] (#7913); - Update Quickstart & Integration to use re-blocked v2 gVCFs [VS-491] (#7924); - KM GVS documentation (#7903); - Track BigQuery costs of GVS python VS-480 (#7915); - Read cost observability table [VS-475] (#7923); - Fix Race Condition, Add Support for Extract by Array of Sample Names (ie from a Sample Set) (#7917); - Rightsize import batches [VS-486] (#7925); - [AoU DRC] Support uppercase site_ids for reblocking (#7929); - Populate cost metadata for GATK tasks. (#7919); - remove accidentally added input (#7931); - VS_492 - Beta User Jar release (#7934); - Cost WDL should throw on FISS API errors [VS-518] (#7942); - Fix bad check for missing workflow name [VS-520] (#7943); - Remove usage of service account from GvsValidateVAT.wdl (#7937",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:25419,Deployability,integrat,integration,25419,"to validate inputs. (#7845); - Compute filter scatter [VS-392] (#7852); - remove withdrawn req (#7844); - Improve import error message [VS-437] (#7855); - Fix Input Validation python notebook (#7853); - Add VAT Validation check that aa_change and exon_number are consistently set. (#7850); - Ingest 10K [VS-344] (#7860); - X/Y chromosome reweighting for better extract shard runtime balance [VS-389] (#7868); - VET Ingest Validation / Allow Ingest of non-VQSR'ed data (#7870); - Fix AoU workflow bugs (#7874); - Curate input arrays to skip already ingested sample data [VS-246] (#7862); - KM upload GVS product sheet (#7883); - Default extract scatter width [VS-415] (#7878); - Volatile tasks review [VS-447] (#7880); - Update Quickstart Integration for X/Y scaling changes [VS-464] (#7881); - clean up dockstore; - Rc vs 63 vat sop documentation (#7879); - Fix up FQ and race condition issues with volatile tasks work [VS-478] (#7888); - Use gvs-internal project in integration test (#7901); - Add cost observability BQ table [VS-441] (#7891); - Add preliminary labels to queries [VS-381] (#7902); - Workflow compute costs [VS-472] (#7905); - Fix bug and update images (#7912); - VS 483 Beta user wdl (#7894); - Core storage model cost [VS-473] (#7913); - Update Quickstart & Integration to use re-blocked v2 gVCFs [VS-491] (#7924); - KM GVS documentation (#7903); - Track BigQuery costs of GVS python VS-480 (#7915); - Read cost observability table [VS-475] (#7923); - Fix Race Condition, Add Support for Extract by Array of Sample Names (ie from a Sample Set) (#7917); - Rightsize import batches [VS-486] (#7925); - [AoU DRC] Support uppercase site_ids for reblocking (#7929); - Populate cost metadata for GATK tasks. (#7919); - remove accidentally added input (#7931); - VS_492 - Beta User Jar release (#7934); - Cost WDL should throw on FISS API errors [VS-518] (#7942); - Fix bad check for missing workflow name [VS-520] (#7943); - Remove usage of service account from GvsValidateVAT.wdl (#7937",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:25608,Deployability,update,update,25608,"to validate inputs. (#7845); - Compute filter scatter [VS-392] (#7852); - remove withdrawn req (#7844); - Improve import error message [VS-437] (#7855); - Fix Input Validation python notebook (#7853); - Add VAT Validation check that aa_change and exon_number are consistently set. (#7850); - Ingest 10K [VS-344] (#7860); - X/Y chromosome reweighting for better extract shard runtime balance [VS-389] (#7868); - VET Ingest Validation / Allow Ingest of non-VQSR'ed data (#7870); - Fix AoU workflow bugs (#7874); - Curate input arrays to skip already ingested sample data [VS-246] (#7862); - KM upload GVS product sheet (#7883); - Default extract scatter width [VS-415] (#7878); - Volatile tasks review [VS-447] (#7880); - Update Quickstart Integration for X/Y scaling changes [VS-464] (#7881); - clean up dockstore; - Rc vs 63 vat sop documentation (#7879); - Fix up FQ and race condition issues with volatile tasks work [VS-478] (#7888); - Use gvs-internal project in integration test (#7901); - Add cost observability BQ table [VS-441] (#7891); - Add preliminary labels to queries [VS-381] (#7902); - Workflow compute costs [VS-472] (#7905); - Fix bug and update images (#7912); - VS 483 Beta user wdl (#7894); - Core storage model cost [VS-473] (#7913); - Update Quickstart & Integration to use re-blocked v2 gVCFs [VS-491] (#7924); - KM GVS documentation (#7903); - Track BigQuery costs of GVS python VS-480 (#7915); - Read cost observability table [VS-475] (#7923); - Fix Race Condition, Add Support for Extract by Array of Sample Names (ie from a Sample Set) (#7917); - Rightsize import batches [VS-486] (#7925); - [AoU DRC] Support uppercase site_ids for reblocking (#7929); - Populate cost metadata for GATK tasks. (#7919); - remove accidentally added input (#7931); - VS_492 - Beta User Jar release (#7934); - Cost WDL should throw on FISS API errors [VS-518] (#7942); - Fix bad check for missing workflow name [VS-520] (#7943); - Remove usage of service account from GvsValidateVAT.wdl (#7937",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:25709,Deployability,Update,Update,25709,"to validate inputs. (#7845); - Compute filter scatter [VS-392] (#7852); - remove withdrawn req (#7844); - Improve import error message [VS-437] (#7855); - Fix Input Validation python notebook (#7853); - Add VAT Validation check that aa_change and exon_number are consistently set. (#7850); - Ingest 10K [VS-344] (#7860); - X/Y chromosome reweighting for better extract shard runtime balance [VS-389] (#7868); - VET Ingest Validation / Allow Ingest of non-VQSR'ed data (#7870); - Fix AoU workflow bugs (#7874); - Curate input arrays to skip already ingested sample data [VS-246] (#7862); - KM upload GVS product sheet (#7883); - Default extract scatter width [VS-415] (#7878); - Volatile tasks review [VS-447] (#7880); - Update Quickstart Integration for X/Y scaling changes [VS-464] (#7881); - clean up dockstore; - Rc vs 63 vat sop documentation (#7879); - Fix up FQ and race condition issues with volatile tasks work [VS-478] (#7888); - Use gvs-internal project in integration test (#7901); - Add cost observability BQ table [VS-441] (#7891); - Add preliminary labels to queries [VS-381] (#7902); - Workflow compute costs [VS-472] (#7905); - Fix bug and update images (#7912); - VS 483 Beta user wdl (#7894); - Core storage model cost [VS-473] (#7913); - Update Quickstart & Integration to use re-blocked v2 gVCFs [VS-491] (#7924); - KM GVS documentation (#7903); - Track BigQuery costs of GVS python VS-480 (#7915); - Read cost observability table [VS-475] (#7923); - Fix Race Condition, Add Support for Extract by Array of Sample Names (ie from a Sample Set) (#7917); - Rightsize import batches [VS-486] (#7925); - [AoU DRC] Support uppercase site_ids for reblocking (#7929); - Populate cost metadata for GATK tasks. (#7919); - remove accidentally added input (#7931); - VS_492 - Beta User Jar release (#7934); - Cost WDL should throw on FISS API errors [VS-518] (#7942); - Fix bad check for missing workflow name [VS-520] (#7943); - Remove usage of service account from GvsValidateVAT.wdl (#7937",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:25729,Deployability,Integrat,Integration,25729,"to validate inputs. (#7845); - Compute filter scatter [VS-392] (#7852); - remove withdrawn req (#7844); - Improve import error message [VS-437] (#7855); - Fix Input Validation python notebook (#7853); - Add VAT Validation check that aa_change and exon_number are consistently set. (#7850); - Ingest 10K [VS-344] (#7860); - X/Y chromosome reweighting for better extract shard runtime balance [VS-389] (#7868); - VET Ingest Validation / Allow Ingest of non-VQSR'ed data (#7870); - Fix AoU workflow bugs (#7874); - Curate input arrays to skip already ingested sample data [VS-246] (#7862); - KM upload GVS product sheet (#7883); - Default extract scatter width [VS-415] (#7878); - Volatile tasks review [VS-447] (#7880); - Update Quickstart Integration for X/Y scaling changes [VS-464] (#7881); - clean up dockstore; - Rc vs 63 vat sop documentation (#7879); - Fix up FQ and race condition issues with volatile tasks work [VS-478] (#7888); - Use gvs-internal project in integration test (#7901); - Add cost observability BQ table [VS-441] (#7891); - Add preliminary labels to queries [VS-381] (#7902); - Workflow compute costs [VS-472] (#7905); - Fix bug and update images (#7912); - VS 483 Beta user wdl (#7894); - Core storage model cost [VS-473] (#7913); - Update Quickstart & Integration to use re-blocked v2 gVCFs [VS-491] (#7924); - KM GVS documentation (#7903); - Track BigQuery costs of GVS python VS-480 (#7915); - Read cost observability table [VS-475] (#7923); - Fix Race Condition, Add Support for Extract by Array of Sample Names (ie from a Sample Set) (#7917); - Rightsize import batches [VS-486] (#7925); - [AoU DRC] Support uppercase site_ids for reblocking (#7929); - Populate cost metadata for GATK tasks. (#7919); - remove accidentally added input (#7931); - VS_492 - Beta User Jar release (#7934); - Cost WDL should throw on FISS API errors [VS-518] (#7942); - Fix bad check for missing workflow name [VS-520] (#7943); - Remove usage of service account from GvsValidateVAT.wdl (#7937",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:26250,Deployability,release,release,26250," - Fix up FQ and race condition issues with volatile tasks work [VS-478] (#7888); - Use gvs-internal project in integration test (#7901); - Add cost observability BQ table [VS-441] (#7891); - Add preliminary labels to queries [VS-381] (#7902); - Workflow compute costs [VS-472] (#7905); - Fix bug and update images (#7912); - VS 483 Beta user wdl (#7894); - Core storage model cost [VS-473] (#7913); - Update Quickstart & Integration to use re-blocked v2 gVCFs [VS-491] (#7924); - KM GVS documentation (#7903); - Track BigQuery costs of GVS python VS-480 (#7915); - Read cost observability table [VS-475] (#7923); - Fix Race Condition, Add Support for Extract by Array of Sample Names (ie from a Sample Set) (#7917); - Rightsize import batches [VS-486] (#7925); - [AoU DRC] Support uppercase site_ids for reblocking (#7929); - Populate cost metadata for GATK tasks. (#7919); - remove accidentally added input (#7931); - VS_492 - Beta User Jar release (#7934); - Cost WDL should throw on FISS API errors [VS-518] (#7942); - Fix bad check for missing workflow name [VS-520] (#7943); - Remove usage of service account from GvsValidateVAT.wdl (#7937); - refactoring for testablity (#7946); - More import retries [VS-532] (#7953); - A few last doc changes (#7927); - WDL to extract a single callset cost (BQ only, not Terra) (#7940); - Temporarily swap in Corretto for Temurin as we can't download Temurin. (#7969); - GL-548 - Update CreateVat code to handle samples that do not contain all population groups. (#7965); - Restore Temurin 11 [VS-570] (#7972); - Add table size check to quickstart integration test [VS-501] (#7970); - Consolidate various docs for AoU callset generation into one to rule them all [VS-553] (#7971); - VS-567. Removing usage of ServiceAccount from CreateVat related WDLs (#7974); - WDL to extract Avro files for Hail import [VS-579] (#7981); - Removed usage of service account from WDLs (#7985); - Document steps for GVS cleanup for base use case [VS-586] (#7989); - Change bac",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:26729,Deployability,Update,Update,26729,"v2 gVCFs [VS-491] (#7924); - KM GVS documentation (#7903); - Track BigQuery costs of GVS python VS-480 (#7915); - Read cost observability table [VS-475] (#7923); - Fix Race Condition, Add Support for Extract by Array of Sample Names (ie from a Sample Set) (#7917); - Rightsize import batches [VS-486] (#7925); - [AoU DRC] Support uppercase site_ids for reblocking (#7929); - Populate cost metadata for GATK tasks. (#7919); - remove accidentally added input (#7931); - VS_492 - Beta User Jar release (#7934); - Cost WDL should throw on FISS API errors [VS-518] (#7942); - Fix bad check for missing workflow name [VS-520] (#7943); - Remove usage of service account from GvsValidateVAT.wdl (#7937); - refactoring for testablity (#7946); - More import retries [VS-532] (#7953); - A few last doc changes (#7927); - WDL to extract a single callset cost (BQ only, not Terra) (#7940); - Temporarily swap in Corretto for Temurin as we can't download Temurin. (#7969); - GL-548 - Update CreateVat code to handle samples that do not contain all population groups. (#7965); - Restore Temurin 11 [VS-570] (#7972); - Add table size check to quickstart integration test [VS-501] (#7970); - Consolidate various docs for AoU callset generation into one to rule them all [VS-553] (#7971); - VS-567. Removing usage of ServiceAccount from CreateVat related WDLs (#7974); - WDL to extract Avro files for Hail import [VS-579] (#7981); - Removed usage of service account from WDLs (#7985); - Document steps for GVS cleanup for base use case [VS-586] (#7989); - Change backticks to single quotes in several error messages - causing shell to attempt to execute. (#7995); - VS-598 - Minor update to AoU Documentation. (#7994); - Allow for incremental addition of data to alt_allele [VS-52] (#7993); - Minor AoU Documentation Update (#7999); - Batch population of alt_allele table from vet_ tables [VS-265] (#7998); - Change drop_state to NONE for Ingest/Extract [VS-607] (#8000); - python -> python3 (#8001); - Generate Hail im",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:26897,Deployability,integrat,integration,26897,"x Race Condition, Add Support for Extract by Array of Sample Names (ie from a Sample Set) (#7917); - Rightsize import batches [VS-486] (#7925); - [AoU DRC] Support uppercase site_ids for reblocking (#7929); - Populate cost metadata for GATK tasks. (#7919); - remove accidentally added input (#7931); - VS_492 - Beta User Jar release (#7934); - Cost WDL should throw on FISS API errors [VS-518] (#7942); - Fix bad check for missing workflow name [VS-520] (#7943); - Remove usage of service account from GvsValidateVAT.wdl (#7937); - refactoring for testablity (#7946); - More import retries [VS-532] (#7953); - A few last doc changes (#7927); - WDL to extract a single callset cost (BQ only, not Terra) (#7940); - Temporarily swap in Corretto for Temurin as we can't download Temurin. (#7969); - GL-548 - Update CreateVat code to handle samples that do not contain all population groups. (#7965); - Restore Temurin 11 [VS-570] (#7972); - Add table size check to quickstart integration test [VS-501] (#7970); - Consolidate various docs for AoU callset generation into one to rule them all [VS-553] (#7971); - VS-567. Removing usage of ServiceAccount from CreateVat related WDLs (#7974); - WDL to extract Avro files for Hail import [VS-579] (#7981); - Removed usage of service account from WDLs (#7985); - Document steps for GVS cleanup for base use case [VS-586] (#7989); - Change backticks to single quotes in several error messages - causing shell to attempt to execute. (#7995); - VS-598 - Minor update to AoU Documentation. (#7994); - Allow for incremental addition of data to alt_allele [VS-52] (#7993); - Minor AoU Documentation Update (#7999); - Batch population of alt_allele table from vet_ tables [VS-265] (#7998); - Change drop_state to NONE for Ingest/Extract [VS-607] (#8000); - python -> python3 (#8001); - Generate Hail import/export script [VS-605] (#8002); - clearer error when values are missing (#7939); - Ah [VS-565] output intervals and sample list (#8010); - make CreateAltAlleleT",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:27422,Deployability,update,update,27422,"from GvsValidateVAT.wdl (#7937); - refactoring for testablity (#7946); - More import retries [VS-532] (#7953); - A few last doc changes (#7927); - WDL to extract a single callset cost (BQ only, not Terra) (#7940); - Temporarily swap in Corretto for Temurin as we can't download Temurin. (#7969); - GL-548 - Update CreateVat code to handle samples that do not contain all population groups. (#7965); - Restore Temurin 11 [VS-570] (#7972); - Add table size check to quickstart integration test [VS-501] (#7970); - Consolidate various docs for AoU callset generation into one to rule them all [VS-553] (#7971); - VS-567. Removing usage of ServiceAccount from CreateVat related WDLs (#7974); - WDL to extract Avro files for Hail import [VS-579] (#7981); - Removed usage of service account from WDLs (#7985); - Document steps for GVS cleanup for base use case [VS-586] (#7989); - Change backticks to single quotes in several error messages - causing shell to attempt to execute. (#7995); - VS-598 - Minor update to AoU Documentation. (#7994); - Allow for incremental addition of data to alt_allele [VS-52] (#7993); - Minor AoU Documentation Update (#7999); - Batch population of alt_allele table from vet_ tables [VS-265] (#7998); - Change drop_state to NONE for Ingest/Extract [VS-607] (#8000); - python -> python3 (#8001); - Generate Hail import/export script [VS-605] (#8002); - clearer error when values are missing (#7939); - Ah [VS-565] output intervals and sample list (#8010); - make CreateAltAlleleTable task volatile (#8011); - Restore withdrawn [VS-581] (#8006); - Km gvs add storage cost and cleanup doc (#8012); - Updating documentation to reflect the changed outputs [VS-565] (#8014); - File of callset samples -> samples marked as 'withdrawn' in GVS [VS-436] (#8009); - fix quota guidelines for CPUs (#8016); - Add in ability to tweak sample-every-Nth-variant parameter for SNP model creation (#8019); - add initial notebook copy pasta (#8008); - add sample_table_timestamp to GetNumSamplesL",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:27558,Deployability,Update,Update,27558,ge of ServiceAccount from CreateVat related WDLs (#7974); - WDL to extract Avro files for Hail import [VS-579] (#7981); - Removed usage of service account from WDLs (#7985); - Document steps for GVS cleanup for base use case [VS-586] (#7989); - Change backticks to single quotes in several error messages - causing shell to attempt to execute. (#7995); - VS-598 - Minor update to AoU Documentation. (#7994); - Allow for incremental addition of data to alt_allele [VS-52] (#7993); - Minor AoU Documentation Update (#7999); - Batch population of alt_allele table from vet_ tables [VS-265] (#7998); - Change drop_state to NONE for Ingest/Extract [VS-607] (#8000); - python -> python3 (#8001); - Generate Hail import/export script [VS-605] (#8002); - clearer error when values are missing (#7939); - Ah [VS-565] output intervals and sample list (#8010); - make CreateAltAlleleTable task volatile (#8011); - Restore withdrawn [VS-581] (#8006); - Km gvs add storage cost and cleanup doc (#8012); - Updating documentation to reflect the changed outputs [VS-565] (#8014); - File of callset samples -> samples marked as 'withdrawn' in GVS [VS-436] (#8009); - fix quota guidelines for CPUs (#8016); - Add in ability to tweak sample-every-Nth-variant parameter for SNP model creation (#8019); - add initial notebook copy pasta (#8008); - add sample_table_timestamp to GetNumSamplesLoaded (#8022); - Batched Avro export [VS-630] (#8020); - Updating references to old GATK for VS-620 (#8023); - VS-517 Use standard version of GetBQTableLastModifiedDatetime in GvsValidateVat (#8024); - Fix bug in GvsWithdrawSamples.wdl (#8026); - Ah 617 exposing the drop_state parameter to the GvsJointVariantCalling wdl used for beta (and internal customer) (#8032); - Expose maximum-training-variants VQSR parameter [VS-634] (#8029); - Callset statistics [VS-560] (#8018); - Check for withdrawn before exporting to AVRO files [VS-646] (#8039); - Small updates to GVS Integration WDL [VS-618] (#8042); - Rework Hail script gener,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:28978,Deployability,update,updates,28978,"ility to tweak sample-every-Nth-variant parameter for SNP model creation (#8019); - add initial notebook copy pasta (#8008); - add sample_table_timestamp to GetNumSamplesLoaded (#8022); - Batched Avro export [VS-630] (#8020); - Updating references to old GATK for VS-620 (#8023); - VS-517 Use standard version of GetBQTableLastModifiedDatetime in GvsValidateVat (#8024); - Fix bug in GvsWithdrawSamples.wdl (#8026); - Ah 617 exposing the drop_state parameter to the GvsJointVariantCalling wdl used for beta (and internal customer) (#8032); - Expose maximum-training-variants VQSR parameter [VS-634] (#8029); - Callset statistics [VS-560] (#8018); - Check for withdrawn before exporting to AVRO files [VS-646] (#8039); - Small updates to GVS Integration WDL [VS-618] (#8042); - Rework Hail script generation [VS-616] (#8034); - Alpine based Variant Store Docker image [VS-648] (#8044); - update warp version (#7906); - Fail Avro extract and callset stats on bad filter name [VS-655] (#8046); - Vs 629 failure to retrieve job information during ingest (#8047); - Restore accidentally removed bcftools [VS-661] (#8051); - Allowing our pipeline to function with a sample size of one (#8055); - Vs 665 re create vcf for cd 68 po 52339 with ad padding fixed (#8057); - VS-665 and VS-620 updating code to use latest docker images containing Rori's AD calculation changes in extract (#8061); - updating the beta workflow to use the latest jar, representing the version of GATK George tested against the workflow (#8062); - VS-637 Address a couple of issues in SampleLoadStatus handling in GVSImportGenomes. (#8052); - Revert Alpinizing of apt dependent task [VS-688] (#8065); - Fix missing vat schema JSONs [VS-699] (#8072); - Fix integration expectations for fixed AD [VS-689] (#8066); - VS-698 Remove unnecessary columns from Call set statistics (#8073); - Fix Dockerfile nits that break 20.10.21 (#8078); - Nirvana 3.18.1 Docker images support [VS-661] (#8082); - Add option to not prepare __REF_DATA or __",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:28993,Deployability,Integrat,Integration,28993,"ility to tweak sample-every-Nth-variant parameter for SNP model creation (#8019); - add initial notebook copy pasta (#8008); - add sample_table_timestamp to GetNumSamplesLoaded (#8022); - Batched Avro export [VS-630] (#8020); - Updating references to old GATK for VS-620 (#8023); - VS-517 Use standard version of GetBQTableLastModifiedDatetime in GvsValidateVat (#8024); - Fix bug in GvsWithdrawSamples.wdl (#8026); - Ah 617 exposing the drop_state parameter to the GvsJointVariantCalling wdl used for beta (and internal customer) (#8032); - Expose maximum-training-variants VQSR parameter [VS-634] (#8029); - Callset statistics [VS-560] (#8018); - Check for withdrawn before exporting to AVRO files [VS-646] (#8039); - Small updates to GVS Integration WDL [VS-618] (#8042); - Rework Hail script generation [VS-616] (#8034); - Alpine based Variant Store Docker image [VS-648] (#8044); - update warp version (#7906); - Fail Avro extract and callset stats on bad filter name [VS-655] (#8046); - Vs 629 failure to retrieve job information during ingest (#8047); - Restore accidentally removed bcftools [VS-661] (#8051); - Allowing our pipeline to function with a sample size of one (#8055); - Vs 665 re create vcf for cd 68 po 52339 with ad padding fixed (#8057); - VS-665 and VS-620 updating code to use latest docker images containing Rori's AD calculation changes in extract (#8061); - updating the beta workflow to use the latest jar, representing the version of GATK George tested against the workflow (#8062); - VS-637 Address a couple of issues in SampleLoadStatus handling in GVSImportGenomes. (#8052); - Revert Alpinizing of apt dependent task [VS-688] (#8065); - Fix missing vat schema JSONs [VS-699] (#8072); - Fix integration expectations for fixed AD [VS-689] (#8066); - VS-698 Remove unnecessary columns from Call set statistics (#8073); - Fix Dockerfile nits that break 20.10.21 (#8078); - Nirvana 3.18.1 Docker images support [VS-661] (#8082); - Add option to not prepare __REF_DATA or __",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:29139,Deployability,update,update,29139,"ility to tweak sample-every-Nth-variant parameter for SNP model creation (#8019); - add initial notebook copy pasta (#8008); - add sample_table_timestamp to GetNumSamplesLoaded (#8022); - Batched Avro export [VS-630] (#8020); - Updating references to old GATK for VS-620 (#8023); - VS-517 Use standard version of GetBQTableLastModifiedDatetime in GvsValidateVat (#8024); - Fix bug in GvsWithdrawSamples.wdl (#8026); - Ah 617 exposing the drop_state parameter to the GvsJointVariantCalling wdl used for beta (and internal customer) (#8032); - Expose maximum-training-variants VQSR parameter [VS-634] (#8029); - Callset statistics [VS-560] (#8018); - Check for withdrawn before exporting to AVRO files [VS-646] (#8039); - Small updates to GVS Integration WDL [VS-618] (#8042); - Rework Hail script generation [VS-616] (#8034); - Alpine based Variant Store Docker image [VS-648] (#8044); - update warp version (#7906); - Fail Avro extract and callset stats on bad filter name [VS-655] (#8046); - Vs 629 failure to retrieve job information during ingest (#8047); - Restore accidentally removed bcftools [VS-661] (#8051); - Allowing our pipeline to function with a sample size of one (#8055); - Vs 665 re create vcf for cd 68 po 52339 with ad padding fixed (#8057); - VS-665 and VS-620 updating code to use latest docker images containing Rori's AD calculation changes in extract (#8061); - updating the beta workflow to use the latest jar, representing the version of GATK George tested against the workflow (#8062); - VS-637 Address a couple of issues in SampleLoadStatus handling in GVSImportGenomes. (#8052); - Revert Alpinizing of apt dependent task [VS-688] (#8065); - Fix missing vat schema JSONs [VS-699] (#8072); - Fix integration expectations for fixed AD [VS-689] (#8066); - VS-698 Remove unnecessary columns from Call set statistics (#8073); - Fix Dockerfile nits that break 20.10.21 (#8078); - Nirvana 3.18.1 Docker images support [VS-661] (#8082); - Add option to not prepare __REF_DATA or __",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:29384,Deployability,pipeline,pipeline,29384,"ility to tweak sample-every-Nth-variant parameter for SNP model creation (#8019); - add initial notebook copy pasta (#8008); - add sample_table_timestamp to GetNumSamplesLoaded (#8022); - Batched Avro export [VS-630] (#8020); - Updating references to old GATK for VS-620 (#8023); - VS-517 Use standard version of GetBQTableLastModifiedDatetime in GvsValidateVat (#8024); - Fix bug in GvsWithdrawSamples.wdl (#8026); - Ah 617 exposing the drop_state parameter to the GvsJointVariantCalling wdl used for beta (and internal customer) (#8032); - Expose maximum-training-variants VQSR parameter [VS-634] (#8029); - Callset statistics [VS-560] (#8018); - Check for withdrawn before exporting to AVRO files [VS-646] (#8039); - Small updates to GVS Integration WDL [VS-618] (#8042); - Rework Hail script generation [VS-616] (#8034); - Alpine based Variant Store Docker image [VS-648] (#8044); - update warp version (#7906); - Fail Avro extract and callset stats on bad filter name [VS-655] (#8046); - Vs 629 failure to retrieve job information during ingest (#8047); - Restore accidentally removed bcftools [VS-661] (#8051); - Allowing our pipeline to function with a sample size of one (#8055); - Vs 665 re create vcf for cd 68 po 52339 with ad padding fixed (#8057); - VS-665 and VS-620 updating code to use latest docker images containing Rori's AD calculation changes in extract (#8061); - updating the beta workflow to use the latest jar, representing the version of GATK George tested against the workflow (#8062); - VS-637 Address a couple of issues in SampleLoadStatus handling in GVSImportGenomes. (#8052); - Revert Alpinizing of apt dependent task [VS-688] (#8065); - Fix missing vat schema JSONs [VS-699] (#8072); - Fix integration expectations for fixed AD [VS-689] (#8066); - VS-698 Remove unnecessary columns from Call set statistics (#8073); - Fix Dockerfile nits that break 20.10.21 (#8078); - Nirvana 3.18.1 Docker images support [VS-661] (#8082); - Add option to not prepare __REF_DATA or __",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:29975,Deployability,integrat,integration,29975,"Integration WDL [VS-618] (#8042); - Rework Hail script generation [VS-616] (#8034); - Alpine based Variant Store Docker image [VS-648] (#8044); - update warp version (#7906); - Fail Avro extract and callset stats on bad filter name [VS-655] (#8046); - Vs 629 failure to retrieve job information during ingest (#8047); - Restore accidentally removed bcftools [VS-661] (#8051); - Allowing our pipeline to function with a sample size of one (#8055); - Vs 665 re create vcf for cd 68 po 52339 with ad padding fixed (#8057); - VS-665 and VS-620 updating code to use latest docker images containing Rori's AD calculation changes in extract (#8061); - updating the beta workflow to use the latest jar, representing the version of GATK George tested against the workflow (#8062); - VS-637 Address a couple of issues in SampleLoadStatus handling in GVSImportGenomes. (#8052); - Revert Alpinizing of apt dependent task [VS-688] (#8065); - Fix missing vat schema JSONs [VS-699] (#8072); - Fix integration expectations for fixed AD [VS-689] (#8066); - VS-698 Remove unnecessary columns from Call set statistics (#8073); - Fix Dockerfile nits that break 20.10.21 (#8078); - Nirvana 3.18.1 Docker images support [VS-661] (#8082); - Add option to not prepare __REF_DATA or __SAMPLES tables to Prepare [VS-697] (#8079); - ""build-base"" Docker image for faster variantstore image builds [VS-712] (#8085); - GVS / Hail VDS integration test [VS-639] (#8086); - Remove AI/AN from VDS docs [VS-726] (#8096); - Add flag for cost_observability table writing to support sub-cohort use case [VS-521] (#8093); - Document STS delivery process for VDS [VS-727] (#8101); - delete obsolete callset_QC directory and its contents [VS-318] (#8108); - doc link typo and add check for control samples in AVRO export (#8110); - Add defaults for scatter_count in GvsExtractCohortFromSampleNames [VS-496] (#8109); - Escape table names properly in ValidateVat WDL (#8116); - Vs 741 fix indefinite freeze in split intervals task when using ex",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:30397,Deployability,integrat,integration,30397,"ge tested against the workflow (#8062); - VS-637 Address a couple of issues in SampleLoadStatus handling in GVSImportGenomes. (#8052); - Revert Alpinizing of apt dependent task [VS-688] (#8065); - Fix missing vat schema JSONs [VS-699] (#8072); - Fix integration expectations for fixed AD [VS-689] (#8066); - VS-698 Remove unnecessary columns from Call set statistics (#8073); - Fix Dockerfile nits that break 20.10.21 (#8078); - Nirvana 3.18.1 Docker images support [VS-661] (#8082); - Add option to not prepare __REF_DATA or __SAMPLES tables to Prepare [VS-697] (#8079); - ""build-base"" Docker image for faster variantstore image builds [VS-712] (#8085); - GVS / Hail VDS integration test [VS-639] (#8086); - Remove AI/AN from VDS docs [VS-726] (#8096); - Add flag for cost_observability table writing to support sub-cohort use case [VS-521] (#8093); - Document STS delivery process for VDS [VS-727] (#8101); - delete obsolete callset_QC directory and its contents [VS-318] (#8108); - doc link typo and add check for control samples in AVRO export (#8110); - Add defaults for scatter_count in GvsExtractCohortFromSampleNames [VS-496] (#8109); - Escape table names properly in ValidateVat WDL (#8116); - Vs 741 fix indefinite freeze in split intervals task when using exome data (#8113); - VAT Readme updates (#8090); - WDL and python scripts to use the VDS in the VAT (#8077); - VS-757 - Use JASIX to make sub-jsons of annotated output of Nirvana (#8133); - add note about permissions for P&S workflow to work (#8135); - VS-759 (and VS-760) (#8137); - VS-765. Scatter the RemoveDuplicates task. (#8144); - update delivery docs based on latest VDS delivery run [VS-770] (#8150); - Add monitoring to index vcf (#8151); - Make some noise when VDS validation succeeds (#8155); - Handle empty genes annotation file. (#8153); - Add escapes for otherwise problematic dataset / table names. (#8162); - New WDL to create VAT tsvs from previously generated BigQuery table. (#8165); - Treat withdrawn samples in ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:31025,Deployability,update,updates,31025,"ge tested against the workflow (#8062); - VS-637 Address a couple of issues in SampleLoadStatus handling in GVSImportGenomes. (#8052); - Revert Alpinizing of apt dependent task [VS-688] (#8065); - Fix missing vat schema JSONs [VS-699] (#8072); - Fix integration expectations for fixed AD [VS-689] (#8066); - VS-698 Remove unnecessary columns from Call set statistics (#8073); - Fix Dockerfile nits that break 20.10.21 (#8078); - Nirvana 3.18.1 Docker images support [VS-661] (#8082); - Add option to not prepare __REF_DATA or __SAMPLES tables to Prepare [VS-697] (#8079); - ""build-base"" Docker image for faster variantstore image builds [VS-712] (#8085); - GVS / Hail VDS integration test [VS-639] (#8086); - Remove AI/AN from VDS docs [VS-726] (#8096); - Add flag for cost_observability table writing to support sub-cohort use case [VS-521] (#8093); - Document STS delivery process for VDS [VS-727] (#8101); - delete obsolete callset_QC directory and its contents [VS-318] (#8108); - doc link typo and add check for control samples in AVRO export (#8110); - Add defaults for scatter_count in GvsExtractCohortFromSampleNames [VS-496] (#8109); - Escape table names properly in ValidateVat WDL (#8116); - Vs 741 fix indefinite freeze in split intervals task when using exome data (#8113); - VAT Readme updates (#8090); - WDL and python scripts to use the VDS in the VAT (#8077); - VS-757 - Use JASIX to make sub-jsons of annotated output of Nirvana (#8133); - add note about permissions for P&S workflow to work (#8135); - VS-759 (and VS-760) (#8137); - VS-765. Scatter the RemoveDuplicates task. (#8144); - update delivery docs based on latest VDS delivery run [VS-770] (#8150); - Add monitoring to index vcf (#8151); - Make some noise when VDS validation succeeds (#8155); - Handle empty genes annotation file. (#8153); - Add escapes for otherwise problematic dataset / table names. (#8162); - New WDL to create VAT tsvs from previously generated BigQuery table. (#8165); - Treat withdrawn samples in ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:31331,Deployability,update,update,31331,"86); - Remove AI/AN from VDS docs [VS-726] (#8096); - Add flag for cost_observability table writing to support sub-cohort use case [VS-521] (#8093); - Document STS delivery process for VDS [VS-727] (#8101); - delete obsolete callset_QC directory and its contents [VS-318] (#8108); - doc link typo and add check for control samples in AVRO export (#8110); - Add defaults for scatter_count in GvsExtractCohortFromSampleNames [VS-496] (#8109); - Escape table names properly in ValidateVat WDL (#8116); - Vs 741 fix indefinite freeze in split intervals task when using exome data (#8113); - VAT Readme updates (#8090); - WDL and python scripts to use the VDS in the VAT (#8077); - VS-757 - Use JASIX to make sub-jsons of annotated output of Nirvana (#8133); - add note about permissions for P&S workflow to work (#8135); - VS-759 (and VS-760) (#8137); - VS-765. Scatter the RemoveDuplicates task. (#8144); - update delivery docs based on latest VDS delivery run [VS-770] (#8150); - Add monitoring to index vcf (#8151); - Make some noise when VDS validation succeeds (#8155); - Handle empty genes annotation file. (#8153); - Add escapes for otherwise problematic dataset / table names. (#8162); - New WDL to create VAT tsvs from previously generated BigQuery table. (#8165); - Treat withdrawn samples in sub-cohort prepare correctly [VS-772] (#8156); - Remove unused VAT Creation WDL (#8172); - Gg consistently use dataset name as input parameter (#8173); - AoU cleanup docs, round 1 [VS-671] (#8104); - VDS docs remove samples and correct GT [VS-807] (#8178); - [VS-693] Add support for VQSR Lite to GvsCreateFilterSet (#8157); - VAT Documentation Update Round 1 [VS-531]; - VS-530 VDS creation documentation for AoU (#8169); - Update beta docs to tell people not to use free credits (#8184); - VS-816 Keeping ingestion under quota (#8193); - CromwellOnAzure + Azure SQL DB + AAD first steps doc [VS-805] (#8191); - Edit and re-format VDS -> VAT doc [VS-821] (#8187); - VS-820 Incorporate code to stay un",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:32071,Deployability,Update,Update,32071,"run [VS-770] (#8150); - Add monitoring to index vcf (#8151); - Make some noise when VDS validation succeeds (#8155); - Handle empty genes annotation file. (#8153); - Add escapes for otherwise problematic dataset / table names. (#8162); - New WDL to create VAT tsvs from previously generated BigQuery table. (#8165); - Treat withdrawn samples in sub-cohort prepare correctly [VS-772] (#8156); - Remove unused VAT Creation WDL (#8172); - Gg consistently use dataset name as input parameter (#8173); - AoU cleanup docs, round 1 [VS-671] (#8104); - VDS docs remove samples and correct GT [VS-807] (#8178); - [VS-693] Add support for VQSR Lite to GvsCreateFilterSet (#8157); - VAT Documentation Update Round 1 [VS-531]; - VS-530 VDS creation documentation for AoU (#8169); - Update beta docs to tell people not to use free credits (#8184); - VS-816 Keeping ingestion under quota (#8193); - CromwellOnAzure + Azure SQL DB + AAD first steps doc [VS-805] (#8191); - Edit and re-format VDS -> VAT doc [VS-821] (#8187); - VS-820 Incorporate code to stay under Google quotas for new accounts into beta workflow (#8200); - Update docs for Nirvana reference disk [VS-531] [VS-796] (#8170); - VS-694 - Extract Callset for VQSR Lite (#8182); - Updating docker image (#8210); - Document VCF generation [VS-795] (#8202); - Variants GATK Docker image building docs + script [VS-827] (#8207); - Update GATK jar used in GvsJointVariantCalling WDL (#8216); - Hello Azure SQL Database from Cromwell on Azure [VS-812] (#8220); - Remove what appear to be accidentally added files [VS-834] (#8225); - VS-815: Add Support for YNG to VQSR Lite (#8206); - Disentangle non-GVS code from GVS code [VS-834] (#8229); - VS-695. Updates to run Precision and Sensitivity on VQSR Lite (#8230); - Track avro export costs [VS-769] (#8236); - Add note that we deleted a VDS! (#8214); - Vs 822 Add documentation for the work that we did on the latest iteration of Delta (#8205); - Rc vs 822 gq0 documentation (#8240); - Add a test exclusion",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:32151,Deployability,Update,Update,32151,"run [VS-770] (#8150); - Add monitoring to index vcf (#8151); - Make some noise when VDS validation succeeds (#8155); - Handle empty genes annotation file. (#8153); - Add escapes for otherwise problematic dataset / table names. (#8162); - New WDL to create VAT tsvs from previously generated BigQuery table. (#8165); - Treat withdrawn samples in sub-cohort prepare correctly [VS-772] (#8156); - Remove unused VAT Creation WDL (#8172); - Gg consistently use dataset name as input parameter (#8173); - AoU cleanup docs, round 1 [VS-671] (#8104); - VDS docs remove samples and correct GT [VS-807] (#8178); - [VS-693] Add support for VQSR Lite to GvsCreateFilterSet (#8157); - VAT Documentation Update Round 1 [VS-531]; - VS-530 VDS creation documentation for AoU (#8169); - Update beta docs to tell people not to use free credits (#8184); - VS-816 Keeping ingestion under quota (#8193); - CromwellOnAzure + Azure SQL DB + AAD first steps doc [VS-805] (#8191); - Edit and re-format VDS -> VAT doc [VS-821] (#8187); - VS-820 Incorporate code to stay under Google quotas for new accounts into beta workflow (#8200); - Update docs for Nirvana reference disk [VS-531] [VS-796] (#8170); - VS-694 - Extract Callset for VQSR Lite (#8182); - Updating docker image (#8210); - Document VCF generation [VS-795] (#8202); - Variants GATK Docker image building docs + script [VS-827] (#8207); - Update GATK jar used in GvsJointVariantCalling WDL (#8216); - Hello Azure SQL Database from Cromwell on Azure [VS-812] (#8220); - Remove what appear to be accidentally added files [VS-834] (#8225); - VS-815: Add Support for YNG to VQSR Lite (#8206); - Disentangle non-GVS code from GVS code [VS-834] (#8229); - VS-695. Updates to run Precision and Sensitivity on VQSR Lite (#8230); - Track avro export costs [VS-769] (#8236); - Add note that we deleted a VDS! (#8214); - Vs 822 Add documentation for the work that we did on the latest iteration of Delta (#8205); - Rc vs 822 gq0 documentation (#8240); - Add a test exclusion",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:32492,Deployability,Update,Update,32492,"run [VS-770] (#8150); - Add monitoring to index vcf (#8151); - Make some noise when VDS validation succeeds (#8155); - Handle empty genes annotation file. (#8153); - Add escapes for otherwise problematic dataset / table names. (#8162); - New WDL to create VAT tsvs from previously generated BigQuery table. (#8165); - Treat withdrawn samples in sub-cohort prepare correctly [VS-772] (#8156); - Remove unused VAT Creation WDL (#8172); - Gg consistently use dataset name as input parameter (#8173); - AoU cleanup docs, round 1 [VS-671] (#8104); - VDS docs remove samples and correct GT [VS-807] (#8178); - [VS-693] Add support for VQSR Lite to GvsCreateFilterSet (#8157); - VAT Documentation Update Round 1 [VS-531]; - VS-530 VDS creation documentation for AoU (#8169); - Update beta docs to tell people not to use free credits (#8184); - VS-816 Keeping ingestion under quota (#8193); - CromwellOnAzure + Azure SQL DB + AAD first steps doc [VS-805] (#8191); - Edit and re-format VDS -> VAT doc [VS-821] (#8187); - VS-820 Incorporate code to stay under Google quotas for new accounts into beta workflow (#8200); - Update docs for Nirvana reference disk [VS-531] [VS-796] (#8170); - VS-694 - Extract Callset for VQSR Lite (#8182); - Updating docker image (#8210); - Document VCF generation [VS-795] (#8202); - Variants GATK Docker image building docs + script [VS-827] (#8207); - Update GATK jar used in GvsJointVariantCalling WDL (#8216); - Hello Azure SQL Database from Cromwell on Azure [VS-812] (#8220); - Remove what appear to be accidentally added files [VS-834] (#8225); - VS-815: Add Support for YNG to VQSR Lite (#8206); - Disentangle non-GVS code from GVS code [VS-834] (#8229); - VS-695. Updates to run Precision and Sensitivity on VQSR Lite (#8230); - Track avro export costs [VS-769] (#8236); - Add note that we deleted a VDS! (#8214); - Vs 822 Add documentation for the work that we did on the latest iteration of Delta (#8205); - Rc vs 822 gq0 documentation (#8240); - Add a test exclusion",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:32757,Deployability,Update,Update,32757,"run [VS-770] (#8150); - Add monitoring to index vcf (#8151); - Make some noise when VDS validation succeeds (#8155); - Handle empty genes annotation file. (#8153); - Add escapes for otherwise problematic dataset / table names. (#8162); - New WDL to create VAT tsvs from previously generated BigQuery table. (#8165); - Treat withdrawn samples in sub-cohort prepare correctly [VS-772] (#8156); - Remove unused VAT Creation WDL (#8172); - Gg consistently use dataset name as input parameter (#8173); - AoU cleanup docs, round 1 [VS-671] (#8104); - VDS docs remove samples and correct GT [VS-807] (#8178); - [VS-693] Add support for VQSR Lite to GvsCreateFilterSet (#8157); - VAT Documentation Update Round 1 [VS-531]; - VS-530 VDS creation documentation for AoU (#8169); - Update beta docs to tell people not to use free credits (#8184); - VS-816 Keeping ingestion under quota (#8193); - CromwellOnAzure + Azure SQL DB + AAD first steps doc [VS-805] (#8191); - Edit and re-format VDS -> VAT doc [VS-821] (#8187); - VS-820 Incorporate code to stay under Google quotas for new accounts into beta workflow (#8200); - Update docs for Nirvana reference disk [VS-531] [VS-796] (#8170); - VS-694 - Extract Callset for VQSR Lite (#8182); - Updating docker image (#8210); - Document VCF generation [VS-795] (#8202); - Variants GATK Docker image building docs + script [VS-827] (#8207); - Update GATK jar used in GvsJointVariantCalling WDL (#8216); - Hello Azure SQL Database from Cromwell on Azure [VS-812] (#8220); - Remove what appear to be accidentally added files [VS-834] (#8225); - VS-815: Add Support for YNG to VQSR Lite (#8206); - Disentangle non-GVS code from GVS code [VS-834] (#8229); - VS-695. Updates to run Precision and Sensitivity on VQSR Lite (#8230); - Track avro export costs [VS-769] (#8236); - Add note that we deleted a VDS! (#8214); - Vs 822 Add documentation for the work that we did on the latest iteration of Delta (#8205); - Rc vs 822 gq0 documentation (#8240); - Add a test exclusion",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:33076,Deployability,Update,Updates,33076," dataset / table names. (#8162); - New WDL to create VAT tsvs from previously generated BigQuery table. (#8165); - Treat withdrawn samples in sub-cohort prepare correctly [VS-772] (#8156); - Remove unused VAT Creation WDL (#8172); - Gg consistently use dataset name as input parameter (#8173); - AoU cleanup docs, round 1 [VS-671] (#8104); - VDS docs remove samples and correct GT [VS-807] (#8178); - [VS-693] Add support for VQSR Lite to GvsCreateFilterSet (#8157); - VAT Documentation Update Round 1 [VS-531]; - VS-530 VDS creation documentation for AoU (#8169); - Update beta docs to tell people not to use free credits (#8184); - VS-816 Keeping ingestion under quota (#8193); - CromwellOnAzure + Azure SQL DB + AAD first steps doc [VS-805] (#8191); - Edit and re-format VDS -> VAT doc [VS-821] (#8187); - VS-820 Incorporate code to stay under Google quotas for new accounts into beta workflow (#8200); - Update docs for Nirvana reference disk [VS-531] [VS-796] (#8170); - VS-694 - Extract Callset for VQSR Lite (#8182); - Updating docker image (#8210); - Document VCF generation [VS-795] (#8202); - Variants GATK Docker image building docs + script [VS-827] (#8207); - Update GATK jar used in GvsJointVariantCalling WDL (#8216); - Hello Azure SQL Database from Cromwell on Azure [VS-812] (#8220); - Remove what appear to be accidentally added files [VS-834] (#8225); - VS-815: Add Support for YNG to VQSR Lite (#8206); - Disentangle non-GVS code from GVS code [VS-834] (#8229); - VS-695. Updates to run Precision and Sensitivity on VQSR Lite (#8230); - Track avro export costs [VS-769] (#8236); - Add note that we deleted a VDS! (#8214); - Vs 822 Add documentation for the work that we did on the latest iteration of Delta (#8205); - Rc vs 822 gq0 documentation (#8240); - Add a test exclusion for gvs scripts; - testing if the exclusion works. [VS-16]: https://broadworkbench.atlassian.net/browse/VS-16?atlOrigin=eyJpIjoiNWRkNTljNzYxNjVmNDY3MDlhMDU5Y2ZhYzA5YTRkZjUiLCJwIjoiZ2l0aHViLWNvbS1KU1cifQ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:5765,Energy Efficiency,Reduce,Reduce,5765," file for warp pipeline; - reverting logging; - included top level WDL; - use gnarly with BQ extract cohort; - remove unused file; - cleaning up; - tidy; - tidy up before PR; - tidy up before PR; - PR comments; - merge conflict misfires; - added example SQL to create alt allele table from VET; - option to remove PLs; - fixed and enhanced unit test; - removing unused config, causing travis to fail; - add CreateVariantIngestFiles integration test (#7071); - add sampleName (instead of NULL) to error message (#7074); - Update To handle if no data error (#7084); - Memory improvement when writing missing positions to pet (#7098); - added support for loading QUALapprox into VET (#7101); - Add -m flag to gsutil step; add dockstore branch filters to facilitate development (#7104); - updates to ImportGenomes and LoadBigQueryData (#7112); - Add ngs to cohort extract Dockerfile; remove exception catching in extract python script (#7113); - remove problematic storage_location imports (#7119); - Reduce memory and CPU for CreateImportTsvs task, check for files before attempting load (#7121); - add -m flag to gsutil mv step (#7129); - ah_var_store : Add sample file argument to cohort extract (#7117); - wip; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - forgot this file; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691);",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:11737,Energy Efficiency,Reduce,Reduce,11737," table creation and data loading in LoadData (#7056); - WIP; - tieout scripts; - notes files; - updated diff scripts; - fixed bug...; - add wdl and inputs file for warp pipeline; - reverting logging; - included top level WDL; - use gnarly with BQ extract cohort; - remove unused file; - cleaning up; - tidy; - tidy up before PR; - tidy up before PR; - PR comments; - merge conflict misfires; - added example SQL to create alt allele table from VET; - option to remove PLs; - fixed and enhanced unit test; - removing unused config, causing travis to fail; - add CreateVariantIngestFiles integration test (#7071); - add sampleName (instead of NULL) to error message (#7074); - Update To handle if no data error (#7084); - Memory improvement when writing missing positions to pet (#7098); - added support for loading QUALapprox into VET (#7101); - Add -m flag to gsutil step; add dockstore branch filters to facilitate development (#7104); - updates to ImportGenomes and LoadBigQueryData (#7112); - Add ngs to cohort extract Dockerfile; remove exception catching in extract python script (#7113); - remove problematic storage_location imports (#7119); - Reduce memory and CPU for CreateImportTsvs task, check for files before attempting load (#7121); - add -m flag to gsutil mv step (#7129); - ah_var_store : Add sample file argument to cohort extract (#7117); - Perform full WGS cohort extract scientific tieout for 35 ACMG59 samples (#7106); - Enable Read/Execution Project for BQ Queries (#7136); - ah - optional service account (#7140); - Add load lock file to prevent accidental re-loading of data to BQ (#7138); - #251 Address gvcf no-calls missing QUALapprox and other features (#7146); - Job Add labels to BQ operations from GATK (Issues-199) (#7115); - parse map to list to avoid brackets and spaces in vcf output (#7168); - #259 Inline schema for importgenomes.wdl (#7171); - Created AvroFileReader and unittest, Update ExtractCohort and ExtractCohortEngine (#7174); - #224 Import WDL: handle ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:14125,Energy Efficiency,reduce,reduce,14125,"ut allow NONE as input (#7206); - SA support and consistent naming for all GVS WDLs (#7205); - fix GvsExtractCallset inputs file (#7210); - add clustering to tables (#7207); - add vqsr cutoffs to GvsExtractCallset wdl; clean up dockstore yml (#7209); - Avro test (#7192); - Enable call caching of TSV generation in GvsImportGenomes (#7226); - 266 Clean up ExtractCohort -- remove query mode param (#7227); - 288 Add an excess alleles param (#7221); - take sample name as a param (#7236); - How to run GIAB comparisons (#7237); - Update GvsCreateFilterSet.wdl (#7239); - Use GatherVcfsCloud in GvsCreateFilterSet.wdl (#7241); - parameterize TTL with defaults, reduce memory allocation (#7244); - Addressing OOM in CohortExtract (#7245); - make outputs optional, change case in output (#7252); - Support for FORMAT/FT VQSLod Filtering and cohort-wide LowQual filter (#7248); - removed arrays code, renamed packages (#7260); - 279 labels (#7233); - add conda commands to GIAB readme (#7268); - remove gvs branch (#7263); - remove gvs branch (#7263); - upgrade bq libraries (#7264); - #299 - Sample list ease of use for cohort extracts (#7272); - check for duplicate ids (#7273); - Rc 274 passing sites only (#7275); - added default value to drop_state; broadinstitute/dsp-spec-ops#310 (#7278); - version bump for reliability (#7284); - add timestamp check to ExtractTask call https://github.com/broadinstitute/dsp-spec-ops/issues/320; - serial inserts for scaling prepare, factored out sample name (#7288); - Remove training sites only param from ExtractFeatures broadinstitute/dsp-spec-ops#261; - add param for mem for indels (#7282); - Ah prepare localize option (#7299); - Export sites only vcf STEP 1-- 317 add AC, AN, AF to the final VCF (#7279); - AoU GVS Cohort Extract wdl (#7242); - reliability (#7310); - bump to include FT tag filtering (#7316); - First pass at a Terra QuickStart (#7267); - Ah fix timestamp query (#7319); - 313 Cleanup Extract Cohort params (#7293); - bump bq storage versi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:31409,Energy Efficiency,monitor,monitoring,31409,"86); - Remove AI/AN from VDS docs [VS-726] (#8096); - Add flag for cost_observability table writing to support sub-cohort use case [VS-521] (#8093); - Document STS delivery process for VDS [VS-727] (#8101); - delete obsolete callset_QC directory and its contents [VS-318] (#8108); - doc link typo and add check for control samples in AVRO export (#8110); - Add defaults for scatter_count in GvsExtractCohortFromSampleNames [VS-496] (#8109); - Escape table names properly in ValidateVat WDL (#8116); - Vs 741 fix indefinite freeze in split intervals task when using exome data (#8113); - VAT Readme updates (#8090); - WDL and python scripts to use the VDS in the VAT (#8077); - VS-757 - Use JASIX to make sub-jsons of annotated output of Nirvana (#8133); - add note about permissions for P&S workflow to work (#8135); - VS-759 (and VS-760) (#8137); - VS-765. Scatter the RemoveDuplicates task. (#8144); - update delivery docs based on latest VDS delivery run [VS-770] (#8150); - Add monitoring to index vcf (#8151); - Make some noise when VDS validation succeeds (#8155); - Handle empty genes annotation file. (#8153); - Add escapes for otherwise problematic dataset / table names. (#8162); - New WDL to create VAT tsvs from previously generated BigQuery table. (#8165); - Treat withdrawn samples in sub-cohort prepare correctly [VS-772] (#8156); - Remove unused VAT Creation WDL (#8172); - Gg consistently use dataset name as input parameter (#8173); - AoU cleanup docs, round 1 [VS-671] (#8104); - VDS docs remove samples and correct GT [VS-807] (#8178); - [VS-693] Add support for VQSR Lite to GvsCreateFilterSet (#8157); - VAT Documentation Update Round 1 [VS-531]; - VS-530 VDS creation documentation for AoU (#8169); - Update beta docs to tell people not to use free credits (#8184); - VS-816 Keeping ingestion under quota (#8193); - CromwellOnAzure + Azure SQL DB + AAD first steps doc [VS-805] (#8191); - Edit and re-format VDS -> VAT doc [VS-821] (#8187); - VS-820 Incorporate code to stay un",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:5200,Integrability,integrat,integration,5200,,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:5270,Integrability,message,message,5270,,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:11172,Integrability,integrat,integration,11172," table creation and data loading in LoadData (#7056); - WIP; - tieout scripts; - notes files; - updated diff scripts; - fixed bug...; - add wdl and inputs file for warp pipeline; - reverting logging; - included top level WDL; - use gnarly with BQ extract cohort; - remove unused file; - cleaning up; - tidy; - tidy up before PR; - tidy up before PR; - PR comments; - merge conflict misfires; - added example SQL to create alt allele table from VET; - option to remove PLs; - fixed and enhanced unit test; - removing unused config, causing travis to fail; - add CreateVariantIngestFiles integration test (#7071); - add sampleName (instead of NULL) to error message (#7074); - Update To handle if no data error (#7084); - Memory improvement when writing missing positions to pet (#7098); - added support for loading QUALapprox into VET (#7101); - Add -m flag to gsutil step; add dockstore branch filters to facilitate development (#7104); - updates to ImportGenomes and LoadBigQueryData (#7112); - Add ngs to cohort extract Dockerfile; remove exception catching in extract python script (#7113); - remove problematic storage_location imports (#7119); - Reduce memory and CPU for CreateImportTsvs task, check for files before attempting load (#7121); - add -m flag to gsutil mv step (#7129); - ah_var_store : Add sample file argument to cohort extract (#7117); - Perform full WGS cohort extract scientific tieout for 35 ACMG59 samples (#7106); - Enable Read/Execution Project for BQ Queries (#7136); - ah - optional service account (#7140); - Add load lock file to prevent accidental re-loading of data to BQ (#7138); - #251 Address gvcf no-calls missing QUALapprox and other features (#7146); - Job Add labels to BQ operations from GATK (Issues-199) (#7115); - parse map to list to avoid brackets and spaces in vcf output (#7168); - #259 Inline schema for importgenomes.wdl (#7171); - Created AvroFileReader and unittest, Update ExtractCohort and ExtractCohortEngine (#7174); - #224 Import WDL: handle ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:11242,Integrability,message,message,11242," table creation and data loading in LoadData (#7056); - WIP; - tieout scripts; - notes files; - updated diff scripts; - fixed bug...; - add wdl and inputs file for warp pipeline; - reverting logging; - included top level WDL; - use gnarly with BQ extract cohort; - remove unused file; - cleaning up; - tidy; - tidy up before PR; - tidy up before PR; - PR comments; - merge conflict misfires; - added example SQL to create alt allele table from VET; - option to remove PLs; - fixed and enhanced unit test; - removing unused config, causing travis to fail; - add CreateVariantIngestFiles integration test (#7071); - add sampleName (instead of NULL) to error message (#7074); - Update To handle if no data error (#7084); - Memory improvement when writing missing positions to pet (#7098); - added support for loading QUALapprox into VET (#7101); - Add -m flag to gsutil step; add dockstore branch filters to facilitate development (#7104); - updates to ImportGenomes and LoadBigQueryData (#7112); - Add ngs to cohort extract Dockerfile; remove exception catching in extract python script (#7113); - remove problematic storage_location imports (#7119); - Reduce memory and CPU for CreateImportTsvs task, check for files before attempting load (#7121); - add -m flag to gsutil mv step (#7129); - ah_var_store : Add sample file argument to cohort extract (#7117); - Perform full WGS cohort extract scientific tieout for 35 ACMG59 samples (#7106); - Enable Read/Execution Project for BQ Queries (#7136); - ah - optional service account (#7140); - Add load lock file to prevent accidental re-loading of data to BQ (#7138); - #251 Address gvcf no-calls missing QUALapprox and other features (#7146); - Job Add labels to BQ operations from GATK (Issues-199) (#7115); - parse map to list to avoid brackets and spaces in vcf output (#7168); - #259 Inline schema for importgenomes.wdl (#7171); - Created AvroFileReader and unittest, Update ExtractCohort and ExtractCohortEngine (#7174); - #224 Import WDL: handle ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:23188,Integrability,wrap,wrap,23188,"res (#7735); - Add withdrawn and is_control columns [VS-70] [VS-213] (#7736); - Allow interval lists that require the SA to see (#7743); - allow for gatk to be overridden, update with known good jar (#7758); - VS-361 Add GvsWithdrawSamples wdl (#7765); - Extract Performance Improvements (#7686); - Don't put withdrawn sample data in alt_allele table [VS-369] (#7762); - remove PET code (#7768); - Adding AD for scale testing VS 225 add AD (#7713); - Deterministic Sample ID assignments [VS-371] (#7770); - remove R scripts from filtering (#7781); - Remove an old ""temp table"" dataset (#7780); - Clean up LocalizeFile [VS-314] (#7771); - Remove pet code from CreateVariantIngestFiles and friends [VS-375] (#7773); - 317 remove excess header values in VCF extract (#7786); - correct auth in split intervals (#7790); - Add code to (optionally) zero pad the vcf filename. (#7783); - LoadData `maxRetries` parameterized, default increased [VS-383] (#7791); - Update to latest version of ah_var_store gatk override jar (#7793); - GvsUnified WDL to wrap the 6 core GVS WDLs [VS-382] (#7789); - Pinned typing_extensions python package to 4.1.1 to fix conda environment. (#7802); - WeightedSplitInterval fixes [VS-384] [VS-332] (#7795); - Replace Travis with GithubActions (#7754); - Docker build only lfs pulls main/src/resources/large (#7727); - Clean up gatk jars -- looks like we are not passing them properly in the extract (#7788); - Fix typo that broke git lfs pull (#7806); - Document AoU SOP (up to the VAT) [VS-63] (#7807); - Incident VS 365 clinvar classification fix (#7769); - VS-390. Add precision and sensitivity wdl (#7813); - Quickstart based integration test [VS-357] (#7812); - 365 vat python testing additions (#7756); - VS 396 clinvar grabs too many values (#7823); - Added a test to validate WDLs in the scripts directory. (#7826) (#7829); - VAT Performance / Reliability Improvements (#7828); - VAT naming conventions [VS-410] (#7827); - Rc remove ad from vat (#7832); - bugfix, we were",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:23797,Integrability,integrat,integration,23797,"; - 317 remove excess header values in VCF extract (#7786); - correct auth in split intervals (#7790); - Add code to (optionally) zero pad the vcf filename. (#7783); - LoadData `maxRetries` parameterized, default increased [VS-383] (#7791); - Update to latest version of ah_var_store gatk override jar (#7793); - GvsUnified WDL to wrap the 6 core GVS WDLs [VS-382] (#7789); - Pinned typing_extensions python package to 4.1.1 to fix conda environment. (#7802); - WeightedSplitInterval fixes [VS-384] [VS-332] (#7795); - Replace Travis with GithubActions (#7754); - Docker build only lfs pulls main/src/resources/large (#7727); - Clean up gatk jars -- looks like we are not passing them properly in the extract (#7788); - Fix typo that broke git lfs pull (#7806); - Document AoU SOP (up to the VAT) [VS-63] (#7807); - Incident VS 365 clinvar classification fix (#7769); - VS-390. Add precision and sensitivity wdl (#7813); - Quickstart based integration test [VS-357] (#7812); - 365 vat python testing additions (#7756); - VS 396 clinvar grabs too many values (#7823); - Added a test to validate WDLs in the scripts directory. (#7826) (#7829); - VAT Performance / Reliability Improvements (#7828); - VAT naming conventions [VS-410] (#7827); - Rc remove ad from vat (#7832); - bugfix, we were trying to grep a binary file (#7837); - Cleanup scripts/variantstore [VS-414] (#7834); - Merge VAT TSV files into single bgzipped file [VS-304] (#7848); - Handle fully and partially loaded samples [VS-262] [VS-258] (#7843); - Ingest Error Handling Fixes [VS-261] (#7841); - First cut at a python notebook to validate inputs. (#7845); - Compute filter scatter [VS-392] (#7852); - remove withdrawn req (#7844); - Improve import error message [VS-437] (#7855); - Fix Input Validation python notebook (#7853); - Add VAT Validation check that aa_change and exon_number are consistently set. (#7850); - Ingest 10K [VS-344] (#7860); - X/Y chromosome reweighting for better extract shard runtime balance [VS-389] (#7868",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:24579,Integrability,message,message,24579,"lfs pull (#7806); - Document AoU SOP (up to the VAT) [VS-63] (#7807); - Incident VS 365 clinvar classification fix (#7769); - VS-390. Add precision and sensitivity wdl (#7813); - Quickstart based integration test [VS-357] (#7812); - 365 vat python testing additions (#7756); - VS 396 clinvar grabs too many values (#7823); - Added a test to validate WDLs in the scripts directory. (#7826) (#7829); - VAT Performance / Reliability Improvements (#7828); - VAT naming conventions [VS-410] (#7827); - Rc remove ad from vat (#7832); - bugfix, we were trying to grep a binary file (#7837); - Cleanup scripts/variantstore [VS-414] (#7834); - Merge VAT TSV files into single bgzipped file [VS-304] (#7848); - Handle fully and partially loaded samples [VS-262] [VS-258] (#7843); - Ingest Error Handling Fixes [VS-261] (#7841); - First cut at a python notebook to validate inputs. (#7845); - Compute filter scatter [VS-392] (#7852); - remove withdrawn req (#7844); - Improve import error message [VS-437] (#7855); - Fix Input Validation python notebook (#7853); - Add VAT Validation check that aa_change and exon_number are consistently set. (#7850); - Ingest 10K [VS-344] (#7860); - X/Y chromosome reweighting for better extract shard runtime balance [VS-389] (#7868); - VET Ingest Validation / Allow Ingest of non-VQSR'ed data (#7870); - Fix AoU workflow bugs (#7874); - Curate input arrays to skip already ingested sample data [VS-246] (#7862); - KM upload GVS product sheet (#7883); - Default extract scatter width [VS-415] (#7878); - Volatile tasks review [VS-447] (#7880); - Update Quickstart Integration for X/Y scaling changes [VS-464] (#7881); - clean up dockstore; - Rc vs 63 vat sop documentation (#7879); - Fix up FQ and race condition issues with volatile tasks work [VS-478] (#7888); - Use gvs-internal project in integration test (#7901); - Add cost observability BQ table [VS-441] (#7891); - Add preliminary labels to queries [VS-381] (#7902); - Workflow compute costs [VS-472] (#7905); - Fix bu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:25190,Integrability,Integrat,Integration,25190,"to validate inputs. (#7845); - Compute filter scatter [VS-392] (#7852); - remove withdrawn req (#7844); - Improve import error message [VS-437] (#7855); - Fix Input Validation python notebook (#7853); - Add VAT Validation check that aa_change and exon_number are consistently set. (#7850); - Ingest 10K [VS-344] (#7860); - X/Y chromosome reweighting for better extract shard runtime balance [VS-389] (#7868); - VET Ingest Validation / Allow Ingest of non-VQSR'ed data (#7870); - Fix AoU workflow bugs (#7874); - Curate input arrays to skip already ingested sample data [VS-246] (#7862); - KM upload GVS product sheet (#7883); - Default extract scatter width [VS-415] (#7878); - Volatile tasks review [VS-447] (#7880); - Update Quickstart Integration for X/Y scaling changes [VS-464] (#7881); - clean up dockstore; - Rc vs 63 vat sop documentation (#7879); - Fix up FQ and race condition issues with volatile tasks work [VS-478] (#7888); - Use gvs-internal project in integration test (#7901); - Add cost observability BQ table [VS-441] (#7891); - Add preliminary labels to queries [VS-381] (#7902); - Workflow compute costs [VS-472] (#7905); - Fix bug and update images (#7912); - VS 483 Beta user wdl (#7894); - Core storage model cost [VS-473] (#7913); - Update Quickstart & Integration to use re-blocked v2 gVCFs [VS-491] (#7924); - KM GVS documentation (#7903); - Track BigQuery costs of GVS python VS-480 (#7915); - Read cost observability table [VS-475] (#7923); - Fix Race Condition, Add Support for Extract by Array of Sample Names (ie from a Sample Set) (#7917); - Rightsize import batches [VS-486] (#7925); - [AoU DRC] Support uppercase site_ids for reblocking (#7929); - Populate cost metadata for GATK tasks. (#7919); - remove accidentally added input (#7931); - VS_492 - Beta User Jar release (#7934); - Cost WDL should throw on FISS API errors [VS-518] (#7942); - Fix bad check for missing workflow name [VS-520] (#7943); - Remove usage of service account from GvsValidateVAT.wdl (#7937",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:25419,Integrability,integrat,integration,25419,"to validate inputs. (#7845); - Compute filter scatter [VS-392] (#7852); - remove withdrawn req (#7844); - Improve import error message [VS-437] (#7855); - Fix Input Validation python notebook (#7853); - Add VAT Validation check that aa_change and exon_number are consistently set. (#7850); - Ingest 10K [VS-344] (#7860); - X/Y chromosome reweighting for better extract shard runtime balance [VS-389] (#7868); - VET Ingest Validation / Allow Ingest of non-VQSR'ed data (#7870); - Fix AoU workflow bugs (#7874); - Curate input arrays to skip already ingested sample data [VS-246] (#7862); - KM upload GVS product sheet (#7883); - Default extract scatter width [VS-415] (#7878); - Volatile tasks review [VS-447] (#7880); - Update Quickstart Integration for X/Y scaling changes [VS-464] (#7881); - clean up dockstore; - Rc vs 63 vat sop documentation (#7879); - Fix up FQ and race condition issues with volatile tasks work [VS-478] (#7888); - Use gvs-internal project in integration test (#7901); - Add cost observability BQ table [VS-441] (#7891); - Add preliminary labels to queries [VS-381] (#7902); - Workflow compute costs [VS-472] (#7905); - Fix bug and update images (#7912); - VS 483 Beta user wdl (#7894); - Core storage model cost [VS-473] (#7913); - Update Quickstart & Integration to use re-blocked v2 gVCFs [VS-491] (#7924); - KM GVS documentation (#7903); - Track BigQuery costs of GVS python VS-480 (#7915); - Read cost observability table [VS-475] (#7923); - Fix Race Condition, Add Support for Extract by Array of Sample Names (ie from a Sample Set) (#7917); - Rightsize import batches [VS-486] (#7925); - [AoU DRC] Support uppercase site_ids for reblocking (#7929); - Populate cost metadata for GATK tasks. (#7919); - remove accidentally added input (#7931); - VS_492 - Beta User Jar release (#7934); - Cost WDL should throw on FISS API errors [VS-518] (#7942); - Fix bad check for missing workflow name [VS-520] (#7943); - Remove usage of service account from GvsValidateVAT.wdl (#7937",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:25729,Integrability,Integrat,Integration,25729,"to validate inputs. (#7845); - Compute filter scatter [VS-392] (#7852); - remove withdrawn req (#7844); - Improve import error message [VS-437] (#7855); - Fix Input Validation python notebook (#7853); - Add VAT Validation check that aa_change and exon_number are consistently set. (#7850); - Ingest 10K [VS-344] (#7860); - X/Y chromosome reweighting for better extract shard runtime balance [VS-389] (#7868); - VET Ingest Validation / Allow Ingest of non-VQSR'ed data (#7870); - Fix AoU workflow bugs (#7874); - Curate input arrays to skip already ingested sample data [VS-246] (#7862); - KM upload GVS product sheet (#7883); - Default extract scatter width [VS-415] (#7878); - Volatile tasks review [VS-447] (#7880); - Update Quickstart Integration for X/Y scaling changes [VS-464] (#7881); - clean up dockstore; - Rc vs 63 vat sop documentation (#7879); - Fix up FQ and race condition issues with volatile tasks work [VS-478] (#7888); - Use gvs-internal project in integration test (#7901); - Add cost observability BQ table [VS-441] (#7891); - Add preliminary labels to queries [VS-381] (#7902); - Workflow compute costs [VS-472] (#7905); - Fix bug and update images (#7912); - VS 483 Beta user wdl (#7894); - Core storage model cost [VS-473] (#7913); - Update Quickstart & Integration to use re-blocked v2 gVCFs [VS-491] (#7924); - KM GVS documentation (#7903); - Track BigQuery costs of GVS python VS-480 (#7915); - Read cost observability table [VS-475] (#7923); - Fix Race Condition, Add Support for Extract by Array of Sample Names (ie from a Sample Set) (#7917); - Rightsize import batches [VS-486] (#7925); - [AoU DRC] Support uppercase site_ids for reblocking (#7929); - Populate cost metadata for GATK tasks. (#7919); - remove accidentally added input (#7931); - VS_492 - Beta User Jar release (#7934); - Cost WDL should throw on FISS API errors [VS-518] (#7942); - Fix bad check for missing workflow name [VS-520] (#7943); - Remove usage of service account from GvsValidateVAT.wdl (#7937",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:26897,Integrability,integrat,integration,26897,"x Race Condition, Add Support for Extract by Array of Sample Names (ie from a Sample Set) (#7917); - Rightsize import batches [VS-486] (#7925); - [AoU DRC] Support uppercase site_ids for reblocking (#7929); - Populate cost metadata for GATK tasks. (#7919); - remove accidentally added input (#7931); - VS_492 - Beta User Jar release (#7934); - Cost WDL should throw on FISS API errors [VS-518] (#7942); - Fix bad check for missing workflow name [VS-520] (#7943); - Remove usage of service account from GvsValidateVAT.wdl (#7937); - refactoring for testablity (#7946); - More import retries [VS-532] (#7953); - A few last doc changes (#7927); - WDL to extract a single callset cost (BQ only, not Terra) (#7940); - Temporarily swap in Corretto for Temurin as we can't download Temurin. (#7969); - GL-548 - Update CreateVat code to handle samples that do not contain all population groups. (#7965); - Restore Temurin 11 [VS-570] (#7972); - Add table size check to quickstart integration test [VS-501] (#7970); - Consolidate various docs for AoU callset generation into one to rule them all [VS-553] (#7971); - VS-567. Removing usage of ServiceAccount from CreateVat related WDLs (#7974); - WDL to extract Avro files for Hail import [VS-579] (#7981); - Removed usage of service account from WDLs (#7985); - Document steps for GVS cleanup for base use case [VS-586] (#7989); - Change backticks to single quotes in several error messages - causing shell to attempt to execute. (#7995); - VS-598 - Minor update to AoU Documentation. (#7994); - Allow for incremental addition of data to alt_allele [VS-52] (#7993); - Minor AoU Documentation Update (#7999); - Batch population of alt_allele table from vet_ tables [VS-265] (#7998); - Change drop_state to NONE for Ingest/Extract [VS-607] (#8000); - python -> python3 (#8001); - Generate Hail import/export script [VS-605] (#8002); - clearer error when values are missing (#7939); - Ah [VS-565] output intervals and sample list (#8010); - make CreateAltAlleleT",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:27348,Integrability,message,messages,27348,"#7931); - VS_492 - Beta User Jar release (#7934); - Cost WDL should throw on FISS API errors [VS-518] (#7942); - Fix bad check for missing workflow name [VS-520] (#7943); - Remove usage of service account from GvsValidateVAT.wdl (#7937); - refactoring for testablity (#7946); - More import retries [VS-532] (#7953); - A few last doc changes (#7927); - WDL to extract a single callset cost (BQ only, not Terra) (#7940); - Temporarily swap in Corretto for Temurin as we can't download Temurin. (#7969); - GL-548 - Update CreateVat code to handle samples that do not contain all population groups. (#7965); - Restore Temurin 11 [VS-570] (#7972); - Add table size check to quickstart integration test [VS-501] (#7970); - Consolidate various docs for AoU callset generation into one to rule them all [VS-553] (#7971); - VS-567. Removing usage of ServiceAccount from CreateVat related WDLs (#7974); - WDL to extract Avro files for Hail import [VS-579] (#7981); - Removed usage of service account from WDLs (#7985); - Document steps for GVS cleanup for base use case [VS-586] (#7989); - Change backticks to single quotes in several error messages - causing shell to attempt to execute. (#7995); - VS-598 - Minor update to AoU Documentation. (#7994); - Allow for incremental addition of data to alt_allele [VS-52] (#7993); - Minor AoU Documentation Update (#7999); - Batch population of alt_allele table from vet_ tables [VS-265] (#7998); - Change drop_state to NONE for Ingest/Extract [VS-607] (#8000); - python -> python3 (#8001); - Generate Hail import/export script [VS-605] (#8002); - clearer error when values are missing (#7939); - Ah [VS-565] output intervals and sample list (#8010); - make CreateAltAlleleTable task volatile (#8011); - Restore withdrawn [VS-581] (#8006); - Km gvs add storage cost and cleanup doc (#8012); - Updating documentation to reflect the changed outputs [VS-565] (#8014); - File of callset samples -> samples marked as 'withdrawn' in GVS [VS-436] (#8009); - fix quota guide",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:28993,Integrability,Integrat,Integration,28993,"ility to tweak sample-every-Nth-variant parameter for SNP model creation (#8019); - add initial notebook copy pasta (#8008); - add sample_table_timestamp to GetNumSamplesLoaded (#8022); - Batched Avro export [VS-630] (#8020); - Updating references to old GATK for VS-620 (#8023); - VS-517 Use standard version of GetBQTableLastModifiedDatetime in GvsValidateVat (#8024); - Fix bug in GvsWithdrawSamples.wdl (#8026); - Ah 617 exposing the drop_state parameter to the GvsJointVariantCalling wdl used for beta (and internal customer) (#8032); - Expose maximum-training-variants VQSR parameter [VS-634] (#8029); - Callset statistics [VS-560] (#8018); - Check for withdrawn before exporting to AVRO files [VS-646] (#8039); - Small updates to GVS Integration WDL [VS-618] (#8042); - Rework Hail script generation [VS-616] (#8034); - Alpine based Variant Store Docker image [VS-648] (#8044); - update warp version (#7906); - Fail Avro extract and callset stats on bad filter name [VS-655] (#8046); - Vs 629 failure to retrieve job information during ingest (#8047); - Restore accidentally removed bcftools [VS-661] (#8051); - Allowing our pipeline to function with a sample size of one (#8055); - Vs 665 re create vcf for cd 68 po 52339 with ad padding fixed (#8057); - VS-665 and VS-620 updating code to use latest docker images containing Rori's AD calculation changes in extract (#8061); - updating the beta workflow to use the latest jar, representing the version of GATK George tested against the workflow (#8062); - VS-637 Address a couple of issues in SampleLoadStatus handling in GVSImportGenomes. (#8052); - Revert Alpinizing of apt dependent task [VS-688] (#8065); - Fix missing vat schema JSONs [VS-699] (#8072); - Fix integration expectations for fixed AD [VS-689] (#8066); - VS-698 Remove unnecessary columns from Call set statistics (#8073); - Fix Dockerfile nits that break 20.10.21 (#8078); - Nirvana 3.18.1 Docker images support [VS-661] (#8082); - Add option to not prepare __REF_DATA or __",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:29887,Integrability,depend,dependent,29887,"Integration WDL [VS-618] (#8042); - Rework Hail script generation [VS-616] (#8034); - Alpine based Variant Store Docker image [VS-648] (#8044); - update warp version (#7906); - Fail Avro extract and callset stats on bad filter name [VS-655] (#8046); - Vs 629 failure to retrieve job information during ingest (#8047); - Restore accidentally removed bcftools [VS-661] (#8051); - Allowing our pipeline to function with a sample size of one (#8055); - Vs 665 re create vcf for cd 68 po 52339 with ad padding fixed (#8057); - VS-665 and VS-620 updating code to use latest docker images containing Rori's AD calculation changes in extract (#8061); - updating the beta workflow to use the latest jar, representing the version of GATK George tested against the workflow (#8062); - VS-637 Address a couple of issues in SampleLoadStatus handling in GVSImportGenomes. (#8052); - Revert Alpinizing of apt dependent task [VS-688] (#8065); - Fix missing vat schema JSONs [VS-699] (#8072); - Fix integration expectations for fixed AD [VS-689] (#8066); - VS-698 Remove unnecessary columns from Call set statistics (#8073); - Fix Dockerfile nits that break 20.10.21 (#8078); - Nirvana 3.18.1 Docker images support [VS-661] (#8082); - Add option to not prepare __REF_DATA or __SAMPLES tables to Prepare [VS-697] (#8079); - ""build-base"" Docker image for faster variantstore image builds [VS-712] (#8085); - GVS / Hail VDS integration test [VS-639] (#8086); - Remove AI/AN from VDS docs [VS-726] (#8096); - Add flag for cost_observability table writing to support sub-cohort use case [VS-521] (#8093); - Document STS delivery process for VDS [VS-727] (#8101); - delete obsolete callset_QC directory and its contents [VS-318] (#8108); - doc link typo and add check for control samples in AVRO export (#8110); - Add defaults for scatter_count in GvsExtractCohortFromSampleNames [VS-496] (#8109); - Escape table names properly in ValidateVat WDL (#8116); - Vs 741 fix indefinite freeze in split intervals task when using ex",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:29975,Integrability,integrat,integration,29975,"Integration WDL [VS-618] (#8042); - Rework Hail script generation [VS-616] (#8034); - Alpine based Variant Store Docker image [VS-648] (#8044); - update warp version (#7906); - Fail Avro extract and callset stats on bad filter name [VS-655] (#8046); - Vs 629 failure to retrieve job information during ingest (#8047); - Restore accidentally removed bcftools [VS-661] (#8051); - Allowing our pipeline to function with a sample size of one (#8055); - Vs 665 re create vcf for cd 68 po 52339 with ad padding fixed (#8057); - VS-665 and VS-620 updating code to use latest docker images containing Rori's AD calculation changes in extract (#8061); - updating the beta workflow to use the latest jar, representing the version of GATK George tested against the workflow (#8062); - VS-637 Address a couple of issues in SampleLoadStatus handling in GVSImportGenomes. (#8052); - Revert Alpinizing of apt dependent task [VS-688] (#8065); - Fix missing vat schema JSONs [VS-699] (#8072); - Fix integration expectations for fixed AD [VS-689] (#8066); - VS-698 Remove unnecessary columns from Call set statistics (#8073); - Fix Dockerfile nits that break 20.10.21 (#8078); - Nirvana 3.18.1 Docker images support [VS-661] (#8082); - Add option to not prepare __REF_DATA or __SAMPLES tables to Prepare [VS-697] (#8079); - ""build-base"" Docker image for faster variantstore image builds [VS-712] (#8085); - GVS / Hail VDS integration test [VS-639] (#8086); - Remove AI/AN from VDS docs [VS-726] (#8096); - Add flag for cost_observability table writing to support sub-cohort use case [VS-521] (#8093); - Document STS delivery process for VDS [VS-727] (#8101); - delete obsolete callset_QC directory and its contents [VS-318] (#8108); - doc link typo and add check for control samples in AVRO export (#8110); - Add defaults for scatter_count in GvsExtractCohortFromSampleNames [VS-496] (#8109); - Escape table names properly in ValidateVat WDL (#8116); - Vs 741 fix indefinite freeze in split intervals task when using ex",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:30397,Integrability,integrat,integration,30397,"ge tested against the workflow (#8062); - VS-637 Address a couple of issues in SampleLoadStatus handling in GVSImportGenomes. (#8052); - Revert Alpinizing of apt dependent task [VS-688] (#8065); - Fix missing vat schema JSONs [VS-699] (#8072); - Fix integration expectations for fixed AD [VS-689] (#8066); - VS-698 Remove unnecessary columns from Call set statistics (#8073); - Fix Dockerfile nits that break 20.10.21 (#8078); - Nirvana 3.18.1 Docker images support [VS-661] (#8082); - Add option to not prepare __REF_DATA or __SAMPLES tables to Prepare [VS-697] (#8079); - ""build-base"" Docker image for faster variantstore image builds [VS-712] (#8085); - GVS / Hail VDS integration test [VS-639] (#8086); - Remove AI/AN from VDS docs [VS-726] (#8096); - Add flag for cost_observability table writing to support sub-cohort use case [VS-521] (#8093); - Document STS delivery process for VDS [VS-727] (#8101); - delete obsolete callset_QC directory and its contents [VS-318] (#8108); - doc link typo and add check for control samples in AVRO export (#8110); - Add defaults for scatter_count in GvsExtractCohortFromSampleNames [VS-496] (#8109); - Escape table names properly in ValidateVat WDL (#8116); - Vs 741 fix indefinite freeze in split intervals task when using exome data (#8113); - VAT Readme updates (#8090); - WDL and python scripts to use the VDS in the VAT (#8077); - VS-757 - Use JASIX to make sub-jsons of annotated output of Nirvana (#8133); - add note about permissions for P&S workflow to work (#8135); - VS-759 (and VS-760) (#8137); - VS-765. Scatter the RemoveDuplicates task. (#8144); - update delivery docs based on latest VDS delivery run [VS-770] (#8150); - Add monitoring to index vcf (#8151); - Make some noise when VDS validation succeeds (#8155); - Handle empty genes annotation file. (#8153); - Add escapes for otherwise problematic dataset / table names. (#8162); - New WDL to create VAT tsvs from previously generated BigQuery table. (#8165); - Treat withdrawn samples in ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:329,Modifiability,refactor,refactored,329,,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:410,Modifiability,refactor,refactor,410,,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:800,Modifiability,refactor,refactored,800,,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:1017,Modifiability,refactor,refactor,1017,"rt extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - forgot this file; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - add fields for uncompressed imputed data; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - Tool for arrays QC metrics calculations (#6812); - ah update array extract tool (#6827); - fix enum (#6834); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - resolved rebase conflicts; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to samp",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:1975,Modifiability,refactor,refactored,1975,"ngest; - add imputed tsv creator and refactor; - add fields for uncompressed imputed data; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - Tool for arrays QC metrics calculations (#6812); - ah update array extract tool (#6827); - fix enum (#6834); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - resolved rebase conflicts; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - updating ArrayCalculateMetrics fo",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:2056,Modifiability,refactor,refactor,2056,"uted data; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - Tool for arrays QC metrics calculations (#6812); - ah update array extract tool (#6827); - fix enum (#6834); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - resolved rebase conflicts; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:2446,Modifiability,refactor,refactored,2446,"4); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - resolved rebase conflicts; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - Moving the WDL for importing array manifest to BQ (#6860); - fix up after rebase; - Moving and testing ingest scripts from variantstore (#6881); - optionally provide sample-map-file instead of sample-map-table (#6872); - Moving extract wdls from variantstore repo (#6902); - update for genomes (#6918); - update pat",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:2663,Modifiability,refactor,refactor,2663,"sed on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - resolved rebase conflicts; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - Moving the WDL for importing array manifest to BQ (#6860); - fix up after rebase; - Moving and testing ingest scripts from variantstore (#6881); - optionally provide sample-map-file instead of sample-map-table (#6872); - Moving extract wdls from variantstore repo (#6902); - update for genomes (#6918); - update paths; - update field name; - consolidate exome and genome code; - missing comma; - allow null for drop state.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:5099,Modifiability,enhance,enhanced,5099,,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:5137,Modifiability,config,config,5137,,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:6301,Modifiability,refactor,refactored,6301,"if no data error (#7084); - Memory improvement when writing missing positions to pet (#7098); - added support for loading QUALapprox into VET (#7101); - Add -m flag to gsutil step; add dockstore branch filters to facilitate development (#7104); - updates to ImportGenomes and LoadBigQueryData (#7112); - Add ngs to cohort extract Dockerfile; remove exception catching in extract python script (#7113); - remove problematic storage_location imports (#7119); - Reduce memory and CPU for CreateImportTsvs task, check for files before attempting load (#7121); - add -m flag to gsutil mv step (#7129); - ah_var_store : Add sample file argument to cohort extract (#7117); - wip; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - forgot this file; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - add fields for uncompressed imputed data; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:6382,Modifiability,refactor,refactor,6382," pet (#7098); - added support for loading QUALapprox into VET (#7101); - Add -m flag to gsutil step; add dockstore branch filters to facilitate development (#7104); - updates to ImportGenomes and LoadBigQueryData (#7112); - Add ngs to cohort extract Dockerfile; remove exception catching in extract python script (#7113); - remove problematic storage_location imports (#7119); - Reduce memory and CPU for CreateImportTsvs task, check for files before attempting load (#7121); - add -m flag to gsutil mv step (#7129); - ah_var_store : Add sample file argument to cohort extract (#7117); - wip; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - forgot this file; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - add fields for uncompressed imputed data; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - Tool for arrays QC metrics calculations (#6812); - ah update array ex",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:6772,Modifiability,refactor,refactored,6772,"y and CPU for CreateImportTsvs task, check for files before attempting load (#7121); - add -m flag to gsutil mv step (#7129); - ah_var_store : Add sample file argument to cohort extract (#7117); - wip; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - forgot this file; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - add fields for uncompressed imputed data; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - Tool for arrays QC metrics calculations (#6812); - ah update array extract tool (#6827); - fix enum (#6834); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - resolved rebase conflicts; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is manda",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:6989,Modifiability,refactor,refactor,6989,"rt extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - forgot this file; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - add fields for uncompressed imputed data; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - Tool for arrays QC metrics calculations (#6812); - ah update array extract tool (#6827); - fix enum (#6834); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - resolved rebase conflicts; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to samp",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:7947,Modifiability,refactor,refactored,7947,"ngest; - add imputed tsv creator and refactor; - add fields for uncompressed imputed data; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - Tool for arrays QC metrics calculations (#6812); - ah update array extract tool (#6827); - fix enum (#6834); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - resolved rebase conflicts; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - updating ArrayCalculateMetrics fo",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:8028,Modifiability,refactor,refactor,8028,"uted data; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - Tool for arrays QC metrics calculations (#6812); - ah update array extract tool (#6827); - fix enum (#6834); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - resolved rebase conflicts; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:8418,Modifiability,refactor,refactored,8418,"4); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - resolved rebase conflicts; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - Moving the WDL for importing array manifest to BQ (#6860); - fix up after rebase; - Moving and testing ingest scripts from variantstore (#6881); - optionally provide sample-map-file instead of sample-map-table (#6872); - Moving extract wdls from variantstore repo (#6902); - update for genomes (#6918); - update pat",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:8635,Modifiability,refactor,refactor,8635,"sed on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - resolved rebase conflicts; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - Moving the WDL for importing array manifest to BQ (#6860); - fix up after rebase; - Moving and testing ingest scripts from variantstore (#6881); - optionally provide sample-map-file instead of sample-map-table (#6872); - Moving extract wdls from variantstore repo (#6902); - update for genomes (#6918); - update paths; - update field name; - consolidate exome and genome code; - missing comma; - allow null for drop state.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:11071,Modifiability,enhance,enhanced,11071," table creation and data loading in LoadData (#7056); - WIP; - tieout scripts; - notes files; - updated diff scripts; - fixed bug...; - add wdl and inputs file for warp pipeline; - reverting logging; - included top level WDL; - use gnarly with BQ extract cohort; - remove unused file; - cleaning up; - tidy; - tidy up before PR; - tidy up before PR; - PR comments; - merge conflict misfires; - added example SQL to create alt allele table from VET; - option to remove PLs; - fixed and enhanced unit test; - removing unused config, causing travis to fail; - add CreateVariantIngestFiles integration test (#7071); - add sampleName (instead of NULL) to error message (#7074); - Update To handle if no data error (#7084); - Memory improvement when writing missing positions to pet (#7098); - added support for loading QUALapprox into VET (#7101); - Add -m flag to gsutil step; add dockstore branch filters to facilitate development (#7104); - updates to ImportGenomes and LoadBigQueryData (#7112); - Add ngs to cohort extract Dockerfile; remove exception catching in extract python script (#7113); - remove problematic storage_location imports (#7119); - Reduce memory and CPU for CreateImportTsvs task, check for files before attempting load (#7121); - add -m flag to gsutil mv step (#7129); - ah_var_store : Add sample file argument to cohort extract (#7117); - Perform full WGS cohort extract scientific tieout for 35 ACMG59 samples (#7106); - Enable Read/Execution Project for BQ Queries (#7136); - ah - optional service account (#7140); - Add load lock file to prevent accidental re-loading of data to BQ (#7138); - #251 Address gvcf no-calls missing QUALapprox and other features (#7146); - Job Add labels to BQ operations from GATK (Issues-199) (#7115); - parse map to list to avoid brackets and spaces in vcf output (#7168); - #259 Inline schema for importgenomes.wdl (#7171); - Created AvroFileReader and unittest, Update ExtractCohort and ExtractCohortEngine (#7174); - #224 Import WDL: handle ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:11109,Modifiability,config,config,11109," table creation and data loading in LoadData (#7056); - WIP; - tieout scripts; - notes files; - updated diff scripts; - fixed bug...; - add wdl and inputs file for warp pipeline; - reverting logging; - included top level WDL; - use gnarly with BQ extract cohort; - remove unused file; - cleaning up; - tidy; - tidy up before PR; - tidy up before PR; - PR comments; - merge conflict misfires; - added example SQL to create alt allele table from VET; - option to remove PLs; - fixed and enhanced unit test; - removing unused config, causing travis to fail; - add CreateVariantIngestFiles integration test (#7071); - add sampleName (instead of NULL) to error message (#7074); - Update To handle if no data error (#7084); - Memory improvement when writing missing positions to pet (#7098); - added support for loading QUALapprox into VET (#7101); - Add -m flag to gsutil step; add dockstore branch filters to facilitate development (#7104); - updates to ImportGenomes and LoadBigQueryData (#7112); - Add ngs to cohort extract Dockerfile; remove exception catching in extract python script (#7113); - remove problematic storage_location imports (#7119); - Reduce memory and CPU for CreateImportTsvs task, check for files before attempting load (#7121); - add -m flag to gsutil mv step (#7129); - ah_var_store : Add sample file argument to cohort extract (#7117); - Perform full WGS cohort extract scientific tieout for 35 ACMG59 samples (#7106); - Enable Read/Execution Project for BQ Queries (#7136); - ah - optional service account (#7140); - Add load lock file to prevent accidental re-loading of data to BQ (#7138); - #251 Address gvcf no-calls missing QUALapprox and other features (#7146); - Job Add labels to BQ operations from GATK (Issues-199) (#7115); - parse map to list to avoid brackets and spaces in vcf output (#7168); - #259 Inline schema for importgenomes.wdl (#7171); - Created AvroFileReader and unittest, Update ExtractCohort and ExtractCohortEngine (#7174); - #224 Import WDL: handle ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:14093,Modifiability,parameteriz,parameterize,14093,"ut allow NONE as input (#7206); - SA support and consistent naming for all GVS WDLs (#7205); - fix GvsExtractCallset inputs file (#7210); - add clustering to tables (#7207); - add vqsr cutoffs to GvsExtractCallset wdl; clean up dockstore yml (#7209); - Avro test (#7192); - Enable call caching of TSV generation in GvsImportGenomes (#7226); - 266 Clean up ExtractCohort -- remove query mode param (#7227); - 288 Add an excess alleles param (#7221); - take sample name as a param (#7236); - How to run GIAB comparisons (#7237); - Update GvsCreateFilterSet.wdl (#7239); - Use GatherVcfsCloud in GvsCreateFilterSet.wdl (#7241); - parameterize TTL with defaults, reduce memory allocation (#7244); - Addressing OOM in CohortExtract (#7245); - make outputs optional, change case in output (#7252); - Support for FORMAT/FT VQSLod Filtering and cohort-wide LowQual filter (#7248); - removed arrays code, renamed packages (#7260); - 279 labels (#7233); - add conda commands to GIAB readme (#7268); - remove gvs branch (#7263); - remove gvs branch (#7263); - upgrade bq libraries (#7264); - #299 - Sample list ease of use for cohort extracts (#7272); - check for duplicate ids (#7273); - Rc 274 passing sites only (#7275); - added default value to drop_state; broadinstitute/dsp-spec-ops#310 (#7278); - version bump for reliability (#7284); - add timestamp check to ExtractTask call https://github.com/broadinstitute/dsp-spec-ops/issues/320; - serial inserts for scaling prepare, factored out sample name (#7288); - Remove training sites only param from ExtractFeatures broadinstitute/dsp-spec-ops#261; - add param for mem for indels (#7282); - Ah prepare localize option (#7299); - Export sites only vcf STEP 1-- 317 add AC, AN, AF to the final VCF (#7279); - AoU GVS Cohort Extract wdl (#7242); - reliability (#7310); - bump to include FT tag filtering (#7316); - First pass at a Terra QuickStart (#7267); - Ah fix timestamp query (#7319); - 313 Cleanup Extract Cohort params (#7293); - bump bq storage versi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:19005,Modifiability,refactor,refactor,19005,"463); - formatting on sample QC README; - formatting change #2 to sample QC README; - address VS-152, remove extra headers from extract (#7466); - Update GvsExtractCallset.example.inputs.json (#7469); - Add ability to copy interval list files to gs directory [VS-191] (#7467); - add an expiration date to the temp tables (#7455); - fix the check for duplicates in import genomes (#7470); - added job ID to alt_allele population call output [VS-194] (#7473); - added steps and deliverables to GVS README [VS-181] (#7452); - Ah check the is loaded field in feature extract (#7475); - changes to put pet data directly into data table (#7478); - added override for ExtractTasks' preemptible value (#7477); - bcftools to the rescue (#7456); - execute_with_retry() refactor and error handling improvements [VS-159] (#7480); - Small updates to GvsExtractCallset from beta callset, new workflow for re-scattered shards (#7493); - add flag in prepare to print out sql instead of executing (#7501); - Workflow to re-scatter and then merge ""problematic"" intervals from ExtractCallset [VS-209] (#7495); - changed README to reflect comments from Lee [VS-210] (#7502); - Export the VAT into GCS (#7472); - addresses VS-219 (#7508); - small fix to MergeVCFs (#7517); - small fixes to GVS pipeline (#7522); - make sure ExtractTask is run on all interval files; - Revert ""make sure ExtractTask is run on all interval files""; - make sure ExtractTask is run on all interval files (#7527); - Remove Sites only step from the VAT creation WDL (#7510); - fix bad argument processing for bool (#7529); - Support for TDR DRS URIs in Import (#7528); - Match format of filename output in GvsRescatterCallsetInterval (#7539); - Reference block storage and query support (#7498); - update docs (#7540); - Kc fix rr load bug (#7550); - Update .dockstore.yml (#7553); - Ah add reblocking wdl (#7544); - Scatter over all interval files, not just scatter count (#7551); - fixed docker (#7558); - take advantage of fixed version of Spl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:23047,Modifiability,parameteriz,parameterized,23047,"res (#7735); - Add withdrawn and is_control columns [VS-70] [VS-213] (#7736); - Allow interval lists that require the SA to see (#7743); - allow for gatk to be overridden, update with known good jar (#7758); - VS-361 Add GvsWithdrawSamples wdl (#7765); - Extract Performance Improvements (#7686); - Don't put withdrawn sample data in alt_allele table [VS-369] (#7762); - remove PET code (#7768); - Adding AD for scale testing VS 225 add AD (#7713); - Deterministic Sample ID assignments [VS-371] (#7770); - remove R scripts from filtering (#7781); - Remove an old ""temp table"" dataset (#7780); - Clean up LocalizeFile [VS-314] (#7771); - Remove pet code from CreateVariantIngestFiles and friends [VS-375] (#7773); - 317 remove excess header values in VCF extract (#7786); - correct auth in split intervals (#7790); - Add code to (optionally) zero pad the vcf filename. (#7783); - LoadData `maxRetries` parameterized, default increased [VS-383] (#7791); - Update to latest version of ah_var_store gatk override jar (#7793); - GvsUnified WDL to wrap the 6 core GVS WDLs [VS-382] (#7789); - Pinned typing_extensions python package to 4.1.1 to fix conda environment. (#7802); - WeightedSplitInterval fixes [VS-384] [VS-332] (#7795); - Replace Travis with GithubActions (#7754); - Docker build only lfs pulls main/src/resources/large (#7727); - Clean up gatk jars -- looks like we are not passing them properly in the extract (#7788); - Fix typo that broke git lfs pull (#7806); - Document AoU SOP (up to the VAT) [VS-63] (#7807); - Incident VS 365 clinvar classification fix (#7769); - VS-390. Add precision and sensitivity wdl (#7813); - Quickstart based integration test [VS-357] (#7812); - 365 vat python testing additions (#7756); - VS 396 clinvar grabs too many values (#7823); - Added a test to validate WDLs in the scripts directory. (#7826) (#7829); - VAT Performance / Reliability Improvements (#7828); - VAT naming conventions [VS-410] (#7827); - Rc remove ad from vat (#7832); - bugfix, we were",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:26457,Modifiability,refactor,refactoring,26457," [VS-472] (#7905); - Fix bug and update images (#7912); - VS 483 Beta user wdl (#7894); - Core storage model cost [VS-473] (#7913); - Update Quickstart & Integration to use re-blocked v2 gVCFs [VS-491] (#7924); - KM GVS documentation (#7903); - Track BigQuery costs of GVS python VS-480 (#7915); - Read cost observability table [VS-475] (#7923); - Fix Race Condition, Add Support for Extract by Array of Sample Names (ie from a Sample Set) (#7917); - Rightsize import batches [VS-486] (#7925); - [AoU DRC] Support uppercase site_ids for reblocking (#7929); - Populate cost metadata for GATK tasks. (#7919); - remove accidentally added input (#7931); - VS_492 - Beta User Jar release (#7934); - Cost WDL should throw on FISS API errors [VS-518] (#7942); - Fix bad check for missing workflow name [VS-520] (#7943); - Remove usage of service account from GvsValidateVAT.wdl (#7937); - refactoring for testablity (#7946); - More import retries [VS-532] (#7953); - A few last doc changes (#7927); - WDL to extract a single callset cost (BQ only, not Terra) (#7940); - Temporarily swap in Corretto for Temurin as we can't download Temurin. (#7969); - GL-548 - Update CreateVat code to handle samples that do not contain all population groups. (#7965); - Restore Temurin 11 [VS-570] (#7972); - Add table size check to quickstart integration test [VS-501] (#7970); - Consolidate various docs for AoU callset generation into one to rule them all [VS-553] (#7971); - VS-567. Removing usage of ServiceAccount from CreateVat related WDLs (#7974); - WDL to extract Avro files for Hail import [VS-579] (#7981); - Removed usage of service account from WDLs (#7985); - Document steps for GVS cleanup for base use case [VS-586] (#7989); - Change backticks to single quotes in several error messages - causing shell to attempt to execute. (#7995); - VS-598 - Minor update to AoU Documentation. (#7994); - Allow for incremental addition of data to alt_allele [VS-52] (#7993); - Minor AoU Documentation Update (#7999); -",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:4470,Performance,optimiz,optimized,4470,"e-map-file instead of sample-map-table (#6872); - Moving extract wdls from variantstore repo (#6902); - update for genomes (#6918); - update paths; - update field name; - consolidate exome and genome code; - missing comma; - allow null for drop state. add to wdl; - fix when NON-REF has value (drop it anyway). add memory to wdl; - fix order of columns in vet; - Making ingest work on gvcfs without allele specific annotations (#6934); - fix partitioning (#6939); - more fixes for non-allele-specific gvcfs (#6945); - fix GT for ./.; - add in feature extract code. resulting vcf still has bugs (#6947); - Rudimentary support for other output types, support for scattering extract cohort (#6949); - added batch/interactive flag, removed samples (#6953); - WIP for Feature Extract code (#6958); - move wdl and cromwell_tests dirs under variantstore (#6961); - WDL to run feature extract, VQSR, and upload (#6966); - support for multiple PET/VETs (#6969); - Add filtering to ExtractCohort (#6971); - support for non-AS called data (#6975); - fixing bug in filtering (#6976); - Updating extract wdls to apply filtering (#6977); - moving from specops repo (#6983); - optimized TSV experiment, and range GQ dropping (#6987); - back to 30x defaults; - allow no filtering to be applied (#7004); - Separate bigquery table creation and data loading in LoadData (#7056); - WIP; - tieout scripts; - notes files; - updated diff scripts; - fixed bug...; - add wdl and inputs file for warp pipeline; - reverting logging; - included top level WDL; - use gnarly with BQ extract cohort; - remove unused file; - cleaning up; - tidy; - tidy up before PR; - tidy up before PR; - PR comments; - merge conflict misfires; - added example SQL to create alt allele table from VET; - option to remove PLs; - fixed and enhanced unit test; - removing unused config, causing travis to fail; - add CreateVariantIngestFiles integration test (#7071); - add sampleName (instead of NULL) to error message (#7074); - Update To handle if",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:4639,Performance,load,loading,4639,"e-map-file instead of sample-map-table (#6872); - Moving extract wdls from variantstore repo (#6902); - update for genomes (#6918); - update paths; - update field name; - consolidate exome and genome code; - missing comma; - allow null for drop state. add to wdl; - fix when NON-REF has value (drop it anyway). add memory to wdl; - fix order of columns in vet; - Making ingest work on gvcfs without allele specific annotations (#6934); - fix partitioning (#6939); - more fixes for non-allele-specific gvcfs (#6945); - fix GT for ./.; - add in feature extract code. resulting vcf still has bugs (#6947); - Rudimentary support for other output types, support for scattering extract cohort (#6949); - added batch/interactive flag, removed samples (#6953); - WIP for Feature Extract code (#6958); - move wdl and cromwell_tests dirs under variantstore (#6961); - WDL to run feature extract, VQSR, and upload (#6966); - support for multiple PET/VETs (#6969); - Add filtering to ExtractCohort (#6971); - support for non-AS called data (#6975); - fixing bug in filtering (#6976); - Updating extract wdls to apply filtering (#6977); - moving from specops repo (#6983); - optimized TSV experiment, and range GQ dropping (#6987); - back to 30x defaults; - allow no filtering to be applied (#7004); - Separate bigquery table creation and data loading in LoadData (#7056); - WIP; - tieout scripts; - notes files; - updated diff scripts; - fixed bug...; - add wdl and inputs file for warp pipeline; - reverting logging; - included top level WDL; - use gnarly with BQ extract cohort; - remove unused file; - cleaning up; - tidy; - tidy up before PR; - tidy up before PR; - PR comments; - merge conflict misfires; - added example SQL to create alt allele table from VET; - option to remove PLs; - fixed and enhanced unit test; - removing unused config, causing travis to fail; - add CreateVariantIngestFiles integration test (#7071); - add sampleName (instead of NULL) to error message (#7074); - Update To handle if",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:4650,Performance,Load,LoadData,4650,"e-map-file instead of sample-map-table (#6872); - Moving extract wdls from variantstore repo (#6902); - update for genomes (#6918); - update paths; - update field name; - consolidate exome and genome code; - missing comma; - allow null for drop state. add to wdl; - fix when NON-REF has value (drop it anyway). add memory to wdl; - fix order of columns in vet; - Making ingest work on gvcfs without allele specific annotations (#6934); - fix partitioning (#6939); - more fixes for non-allele-specific gvcfs (#6945); - fix GT for ./.; - add in feature extract code. resulting vcf still has bugs (#6947); - Rudimentary support for other output types, support for scattering extract cohort (#6949); - added batch/interactive flag, removed samples (#6953); - WIP for Feature Extract code (#6958); - move wdl and cromwell_tests dirs under variantstore (#6961); - WDL to run feature extract, VQSR, and upload (#6966); - support for multiple PET/VETs (#6969); - Add filtering to ExtractCohort (#6971); - support for non-AS called data (#6975); - fixing bug in filtering (#6976); - Updating extract wdls to apply filtering (#6977); - moving from specops repo (#6983); - optimized TSV experiment, and range GQ dropping (#6987); - back to 30x defaults; - allow no filtering to be applied (#7004); - Separate bigquery table creation and data loading in LoadData (#7056); - WIP; - tieout scripts; - notes files; - updated diff scripts; - fixed bug...; - add wdl and inputs file for warp pipeline; - reverting logging; - included top level WDL; - use gnarly with BQ extract cohort; - remove unused file; - cleaning up; - tidy; - tidy up before PR; - tidy up before PR; - PR comments; - merge conflict misfires; - added example SQL to create alt allele table from VET; - option to remove PLs; - fixed and enhanced unit test; - removing unused config, causing travis to fail; - add CreateVariantIngestFiles integration test (#7071); - add sampleName (instead of NULL) to error message (#7074); - Update To handle if",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:5420,Performance,load,loading,5420,,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:5582,Performance,Load,LoadBigQueryData,5582,,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:5848,Performance,load,load,5848,"ly with BQ extract cohort; - remove unused file; - cleaning up; - tidy; - tidy up before PR; - tidy up before PR; - PR comments; - merge conflict misfires; - added example SQL to create alt allele table from VET; - option to remove PLs; - fixed and enhanced unit test; - removing unused config, causing travis to fail; - add CreateVariantIngestFiles integration test (#7071); - add sampleName (instead of NULL) to error message (#7074); - Update To handle if no data error (#7084); - Memory improvement when writing missing positions to pet (#7098); - added support for loading QUALapprox into VET (#7101); - Add -m flag to gsutil step; add dockstore branch filters to facilitate development (#7104); - updates to ImportGenomes and LoadBigQueryData (#7112); - Add ngs to cohort extract Dockerfile; remove exception catching in extract python script (#7113); - remove problematic storage_location imports (#7119); - Reduce memory and CPU for CreateImportTsvs task, check for files before attempting load (#7121); - add -m flag to gsutil mv step (#7129); - ah_var_store : Add sample file argument to cohort extract (#7117); - wip; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - forgot this file; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support loca",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:10442,Performance,optimiz,optimized,10442,"e-map-file instead of sample-map-table (#6872); - Moving extract wdls from variantstore repo (#6902); - update for genomes (#6918); - update paths; - update field name; - consolidate exome and genome code; - missing comma; - allow null for drop state. add to wdl; - fix when NON-REF has value (drop it anyway). add memory to wdl; - fix order of columns in vet; - Making ingest work on gvcfs without allele specific annotations (#6934); - fix partitioning (#6939); - more fixes for non-allele-specific gvcfs (#6945); - fix GT for ./.; - add in feature extract code. resulting vcf still has bugs (#6947); - Rudimentary support for other output types, support for scattering extract cohort (#6949); - added batch/interactive flag, removed samples (#6953); - WIP for Feature Extract code (#6958); - move wdl and cromwell_tests dirs under variantstore (#6961); - WDL to run feature extract, VQSR, and upload (#6966); - support for multiple PET/VETs (#6969); - Add filtering to ExtractCohort (#6971); - support for non-AS called data (#6975); - fixing bug in filtering (#6976); - Updating extract wdls to apply filtering (#6977); - moving from specops repo (#6983); - optimized TSV experiment, and range GQ dropping (#6987); - back to 30x defaults; - allow no filtering to be applied (#7004); - Separate bigquery table creation and data loading in LoadData (#7056); - WIP; - tieout scripts; - notes files; - updated diff scripts; - fixed bug...; - add wdl and inputs file for warp pipeline; - reverting logging; - included top level WDL; - use gnarly with BQ extract cohort; - remove unused file; - cleaning up; - tidy; - tidy up before PR; - tidy up before PR; - PR comments; - merge conflict misfires; - added example SQL to create alt allele table from VET; - option to remove PLs; - fixed and enhanced unit test; - removing unused config, causing travis to fail; - add CreateVariantIngestFiles integration test (#7071); - add sampleName (instead of NULL) to error message (#7074); - Update To handle if",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:10611,Performance,load,loading,10611,"e-map-file instead of sample-map-table (#6872); - Moving extract wdls from variantstore repo (#6902); - update for genomes (#6918); - update paths; - update field name; - consolidate exome and genome code; - missing comma; - allow null for drop state. add to wdl; - fix when NON-REF has value (drop it anyway). add memory to wdl; - fix order of columns in vet; - Making ingest work on gvcfs without allele specific annotations (#6934); - fix partitioning (#6939); - more fixes for non-allele-specific gvcfs (#6945); - fix GT for ./.; - add in feature extract code. resulting vcf still has bugs (#6947); - Rudimentary support for other output types, support for scattering extract cohort (#6949); - added batch/interactive flag, removed samples (#6953); - WIP for Feature Extract code (#6958); - move wdl and cromwell_tests dirs under variantstore (#6961); - WDL to run feature extract, VQSR, and upload (#6966); - support for multiple PET/VETs (#6969); - Add filtering to ExtractCohort (#6971); - support for non-AS called data (#6975); - fixing bug in filtering (#6976); - Updating extract wdls to apply filtering (#6977); - moving from specops repo (#6983); - optimized TSV experiment, and range GQ dropping (#6987); - back to 30x defaults; - allow no filtering to be applied (#7004); - Separate bigquery table creation and data loading in LoadData (#7056); - WIP; - tieout scripts; - notes files; - updated diff scripts; - fixed bug...; - add wdl and inputs file for warp pipeline; - reverting logging; - included top level WDL; - use gnarly with BQ extract cohort; - remove unused file; - cleaning up; - tidy; - tidy up before PR; - tidy up before PR; - PR comments; - merge conflict misfires; - added example SQL to create alt allele table from VET; - option to remove PLs; - fixed and enhanced unit test; - removing unused config, causing travis to fail; - add CreateVariantIngestFiles integration test (#7071); - add sampleName (instead of NULL) to error message (#7074); - Update To handle if",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:10622,Performance,Load,LoadData,10622,"e-map-file instead of sample-map-table (#6872); - Moving extract wdls from variantstore repo (#6902); - update for genomes (#6918); - update paths; - update field name; - consolidate exome and genome code; - missing comma; - allow null for drop state. add to wdl; - fix when NON-REF has value (drop it anyway). add memory to wdl; - fix order of columns in vet; - Making ingest work on gvcfs without allele specific annotations (#6934); - fix partitioning (#6939); - more fixes for non-allele-specific gvcfs (#6945); - fix GT for ./.; - add in feature extract code. resulting vcf still has bugs (#6947); - Rudimentary support for other output types, support for scattering extract cohort (#6949); - added batch/interactive flag, removed samples (#6953); - WIP for Feature Extract code (#6958); - move wdl and cromwell_tests dirs under variantstore (#6961); - WDL to run feature extract, VQSR, and upload (#6966); - support for multiple PET/VETs (#6969); - Add filtering to ExtractCohort (#6971); - support for non-AS called data (#6975); - fixing bug in filtering (#6976); - Updating extract wdls to apply filtering (#6977); - moving from specops repo (#6983); - optimized TSV experiment, and range GQ dropping (#6987); - back to 30x defaults; - allow no filtering to be applied (#7004); - Separate bigquery table creation and data loading in LoadData (#7056); - WIP; - tieout scripts; - notes files; - updated diff scripts; - fixed bug...; - add wdl and inputs file for warp pipeline; - reverting logging; - included top level WDL; - use gnarly with BQ extract cohort; - remove unused file; - cleaning up; - tidy; - tidy up before PR; - tidy up before PR; - PR comments; - merge conflict misfires; - added example SQL to create alt allele table from VET; - option to remove PLs; - fixed and enhanced unit test; - removing unused config, causing travis to fail; - add CreateVariantIngestFiles integration test (#7071); - add sampleName (instead of NULL) to error message (#7074); - Update To handle if",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:11392,Performance,load,loading,11392," table creation and data loading in LoadData (#7056); - WIP; - tieout scripts; - notes files; - updated diff scripts; - fixed bug...; - add wdl and inputs file for warp pipeline; - reverting logging; - included top level WDL; - use gnarly with BQ extract cohort; - remove unused file; - cleaning up; - tidy; - tidy up before PR; - tidy up before PR; - PR comments; - merge conflict misfires; - added example SQL to create alt allele table from VET; - option to remove PLs; - fixed and enhanced unit test; - removing unused config, causing travis to fail; - add CreateVariantIngestFiles integration test (#7071); - add sampleName (instead of NULL) to error message (#7074); - Update To handle if no data error (#7084); - Memory improvement when writing missing positions to pet (#7098); - added support for loading QUALapprox into VET (#7101); - Add -m flag to gsutil step; add dockstore branch filters to facilitate development (#7104); - updates to ImportGenomes and LoadBigQueryData (#7112); - Add ngs to cohort extract Dockerfile; remove exception catching in extract python script (#7113); - remove problematic storage_location imports (#7119); - Reduce memory and CPU for CreateImportTsvs task, check for files before attempting load (#7121); - add -m flag to gsutil mv step (#7129); - ah_var_store : Add sample file argument to cohort extract (#7117); - Perform full WGS cohort extract scientific tieout for 35 ACMG59 samples (#7106); - Enable Read/Execution Project for BQ Queries (#7136); - ah - optional service account (#7140); - Add load lock file to prevent accidental re-loading of data to BQ (#7138); - #251 Address gvcf no-calls missing QUALapprox and other features (#7146); - Job Add labels to BQ operations from GATK (Issues-199) (#7115); - parse map to list to avoid brackets and spaces in vcf output (#7168); - #259 Inline schema for importgenomes.wdl (#7171); - Created AvroFileReader and unittest, Update ExtractCohort and ExtractCohortEngine (#7174); - #224 Import WDL: handle ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:11554,Performance,Load,LoadBigQueryData,11554," table creation and data loading in LoadData (#7056); - WIP; - tieout scripts; - notes files; - updated diff scripts; - fixed bug...; - add wdl and inputs file for warp pipeline; - reverting logging; - included top level WDL; - use gnarly with BQ extract cohort; - remove unused file; - cleaning up; - tidy; - tidy up before PR; - tidy up before PR; - PR comments; - merge conflict misfires; - added example SQL to create alt allele table from VET; - option to remove PLs; - fixed and enhanced unit test; - removing unused config, causing travis to fail; - add CreateVariantIngestFiles integration test (#7071); - add sampleName (instead of NULL) to error message (#7074); - Update To handle if no data error (#7084); - Memory improvement when writing missing positions to pet (#7098); - added support for loading QUALapprox into VET (#7101); - Add -m flag to gsutil step; add dockstore branch filters to facilitate development (#7104); - updates to ImportGenomes and LoadBigQueryData (#7112); - Add ngs to cohort extract Dockerfile; remove exception catching in extract python script (#7113); - remove problematic storage_location imports (#7119); - Reduce memory and CPU for CreateImportTsvs task, check for files before attempting load (#7121); - add -m flag to gsutil mv step (#7129); - ah_var_store : Add sample file argument to cohort extract (#7117); - Perform full WGS cohort extract scientific tieout for 35 ACMG59 samples (#7106); - Enable Read/Execution Project for BQ Queries (#7136); - ah - optional service account (#7140); - Add load lock file to prevent accidental re-loading of data to BQ (#7138); - #251 Address gvcf no-calls missing QUALapprox and other features (#7146); - Job Add labels to BQ operations from GATK (Issues-199) (#7115); - parse map to list to avoid brackets and spaces in vcf output (#7168); - #259 Inline schema for importgenomes.wdl (#7171); - Created AvroFileReader and unittest, Update ExtractCohort and ExtractCohortEngine (#7174); - #224 Import WDL: handle ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:11820,Performance,load,load,11820," table creation and data loading in LoadData (#7056); - WIP; - tieout scripts; - notes files; - updated diff scripts; - fixed bug...; - add wdl and inputs file for warp pipeline; - reverting logging; - included top level WDL; - use gnarly with BQ extract cohort; - remove unused file; - cleaning up; - tidy; - tidy up before PR; - tidy up before PR; - PR comments; - merge conflict misfires; - added example SQL to create alt allele table from VET; - option to remove PLs; - fixed and enhanced unit test; - removing unused config, causing travis to fail; - add CreateVariantIngestFiles integration test (#7071); - add sampleName (instead of NULL) to error message (#7074); - Update To handle if no data error (#7084); - Memory improvement when writing missing positions to pet (#7098); - added support for loading QUALapprox into VET (#7101); - Add -m flag to gsutil step; add dockstore branch filters to facilitate development (#7104); - updates to ImportGenomes and LoadBigQueryData (#7112); - Add ngs to cohort extract Dockerfile; remove exception catching in extract python script (#7113); - remove problematic storage_location imports (#7119); - Reduce memory and CPU for CreateImportTsvs task, check for files before attempting load (#7121); - add -m flag to gsutil mv step (#7129); - ah_var_store : Add sample file argument to cohort extract (#7117); - Perform full WGS cohort extract scientific tieout for 35 ACMG59 samples (#7106); - Enable Read/Execution Project for BQ Queries (#7136); - ah - optional service account (#7140); - Add load lock file to prevent accidental re-loading of data to BQ (#7138); - #251 Address gvcf no-calls missing QUALapprox and other features (#7146); - Job Add labels to BQ operations from GATK (Issues-199) (#7115); - parse map to list to avoid brackets and spaces in vcf output (#7168); - #259 Inline schema for importgenomes.wdl (#7171); - Created AvroFileReader and unittest, Update ExtractCohort and ExtractCohortEngine (#7174); - #224 Import WDL: handle ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:11946,Performance,Perform,Perform,11946," table creation and data loading in LoadData (#7056); - WIP; - tieout scripts; - notes files; - updated diff scripts; - fixed bug...; - add wdl and inputs file for warp pipeline; - reverting logging; - included top level WDL; - use gnarly with BQ extract cohort; - remove unused file; - cleaning up; - tidy; - tidy up before PR; - tidy up before PR; - PR comments; - merge conflict misfires; - added example SQL to create alt allele table from VET; - option to remove PLs; - fixed and enhanced unit test; - removing unused config, causing travis to fail; - add CreateVariantIngestFiles integration test (#7071); - add sampleName (instead of NULL) to error message (#7074); - Update To handle if no data error (#7084); - Memory improvement when writing missing positions to pet (#7098); - added support for loading QUALapprox into VET (#7101); - Add -m flag to gsutil step; add dockstore branch filters to facilitate development (#7104); - updates to ImportGenomes and LoadBigQueryData (#7112); - Add ngs to cohort extract Dockerfile; remove exception catching in extract python script (#7113); - remove problematic storage_location imports (#7119); - Reduce memory and CPU for CreateImportTsvs task, check for files before attempting load (#7121); - add -m flag to gsutil mv step (#7129); - ah_var_store : Add sample file argument to cohort extract (#7117); - Perform full WGS cohort extract scientific tieout for 35 ACMG59 samples (#7106); - Enable Read/Execution Project for BQ Queries (#7136); - ah - optional service account (#7140); - Add load lock file to prevent accidental re-loading of data to BQ (#7138); - #251 Address gvcf no-calls missing QUALapprox and other features (#7146); - Job Add labels to BQ operations from GATK (Issues-199) (#7115); - parse map to list to avoid brackets and spaces in vcf output (#7168); - #259 Inline schema for importgenomes.wdl (#7171); - Created AvroFileReader and unittest, Update ExtractCohort and ExtractCohortEngine (#7174); - #224 Import WDL: handle ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:12130,Performance,load,load,12130," table creation and data loading in LoadData (#7056); - WIP; - tieout scripts; - notes files; - updated diff scripts; - fixed bug...; - add wdl and inputs file for warp pipeline; - reverting logging; - included top level WDL; - use gnarly with BQ extract cohort; - remove unused file; - cleaning up; - tidy; - tidy up before PR; - tidy up before PR; - PR comments; - merge conflict misfires; - added example SQL to create alt allele table from VET; - option to remove PLs; - fixed and enhanced unit test; - removing unused config, causing travis to fail; - add CreateVariantIngestFiles integration test (#7071); - add sampleName (instead of NULL) to error message (#7074); - Update To handle if no data error (#7084); - Memory improvement when writing missing positions to pet (#7098); - added support for loading QUALapprox into VET (#7101); - Add -m flag to gsutil step; add dockstore branch filters to facilitate development (#7104); - updates to ImportGenomes and LoadBigQueryData (#7112); - Add ngs to cohort extract Dockerfile; remove exception catching in extract python script (#7113); - remove problematic storage_location imports (#7119); - Reduce memory and CPU for CreateImportTsvs task, check for files before attempting load (#7121); - add -m flag to gsutil mv step (#7129); - ah_var_store : Add sample file argument to cohort extract (#7117); - Perform full WGS cohort extract scientific tieout for 35 ACMG59 samples (#7106); - Enable Read/Execution Project for BQ Queries (#7136); - ah - optional service account (#7140); - Add load lock file to prevent accidental re-loading of data to BQ (#7138); - #251 Address gvcf no-calls missing QUALapprox and other features (#7146); - Job Add labels to BQ operations from GATK (Issues-199) (#7115); - parse map to list to avoid brackets and spaces in vcf output (#7168); - #259 Inline schema for importgenomes.wdl (#7171); - Created AvroFileReader and unittest, Update ExtractCohort and ExtractCohortEngine (#7174); - #224 Import WDL: handle ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:12170,Performance,load,loading,12170," table creation and data loading in LoadData (#7056); - WIP; - tieout scripts; - notes files; - updated diff scripts; - fixed bug...; - add wdl and inputs file for warp pipeline; - reverting logging; - included top level WDL; - use gnarly with BQ extract cohort; - remove unused file; - cleaning up; - tidy; - tidy up before PR; - tidy up before PR; - PR comments; - merge conflict misfires; - added example SQL to create alt allele table from VET; - option to remove PLs; - fixed and enhanced unit test; - removing unused config, causing travis to fail; - add CreateVariantIngestFiles integration test (#7071); - add sampleName (instead of NULL) to error message (#7074); - Update To handle if no data error (#7084); - Memory improvement when writing missing positions to pet (#7098); - added support for loading QUALapprox into VET (#7101); - Add -m flag to gsutil step; add dockstore branch filters to facilitate development (#7104); - updates to ImportGenomes and LoadBigQueryData (#7112); - Add ngs to cohort extract Dockerfile; remove exception catching in extract python script (#7113); - remove problematic storage_location imports (#7119); - Reduce memory and CPU for CreateImportTsvs task, check for files before attempting load (#7121); - add -m flag to gsutil mv step (#7129); - ah_var_store : Add sample file argument to cohort extract (#7117); - Perform full WGS cohort extract scientific tieout for 35 ACMG59 samples (#7106); - Enable Read/Execution Project for BQ Queries (#7136); - ah - optional service account (#7140); - Add load lock file to prevent accidental re-loading of data to BQ (#7138); - #251 Address gvcf no-calls missing QUALapprox and other features (#7146); - Job Add labels to BQ operations from GATK (Issues-199) (#7115); - parse map to list to avoid brackets and spaces in vcf output (#7168); - #259 Inline schema for importgenomes.wdl (#7171); - Created AvroFileReader and unittest, Update ExtractCohort and ExtractCohortEngine (#7174); - #224 Import WDL: handle ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:12928,Performance,optimiz,optimizations,12928," QUALapprox and other features (#7146); - Job Add labels to BQ operations from GATK (Issues-199) (#7115); - parse map to list to avoid brackets and spaces in vcf output (#7168); - #259 Inline schema for importgenomes.wdl (#7171); - Created AvroFileReader and unittest, Update ExtractCohort and ExtractCohortEngine (#7174); - #224 Import WDL: handle 15 TB /table /day import limit (#7167); - #260 filter out AS_QD, SOR, FS from cohort extract VCF (#7173); - Full scientific validation via end to end comparison of filtered results between WARP and BQ (#7179); - Cherry pick of commits to fix GATK tests from master (#7183); - ExtractCohort supports -XL exclusion and follows intervals, other optimizations (#7181); - ExtractFeatures supports -XL exclusion and follows intervals, other optimizations (#7184); - change 0/0 GQ0 sites to nocalls (#7190); - updated (#7195); - Rename ""metadata"" table to ""sample_info"" table, fix vet schema (#7196); - Allow users to specify VQSLOD sensitivity and apply threshold in ExtractCohort (#7194); - Calculate and Store site-level QCs (#7197); - Filter Failing QC Sites from Extract (#7201); - WDLize GvsPrepareCallset (briefly known as CreateCohortTable) (#7200); - default drop_state to 60, but allow NONE as input (#7206); - SA support and consistent naming for all GVS WDLs (#7205); - fix GvsExtractCallset inputs file (#7210); - add clustering to tables (#7207); - add vqsr cutoffs to GvsExtractCallset wdl; clean up dockstore yml (#7209); - Avro test (#7192); - Enable call caching of TSV generation in GvsImportGenomes (#7226); - 266 Clean up ExtractCohort -- remove query mode param (#7227); - 288 Add an excess alleles param (#7221); - take sample name as a param (#7236); - How to run GIAB comparisons (#7237); - Update GvsCreateFilterSet.wdl (#7239); - Use GatherVcfsCloud in GvsCreateFilterSet.wdl (#7241); - parameterize TTL with defaults, reduce memory allocation (#7244); - Addressing OOM in CohortExtract (#7245); - make outputs optional, change case",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:13021,Performance,optimiz,optimizations,13021," QUALapprox and other features (#7146); - Job Add labels to BQ operations from GATK (Issues-199) (#7115); - parse map to list to avoid brackets and spaces in vcf output (#7168); - #259 Inline schema for importgenomes.wdl (#7171); - Created AvroFileReader and unittest, Update ExtractCohort and ExtractCohortEngine (#7174); - #224 Import WDL: handle 15 TB /table /day import limit (#7167); - #260 filter out AS_QD, SOR, FS from cohort extract VCF (#7173); - Full scientific validation via end to end comparison of filtered results between WARP and BQ (#7179); - Cherry pick of commits to fix GATK tests from master (#7183); - ExtractCohort supports -XL exclusion and follows intervals, other optimizations (#7181); - ExtractFeatures supports -XL exclusion and follows intervals, other optimizations (#7184); - change 0/0 GQ0 sites to nocalls (#7190); - updated (#7195); - Rename ""metadata"" table to ""sample_info"" table, fix vet schema (#7196); - Allow users to specify VQSLOD sensitivity and apply threshold in ExtractCohort (#7194); - Calculate and Store site-level QCs (#7197); - Filter Failing QC Sites from Extract (#7201); - WDLize GvsPrepareCallset (briefly known as CreateCohortTable) (#7200); - default drop_state to 60, but allow NONE as input (#7206); - SA support and consistent naming for all GVS WDLs (#7205); - fix GvsExtractCallset inputs file (#7210); - add clustering to tables (#7207); - add vqsr cutoffs to GvsExtractCallset wdl; clean up dockstore yml (#7209); - Avro test (#7192); - Enable call caching of TSV generation in GvsImportGenomes (#7226); - 266 Clean up ExtractCohort -- remove query mode param (#7227); - 288 Add an excess alleles param (#7221); - take sample name as a param (#7236); - How to run GIAB comparisons (#7237); - Update GvsCreateFilterSet.wdl (#7239); - Use GatherVcfsCloud in GvsCreateFilterSet.wdl (#7241); - parameterize TTL with defaults, reduce memory allocation (#7244); - Addressing OOM in CohortExtract (#7245); - make outputs optional, change case",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:15918,Performance,load,loads,15918,"epare, factored out sample name (#7288); - Remove training sites only param from ExtractFeatures broadinstitute/dsp-spec-ops#261; - add param for mem for indels (#7282); - Ah prepare localize option (#7299); - Export sites only vcf STEP 1-- 317 add AC, AN, AF to the final VCF (#7279); - AoU GVS Cohort Extract wdl (#7242); - reliability (#7310); - bump to include FT tag filtering (#7316); - First pass at a Terra QuickStart (#7267); - Ah fix timestamp query (#7319); - 313 Cleanup Extract Cohort params (#7293); - bump bq storage version. See GVS-332 (#7330); - Variant Store extraction - Add VCF size to output (#7329); - add WARP-style scattering to SNPsVariantRecalibrator in GvsCreateFilterSet (#7320); - added ref ranges support (#7337); - 318 Sites only filtered vcf then annotate wdl (#7305); - Replace service_account_json (file) with service_account_json_path (string) to allow call-caching (#7347); - Parallelize create filterset by breaking out the 3 filter set file creation/loads into separate tasks (#7342); - Create WDL to validate VAT and add first test (#7352); - Add task for VAT validation #3 (#7360); - Add task for VAT validation #4 (#7363); - Instructions on how to download BQ Metadata and visualize results (#7359); - don't mix contigs, rightsize memory (#7361); - Add custom annotations as ac an af (#7351); - Add task for VAT validation #8 & 9 (#7364); - added bcftools, upgraded gcloud version (#7369); - fix wdl (#7378); - Update .dockstore.yml; - Add VAT validation rule #5 [VS-16] (#7365); - Add VAT validation rule #7 [VS-14] and validation rule #6 [VS-15] (#7379); - Batching of samples for create import TSVs (#7382); - Add VAT validation rule #2 [VS-19] (#7374); - Create VAT scripts directory (#7386); - fixing SA change from file to string (#7371); - add extract_subpop script (#7387); - Add is_loaded column to sample_info and logic to populate after ingest [VS-158] (#7389); - Add Gnomad subpopulation info into the VAT (#7381); - implement GVS ID assignment (#",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:16949,Performance,load,loading,16949,"#7359); - don't mix contigs, rightsize memory (#7361); - Add custom annotations as ac an af (#7351); - Add task for VAT validation #8 & 9 (#7364); - added bcftools, upgraded gcloud version (#7369); - fix wdl (#7378); - Update .dockstore.yml; - Add VAT validation rule #5 [VS-16] (#7365); - Add VAT validation rule #7 [VS-14] and validation rule #6 [VS-15] (#7379); - Batching of samples for create import TSVs (#7382); - Add VAT validation rule #2 [VS-19] (#7374); - Create VAT scripts directory (#7386); - fixing SA change from file to string (#7371); - add extract_subpop script (#7387); - Add is_loaded column to sample_info and logic to populate after ingest [VS-158] (#7389); - Add Gnomad subpopulation info into the VAT (#7381); - implement GVS ID assignment (#7355); - no longer loading sample info table in this wdl (#7407); - divide up creation/population of temp pet table [VS-48] (#7395); - Sample QC metrics (#7396); - update import for is_loaded (#7416); - fix partition end, add 1 (#7420); - Fixes to CreateFilterSet and ExtractCallset from 30K run (#7423); - Also changed file size from Int to Float in SumBytes task python (#7429); - Adding the subpopulation calculations to the VAT creation WDL (#7399); - 154 De obfuscate (#7435); - filter on gvs_ids for workflow (#7428); - update for assign ids and changes in import (#7439); - need to loop through sets when moving to done (#7440); - add option for create filter set to use sample_info with is_loaded (#7434); - remove dead branch (#7443); - Scaling the VAT -- switch the input to take in a file of vcf shard file names (#7446); - dockstore testing: move validate vat inputs (#7449); - Update GVS sample QC to support multiple callsets per datasset [VS-177] (#7451); - Update GvsImportGenomes.wdl (#7462); - Add extraction uuid BQ label to GvsPrepareCallstep from GvsExtractCohortFromSampleNames (#7458); - Add manifest summary file to GvsExtractCallset (#7457); - Create workflow to create and populate alt_allele table [VS-51] (",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:18785,Performance,load,loaded,18785,"463); - formatting on sample QC README; - formatting change #2 to sample QC README; - address VS-152, remove extra headers from extract (#7466); - Update GvsExtractCallset.example.inputs.json (#7469); - Add ability to copy interval list files to gs directory [VS-191] (#7467); - add an expiration date to the temp tables (#7455); - fix the check for duplicates in import genomes (#7470); - added job ID to alt_allele population call output [VS-194] (#7473); - added steps and deliverables to GVS README [VS-181] (#7452); - Ah check the is loaded field in feature extract (#7475); - changes to put pet data directly into data table (#7478); - added override for ExtractTasks' preemptible value (#7477); - bcftools to the rescue (#7456); - execute_with_retry() refactor and error handling improvements [VS-159] (#7480); - Small updates to GvsExtractCallset from beta callset, new workflow for re-scattered shards (#7493); - add flag in prepare to print out sql instead of executing (#7501); - Workflow to re-scatter and then merge ""problematic"" intervals from ExtractCallset [VS-209] (#7495); - changed README to reflect comments from Lee [VS-210] (#7502); - Export the VAT into GCS (#7472); - addresses VS-219 (#7508); - small fix to MergeVCFs (#7517); - small fixes to GVS pipeline (#7522); - make sure ExtractTask is run on all interval files; - Revert ""make sure ExtractTask is run on all interval files""; - make sure ExtractTask is run on all interval files (#7527); - Remove Sites only step from the VAT creation WDL (#7510); - fix bad argument processing for bool (#7529); - Support for TDR DRS URIs in Import (#7528); - Match format of filename output in GvsRescatterCallsetInterval (#7539); - Reference block storage and query support (#7498); - update docs (#7540); - Kc fix rr load bug (#7550); - Update .dockstore.yml (#7553); - Ah add reblocking wdl (#7544); - Scatter over all interval files, not just scatter count (#7551); - fixed docker (#7558); - take advantage of fixed version of Spl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:20032,Performance,load,load,20032,"463); - formatting on sample QC README; - formatting change #2 to sample QC README; - address VS-152, remove extra headers from extract (#7466); - Update GvsExtractCallset.example.inputs.json (#7469); - Add ability to copy interval list files to gs directory [VS-191] (#7467); - add an expiration date to the temp tables (#7455); - fix the check for duplicates in import genomes (#7470); - added job ID to alt_allele population call output [VS-194] (#7473); - added steps and deliverables to GVS README [VS-181] (#7452); - Ah check the is loaded field in feature extract (#7475); - changes to put pet data directly into data table (#7478); - added override for ExtractTasks' preemptible value (#7477); - bcftools to the rescue (#7456); - execute_with_retry() refactor and error handling improvements [VS-159] (#7480); - Small updates to GvsExtractCallset from beta callset, new workflow for re-scattered shards (#7493); - add flag in prepare to print out sql instead of executing (#7501); - Workflow to re-scatter and then merge ""problematic"" intervals from ExtractCallset [VS-209] (#7495); - changed README to reflect comments from Lee [VS-210] (#7502); - Export the VAT into GCS (#7472); - addresses VS-219 (#7508); - small fix to MergeVCFs (#7517); - small fixes to GVS pipeline (#7522); - make sure ExtractTask is run on all interval files; - Revert ""make sure ExtractTask is run on all interval files""; - make sure ExtractTask is run on all interval files (#7527); - Remove Sites only step from the VAT creation WDL (#7510); - fix bad argument processing for bool (#7529); - Support for TDR DRS URIs in Import (#7528); - Match format of filename output in GvsRescatterCallsetInterval (#7539); - Reference block storage and query support (#7498); - update docs (#7540); - Kc fix rr load bug (#7550); - Update .dockstore.yml (#7553); - Ah add reblocking wdl (#7544); - Scatter over all interval files, not just scatter count (#7551); - fixed docker (#7558); - take advantage of fixed version of Spl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:20865,Performance,perform,performance,20865,"filename output in GvsRescatterCallsetInterval (#7539); - Reference block storage and query support (#7498); - update docs (#7540); - Kc fix rr load bug (#7550); - Update .dockstore.yml (#7553); - Ah add reblocking wdl (#7544); - Scatter over all interval files, not just scatter count (#7551); - fixed docker (#7558); - take advantage of fixed version of SplitIntervals (#7566); - Document AoU-specific tieout [VS-233] (#7552); - bad param assignment in aou reblocking (#7572); - Small fixes to ImportGenomes (non-write api version) (#7574); - Ah change output of reblocking wdl to external path (#7575); - close BQ Readers (#7583); - Ah spike writeapi (#7530); - bump WDL jar (#7593); - read api bytes logging, upgrade bigquery client versions (#7601); - bump (#7610); - upgrade log4j to 2.17 (#7616); - Add drop_state default of Forty to extract (#7619); - Kc fix type (#7620); - VAT cleanup and documentation (#7531); - fix empty flush (#7627); - presorted avro files, fix performance issue (#7635); - WIP extract for ranges (#7640); - VS-268 import more samples at once (#7629); - clustering vqsr tables by location (#7656); - First Version of a weight-based splitter (#7643); - Update GvsExtractCallset.wdl; - Quoting of table names (#7666); - docs for analysis of shard runtimes for balanced sharding (#7645); - Wire through GvsExtractCohortFromSampleNames with new prepare/extract [VS-283] (#7654); - Update GvsExtractCallset.wdl (#7678); - cherry pick lb_lfs_force change (#7683); - Tweak ingest messaging and failure mode [VS-267] (#7680); - Additional tweaks for GvsExtractCohortFromSampleNames [VS-283] (#7698); - VS-280 Create a VAT intermediary (#7657); - There something about split intervals [VS-306] (#7694); - VS 284 Add prepare step to Quick Start (#7685); - VS-222 dont hard code the dataset name! (#7704); - fixed bug; added tests (#7717); - Clean up optional and inconsistently named inputs [VS-294] [VS-218] (#7715); - VS-263 notes on ingest and beyond (#7618); - Add task to Ex",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:22408,Performance,Perform,Performance,22408,"ntimes for balanced sharding (#7645); - Wire through GvsExtractCohortFromSampleNames with new prepare/extract [VS-283] (#7654); - Update GvsExtractCallset.wdl (#7678); - cherry pick lb_lfs_force change (#7683); - Tweak ingest messaging and failure mode [VS-267] (#7680); - Additional tweaks for GvsExtractCohortFromSampleNames [VS-283] (#7698); - VS-280 Create a VAT intermediary (#7657); - There something about split intervals [VS-306] (#7694); - VS 284 Add prepare step to Quick Start (#7685); - VS-222 dont hard code the dataset name! (#7704); - fixed bug; added tests (#7717); - Clean up optional and inconsistently named inputs [VS-294] [VS-218] (#7715); - VS-263 notes on ingest and beyond (#7618); - Add task to ExtractCallset that verifies filter_set_name exists in GVS dataset [VS-335] (#7734); - Clean up input json files to reflect changes inputs [VS-337] (#7733); - used constants; implemented non-AS transformation (#7718); - Pass dataset name to gatk ExtractFeatures (#7735); - Add withdrawn and is_control columns [VS-70] [VS-213] (#7736); - Allow interval lists that require the SA to see (#7743); - allow for gatk to be overridden, update with known good jar (#7758); - VS-361 Add GvsWithdrawSamples wdl (#7765); - Extract Performance Improvements (#7686); - Don't put withdrawn sample data in alt_allele table [VS-369] (#7762); - remove PET code (#7768); - Adding AD for scale testing VS 225 add AD (#7713); - Deterministic Sample ID assignments [VS-371] (#7770); - remove R scripts from filtering (#7781); - Remove an old ""temp table"" dataset (#7780); - Clean up LocalizeFile [VS-314] (#7771); - Remove pet code from CreateVariantIngestFiles and friends [VS-375] (#7773); - 317 remove excess header values in VCF extract (#7786); - correct auth in split intervals (#7790); - Add code to (optionally) zero pad the vcf filename. (#7783); - LoadData `maxRetries` parameterized, default increased [VS-383] (#7791); - Update to latest version of ah_var_store gatk override jar (#7793); ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:23025,Performance,Load,LoadData,23025,"res (#7735); - Add withdrawn and is_control columns [VS-70] [VS-213] (#7736); - Allow interval lists that require the SA to see (#7743); - allow for gatk to be overridden, update with known good jar (#7758); - VS-361 Add GvsWithdrawSamples wdl (#7765); - Extract Performance Improvements (#7686); - Don't put withdrawn sample data in alt_allele table [VS-369] (#7762); - remove PET code (#7768); - Adding AD for scale testing VS 225 add AD (#7713); - Deterministic Sample ID assignments [VS-371] (#7770); - remove R scripts from filtering (#7781); - Remove an old ""temp table"" dataset (#7780); - Clean up LocalizeFile [VS-314] (#7771); - Remove pet code from CreateVariantIngestFiles and friends [VS-375] (#7773); - 317 remove excess header values in VCF extract (#7786); - correct auth in split intervals (#7790); - Add code to (optionally) zero pad the vcf filename. (#7783); - LoadData `maxRetries` parameterized, default increased [VS-383] (#7791); - Update to latest version of ah_var_store gatk override jar (#7793); - GvsUnified WDL to wrap the 6 core GVS WDLs [VS-382] (#7789); - Pinned typing_extensions python package to 4.1.1 to fix conda environment. (#7802); - WeightedSplitInterval fixes [VS-384] [VS-332] (#7795); - Replace Travis with GithubActions (#7754); - Docker build only lfs pulls main/src/resources/large (#7727); - Clean up gatk jars -- looks like we are not passing them properly in the extract (#7788); - Fix typo that broke git lfs pull (#7806); - Document AoU SOP (up to the VAT) [VS-63] (#7807); - Incident VS 365 clinvar classification fix (#7769); - VS-390. Add precision and sensitivity wdl (#7813); - Quickstart based integration test [VS-357] (#7812); - 365 vat python testing additions (#7756); - VS 396 clinvar grabs too many values (#7823); - Added a test to validate WDLs in the scripts directory. (#7826) (#7829); - VAT Performance / Reliability Improvements (#7828); - VAT naming conventions [VS-410] (#7827); - Rc remove ad from vat (#7832); - bugfix, we were",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:24005,Performance,Perform,Performance,24005,"89); - Pinned typing_extensions python package to 4.1.1 to fix conda environment. (#7802); - WeightedSplitInterval fixes [VS-384] [VS-332] (#7795); - Replace Travis with GithubActions (#7754); - Docker build only lfs pulls main/src/resources/large (#7727); - Clean up gatk jars -- looks like we are not passing them properly in the extract (#7788); - Fix typo that broke git lfs pull (#7806); - Document AoU SOP (up to the VAT) [VS-63] (#7807); - Incident VS 365 clinvar classification fix (#7769); - VS-390. Add precision and sensitivity wdl (#7813); - Quickstart based integration test [VS-357] (#7812); - 365 vat python testing additions (#7756); - VS 396 clinvar grabs too many values (#7823); - Added a test to validate WDLs in the scripts directory. (#7826) (#7829); - VAT Performance / Reliability Improvements (#7828); - VAT naming conventions [VS-410] (#7827); - Rc remove ad from vat (#7832); - bugfix, we were trying to grep a binary file (#7837); - Cleanup scripts/variantstore [VS-414] (#7834); - Merge VAT TSV files into single bgzipped file [VS-304] (#7848); - Handle fully and partially loaded samples [VS-262] [VS-258] (#7843); - Ingest Error Handling Fixes [VS-261] (#7841); - First cut at a python notebook to validate inputs. (#7845); - Compute filter scatter [VS-392] (#7852); - remove withdrawn req (#7844); - Improve import error message [VS-437] (#7855); - Fix Input Validation python notebook (#7853); - Add VAT Validation check that aa_change and exon_number are consistently set. (#7850); - Ingest 10K [VS-344] (#7860); - X/Y chromosome reweighting for better extract shard runtime balance [VS-389] (#7868); - VET Ingest Validation / Allow Ingest of non-VQSR'ed data (#7870); - Fix AoU workflow bugs (#7874); - Curate input arrays to skip already ingested sample data [VS-246] (#7862); - KM upload GVS product sheet (#7883); - Default extract scatter width [VS-415] (#7878); - Volatile tasks review [VS-447] (#7880); - Update Quickstart Integration for X/Y scaling changes ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:24329,Performance,load,loaded,24329,"89); - Pinned typing_extensions python package to 4.1.1 to fix conda environment. (#7802); - WeightedSplitInterval fixes [VS-384] [VS-332] (#7795); - Replace Travis with GithubActions (#7754); - Docker build only lfs pulls main/src/resources/large (#7727); - Clean up gatk jars -- looks like we are not passing them properly in the extract (#7788); - Fix typo that broke git lfs pull (#7806); - Document AoU SOP (up to the VAT) [VS-63] (#7807); - Incident VS 365 clinvar classification fix (#7769); - VS-390. Add precision and sensitivity wdl (#7813); - Quickstart based integration test [VS-357] (#7812); - 365 vat python testing additions (#7756); - VS 396 clinvar grabs too many values (#7823); - Added a test to validate WDLs in the scripts directory. (#7826) (#7829); - VAT Performance / Reliability Improvements (#7828); - VAT naming conventions [VS-410] (#7827); - Rc remove ad from vat (#7832); - bugfix, we were trying to grep a binary file (#7837); - Cleanup scripts/variantstore [VS-414] (#7834); - Merge VAT TSV files into single bgzipped file [VS-304] (#7848); - Handle fully and partially loaded samples [VS-262] [VS-258] (#7843); - Ingest Error Handling Fixes [VS-261] (#7841); - First cut at a python notebook to validate inputs. (#7845); - Compute filter scatter [VS-392] (#7852); - remove withdrawn req (#7844); - Improve import error message [VS-437] (#7855); - Fix Input Validation python notebook (#7853); - Add VAT Validation check that aa_change and exon_number are consistently set. (#7850); - Ingest 10K [VS-344] (#7860); - X/Y chromosome reweighting for better extract shard runtime balance [VS-389] (#7868); - VET Ingest Validation / Allow Ingest of non-VQSR'ed data (#7870); - Fix AoU workflow bugs (#7874); - Curate input arrays to skip already ingested sample data [VS-246] (#7862); - KM upload GVS product sheet (#7883); - Default extract scatter width [VS-415] (#7878); - Volatile tasks review [VS-447] (#7880); - Update Quickstart Integration for X/Y scaling changes ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:25324,Performance,race condition,race condition,25324,"to validate inputs. (#7845); - Compute filter scatter [VS-392] (#7852); - remove withdrawn req (#7844); - Improve import error message [VS-437] (#7855); - Fix Input Validation python notebook (#7853); - Add VAT Validation check that aa_change and exon_number are consistently set. (#7850); - Ingest 10K [VS-344] (#7860); - X/Y chromosome reweighting for better extract shard runtime balance [VS-389] (#7868); - VET Ingest Validation / Allow Ingest of non-VQSR'ed data (#7870); - Fix AoU workflow bugs (#7874); - Curate input arrays to skip already ingested sample data [VS-246] (#7862); - KM upload GVS product sheet (#7883); - Default extract scatter width [VS-415] (#7878); - Volatile tasks review [VS-447] (#7880); - Update Quickstart Integration for X/Y scaling changes [VS-464] (#7881); - clean up dockstore; - Rc vs 63 vat sop documentation (#7879); - Fix up FQ and race condition issues with volatile tasks work [VS-478] (#7888); - Use gvs-internal project in integration test (#7901); - Add cost observability BQ table [VS-441] (#7891); - Add preliminary labels to queries [VS-381] (#7902); - Workflow compute costs [VS-472] (#7905); - Fix bug and update images (#7912); - VS 483 Beta user wdl (#7894); - Core storage model cost [VS-473] (#7913); - Update Quickstart & Integration to use re-blocked v2 gVCFs [VS-491] (#7924); - KM GVS documentation (#7903); - Track BigQuery costs of GVS python VS-480 (#7915); - Read cost observability table [VS-475] (#7923); - Fix Race Condition, Add Support for Extract by Array of Sample Names (ie from a Sample Set) (#7917); - Rightsize import batches [VS-486] (#7925); - [AoU DRC] Support uppercase site_ids for reblocking (#7929); - Populate cost metadata for GATK tasks. (#7919); - remove accidentally added input (#7931); - VS_492 - Beta User Jar release (#7934); - Cost WDL should throw on FISS API errors [VS-518] (#7942); - Fix bad check for missing workflow name [VS-520] (#7943); - Remove usage of service account from GvsValidateVAT.wdl (#7937",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:25927,Performance,Race Condition,Race Condition,25927,"to validate inputs. (#7845); - Compute filter scatter [VS-392] (#7852); - remove withdrawn req (#7844); - Improve import error message [VS-437] (#7855); - Fix Input Validation python notebook (#7853); - Add VAT Validation check that aa_change and exon_number are consistently set. (#7850); - Ingest 10K [VS-344] (#7860); - X/Y chromosome reweighting for better extract shard runtime balance [VS-389] (#7868); - VET Ingest Validation / Allow Ingest of non-VQSR'ed data (#7870); - Fix AoU workflow bugs (#7874); - Curate input arrays to skip already ingested sample data [VS-246] (#7862); - KM upload GVS product sheet (#7883); - Default extract scatter width [VS-415] (#7878); - Volatile tasks review [VS-447] (#7880); - Update Quickstart Integration for X/Y scaling changes [VS-464] (#7881); - clean up dockstore; - Rc vs 63 vat sop documentation (#7879); - Fix up FQ and race condition issues with volatile tasks work [VS-478] (#7888); - Use gvs-internal project in integration test (#7901); - Add cost observability BQ table [VS-441] (#7891); - Add preliminary labels to queries [VS-381] (#7902); - Workflow compute costs [VS-472] (#7905); - Fix bug and update images (#7912); - VS 483 Beta user wdl (#7894); - Core storage model cost [VS-473] (#7913); - Update Quickstart & Integration to use re-blocked v2 gVCFs [VS-491] (#7924); - KM GVS documentation (#7903); - Track BigQuery costs of GVS python VS-480 (#7915); - Read cost observability table [VS-475] (#7923); - Fix Race Condition, Add Support for Extract by Array of Sample Names (ie from a Sample Set) (#7917); - Rightsize import batches [VS-486] (#7925); - [AoU DRC] Support uppercase site_ids for reblocking (#7929); - Populate cost metadata for GATK tasks. (#7919); - remove accidentally added input (#7931); - VS_492 - Beta User Jar release (#7934); - Cost WDL should throw on FISS API errors [VS-518] (#7942); - Fix bad check for missing workflow name [VS-520] (#7943); - Remove usage of service account from GvsValidateVAT.wdl (#7937",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:12366,Safety,avoid,avoid,12366," table creation and data loading in LoadData (#7056); - WIP; - tieout scripts; - notes files; - updated diff scripts; - fixed bug...; - add wdl and inputs file for warp pipeline; - reverting logging; - included top level WDL; - use gnarly with BQ extract cohort; - remove unused file; - cleaning up; - tidy; - tidy up before PR; - tidy up before PR; - PR comments; - merge conflict misfires; - added example SQL to create alt allele table from VET; - option to remove PLs; - fixed and enhanced unit test; - removing unused config, causing travis to fail; - add CreateVariantIngestFiles integration test (#7071); - add sampleName (instead of NULL) to error message (#7074); - Update To handle if no data error (#7084); - Memory improvement when writing missing positions to pet (#7098); - added support for loading QUALapprox into VET (#7101); - Add -m flag to gsutil step; add dockstore branch filters to facilitate development (#7104); - updates to ImportGenomes and LoadBigQueryData (#7112); - Add ngs to cohort extract Dockerfile; remove exception catching in extract python script (#7113); - remove problematic storage_location imports (#7119); - Reduce memory and CPU for CreateImportTsvs task, check for files before attempting load (#7121); - add -m flag to gsutil mv step (#7129); - ah_var_store : Add sample file argument to cohort extract (#7117); - Perform full WGS cohort extract scientific tieout for 35 ACMG59 samples (#7106); - Enable Read/Execution Project for BQ Queries (#7136); - ah - optional service account (#7140); - Add load lock file to prevent accidental re-loading of data to BQ (#7138); - #251 Address gvcf no-calls missing QUALapprox and other features (#7146); - Job Add labels to BQ operations from GATK (Issues-199) (#7115); - parse map to list to avoid brackets and spaces in vcf output (#7168); - #259 Inline schema for importgenomes.wdl (#7171); - Created AvroFileReader and unittest, Update ExtractCohort and ExtractCohortEngine (#7174); - #224 Import WDL: handle ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:12710,Security,validat,validation,12710," QUALapprox and other features (#7146); - Job Add labels to BQ operations from GATK (Issues-199) (#7115); - parse map to list to avoid brackets and spaces in vcf output (#7168); - #259 Inline schema for importgenomes.wdl (#7171); - Created AvroFileReader and unittest, Update ExtractCohort and ExtractCohortEngine (#7174); - #224 Import WDL: handle 15 TB /table /day import limit (#7167); - #260 filter out AS_QD, SOR, FS from cohort extract VCF (#7173); - Full scientific validation via end to end comparison of filtered results between WARP and BQ (#7179); - Cherry pick of commits to fix GATK tests from master (#7183); - ExtractCohort supports -XL exclusion and follows intervals, other optimizations (#7181); - ExtractFeatures supports -XL exclusion and follows intervals, other optimizations (#7184); - change 0/0 GQ0 sites to nocalls (#7190); - updated (#7195); - Rename ""metadata"" table to ""sample_info"" table, fix vet schema (#7196); - Allow users to specify VQSLOD sensitivity and apply threshold in ExtractCohort (#7194); - Calculate and Store site-level QCs (#7197); - Filter Failing QC Sites from Extract (#7201); - WDLize GvsPrepareCallset (briefly known as CreateCohortTable) (#7200); - default drop_state to 60, but allow NONE as input (#7206); - SA support and consistent naming for all GVS WDLs (#7205); - fix GvsExtractCallset inputs file (#7210); - add clustering to tables (#7207); - add vqsr cutoffs to GvsExtractCallset wdl; clean up dockstore yml (#7209); - Avro test (#7192); - Enable call caching of TSV generation in GvsImportGenomes (#7226); - 266 Clean up ExtractCohort -- remove query mode param (#7227); - 288 Add an excess alleles param (#7221); - take sample name as a param (#7236); - How to run GIAB comparisons (#7237); - Update GvsCreateFilterSet.wdl (#7239); - Use GatherVcfsCloud in GvsCreateFilterSet.wdl (#7241); - parameterize TTL with defaults, reduce memory allocation (#7244); - Addressing OOM in CohortExtract (#7245); - make outputs optional, change case",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:15969,Security,validat,validate,15969,"epare, factored out sample name (#7288); - Remove training sites only param from ExtractFeatures broadinstitute/dsp-spec-ops#261; - add param for mem for indels (#7282); - Ah prepare localize option (#7299); - Export sites only vcf STEP 1-- 317 add AC, AN, AF to the final VCF (#7279); - AoU GVS Cohort Extract wdl (#7242); - reliability (#7310); - bump to include FT tag filtering (#7316); - First pass at a Terra QuickStart (#7267); - Ah fix timestamp query (#7319); - 313 Cleanup Extract Cohort params (#7293); - bump bq storage version. See GVS-332 (#7330); - Variant Store extraction - Add VCF size to output (#7329); - add WARP-style scattering to SNPsVariantRecalibrator in GvsCreateFilterSet (#7320); - added ref ranges support (#7337); - 318 Sites only filtered vcf then annotate wdl (#7305); - Replace service_account_json (file) with service_account_json_path (string) to allow call-caching (#7347); - Parallelize create filterset by breaking out the 3 filter set file creation/loads into separate tasks (#7342); - Create WDL to validate VAT and add first test (#7352); - Add task for VAT validation #3 (#7360); - Add task for VAT validation #4 (#7363); - Instructions on how to download BQ Metadata and visualize results (#7359); - don't mix contigs, rightsize memory (#7361); - Add custom annotations as ac an af (#7351); - Add task for VAT validation #8 & 9 (#7364); - added bcftools, upgraded gcloud version (#7369); - fix wdl (#7378); - Update .dockstore.yml; - Add VAT validation rule #5 [VS-16] (#7365); - Add VAT validation rule #7 [VS-14] and validation rule #6 [VS-15] (#7379); - Batching of samples for create import TSVs (#7382); - Add VAT validation rule #2 [VS-19] (#7374); - Create VAT scripts directory (#7386); - fixing SA change from file to string (#7371); - add extract_subpop script (#7387); - Add is_loaded column to sample_info and logic to populate after ingest [VS-158] (#7389); - Add Gnomad subpopulation info into the VAT (#7381); - implement GVS ID assignment (#",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:16029,Security,validat,validation,16029,"epare, factored out sample name (#7288); - Remove training sites only param from ExtractFeatures broadinstitute/dsp-spec-ops#261; - add param for mem for indels (#7282); - Ah prepare localize option (#7299); - Export sites only vcf STEP 1-- 317 add AC, AN, AF to the final VCF (#7279); - AoU GVS Cohort Extract wdl (#7242); - reliability (#7310); - bump to include FT tag filtering (#7316); - First pass at a Terra QuickStart (#7267); - Ah fix timestamp query (#7319); - 313 Cleanup Extract Cohort params (#7293); - bump bq storage version. See GVS-332 (#7330); - Variant Store extraction - Add VCF size to output (#7329); - add WARP-style scattering to SNPsVariantRecalibrator in GvsCreateFilterSet (#7320); - added ref ranges support (#7337); - 318 Sites only filtered vcf then annotate wdl (#7305); - Replace service_account_json (file) with service_account_json_path (string) to allow call-caching (#7347); - Parallelize create filterset by breaking out the 3 filter set file creation/loads into separate tasks (#7342); - Create WDL to validate VAT and add first test (#7352); - Add task for VAT validation #3 (#7360); - Add task for VAT validation #4 (#7363); - Instructions on how to download BQ Metadata and visualize results (#7359); - don't mix contigs, rightsize memory (#7361); - Add custom annotations as ac an af (#7351); - Add task for VAT validation #8 & 9 (#7364); - added bcftools, upgraded gcloud version (#7369); - fix wdl (#7378); - Update .dockstore.yml; - Add VAT validation rule #5 [VS-16] (#7365); - Add VAT validation rule #7 [VS-14] and validation rule #6 [VS-15] (#7379); - Batching of samples for create import TSVs (#7382); - Add VAT validation rule #2 [VS-19] (#7374); - Create VAT scripts directory (#7386); - fixing SA change from file to string (#7371); - add extract_subpop script (#7387); - Add is_loaded column to sample_info and logic to populate after ingest [VS-158] (#7389); - Add Gnomad subpopulation info into the VAT (#7381); - implement GVS ID assignment (#",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:16071,Security,validat,validation,16071,"epare, factored out sample name (#7288); - Remove training sites only param from ExtractFeatures broadinstitute/dsp-spec-ops#261; - add param for mem for indels (#7282); - Ah prepare localize option (#7299); - Export sites only vcf STEP 1-- 317 add AC, AN, AF to the final VCF (#7279); - AoU GVS Cohort Extract wdl (#7242); - reliability (#7310); - bump to include FT tag filtering (#7316); - First pass at a Terra QuickStart (#7267); - Ah fix timestamp query (#7319); - 313 Cleanup Extract Cohort params (#7293); - bump bq storage version. See GVS-332 (#7330); - Variant Store extraction - Add VCF size to output (#7329); - add WARP-style scattering to SNPsVariantRecalibrator in GvsCreateFilterSet (#7320); - added ref ranges support (#7337); - 318 Sites only filtered vcf then annotate wdl (#7305); - Replace service_account_json (file) with service_account_json_path (string) to allow call-caching (#7347); - Parallelize create filterset by breaking out the 3 filter set file creation/loads into separate tasks (#7342); - Create WDL to validate VAT and add first test (#7352); - Add task for VAT validation #3 (#7360); - Add task for VAT validation #4 (#7363); - Instructions on how to download BQ Metadata and visualize results (#7359); - don't mix contigs, rightsize memory (#7361); - Add custom annotations as ac an af (#7351); - Add task for VAT validation #8 & 9 (#7364); - added bcftools, upgraded gcloud version (#7369); - fix wdl (#7378); - Update .dockstore.yml; - Add VAT validation rule #5 [VS-16] (#7365); - Add VAT validation rule #7 [VS-14] and validation rule #6 [VS-15] (#7379); - Batching of samples for create import TSVs (#7382); - Add VAT validation rule #2 [VS-19] (#7374); - Create VAT scripts directory (#7386); - fixing SA change from file to string (#7371); - add extract_subpop script (#7387); - Add is_loaded column to sample_info and logic to populate after ingest [VS-158] (#7389); - Add Gnomad subpopulation info into the VAT (#7381); - implement GVS ID assignment (#",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:16283,Security,validat,validation,16283,"epare, factored out sample name (#7288); - Remove training sites only param from ExtractFeatures broadinstitute/dsp-spec-ops#261; - add param for mem for indels (#7282); - Ah prepare localize option (#7299); - Export sites only vcf STEP 1-- 317 add AC, AN, AF to the final VCF (#7279); - AoU GVS Cohort Extract wdl (#7242); - reliability (#7310); - bump to include FT tag filtering (#7316); - First pass at a Terra QuickStart (#7267); - Ah fix timestamp query (#7319); - 313 Cleanup Extract Cohort params (#7293); - bump bq storage version. See GVS-332 (#7330); - Variant Store extraction - Add VCF size to output (#7329); - add WARP-style scattering to SNPsVariantRecalibrator in GvsCreateFilterSet (#7320); - added ref ranges support (#7337); - 318 Sites only filtered vcf then annotate wdl (#7305); - Replace service_account_json (file) with service_account_json_path (string) to allow call-caching (#7347); - Parallelize create filterset by breaking out the 3 filter set file creation/loads into separate tasks (#7342); - Create WDL to validate VAT and add first test (#7352); - Add task for VAT validation #3 (#7360); - Add task for VAT validation #4 (#7363); - Instructions on how to download BQ Metadata and visualize results (#7359); - don't mix contigs, rightsize memory (#7361); - Add custom annotations as ac an af (#7351); - Add task for VAT validation #8 & 9 (#7364); - added bcftools, upgraded gcloud version (#7369); - fix wdl (#7378); - Update .dockstore.yml; - Add VAT validation rule #5 [VS-16] (#7365); - Add VAT validation rule #7 [VS-14] and validation rule #6 [VS-15] (#7379); - Batching of samples for create import TSVs (#7382); - Add VAT validation rule #2 [VS-19] (#7374); - Create VAT scripts directory (#7386); - fixing SA change from file to string (#7371); - add extract_subpop script (#7387); - Add is_loaded column to sample_info and logic to populate after ingest [VS-158] (#7389); - Add Gnomad subpopulation info into the VAT (#7381); - implement GVS ID assignment (#",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:16415,Security,validat,validation,16415,"#7359); - don't mix contigs, rightsize memory (#7361); - Add custom annotations as ac an af (#7351); - Add task for VAT validation #8 & 9 (#7364); - added bcftools, upgraded gcloud version (#7369); - fix wdl (#7378); - Update .dockstore.yml; - Add VAT validation rule #5 [VS-16] (#7365); - Add VAT validation rule #7 [VS-14] and validation rule #6 [VS-15] (#7379); - Batching of samples for create import TSVs (#7382); - Add VAT validation rule #2 [VS-19] (#7374); - Create VAT scripts directory (#7386); - fixing SA change from file to string (#7371); - add extract_subpop script (#7387); - Add is_loaded column to sample_info and logic to populate after ingest [VS-158] (#7389); - Add Gnomad subpopulation info into the VAT (#7381); - implement GVS ID assignment (#7355); - no longer loading sample info table in this wdl (#7407); - divide up creation/population of temp pet table [VS-48] (#7395); - Sample QC metrics (#7396); - update import for is_loaded (#7416); - fix partition end, add 1 (#7420); - Fixes to CreateFilterSet and ExtractCallset from 30K run (#7423); - Also changed file size from Int to Float in SumBytes task python (#7429); - Adding the subpopulation calculations to the VAT creation WDL (#7399); - 154 De obfuscate (#7435); - filter on gvs_ids for workflow (#7428); - update for assign ids and changes in import (#7439); - need to loop through sets when moving to done (#7440); - add option for create filter set to use sample_info with is_loaded (#7434); - remove dead branch (#7443); - Scaling the VAT -- switch the input to take in a file of vcf shard file names (#7446); - dockstore testing: move validate vat inputs (#7449); - Update GVS sample QC to support multiple callsets per datasset [VS-177] (#7451); - Update GvsImportGenomes.wdl (#7462); - Add extraction uuid BQ label to GvsPrepareCallstep from GvsExtractCohortFromSampleNames (#7458); - Add manifest summary file to GvsExtractCallset (#7457); - Create workflow to create and populate alt_allele table [VS-51] (",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:16461,Security,validat,validation,16461,"#7359); - don't mix contigs, rightsize memory (#7361); - Add custom annotations as ac an af (#7351); - Add task for VAT validation #8 & 9 (#7364); - added bcftools, upgraded gcloud version (#7369); - fix wdl (#7378); - Update .dockstore.yml; - Add VAT validation rule #5 [VS-16] (#7365); - Add VAT validation rule #7 [VS-14] and validation rule #6 [VS-15] (#7379); - Batching of samples for create import TSVs (#7382); - Add VAT validation rule #2 [VS-19] (#7374); - Create VAT scripts directory (#7386); - fixing SA change from file to string (#7371); - add extract_subpop script (#7387); - Add is_loaded column to sample_info and logic to populate after ingest [VS-158] (#7389); - Add Gnomad subpopulation info into the VAT (#7381); - implement GVS ID assignment (#7355); - no longer loading sample info table in this wdl (#7407); - divide up creation/population of temp pet table [VS-48] (#7395); - Sample QC metrics (#7396); - update import for is_loaded (#7416); - fix partition end, add 1 (#7420); - Fixes to CreateFilterSet and ExtractCallset from 30K run (#7423); - Also changed file size from Int to Float in SumBytes task python (#7429); - Adding the subpopulation calculations to the VAT creation WDL (#7399); - 154 De obfuscate (#7435); - filter on gvs_ids for workflow (#7428); - update for assign ids and changes in import (#7439); - need to loop through sets when moving to done (#7440); - add option for create filter set to use sample_info with is_loaded (#7434); - remove dead branch (#7443); - Scaling the VAT -- switch the input to take in a file of vcf shard file names (#7446); - dockstore testing: move validate vat inputs (#7449); - Update GVS sample QC to support multiple callsets per datasset [VS-177] (#7451); - Update GvsImportGenomes.wdl (#7462); - Add extraction uuid BQ label to GvsPrepareCallstep from GvsExtractCohortFromSampleNames (#7458); - Add manifest summary file to GvsExtractCallset (#7457); - Create workflow to create and populate alt_allele table [VS-51] (",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:16492,Security,validat,validation,16492,"#7359); - don't mix contigs, rightsize memory (#7361); - Add custom annotations as ac an af (#7351); - Add task for VAT validation #8 & 9 (#7364); - added bcftools, upgraded gcloud version (#7369); - fix wdl (#7378); - Update .dockstore.yml; - Add VAT validation rule #5 [VS-16] (#7365); - Add VAT validation rule #7 [VS-14] and validation rule #6 [VS-15] (#7379); - Batching of samples for create import TSVs (#7382); - Add VAT validation rule #2 [VS-19] (#7374); - Create VAT scripts directory (#7386); - fixing SA change from file to string (#7371); - add extract_subpop script (#7387); - Add is_loaded column to sample_info and logic to populate after ingest [VS-158] (#7389); - Add Gnomad subpopulation info into the VAT (#7381); - implement GVS ID assignment (#7355); - no longer loading sample info table in this wdl (#7407); - divide up creation/population of temp pet table [VS-48] (#7395); - Sample QC metrics (#7396); - update import for is_loaded (#7416); - fix partition end, add 1 (#7420); - Fixes to CreateFilterSet and ExtractCallset from 30K run (#7423); - Also changed file size from Int to Float in SumBytes task python (#7429); - Adding the subpopulation calculations to the VAT creation WDL (#7399); - 154 De obfuscate (#7435); - filter on gvs_ids for workflow (#7428); - update for assign ids and changes in import (#7439); - need to loop through sets when moving to done (#7440); - add option for create filter set to use sample_info with is_loaded (#7434); - remove dead branch (#7443); - Scaling the VAT -- switch the input to take in a file of vcf shard file names (#7446); - dockstore testing: move validate vat inputs (#7449); - Update GVS sample QC to support multiple callsets per datasset [VS-177] (#7451); - Update GvsImportGenomes.wdl (#7462); - Add extraction uuid BQ label to GvsPrepareCallstep from GvsExtractCohortFromSampleNames (#7458); - Add manifest summary file to GvsExtractCallset (#7457); - Create workflow to create and populate alt_allele table [VS-51] (",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:16592,Security,validat,validation,16592,"#7359); - don't mix contigs, rightsize memory (#7361); - Add custom annotations as ac an af (#7351); - Add task for VAT validation #8 & 9 (#7364); - added bcftools, upgraded gcloud version (#7369); - fix wdl (#7378); - Update .dockstore.yml; - Add VAT validation rule #5 [VS-16] (#7365); - Add VAT validation rule #7 [VS-14] and validation rule #6 [VS-15] (#7379); - Batching of samples for create import TSVs (#7382); - Add VAT validation rule #2 [VS-19] (#7374); - Create VAT scripts directory (#7386); - fixing SA change from file to string (#7371); - add extract_subpop script (#7387); - Add is_loaded column to sample_info and logic to populate after ingest [VS-158] (#7389); - Add Gnomad subpopulation info into the VAT (#7381); - implement GVS ID assignment (#7355); - no longer loading sample info table in this wdl (#7407); - divide up creation/population of temp pet table [VS-48] (#7395); - Sample QC metrics (#7396); - update import for is_loaded (#7416); - fix partition end, add 1 (#7420); - Fixes to CreateFilterSet and ExtractCallset from 30K run (#7423); - Also changed file size from Int to Float in SumBytes task python (#7429); - Adding the subpopulation calculations to the VAT creation WDL (#7399); - 154 De obfuscate (#7435); - filter on gvs_ids for workflow (#7428); - update for assign ids and changes in import (#7439); - need to loop through sets when moving to done (#7440); - add option for create filter set to use sample_info with is_loaded (#7434); - remove dead branch (#7443); - Scaling the VAT -- switch the input to take in a file of vcf shard file names (#7446); - dockstore testing: move validate vat inputs (#7449); - Update GVS sample QC to support multiple callsets per datasset [VS-177] (#7451); - Update GvsImportGenomes.wdl (#7462); - Add extraction uuid BQ label to GvsPrepareCallstep from GvsExtractCohortFromSampleNames (#7458); - Add manifest summary file to GvsExtractCallset (#7457); - Create workflow to create and populate alt_allele table [VS-51] (",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:17789,Security,validat,validate,17789,"#7359); - don't mix contigs, rightsize memory (#7361); - Add custom annotations as ac an af (#7351); - Add task for VAT validation #8 & 9 (#7364); - added bcftools, upgraded gcloud version (#7369); - fix wdl (#7378); - Update .dockstore.yml; - Add VAT validation rule #5 [VS-16] (#7365); - Add VAT validation rule #7 [VS-14] and validation rule #6 [VS-15] (#7379); - Batching of samples for create import TSVs (#7382); - Add VAT validation rule #2 [VS-19] (#7374); - Create VAT scripts directory (#7386); - fixing SA change from file to string (#7371); - add extract_subpop script (#7387); - Add is_loaded column to sample_info and logic to populate after ingest [VS-158] (#7389); - Add Gnomad subpopulation info into the VAT (#7381); - implement GVS ID assignment (#7355); - no longer loading sample info table in this wdl (#7407); - divide up creation/population of temp pet table [VS-48] (#7395); - Sample QC metrics (#7396); - update import for is_loaded (#7416); - fix partition end, add 1 (#7420); - Fixes to CreateFilterSet and ExtractCallset from 30K run (#7423); - Also changed file size from Int to Float in SumBytes task python (#7429); - Adding the subpopulation calculations to the VAT creation WDL (#7399); - 154 De obfuscate (#7435); - filter on gvs_ids for workflow (#7428); - update for assign ids and changes in import (#7439); - need to loop through sets when moving to done (#7440); - add option for create filter set to use sample_info with is_loaded (#7434); - remove dead branch (#7443); - Scaling the VAT -- switch the input to take in a file of vcf shard file names (#7446); - dockstore testing: move validate vat inputs (#7449); - Update GVS sample QC to support multiple callsets per datasset [VS-177] (#7451); - Update GvsImportGenomes.wdl (#7462); - Add extraction uuid BQ label to GvsPrepareCallstep from GvsExtractCohortFromSampleNames (#7458); - Add manifest summary file to GvsExtractCallset (#7457); - Create workflow to create and populate alt_allele table [VS-51] (",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:23942,Security,validat,validate,23942,"; - 317 remove excess header values in VCF extract (#7786); - correct auth in split intervals (#7790); - Add code to (optionally) zero pad the vcf filename. (#7783); - LoadData `maxRetries` parameterized, default increased [VS-383] (#7791); - Update to latest version of ah_var_store gatk override jar (#7793); - GvsUnified WDL to wrap the 6 core GVS WDLs [VS-382] (#7789); - Pinned typing_extensions python package to 4.1.1 to fix conda environment. (#7802); - WeightedSplitInterval fixes [VS-384] [VS-332] (#7795); - Replace Travis with GithubActions (#7754); - Docker build only lfs pulls main/src/resources/large (#7727); - Clean up gatk jars -- looks like we are not passing them properly in the extract (#7788); - Fix typo that broke git lfs pull (#7806); - Document AoU SOP (up to the VAT) [VS-63] (#7807); - Incident VS 365 clinvar classification fix (#7769); - VS-390. Add precision and sensitivity wdl (#7813); - Quickstart based integration test [VS-357] (#7812); - 365 vat python testing additions (#7756); - VS 396 clinvar grabs too many values (#7823); - Added a test to validate WDLs in the scripts directory. (#7826) (#7829); - VAT Performance / Reliability Improvements (#7828); - VAT naming conventions [VS-410] (#7827); - Rc remove ad from vat (#7832); - bugfix, we were trying to grep a binary file (#7837); - Cleanup scripts/variantstore [VS-414] (#7834); - Merge VAT TSV files into single bgzipped file [VS-304] (#7848); - Handle fully and partially loaded samples [VS-262] [VS-258] (#7843); - Ingest Error Handling Fixes [VS-261] (#7841); - First cut at a python notebook to validate inputs. (#7845); - Compute filter scatter [VS-392] (#7852); - remove withdrawn req (#7844); - Improve import error message [VS-437] (#7855); - Fix Input Validation python notebook (#7853); - Add VAT Validation check that aa_change and exon_number are consistently set. (#7850); - Ingest 10K [VS-344] (#7860); - X/Y chromosome reweighting for better extract shard runtime balance [VS-389] (#7868",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:24455,Security,validat,validate,24455,"89); - Pinned typing_extensions python package to 4.1.1 to fix conda environment. (#7802); - WeightedSplitInterval fixes [VS-384] [VS-332] (#7795); - Replace Travis with GithubActions (#7754); - Docker build only lfs pulls main/src/resources/large (#7727); - Clean up gatk jars -- looks like we are not passing them properly in the extract (#7788); - Fix typo that broke git lfs pull (#7806); - Document AoU SOP (up to the VAT) [VS-63] (#7807); - Incident VS 365 clinvar classification fix (#7769); - VS-390. Add precision and sensitivity wdl (#7813); - Quickstart based integration test [VS-357] (#7812); - 365 vat python testing additions (#7756); - VS 396 clinvar grabs too many values (#7823); - Added a test to validate WDLs in the scripts directory. (#7826) (#7829); - VAT Performance / Reliability Improvements (#7828); - VAT naming conventions [VS-410] (#7827); - Rc remove ad from vat (#7832); - bugfix, we were trying to grep a binary file (#7837); - Cleanup scripts/variantstore [VS-414] (#7834); - Merge VAT TSV files into single bgzipped file [VS-304] (#7848); - Handle fully and partially loaded samples [VS-262] [VS-258] (#7843); - Ingest Error Handling Fixes [VS-261] (#7841); - First cut at a python notebook to validate inputs. (#7845); - Compute filter scatter [VS-392] (#7852); - remove withdrawn req (#7844); - Improve import error message [VS-437] (#7855); - Fix Input Validation python notebook (#7853); - Add VAT Validation check that aa_change and exon_number are consistently set. (#7850); - Ingest 10K [VS-344] (#7860); - X/Y chromosome reweighting for better extract shard runtime balance [VS-389] (#7868); - VET Ingest Validation / Allow Ingest of non-VQSR'ed data (#7870); - Fix AoU workflow bugs (#7874); - Curate input arrays to skip already ingested sample data [VS-246] (#7862); - KM upload GVS product sheet (#7883); - Default extract scatter width [VS-415] (#7878); - Volatile tasks review [VS-447] (#7880); - Update Quickstart Integration for X/Y scaling changes ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:24617,Security,Validat,Validation,24617,"lfs pull (#7806); - Document AoU SOP (up to the VAT) [VS-63] (#7807); - Incident VS 365 clinvar classification fix (#7769); - VS-390. Add precision and sensitivity wdl (#7813); - Quickstart based integration test [VS-357] (#7812); - 365 vat python testing additions (#7756); - VS 396 clinvar grabs too many values (#7823); - Added a test to validate WDLs in the scripts directory. (#7826) (#7829); - VAT Performance / Reliability Improvements (#7828); - VAT naming conventions [VS-410] (#7827); - Rc remove ad from vat (#7832); - bugfix, we were trying to grep a binary file (#7837); - Cleanup scripts/variantstore [VS-414] (#7834); - Merge VAT TSV files into single bgzipped file [VS-304] (#7848); - Handle fully and partially loaded samples [VS-262] [VS-258] (#7843); - Ingest Error Handling Fixes [VS-261] (#7841); - First cut at a python notebook to validate inputs. (#7845); - Compute filter scatter [VS-392] (#7852); - remove withdrawn req (#7844); - Improve import error message [VS-437] (#7855); - Fix Input Validation python notebook (#7853); - Add VAT Validation check that aa_change and exon_number are consistently set. (#7850); - Ingest 10K [VS-344] (#7860); - X/Y chromosome reweighting for better extract shard runtime balance [VS-389] (#7868); - VET Ingest Validation / Allow Ingest of non-VQSR'ed data (#7870); - Fix AoU workflow bugs (#7874); - Curate input arrays to skip already ingested sample data [VS-246] (#7862); - KM upload GVS product sheet (#7883); - Default extract scatter width [VS-415] (#7878); - Volatile tasks review [VS-447] (#7880); - Update Quickstart Integration for X/Y scaling changes [VS-464] (#7881); - clean up dockstore; - Rc vs 63 vat sop documentation (#7879); - Fix up FQ and race condition issues with volatile tasks work [VS-478] (#7888); - Use gvs-internal project in integration test (#7901); - Add cost observability BQ table [VS-441] (#7891); - Add preliminary labels to queries [VS-381] (#7902); - Workflow compute costs [VS-472] (#7905); - Fix bu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:24663,Security,Validat,Validation,24663,"lfs pull (#7806); - Document AoU SOP (up to the VAT) [VS-63] (#7807); - Incident VS 365 clinvar classification fix (#7769); - VS-390. Add precision and sensitivity wdl (#7813); - Quickstart based integration test [VS-357] (#7812); - 365 vat python testing additions (#7756); - VS 396 clinvar grabs too many values (#7823); - Added a test to validate WDLs in the scripts directory. (#7826) (#7829); - VAT Performance / Reliability Improvements (#7828); - VAT naming conventions [VS-410] (#7827); - Rc remove ad from vat (#7832); - bugfix, we were trying to grep a binary file (#7837); - Cleanup scripts/variantstore [VS-414] (#7834); - Merge VAT TSV files into single bgzipped file [VS-304] (#7848); - Handle fully and partially loaded samples [VS-262] [VS-258] (#7843); - Ingest Error Handling Fixes [VS-261] (#7841); - First cut at a python notebook to validate inputs. (#7845); - Compute filter scatter [VS-392] (#7852); - remove withdrawn req (#7844); - Improve import error message [VS-437] (#7855); - Fix Input Validation python notebook (#7853); - Add VAT Validation check that aa_change and exon_number are consistently set. (#7850); - Ingest 10K [VS-344] (#7860); - X/Y chromosome reweighting for better extract shard runtime balance [VS-389] (#7868); - VET Ingest Validation / Allow Ingest of non-VQSR'ed data (#7870); - Fix AoU workflow bugs (#7874); - Curate input arrays to skip already ingested sample data [VS-246] (#7862); - KM upload GVS product sheet (#7883); - Default extract scatter width [VS-415] (#7878); - Volatile tasks review [VS-447] (#7880); - Update Quickstart Integration for X/Y scaling changes [VS-464] (#7881); - clean up dockstore; - Rc vs 63 vat sop documentation (#7879); - Fix up FQ and race condition issues with volatile tasks work [VS-478] (#7888); - Use gvs-internal project in integration test (#7901); - Add cost observability BQ table [VS-441] (#7891); - Add preliminary labels to queries [VS-381] (#7902); - Workflow compute costs [VS-472] (#7905); - Fix bu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:24874,Security,Validat,Validation,24874,"to validate inputs. (#7845); - Compute filter scatter [VS-392] (#7852); - remove withdrawn req (#7844); - Improve import error message [VS-437] (#7855); - Fix Input Validation python notebook (#7853); - Add VAT Validation check that aa_change and exon_number are consistently set. (#7850); - Ingest 10K [VS-344] (#7860); - X/Y chromosome reweighting for better extract shard runtime balance [VS-389] (#7868); - VET Ingest Validation / Allow Ingest of non-VQSR'ed data (#7870); - Fix AoU workflow bugs (#7874); - Curate input arrays to skip already ingested sample data [VS-246] (#7862); - KM upload GVS product sheet (#7883); - Default extract scatter width [VS-415] (#7878); - Volatile tasks review [VS-447] (#7880); - Update Quickstart Integration for X/Y scaling changes [VS-464] (#7881); - clean up dockstore; - Rc vs 63 vat sop documentation (#7879); - Fix up FQ and race condition issues with volatile tasks work [VS-478] (#7888); - Use gvs-internal project in integration test (#7901); - Add cost observability BQ table [VS-441] (#7891); - Add preliminary labels to queries [VS-381] (#7902); - Workflow compute costs [VS-472] (#7905); - Fix bug and update images (#7912); - VS 483 Beta user wdl (#7894); - Core storage model cost [VS-473] (#7913); - Update Quickstart & Integration to use re-blocked v2 gVCFs [VS-491] (#7924); - KM GVS documentation (#7903); - Track BigQuery costs of GVS python VS-480 (#7915); - Read cost observability table [VS-475] (#7923); - Fix Race Condition, Add Support for Extract by Array of Sample Names (ie from a Sample Set) (#7917); - Rightsize import batches [VS-486] (#7925); - [AoU DRC] Support uppercase site_ids for reblocking (#7929); - Populate cost metadata for GATK tasks. (#7919); - remove accidentally added input (#7931); - VS_492 - Beta User Jar release (#7934); - Cost WDL should throw on FISS API errors [VS-518] (#7942); - Fix bad check for missing workflow name [VS-520] (#7943); - Remove usage of service account from GvsValidateVAT.wdl (#7937",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:28794,Security,Expose,Expose,28794,"ility to tweak sample-every-Nth-variant parameter for SNP model creation (#8019); - add initial notebook copy pasta (#8008); - add sample_table_timestamp to GetNumSamplesLoaded (#8022); - Batched Avro export [VS-630] (#8020); - Updating references to old GATK for VS-620 (#8023); - VS-517 Use standard version of GetBQTableLastModifiedDatetime in GvsValidateVat (#8024); - Fix bug in GvsWithdrawSamples.wdl (#8026); - Ah 617 exposing the drop_state parameter to the GvsJointVariantCalling wdl used for beta (and internal customer) (#8032); - Expose maximum-training-variants VQSR parameter [VS-634] (#8029); - Callset statistics [VS-560] (#8018); - Check for withdrawn before exporting to AVRO files [VS-646] (#8039); - Small updates to GVS Integration WDL [VS-618] (#8042); - Rework Hail script generation [VS-616] (#8034); - Alpine based Variant Store Docker image [VS-648] (#8044); - update warp version (#7906); - Fail Avro extract and callset stats on bad filter name [VS-655] (#8046); - Vs 629 failure to retrieve job information during ingest (#8047); - Restore accidentally removed bcftools [VS-661] (#8051); - Allowing our pipeline to function with a sample size of one (#8055); - Vs 665 re create vcf for cd 68 po 52339 with ad padding fixed (#8057); - VS-665 and VS-620 updating code to use latest docker images containing Rori's AD calculation changes in extract (#8061); - updating the beta workflow to use the latest jar, representing the version of GATK George tested against the workflow (#8062); - VS-637 Address a couple of issues in SampleLoadStatus handling in GVSImportGenomes. (#8052); - Revert Alpinizing of apt dependent task [VS-688] (#8065); - Fix missing vat schema JSONs [VS-699] (#8072); - Fix integration expectations for fixed AD [VS-689] (#8066); - VS-698 Remove unnecessary columns from Call set statistics (#8073); - Fix Dockerfile nits that break 20.10.21 (#8078); - Nirvana 3.18.1 Docker images support [VS-661] (#8082); - Add option to not prepare __REF_DATA or __",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:30901,Security,Validat,ValidateVat,30901,"ge tested against the workflow (#8062); - VS-637 Address a couple of issues in SampleLoadStatus handling in GVSImportGenomes. (#8052); - Revert Alpinizing of apt dependent task [VS-688] (#8065); - Fix missing vat schema JSONs [VS-699] (#8072); - Fix integration expectations for fixed AD [VS-689] (#8066); - VS-698 Remove unnecessary columns from Call set statistics (#8073); - Fix Dockerfile nits that break 20.10.21 (#8078); - Nirvana 3.18.1 Docker images support [VS-661] (#8082); - Add option to not prepare __REF_DATA or __SAMPLES tables to Prepare [VS-697] (#8079); - ""build-base"" Docker image for faster variantstore image builds [VS-712] (#8085); - GVS / Hail VDS integration test [VS-639] (#8086); - Remove AI/AN from VDS docs [VS-726] (#8096); - Add flag for cost_observability table writing to support sub-cohort use case [VS-521] (#8093); - Document STS delivery process for VDS [VS-727] (#8101); - delete obsolete callset_QC directory and its contents [VS-318] (#8108); - doc link typo and add check for control samples in AVRO export (#8110); - Add defaults for scatter_count in GvsExtractCohortFromSampleNames [VS-496] (#8109); - Escape table names properly in ValidateVat WDL (#8116); - Vs 741 fix indefinite freeze in split intervals task when using exome data (#8113); - VAT Readme updates (#8090); - WDL and python scripts to use the VDS in the VAT (#8077); - VS-757 - Use JASIX to make sub-jsons of annotated output of Nirvana (#8133); - add note about permissions for P&S workflow to work (#8135); - VS-759 (and VS-760) (#8137); - VS-765. Scatter the RemoveDuplicates task. (#8144); - update delivery docs based on latest VDS delivery run [VS-770] (#8150); - Add monitoring to index vcf (#8151); - Make some noise when VDS validation succeeds (#8155); - Handle empty genes annotation file. (#8153); - Add escapes for otherwise problematic dataset / table names. (#8162); - New WDL to create VAT tsvs from previously generated BigQuery table. (#8165); - Treat withdrawn samples in ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:31469,Security,validat,validation,31469,"86); - Remove AI/AN from VDS docs [VS-726] (#8096); - Add flag for cost_observability table writing to support sub-cohort use case [VS-521] (#8093); - Document STS delivery process for VDS [VS-727] (#8101); - delete obsolete callset_QC directory and its contents [VS-318] (#8108); - doc link typo and add check for control samples in AVRO export (#8110); - Add defaults for scatter_count in GvsExtractCohortFromSampleNames [VS-496] (#8109); - Escape table names properly in ValidateVat WDL (#8116); - Vs 741 fix indefinite freeze in split intervals task when using exome data (#8113); - VAT Readme updates (#8090); - WDL and python scripts to use the VDS in the VAT (#8077); - VS-757 - Use JASIX to make sub-jsons of annotated output of Nirvana (#8133); - add note about permissions for P&S workflow to work (#8135); - VS-759 (and VS-760) (#8137); - VS-765. Scatter the RemoveDuplicates task. (#8144); - update delivery docs based on latest VDS delivery run [VS-770] (#8150); - Add monitoring to index vcf (#8151); - Make some noise when VDS validation succeeds (#8155); - Handle empty genes annotation file. (#8153); - Add escapes for otherwise problematic dataset / table names. (#8162); - New WDL to create VAT tsvs from previously generated BigQuery table. (#8165); - Treat withdrawn samples in sub-cohort prepare correctly [VS-772] (#8156); - Remove unused VAT Creation WDL (#8172); - Gg consistently use dataset name as input parameter (#8173); - AoU cleanup docs, round 1 [VS-671] (#8104); - VDS docs remove samples and correct GT [VS-807] (#8178); - [VS-693] Add support for VQSR Lite to GvsCreateFilterSet (#8157); - VAT Documentation Update Round 1 [VS-531]; - VS-530 VDS creation documentation for AoU (#8169); - Update beta docs to tell people not to use free credits (#8184); - VS-816 Keeping ingestion under quota (#8193); - CromwellOnAzure + Azure SQL DB + AAD first steps doc [VS-805] (#8191); - Edit and re-format VDS -> VAT doc [VS-821] (#8187); - VS-820 Incorporate code to stay un",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:1082,Testability,test,test,1082,"clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - forgot this file; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - add fields for uncompressed imputed data; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - Tool for arrays QC metrics calculations (#6812); - ah update array extract tool (#6827); - fix enum (#6834); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - resolved rebase conflicts; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:2684,Testability,test,test,2684,"sed on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - resolved rebase conflicts; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - Moving the WDL for importing array manifest to BQ (#6860); - fix up after rebase; - Moving and testing ingest scripts from variantstore (#6881); - optionally provide sample-map-file instead of sample-map-table (#6872); - Moving extract wdls from variantstore repo (#6902); - update for genomes (#6918); - update paths; - update field name; - consolidate exome and genome code; - missing comma; - allow null for drop state.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:3232,Testability,test,testing,3232,"sed on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - resolved rebase conflicts; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - Moving the WDL for importing array manifest to BQ (#6860); - fix up after rebase; - Moving and testing ingest scripts from variantstore (#6881); - optionally provide sample-map-file instead of sample-map-table (#6872); - Moving extract wdls from variantstore repo (#6902); - update for genomes (#6918); - update paths; - update field name; - consolidate exome and genome code; - missing comma; - allow null for drop state.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:4805,Testability,log,logging,4805,,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:5113,Testability,test,test,5113,,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:5212,Testability,test,test,5212,,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:7054,Testability,test,test,7054,"clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - forgot this file; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - add fields for uncompressed imputed data; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - Tool for arrays QC metrics calculations (#6812); - ah update array extract tool (#6827); - fix enum (#6834); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - resolved rebase conflicts; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:8656,Testability,test,test,8656,"sed on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - resolved rebase conflicts; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - Moving the WDL for importing array manifest to BQ (#6860); - fix up after rebase; - Moving and testing ingest scripts from variantstore (#6881); - optionally provide sample-map-file instead of sample-map-table (#6872); - Moving extract wdls from variantstore repo (#6902); - update for genomes (#6918); - update paths; - update field name; - consolidate exome and genome code; - missing comma; - allow null for drop state.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:9204,Testability,test,testing,9204,"sed on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - resolved rebase conflicts; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - Moving the WDL for importing array manifest to BQ (#6860); - fix up after rebase; - Moving and testing ingest scripts from variantstore (#6881); - optionally provide sample-map-file instead of sample-map-table (#6872); - Moving extract wdls from variantstore repo (#6902); - update for genomes (#6918); - update paths; - update field name; - consolidate exome and genome code; - missing comma; - allow null for drop state.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:10777,Testability,log,logging,10777," table creation and data loading in LoadData (#7056); - WIP; - tieout scripts; - notes files; - updated diff scripts; - fixed bug...; - add wdl and inputs file for warp pipeline; - reverting logging; - included top level WDL; - use gnarly with BQ extract cohort; - remove unused file; - cleaning up; - tidy; - tidy up before PR; - tidy up before PR; - PR comments; - merge conflict misfires; - added example SQL to create alt allele table from VET; - option to remove PLs; - fixed and enhanced unit test; - removing unused config, causing travis to fail; - add CreateVariantIngestFiles integration test (#7071); - add sampleName (instead of NULL) to error message (#7074); - Update To handle if no data error (#7084); - Memory improvement when writing missing positions to pet (#7098); - added support for loading QUALapprox into VET (#7101); - Add -m flag to gsutil step; add dockstore branch filters to facilitate development (#7104); - updates to ImportGenomes and LoadBigQueryData (#7112); - Add ngs to cohort extract Dockerfile; remove exception catching in extract python script (#7113); - remove problematic storage_location imports (#7119); - Reduce memory and CPU for CreateImportTsvs task, check for files before attempting load (#7121); - add -m flag to gsutil mv step (#7129); - ah_var_store : Add sample file argument to cohort extract (#7117); - Perform full WGS cohort extract scientific tieout for 35 ACMG59 samples (#7106); - Enable Read/Execution Project for BQ Queries (#7136); - ah - optional service account (#7140); - Add load lock file to prevent accidental re-loading of data to BQ (#7138); - #251 Address gvcf no-calls missing QUALapprox and other features (#7146); - Job Add labels to BQ operations from GATK (Issues-199) (#7115); - parse map to list to avoid brackets and spaces in vcf output (#7168); - #259 Inline schema for importgenomes.wdl (#7171); - Created AvroFileReader and unittest, Update ExtractCohort and ExtractCohortEngine (#7174); - #224 Import WDL: handle ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:11085,Testability,test,test,11085," table creation and data loading in LoadData (#7056); - WIP; - tieout scripts; - notes files; - updated diff scripts; - fixed bug...; - add wdl and inputs file for warp pipeline; - reverting logging; - included top level WDL; - use gnarly with BQ extract cohort; - remove unused file; - cleaning up; - tidy; - tidy up before PR; - tidy up before PR; - PR comments; - merge conflict misfires; - added example SQL to create alt allele table from VET; - option to remove PLs; - fixed and enhanced unit test; - removing unused config, causing travis to fail; - add CreateVariantIngestFiles integration test (#7071); - add sampleName (instead of NULL) to error message (#7074); - Update To handle if no data error (#7084); - Memory improvement when writing missing positions to pet (#7098); - added support for loading QUALapprox into VET (#7101); - Add -m flag to gsutil step; add dockstore branch filters to facilitate development (#7104); - updates to ImportGenomes and LoadBigQueryData (#7112); - Add ngs to cohort extract Dockerfile; remove exception catching in extract python script (#7113); - remove problematic storage_location imports (#7119); - Reduce memory and CPU for CreateImportTsvs task, check for files before attempting load (#7121); - add -m flag to gsutil mv step (#7129); - ah_var_store : Add sample file argument to cohort extract (#7117); - Perform full WGS cohort extract scientific tieout for 35 ACMG59 samples (#7106); - Enable Read/Execution Project for BQ Queries (#7136); - ah - optional service account (#7140); - Add load lock file to prevent accidental re-loading of data to BQ (#7138); - #251 Address gvcf no-calls missing QUALapprox and other features (#7146); - Job Add labels to BQ operations from GATK (Issues-199) (#7115); - parse map to list to avoid brackets and spaces in vcf output (#7168); - #259 Inline schema for importgenomes.wdl (#7171); - Created AvroFileReader and unittest, Update ExtractCohort and ExtractCohortEngine (#7174); - #224 Import WDL: handle ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:11184,Testability,test,test,11184," table creation and data loading in LoadData (#7056); - WIP; - tieout scripts; - notes files; - updated diff scripts; - fixed bug...; - add wdl and inputs file for warp pipeline; - reverting logging; - included top level WDL; - use gnarly with BQ extract cohort; - remove unused file; - cleaning up; - tidy; - tidy up before PR; - tidy up before PR; - PR comments; - merge conflict misfires; - added example SQL to create alt allele table from VET; - option to remove PLs; - fixed and enhanced unit test; - removing unused config, causing travis to fail; - add CreateVariantIngestFiles integration test (#7071); - add sampleName (instead of NULL) to error message (#7074); - Update To handle if no data error (#7084); - Memory improvement when writing missing positions to pet (#7098); - added support for loading QUALapprox into VET (#7101); - Add -m flag to gsutil step; add dockstore branch filters to facilitate development (#7104); - updates to ImportGenomes and LoadBigQueryData (#7112); - Add ngs to cohort extract Dockerfile; remove exception catching in extract python script (#7113); - remove problematic storage_location imports (#7119); - Reduce memory and CPU for CreateImportTsvs task, check for files before attempting load (#7121); - add -m flag to gsutil mv step (#7129); - ah_var_store : Add sample file argument to cohort extract (#7117); - Perform full WGS cohort extract scientific tieout for 35 ACMG59 samples (#7106); - Enable Read/Execution Project for BQ Queries (#7136); - ah - optional service account (#7140); - Add load lock file to prevent accidental re-loading of data to BQ (#7138); - #251 Address gvcf no-calls missing QUALapprox and other features (#7146); - Job Add labels to BQ operations from GATK (Issues-199) (#7115); - parse map to list to avoid brackets and spaces in vcf output (#7168); - #259 Inline schema for importgenomes.wdl (#7171); - Created AvroFileReader and unittest, Update ExtractCohort and ExtractCohortEngine (#7174); - #224 Import WDL: handle ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:12833,Testability,test,tests,12833," QUALapprox and other features (#7146); - Job Add labels to BQ operations from GATK (Issues-199) (#7115); - parse map to list to avoid brackets and spaces in vcf output (#7168); - #259 Inline schema for importgenomes.wdl (#7171); - Created AvroFileReader and unittest, Update ExtractCohort and ExtractCohortEngine (#7174); - #224 Import WDL: handle 15 TB /table /day import limit (#7167); - #260 filter out AS_QD, SOR, FS from cohort extract VCF (#7173); - Full scientific validation via end to end comparison of filtered results between WARP and BQ (#7179); - Cherry pick of commits to fix GATK tests from master (#7183); - ExtractCohort supports -XL exclusion and follows intervals, other optimizations (#7181); - ExtractFeatures supports -XL exclusion and follows intervals, other optimizations (#7184); - change 0/0 GQ0 sites to nocalls (#7190); - updated (#7195); - Rename ""metadata"" table to ""sample_info"" table, fix vet schema (#7196); - Allow users to specify VQSLOD sensitivity and apply threshold in ExtractCohort (#7194); - Calculate and Store site-level QCs (#7197); - Filter Failing QC Sites from Extract (#7201); - WDLize GvsPrepareCallset (briefly known as CreateCohortTable) (#7200); - default drop_state to 60, but allow NONE as input (#7206); - SA support and consistent naming for all GVS WDLs (#7205); - fix GvsExtractCallset inputs file (#7210); - add clustering to tables (#7207); - add vqsr cutoffs to GvsExtractCallset wdl; clean up dockstore yml (#7209); - Avro test (#7192); - Enable call caching of TSV generation in GvsImportGenomes (#7226); - 266 Clean up ExtractCohort -- remove query mode param (#7227); - 288 Add an excess alleles param (#7221); - take sample name as a param (#7236); - How to run GIAB comparisons (#7237); - Update GvsCreateFilterSet.wdl (#7239); - Use GatherVcfsCloud in GvsCreateFilterSet.wdl (#7241); - parameterize TTL with defaults, reduce memory allocation (#7244); - Addressing OOM in CohortExtract (#7245); - make outputs optional, change case",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:13724,Testability,test,test,13724," QUALapprox and other features (#7146); - Job Add labels to BQ operations from GATK (Issues-199) (#7115); - parse map to list to avoid brackets and spaces in vcf output (#7168); - #259 Inline schema for importgenomes.wdl (#7171); - Created AvroFileReader and unittest, Update ExtractCohort and ExtractCohortEngine (#7174); - #224 Import WDL: handle 15 TB /table /day import limit (#7167); - #260 filter out AS_QD, SOR, FS from cohort extract VCF (#7173); - Full scientific validation via end to end comparison of filtered results between WARP and BQ (#7179); - Cherry pick of commits to fix GATK tests from master (#7183); - ExtractCohort supports -XL exclusion and follows intervals, other optimizations (#7181); - ExtractFeatures supports -XL exclusion and follows intervals, other optimizations (#7184); - change 0/0 GQ0 sites to nocalls (#7190); - updated (#7195); - Rename ""metadata"" table to ""sample_info"" table, fix vet schema (#7196); - Allow users to specify VQSLOD sensitivity and apply threshold in ExtractCohort (#7194); - Calculate and Store site-level QCs (#7197); - Filter Failing QC Sites from Extract (#7201); - WDLize GvsPrepareCallset (briefly known as CreateCohortTable) (#7200); - default drop_state to 60, but allow NONE as input (#7206); - SA support and consistent naming for all GVS WDLs (#7205); - fix GvsExtractCallset inputs file (#7210); - add clustering to tables (#7207); - add vqsr cutoffs to GvsExtractCallset wdl; clean up dockstore yml (#7209); - Avro test (#7192); - Enable call caching of TSV generation in GvsImportGenomes (#7226); - 266 Clean up ExtractCohort -- remove query mode param (#7227); - 288 Add an excess alleles param (#7221); - take sample name as a param (#7236); - How to run GIAB comparisons (#7237); - Update GvsCreateFilterSet.wdl (#7239); - Use GatherVcfsCloud in GvsCreateFilterSet.wdl (#7241); - parameterize TTL with defaults, reduce memory allocation (#7244); - Addressing OOM in CohortExtract (#7245); - make outputs optional, change case",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:15996,Testability,test,test,15996,"epare, factored out sample name (#7288); - Remove training sites only param from ExtractFeatures broadinstitute/dsp-spec-ops#261; - add param for mem for indels (#7282); - Ah prepare localize option (#7299); - Export sites only vcf STEP 1-- 317 add AC, AN, AF to the final VCF (#7279); - AoU GVS Cohort Extract wdl (#7242); - reliability (#7310); - bump to include FT tag filtering (#7316); - First pass at a Terra QuickStart (#7267); - Ah fix timestamp query (#7319); - 313 Cleanup Extract Cohort params (#7293); - bump bq storage version. See GVS-332 (#7330); - Variant Store extraction - Add VCF size to output (#7329); - add WARP-style scattering to SNPsVariantRecalibrator in GvsCreateFilterSet (#7320); - added ref ranges support (#7337); - 318 Sites only filtered vcf then annotate wdl (#7305); - Replace service_account_json (file) with service_account_json_path (string) to allow call-caching (#7347); - Parallelize create filterset by breaking out the 3 filter set file creation/loads into separate tasks (#7342); - Create WDL to validate VAT and add first test (#7352); - Add task for VAT validation #3 (#7360); - Add task for VAT validation #4 (#7363); - Instructions on how to download BQ Metadata and visualize results (#7359); - don't mix contigs, rightsize memory (#7361); - Add custom annotations as ac an af (#7351); - Add task for VAT validation #8 & 9 (#7364); - added bcftools, upgraded gcloud version (#7369); - fix wdl (#7378); - Update .dockstore.yml; - Add VAT validation rule #5 [VS-16] (#7365); - Add VAT validation rule #7 [VS-14] and validation rule #6 [VS-15] (#7379); - Batching of samples for create import TSVs (#7382); - Add VAT validation rule #2 [VS-19] (#7374); - Create VAT scripts directory (#7386); - fixing SA change from file to string (#7371); - add extract_subpop script (#7387); - Add is_loaded column to sample_info and logic to populate after ingest [VS-158] (#7389); - Add Gnomad subpopulation info into the VAT (#7381); - implement GVS ID assignment (#",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:16795,Testability,log,logic,16795,"#7359); - don't mix contigs, rightsize memory (#7361); - Add custom annotations as ac an af (#7351); - Add task for VAT validation #8 & 9 (#7364); - added bcftools, upgraded gcloud version (#7369); - fix wdl (#7378); - Update .dockstore.yml; - Add VAT validation rule #5 [VS-16] (#7365); - Add VAT validation rule #7 [VS-14] and validation rule #6 [VS-15] (#7379); - Batching of samples for create import TSVs (#7382); - Add VAT validation rule #2 [VS-19] (#7374); - Create VAT scripts directory (#7386); - fixing SA change from file to string (#7371); - add extract_subpop script (#7387); - Add is_loaded column to sample_info and logic to populate after ingest [VS-158] (#7389); - Add Gnomad subpopulation info into the VAT (#7381); - implement GVS ID assignment (#7355); - no longer loading sample info table in this wdl (#7407); - divide up creation/population of temp pet table [VS-48] (#7395); - Sample QC metrics (#7396); - update import for is_loaded (#7416); - fix partition end, add 1 (#7420); - Fixes to CreateFilterSet and ExtractCallset from 30K run (#7423); - Also changed file size from Int to Float in SumBytes task python (#7429); - Adding the subpopulation calculations to the VAT creation WDL (#7399); - 154 De obfuscate (#7435); - filter on gvs_ids for workflow (#7428); - update for assign ids and changes in import (#7439); - need to loop through sets when moving to done (#7440); - add option for create filter set to use sample_info with is_loaded (#7434); - remove dead branch (#7443); - Scaling the VAT -- switch the input to take in a file of vcf shard file names (#7446); - dockstore testing: move validate vat inputs (#7449); - Update GVS sample QC to support multiple callsets per datasset [VS-177] (#7451); - Update GvsImportGenomes.wdl (#7462); - Add extraction uuid BQ label to GvsPrepareCallstep from GvsExtractCohortFromSampleNames (#7458); - Add manifest summary file to GvsExtractCallset (#7457); - Create workflow to create and populate alt_allele table [VS-51] (",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:17775,Testability,test,testing,17775,"#7359); - don't mix contigs, rightsize memory (#7361); - Add custom annotations as ac an af (#7351); - Add task for VAT validation #8 & 9 (#7364); - added bcftools, upgraded gcloud version (#7369); - fix wdl (#7378); - Update .dockstore.yml; - Add VAT validation rule #5 [VS-16] (#7365); - Add VAT validation rule #7 [VS-14] and validation rule #6 [VS-15] (#7379); - Batching of samples for create import TSVs (#7382); - Add VAT validation rule #2 [VS-19] (#7374); - Create VAT scripts directory (#7386); - fixing SA change from file to string (#7371); - add extract_subpop script (#7387); - Add is_loaded column to sample_info and logic to populate after ingest [VS-158] (#7389); - Add Gnomad subpopulation info into the VAT (#7381); - implement GVS ID assignment (#7355); - no longer loading sample info table in this wdl (#7407); - divide up creation/population of temp pet table [VS-48] (#7395); - Sample QC metrics (#7396); - update import for is_loaded (#7416); - fix partition end, add 1 (#7420); - Fixes to CreateFilterSet and ExtractCallset from 30K run (#7423); - Also changed file size from Int to Float in SumBytes task python (#7429); - Adding the subpopulation calculations to the VAT creation WDL (#7399); - 154 De obfuscate (#7435); - filter on gvs_ids for workflow (#7428); - update for assign ids and changes in import (#7439); - need to loop through sets when moving to done (#7440); - add option for create filter set to use sample_info with is_loaded (#7434); - remove dead branch (#7443); - Scaling the VAT -- switch the input to take in a file of vcf shard file names (#7446); - dockstore testing: move validate vat inputs (#7449); - Update GVS sample QC to support multiple callsets per datasset [VS-177] (#7451); - Update GvsImportGenomes.wdl (#7462); - Add extraction uuid BQ label to GvsPrepareCallstep from GvsExtractCohortFromSampleNames (#7458); - Add manifest summary file to GvsExtractCallset (#7457); - Create workflow to create and populate alt_allele table [VS-51] (",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:20592,Testability,log,logging,20592,"rom Lee [VS-210] (#7502); - Export the VAT into GCS (#7472); - addresses VS-219 (#7508); - small fix to MergeVCFs (#7517); - small fixes to GVS pipeline (#7522); - make sure ExtractTask is run on all interval files; - Revert ""make sure ExtractTask is run on all interval files""; - make sure ExtractTask is run on all interval files (#7527); - Remove Sites only step from the VAT creation WDL (#7510); - fix bad argument processing for bool (#7529); - Support for TDR DRS URIs in Import (#7528); - Match format of filename output in GvsRescatterCallsetInterval (#7539); - Reference block storage and query support (#7498); - update docs (#7540); - Kc fix rr load bug (#7550); - Update .dockstore.yml (#7553); - Ah add reblocking wdl (#7544); - Scatter over all interval files, not just scatter count (#7551); - fixed docker (#7558); - take advantage of fixed version of SplitIntervals (#7566); - Document AoU-specific tieout [VS-233] (#7552); - bad param assignment in aou reblocking (#7572); - Small fixes to ImportGenomes (non-write api version) (#7574); - Ah change output of reblocking wdl to external path (#7575); - close BQ Readers (#7583); - Ah spike writeapi (#7530); - bump WDL jar (#7593); - read api bytes logging, upgrade bigquery client versions (#7601); - bump (#7610); - upgrade log4j to 2.17 (#7616); - Add drop_state default of Forty to extract (#7619); - Kc fix type (#7620); - VAT cleanup and documentation (#7531); - fix empty flush (#7627); - presorted avro files, fix performance issue (#7635); - WIP extract for ranges (#7640); - VS-268 import more samples at once (#7629); - clustering vqsr tables by location (#7656); - First Version of a weight-based splitter (#7643); - Update GvsExtractCallset.wdl; - Quoting of table names (#7666); - docs for analysis of shard runtimes for balanced sharding (#7645); - Wire through GvsExtractCohortFromSampleNames with new prepare/extract [VS-283] (#7654); - Update GvsExtractCallset.wdl (#7678); - cherry pick lb_lfs_force change (#7683",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:21734,Testability,test,tests,21734,"ntimes for balanced sharding (#7645); - Wire through GvsExtractCohortFromSampleNames with new prepare/extract [VS-283] (#7654); - Update GvsExtractCallset.wdl (#7678); - cherry pick lb_lfs_force change (#7683); - Tweak ingest messaging and failure mode [VS-267] (#7680); - Additional tweaks for GvsExtractCohortFromSampleNames [VS-283] (#7698); - VS-280 Create a VAT intermediary (#7657); - There something about split intervals [VS-306] (#7694); - VS 284 Add prepare step to Quick Start (#7685); - VS-222 dont hard code the dataset name! (#7704); - fixed bug; added tests (#7717); - Clean up optional and inconsistently named inputs [VS-294] [VS-218] (#7715); - VS-263 notes on ingest and beyond (#7618); - Add task to ExtractCallset that verifies filter_set_name exists in GVS dataset [VS-335] (#7734); - Clean up input json files to reflect changes inputs [VS-337] (#7733); - used constants; implemented non-AS transformation (#7718); - Pass dataset name to gatk ExtractFeatures (#7735); - Add withdrawn and is_control columns [VS-70] [VS-213] (#7736); - Allow interval lists that require the SA to see (#7743); - allow for gatk to be overridden, update with known good jar (#7758); - VS-361 Add GvsWithdrawSamples wdl (#7765); - Extract Performance Improvements (#7686); - Don't put withdrawn sample data in alt_allele table [VS-369] (#7762); - remove PET code (#7768); - Adding AD for scale testing VS 225 add AD (#7713); - Deterministic Sample ID assignments [VS-371] (#7770); - remove R scripts from filtering (#7781); - Remove an old ""temp table"" dataset (#7780); - Clean up LocalizeFile [VS-314] (#7771); - Remove pet code from CreateVariantIngestFiles and friends [VS-375] (#7773); - 317 remove excess header values in VCF extract (#7786); - correct auth in split intervals (#7790); - Add code to (optionally) zero pad the vcf filename. (#7783); - LoadData `maxRetries` parameterized, default increased [VS-383] (#7791); - Update to latest version of ah_var_store gatk override jar (#7793); ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:22563,Testability,test,testing,22563,"ntimes for balanced sharding (#7645); - Wire through GvsExtractCohortFromSampleNames with new prepare/extract [VS-283] (#7654); - Update GvsExtractCallset.wdl (#7678); - cherry pick lb_lfs_force change (#7683); - Tweak ingest messaging and failure mode [VS-267] (#7680); - Additional tweaks for GvsExtractCohortFromSampleNames [VS-283] (#7698); - VS-280 Create a VAT intermediary (#7657); - There something about split intervals [VS-306] (#7694); - VS 284 Add prepare step to Quick Start (#7685); - VS-222 dont hard code the dataset name! (#7704); - fixed bug; added tests (#7717); - Clean up optional and inconsistently named inputs [VS-294] [VS-218] (#7715); - VS-263 notes on ingest and beyond (#7618); - Add task to ExtractCallset that verifies filter_set_name exists in GVS dataset [VS-335] (#7734); - Clean up input json files to reflect changes inputs [VS-337] (#7733); - used constants; implemented non-AS transformation (#7718); - Pass dataset name to gatk ExtractFeatures (#7735); - Add withdrawn and is_control columns [VS-70] [VS-213] (#7736); - Allow interval lists that require the SA to see (#7743); - allow for gatk to be overridden, update with known good jar (#7758); - VS-361 Add GvsWithdrawSamples wdl (#7765); - Extract Performance Improvements (#7686); - Don't put withdrawn sample data in alt_allele table [VS-369] (#7762); - remove PET code (#7768); - Adding AD for scale testing VS 225 add AD (#7713); - Deterministic Sample ID assignments [VS-371] (#7770); - remove R scripts from filtering (#7781); - Remove an old ""temp table"" dataset (#7780); - Clean up LocalizeFile [VS-314] (#7771); - Remove pet code from CreateVariantIngestFiles and friends [VS-375] (#7773); - 317 remove excess header values in VCF extract (#7786); - correct auth in split intervals (#7790); - Add code to (optionally) zero pad the vcf filename. (#7783); - LoadData `maxRetries` parameterized, default increased [VS-383] (#7791); - Update to latest version of ah_var_store gatk override jar (#7793); ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:23809,Testability,test,test,23809,"; - 317 remove excess header values in VCF extract (#7786); - correct auth in split intervals (#7790); - Add code to (optionally) zero pad the vcf filename. (#7783); - LoadData `maxRetries` parameterized, default increased [VS-383] (#7791); - Update to latest version of ah_var_store gatk override jar (#7793); - GvsUnified WDL to wrap the 6 core GVS WDLs [VS-382] (#7789); - Pinned typing_extensions python package to 4.1.1 to fix conda environment. (#7802); - WeightedSplitInterval fixes [VS-384] [VS-332] (#7795); - Replace Travis with GithubActions (#7754); - Docker build only lfs pulls main/src/resources/large (#7727); - Clean up gatk jars -- looks like we are not passing them properly in the extract (#7788); - Fix typo that broke git lfs pull (#7806); - Document AoU SOP (up to the VAT) [VS-63] (#7807); - Incident VS 365 clinvar classification fix (#7769); - VS-390. Add precision and sensitivity wdl (#7813); - Quickstart based integration test [VS-357] (#7812); - 365 vat python testing additions (#7756); - VS 396 clinvar grabs too many values (#7823); - Added a test to validate WDLs in the scripts directory. (#7826) (#7829); - VAT Performance / Reliability Improvements (#7828); - VAT naming conventions [VS-410] (#7827); - Rc remove ad from vat (#7832); - bugfix, we were trying to grep a binary file (#7837); - Cleanup scripts/variantstore [VS-414] (#7834); - Merge VAT TSV files into single bgzipped file [VS-304] (#7848); - Handle fully and partially loaded samples [VS-262] [VS-258] (#7843); - Ingest Error Handling Fixes [VS-261] (#7841); - First cut at a python notebook to validate inputs. (#7845); - Compute filter scatter [VS-392] (#7852); - remove withdrawn req (#7844); - Improve import error message [VS-437] (#7855); - Fix Input Validation python notebook (#7853); - Add VAT Validation check that aa_change and exon_number are consistently set. (#7850); - Ingest 10K [VS-344] (#7860); - X/Y chromosome reweighting for better extract shard runtime balance [VS-389] (#7868",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:23849,Testability,test,testing,23849,"; - 317 remove excess header values in VCF extract (#7786); - correct auth in split intervals (#7790); - Add code to (optionally) zero pad the vcf filename. (#7783); - LoadData `maxRetries` parameterized, default increased [VS-383] (#7791); - Update to latest version of ah_var_store gatk override jar (#7793); - GvsUnified WDL to wrap the 6 core GVS WDLs [VS-382] (#7789); - Pinned typing_extensions python package to 4.1.1 to fix conda environment. (#7802); - WeightedSplitInterval fixes [VS-384] [VS-332] (#7795); - Replace Travis with GithubActions (#7754); - Docker build only lfs pulls main/src/resources/large (#7727); - Clean up gatk jars -- looks like we are not passing them properly in the extract (#7788); - Fix typo that broke git lfs pull (#7806); - Document AoU SOP (up to the VAT) [VS-63] (#7807); - Incident VS 365 clinvar classification fix (#7769); - VS-390. Add precision and sensitivity wdl (#7813); - Quickstart based integration test [VS-357] (#7812); - 365 vat python testing additions (#7756); - VS 396 clinvar grabs too many values (#7823); - Added a test to validate WDLs in the scripts directory. (#7826) (#7829); - VAT Performance / Reliability Improvements (#7828); - VAT naming conventions [VS-410] (#7827); - Rc remove ad from vat (#7832); - bugfix, we were trying to grep a binary file (#7837); - Cleanup scripts/variantstore [VS-414] (#7834); - Merge VAT TSV files into single bgzipped file [VS-304] (#7848); - Handle fully and partially loaded samples [VS-262] [VS-258] (#7843); - Ingest Error Handling Fixes [VS-261] (#7841); - First cut at a python notebook to validate inputs. (#7845); - Compute filter scatter [VS-392] (#7852); - remove withdrawn req (#7844); - Improve import error message [VS-437] (#7855); - Fix Input Validation python notebook (#7853); - Add VAT Validation check that aa_change and exon_number are consistently set. (#7850); - Ingest 10K [VS-344] (#7860); - X/Y chromosome reweighting for better extract shard runtime balance [VS-389] (#7868",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:23934,Testability,test,test,23934,"; - 317 remove excess header values in VCF extract (#7786); - correct auth in split intervals (#7790); - Add code to (optionally) zero pad the vcf filename. (#7783); - LoadData `maxRetries` parameterized, default increased [VS-383] (#7791); - Update to latest version of ah_var_store gatk override jar (#7793); - GvsUnified WDL to wrap the 6 core GVS WDLs [VS-382] (#7789); - Pinned typing_extensions python package to 4.1.1 to fix conda environment. (#7802); - WeightedSplitInterval fixes [VS-384] [VS-332] (#7795); - Replace Travis with GithubActions (#7754); - Docker build only lfs pulls main/src/resources/large (#7727); - Clean up gatk jars -- looks like we are not passing them properly in the extract (#7788); - Fix typo that broke git lfs pull (#7806); - Document AoU SOP (up to the VAT) [VS-63] (#7807); - Incident VS 365 clinvar classification fix (#7769); - VS-390. Add precision and sensitivity wdl (#7813); - Quickstart based integration test [VS-357] (#7812); - 365 vat python testing additions (#7756); - VS 396 clinvar grabs too many values (#7823); - Added a test to validate WDLs in the scripts directory. (#7826) (#7829); - VAT Performance / Reliability Improvements (#7828); - VAT naming conventions [VS-410] (#7827); - Rc remove ad from vat (#7832); - bugfix, we were trying to grep a binary file (#7837); - Cleanup scripts/variantstore [VS-414] (#7834); - Merge VAT TSV files into single bgzipped file [VS-304] (#7848); - Handle fully and partially loaded samples [VS-262] [VS-258] (#7843); - Ingest Error Handling Fixes [VS-261] (#7841); - First cut at a python notebook to validate inputs. (#7845); - Compute filter scatter [VS-392] (#7852); - remove withdrawn req (#7844); - Improve import error message [VS-437] (#7855); - Fix Input Validation python notebook (#7853); - Add VAT Validation check that aa_change and exon_number are consistently set. (#7850); - Ingest 10K [VS-344] (#7860); - X/Y chromosome reweighting for better extract shard runtime balance [VS-389] (#7868",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:25431,Testability,test,test,25431,"to validate inputs. (#7845); - Compute filter scatter [VS-392] (#7852); - remove withdrawn req (#7844); - Improve import error message [VS-437] (#7855); - Fix Input Validation python notebook (#7853); - Add VAT Validation check that aa_change and exon_number are consistently set. (#7850); - Ingest 10K [VS-344] (#7860); - X/Y chromosome reweighting for better extract shard runtime balance [VS-389] (#7868); - VET Ingest Validation / Allow Ingest of non-VQSR'ed data (#7870); - Fix AoU workflow bugs (#7874); - Curate input arrays to skip already ingested sample data [VS-246] (#7862); - KM upload GVS product sheet (#7883); - Default extract scatter width [VS-415] (#7878); - Volatile tasks review [VS-447] (#7880); - Update Quickstart Integration for X/Y scaling changes [VS-464] (#7881); - clean up dockstore; - Rc vs 63 vat sop documentation (#7879); - Fix up FQ and race condition issues with volatile tasks work [VS-478] (#7888); - Use gvs-internal project in integration test (#7901); - Add cost observability BQ table [VS-441] (#7891); - Add preliminary labels to queries [VS-381] (#7902); - Workflow compute costs [VS-472] (#7905); - Fix bug and update images (#7912); - VS 483 Beta user wdl (#7894); - Core storage model cost [VS-473] (#7913); - Update Quickstart & Integration to use re-blocked v2 gVCFs [VS-491] (#7924); - KM GVS documentation (#7903); - Track BigQuery costs of GVS python VS-480 (#7915); - Read cost observability table [VS-475] (#7923); - Fix Race Condition, Add Support for Extract by Array of Sample Names (ie from a Sample Set) (#7917); - Rightsize import batches [VS-486] (#7925); - [AoU DRC] Support uppercase site_ids for reblocking (#7929); - Populate cost metadata for GATK tasks. (#7919); - remove accidentally added input (#7931); - VS_492 - Beta User Jar release (#7934); - Cost WDL should throw on FISS API errors [VS-518] (#7942); - Fix bad check for missing workflow name [VS-520] (#7943); - Remove usage of service account from GvsValidateVAT.wdl (#7937",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:26473,Testability,test,testablity,26473," [VS-472] (#7905); - Fix bug and update images (#7912); - VS 483 Beta user wdl (#7894); - Core storage model cost [VS-473] (#7913); - Update Quickstart & Integration to use re-blocked v2 gVCFs [VS-491] (#7924); - KM GVS documentation (#7903); - Track BigQuery costs of GVS python VS-480 (#7915); - Read cost observability table [VS-475] (#7923); - Fix Race Condition, Add Support for Extract by Array of Sample Names (ie from a Sample Set) (#7917); - Rightsize import batches [VS-486] (#7925); - [AoU DRC] Support uppercase site_ids for reblocking (#7929); - Populate cost metadata for GATK tasks. (#7919); - remove accidentally added input (#7931); - VS_492 - Beta User Jar release (#7934); - Cost WDL should throw on FISS API errors [VS-518] (#7942); - Fix bad check for missing workflow name [VS-520] (#7943); - Remove usage of service account from GvsValidateVAT.wdl (#7937); - refactoring for testablity (#7946); - More import retries [VS-532] (#7953); - A few last doc changes (#7927); - WDL to extract a single callset cost (BQ only, not Terra) (#7940); - Temporarily swap in Corretto for Temurin as we can't download Temurin. (#7969); - GL-548 - Update CreateVat code to handle samples that do not contain all population groups. (#7965); - Restore Temurin 11 [VS-570] (#7972); - Add table size check to quickstart integration test [VS-501] (#7970); - Consolidate various docs for AoU callset generation into one to rule them all [VS-553] (#7971); - VS-567. Removing usage of ServiceAccount from CreateVat related WDLs (#7974); - WDL to extract Avro files for Hail import [VS-579] (#7981); - Removed usage of service account from WDLs (#7985); - Document steps for GVS cleanup for base use case [VS-586] (#7989); - Change backticks to single quotes in several error messages - causing shell to attempt to execute. (#7995); - VS-598 - Minor update to AoU Documentation. (#7994); - Allow for incremental addition of data to alt_allele [VS-52] (#7993); - Minor AoU Documentation Update (#7999); -",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:26909,Testability,test,test,26909,"x Race Condition, Add Support for Extract by Array of Sample Names (ie from a Sample Set) (#7917); - Rightsize import batches [VS-486] (#7925); - [AoU DRC] Support uppercase site_ids for reblocking (#7929); - Populate cost metadata for GATK tasks. (#7919); - remove accidentally added input (#7931); - VS_492 - Beta User Jar release (#7934); - Cost WDL should throw on FISS API errors [VS-518] (#7942); - Fix bad check for missing workflow name [VS-520] (#7943); - Remove usage of service account from GvsValidateVAT.wdl (#7937); - refactoring for testablity (#7946); - More import retries [VS-532] (#7953); - A few last doc changes (#7927); - WDL to extract a single callset cost (BQ only, not Terra) (#7940); - Temporarily swap in Corretto for Temurin as we can't download Temurin. (#7969); - GL-548 - Update CreateVat code to handle samples that do not contain all population groups. (#7965); - Restore Temurin 11 [VS-570] (#7972); - Add table size check to quickstart integration test [VS-501] (#7970); - Consolidate various docs for AoU callset generation into one to rule them all [VS-553] (#7971); - VS-567. Removing usage of ServiceAccount from CreateVat related WDLs (#7974); - WDL to extract Avro files for Hail import [VS-579] (#7981); - Removed usage of service account from WDLs (#7985); - Document steps for GVS cleanup for base use case [VS-586] (#7989); - Change backticks to single quotes in several error messages - causing shell to attempt to execute. (#7995); - VS-598 - Minor update to AoU Documentation. (#7994); - Allow for incremental addition of data to alt_allele [VS-52] (#7993); - Minor AoU Documentation Update (#7999); - Batch population of alt_allele table from vet_ tables [VS-265] (#7998); - Change drop_state to NONE for Ingest/Extract [VS-607] (#8000); - python -> python3 (#8001); - Generate Hail import/export script [VS-605] (#8002); - clearer error when values are missing (#7939); - Ah [VS-565] output intervals and sample list (#8010); - make CreateAltAlleleT",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:29728,Testability,test,tested,29728,"ility to tweak sample-every-Nth-variant parameter for SNP model creation (#8019); - add initial notebook copy pasta (#8008); - add sample_table_timestamp to GetNumSamplesLoaded (#8022); - Batched Avro export [VS-630] (#8020); - Updating references to old GATK for VS-620 (#8023); - VS-517 Use standard version of GetBQTableLastModifiedDatetime in GvsValidateVat (#8024); - Fix bug in GvsWithdrawSamples.wdl (#8026); - Ah 617 exposing the drop_state parameter to the GvsJointVariantCalling wdl used for beta (and internal customer) (#8032); - Expose maximum-training-variants VQSR parameter [VS-634] (#8029); - Callset statistics [VS-560] (#8018); - Check for withdrawn before exporting to AVRO files [VS-646] (#8039); - Small updates to GVS Integration WDL [VS-618] (#8042); - Rework Hail script generation [VS-616] (#8034); - Alpine based Variant Store Docker image [VS-648] (#8044); - update warp version (#7906); - Fail Avro extract and callset stats on bad filter name [VS-655] (#8046); - Vs 629 failure to retrieve job information during ingest (#8047); - Restore accidentally removed bcftools [VS-661] (#8051); - Allowing our pipeline to function with a sample size of one (#8055); - Vs 665 re create vcf for cd 68 po 52339 with ad padding fixed (#8057); - VS-665 and VS-620 updating code to use latest docker images containing Rori's AD calculation changes in extract (#8061); - updating the beta workflow to use the latest jar, representing the version of GATK George tested against the workflow (#8062); - VS-637 Address a couple of issues in SampleLoadStatus handling in GVSImportGenomes. (#8052); - Revert Alpinizing of apt dependent task [VS-688] (#8065); - Fix missing vat schema JSONs [VS-699] (#8072); - Fix integration expectations for fixed AD [VS-689] (#8066); - VS-698 Remove unnecessary columns from Call set statistics (#8073); - Fix Dockerfile nits that break 20.10.21 (#8078); - Nirvana 3.18.1 Docker images support [VS-661] (#8082); - Add option to not prepare __REF_DATA or __",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:30409,Testability,test,test,30409,"ge tested against the workflow (#8062); - VS-637 Address a couple of issues in SampleLoadStatus handling in GVSImportGenomes. (#8052); - Revert Alpinizing of apt dependent task [VS-688] (#8065); - Fix missing vat schema JSONs [VS-699] (#8072); - Fix integration expectations for fixed AD [VS-689] (#8066); - VS-698 Remove unnecessary columns from Call set statistics (#8073); - Fix Dockerfile nits that break 20.10.21 (#8078); - Nirvana 3.18.1 Docker images support [VS-661] (#8082); - Add option to not prepare __REF_DATA or __SAMPLES tables to Prepare [VS-697] (#8079); - ""build-base"" Docker image for faster variantstore image builds [VS-712] (#8085); - GVS / Hail VDS integration test [VS-639] (#8086); - Remove AI/AN from VDS docs [VS-726] (#8096); - Add flag for cost_observability table writing to support sub-cohort use case [VS-521] (#8093); - Document STS delivery process for VDS [VS-727] (#8101); - delete obsolete callset_QC directory and its contents [VS-318] (#8108); - doc link typo and add check for control samples in AVRO export (#8110); - Add defaults for scatter_count in GvsExtractCohortFromSampleNames [VS-496] (#8109); - Escape table names properly in ValidateVat WDL (#8116); - Vs 741 fix indefinite freeze in split intervals task when using exome data (#8113); - VAT Readme updates (#8090); - WDL and python scripts to use the VDS in the VAT (#8077); - VS-757 - Use JASIX to make sub-jsons of annotated output of Nirvana (#8133); - add note about permissions for P&S workflow to work (#8135); - VS-759 (and VS-760) (#8137); - VS-765. Scatter the RemoveDuplicates task. (#8144); - update delivery docs based on latest VDS delivery run [VS-770] (#8150); - Add monitoring to index vcf (#8151); - Make some noise when VDS validation succeeds (#8155); - Handle empty genes annotation file. (#8153); - Add escapes for otherwise problematic dataset / table names. (#8162); - New WDL to create VAT tsvs from previously generated BigQuery table. (#8165); - Treat withdrawn samples in ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:33367,Testability,test,test,33367," dataset / table names. (#8162); - New WDL to create VAT tsvs from previously generated BigQuery table. (#8165); - Treat withdrawn samples in sub-cohort prepare correctly [VS-772] (#8156); - Remove unused VAT Creation WDL (#8172); - Gg consistently use dataset name as input parameter (#8173); - AoU cleanup docs, round 1 [VS-671] (#8104); - VDS docs remove samples and correct GT [VS-807] (#8178); - [VS-693] Add support for VQSR Lite to GvsCreateFilterSet (#8157); - VAT Documentation Update Round 1 [VS-531]; - VS-530 VDS creation documentation for AoU (#8169); - Update beta docs to tell people not to use free credits (#8184); - VS-816 Keeping ingestion under quota (#8193); - CromwellOnAzure + Azure SQL DB + AAD first steps doc [VS-805] (#8191); - Edit and re-format VDS -> VAT doc [VS-821] (#8187); - VS-820 Incorporate code to stay under Google quotas for new accounts into beta workflow (#8200); - Update docs for Nirvana reference disk [VS-531] [VS-796] (#8170); - VS-694 - Extract Callset for VQSR Lite (#8182); - Updating docker image (#8210); - Document VCF generation [VS-795] (#8202); - Variants GATK Docker image building docs + script [VS-827] (#8207); - Update GATK jar used in GvsJointVariantCalling WDL (#8216); - Hello Azure SQL Database from Cromwell on Azure [VS-812] (#8220); - Remove what appear to be accidentally added files [VS-834] (#8225); - VS-815: Add Support for YNG to VQSR Lite (#8206); - Disentangle non-GVS code from GVS code [VS-834] (#8229); - VS-695. Updates to run Precision and Sensitivity on VQSR Lite (#8230); - Track avro export costs [VS-769] (#8236); - Add note that we deleted a VDS! (#8214); - Vs 822 Add documentation for the work that we did on the latest iteration of Delta (#8205); - Rc vs 822 gq0 documentation (#8240); - Add a test exclusion for gvs scripts; - testing if the exclusion works. [VS-16]: https://broadworkbench.atlassian.net/browse/VS-16?atlOrigin=eyJpIjoiNWRkNTljNzYxNjVmNDY3MDlhMDU5Y2ZhYzA5YTRkZjUiLCJwIjoiZ2l0aHViLWNvbS1KU1cifQ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:33401,Testability,test,testing,33401," dataset / table names. (#8162); - New WDL to create VAT tsvs from previously generated BigQuery table. (#8165); - Treat withdrawn samples in sub-cohort prepare correctly [VS-772] (#8156); - Remove unused VAT Creation WDL (#8172); - Gg consistently use dataset name as input parameter (#8173); - AoU cleanup docs, round 1 [VS-671] (#8104); - VDS docs remove samples and correct GT [VS-807] (#8178); - [VS-693] Add support for VQSR Lite to GvsCreateFilterSet (#8157); - VAT Documentation Update Round 1 [VS-531]; - VS-530 VDS creation documentation for AoU (#8169); - Update beta docs to tell people not to use free credits (#8184); - VS-816 Keeping ingestion under quota (#8193); - CromwellOnAzure + Azure SQL DB + AAD first steps doc [VS-805] (#8191); - Edit and re-format VDS -> VAT doc [VS-821] (#8187); - VS-820 Incorporate code to stay under Google quotas for new accounts into beta workflow (#8200); - Update docs for Nirvana reference disk [VS-531] [VS-796] (#8170); - VS-694 - Extract Callset for VQSR Lite (#8182); - Updating docker image (#8210); - Document VCF generation [VS-795] (#8202); - Variants GATK Docker image building docs + script [VS-827] (#8207); - Update GATK jar used in GvsJointVariantCalling WDL (#8216); - Hello Azure SQL Database from Cromwell on Azure [VS-812] (#8220); - Remove what appear to be accidentally added files [VS-834] (#8225); - VS-815: Add Support for YNG to VQSR Lite (#8206); - Disentangle non-GVS code from GVS code [VS-834] (#8229); - VS-695. Updates to run Precision and Sensitivity on VQSR Lite (#8230); - Track avro export costs [VS-769] (#8236); - Add note that we deleted a VDS! (#8214); - Vs 822 Add documentation for the work that we did on the latest iteration of Delta (#8205); - Rc vs 822 gq0 documentation (#8240); - Add a test exclusion for gvs scripts; - testing if the exclusion works. [VS-16]: https://broadworkbench.atlassian.net/browse/VS-16?atlOrigin=eyJpIjoiNWRkNTljNzYxNjVmNDY3MDlhMDU5Y2ZhYzA5YTRkZjUiLCJwIjoiZ2l0aHViLWNvbS1KU1cifQ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:27799,Usability,clear,clearer,27799,ge of ServiceAccount from CreateVat related WDLs (#7974); - WDL to extract Avro files for Hail import [VS-579] (#7981); - Removed usage of service account from WDLs (#7985); - Document steps for GVS cleanup for base use case [VS-586] (#7989); - Change backticks to single quotes in several error messages - causing shell to attempt to execute. (#7995); - VS-598 - Minor update to AoU Documentation. (#7994); - Allow for incremental addition of data to alt_allele [VS-52] (#7993); - Minor AoU Documentation Update (#7999); - Batch population of alt_allele table from vet_ tables [VS-265] (#7998); - Change drop_state to NONE for Ingest/Extract [VS-607] (#8000); - python -> python3 (#8001); - Generate Hail import/export script [VS-605] (#8002); - clearer error when values are missing (#7939); - Ah [VS-565] output intervals and sample list (#8010); - make CreateAltAlleleTable task volatile (#8011); - Restore withdrawn [VS-581] (#8006); - Km gvs add storage cost and cleanup doc (#8012); - Updating documentation to reflect the changed outputs [VS-565] (#8014); - File of callset samples -> samples marked as 'withdrawn' in GVS [VS-436] (#8009); - fix quota guidelines for CPUs (#8016); - Add in ability to tweak sample-every-Nth-variant parameter for SNP model creation (#8019); - add initial notebook copy pasta (#8008); - add sample_table_timestamp to GetNumSamplesLoaded (#8022); - Batched Avro export [VS-630] (#8020); - Updating references to old GATK for VS-620 (#8023); - VS-517 Use standard version of GetBQTableLastModifiedDatetime in GvsValidateVat (#8024); - Fix bug in GvsWithdrawSamples.wdl (#8026); - Ah 617 exposing the drop_state parameter to the GvsJointVariantCalling wdl used for beta (and internal customer) (#8032); - Expose maximum-training-variants VQSR parameter [VS-634] (#8029); - Callset statistics [VS-560] (#8018); - Check for withdrawn before exporting to AVRO files [VS-646] (#8039); - Small updates to GVS Integration WDL [VS-618] (#8042); - Rework Hail script gener,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8251:28212,Usability,guid,guidelines,28212,ge of ServiceAccount from CreateVat related WDLs (#7974); - WDL to extract Avro files for Hail import [VS-579] (#7981); - Removed usage of service account from WDLs (#7985); - Document steps for GVS cleanup for base use case [VS-586] (#7989); - Change backticks to single quotes in several error messages - causing shell to attempt to execute. (#7995); - VS-598 - Minor update to AoU Documentation. (#7994); - Allow for incremental addition of data to alt_allele [VS-52] (#7993); - Minor AoU Documentation Update (#7999); - Batch population of alt_allele table from vet_ tables [VS-265] (#7998); - Change drop_state to NONE for Ingest/Extract [VS-607] (#8000); - python -> python3 (#8001); - Generate Hail import/export script [VS-605] (#8002); - clearer error when values are missing (#7939); - Ah [VS-565] output intervals and sample list (#8010); - make CreateAltAlleleTable task volatile (#8011); - Restore withdrawn [VS-581] (#8006); - Km gvs add storage cost and cleanup doc (#8012); - Updating documentation to reflect the changed outputs [VS-565] (#8014); - File of callset samples -> samples marked as 'withdrawn' in GVS [VS-436] (#8009); - fix quota guidelines for CPUs (#8016); - Add in ability to tweak sample-every-Nth-variant parameter for SNP model creation (#8019); - add initial notebook copy pasta (#8008); - add sample_table_timestamp to GetNumSamplesLoaded (#8022); - Batched Avro export [VS-630] (#8020); - Updating references to old GATK for VS-620 (#8023); - VS-517 Use standard version of GetBQTableLastModifiedDatetime in GvsValidateVat (#8024); - Fix bug in GvsWithdrawSamples.wdl (#8026); - Ah 617 exposing the drop_state parameter to the GvsJointVariantCalling wdl used for beta (and internal customer) (#8032); - Expose maximum-training-variants VQSR parameter [VS-634] (#8029); - Callset statistics [VS-560] (#8018); - Check for withdrawn before exporting to AVRO files [VS-646] (#8039); - Small updates to GVS Integration WDL [VS-618] (#8042); - Rework Hail script gener,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251
https://github.com/broadinstitute/gatk/pull/8252:0,Testability,Test,Testing,0,Testing that this is excluded from tests.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8252
https://github.com/broadinstitute/gatk/pull/8252:35,Testability,test,tests,35,Testing that this is excluded from tests.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8252
https://github.com/broadinstitute/gatk/issues/8253:112,Deployability,release,release,112,"## Bug Report. ### Affected tool(s) or class(es). HaplotypeCaller. ### Affected version(s); - [x] Latest public release version (4.4.0.0, also 4.1.4.1); - [ ] Latest master branch as of [date of test?]. ### Description . I am using the HaplotypeCaller (GATK 4.4.0.0). When I look at the input BAM file in IGV, I expect the variant `NC_000015.9:g.48760182_48760185delinsGGGT`. However, HaplotypeCaller reports `NC_000015.9:g.48760182_48760185del` as well as an insertion `NC_000015.9:g.48760184_48760185insGGGT` (i.e. two distinct variants instead of a single indel). In the `bamout`, one can clearly see that the local realignment suggests the deletion + insertion and not the indel. ![image](https://user-images.githubusercontent.com/58295931/226553360-bff887ea-3823-44b7-bddb-46f70705c0b3.png). I understand that the local realignment is expected to improve variant calling and that his approach is battle-tested. I am thus not convinced this is a bug. However, the realignment/variant call is not obvious to the human eye - one would expect the indel instead. The variant seems like a clear heterozygous indel. I checked this [blog post](https://gatk.broadinstitute.org/hc/en-us/articles/360035891111-Expected-variant-at-a-specific-site-was-not-called): The bamout is as outlined above, the mapping + base quality seems fine (judging by IGV) and `--max-alternate-alleles` doesn't seem useful here (and indeed doesn't do anything to the result). I didn't got into kmer fiddling as suggested by the blog post. This is not a homopoly region. I also tested with 4.1.4.1 which only reports the deletion. The screenshot from above is from the 4.4.0.0 invocation. Here is the same situation for 4.1.4.1 (realignment is similar, `out.vcf` does not contain the insertion):. ![image](https://user-images.githubusercontent.com/58295931/226554045-0d9dd7e3-65ec-40ce-a6bd-74d73d4a2507.png). FYI, the variant lies on FBN1 / NM_000138.5 (rev strand). cDNA notation would be `NM_000138.5:c.4698_4701del` or `NM_000",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8253
https://github.com/broadinstitute/gatk/issues/8253:195,Testability,test,test,195,"## Bug Report. ### Affected tool(s) or class(es). HaplotypeCaller. ### Affected version(s); - [x] Latest public release version (4.4.0.0, also 4.1.4.1); - [ ] Latest master branch as of [date of test?]. ### Description . I am using the HaplotypeCaller (GATK 4.4.0.0). When I look at the input BAM file in IGV, I expect the variant `NC_000015.9:g.48760182_48760185delinsGGGT`. However, HaplotypeCaller reports `NC_000015.9:g.48760182_48760185del` as well as an insertion `NC_000015.9:g.48760184_48760185insGGGT` (i.e. two distinct variants instead of a single indel). In the `bamout`, one can clearly see that the local realignment suggests the deletion + insertion and not the indel. ![image](https://user-images.githubusercontent.com/58295931/226553360-bff887ea-3823-44b7-bddb-46f70705c0b3.png). I understand that the local realignment is expected to improve variant calling and that his approach is battle-tested. I am thus not convinced this is a bug. However, the realignment/variant call is not obvious to the human eye - one would expect the indel instead. The variant seems like a clear heterozygous indel. I checked this [blog post](https://gatk.broadinstitute.org/hc/en-us/articles/360035891111-Expected-variant-at-a-specific-site-was-not-called): The bamout is as outlined above, the mapping + base quality seems fine (judging by IGV) and `--max-alternate-alleles` doesn't seem useful here (and indeed doesn't do anything to the result). I didn't got into kmer fiddling as suggested by the blog post. This is not a homopoly region. I also tested with 4.1.4.1 which only reports the deletion. The screenshot from above is from the 4.4.0.0 invocation. Here is the same situation for 4.1.4.1 (realignment is similar, `out.vcf` does not contain the insertion):. ![image](https://user-images.githubusercontent.com/58295931/226554045-0d9dd7e3-65ec-40ce-a6bd-74d73d4a2507.png). FYI, the variant lies on FBN1 / NM_000138.5 (rev strand). cDNA notation would be `NM_000138.5:c.4698_4701del` or `NM_000",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8253
https://github.com/broadinstitute/gatk/issues/8253:908,Testability,test,tested,908,"## Bug Report. ### Affected tool(s) or class(es). HaplotypeCaller. ### Affected version(s); - [x] Latest public release version (4.4.0.0, also 4.1.4.1); - [ ] Latest master branch as of [date of test?]. ### Description . I am using the HaplotypeCaller (GATK 4.4.0.0). When I look at the input BAM file in IGV, I expect the variant `NC_000015.9:g.48760182_48760185delinsGGGT`. However, HaplotypeCaller reports `NC_000015.9:g.48760182_48760185del` as well as an insertion `NC_000015.9:g.48760184_48760185insGGGT` (i.e. two distinct variants instead of a single indel). In the `bamout`, one can clearly see that the local realignment suggests the deletion + insertion and not the indel. ![image](https://user-images.githubusercontent.com/58295931/226553360-bff887ea-3823-44b7-bddb-46f70705c0b3.png). I understand that the local realignment is expected to improve variant calling and that his approach is battle-tested. I am thus not convinced this is a bug. However, the realignment/variant call is not obvious to the human eye - one would expect the indel instead. The variant seems like a clear heterozygous indel. I checked this [blog post](https://gatk.broadinstitute.org/hc/en-us/articles/360035891111-Expected-variant-at-a-specific-site-was-not-called): The bamout is as outlined above, the mapping + base quality seems fine (judging by IGV) and `--max-alternate-alleles` doesn't seem useful here (and indeed doesn't do anything to the result). I didn't got into kmer fiddling as suggested by the blog post. This is not a homopoly region. I also tested with 4.1.4.1 which only reports the deletion. The screenshot from above is from the 4.4.0.0 invocation. Here is the same situation for 4.1.4.1 (realignment is similar, `out.vcf` does not contain the insertion):. ![image](https://user-images.githubusercontent.com/58295931/226554045-0d9dd7e3-65ec-40ce-a6bd-74d73d4a2507.png). FYI, the variant lies on FBN1 / NM_000138.5 (rev strand). cDNA notation would be `NM_000138.5:c.4698_4701del` or `NM_000",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8253
https://github.com/broadinstitute/gatk/issues/8253:1549,Testability,test,tested,1549,"single indel). In the `bamout`, one can clearly see that the local realignment suggests the deletion + insertion and not the indel. ![image](https://user-images.githubusercontent.com/58295931/226553360-bff887ea-3823-44b7-bddb-46f70705c0b3.png). I understand that the local realignment is expected to improve variant calling and that his approach is battle-tested. I am thus not convinced this is a bug. However, the realignment/variant call is not obvious to the human eye - one would expect the indel instead. The variant seems like a clear heterozygous indel. I checked this [blog post](https://gatk.broadinstitute.org/hc/en-us/articles/360035891111-Expected-variant-at-a-specific-site-was-not-called): The bamout is as outlined above, the mapping + base quality seems fine (judging by IGV) and `--max-alternate-alleles` doesn't seem useful here (and indeed doesn't do anything to the result). I didn't got into kmer fiddling as suggested by the blog post. This is not a homopoly region. I also tested with 4.1.4.1 which only reports the deletion. The screenshot from above is from the 4.4.0.0 invocation. Here is the same situation for 4.1.4.1 (realignment is similar, `out.vcf` does not contain the insertion):. ![image](https://user-images.githubusercontent.com/58295931/226554045-0d9dd7e3-65ec-40ce-a6bd-74d73d4a2507.png). FYI, the variant lies on FBN1 / NM_000138.5 (rev strand). cDNA notation would be `NM_000138.5:c.4698_4701del` or `NM_000138.5:c.4698_4701delinsACCC`.; (Actually, assuming delins is correct, this could rather be `NC_000015.9:g.48760182_48760185inv` / `NM_000138.5:c.4698_4701inv`). #### Steps to reproduce. Please find [attached](https://github.com/broadinstitute/gatk/files/11026375/hc-del-and-ins-vs-indel.zip) the necessary data to reproduce the scenario. #### Expected behavior. The indel variant `NC_000015.9:g.48760182_48760185delinsGGGT` gets called. #### Actual behavior. Two distinct variants `NC_000015.9:g.48760182_48760185del` as well as a `NC_000015.9:g.48760",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8253
https://github.com/broadinstitute/gatk/issues/8253:592,Usability,clear,clearly,592,"## Bug Report. ### Affected tool(s) or class(es). HaplotypeCaller. ### Affected version(s); - [x] Latest public release version (4.4.0.0, also 4.1.4.1); - [ ] Latest master branch as of [date of test?]. ### Description . I am using the HaplotypeCaller (GATK 4.4.0.0). When I look at the input BAM file in IGV, I expect the variant `NC_000015.9:g.48760182_48760185delinsGGGT`. However, HaplotypeCaller reports `NC_000015.9:g.48760182_48760185del` as well as an insertion `NC_000015.9:g.48760184_48760185insGGGT` (i.e. two distinct variants instead of a single indel). In the `bamout`, one can clearly see that the local realignment suggests the deletion + insertion and not the indel. ![image](https://user-images.githubusercontent.com/58295931/226553360-bff887ea-3823-44b7-bddb-46f70705c0b3.png). I understand that the local realignment is expected to improve variant calling and that his approach is battle-tested. I am thus not convinced this is a bug. However, the realignment/variant call is not obvious to the human eye - one would expect the indel instead. The variant seems like a clear heterozygous indel. I checked this [blog post](https://gatk.broadinstitute.org/hc/en-us/articles/360035891111-Expected-variant-at-a-specific-site-was-not-called): The bamout is as outlined above, the mapping + base quality seems fine (judging by IGV) and `--max-alternate-alleles` doesn't seem useful here (and indeed doesn't do anything to the result). I didn't got into kmer fiddling as suggested by the blog post. This is not a homopoly region. I also tested with 4.1.4.1 which only reports the deletion. The screenshot from above is from the 4.4.0.0 invocation. Here is the same situation for 4.1.4.1 (realignment is similar, `out.vcf` does not contain the insertion):. ![image](https://user-images.githubusercontent.com/58295931/226554045-0d9dd7e3-65ec-40ce-a6bd-74d73d4a2507.png). FYI, the variant lies on FBN1 / NM_000138.5 (rev strand). cDNA notation would be `NM_000138.5:c.4698_4701del` or `NM_000",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8253
https://github.com/broadinstitute/gatk/issues/8253:1088,Usability,clear,clear,1088,"(s); - [x] Latest public release version (4.4.0.0, also 4.1.4.1); - [ ] Latest master branch as of [date of test?]. ### Description . I am using the HaplotypeCaller (GATK 4.4.0.0). When I look at the input BAM file in IGV, I expect the variant `NC_000015.9:g.48760182_48760185delinsGGGT`. However, HaplotypeCaller reports `NC_000015.9:g.48760182_48760185del` as well as an insertion `NC_000015.9:g.48760184_48760185insGGGT` (i.e. two distinct variants instead of a single indel). In the `bamout`, one can clearly see that the local realignment suggests the deletion + insertion and not the indel. ![image](https://user-images.githubusercontent.com/58295931/226553360-bff887ea-3823-44b7-bddb-46f70705c0b3.png). I understand that the local realignment is expected to improve variant calling and that his approach is battle-tested. I am thus not convinced this is a bug. However, the realignment/variant call is not obvious to the human eye - one would expect the indel instead. The variant seems like a clear heterozygous indel. I checked this [blog post](https://gatk.broadinstitute.org/hc/en-us/articles/360035891111-Expected-variant-at-a-specific-site-was-not-called): The bamout is as outlined above, the mapping + base quality seems fine (judging by IGV) and `--max-alternate-alleles` doesn't seem useful here (and indeed doesn't do anything to the result). I didn't got into kmer fiddling as suggested by the blog post. This is not a homopoly region. I also tested with 4.1.4.1 which only reports the deletion. The screenshot from above is from the 4.4.0.0 invocation. Here is the same situation for 4.1.4.1 (realignment is similar, `out.vcf` does not contain the insertion):. ![image](https://user-images.githubusercontent.com/58295931/226554045-0d9dd7e3-65ec-40ce-a6bd-74d73d4a2507.png). FYI, the variant lies on FBN1 / NM_000138.5 (rev strand). cDNA notation would be `NM_000138.5:c.4698_4701del` or `NM_000138.5:c.4698_4701delinsACCC`.; (Actually, assuming delins is correct, this could rather",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8253
https://github.com/broadinstitute/gatk/pull/8254:71,Usability,learn,learnings,71,Intro to Cosmos spike comprised of a couple of scripts and quite a few learnings.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8254
https://github.com/broadinstitute/gatk/issues/8255:47,Availability,error,error,47,"When I use gatk to CombineGVCFs, the following error occurs: ""Cannot read file because no suitable codecs found"".; How can I solve this problem？",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8255
https://github.com/broadinstitute/gatk/pull/8257:748,Availability,error,error,748,"- Adds size similarity criterion to SVConcordance and SVCluster tools. This is particularly useful for accurately matching smaller SVs that have a high degree of breakpoint uncertainty, in which case reciprocal overlap does not work well. PESR/mixed variant types must have size similarity, reciprocal overlap, and breakend window criteria met. Depth-only variants may have either size similarity + reciprocal overlap OR breakend window criteria met (or both).; - Rewrites some of the linkage logic to be simpler to read.; - Fixes a rare bug with `SortedMultiset` in `SVClusterEngine` that sometimes caused records with identical start positions to get lost.; - Removes null record attributes to avoid `.` INFO/FORMAT fields, which cause a parsing error with Integer types.; - Add check that the vcf header contigs are sorted in the same order.; - Retain FILTER and QUAL fields in output.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8257
https://github.com/broadinstitute/gatk/pull/8257:464,Modifiability,Rewrite,Rewrites,464,"- Adds size similarity criterion to SVConcordance and SVCluster tools. This is particularly useful for accurately matching smaller SVs that have a high degree of breakpoint uncertainty, in which case reciprocal overlap does not work well. PESR/mixed variant types must have size similarity, reciprocal overlap, and breakend window criteria met. Depth-only variants may have either size similarity + reciprocal overlap OR breakend window criteria met (or both).; - Rewrites some of the linkage logic to be simpler to read.; - Fixes a rare bug with `SortedMultiset` in `SVClusterEngine` that sometimes caused records with identical start positions to get lost.; - Removes null record attributes to avoid `.` INFO/FORMAT fields, which cause a parsing error with Integer types.; - Add check that the vcf header contigs are sorted in the same order.; - Retain FILTER and QUAL fields in output.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8257
https://github.com/broadinstitute/gatk/pull/8257:696,Safety,avoid,avoid,696,"- Adds size similarity criterion to SVConcordance and SVCluster tools. This is particularly useful for accurately matching smaller SVs that have a high degree of breakpoint uncertainty, in which case reciprocal overlap does not work well. PESR/mixed variant types must have size similarity, reciprocal overlap, and breakend window criteria met. Depth-only variants may have either size similarity + reciprocal overlap OR breakend window criteria met (or both).; - Rewrites some of the linkage logic to be simpler to read.; - Fixes a rare bug with `SortedMultiset` in `SVClusterEngine` that sometimes caused records with identical start positions to get lost.; - Removes null record attributes to avoid `.` INFO/FORMAT fields, which cause a parsing error with Integer types.; - Add check that the vcf header contigs are sorted in the same order.; - Retain FILTER and QUAL fields in output.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8257
https://github.com/broadinstitute/gatk/pull/8257:493,Testability,log,logic,493,"- Adds size similarity criterion to SVConcordance and SVCluster tools. This is particularly useful for accurately matching smaller SVs that have a high degree of breakpoint uncertainty, in which case reciprocal overlap does not work well. PESR/mixed variant types must have size similarity, reciprocal overlap, and breakend window criteria met. Depth-only variants may have either size similarity + reciprocal overlap OR breakend window criteria met (or both).; - Rewrites some of the linkage logic to be simpler to read.; - Fixes a rare bug with `SortedMultiset` in `SVClusterEngine` that sometimes caused records with identical start positions to get lost.; - Removes null record attributes to avoid `.` INFO/FORMAT fields, which cause a parsing error with Integer types.; - Add check that the vcf header contigs are sorted in the same order.; - Retain FILTER and QUAL fields in output.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8257
https://github.com/broadinstitute/gatk/pull/8257:505,Usability,simpl,simpler,505,"- Adds size similarity criterion to SVConcordance and SVCluster tools. This is particularly useful for accurately matching smaller SVs that have a high degree of breakpoint uncertainty, in which case reciprocal overlap does not work well. PESR/mixed variant types must have size similarity, reciprocal overlap, and breakend window criteria met. Depth-only variants may have either size similarity + reciprocal overlap OR breakend window criteria met (or both).; - Rewrites some of the linkage logic to be simpler to read.; - Fixes a rare bug with `SortedMultiset` in `SVClusterEngine` that sometimes caused records with identical start positions to get lost.; - Removes null record attributes to avoid `.` INFO/FORMAT fields, which cause a parsing error with Integer types.; - Add check that the vcf header contigs are sorted in the same order.; - Retain FILTER and QUAL fields in output.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8257
https://github.com/broadinstitute/gatk/pull/8261:0,Energy Efficiency,Monitor,Monitoring,0,Monitoring script removed.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8261
https://github.com/broadinstitute/gatk/pull/8262:0,Deployability,Integrat,Integration,0,Integration run kicked off [here](https://job-manager.dsde-prod.broadinstitute.org/jobs/e706347d-cae4-4138-a5a3-34692b39ae89). Previous versions completed successfully but failed cost comparison because the name of the WDL step changed and was unrecognized by the branches of the comparison code that cut slack.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8262
https://github.com/broadinstitute/gatk/pull/8262:0,Integrability,Integrat,Integration,0,Integration run kicked off [here](https://job-manager.dsde-prod.broadinstitute.org/jobs/e706347d-cae4-4138-a5a3-34692b39ae89). Previous versions completed successfully but failed cost comparison because the name of the WDL step changed and was unrecognized by the branches of the comparison code that cut slack.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8262
https://github.com/broadinstitute/gatk/pull/8263:14,Testability,TEST,TEST,14,DRAFT - SMOKE TEST. Add in all the GVS code with some questionable conflict resolution of 95b154810f0614214b4a0fdea809a7eeaa8b2b70,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8263
https://github.com/broadinstitute/gatk/issues/8264:243,Availability,error,error,243,"## Feature request. ### Tool(s) or class(es) involved; LearnReadOrientation (& others I believe). ### Description; Wondering if you would consider modifying the exit status for 'java.lang.OutOfMemoryError` to reflect it being a memory-related error, perhaps `137`? This would help with pipelines that will retry with more memory in response to a memory-related error. The exit status is currently a generic `1`:. ```; Command exit status:; 1. ...; [March 23, 2023 at 5:50:17 AM GMT] org.broadinstitute.hellbender.tools.walkers.readorientation.LearnReadOrientationModel done. Elapsed time: 2,210.83 minutes.; Runtime.totalMemory()=7796817920; Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space; at org.apache.commons.math3.linear.Array2DRowRealMatrix.<init>(Array2DRowRealMatrix.java:61); at org.broadinstitute.hellbender.tools.walkers.readorientation.LearnReadOrientationModelEngine.<init>(LearnReadOrientationModelEngine.java:131); at org.broadinstitute.hellbender.tools.walkers.readorientation.LearnReadOrientationModel.doWork(LearnReadOrientationModel.java:163); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:149); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:198); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:217); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); Using GATK jar /gatk/gatk-package-4.4.0.0-local.jar; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8264
https://github.com/broadinstitute/gatk/issues/8264:361,Availability,error,error,361,"## Feature request. ### Tool(s) or class(es) involved; LearnReadOrientation (& others I believe). ### Description; Wondering if you would consider modifying the exit status for 'java.lang.OutOfMemoryError` to reflect it being a memory-related error, perhaps `137`? This would help with pipelines that will retry with more memory in response to a memory-related error. The exit status is currently a generic `1`:. ```; Command exit status:; 1. ...; [March 23, 2023 at 5:50:17 AM GMT] org.broadinstitute.hellbender.tools.walkers.readorientation.LearnReadOrientationModel done. Elapsed time: 2,210.83 minutes.; Runtime.totalMemory()=7796817920; Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space; at org.apache.commons.math3.linear.Array2DRowRealMatrix.<init>(Array2DRowRealMatrix.java:61); at org.broadinstitute.hellbender.tools.walkers.readorientation.LearnReadOrientationModelEngine.<init>(LearnReadOrientationModelEngine.java:131); at org.broadinstitute.hellbender.tools.walkers.readorientation.LearnReadOrientationModel.doWork(LearnReadOrientationModel.java:163); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:149); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:198); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:217); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); Using GATK jar /gatk/gatk-package-4.4.0.0-local.jar; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8264
https://github.com/broadinstitute/gatk/issues/8264:286,Deployability,pipeline,pipelines,286,"## Feature request. ### Tool(s) or class(es) involved; LearnReadOrientation (& others I believe). ### Description; Wondering if you would consider modifying the exit status for 'java.lang.OutOfMemoryError` to reflect it being a memory-related error, perhaps `137`? This would help with pipelines that will retry with more memory in response to a memory-related error. The exit status is currently a generic `1`:. ```; Command exit status:; 1. ...; [March 23, 2023 at 5:50:17 AM GMT] org.broadinstitute.hellbender.tools.walkers.readorientation.LearnReadOrientationModel done. Elapsed time: 2,210.83 minutes.; Runtime.totalMemory()=7796817920; Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space; at org.apache.commons.math3.linear.Array2DRowRealMatrix.<init>(Array2DRowRealMatrix.java:61); at org.broadinstitute.hellbender.tools.walkers.readorientation.LearnReadOrientationModelEngine.<init>(LearnReadOrientationModelEngine.java:131); at org.broadinstitute.hellbender.tools.walkers.readorientation.LearnReadOrientationModel.doWork(LearnReadOrientationModel.java:163); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:149); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:198); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:217); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); Using GATK jar /gatk/gatk-package-4.4.0.0-local.jar; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8264
https://github.com/broadinstitute/gatk/issues/8264:55,Usability,Learn,LearnReadOrientation,55,"## Feature request. ### Tool(s) or class(es) involved; LearnReadOrientation (& others I believe). ### Description; Wondering if you would consider modifying the exit status for 'java.lang.OutOfMemoryError` to reflect it being a memory-related error, perhaps `137`? This would help with pipelines that will retry with more memory in response to a memory-related error. The exit status is currently a generic `1`:. ```; Command exit status:; 1. ...; [March 23, 2023 at 5:50:17 AM GMT] org.broadinstitute.hellbender.tools.walkers.readorientation.LearnReadOrientationModel done. Elapsed time: 2,210.83 minutes.; Runtime.totalMemory()=7796817920; Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space; at org.apache.commons.math3.linear.Array2DRowRealMatrix.<init>(Array2DRowRealMatrix.java:61); at org.broadinstitute.hellbender.tools.walkers.readorientation.LearnReadOrientationModelEngine.<init>(LearnReadOrientationModelEngine.java:131); at org.broadinstitute.hellbender.tools.walkers.readorientation.LearnReadOrientationModel.doWork(LearnReadOrientationModel.java:163); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:149); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:198); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:217); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); Using GATK jar /gatk/gatk-package-4.4.0.0-local.jar; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8264
https://github.com/broadinstitute/gatk/issues/8264:543,Usability,Learn,LearnReadOrientationModel,543,"## Feature request. ### Tool(s) or class(es) involved; LearnReadOrientation (& others I believe). ### Description; Wondering if you would consider modifying the exit status for 'java.lang.OutOfMemoryError` to reflect it being a memory-related error, perhaps `137`? This would help with pipelines that will retry with more memory in response to a memory-related error. The exit status is currently a generic `1`:. ```; Command exit status:; 1. ...; [March 23, 2023 at 5:50:17 AM GMT] org.broadinstitute.hellbender.tools.walkers.readorientation.LearnReadOrientationModel done. Elapsed time: 2,210.83 minutes.; Runtime.totalMemory()=7796817920; Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space; at org.apache.commons.math3.linear.Array2DRowRealMatrix.<init>(Array2DRowRealMatrix.java:61); at org.broadinstitute.hellbender.tools.walkers.readorientation.LearnReadOrientationModelEngine.<init>(LearnReadOrientationModelEngine.java:131); at org.broadinstitute.hellbender.tools.walkers.readorientation.LearnReadOrientationModel.doWork(LearnReadOrientationModel.java:163); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:149); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:198); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:217); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); Using GATK jar /gatk/gatk-package-4.4.0.0-local.jar; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8264
https://github.com/broadinstitute/gatk/issues/8264:871,Usability,Learn,LearnReadOrientationModelEngine,871,"## Feature request. ### Tool(s) or class(es) involved; LearnReadOrientation (& others I believe). ### Description; Wondering if you would consider modifying the exit status for 'java.lang.OutOfMemoryError` to reflect it being a memory-related error, perhaps `137`? This would help with pipelines that will retry with more memory in response to a memory-related error. The exit status is currently a generic `1`:. ```; Command exit status:; 1. ...; [March 23, 2023 at 5:50:17 AM GMT] org.broadinstitute.hellbender.tools.walkers.readorientation.LearnReadOrientationModel done. Elapsed time: 2,210.83 minutes.; Runtime.totalMemory()=7796817920; Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space; at org.apache.commons.math3.linear.Array2DRowRealMatrix.<init>(Array2DRowRealMatrix.java:61); at org.broadinstitute.hellbender.tools.walkers.readorientation.LearnReadOrientationModelEngine.<init>(LearnReadOrientationModelEngine.java:131); at org.broadinstitute.hellbender.tools.walkers.readorientation.LearnReadOrientationModel.doWork(LearnReadOrientationModel.java:163); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:149); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:198); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:217); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); Using GATK jar /gatk/gatk-package-4.4.0.0-local.jar; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8264
https://github.com/broadinstitute/gatk/issues/8264:910,Usability,Learn,LearnReadOrientationModelEngine,910,"## Feature request. ### Tool(s) or class(es) involved; LearnReadOrientation (& others I believe). ### Description; Wondering if you would consider modifying the exit status for 'java.lang.OutOfMemoryError` to reflect it being a memory-related error, perhaps `137`? This would help with pipelines that will retry with more memory in response to a memory-related error. The exit status is currently a generic `1`:. ```; Command exit status:; 1. ...; [March 23, 2023 at 5:50:17 AM GMT] org.broadinstitute.hellbender.tools.walkers.readorientation.LearnReadOrientationModel done. Elapsed time: 2,210.83 minutes.; Runtime.totalMemory()=7796817920; Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space; at org.apache.commons.math3.linear.Array2DRowRealMatrix.<init>(Array2DRowRealMatrix.java:61); at org.broadinstitute.hellbender.tools.walkers.readorientation.LearnReadOrientationModelEngine.<init>(LearnReadOrientationModelEngine.java:131); at org.broadinstitute.hellbender.tools.walkers.readorientation.LearnReadOrientationModel.doWork(LearnReadOrientationModel.java:163); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:149); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:198); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:217); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); Using GATK jar /gatk/gatk-package-4.4.0.0-local.jar; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8264
https://github.com/broadinstitute/gatk/issues/8264:1016,Usability,Learn,LearnReadOrientationModel,1016,"## Feature request. ### Tool(s) or class(es) involved; LearnReadOrientation (& others I believe). ### Description; Wondering if you would consider modifying the exit status for 'java.lang.OutOfMemoryError` to reflect it being a memory-related error, perhaps `137`? This would help with pipelines that will retry with more memory in response to a memory-related error. The exit status is currently a generic `1`:. ```; Command exit status:; 1. ...; [March 23, 2023 at 5:50:17 AM GMT] org.broadinstitute.hellbender.tools.walkers.readorientation.LearnReadOrientationModel done. Elapsed time: 2,210.83 minutes.; Runtime.totalMemory()=7796817920; Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space; at org.apache.commons.math3.linear.Array2DRowRealMatrix.<init>(Array2DRowRealMatrix.java:61); at org.broadinstitute.hellbender.tools.walkers.readorientation.LearnReadOrientationModelEngine.<init>(LearnReadOrientationModelEngine.java:131); at org.broadinstitute.hellbender.tools.walkers.readorientation.LearnReadOrientationModel.doWork(LearnReadOrientationModel.java:163); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:149); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:198); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:217); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); Using GATK jar /gatk/gatk-package-4.4.0.0-local.jar; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8264
https://github.com/broadinstitute/gatk/issues/8264:1049,Usability,Learn,LearnReadOrientationModel,1049,"## Feature request. ### Tool(s) or class(es) involved; LearnReadOrientation (& others I believe). ### Description; Wondering if you would consider modifying the exit status for 'java.lang.OutOfMemoryError` to reflect it being a memory-related error, perhaps `137`? This would help with pipelines that will retry with more memory in response to a memory-related error. The exit status is currently a generic `1`:. ```; Command exit status:; 1. ...; [March 23, 2023 at 5:50:17 AM GMT] org.broadinstitute.hellbender.tools.walkers.readorientation.LearnReadOrientationModel done. Elapsed time: 2,210.83 minutes.; Runtime.totalMemory()=7796817920; Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space; at org.apache.commons.math3.linear.Array2DRowRealMatrix.<init>(Array2DRowRealMatrix.java:61); at org.broadinstitute.hellbender.tools.walkers.readorientation.LearnReadOrientationModelEngine.<init>(LearnReadOrientationModelEngine.java:131); at org.broadinstitute.hellbender.tools.walkers.readorientation.LearnReadOrientationModel.doWork(LearnReadOrientationModel.java:163); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:149); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:198); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:217); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); Using GATK jar /gatk/gatk-package-4.4.0.0-local.jar; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8264
https://github.com/broadinstitute/gatk/pull/8265:33,Availability,toler,tolerate,33,Modern versions of womtool don't tolerate task inputs with the same name as task outputs and produce baffling error messages like [this](https://broadinstitute.slack.com/archives/C4GSMFXS9/p1654807836682679).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8265
https://github.com/broadinstitute/gatk/pull/8265:110,Availability,error,error,110,Modern versions of womtool don't tolerate task inputs with the same name as task outputs and produce baffling error messages like [this](https://broadinstitute.slack.com/archives/C4GSMFXS9/p1654807836682679).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8265
https://github.com/broadinstitute/gatk/pull/8265:116,Integrability,message,messages,116,Modern versions of womtool don't tolerate task inputs with the same name as task outputs and produce baffling error messages like [this](https://broadinstitute.slack.com/archives/C4GSMFXS9/p1654807836682679).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8265
https://github.com/broadinstitute/gatk/pull/8266:109,Deployability,release,release-,109,If the requested key is missing in an Avro record:. - Avro 1.11 [throws](https://github.com/apache/avro/blob/release-1.11.0/lang/java/avro/src/main/java/org/apache/avro/generic/GenericData.java#L267-L269); - Avro 1.8 [returns null](https://github.com/apache/avro/blob/release-1.8.0/lang/java/avro/src/main/java/org/apache/avro/generic/GenericData.java#L208). Most of the code here was written for Avro 1.8 behavior; these changes adapt for Avro 1.11.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8266
https://github.com/broadinstitute/gatk/pull/8266:268,Deployability,release,release-,268,If the requested key is missing in an Avro record:. - Avro 1.11 [throws](https://github.com/apache/avro/blob/release-1.11.0/lang/java/avro/src/main/java/org/apache/avro/generic/GenericData.java#L267-L269); - Avro 1.8 [returns null](https://github.com/apache/avro/blob/release-1.8.0/lang/java/avro/src/main/java/org/apache/avro/generic/GenericData.java#L208). Most of the code here was written for Avro 1.8 behavior; these changes adapt for Avro 1.11.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8266
https://github.com/broadinstitute/gatk/pull/8266:430,Energy Efficiency,adapt,adapt,430,If the requested key is missing in an Avro record:. - Avro 1.11 [throws](https://github.com/apache/avro/blob/release-1.11.0/lang/java/avro/src/main/java/org/apache/avro/generic/GenericData.java#L267-L269); - Avro 1.8 [returns null](https://github.com/apache/avro/blob/release-1.8.0/lang/java/avro/src/main/java/org/apache/avro/generic/GenericData.java#L208). Most of the code here was written for Avro 1.8 behavior; these changes adapt for Avro 1.11.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8266
https://github.com/broadinstitute/gatk/pull/8266:430,Modifiability,adapt,adapt,430,If the requested key is missing in an Avro record:. - Avro 1.11 [throws](https://github.com/apache/avro/blob/release-1.11.0/lang/java/avro/src/main/java/org/apache/avro/generic/GenericData.java#L267-L269); - Avro 1.8 [returns null](https://github.com/apache/avro/blob/release-1.8.0/lang/java/avro/src/main/java/org/apache/avro/generic/GenericData.java#L208). Most of the code here was written for Avro 1.8 behavior; these changes adapt for Avro 1.11.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8266
https://github.com/broadinstitute/gatk/pull/8268:104,Testability,test,test,104,Adding the uber_monitor.py script to GvsUtils.wdl; Threaded it into GvsCreateFilterSet; Included a unit test.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8268
https://github.com/broadinstitute/gatk/pull/8269:0,Deployability,Update,Update,0,Update to latest version of VQSR Lite; Refactor GvsCreateFilterSet.wdl to move VQSR Classic code to its own WDL,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8269
https://github.com/broadinstitute/gatk/pull/8269:39,Modifiability,Refactor,Refactor,39,Update to latest version of VQSR Lite; Refactor GvsCreateFilterSet.wdl to move VQSR Classic code to its own WDL,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8269
https://github.com/broadinstitute/gatk/issues/8272:502,Availability,error,error,502,There were a few dead links in the GATK to http://gatkforums.broadinstitute.org/gatk/discussion/58/companion-utilities-reordersam which is still archived here: https://web.archive.org/web/20160720131152/http://gatkforums.broadinstitute.org/gatk/discussion/58/companion-utilities-reordersam. We should write a new short technical article here: https://gatk.broadinstitute.org/hc/en-us/sections/360007134392-Glossary preserving the knowledge about sort ordering and update the remaining two links in our error messages to be current with that.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8272
https://github.com/broadinstitute/gatk/issues/8272:464,Deployability,update,update,464,There were a few dead links in the GATK to http://gatkforums.broadinstitute.org/gatk/discussion/58/companion-utilities-reordersam which is still archived here: https://web.archive.org/web/20160720131152/http://gatkforums.broadinstitute.org/gatk/discussion/58/companion-utilities-reordersam. We should write a new short technical article here: https://gatk.broadinstitute.org/hc/en-us/sections/360007134392-Glossary preserving the knowledge about sort ordering and update the remaining two links in our error messages to be current with that.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8272
https://github.com/broadinstitute/gatk/issues/8272:508,Integrability,message,messages,508,There were a few dead links in the GATK to http://gatkforums.broadinstitute.org/gatk/discussion/58/companion-utilities-reordersam which is still archived here: https://web.archive.org/web/20160720131152/http://gatkforums.broadinstitute.org/gatk/discussion/58/companion-utilities-reordersam. We should write a new short technical article here: https://gatk.broadinstitute.org/hc/en-us/sections/360007134392-Glossary preserving the knowledge about sort ordering and update the remaining two links in our error messages to be current with that.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8272
https://github.com/broadinstitute/gatk/pull/8274:7,Testability,test,test,7,- Make test `public` so it can be run within IntelliJ.; - Make test clean up after itself.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8274
https://github.com/broadinstitute/gatk/pull/8274:63,Testability,test,test,63,- Make test `public` so it can be run within IntelliJ.; - Make test clean up after itself.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8274
https://github.com/broadinstitute/gatk/issues/8275:206,Availability,down,download,206,"Bug Report. Affected tool(s) or class(es); mutect2 and Funcotator. Affected version(s); gatk-4.1.8.1. ### Description ; I tried to use Funcotator to annotate a vcf file. The vcf file had 436 records. I had download database funcotator_dataSources.v1.7.20200521s to my linux machine, and use --data-sources-path to appoint it; 1. when Funcotator with contidion ""gnomAD_genome.tar.gz "",no results got. ; The error is ; message: All 20 retries failed. Waited a total of 1918000 ms between attempts; Caused by: com.google.cloud.storage.StorageException: Connection reset; Caused by: java.net.SocketException: Connection reset; 2. when Funcotator with contidion ""gnomAD_genome "", results got but has nothing, this is a file with title annotation message but no records. ; The error is ; java.lang.IllegalArgumentException: Unexpected value: MANE_Select. ###Steps to reproduce; vcf file from mutect2-FilterMutectCalls,both snv and indel record. code1; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /root/software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar Funcotator -R /data/Homo_sapiens/Homo_sapiens_38/Homo_sapiens_assembly38.fasta -V /data/project/naangda_panel_20230228/output/mutect/probe/normal_mutect/sssss4.filtered.mutect2.vcf -O /data/project/naangda_panel_20230228/output/mutect/Funcotator/sssss4.funcocator.maf --output-file-format MAF --data-sources-path /data/Homo_sapiens/Homo_sapiens_38/funcotator/funcotator_dataSources.v1.7.20200521s/ --ref-version hg38. code2; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /root/software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar Funcotator -R /data/Homo_sapiens/Homo_sapiens_38/Homo_sapiens_assembly38.fasta -V /data/project/naan",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8275
https://github.com/broadinstitute/gatk/issues/8275:406,Availability,error,error,406,"Bug Report. Affected tool(s) or class(es); mutect2 and Funcotator. Affected version(s); gatk-4.1.8.1. ### Description ; I tried to use Funcotator to annotate a vcf file. The vcf file had 436 records. I had download database funcotator_dataSources.v1.7.20200521s to my linux machine, and use --data-sources-path to appoint it; 1. when Funcotator with contidion ""gnomAD_genome.tar.gz "",no results got. ; The error is ; message: All 20 retries failed. Waited a total of 1918000 ms between attempts; Caused by: com.google.cloud.storage.StorageException: Connection reset; Caused by: java.net.SocketException: Connection reset; 2. when Funcotator with contidion ""gnomAD_genome "", results got but has nothing, this is a file with title annotation message but no records. ; The error is ; java.lang.IllegalArgumentException: Unexpected value: MANE_Select. ###Steps to reproduce; vcf file from mutect2-FilterMutectCalls,both snv and indel record. code1; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /root/software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar Funcotator -R /data/Homo_sapiens/Homo_sapiens_38/Homo_sapiens_assembly38.fasta -V /data/project/naangda_panel_20230228/output/mutect/probe/normal_mutect/sssss4.filtered.mutect2.vcf -O /data/project/naangda_panel_20230228/output/mutect/Funcotator/sssss4.funcocator.maf --output-file-format MAF --data-sources-path /data/Homo_sapiens/Homo_sapiens_38/funcotator/funcotator_dataSources.v1.7.20200521s/ --ref-version hg38. code2; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /root/software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar Funcotator -R /data/Homo_sapiens/Homo_sapiens_38/Homo_sapiens_assembly38.fasta -V /data/project/naan",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8275
https://github.com/broadinstitute/gatk/issues/8275:771,Availability,error,error,771,"Bug Report. Affected tool(s) or class(es); mutect2 and Funcotator. Affected version(s); gatk-4.1.8.1. ### Description ; I tried to use Funcotator to annotate a vcf file. The vcf file had 436 records. I had download database funcotator_dataSources.v1.7.20200521s to my linux machine, and use --data-sources-path to appoint it; 1. when Funcotator with contidion ""gnomAD_genome.tar.gz "",no results got. ; The error is ; message: All 20 retries failed. Waited a total of 1918000 ms between attempts; Caused by: com.google.cloud.storage.StorageException: Connection reset; Caused by: java.net.SocketException: Connection reset; 2. when Funcotator with contidion ""gnomAD_genome "", results got but has nothing, this is a file with title annotation message but no records. ; The error is ; java.lang.IllegalArgumentException: Unexpected value: MANE_Select. ###Steps to reproduce; vcf file from mutect2-FilterMutectCalls,both snv and indel record. code1; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /root/software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar Funcotator -R /data/Homo_sapiens/Homo_sapiens_38/Homo_sapiens_assembly38.fasta -V /data/project/naangda_panel_20230228/output/mutect/probe/normal_mutect/sssss4.filtered.mutect2.vcf -O /data/project/naangda_panel_20230228/output/mutect/Funcotator/sssss4.funcocator.maf --output-file-format MAF --data-sources-path /data/Homo_sapiens/Homo_sapiens_38/funcotator/funcotator_dataSources.v1.7.20200521s/ --ref-version hg38. code2; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /root/software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar Funcotator -R /data/Homo_sapiens/Homo_sapiens_38/Homo_sapiens_assembly38.fasta -V /data/project/naan",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8275
https://github.com/broadinstitute/gatk/issues/8275:2366,Availability,down,download,2366,"/data/project/naangda_panel_20230228/output/mutect/probe/normal_mutect/sssss4.filtered.mutect2.vcf -O /data/project/naangda_panel_20230228/output/mutect/Funcotator/sssss4.funcocator.maf --output-file-format MAF --data-sources-path /data/Homo_sapiens/Homo_sapiens_38/funcotator/funcotator_dataSources.v1.7.20200521s/ --ref-version hg38. code2; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /root/software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar Funcotator -R /data/Homo_sapiens/Homo_sapiens_38/Homo_sapiens_assembly38.fasta -V /data/project/naangda_panel_20230228/output/mutect/probe/normal_mutect/sssss4.filtered.mutect2.vcf -O /data/project/naangda_panel_20230228/output/mutect/Funcotator/sssss4.funcocator.maf --output-file-format MAF --data-sources-path /data/Homo_sapiens/Homo_sapiens_38/funcotator/funcotator_dataSources.v1.7.20200521s/ --ref-version hg38. what's the problem with the program?; 1. I had download database, why error ""com.google.cloud.storage.StorageException: Connection reset"" happened?; 2. why I got only a file with title but no records like this, what should I do to solve this problem？. ## Funcotator 4.1.8.1 | Date 20233731T033757 | Gencode 34 CANONICAL | Achilles 110303 | ClinVar_VCF 20180429_hg38 | Cosmic v84 | CosmicFusion v84 | CosmicTissue v83 | Familial_Cancer_Genes 20110905 | Gencode_XHGNC 90_38 | Gencode_XRefSeq 90_38 | HGNC Nov302017 | Oreganno 20160119 | Simple_Uniprot 2014_12 | dbSNP 9606_b151; Hugo_Symbol	Entrez_Gene_Id	Center	NCBI_Build	Chromosome	Start_Position	End_Position	Strand	Variant_Classification	Variant_Type	Reference_Allele	Tumor_Seq_Allele1	Tumor_Seq_Allele2	dbSNP_RSdbSNP_Val_Status	Tumor_Sample_Barcode	Matched_Norm_Sample_Barcode	Match_Norm_Seq_Allele1	Match_Norm_Seq_Allele2	Tumor_Validation_Allele1	Tumor_Validation_Allele2	Match_Norm_Validation_Allele1	Match_Norm_Valida",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8275
https://github.com/broadinstitute/gatk/issues/8275:2389,Availability,error,error,2389,"/data/project/naangda_panel_20230228/output/mutect/probe/normal_mutect/sssss4.filtered.mutect2.vcf -O /data/project/naangda_panel_20230228/output/mutect/Funcotator/sssss4.funcocator.maf --output-file-format MAF --data-sources-path /data/Homo_sapiens/Homo_sapiens_38/funcotator/funcotator_dataSources.v1.7.20200521s/ --ref-version hg38. code2; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /root/software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar Funcotator -R /data/Homo_sapiens/Homo_sapiens_38/Homo_sapiens_assembly38.fasta -V /data/project/naangda_panel_20230228/output/mutect/probe/normal_mutect/sssss4.filtered.mutect2.vcf -O /data/project/naangda_panel_20230228/output/mutect/Funcotator/sssss4.funcocator.maf --output-file-format MAF --data-sources-path /data/Homo_sapiens/Homo_sapiens_38/funcotator/funcotator_dataSources.v1.7.20200521s/ --ref-version hg38. what's the problem with the program?; 1. I had download database, why error ""com.google.cloud.storage.StorageException: Connection reset"" happened?; 2. why I got only a file with title but no records like this, what should I do to solve this problem？. ## Funcotator 4.1.8.1 | Date 20233731T033757 | Gencode 34 CANONICAL | Achilles 110303 | ClinVar_VCF 20180429_hg38 | Cosmic v84 | CosmicFusion v84 | CosmicTissue v83 | Familial_Cancer_Genes 20110905 | Gencode_XHGNC 90_38 | Gencode_XRefSeq 90_38 | HGNC Nov302017 | Oreganno 20160119 | Simple_Uniprot 2014_12 | dbSNP 9606_b151; Hugo_Symbol	Entrez_Gene_Id	Center	NCBI_Build	Chromosome	Start_Position	End_Position	Strand	Variant_Classification	Variant_Type	Reference_Allele	Tumor_Seq_Allele1	Tumor_Seq_Allele2	dbSNP_RSdbSNP_Val_Status	Tumor_Sample_Barcode	Matched_Norm_Sample_Barcode	Match_Norm_Seq_Allele1	Match_Norm_Seq_Allele2	Tumor_Validation_Allele1	Tumor_Validation_Allele2	Match_Norm_Validation_Allele1	Match_Norm_Valida",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8275
https://github.com/broadinstitute/gatk/issues/8275:417,Integrability,message,message,417,"Bug Report. Affected tool(s) or class(es); mutect2 and Funcotator. Affected version(s); gatk-4.1.8.1. ### Description ; I tried to use Funcotator to annotate a vcf file. The vcf file had 436 records. I had download database funcotator_dataSources.v1.7.20200521s to my linux machine, and use --data-sources-path to appoint it; 1. when Funcotator with contidion ""gnomAD_genome.tar.gz "",no results got. ; The error is ; message: All 20 retries failed. Waited a total of 1918000 ms between attempts; Caused by: com.google.cloud.storage.StorageException: Connection reset; Caused by: java.net.SocketException: Connection reset; 2. when Funcotator with contidion ""gnomAD_genome "", results got but has nothing, this is a file with title annotation message but no records. ; The error is ; java.lang.IllegalArgumentException: Unexpected value: MANE_Select. ###Steps to reproduce; vcf file from mutect2-FilterMutectCalls,both snv and indel record. code1; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /root/software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar Funcotator -R /data/Homo_sapiens/Homo_sapiens_38/Homo_sapiens_assembly38.fasta -V /data/project/naangda_panel_20230228/output/mutect/probe/normal_mutect/sssss4.filtered.mutect2.vcf -O /data/project/naangda_panel_20230228/output/mutect/Funcotator/sssss4.funcocator.maf --output-file-format MAF --data-sources-path /data/Homo_sapiens/Homo_sapiens_38/funcotator/funcotator_dataSources.v1.7.20200521s/ --ref-version hg38. code2; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /root/software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar Funcotator -R /data/Homo_sapiens/Homo_sapiens_38/Homo_sapiens_assembly38.fasta -V /data/project/naan",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8275
https://github.com/broadinstitute/gatk/issues/8275:741,Integrability,message,message,741,"Bug Report. Affected tool(s) or class(es); mutect2 and Funcotator. Affected version(s); gatk-4.1.8.1. ### Description ; I tried to use Funcotator to annotate a vcf file. The vcf file had 436 records. I had download database funcotator_dataSources.v1.7.20200521s to my linux machine, and use --data-sources-path to appoint it; 1. when Funcotator with contidion ""gnomAD_genome.tar.gz "",no results got. ; The error is ; message: All 20 retries failed. Waited a total of 1918000 ms between attempts; Caused by: com.google.cloud.storage.StorageException: Connection reset; Caused by: java.net.SocketException: Connection reset; 2. when Funcotator with contidion ""gnomAD_genome "", results got but has nothing, this is a file with title annotation message but no records. ; The error is ; java.lang.IllegalArgumentException: Unexpected value: MANE_Select. ###Steps to reproduce; vcf file from mutect2-FilterMutectCalls,both snv and indel record. code1; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /root/software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar Funcotator -R /data/Homo_sapiens/Homo_sapiens_38/Homo_sapiens_assembly38.fasta -V /data/project/naangda_panel_20230228/output/mutect/probe/normal_mutect/sssss4.filtered.mutect2.vcf -O /data/project/naangda_panel_20230228/output/mutect/Funcotator/sssss4.funcocator.maf --output-file-format MAF --data-sources-path /data/Homo_sapiens/Homo_sapiens_38/funcotator/funcotator_dataSources.v1.7.20200521s/ --ref-version hg38. code2; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /root/software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar Funcotator -R /data/Homo_sapiens/Homo_sapiens_38/Homo_sapiens_assembly38.fasta -V /data/project/naan",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8275
https://github.com/broadinstitute/gatk/pull/8279:250,Testability,test,test,250,"Hi GATK team,. this is a fix for the following issue : https://github.com/broadinstitute/gatk/issues/7304 where an IndexOutOfBoundsException is raised when there is a non-diploid genotype in `MendelianViolation.isViolation`. I added a switch case to test the child ploidy as well as a few tests in MendelianViolationUnitTest",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8279
https://github.com/broadinstitute/gatk/pull/8279:289,Testability,test,tests,289,"Hi GATK team,. this is a fix for the following issue : https://github.com/broadinstitute/gatk/issues/7304 where an IndexOutOfBoundsException is raised when there is a non-diploid genotype in `MendelianViolation.isViolation`. I added a switch case to test the child ploidy as well as a few tests in MendelianViolationUnitTest",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8279
https://github.com/broadinstitute/gatk/issues/8280:439,Availability,avail,available,439,"Hi, ; This is what i got when i run the command gatk --help ; (base) ameni@ameni-Aspire-A315-55G:~/Documents/pharmacogenomics$ gatk --help. Usage template for all tools (uses --spark-runner LOCAL when used with a Spark tool); gatk AnyTool toolArgs. Usage template for Spark tools (will NOT work on non-Spark tools); gatk SparkTool toolArgs [ -- --spark-runner <LOCAL | SPARK | GCS> sparkArgs ]. Getting help; gatk --list Print the list of available tools. gatk Tool --help Print help on a particular tool. Configuration File Specification; --gatk-config-file PATH/TO/GATK/PROPERTIES/FILE. gatk forwards commands to GATK and adds some sugar for submitting spark jobs. --spark-runner <target> controls how spark tools are run; valid targets are:; LOCAL: run using the in-memory spark runner; SPARK: run using spark-submit on an existing cluster ; --spark-master must be specified; --spark-submit-command may be specified to control the Spark submit command; arguments to spark-submit may optionally be specified after -- ; GCS: run using Google cloud dataproc; commands after the -- will be passed to dataproc; --cluster <your-cluster> must be specified after the --; spark properties and some common spark-submit parameters will be translated ; to dataproc equivalents. --dry-run may be specified to output the generated command line without running it; --java-options 'OPTION1[ OPTION2=Y ... ]' optional - pass the given string of options to the ; java JVM at runtime. ; Java options MUST be passed inside a single string with space-separated values. --debug-port <number> sets up a Java VM debug agent to listen to debugger connections on a; particular port number. This in turn will add the necessary java VM arguments; so that you don't need to explicitly indicate these using --java-options.; --debug-suspend sets the Java VM debug agent up so that the run get immediatelly suspended; waiting for a debugger to connect. By default the port number is 5005 but; can be customized using --debug-port.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8280
https://github.com/broadinstitute/gatk/issues/8280:2071,Availability,error,error,2071,"RK: run using spark-submit on an existing cluster ; --spark-master must be specified; --spark-submit-command may be specified to control the Spark submit command; arguments to spark-submit may optionally be specified after -- ; GCS: run using Google cloud dataproc; commands after the -- will be passed to dataproc; --cluster <your-cluster> must be specified after the --; spark properties and some common spark-submit parameters will be translated ; to dataproc equivalents. --dry-run may be specified to output the generated command line without running it; --java-options 'OPTION1[ OPTION2=Y ... ]' optional - pass the given string of options to the ; java JVM at runtime. ; Java options MUST be passed inside a single string with space-separated values. --debug-port <number> sets up a Java VM debug agent to listen to debugger connections on a; particular port number. This in turn will add the necessary java VM arguments; so that you don't need to explicitly indicate these using --java-options.; --debug-suspend sets the Java VM debug agent up so that the run get immediatelly suspended; waiting for a debugger to connect. By default the port number is 5005 but; can be customized using --debug-port. But when i run other commands with gatk like gatk --list, i got this error :; gatk --list; Using GATK jar /home/ameni/Documents/pharmacogenomics/gatk-4.4.0.0/gatk-package-4.4.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/ameni/Documents/pharmacogenomics/gatk-4.4.0.0/gatk-package-4.4.0.0-local.jar --help; Error: LinkageError occurred while loading main class org.broadinstitute.hellbender.Main; 	java.lang.UnsupportedClassVersionError: org/broadinstitute/hellbender/Main has been compiled by a more recent version of the Java Runtime (class file version 61.0), this version of the Java Runtime only recognizes class file versions up to 55.0",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8280
https://github.com/broadinstitute/gatk/issues/8280:2458,Availability,Error,Error,2458,"RK: run using spark-submit on an existing cluster ; --spark-master must be specified; --spark-submit-command may be specified to control the Spark submit command; arguments to spark-submit may optionally be specified after -- ; GCS: run using Google cloud dataproc; commands after the -- will be passed to dataproc; --cluster <your-cluster> must be specified after the --; spark properties and some common spark-submit parameters will be translated ; to dataproc equivalents. --dry-run may be specified to output the generated command line without running it; --java-options 'OPTION1[ OPTION2=Y ... ]' optional - pass the given string of options to the ; java JVM at runtime. ; Java options MUST be passed inside a single string with space-separated values. --debug-port <number> sets up a Java VM debug agent to listen to debugger connections on a; particular port number. This in turn will add the necessary java VM arguments; so that you don't need to explicitly indicate these using --java-options.; --debug-suspend sets the Java VM debug agent up so that the run get immediatelly suspended; waiting for a debugger to connect. By default the port number is 5005 but; can be customized using --debug-port. But when i run other commands with gatk like gatk --list, i got this error :; gatk --list; Using GATK jar /home/ameni/Documents/pharmacogenomics/gatk-4.4.0.0/gatk-package-4.4.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/ameni/Documents/pharmacogenomics/gatk-4.4.0.0/gatk-package-4.4.0.0-local.jar --help; Error: LinkageError occurred while loading main class org.broadinstitute.hellbender.Main; 	java.lang.UnsupportedClassVersionError: org/broadinstitute/hellbender/Main has been compiled by a more recent version of the Java Runtime (class file version 61.0), this version of the Java Runtime only recognizes class file versions up to 55.0",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8280
https://github.com/broadinstitute/gatk/issues/8280:506,Deployability,Configurat,Configuration,506,"Hi, ; This is what i got when i run the command gatk --help ; (base) ameni@ameni-Aspire-A315-55G:~/Documents/pharmacogenomics$ gatk --help. Usage template for all tools (uses --spark-runner LOCAL when used with a Spark tool); gatk AnyTool toolArgs. Usage template for Spark tools (will NOT work on non-Spark tools); gatk SparkTool toolArgs [ -- --spark-runner <LOCAL | SPARK | GCS> sparkArgs ]. Getting help; gatk --list Print the list of available tools. gatk Tool --help Print help on a particular tool. Configuration File Specification; --gatk-config-file PATH/TO/GATK/PROPERTIES/FILE. gatk forwards commands to GATK and adds some sugar for submitting spark jobs. --spark-runner <target> controls how spark tools are run; valid targets are:; LOCAL: run using the in-memory spark runner; SPARK: run using spark-submit on an existing cluster ; --spark-master must be specified; --spark-submit-command may be specified to control the Spark submit command; arguments to spark-submit may optionally be specified after -- ; GCS: run using Google cloud dataproc; commands after the -- will be passed to dataproc; --cluster <your-cluster> must be specified after the --; spark properties and some common spark-submit parameters will be translated ; to dataproc equivalents. --dry-run may be specified to output the generated command line without running it; --java-options 'OPTION1[ OPTION2=Y ... ]' optional - pass the given string of options to the ; java JVM at runtime. ; Java options MUST be passed inside a single string with space-separated values. --debug-port <number> sets up a Java VM debug agent to listen to debugger connections on a; particular port number. This in turn will add the necessary java VM arguments; so that you don't need to explicitly indicate these using --java-options.; --debug-suspend sets the Java VM debug agent up so that the run get immediatelly suspended; waiting for a debugger to connect. By default the port number is 5005 but; can be customized using --debug-port.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8280
https://github.com/broadinstitute/gatk/issues/8280:506,Modifiability,Config,Configuration,506,"Hi, ; This is what i got when i run the command gatk --help ; (base) ameni@ameni-Aspire-A315-55G:~/Documents/pharmacogenomics$ gatk --help. Usage template for all tools (uses --spark-runner LOCAL when used with a Spark tool); gatk AnyTool toolArgs. Usage template for Spark tools (will NOT work on non-Spark tools); gatk SparkTool toolArgs [ -- --spark-runner <LOCAL | SPARK | GCS> sparkArgs ]. Getting help; gatk --list Print the list of available tools. gatk Tool --help Print help on a particular tool. Configuration File Specification; --gatk-config-file PATH/TO/GATK/PROPERTIES/FILE. gatk forwards commands to GATK and adds some sugar for submitting spark jobs. --spark-runner <target> controls how spark tools are run; valid targets are:; LOCAL: run using the in-memory spark runner; SPARK: run using spark-submit on an existing cluster ; --spark-master must be specified; --spark-submit-command may be specified to control the Spark submit command; arguments to spark-submit may optionally be specified after -- ; GCS: run using Google cloud dataproc; commands after the -- will be passed to dataproc; --cluster <your-cluster> must be specified after the --; spark properties and some common spark-submit parameters will be translated ; to dataproc equivalents. --dry-run may be specified to output the generated command line without running it; --java-options 'OPTION1[ OPTION2=Y ... ]' optional - pass the given string of options to the ; java JVM at runtime. ; Java options MUST be passed inside a single string with space-separated values. --debug-port <number> sets up a Java VM debug agent to listen to debugger connections on a; particular port number. This in turn will add the necessary java VM arguments; so that you don't need to explicitly indicate these using --java-options.; --debug-suspend sets the Java VM debug agent up so that the run get immediatelly suspended; waiting for a debugger to connect. By default the port number is 5005 but; can be customized using --debug-port.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8280
https://github.com/broadinstitute/gatk/issues/8280:547,Modifiability,config,config-file,547,"Hi, ; This is what i got when i run the command gatk --help ; (base) ameni@ameni-Aspire-A315-55G:~/Documents/pharmacogenomics$ gatk --help. Usage template for all tools (uses --spark-runner LOCAL when used with a Spark tool); gatk AnyTool toolArgs. Usage template for Spark tools (will NOT work on non-Spark tools); gatk SparkTool toolArgs [ -- --spark-runner <LOCAL | SPARK | GCS> sparkArgs ]. Getting help; gatk --list Print the list of available tools. gatk Tool --help Print help on a particular tool. Configuration File Specification; --gatk-config-file PATH/TO/GATK/PROPERTIES/FILE. gatk forwards commands to GATK and adds some sugar for submitting spark jobs. --spark-runner <target> controls how spark tools are run; valid targets are:; LOCAL: run using the in-memory spark runner; SPARK: run using spark-submit on an existing cluster ; --spark-master must be specified; --spark-submit-command may be specified to control the Spark submit command; arguments to spark-submit may optionally be specified after -- ; GCS: run using Google cloud dataproc; commands after the -- will be passed to dataproc; --cluster <your-cluster> must be specified after the --; spark properties and some common spark-submit parameters will be translated ; to dataproc equivalents. --dry-run may be specified to output the generated command line without running it; --java-options 'OPTION1[ OPTION2=Y ... ]' optional - pass the given string of options to the ; java JVM at runtime. ; Java options MUST be passed inside a single string with space-separated values. --debug-port <number> sets up a Java VM debug agent to listen to debugger connections on a; particular port number. This in turn will add the necessary java VM arguments; so that you don't need to explicitly indicate these using --java-options.; --debug-suspend sets the Java VM debug agent up so that the run get immediatelly suspended; waiting for a debugger to connect. By default the port number is 5005 but; can be customized using --debug-port.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8280
https://github.com/broadinstitute/gatk/issues/8280:2493,Performance,load,loading,2493,"RK: run using spark-submit on an existing cluster ; --spark-master must be specified; --spark-submit-command may be specified to control the Spark submit command; arguments to spark-submit may optionally be specified after -- ; GCS: run using Google cloud dataproc; commands after the -- will be passed to dataproc; --cluster <your-cluster> must be specified after the --; spark properties and some common spark-submit parameters will be translated ; to dataproc equivalents. --dry-run may be specified to output the generated command line without running it; --java-options 'OPTION1[ OPTION2=Y ... ]' optional - pass the given string of options to the ; java JVM at runtime. ; Java options MUST be passed inside a single string with space-separated values. --debug-port <number> sets up a Java VM debug agent to listen to debugger connections on a; particular port number. This in turn will add the necessary java VM arguments; so that you don't need to explicitly indicate these using --java-options.; --debug-suspend sets the Java VM debug agent up so that the run get immediatelly suspended; waiting for a debugger to connect. By default the port number is 5005 but; can be customized using --debug-port. But when i run other commands with gatk like gatk --list, i got this error :; gatk --list; Using GATK jar /home/ameni/Documents/pharmacogenomics/gatk-4.4.0.0/gatk-package-4.4.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/ameni/Documents/pharmacogenomics/gatk-4.4.0.0/gatk-package-4.4.0.0-local.jar --help; Error: LinkageError occurred while loading main class org.broadinstitute.hellbender.Main; 	java.lang.UnsupportedClassVersionError: org/broadinstitute/hellbender/Main has been compiled by a more recent version of the Java Runtime (class file version 61.0), this version of the Java Runtime only recognizes class file versions up to 55.0",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8280
https://github.com/broadinstitute/gatk/pull/8282:254,Deployability,integrat,integration,254,The first step of taking this code from the Hail team is just pasting it into our repo. Successful run:; https://job-manager.dsde-prod.broadinstitute.org/jobs/49d62f48-2dee-417c-aa65-411cbe47be17. GvsQuickstartHailIntegration--we remove the whl from the integration test---sure seems like we wont need one going forward!. Another ticket will be made for these next steps:; Likely this will need to end up in our docker image and the WDL that creates the Avro files can make a version of the input for this scripts instead; Next we will want to remove the Tranches calculations and instead of that value passed in as yet another parameter; Phasing and dropping GQ0,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8282
https://github.com/broadinstitute/gatk/pull/8282:254,Integrability,integrat,integration,254,The first step of taking this code from the Hail team is just pasting it into our repo. Successful run:; https://job-manager.dsde-prod.broadinstitute.org/jobs/49d62f48-2dee-417c-aa65-411cbe47be17. GvsQuickstartHailIntegration--we remove the whl from the integration test---sure seems like we wont need one going forward!. Another ticket will be made for these next steps:; Likely this will need to end up in our docker image and the WDL that creates the Avro files can make a version of the input for this scripts instead; Next we will want to remove the Tranches calculations and instead of that value passed in as yet another parameter; Phasing and dropping GQ0,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8282
https://github.com/broadinstitute/gatk/pull/8282:266,Testability,test,test---sure,266,The first step of taking this code from the Hail team is just pasting it into our repo. Successful run:; https://job-manager.dsde-prod.broadinstitute.org/jobs/49d62f48-2dee-417c-aa65-411cbe47be17. GvsQuickstartHailIntegration--we remove the whl from the integration test---sure seems like we wont need one going forward!. Another ticket will be made for these next steps:; Likely this will need to end up in our docker image and the WDL that creates the Avro files can make a version of the input for this scripts instead; Next we will want to remove the Tranches calculations and instead of that value passed in as yet another parameter; Phasing and dropping GQ0,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8282
https://github.com/broadinstitute/gatk/issues/8287:312,Testability,test,test,312,"When I tried to run `Markduplicate` from a cram file, some of the tags were missing in the output file. If I converted the cram into bam, and run the same command, the output records were identical in tags. . ```bash; java -jar ~/tools/gatk-4.2.5.0/gatk-package-4.2.5.0-local.jar MarkDuplicates -R ./spike.fa -I test.cram -O out.bam -M out.PCR_duplicates; ```. test file:; [test.tar.gz](https://github.com/broadinstitute/gatk/files/11217049/test.tar.gz)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8287
https://github.com/broadinstitute/gatk/issues/8287:361,Testability,test,test,361,"When I tried to run `Markduplicate` from a cram file, some of the tags were missing in the output file. If I converted the cram into bam, and run the same command, the output records were identical in tags. . ```bash; java -jar ~/tools/gatk-4.2.5.0/gatk-package-4.2.5.0-local.jar MarkDuplicates -R ./spike.fa -I test.cram -O out.bam -M out.PCR_duplicates; ```. test file:; [test.tar.gz](https://github.com/broadinstitute/gatk/files/11217049/test.tar.gz)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8287
https://github.com/broadinstitute/gatk/issues/8287:374,Testability,test,test,374,"When I tried to run `Markduplicate` from a cram file, some of the tags were missing in the output file. If I converted the cram into bam, and run the same command, the output records were identical in tags. . ```bash; java -jar ~/tools/gatk-4.2.5.0/gatk-package-4.2.5.0-local.jar MarkDuplicates -R ./spike.fa -I test.cram -O out.bam -M out.PCR_duplicates; ```. test file:; [test.tar.gz](https://github.com/broadinstitute/gatk/files/11217049/test.tar.gz)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8287
https://github.com/broadinstitute/gatk/issues/8287:441,Testability,test,test,441,"When I tried to run `Markduplicate` from a cram file, some of the tags were missing in the output file. If I converted the cram into bam, and run the same command, the output records were identical in tags. . ```bash; java -jar ~/tools/gatk-4.2.5.0/gatk-package-4.2.5.0-local.jar MarkDuplicates -R ./spike.fa -I test.cram -O out.bam -M out.PCR_duplicates; ```. test file:; [test.tar.gz](https://github.com/broadinstitute/gatk/files/11217049/test.tar.gz)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8287
https://github.com/broadinstitute/gatk/pull/8289:203,Testability,Test,Test,203,"TBH I'm not sure what went wrong with the previous image, I just re-baked a new one with the current version of gcloud-sdk and that seems to be working (or at least gsutil does not immediately explode). Test run here https://job-manager.dsde-prod.broadinstitute.org/jobs/016779a9-62d0-4195-9598-5f0eee559d99",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8289
https://github.com/broadinstitute/gatk/issues/8290:131,Deployability,release,release,131,## Bug Report. ### Affected tool(s) or class(es). `cnv_germline_cohort_workflow.wdl`. ### Affected version(s); - [x] Latest public release version [4.4.0.0]; - [x] Latest master branch as of [2023-04-14]. ### Description . cnv_germline_cohort_workflow.wdl currently outputs the following ; https://github.com/broadinstitute/gatk/blob/0374937bd7b152ecf1c2c922989371fcccedf184/scripts/cnv_wdl/germline/cnv_germline_cohort_workflow.wdl#L405. whereas the task that generates the corresponding result outputs `String`; https://github.com/broadinstitute/gatk/blob/0374937bd7b152ecf1c2c922989371fcccedf184/scripts/cnv_wdl/cnv_common_tasks.wdl#L297. #### Expected behavior. `cnv_germline_cohort_workflow.wdl` should output `Array[String]`.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8290
https://github.com/broadinstitute/gatk/pull/8291:12,Safety,sanity check,sanity checks,12,"tl;dr Added sanity checks, a smoke test, and fixed sample invocations in Docker build scripts. Last month I built a bad base image when I was unsuccessfully trying to combine GCP and Azure CLIs in one image. What's worse is I seem to have tagged this broken image with a tag previously used for a good version of the image. These changes harden the Docker image build script to refuse to write over an existing tag and execute a smoke test against the image before pushing to GCR. While I was in there I also fixed the sample invocations that we saw during mobbing did not work on all versions of `date`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8291
https://github.com/broadinstitute/gatk/pull/8291:35,Testability,test,test,35,"tl;dr Added sanity checks, a smoke test, and fixed sample invocations in Docker build scripts. Last month I built a bad base image when I was unsuccessfully trying to combine GCP and Azure CLIs in one image. What's worse is I seem to have tagged this broken image with a tag previously used for a good version of the image. These changes harden the Docker image build script to refuse to write over an existing tag and execute a smoke test against the image before pushing to GCR. While I was in there I also fixed the sample invocations that we saw during mobbing did not work on all versions of `date`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8291
https://github.com/broadinstitute/gatk/pull/8291:435,Testability,test,test,435,"tl;dr Added sanity checks, a smoke test, and fixed sample invocations in Docker build scripts. Last month I built a bad base image when I was unsuccessfully trying to combine GCP and Azure CLIs in one image. What's worse is I seem to have tagged this broken image with a tag previously used for a good version of the image. These changes harden the Docker image build script to refuse to write over an existing tag and execute a smoke test against the image before pushing to GCR. While I was in there I also fixed the sample invocations that we saw during mobbing did not work on all versions of `date`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8291
https://github.com/broadinstitute/gatk/pull/8292:1622,Usability,clear,clearly,1622,"0452602	GT:DP:GQ:MIN_DP:PL	0/0:147:99:146:0,120,1800; chr21	10452603	.	T	C,<NON_REF>	255.64	.	BaseQRankSum=-0.276;DP=156;ExcessHet=0.0000;MLEAC=1,0;MLEAF=0.500,0.00;MQRankSum=-7.712;RAW_MQandDP=487874,156;ReadPosRankSum=1.061	GT:AD:DP:GQ:PGT:PID:PL:PS:SB	0|1:136,16,0:152:99:0|1:10452603_T_C:263,0,5564,675,5628,6356:10452603:59,77,0,16; chr21	10452604	.	G	*,A,<NON_REF>	255.64	.	BaseQRankSum=-1.417;DP=158;ExcessHet=0.0000;MLEAC=0,1,0;MLEAF=0.00,0.500,0.00;MQRankSum=-7.712;RAW_MQandDP=495074,158;ReadPosRankSum=1.037	GT:AD:DP:GQ:PGT:PID:PL:PS:SB	0|2:136,0,16,0:152:99:0|1:10452603_T_C:263,681,6518,0,5638,5564,681,6462,5634,6446:10452603:59,77,0,16; chr21	10452605	.	C	*,T,<NON_REF>	191.64	.	BaseQRankSum=-0.601;DP=161;ExcessHet=0.0000;MLEAC=0,1,0;MLEAF=0.00,0.500,0.00;MQRankSum=1.656;RAW_MQandDP=505874,161;ReadPosRankSum=-0.404	GT:AD:DP:GQ:PL:SB	0/2:131,0,19,0:150:99:199,665,6662,0,4918,4665,652,6264,4868,6100:50,81,9,10; chr21	10452606	.	C	<NON_REF>	.	.	END=10452611	GT:DP:GQ:MIN_DP:PL	0/0:154:99:150:0,120,1800; chr21	10452612	.	G	A,<NON_REF>	0	.	BaseQRankSum=4.613;DP=166;ExcessHet=0.0000;MLEAC=0,0;MLEAF=0.00,0.00;MQRankSum=-7.647;RAW_MQandDP=530720,166;ReadPosRankSum=1.518	GT:AD:DP:GQ:PL:SB	0/0:146,13,0:159:12:0,12,4420,474,4719,5968:64,82,0,13. `chr21	10452604	.	G	*,A,<NON_REF>` and `chr21	10452605	.	C	*,T,<NON_REF>` are not covered any deletion clearly but treated as spanning deletion. Command is following.; `$java -jar $gatk HaplotypeCaller \; --reference Homo_sapiens_assembly38.fasta \; --input CNR0028194.gatk_best_practice.GRCh38.chr21_10451605-10453605.bam \; --output out.vcf \; --pcr-indel-model NONE \; -ERC GVCF \; -L chr21:10451605-10453605; `; Reference is GATK Bundle hg38.; Input bam is placed in https://pezycomputing-my.sharepoint.com/:f:/g/personal/sakai_pezy_co_jp/EuiUh7J-eCpOmA_Xkf3cOEwByf3lqKpm4N4FdYy7B5FCJA?e=dlZwDl. In recordDeletion method, NON_REF Allele is used for calculation of delesionSize as zero length allele.; I modified it to skip when NON_REF.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8292
https://github.com/broadinstitute/gatk/pull/8293:50,Testability,log,logical,50,"Mostly Markdown with some code snippets. The next logical step to sussing out an Azure storage solution is probably WDLization / filter creation, but that is work for another day. TODO: Figure out how much this cost.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8293
https://github.com/broadinstitute/gatk/issues/8296:112,Deployability,update,update,112,"The latest version of the funcotator data sources dates to mid-2020. This is almost three years now without any update of the following sources:; *gencode (v34, now 43); *dbsnp; *COSMIC; *clinvar. Therefore, a new bundle should be provided to stay up-to-date again.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8296
https://github.com/broadinstitute/gatk/pull/8300:35,Testability,log,logic,35,The WDL GvsAoUReblockGVCF contains logic specific to AoU and as such does not belong in gatk (or warp). It is being added to the broadinstitute/hops-aou-drc repo. Jira ticket: https://broadworkbench.atlassian.net/browse/DFE-761; PR in hops-aou-drc: https://github.com/broadinstitute/hops-aou-drc/pull/148,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8300
https://github.com/broadinstitute/gatk/pull/8301:119,Performance,load,load,119,"Successful run here:; https://job-manager.dsde-prod.broadinstitute.org/jobs/41b11f26-9d55-45ad-b593-ddb5b8c78184. Bulk load data here:; https://console.cloud.google.com/bigquery?project=spec-ops-aou&ws=!1m25!1m4!4m3!1sgvs-internal!2sgg_quickstart1!3sgg-quickstart1_vat_12!1m4!1m3!1sspec-ops-aou!2sbquxjob_11f0b098_187d879575c!3sUS!1m4!4m3!1sspec-ops-aou!2sgg_quickstart!3svet_001!1m4!1m3!1sspec-ops-aou!2sbquxjob_aa2c57_187d8b92d34!3sUS!1m4!4m3!1sgvs-internal!2src_ingest_bulk_test_useability!3ssample_load_status. All that now needs to be input (assuming that we guess the additional parameters correctly). <img width=""914"" alt=""Screen Shot 2023-05-05 at 11 30 33 AM"" src=""https://user-images.githubusercontent.com/6863459/236502025-8f785131-848b-4e4c-a88e-1a143eba1566.png"">. this needs a new docker image and an interactive rebase lol. documentation attempt:; https://docs.google.com/document/d/1fxu0EnNp7ie42BtFQsSSN6QUESUiDl3fm8F5AnNkKhw/edit#heading=h.s5k25ipaom03",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8301
https://github.com/broadinstitute/gatk/issues/8302:107,Availability,error,error,107,"Hello, GATK team. When I genotyped 309 samples using the GenotypeGVCFs module, there was a memory overflow error (either using -Xmx20G/100G/800G with 1TB of physical memory). The same memory error can occur in CombineGVCFs, so I select GenomicsDBImport for genome-merging. This is the code when using GenomicsDBImport, completed successfully.; ```; gatk GenomicsDBImport \; -R $path1/ref/genome.fa --java-options ""-Xmx100g -Xms80g"" \; $(for i in $(ls $path1/sortbam/2/*.g.vcf.gz); do echo ""--variant $i""; done) \; $(for i in $(ls $path1/sortbam/4/*.g.vcf.gz); do echo ""--variant $i""; done) \; $(for i in $(ls $path1/sortbam/6/*.g.vcf.gz); do echo ""--variant $i""; done) \; --genomicsdb-workspace-path $path1/DBI \; --tmp-dir $path1/NOHUP/tmp --intervals $path1/chr.list; ```; But when I run the following **GenotypeGVCFs code**: ; ```; gatk --java-options '-Xmx800G -DGATK_STACKTRACE_ON_USER_EXCEPTION=true' GenotypeGVCFs \; -R $path1/ref/genome.fa -V gendb://$path1/DBI \; -O $path1/sortbam/combDBI.vcf.gz --tmp-dir $path1/NOHUP/tmp. ```; **It warns**: [TileDB::ReadState] Error: Cannot read tile from file; Memory map error. ```; 21:02:06.717 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/wtc/software/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Apr 29, 2023 9:02:06 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 21:02:06.864 INFO GenotypeGVCFs - ------------------------------------------------------------; 21:02:06.864 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.0.0; 21:02:06.864 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 21:02:06.864 INFO GenotypeGVCFs - Executing as wtc@PC10-7742 on Linux v4.4.0-19041-Microsoft amd64; 21:02:06.864 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_121-b15; 21:02:06.865 IN",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8302
https://github.com/broadinstitute/gatk/issues/8302:191,Availability,error,error,191,"Hello, GATK team. When I genotyped 309 samples using the GenotypeGVCFs module, there was a memory overflow error (either using -Xmx20G/100G/800G with 1TB of physical memory). The same memory error can occur in CombineGVCFs, so I select GenomicsDBImport for genome-merging. This is the code when using GenomicsDBImport, completed successfully.; ```; gatk GenomicsDBImport \; -R $path1/ref/genome.fa --java-options ""-Xmx100g -Xms80g"" \; $(for i in $(ls $path1/sortbam/2/*.g.vcf.gz); do echo ""--variant $i""; done) \; $(for i in $(ls $path1/sortbam/4/*.g.vcf.gz); do echo ""--variant $i""; done) \; $(for i in $(ls $path1/sortbam/6/*.g.vcf.gz); do echo ""--variant $i""; done) \; --genomicsdb-workspace-path $path1/DBI \; --tmp-dir $path1/NOHUP/tmp --intervals $path1/chr.list; ```; But when I run the following **GenotypeGVCFs code**: ; ```; gatk --java-options '-Xmx800G -DGATK_STACKTRACE_ON_USER_EXCEPTION=true' GenotypeGVCFs \; -R $path1/ref/genome.fa -V gendb://$path1/DBI \; -O $path1/sortbam/combDBI.vcf.gz --tmp-dir $path1/NOHUP/tmp. ```; **It warns**: [TileDB::ReadState] Error: Cannot read tile from file; Memory map error. ```; 21:02:06.717 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/wtc/software/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Apr 29, 2023 9:02:06 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 21:02:06.864 INFO GenotypeGVCFs - ------------------------------------------------------------; 21:02:06.864 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.0.0; 21:02:06.864 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 21:02:06.864 INFO GenotypeGVCFs - Executing as wtc@PC10-7742 on Linux v4.4.0-19041-Microsoft amd64; 21:02:06.864 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_121-b15; 21:02:06.865 IN",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8302
https://github.com/broadinstitute/gatk/issues/8302:484,Availability,echo,echo,484,"Hello, GATK team. When I genotyped 309 samples using the GenotypeGVCFs module, there was a memory overflow error (either using -Xmx20G/100G/800G with 1TB of physical memory). The same memory error can occur in CombineGVCFs, so I select GenomicsDBImport for genome-merging. This is the code when using GenomicsDBImport, completed successfully.; ```; gatk GenomicsDBImport \; -R $path1/ref/genome.fa --java-options ""-Xmx100g -Xms80g"" \; $(for i in $(ls $path1/sortbam/2/*.g.vcf.gz); do echo ""--variant $i""; done) \; $(for i in $(ls $path1/sortbam/4/*.g.vcf.gz); do echo ""--variant $i""; done) \; $(for i in $(ls $path1/sortbam/6/*.g.vcf.gz); do echo ""--variant $i""; done) \; --genomicsdb-workspace-path $path1/DBI \; --tmp-dir $path1/NOHUP/tmp --intervals $path1/chr.list; ```; But when I run the following **GenotypeGVCFs code**: ; ```; gatk --java-options '-Xmx800G -DGATK_STACKTRACE_ON_USER_EXCEPTION=true' GenotypeGVCFs \; -R $path1/ref/genome.fa -V gendb://$path1/DBI \; -O $path1/sortbam/combDBI.vcf.gz --tmp-dir $path1/NOHUP/tmp. ```; **It warns**: [TileDB::ReadState] Error: Cannot read tile from file; Memory map error. ```; 21:02:06.717 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/wtc/software/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Apr 29, 2023 9:02:06 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 21:02:06.864 INFO GenotypeGVCFs - ------------------------------------------------------------; 21:02:06.864 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.0.0; 21:02:06.864 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 21:02:06.864 INFO GenotypeGVCFs - Executing as wtc@PC10-7742 on Linux v4.4.0-19041-Microsoft amd64; 21:02:06.864 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_121-b15; 21:02:06.865 IN",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8302
https://github.com/broadinstitute/gatk/issues/8302:563,Availability,echo,echo,563,"Hello, GATK team. When I genotyped 309 samples using the GenotypeGVCFs module, there was a memory overflow error (either using -Xmx20G/100G/800G with 1TB of physical memory). The same memory error can occur in CombineGVCFs, so I select GenomicsDBImport for genome-merging. This is the code when using GenomicsDBImport, completed successfully.; ```; gatk GenomicsDBImport \; -R $path1/ref/genome.fa --java-options ""-Xmx100g -Xms80g"" \; $(for i in $(ls $path1/sortbam/2/*.g.vcf.gz); do echo ""--variant $i""; done) \; $(for i in $(ls $path1/sortbam/4/*.g.vcf.gz); do echo ""--variant $i""; done) \; $(for i in $(ls $path1/sortbam/6/*.g.vcf.gz); do echo ""--variant $i""; done) \; --genomicsdb-workspace-path $path1/DBI \; --tmp-dir $path1/NOHUP/tmp --intervals $path1/chr.list; ```; But when I run the following **GenotypeGVCFs code**: ; ```; gatk --java-options '-Xmx800G -DGATK_STACKTRACE_ON_USER_EXCEPTION=true' GenotypeGVCFs \; -R $path1/ref/genome.fa -V gendb://$path1/DBI \; -O $path1/sortbam/combDBI.vcf.gz --tmp-dir $path1/NOHUP/tmp. ```; **It warns**: [TileDB::ReadState] Error: Cannot read tile from file; Memory map error. ```; 21:02:06.717 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/wtc/software/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Apr 29, 2023 9:02:06 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 21:02:06.864 INFO GenotypeGVCFs - ------------------------------------------------------------; 21:02:06.864 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.0.0; 21:02:06.864 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 21:02:06.864 INFO GenotypeGVCFs - Executing as wtc@PC10-7742 on Linux v4.4.0-19041-Microsoft amd64; 21:02:06.864 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_121-b15; 21:02:06.865 IN",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8302
https://github.com/broadinstitute/gatk/issues/8302:642,Availability,echo,echo,642,"Hello, GATK team. When I genotyped 309 samples using the GenotypeGVCFs module, there was a memory overflow error (either using -Xmx20G/100G/800G with 1TB of physical memory). The same memory error can occur in CombineGVCFs, so I select GenomicsDBImport for genome-merging. This is the code when using GenomicsDBImport, completed successfully.; ```; gatk GenomicsDBImport \; -R $path1/ref/genome.fa --java-options ""-Xmx100g -Xms80g"" \; $(for i in $(ls $path1/sortbam/2/*.g.vcf.gz); do echo ""--variant $i""; done) \; $(for i in $(ls $path1/sortbam/4/*.g.vcf.gz); do echo ""--variant $i""; done) \; $(for i in $(ls $path1/sortbam/6/*.g.vcf.gz); do echo ""--variant $i""; done) \; --genomicsdb-workspace-path $path1/DBI \; --tmp-dir $path1/NOHUP/tmp --intervals $path1/chr.list; ```; But when I run the following **GenotypeGVCFs code**: ; ```; gatk --java-options '-Xmx800G -DGATK_STACKTRACE_ON_USER_EXCEPTION=true' GenotypeGVCFs \; -R $path1/ref/genome.fa -V gendb://$path1/DBI \; -O $path1/sortbam/combDBI.vcf.gz --tmp-dir $path1/NOHUP/tmp. ```; **It warns**: [TileDB::ReadState] Error: Cannot read tile from file; Memory map error. ```; 21:02:06.717 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/wtc/software/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Apr 29, 2023 9:02:06 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 21:02:06.864 INFO GenotypeGVCFs - ------------------------------------------------------------; 21:02:06.864 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.0.0; 21:02:06.864 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 21:02:06.864 INFO GenotypeGVCFs - Executing as wtc@PC10-7742 on Linux v4.4.0-19041-Microsoft amd64; 21:02:06.864 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_121-b15; 21:02:06.865 IN",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8302
https://github.com/broadinstitute/gatk/issues/8302:1073,Availability,Error,Error,1073,"there was a memory overflow error (either using -Xmx20G/100G/800G with 1TB of physical memory). The same memory error can occur in CombineGVCFs, so I select GenomicsDBImport for genome-merging. This is the code when using GenomicsDBImport, completed successfully.; ```; gatk GenomicsDBImport \; -R $path1/ref/genome.fa --java-options ""-Xmx100g -Xms80g"" \; $(for i in $(ls $path1/sortbam/2/*.g.vcf.gz); do echo ""--variant $i""; done) \; $(for i in $(ls $path1/sortbam/4/*.g.vcf.gz); do echo ""--variant $i""; done) \; $(for i in $(ls $path1/sortbam/6/*.g.vcf.gz); do echo ""--variant $i""; done) \; --genomicsdb-workspace-path $path1/DBI \; --tmp-dir $path1/NOHUP/tmp --intervals $path1/chr.list; ```; But when I run the following **GenotypeGVCFs code**: ; ```; gatk --java-options '-Xmx800G -DGATK_STACKTRACE_ON_USER_EXCEPTION=true' GenotypeGVCFs \; -R $path1/ref/genome.fa -V gendb://$path1/DBI \; -O $path1/sortbam/combDBI.vcf.gz --tmp-dir $path1/NOHUP/tmp. ```; **It warns**: [TileDB::ReadState] Error: Cannot read tile from file; Memory map error. ```; 21:02:06.717 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/wtc/software/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Apr 29, 2023 9:02:06 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 21:02:06.864 INFO GenotypeGVCFs - ------------------------------------------------------------; 21:02:06.864 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.0.0; 21:02:06.864 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 21:02:06.864 INFO GenotypeGVCFs - Executing as wtc@PC10-7742 on Linux v4.4.0-19041-Microsoft amd64; 21:02:06.864 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_121-b15; 21:02:06.865 INFO GenotypeGVCFs - Start Date/Time: April 29, 2023 9:02:06 PM CST; 21:02:06.86",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8302
https://github.com/broadinstitute/gatk/issues/8302:1119,Availability,error,error,1119,"there was a memory overflow error (either using -Xmx20G/100G/800G with 1TB of physical memory). The same memory error can occur in CombineGVCFs, so I select GenomicsDBImport for genome-merging. This is the code when using GenomicsDBImport, completed successfully.; ```; gatk GenomicsDBImport \; -R $path1/ref/genome.fa --java-options ""-Xmx100g -Xms80g"" \; $(for i in $(ls $path1/sortbam/2/*.g.vcf.gz); do echo ""--variant $i""; done) \; $(for i in $(ls $path1/sortbam/4/*.g.vcf.gz); do echo ""--variant $i""; done) \; $(for i in $(ls $path1/sortbam/6/*.g.vcf.gz); do echo ""--variant $i""; done) \; --genomicsdb-workspace-path $path1/DBI \; --tmp-dir $path1/NOHUP/tmp --intervals $path1/chr.list; ```; But when I run the following **GenotypeGVCFs code**: ; ```; gatk --java-options '-Xmx800G -DGATK_STACKTRACE_ON_USER_EXCEPTION=true' GenotypeGVCFs \; -R $path1/ref/genome.fa -V gendb://$path1/DBI \; -O $path1/sortbam/combDBI.vcf.gz --tmp-dir $path1/NOHUP/tmp. ```; **It warns**: [TileDB::ReadState] Error: Cannot read tile from file; Memory map error. ```; 21:02:06.717 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/wtc/software/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Apr 29, 2023 9:02:06 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 21:02:06.864 INFO GenotypeGVCFs - ------------------------------------------------------------; 21:02:06.864 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.0.0; 21:02:06.864 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 21:02:06.864 INFO GenotypeGVCFs - Executing as wtc@PC10-7742 on Linux v4.4.0-19041-Microsoft amd64; 21:02:06.864 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_121-b15; 21:02:06.865 INFO GenotypeGVCFs - Start Date/Time: April 29, 2023 9:02:06 PM CST; 21:02:06.86",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8302
https://github.com/broadinstitute/gatk/issues/8302:4348,Availability,Error,Error,4348,"records; 21:02:11.780 info NativeGenomicsDB - pid=19608 tid=19609 No valid combination operation found for INFO field MLEAC - the field will NOT be part of INFO fields in the generated VCF records; 21:02:11.780 info NativeGenomicsDB - pid=19608 tid=19609 No valid combination operation found for INFO field MLEAF - the field will NOT be part of INFO fields in the generated VCF records; 21:02:17.065 INFO GenotypeGVCFs - Done initializing engine; 21:02:17.110 INFO ProgressMeter - Starting traversal; 21:02:17.111 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 21:02:45.390 INFO ProgressMeter - Chr1:369351 0.5 1000 2121.8; 21:03:16.132 INFO ProgressMeter - Chr1:505230 1.0 2000 2033.2; 21:03:29.421 INFO ProgressMeter - Chr1:575285 1.2 3000 2489.3; ... (continued for more than 1000 lines); 21:49:51.317 INFO ProgressMeter - Chr1:3713346 47.6 242000 5087.2; 21:50:06.596 INFO ProgressMeter - Chr1:3718941 47.8 244000 5102.0; [TileDB::ReadState] Error: Cannot read tile from file; Memory map error.; terminate called after throwing an instance of 'VariantStorageManagerException'; what(): VariantStorageManagerException exception : VariantArrayCellIterator increment failed; TileDB error message : [TileDB::ReadState] Error: Cannot read tile from file; Memory map error; Using GATK jar /home/wtc/software/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx800G -DGATK_STACKTRACE_ON_USER_EXCEPTION=true -jar /home/wtc/software/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar GenotypeGVCFs -R /mnt/d/Share/SYJ/liulan/ref/genome.fa -V gendb:///mnt/d/Share/SYJ/liulan/DBI -O /mnt/d/Share/SYJ/liulan/sortbam/combDBI.vcf.gz --tmp-dir /mnt/d/Share/SYJ/liulan/NOHUP/tmp; ```. I have tried to change -Xmx to 20G and 100G, etc. But all processes stop running at Variants Processed 244000.; Could you tell me",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8302
https://github.com/broadinstitute/gatk/issues/8302:4394,Availability,error,error,4394,"records; 21:02:11.780 info NativeGenomicsDB - pid=19608 tid=19609 No valid combination operation found for INFO field MLEAC - the field will NOT be part of INFO fields in the generated VCF records; 21:02:11.780 info NativeGenomicsDB - pid=19608 tid=19609 No valid combination operation found for INFO field MLEAF - the field will NOT be part of INFO fields in the generated VCF records; 21:02:17.065 INFO GenotypeGVCFs - Done initializing engine; 21:02:17.110 INFO ProgressMeter - Starting traversal; 21:02:17.111 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 21:02:45.390 INFO ProgressMeter - Chr1:369351 0.5 1000 2121.8; 21:03:16.132 INFO ProgressMeter - Chr1:505230 1.0 2000 2033.2; 21:03:29.421 INFO ProgressMeter - Chr1:575285 1.2 3000 2489.3; ... (continued for more than 1000 lines); 21:49:51.317 INFO ProgressMeter - Chr1:3713346 47.6 242000 5087.2; 21:50:06.596 INFO ProgressMeter - Chr1:3718941 47.8 244000 5102.0; [TileDB::ReadState] Error: Cannot read tile from file; Memory map error.; terminate called after throwing an instance of 'VariantStorageManagerException'; what(): VariantStorageManagerException exception : VariantArrayCellIterator increment failed; TileDB error message : [TileDB::ReadState] Error: Cannot read tile from file; Memory map error; Using GATK jar /home/wtc/software/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx800G -DGATK_STACKTRACE_ON_USER_EXCEPTION=true -jar /home/wtc/software/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar GenotypeGVCFs -R /mnt/d/Share/SYJ/liulan/ref/genome.fa -V gendb:///mnt/d/Share/SYJ/liulan/DBI -O /mnt/d/Share/SYJ/liulan/sortbam/combDBI.vcf.gz --tmp-dir /mnt/d/Share/SYJ/liulan/NOHUP/tmp; ```. I have tried to change -Xmx to 20G and 100G, etc. But all processes stop running at Variants Processed 244000.; Could you tell me",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8302
https://github.com/broadinstitute/gatk/issues/8302:4584,Availability,error,error,4584,"info NativeGenomicsDB - pid=19608 tid=19609 No valid combination operation found for INFO field MLEAC - the field will NOT be part of INFO fields in the generated VCF records; 21:02:11.780 info NativeGenomicsDB - pid=19608 tid=19609 No valid combination operation found for INFO field MLEAF - the field will NOT be part of INFO fields in the generated VCF records; 21:02:17.065 INFO GenotypeGVCFs - Done initializing engine; 21:02:17.110 INFO ProgressMeter - Starting traversal; 21:02:17.111 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 21:02:45.390 INFO ProgressMeter - Chr1:369351 0.5 1000 2121.8; 21:03:16.132 INFO ProgressMeter - Chr1:505230 1.0 2000 2033.2; 21:03:29.421 INFO ProgressMeter - Chr1:575285 1.2 3000 2489.3; ... (continued for more than 1000 lines); 21:49:51.317 INFO ProgressMeter - Chr1:3713346 47.6 242000 5087.2; 21:50:06.596 INFO ProgressMeter - Chr1:3718941 47.8 244000 5102.0; [TileDB::ReadState] Error: Cannot read tile from file; Memory map error.; terminate called after throwing an instance of 'VariantStorageManagerException'; what(): VariantStorageManagerException exception : VariantArrayCellIterator increment failed; TileDB error message : [TileDB::ReadState] Error: Cannot read tile from file; Memory map error; Using GATK jar /home/wtc/software/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx800G -DGATK_STACKTRACE_ON_USER_EXCEPTION=true -jar /home/wtc/software/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar GenotypeGVCFs -R /mnt/d/Share/SYJ/liulan/ref/genome.fa -V gendb:///mnt/d/Share/SYJ/liulan/DBI -O /mnt/d/Share/SYJ/liulan/sortbam/combDBI.vcf.gz --tmp-dir /mnt/d/Share/SYJ/liulan/NOHUP/tmp; ```. I have tried to change -Xmx to 20G and 100G, etc. But all processes stop running at Variants Processed 244000.; Could you tell me what the problem is?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8302
https://github.com/broadinstitute/gatk/issues/8302:4620,Availability,Error,Error,4620,"info NativeGenomicsDB - pid=19608 tid=19609 No valid combination operation found for INFO field MLEAC - the field will NOT be part of INFO fields in the generated VCF records; 21:02:11.780 info NativeGenomicsDB - pid=19608 tid=19609 No valid combination operation found for INFO field MLEAF - the field will NOT be part of INFO fields in the generated VCF records; 21:02:17.065 INFO GenotypeGVCFs - Done initializing engine; 21:02:17.110 INFO ProgressMeter - Starting traversal; 21:02:17.111 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 21:02:45.390 INFO ProgressMeter - Chr1:369351 0.5 1000 2121.8; 21:03:16.132 INFO ProgressMeter - Chr1:505230 1.0 2000 2033.2; 21:03:29.421 INFO ProgressMeter - Chr1:575285 1.2 3000 2489.3; ... (continued for more than 1000 lines); 21:49:51.317 INFO ProgressMeter - Chr1:3713346 47.6 242000 5087.2; 21:50:06.596 INFO ProgressMeter - Chr1:3718941 47.8 244000 5102.0; [TileDB::ReadState] Error: Cannot read tile from file; Memory map error.; terminate called after throwing an instance of 'VariantStorageManagerException'; what(): VariantStorageManagerException exception : VariantArrayCellIterator increment failed; TileDB error message : [TileDB::ReadState] Error: Cannot read tile from file; Memory map error; Using GATK jar /home/wtc/software/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx800G -DGATK_STACKTRACE_ON_USER_EXCEPTION=true -jar /home/wtc/software/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar GenotypeGVCFs -R /mnt/d/Share/SYJ/liulan/ref/genome.fa -V gendb:///mnt/d/Share/SYJ/liulan/DBI -O /mnt/d/Share/SYJ/liulan/sortbam/combDBI.vcf.gz --tmp-dir /mnt/d/Share/SYJ/liulan/NOHUP/tmp; ```. I have tried to change -Xmx to 20G and 100G, etc. But all processes stop running at Variants Processed 244000.; Could you tell me what the problem is?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8302
https://github.com/broadinstitute/gatk/issues/8302:4666,Availability,error,error,4666,"info NativeGenomicsDB - pid=19608 tid=19609 No valid combination operation found for INFO field MLEAC - the field will NOT be part of INFO fields in the generated VCF records; 21:02:11.780 info NativeGenomicsDB - pid=19608 tid=19609 No valid combination operation found for INFO field MLEAF - the field will NOT be part of INFO fields in the generated VCF records; 21:02:17.065 INFO GenotypeGVCFs - Done initializing engine; 21:02:17.110 INFO ProgressMeter - Starting traversal; 21:02:17.111 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 21:02:45.390 INFO ProgressMeter - Chr1:369351 0.5 1000 2121.8; 21:03:16.132 INFO ProgressMeter - Chr1:505230 1.0 2000 2033.2; 21:03:29.421 INFO ProgressMeter - Chr1:575285 1.2 3000 2489.3; ... (continued for more than 1000 lines); 21:49:51.317 INFO ProgressMeter - Chr1:3713346 47.6 242000 5087.2; 21:50:06.596 INFO ProgressMeter - Chr1:3718941 47.8 244000 5102.0; [TileDB::ReadState] Error: Cannot read tile from file; Memory map error.; terminate called after throwing an instance of 'VariantStorageManagerException'; what(): VariantStorageManagerException exception : VariantArrayCellIterator increment failed; TileDB error message : [TileDB::ReadState] Error: Cannot read tile from file; Memory map error; Using GATK jar /home/wtc/software/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx800G -DGATK_STACKTRACE_ON_USER_EXCEPTION=true -jar /home/wtc/software/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar GenotypeGVCFs -R /mnt/d/Share/SYJ/liulan/ref/genome.fa -V gendb:///mnt/d/Share/SYJ/liulan/DBI -O /mnt/d/Share/SYJ/liulan/sortbam/combDBI.vcf.gz --tmp-dir /mnt/d/Share/SYJ/liulan/NOHUP/tmp; ```. I have tried to change -Xmx to 20G and 100G, etc. But all processes stop running at Variants Processed 244000.; Could you tell me what the problem is?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8302
https://github.com/broadinstitute/gatk/issues/8302:4590,Integrability,message,message,4590,"info NativeGenomicsDB - pid=19608 tid=19609 No valid combination operation found for INFO field MLEAC - the field will NOT be part of INFO fields in the generated VCF records; 21:02:11.780 info NativeGenomicsDB - pid=19608 tid=19609 No valid combination operation found for INFO field MLEAF - the field will NOT be part of INFO fields in the generated VCF records; 21:02:17.065 INFO GenotypeGVCFs - Done initializing engine; 21:02:17.110 INFO ProgressMeter - Starting traversal; 21:02:17.111 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 21:02:45.390 INFO ProgressMeter - Chr1:369351 0.5 1000 2121.8; 21:03:16.132 INFO ProgressMeter - Chr1:505230 1.0 2000 2033.2; 21:03:29.421 INFO ProgressMeter - Chr1:575285 1.2 3000 2489.3; ... (continued for more than 1000 lines); 21:49:51.317 INFO ProgressMeter - Chr1:3713346 47.6 242000 5087.2; 21:50:06.596 INFO ProgressMeter - Chr1:3718941 47.8 244000 5102.0; [TileDB::ReadState] Error: Cannot read tile from file; Memory map error.; terminate called after throwing an instance of 'VariantStorageManagerException'; what(): VariantStorageManagerException exception : VariantArrayCellIterator increment failed; TileDB error message : [TileDB::ReadState] Error: Cannot read tile from file; Memory map error; Using GATK jar /home/wtc/software/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx800G -DGATK_STACKTRACE_ON_USER_EXCEPTION=true -jar /home/wtc/software/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar GenotypeGVCFs -R /mnt/d/Share/SYJ/liulan/ref/genome.fa -V gendb:///mnt/d/Share/SYJ/liulan/DBI -O /mnt/d/Share/SYJ/liulan/sortbam/combDBI.vcf.gz --tmp-dir /mnt/d/Share/SYJ/liulan/NOHUP/tmp; ```. I have tried to change -Xmx to 20G and 100G, etc. But all processes stop running at Variants Processed 244000.; Could you tell me what the problem is?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8302
https://github.com/broadinstitute/gatk/issues/8302:1171,Performance,Load,Loading,1171,"ory). The same memory error can occur in CombineGVCFs, so I select GenomicsDBImport for genome-merging. This is the code when using GenomicsDBImport, completed successfully.; ```; gatk GenomicsDBImport \; -R $path1/ref/genome.fa --java-options ""-Xmx100g -Xms80g"" \; $(for i in $(ls $path1/sortbam/2/*.g.vcf.gz); do echo ""--variant $i""; done) \; $(for i in $(ls $path1/sortbam/4/*.g.vcf.gz); do echo ""--variant $i""; done) \; $(for i in $(ls $path1/sortbam/6/*.g.vcf.gz); do echo ""--variant $i""; done) \; --genomicsdb-workspace-path $path1/DBI \; --tmp-dir $path1/NOHUP/tmp --intervals $path1/chr.list; ```; But when I run the following **GenotypeGVCFs code**: ; ```; gatk --java-options '-Xmx800G -DGATK_STACKTRACE_ON_USER_EXCEPTION=true' GenotypeGVCFs \; -R $path1/ref/genome.fa -V gendb://$path1/DBI \; -O $path1/sortbam/combDBI.vcf.gz --tmp-dir $path1/NOHUP/tmp. ```; **It warns**: [TileDB::ReadState] Error: Cannot read tile from file; Memory map error. ```; 21:02:06.717 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/wtc/software/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Apr 29, 2023 9:02:06 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 21:02:06.864 INFO GenotypeGVCFs - ------------------------------------------------------------; 21:02:06.864 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.0.0; 21:02:06.864 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 21:02:06.864 INFO GenotypeGVCFs - Executing as wtc@PC10-7742 on Linux v4.4.0-19041-Microsoft amd64; 21:02:06.864 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_121-b15; 21:02:06.865 INFO GenotypeGVCFs - Start Date/Time: April 29, 2023 9:02:06 PM CST; 21:02:06.865 INFO GenotypeGVCFs - ------------------------------------------------------------; 21:02",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8302
https://github.com/broadinstitute/gatk/issues/8302:1452,Safety,detect,detect,1452,"(ls $path1/sortbam/2/*.g.vcf.gz); do echo ""--variant $i""; done) \; $(for i in $(ls $path1/sortbam/4/*.g.vcf.gz); do echo ""--variant $i""; done) \; $(for i in $(ls $path1/sortbam/6/*.g.vcf.gz); do echo ""--variant $i""; done) \; --genomicsdb-workspace-path $path1/DBI \; --tmp-dir $path1/NOHUP/tmp --intervals $path1/chr.list; ```; But when I run the following **GenotypeGVCFs code**: ; ```; gatk --java-options '-Xmx800G -DGATK_STACKTRACE_ON_USER_EXCEPTION=true' GenotypeGVCFs \; -R $path1/ref/genome.fa -V gendb://$path1/DBI \; -O $path1/sortbam/combDBI.vcf.gz --tmp-dir $path1/NOHUP/tmp. ```; **It warns**: [TileDB::ReadState] Error: Cannot read tile from file; Memory map error. ```; 21:02:06.717 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/wtc/software/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Apr 29, 2023 9:02:06 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 21:02:06.864 INFO GenotypeGVCFs - ------------------------------------------------------------; 21:02:06.864 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.0.0; 21:02:06.864 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 21:02:06.864 INFO GenotypeGVCFs - Executing as wtc@PC10-7742 on Linux v4.4.0-19041-Microsoft amd64; 21:02:06.864 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_121-b15; 21:02:06.865 INFO GenotypeGVCFs - Start Date/Time: April 29, 2023 9:02:06 PM CST; 21:02:06.865 INFO GenotypeGVCFs - ------------------------------------------------------------; 21:02:06.865 INFO GenotypeGVCFs - ------------------------------------------------------------; 21:02:06.865 INFO GenotypeGVCFs - HTSJDK Version: 2.24.0; 21:02:06.865 INFO GenotypeGVCFs - Picard Version: 2.25.0; 21:02:06.865 INFO GenotypeGVCFs - Built for Spark Version: 2.4.5; 21:02",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8302
https://github.com/broadinstitute/gatk/pull/8303:0,Deployability,Integrat,Integration,0,Integration run going [here](https://app.terra.bio/#workspaces/gvs-dev/mlc%20GVS%20Quickstart%20v3/job_history/4adcc972-86be-411e-bc95-a27c8a14de3b),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8303
https://github.com/broadinstitute/gatk/pull/8303:0,Integrability,Integrat,Integration,0,Integration run going [here](https://app.terra.bio/#workspaces/gvs-dev/mlc%20GVS%20Quickstart%20v3/job_history/4adcc972-86be-411e-bc95-a27c8a14de3b),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8303
https://github.com/broadinstitute/gatk/issues/8307:135,Deployability,release,release,135,"### Instructions. ## Bug Report. ### Affected tool(s) or class(es); MarkDuplicatesSpark. ### Affected version(s); - [v ] Latest public release version [4.4.0.0]. ### Description ; Since switching to 4.4.0.0 we are experiencing an increased memory consumption of MarkDuplicatesSpark. We see it on large BAM/CRAM files, not tested on small files. #### Steps to reproduce; This command: ; ```; java -Xmx190g -jar /usr/gitc/GATK_ultima.jar MarkDuplicatesSpark \; --spark-master local[24] \; --input 019242_old.ua.aln.bam \; --output 019242_old.aligned.sorted.duplicates_marked.bam \; --create-output-bam-index true \; --spark-verbosity WARN \; --verbosity WARNING \; --flowbased; ```; required 90GB memory on 4.3.0.0. The input BAM is large: 270GB; However on the 4.4.0.0 it requires >160GB RAM. #### Expected behavior; The memory requirement is not expected to change ; #### Actual behavior; Significantly increased memory requirement",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8307
https://github.com/broadinstitute/gatk/issues/8307:247,Energy Efficiency,consumption,consumption,247,"### Instructions. ## Bug Report. ### Affected tool(s) or class(es); MarkDuplicatesSpark. ### Affected version(s); - [v ] Latest public release version [4.4.0.0]. ### Description ; Since switching to 4.4.0.0 we are experiencing an increased memory consumption of MarkDuplicatesSpark. We see it on large BAM/CRAM files, not tested on small files. #### Steps to reproduce; This command: ; ```; java -Xmx190g -jar /usr/gitc/GATK_ultima.jar MarkDuplicatesSpark \; --spark-master local[24] \; --input 019242_old.ua.aln.bam \; --output 019242_old.aligned.sorted.duplicates_marked.bam \; --create-output-bam-index true \; --spark-verbosity WARN \; --verbosity WARNING \; --flowbased; ```; required 90GB memory on 4.3.0.0. The input BAM is large: 270GB; However on the 4.4.0.0 it requires >160GB RAM. #### Expected behavior; The memory requirement is not expected to change ; #### Actual behavior; Significantly increased memory requirement",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8307
https://github.com/broadinstitute/gatk/issues/8307:322,Testability,test,tested,322,"### Instructions. ## Bug Report. ### Affected tool(s) or class(es); MarkDuplicatesSpark. ### Affected version(s); - [v ] Latest public release version [4.4.0.0]. ### Description ; Since switching to 4.4.0.0 we are experiencing an increased memory consumption of MarkDuplicatesSpark. We see it on large BAM/CRAM files, not tested on small files. #### Steps to reproduce; This command: ; ```; java -Xmx190g -jar /usr/gitc/GATK_ultima.jar MarkDuplicatesSpark \; --spark-master local[24] \; --input 019242_old.ua.aln.bam \; --output 019242_old.aligned.sorted.duplicates_marked.bam \; --create-output-bam-index true \; --spark-verbosity WARN \; --verbosity WARNING \; --flowbased; ```; required 90GB memory on 4.3.0.0. The input BAM is large: 270GB; However on the 4.4.0.0 it requires >160GB RAM. #### Expected behavior; The memory requirement is not expected to change ; #### Actual behavior; Significantly increased memory requirement",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8307
https://github.com/broadinstitute/gatk/issues/8315:144,Testability,Test,Tested,144,"## Bug Report. ### Affected tool(s) or class(es); SplitIntervals using `-MODE BALANCING_WITHOUT_INTERVAL_SUBDIVISION`. ### Affected version(s); Tested on: ; ```; The Genome Analysis Toolkit (GATK) v4.4.0.0; HTSJDK Version: 3.0.5; Picard Version: 3.0.0; ```. ### Description ; SplitIntervals does not produce the requested number of interval lists as specified by `--scatter-count`, even if it possible based on the input. It seems that SplitIntervals prioritizes equal interval list basepair size. See below for example. #### Steps to reproduce; Toy example:; genome.fa:; ```; >test; AGCTTTTCATTCTGACTGCAACGGGCAATATGTCTCTGTGTGGATTAAAAAAAGAGTCTCTGACAGCAGCTTCTGAACTG; GTTACCTGCCGTGAGTAAATTAAAATTTTATTGACTTAGGTCACTAAATACTTTAACCAATATAGGCATAGCGCACAGAC; AGATAAAAATTACAGAGTACACAACATCCATGAAACGCATTAGCACCACCATTACCACCACCATCACCACCACCATCACC; ```; intervals.list:; ```; test:1-5; test:7-10; test:15-20; test:25-30; test:30-35; test:40-45; test:50-55; test:60-65; test:70-75; test:76-80; test:82-88; test:90-96; test:98-102; test:105-130; test:136-139; test:145-159; test:160-220; ```; Command:; ```; gatk SplitIntervals -R genome.fa -L toy_intervals.list -O ./ --scatter-count 17 -mode BALANCING_WITHOUT_INTERVAL_SUBDIVISION --sequence-dictionary genome.dict --interval-merging-rule OVERLAPPING_ONLY; ```; This only produces 3 interval list files. If this is expected behavior, its not entirely clear from the documentation. #### Expected behavior; I have a list of 500 non-overlapping intervals (from ScatterIntervalsByNs) that I would like SplitIntervals to organize into 250 interval lists. I would expect that the tool would produce 250 list files. . #### Actual behavior; The tool only produces 12 interval list files. . ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8315
https://github.com/broadinstitute/gatk/issues/8315:578,Testability,test,test,578,"## Bug Report. ### Affected tool(s) or class(es); SplitIntervals using `-MODE BALANCING_WITHOUT_INTERVAL_SUBDIVISION`. ### Affected version(s); Tested on: ; ```; The Genome Analysis Toolkit (GATK) v4.4.0.0; HTSJDK Version: 3.0.5; Picard Version: 3.0.0; ```. ### Description ; SplitIntervals does not produce the requested number of interval lists as specified by `--scatter-count`, even if it possible based on the input. It seems that SplitIntervals prioritizes equal interval list basepair size. See below for example. #### Steps to reproduce; Toy example:; genome.fa:; ```; >test; AGCTTTTCATTCTGACTGCAACGGGCAATATGTCTCTGTGTGGATTAAAAAAAGAGTCTCTGACAGCAGCTTCTGAACTG; GTTACCTGCCGTGAGTAAATTAAAATTTTATTGACTTAGGTCACTAAATACTTTAACCAATATAGGCATAGCGCACAGAC; AGATAAAAATTACAGAGTACACAACATCCATGAAACGCATTAGCACCACCATTACCACCACCATCACCACCACCATCACC; ```; intervals.list:; ```; test:1-5; test:7-10; test:15-20; test:25-30; test:30-35; test:40-45; test:50-55; test:60-65; test:70-75; test:76-80; test:82-88; test:90-96; test:98-102; test:105-130; test:136-139; test:145-159; test:160-220; ```; Command:; ```; gatk SplitIntervals -R genome.fa -L toy_intervals.list -O ./ --scatter-count 17 -mode BALANCING_WITHOUT_INTERVAL_SUBDIVISION --sequence-dictionary genome.dict --interval-merging-rule OVERLAPPING_ONLY; ```; This only produces 3 interval list files. If this is expected behavior, its not entirely clear from the documentation. #### Expected behavior; I have a list of 500 non-overlapping intervals (from ScatterIntervalsByNs) that I would like SplitIntervals to organize into 250 interval lists. I would expect that the tool would produce 250 list files. . #### Actual behavior; The tool only produces 12 interval list files. . ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8315
https://github.com/broadinstitute/gatk/issues/8315:857,Testability,test,test,857,"## Bug Report. ### Affected tool(s) or class(es); SplitIntervals using `-MODE BALANCING_WITHOUT_INTERVAL_SUBDIVISION`. ### Affected version(s); Tested on: ; ```; The Genome Analysis Toolkit (GATK) v4.4.0.0; HTSJDK Version: 3.0.5; Picard Version: 3.0.0; ```. ### Description ; SplitIntervals does not produce the requested number of interval lists as specified by `--scatter-count`, even if it possible based on the input. It seems that SplitIntervals prioritizes equal interval list basepair size. See below for example. #### Steps to reproduce; Toy example:; genome.fa:; ```; >test; AGCTTTTCATTCTGACTGCAACGGGCAATATGTCTCTGTGTGGATTAAAAAAAGAGTCTCTGACAGCAGCTTCTGAACTG; GTTACCTGCCGTGAGTAAATTAAAATTTTATTGACTTAGGTCACTAAATACTTTAACCAATATAGGCATAGCGCACAGAC; AGATAAAAATTACAGAGTACACAACATCCATGAAACGCATTAGCACCACCATTACCACCACCATCACCACCACCATCACC; ```; intervals.list:; ```; test:1-5; test:7-10; test:15-20; test:25-30; test:30-35; test:40-45; test:50-55; test:60-65; test:70-75; test:76-80; test:82-88; test:90-96; test:98-102; test:105-130; test:136-139; test:145-159; test:160-220; ```; Command:; ```; gatk SplitIntervals -R genome.fa -L toy_intervals.list -O ./ --scatter-count 17 -mode BALANCING_WITHOUT_INTERVAL_SUBDIVISION --sequence-dictionary genome.dict --interval-merging-rule OVERLAPPING_ONLY; ```; This only produces 3 interval list files. If this is expected behavior, its not entirely clear from the documentation. #### Expected behavior; I have a list of 500 non-overlapping intervals (from ScatterIntervalsByNs) that I would like SplitIntervals to organize into 250 interval lists. I would expect that the tool would produce 250 list files. . #### Actual behavior; The tool only produces 12 interval list files. . ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8315
https://github.com/broadinstitute/gatk/issues/8315:867,Testability,test,test,867,"## Bug Report. ### Affected tool(s) or class(es); SplitIntervals using `-MODE BALANCING_WITHOUT_INTERVAL_SUBDIVISION`. ### Affected version(s); Tested on: ; ```; The Genome Analysis Toolkit (GATK) v4.4.0.0; HTSJDK Version: 3.0.5; Picard Version: 3.0.0; ```. ### Description ; SplitIntervals does not produce the requested number of interval lists as specified by `--scatter-count`, even if it possible based on the input. It seems that SplitIntervals prioritizes equal interval list basepair size. See below for example. #### Steps to reproduce; Toy example:; genome.fa:; ```; >test; AGCTTTTCATTCTGACTGCAACGGGCAATATGTCTCTGTGTGGATTAAAAAAAGAGTCTCTGACAGCAGCTTCTGAACTG; GTTACCTGCCGTGAGTAAATTAAAATTTTATTGACTTAGGTCACTAAATACTTTAACCAATATAGGCATAGCGCACAGAC; AGATAAAAATTACAGAGTACACAACATCCATGAAACGCATTAGCACCACCATTACCACCACCATCACCACCACCATCACC; ```; intervals.list:; ```; test:1-5; test:7-10; test:15-20; test:25-30; test:30-35; test:40-45; test:50-55; test:60-65; test:70-75; test:76-80; test:82-88; test:90-96; test:98-102; test:105-130; test:136-139; test:145-159; test:160-220; ```; Command:; ```; gatk SplitIntervals -R genome.fa -L toy_intervals.list -O ./ --scatter-count 17 -mode BALANCING_WITHOUT_INTERVAL_SUBDIVISION --sequence-dictionary genome.dict --interval-merging-rule OVERLAPPING_ONLY; ```; This only produces 3 interval list files. If this is expected behavior, its not entirely clear from the documentation. #### Expected behavior; I have a list of 500 non-overlapping intervals (from ScatterIntervalsByNs) that I would like SplitIntervals to organize into 250 interval lists. I would expect that the tool would produce 250 list files. . #### Actual behavior; The tool only produces 12 interval list files. . ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8315
https://github.com/broadinstitute/gatk/issues/8315:878,Testability,test,test,878,"## Bug Report. ### Affected tool(s) or class(es); SplitIntervals using `-MODE BALANCING_WITHOUT_INTERVAL_SUBDIVISION`. ### Affected version(s); Tested on: ; ```; The Genome Analysis Toolkit (GATK) v4.4.0.0; HTSJDK Version: 3.0.5; Picard Version: 3.0.0; ```. ### Description ; SplitIntervals does not produce the requested number of interval lists as specified by `--scatter-count`, even if it possible based on the input. It seems that SplitIntervals prioritizes equal interval list basepair size. See below for example. #### Steps to reproduce; Toy example:; genome.fa:; ```; >test; AGCTTTTCATTCTGACTGCAACGGGCAATATGTCTCTGTGTGGATTAAAAAAAGAGTCTCTGACAGCAGCTTCTGAACTG; GTTACCTGCCGTGAGTAAATTAAAATTTTATTGACTTAGGTCACTAAATACTTTAACCAATATAGGCATAGCGCACAGAC; AGATAAAAATTACAGAGTACACAACATCCATGAAACGCATTAGCACCACCATTACCACCACCATCACCACCACCATCACC; ```; intervals.list:; ```; test:1-5; test:7-10; test:15-20; test:25-30; test:30-35; test:40-45; test:50-55; test:60-65; test:70-75; test:76-80; test:82-88; test:90-96; test:98-102; test:105-130; test:136-139; test:145-159; test:160-220; ```; Command:; ```; gatk SplitIntervals -R genome.fa -L toy_intervals.list -O ./ --scatter-count 17 -mode BALANCING_WITHOUT_INTERVAL_SUBDIVISION --sequence-dictionary genome.dict --interval-merging-rule OVERLAPPING_ONLY; ```; This only produces 3 interval list files. If this is expected behavior, its not entirely clear from the documentation. #### Expected behavior; I have a list of 500 non-overlapping intervals (from ScatterIntervalsByNs) that I would like SplitIntervals to organize into 250 interval lists. I would expect that the tool would produce 250 list files. . #### Actual behavior; The tool only produces 12 interval list files. . ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8315
https://github.com/broadinstitute/gatk/issues/8315:890,Testability,test,test,890,"## Bug Report. ### Affected tool(s) or class(es); SplitIntervals using `-MODE BALANCING_WITHOUT_INTERVAL_SUBDIVISION`. ### Affected version(s); Tested on: ; ```; The Genome Analysis Toolkit (GATK) v4.4.0.0; HTSJDK Version: 3.0.5; Picard Version: 3.0.0; ```. ### Description ; SplitIntervals does not produce the requested number of interval lists as specified by `--scatter-count`, even if it possible based on the input. It seems that SplitIntervals prioritizes equal interval list basepair size. See below for example. #### Steps to reproduce; Toy example:; genome.fa:; ```; >test; AGCTTTTCATTCTGACTGCAACGGGCAATATGTCTCTGTGTGGATTAAAAAAAGAGTCTCTGACAGCAGCTTCTGAACTG; GTTACCTGCCGTGAGTAAATTAAAATTTTATTGACTTAGGTCACTAAATACTTTAACCAATATAGGCATAGCGCACAGAC; AGATAAAAATTACAGAGTACACAACATCCATGAAACGCATTAGCACCACCATTACCACCACCATCACCACCACCATCACC; ```; intervals.list:; ```; test:1-5; test:7-10; test:15-20; test:25-30; test:30-35; test:40-45; test:50-55; test:60-65; test:70-75; test:76-80; test:82-88; test:90-96; test:98-102; test:105-130; test:136-139; test:145-159; test:160-220; ```; Command:; ```; gatk SplitIntervals -R genome.fa -L toy_intervals.list -O ./ --scatter-count 17 -mode BALANCING_WITHOUT_INTERVAL_SUBDIVISION --sequence-dictionary genome.dict --interval-merging-rule OVERLAPPING_ONLY; ```; This only produces 3 interval list files. If this is expected behavior, its not entirely clear from the documentation. #### Expected behavior; I have a list of 500 non-overlapping intervals (from ScatterIntervalsByNs) that I would like SplitIntervals to organize into 250 interval lists. I would expect that the tool would produce 250 list files. . #### Actual behavior; The tool only produces 12 interval list files. . ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8315
https://github.com/broadinstitute/gatk/issues/8315:902,Testability,test,test,902,"## Bug Report. ### Affected tool(s) or class(es); SplitIntervals using `-MODE BALANCING_WITHOUT_INTERVAL_SUBDIVISION`. ### Affected version(s); Tested on: ; ```; The Genome Analysis Toolkit (GATK) v4.4.0.0; HTSJDK Version: 3.0.5; Picard Version: 3.0.0; ```. ### Description ; SplitIntervals does not produce the requested number of interval lists as specified by `--scatter-count`, even if it possible based on the input. It seems that SplitIntervals prioritizes equal interval list basepair size. See below for example. #### Steps to reproduce; Toy example:; genome.fa:; ```; >test; AGCTTTTCATTCTGACTGCAACGGGCAATATGTCTCTGTGTGGATTAAAAAAAGAGTCTCTGACAGCAGCTTCTGAACTG; GTTACCTGCCGTGAGTAAATTAAAATTTTATTGACTTAGGTCACTAAATACTTTAACCAATATAGGCATAGCGCACAGAC; AGATAAAAATTACAGAGTACACAACATCCATGAAACGCATTAGCACCACCATTACCACCACCATCACCACCACCATCACC; ```; intervals.list:; ```; test:1-5; test:7-10; test:15-20; test:25-30; test:30-35; test:40-45; test:50-55; test:60-65; test:70-75; test:76-80; test:82-88; test:90-96; test:98-102; test:105-130; test:136-139; test:145-159; test:160-220; ```; Command:; ```; gatk SplitIntervals -R genome.fa -L toy_intervals.list -O ./ --scatter-count 17 -mode BALANCING_WITHOUT_INTERVAL_SUBDIVISION --sequence-dictionary genome.dict --interval-merging-rule OVERLAPPING_ONLY; ```; This only produces 3 interval list files. If this is expected behavior, its not entirely clear from the documentation. #### Expected behavior; I have a list of 500 non-overlapping intervals (from ScatterIntervalsByNs) that I would like SplitIntervals to organize into 250 interval lists. I would expect that the tool would produce 250 list files. . #### Actual behavior; The tool only produces 12 interval list files. . ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8315
https://github.com/broadinstitute/gatk/issues/8315:914,Testability,test,test,914,"## Bug Report. ### Affected tool(s) or class(es); SplitIntervals using `-MODE BALANCING_WITHOUT_INTERVAL_SUBDIVISION`. ### Affected version(s); Tested on: ; ```; The Genome Analysis Toolkit (GATK) v4.4.0.0; HTSJDK Version: 3.0.5; Picard Version: 3.0.0; ```. ### Description ; SplitIntervals does not produce the requested number of interval lists as specified by `--scatter-count`, even if it possible based on the input. It seems that SplitIntervals prioritizes equal interval list basepair size. See below for example. #### Steps to reproduce; Toy example:; genome.fa:; ```; >test; AGCTTTTCATTCTGACTGCAACGGGCAATATGTCTCTGTGTGGATTAAAAAAAGAGTCTCTGACAGCAGCTTCTGAACTG; GTTACCTGCCGTGAGTAAATTAAAATTTTATTGACTTAGGTCACTAAATACTTTAACCAATATAGGCATAGCGCACAGAC; AGATAAAAATTACAGAGTACACAACATCCATGAAACGCATTAGCACCACCATTACCACCACCATCACCACCACCATCACC; ```; intervals.list:; ```; test:1-5; test:7-10; test:15-20; test:25-30; test:30-35; test:40-45; test:50-55; test:60-65; test:70-75; test:76-80; test:82-88; test:90-96; test:98-102; test:105-130; test:136-139; test:145-159; test:160-220; ```; Command:; ```; gatk SplitIntervals -R genome.fa -L toy_intervals.list -O ./ --scatter-count 17 -mode BALANCING_WITHOUT_INTERVAL_SUBDIVISION --sequence-dictionary genome.dict --interval-merging-rule OVERLAPPING_ONLY; ```; This only produces 3 interval list files. If this is expected behavior, its not entirely clear from the documentation. #### Expected behavior; I have a list of 500 non-overlapping intervals (from ScatterIntervalsByNs) that I would like SplitIntervals to organize into 250 interval lists. I would expect that the tool would produce 250 list files. . #### Actual behavior; The tool only produces 12 interval list files. . ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8315
https://github.com/broadinstitute/gatk/issues/8315:926,Testability,test,test,926,"## Bug Report. ### Affected tool(s) or class(es); SplitIntervals using `-MODE BALANCING_WITHOUT_INTERVAL_SUBDIVISION`. ### Affected version(s); Tested on: ; ```; The Genome Analysis Toolkit (GATK) v4.4.0.0; HTSJDK Version: 3.0.5; Picard Version: 3.0.0; ```. ### Description ; SplitIntervals does not produce the requested number of interval lists as specified by `--scatter-count`, even if it possible based on the input. It seems that SplitIntervals prioritizes equal interval list basepair size. See below for example. #### Steps to reproduce; Toy example:; genome.fa:; ```; >test; AGCTTTTCATTCTGACTGCAACGGGCAATATGTCTCTGTGTGGATTAAAAAAAGAGTCTCTGACAGCAGCTTCTGAACTG; GTTACCTGCCGTGAGTAAATTAAAATTTTATTGACTTAGGTCACTAAATACTTTAACCAATATAGGCATAGCGCACAGAC; AGATAAAAATTACAGAGTACACAACATCCATGAAACGCATTAGCACCACCATTACCACCACCATCACCACCACCATCACC; ```; intervals.list:; ```; test:1-5; test:7-10; test:15-20; test:25-30; test:30-35; test:40-45; test:50-55; test:60-65; test:70-75; test:76-80; test:82-88; test:90-96; test:98-102; test:105-130; test:136-139; test:145-159; test:160-220; ```; Command:; ```; gatk SplitIntervals -R genome.fa -L toy_intervals.list -O ./ --scatter-count 17 -mode BALANCING_WITHOUT_INTERVAL_SUBDIVISION --sequence-dictionary genome.dict --interval-merging-rule OVERLAPPING_ONLY; ```; This only produces 3 interval list files. If this is expected behavior, its not entirely clear from the documentation. #### Expected behavior; I have a list of 500 non-overlapping intervals (from ScatterIntervalsByNs) that I would like SplitIntervals to organize into 250 interval lists. I would expect that the tool would produce 250 list files. . #### Actual behavior; The tool only produces 12 interval list files. . ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8315
https://github.com/broadinstitute/gatk/issues/8315:938,Testability,test,test,938,"## Bug Report. ### Affected tool(s) or class(es); SplitIntervals using `-MODE BALANCING_WITHOUT_INTERVAL_SUBDIVISION`. ### Affected version(s); Tested on: ; ```; The Genome Analysis Toolkit (GATK) v4.4.0.0; HTSJDK Version: 3.0.5; Picard Version: 3.0.0; ```. ### Description ; SplitIntervals does not produce the requested number of interval lists as specified by `--scatter-count`, even if it possible based on the input. It seems that SplitIntervals prioritizes equal interval list basepair size. See below for example. #### Steps to reproduce; Toy example:; genome.fa:; ```; >test; AGCTTTTCATTCTGACTGCAACGGGCAATATGTCTCTGTGTGGATTAAAAAAAGAGTCTCTGACAGCAGCTTCTGAACTG; GTTACCTGCCGTGAGTAAATTAAAATTTTATTGACTTAGGTCACTAAATACTTTAACCAATATAGGCATAGCGCACAGAC; AGATAAAAATTACAGAGTACACAACATCCATGAAACGCATTAGCACCACCATTACCACCACCATCACCACCACCATCACC; ```; intervals.list:; ```; test:1-5; test:7-10; test:15-20; test:25-30; test:30-35; test:40-45; test:50-55; test:60-65; test:70-75; test:76-80; test:82-88; test:90-96; test:98-102; test:105-130; test:136-139; test:145-159; test:160-220; ```; Command:; ```; gatk SplitIntervals -R genome.fa -L toy_intervals.list -O ./ --scatter-count 17 -mode BALANCING_WITHOUT_INTERVAL_SUBDIVISION --sequence-dictionary genome.dict --interval-merging-rule OVERLAPPING_ONLY; ```; This only produces 3 interval list files. If this is expected behavior, its not entirely clear from the documentation. #### Expected behavior; I have a list of 500 non-overlapping intervals (from ScatterIntervalsByNs) that I would like SplitIntervals to organize into 250 interval lists. I would expect that the tool would produce 250 list files. . #### Actual behavior; The tool only produces 12 interval list files. . ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8315
https://github.com/broadinstitute/gatk/issues/8315:950,Testability,test,test,950,"## Bug Report. ### Affected tool(s) or class(es); SplitIntervals using `-MODE BALANCING_WITHOUT_INTERVAL_SUBDIVISION`. ### Affected version(s); Tested on: ; ```; The Genome Analysis Toolkit (GATK) v4.4.0.0; HTSJDK Version: 3.0.5; Picard Version: 3.0.0; ```. ### Description ; SplitIntervals does not produce the requested number of interval lists as specified by `--scatter-count`, even if it possible based on the input. It seems that SplitIntervals prioritizes equal interval list basepair size. See below for example. #### Steps to reproduce; Toy example:; genome.fa:; ```; >test; AGCTTTTCATTCTGACTGCAACGGGCAATATGTCTCTGTGTGGATTAAAAAAAGAGTCTCTGACAGCAGCTTCTGAACTG; GTTACCTGCCGTGAGTAAATTAAAATTTTATTGACTTAGGTCACTAAATACTTTAACCAATATAGGCATAGCGCACAGAC; AGATAAAAATTACAGAGTACACAACATCCATGAAACGCATTAGCACCACCATTACCACCACCATCACCACCACCATCACC; ```; intervals.list:; ```; test:1-5; test:7-10; test:15-20; test:25-30; test:30-35; test:40-45; test:50-55; test:60-65; test:70-75; test:76-80; test:82-88; test:90-96; test:98-102; test:105-130; test:136-139; test:145-159; test:160-220; ```; Command:; ```; gatk SplitIntervals -R genome.fa -L toy_intervals.list -O ./ --scatter-count 17 -mode BALANCING_WITHOUT_INTERVAL_SUBDIVISION --sequence-dictionary genome.dict --interval-merging-rule OVERLAPPING_ONLY; ```; This only produces 3 interval list files. If this is expected behavior, its not entirely clear from the documentation. #### Expected behavior; I have a list of 500 non-overlapping intervals (from ScatterIntervalsByNs) that I would like SplitIntervals to organize into 250 interval lists. I would expect that the tool would produce 250 list files. . #### Actual behavior; The tool only produces 12 interval list files. . ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8315
https://github.com/broadinstitute/gatk/issues/8315:962,Testability,test,test,962,"## Bug Report. ### Affected tool(s) or class(es); SplitIntervals using `-MODE BALANCING_WITHOUT_INTERVAL_SUBDIVISION`. ### Affected version(s); Tested on: ; ```; The Genome Analysis Toolkit (GATK) v4.4.0.0; HTSJDK Version: 3.0.5; Picard Version: 3.0.0; ```. ### Description ; SplitIntervals does not produce the requested number of interval lists as specified by `--scatter-count`, even if it possible based on the input. It seems that SplitIntervals prioritizes equal interval list basepair size. See below for example. #### Steps to reproduce; Toy example:; genome.fa:; ```; >test; AGCTTTTCATTCTGACTGCAACGGGCAATATGTCTCTGTGTGGATTAAAAAAAGAGTCTCTGACAGCAGCTTCTGAACTG; GTTACCTGCCGTGAGTAAATTAAAATTTTATTGACTTAGGTCACTAAATACTTTAACCAATATAGGCATAGCGCACAGAC; AGATAAAAATTACAGAGTACACAACATCCATGAAACGCATTAGCACCACCATTACCACCACCATCACCACCACCATCACC; ```; intervals.list:; ```; test:1-5; test:7-10; test:15-20; test:25-30; test:30-35; test:40-45; test:50-55; test:60-65; test:70-75; test:76-80; test:82-88; test:90-96; test:98-102; test:105-130; test:136-139; test:145-159; test:160-220; ```; Command:; ```; gatk SplitIntervals -R genome.fa -L toy_intervals.list -O ./ --scatter-count 17 -mode BALANCING_WITHOUT_INTERVAL_SUBDIVISION --sequence-dictionary genome.dict --interval-merging-rule OVERLAPPING_ONLY; ```; This only produces 3 interval list files. If this is expected behavior, its not entirely clear from the documentation. #### Expected behavior; I have a list of 500 non-overlapping intervals (from ScatterIntervalsByNs) that I would like SplitIntervals to organize into 250 interval lists. I would expect that the tool would produce 250 list files. . #### Actual behavior; The tool only produces 12 interval list files. . ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8315
https://github.com/broadinstitute/gatk/issues/8315:974,Testability,test,test,974,"## Bug Report. ### Affected tool(s) or class(es); SplitIntervals using `-MODE BALANCING_WITHOUT_INTERVAL_SUBDIVISION`. ### Affected version(s); Tested on: ; ```; The Genome Analysis Toolkit (GATK) v4.4.0.0; HTSJDK Version: 3.0.5; Picard Version: 3.0.0; ```. ### Description ; SplitIntervals does not produce the requested number of interval lists as specified by `--scatter-count`, even if it possible based on the input. It seems that SplitIntervals prioritizes equal interval list basepair size. See below for example. #### Steps to reproduce; Toy example:; genome.fa:; ```; >test; AGCTTTTCATTCTGACTGCAACGGGCAATATGTCTCTGTGTGGATTAAAAAAAGAGTCTCTGACAGCAGCTTCTGAACTG; GTTACCTGCCGTGAGTAAATTAAAATTTTATTGACTTAGGTCACTAAATACTTTAACCAATATAGGCATAGCGCACAGAC; AGATAAAAATTACAGAGTACACAACATCCATGAAACGCATTAGCACCACCATTACCACCACCATCACCACCACCATCACC; ```; intervals.list:; ```; test:1-5; test:7-10; test:15-20; test:25-30; test:30-35; test:40-45; test:50-55; test:60-65; test:70-75; test:76-80; test:82-88; test:90-96; test:98-102; test:105-130; test:136-139; test:145-159; test:160-220; ```; Command:; ```; gatk SplitIntervals -R genome.fa -L toy_intervals.list -O ./ --scatter-count 17 -mode BALANCING_WITHOUT_INTERVAL_SUBDIVISION --sequence-dictionary genome.dict --interval-merging-rule OVERLAPPING_ONLY; ```; This only produces 3 interval list files. If this is expected behavior, its not entirely clear from the documentation. #### Expected behavior; I have a list of 500 non-overlapping intervals (from ScatterIntervalsByNs) that I would like SplitIntervals to organize into 250 interval lists. I would expect that the tool would produce 250 list files. . #### Actual behavior; The tool only produces 12 interval list files. . ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8315
https://github.com/broadinstitute/gatk/issues/8315:986,Testability,test,test,986,"## Bug Report. ### Affected tool(s) or class(es); SplitIntervals using `-MODE BALANCING_WITHOUT_INTERVAL_SUBDIVISION`. ### Affected version(s); Tested on: ; ```; The Genome Analysis Toolkit (GATK) v4.4.0.0; HTSJDK Version: 3.0.5; Picard Version: 3.0.0; ```. ### Description ; SplitIntervals does not produce the requested number of interval lists as specified by `--scatter-count`, even if it possible based on the input. It seems that SplitIntervals prioritizes equal interval list basepair size. See below for example. #### Steps to reproduce; Toy example:; genome.fa:; ```; >test; AGCTTTTCATTCTGACTGCAACGGGCAATATGTCTCTGTGTGGATTAAAAAAAGAGTCTCTGACAGCAGCTTCTGAACTG; GTTACCTGCCGTGAGTAAATTAAAATTTTATTGACTTAGGTCACTAAATACTTTAACCAATATAGGCATAGCGCACAGAC; AGATAAAAATTACAGAGTACACAACATCCATGAAACGCATTAGCACCACCATTACCACCACCATCACCACCACCATCACC; ```; intervals.list:; ```; test:1-5; test:7-10; test:15-20; test:25-30; test:30-35; test:40-45; test:50-55; test:60-65; test:70-75; test:76-80; test:82-88; test:90-96; test:98-102; test:105-130; test:136-139; test:145-159; test:160-220; ```; Command:; ```; gatk SplitIntervals -R genome.fa -L toy_intervals.list -O ./ --scatter-count 17 -mode BALANCING_WITHOUT_INTERVAL_SUBDIVISION --sequence-dictionary genome.dict --interval-merging-rule OVERLAPPING_ONLY; ```; This only produces 3 interval list files. If this is expected behavior, its not entirely clear from the documentation. #### Expected behavior; I have a list of 500 non-overlapping intervals (from ScatterIntervalsByNs) that I would like SplitIntervals to organize into 250 interval lists. I would expect that the tool would produce 250 list files. . #### Actual behavior; The tool only produces 12 interval list files. . ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8315
https://github.com/broadinstitute/gatk/issues/8315:998,Testability,test,test,998,"## Bug Report. ### Affected tool(s) or class(es); SplitIntervals using `-MODE BALANCING_WITHOUT_INTERVAL_SUBDIVISION`. ### Affected version(s); Tested on: ; ```; The Genome Analysis Toolkit (GATK) v4.4.0.0; HTSJDK Version: 3.0.5; Picard Version: 3.0.0; ```. ### Description ; SplitIntervals does not produce the requested number of interval lists as specified by `--scatter-count`, even if it possible based on the input. It seems that SplitIntervals prioritizes equal interval list basepair size. See below for example. #### Steps to reproduce; Toy example:; genome.fa:; ```; >test; AGCTTTTCATTCTGACTGCAACGGGCAATATGTCTCTGTGTGGATTAAAAAAAGAGTCTCTGACAGCAGCTTCTGAACTG; GTTACCTGCCGTGAGTAAATTAAAATTTTATTGACTTAGGTCACTAAATACTTTAACCAATATAGGCATAGCGCACAGAC; AGATAAAAATTACAGAGTACACAACATCCATGAAACGCATTAGCACCACCATTACCACCACCATCACCACCACCATCACC; ```; intervals.list:; ```; test:1-5; test:7-10; test:15-20; test:25-30; test:30-35; test:40-45; test:50-55; test:60-65; test:70-75; test:76-80; test:82-88; test:90-96; test:98-102; test:105-130; test:136-139; test:145-159; test:160-220; ```; Command:; ```; gatk SplitIntervals -R genome.fa -L toy_intervals.list -O ./ --scatter-count 17 -mode BALANCING_WITHOUT_INTERVAL_SUBDIVISION --sequence-dictionary genome.dict --interval-merging-rule OVERLAPPING_ONLY; ```; This only produces 3 interval list files. If this is expected behavior, its not entirely clear from the documentation. #### Expected behavior; I have a list of 500 non-overlapping intervals (from ScatterIntervalsByNs) that I would like SplitIntervals to organize into 250 interval lists. I would expect that the tool would produce 250 list files. . #### Actual behavior; The tool only produces 12 interval list files. . ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8315
https://github.com/broadinstitute/gatk/issues/8315:1011,Testability,test,test,1011,"## Bug Report. ### Affected tool(s) or class(es); SplitIntervals using `-MODE BALANCING_WITHOUT_INTERVAL_SUBDIVISION`. ### Affected version(s); Tested on: ; ```; The Genome Analysis Toolkit (GATK) v4.4.0.0; HTSJDK Version: 3.0.5; Picard Version: 3.0.0; ```. ### Description ; SplitIntervals does not produce the requested number of interval lists as specified by `--scatter-count`, even if it possible based on the input. It seems that SplitIntervals prioritizes equal interval list basepair size. See below for example. #### Steps to reproduce; Toy example:; genome.fa:; ```; >test; AGCTTTTCATTCTGACTGCAACGGGCAATATGTCTCTGTGTGGATTAAAAAAAGAGTCTCTGACAGCAGCTTCTGAACTG; GTTACCTGCCGTGAGTAAATTAAAATTTTATTGACTTAGGTCACTAAATACTTTAACCAATATAGGCATAGCGCACAGAC; AGATAAAAATTACAGAGTACACAACATCCATGAAACGCATTAGCACCACCATTACCACCACCATCACCACCACCATCACC; ```; intervals.list:; ```; test:1-5; test:7-10; test:15-20; test:25-30; test:30-35; test:40-45; test:50-55; test:60-65; test:70-75; test:76-80; test:82-88; test:90-96; test:98-102; test:105-130; test:136-139; test:145-159; test:160-220; ```; Command:; ```; gatk SplitIntervals -R genome.fa -L toy_intervals.list -O ./ --scatter-count 17 -mode BALANCING_WITHOUT_INTERVAL_SUBDIVISION --sequence-dictionary genome.dict --interval-merging-rule OVERLAPPING_ONLY; ```; This only produces 3 interval list files. If this is expected behavior, its not entirely clear from the documentation. #### Expected behavior; I have a list of 500 non-overlapping intervals (from ScatterIntervalsByNs) that I would like SplitIntervals to organize into 250 interval lists. I would expect that the tool would produce 250 list files. . #### Actual behavior; The tool only produces 12 interval list files. . ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8315
https://github.com/broadinstitute/gatk/issues/8315:1025,Testability,test,test,1025,"## Bug Report. ### Affected tool(s) or class(es); SplitIntervals using `-MODE BALANCING_WITHOUT_INTERVAL_SUBDIVISION`. ### Affected version(s); Tested on: ; ```; The Genome Analysis Toolkit (GATK) v4.4.0.0; HTSJDK Version: 3.0.5; Picard Version: 3.0.0; ```. ### Description ; SplitIntervals does not produce the requested number of interval lists as specified by `--scatter-count`, even if it possible based on the input. It seems that SplitIntervals prioritizes equal interval list basepair size. See below for example. #### Steps to reproduce; Toy example:; genome.fa:; ```; >test; AGCTTTTCATTCTGACTGCAACGGGCAATATGTCTCTGTGTGGATTAAAAAAAGAGTCTCTGACAGCAGCTTCTGAACTG; GTTACCTGCCGTGAGTAAATTAAAATTTTATTGACTTAGGTCACTAAATACTTTAACCAATATAGGCATAGCGCACAGAC; AGATAAAAATTACAGAGTACACAACATCCATGAAACGCATTAGCACCACCATTACCACCACCATCACCACCACCATCACC; ```; intervals.list:; ```; test:1-5; test:7-10; test:15-20; test:25-30; test:30-35; test:40-45; test:50-55; test:60-65; test:70-75; test:76-80; test:82-88; test:90-96; test:98-102; test:105-130; test:136-139; test:145-159; test:160-220; ```; Command:; ```; gatk SplitIntervals -R genome.fa -L toy_intervals.list -O ./ --scatter-count 17 -mode BALANCING_WITHOUT_INTERVAL_SUBDIVISION --sequence-dictionary genome.dict --interval-merging-rule OVERLAPPING_ONLY; ```; This only produces 3 interval list files. If this is expected behavior, its not entirely clear from the documentation. #### Expected behavior; I have a list of 500 non-overlapping intervals (from ScatterIntervalsByNs) that I would like SplitIntervals to organize into 250 interval lists. I would expect that the tool would produce 250 list files. . #### Actual behavior; The tool only produces 12 interval list files. . ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8315
https://github.com/broadinstitute/gatk/issues/8315:1039,Testability,test,test,1039,"## Bug Report. ### Affected tool(s) or class(es); SplitIntervals using `-MODE BALANCING_WITHOUT_INTERVAL_SUBDIVISION`. ### Affected version(s); Tested on: ; ```; The Genome Analysis Toolkit (GATK) v4.4.0.0; HTSJDK Version: 3.0.5; Picard Version: 3.0.0; ```. ### Description ; SplitIntervals does not produce the requested number of interval lists as specified by `--scatter-count`, even if it possible based on the input. It seems that SplitIntervals prioritizes equal interval list basepair size. See below for example. #### Steps to reproduce; Toy example:; genome.fa:; ```; >test; AGCTTTTCATTCTGACTGCAACGGGCAATATGTCTCTGTGTGGATTAAAAAAAGAGTCTCTGACAGCAGCTTCTGAACTG; GTTACCTGCCGTGAGTAAATTAAAATTTTATTGACTTAGGTCACTAAATACTTTAACCAATATAGGCATAGCGCACAGAC; AGATAAAAATTACAGAGTACACAACATCCATGAAACGCATTAGCACCACCATTACCACCACCATCACCACCACCATCACC; ```; intervals.list:; ```; test:1-5; test:7-10; test:15-20; test:25-30; test:30-35; test:40-45; test:50-55; test:60-65; test:70-75; test:76-80; test:82-88; test:90-96; test:98-102; test:105-130; test:136-139; test:145-159; test:160-220; ```; Command:; ```; gatk SplitIntervals -R genome.fa -L toy_intervals.list -O ./ --scatter-count 17 -mode BALANCING_WITHOUT_INTERVAL_SUBDIVISION --sequence-dictionary genome.dict --interval-merging-rule OVERLAPPING_ONLY; ```; This only produces 3 interval list files. If this is expected behavior, its not entirely clear from the documentation. #### Expected behavior; I have a list of 500 non-overlapping intervals (from ScatterIntervalsByNs) that I would like SplitIntervals to organize into 250 interval lists. I would expect that the tool would produce 250 list files. . #### Actual behavior; The tool only produces 12 interval list files. . ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8315
https://github.com/broadinstitute/gatk/issues/8315:1053,Testability,test,test,1053,"## Bug Report. ### Affected tool(s) or class(es); SplitIntervals using `-MODE BALANCING_WITHOUT_INTERVAL_SUBDIVISION`. ### Affected version(s); Tested on: ; ```; The Genome Analysis Toolkit (GATK) v4.4.0.0; HTSJDK Version: 3.0.5; Picard Version: 3.0.0; ```. ### Description ; SplitIntervals does not produce the requested number of interval lists as specified by `--scatter-count`, even if it possible based on the input. It seems that SplitIntervals prioritizes equal interval list basepair size. See below for example. #### Steps to reproduce; Toy example:; genome.fa:; ```; >test; AGCTTTTCATTCTGACTGCAACGGGCAATATGTCTCTGTGTGGATTAAAAAAAGAGTCTCTGACAGCAGCTTCTGAACTG; GTTACCTGCCGTGAGTAAATTAAAATTTTATTGACTTAGGTCACTAAATACTTTAACCAATATAGGCATAGCGCACAGAC; AGATAAAAATTACAGAGTACACAACATCCATGAAACGCATTAGCACCACCATTACCACCACCATCACCACCACCATCACC; ```; intervals.list:; ```; test:1-5; test:7-10; test:15-20; test:25-30; test:30-35; test:40-45; test:50-55; test:60-65; test:70-75; test:76-80; test:82-88; test:90-96; test:98-102; test:105-130; test:136-139; test:145-159; test:160-220; ```; Command:; ```; gatk SplitIntervals -R genome.fa -L toy_intervals.list -O ./ --scatter-count 17 -mode BALANCING_WITHOUT_INTERVAL_SUBDIVISION --sequence-dictionary genome.dict --interval-merging-rule OVERLAPPING_ONLY; ```; This only produces 3 interval list files. If this is expected behavior, its not entirely clear from the documentation. #### Expected behavior; I have a list of 500 non-overlapping intervals (from ScatterIntervalsByNs) that I would like SplitIntervals to organize into 250 interval lists. I would expect that the tool would produce 250 list files. . #### Actual behavior; The tool only produces 12 interval list files. . ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8315
https://github.com/broadinstitute/gatk/issues/8315:1382,Usability,clear,clear,1382,"## Bug Report. ### Affected tool(s) or class(es); SplitIntervals using `-MODE BALANCING_WITHOUT_INTERVAL_SUBDIVISION`. ### Affected version(s); Tested on: ; ```; The Genome Analysis Toolkit (GATK) v4.4.0.0; HTSJDK Version: 3.0.5; Picard Version: 3.0.0; ```. ### Description ; SplitIntervals does not produce the requested number of interval lists as specified by `--scatter-count`, even if it possible based on the input. It seems that SplitIntervals prioritizes equal interval list basepair size. See below for example. #### Steps to reproduce; Toy example:; genome.fa:; ```; >test; AGCTTTTCATTCTGACTGCAACGGGCAATATGTCTCTGTGTGGATTAAAAAAAGAGTCTCTGACAGCAGCTTCTGAACTG; GTTACCTGCCGTGAGTAAATTAAAATTTTATTGACTTAGGTCACTAAATACTTTAACCAATATAGGCATAGCGCACAGAC; AGATAAAAATTACAGAGTACACAACATCCATGAAACGCATTAGCACCACCATTACCACCACCATCACCACCACCATCACC; ```; intervals.list:; ```; test:1-5; test:7-10; test:15-20; test:25-30; test:30-35; test:40-45; test:50-55; test:60-65; test:70-75; test:76-80; test:82-88; test:90-96; test:98-102; test:105-130; test:136-139; test:145-159; test:160-220; ```; Command:; ```; gatk SplitIntervals -R genome.fa -L toy_intervals.list -O ./ --scatter-count 17 -mode BALANCING_WITHOUT_INTERVAL_SUBDIVISION --sequence-dictionary genome.dict --interval-merging-rule OVERLAPPING_ONLY; ```; This only produces 3 interval list files. If this is expected behavior, its not entirely clear from the documentation. #### Expected behavior; I have a list of 500 non-overlapping intervals (from ScatterIntervalsByNs) that I would like SplitIntervals to organize into 250 interval lists. I would expect that the tool would produce 250 list files. . #### Actual behavior; The tool only produces 12 interval list files. . ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8315
https://github.com/broadinstitute/gatk/issues/8317:503,Availability,failure,failures,503,"Currently `ReferenceConfidenceVariantContextMerger.merge()` does not pass in the remapped alleles to `GATKVariantContextUtils.makeGenotypeCall()` for the `originalGT` argument, which seems problematic and could cause the code to incorrectly fall back on reference calls in some cases. The GVS team has found it necessary to use the remapped alleles when genotyping in order to get correct genotypes out of the merge -- we should make this behavior the default. . Currently, making it the default causes failures in the tests for the `--include-non-variant-sites` argument in `GenotypeGVCFs` (see https://github.com/broadinstitute/gatk/pull/8288#issuecomment-1509295869). This appears related to the current implementation incorrectly calculating the reference allele when we are at some non-zero offset from the start of a GVCF reference block.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8317
https://github.com/broadinstitute/gatk/issues/8317:519,Testability,test,tests,519,"Currently `ReferenceConfidenceVariantContextMerger.merge()` does not pass in the remapped alleles to `GATKVariantContextUtils.makeGenotypeCall()` for the `originalGT` argument, which seems problematic and could cause the code to incorrectly fall back on reference calls in some cases. The GVS team has found it necessary to use the remapped alleles when genotyping in order to get correct genotypes out of the merge -- we should make this behavior the default. . Currently, making it the default causes failures in the tests for the `--include-non-variant-sites` argument in `GenotypeGVCFs` (see https://github.com/broadinstitute/gatk/pull/8288#issuecomment-1509295869). This appears related to the current implementation incorrectly calculating the reference allele when we are at some non-zero offset from the start of a GVCF reference block.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8317
https://github.com/broadinstitute/gatk/issues/8320:177,Availability,Down,Downloading,177,"## Bug Report. - OS: Arch Linux; - Java: 17. ### Affected version(s); - [x] Latest public release version [version?]. ### Description . Firstly, I run `./gradle localJar`. ```; Downloading https://services.gradle.org/distributions/gradle-7.5.1-bin.zip; ...........10%............20%...........30%............40%...........50%............60%...........70%............80%...........90%............100%. Welcome to Gradle 7.5.1!. Here are the highlights of this release:; - Support for Java 18; - Support for building with Groovy 4; - Much more responsive continuous builds; - Improved diagnostics for dependency resolution. For more details see https://docs.gradle.org/7.5.1/release-notes.html. Starting a Gradle Daemon (subsequent builds will be faster). > Configure project :; Executing: git lfs pull --include src/main/resources/large. FAILURE: Build failed with an exception. * Where:; Build file '/build/gatk/src/gatk/build.gradle' line: 104. * What went wrong:; A problem occurred evaluating root project 'gatk'.; > Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 2. git-lfs is required to build GATK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK. * Try:; > Run with --stacktrace option to get the stack trace.; > Run with --info or --debug option to get more log output.; > Run with --scan to get full insights. * Get more help at https://help.gradle.org. BUILD FAILED in 17s; ```; However, I already install git-lfs; ```; git-lfs usr/; git-lfs usr/bin/; git-lfs usr/bin/git-lfs; git-lfs usr/share/; git-lfs usr/share/licenses/; git-lfs usr/share/licenses/git-lfs/; git-lfs usr/share/licenses/git-lfs/LICENSE; git-lfs usr/share/man/; git-lfs usr/share/man/man1/; git-lfs usr/share/man/man1/git-lfs-checkout.1.gz; git-lfs usr/share/man/man1/git-lfs-clean.1.gz; git-lfs usr/share/man/man1/git-lfs-clone.1.gz; git-lfs usr/share/man/man1/git-lfs-dedup.1.gz; git-lfs usr/share/man/man1/git-lfs-",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8320
https://github.com/broadinstitute/gatk/issues/8320:837,Availability,FAILURE,FAILURE,837,"## Bug Report. - OS: Arch Linux; - Java: 17. ### Affected version(s); - [x] Latest public release version [version?]. ### Description . Firstly, I run `./gradle localJar`. ```; Downloading https://services.gradle.org/distributions/gradle-7.5.1-bin.zip; ...........10%............20%...........30%............40%...........50%............60%...........70%............80%...........90%............100%. Welcome to Gradle 7.5.1!. Here are the highlights of this release:; - Support for Java 18; - Support for building with Groovy 4; - Much more responsive continuous builds; - Improved diagnostics for dependency resolution. For more details see https://docs.gradle.org/7.5.1/release-notes.html. Starting a Gradle Daemon (subsequent builds will be faster). > Configure project :; Executing: git lfs pull --include src/main/resources/large. FAILURE: Build failed with an exception. * Where:; Build file '/build/gatk/src/gatk/build.gradle' line: 104. * What went wrong:; A problem occurred evaluating root project 'gatk'.; > Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 2. git-lfs is required to build GATK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK. * Try:; > Run with --stacktrace option to get the stack trace.; > Run with --info or --debug option to get more log output.; > Run with --scan to get full insights. * Get more help at https://help.gradle.org. BUILD FAILED in 17s; ```; However, I already install git-lfs; ```; git-lfs usr/; git-lfs usr/bin/; git-lfs usr/bin/git-lfs; git-lfs usr/share/; git-lfs usr/share/licenses/; git-lfs usr/share/licenses/git-lfs/; git-lfs usr/share/licenses/git-lfs/LICENSE; git-lfs usr/share/man/; git-lfs usr/share/man/man1/; git-lfs usr/share/man/man1/git-lfs-checkout.1.gz; git-lfs usr/share/man/man1/git-lfs-clean.1.gz; git-lfs usr/share/man/man1/git-lfs-clone.1.gz; git-lfs usr/share/man/man1/git-lfs-dedup.1.gz; git-lfs usr/share/man/man1/git-lfs-",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8320
https://github.com/broadinstitute/gatk/issues/8320:3565,Availability,error,error,3565,"r/share/man/man1/git-lfs-smudge.1.gz; git-lfs usr/share/man/man1/git-lfs-standalone-file.1.gz; git-lfs usr/share/man/man1/git-lfs-status.1.gz; git-lfs usr/share/man/man1/git-lfs-track.1.gz; git-lfs usr/share/man/man1/git-lfs-uninstall.1.gz; git-lfs usr/share/man/man1/git-lfs-unlock.1.gz; git-lfs usr/share/man/man1/git-lfs-untrack.1.gz; git-lfs usr/share/man/man1/git-lfs-update.1.gz; git-lfs usr/share/man/man1/git-lfs.1.gz; git-lfs usr/share/man/man5/; git-lfs usr/share/man/man5/git-lfs-config.5.gz; ```. Then I run ; ```; git lfs pull --include src/main/resources/large; ./gradle localJar; ```; then; ```; error transferring ""1d70940bd9d7c6c862304c66d64233726dc30342ae7032a4636939e8249cbf46"": [0] remote missing object 1d70940bd9d7c6c862304c66d64233726dc30342ae7032a4636939e8249cbf46; error transferring ""bd17c3a98f7651b4e7ee54d875c47ec12e18b75daf79b3744a2590ddb0d6b44d"": [0] remote missing object bd17c3a98f7651b4e7ee54d875c47ec12e18b75daf79b3744a2590ddb0d6b44d; error transferring ""6f663a2fdbcde0addc5cb755f7af5d4c19bed92dccfd20e25b2acf2bc8c2ca7c"": [0] remote missing object 6f663a2fdbcde0addc5cb755f7af5d4c19bed92dccfd20e25b2acf2bc8c2ca7c; error transferring ""e38e09cfe7b7ffbc80dce4972bc9c382148520147d46738a3f6f3235b2d876c6"": [0] remote missing object e38e09cfe7b7ffbc80dce4972bc9c382148520147d46738a3f6f3235b2d876c6; error transferring ""4ed7feb0343e9ac03135b1456b2c8d2edab1b359c4950908c4d44152c0634a89"": [0] remote missing object 4ed7feb0343e9ac03135b1456b2c8d2edab1b359c4950908c4d44152c0634a89; error transferring ""eda2517817b23238c2b28f69a1fa39e9b85b45985854f0a5d5508280e76da39e"": [0] remote missing object eda2517817b23238c2b28f69a1fa39e9b85b45985854f0a5d5508280e76da39e; error transferring ""5e69a86f301ab9ab0d507ad7659abb4ad3732382ccbeb714db497e51eb3cf87b"": [0] remote missing object 5e69a86f301ab9ab0d507ad7659abb4ad3732382ccbeb714db497e51eb3cf87b; Failed to fetch some objects from 'file:///startdir/gatk'. ```. #### Expected behavior; can be compiled. #### Actual behavior; see above",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8320
https://github.com/broadinstitute/gatk/issues/8320:3744,Availability,error,error,3744,"r/share/man/man1/git-lfs-smudge.1.gz; git-lfs usr/share/man/man1/git-lfs-standalone-file.1.gz; git-lfs usr/share/man/man1/git-lfs-status.1.gz; git-lfs usr/share/man/man1/git-lfs-track.1.gz; git-lfs usr/share/man/man1/git-lfs-uninstall.1.gz; git-lfs usr/share/man/man1/git-lfs-unlock.1.gz; git-lfs usr/share/man/man1/git-lfs-untrack.1.gz; git-lfs usr/share/man/man1/git-lfs-update.1.gz; git-lfs usr/share/man/man1/git-lfs.1.gz; git-lfs usr/share/man/man5/; git-lfs usr/share/man/man5/git-lfs-config.5.gz; ```. Then I run ; ```; git lfs pull --include src/main/resources/large; ./gradle localJar; ```; then; ```; error transferring ""1d70940bd9d7c6c862304c66d64233726dc30342ae7032a4636939e8249cbf46"": [0] remote missing object 1d70940bd9d7c6c862304c66d64233726dc30342ae7032a4636939e8249cbf46; error transferring ""bd17c3a98f7651b4e7ee54d875c47ec12e18b75daf79b3744a2590ddb0d6b44d"": [0] remote missing object bd17c3a98f7651b4e7ee54d875c47ec12e18b75daf79b3744a2590ddb0d6b44d; error transferring ""6f663a2fdbcde0addc5cb755f7af5d4c19bed92dccfd20e25b2acf2bc8c2ca7c"": [0] remote missing object 6f663a2fdbcde0addc5cb755f7af5d4c19bed92dccfd20e25b2acf2bc8c2ca7c; error transferring ""e38e09cfe7b7ffbc80dce4972bc9c382148520147d46738a3f6f3235b2d876c6"": [0] remote missing object e38e09cfe7b7ffbc80dce4972bc9c382148520147d46738a3f6f3235b2d876c6; error transferring ""4ed7feb0343e9ac03135b1456b2c8d2edab1b359c4950908c4d44152c0634a89"": [0] remote missing object 4ed7feb0343e9ac03135b1456b2c8d2edab1b359c4950908c4d44152c0634a89; error transferring ""eda2517817b23238c2b28f69a1fa39e9b85b45985854f0a5d5508280e76da39e"": [0] remote missing object eda2517817b23238c2b28f69a1fa39e9b85b45985854f0a5d5508280e76da39e; error transferring ""5e69a86f301ab9ab0d507ad7659abb4ad3732382ccbeb714db497e51eb3cf87b"": [0] remote missing object 5e69a86f301ab9ab0d507ad7659abb4ad3732382ccbeb714db497e51eb3cf87b; Failed to fetch some objects from 'file:///startdir/gatk'. ```. #### Expected behavior; can be compiled. #### Actual behavior; see above",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8320
https://github.com/broadinstitute/gatk/issues/8320:3923,Availability,error,error,3923,"r/share/man/man1/git-lfs-smudge.1.gz; git-lfs usr/share/man/man1/git-lfs-standalone-file.1.gz; git-lfs usr/share/man/man1/git-lfs-status.1.gz; git-lfs usr/share/man/man1/git-lfs-track.1.gz; git-lfs usr/share/man/man1/git-lfs-uninstall.1.gz; git-lfs usr/share/man/man1/git-lfs-unlock.1.gz; git-lfs usr/share/man/man1/git-lfs-untrack.1.gz; git-lfs usr/share/man/man1/git-lfs-update.1.gz; git-lfs usr/share/man/man1/git-lfs.1.gz; git-lfs usr/share/man/man5/; git-lfs usr/share/man/man5/git-lfs-config.5.gz; ```. Then I run ; ```; git lfs pull --include src/main/resources/large; ./gradle localJar; ```; then; ```; error transferring ""1d70940bd9d7c6c862304c66d64233726dc30342ae7032a4636939e8249cbf46"": [0] remote missing object 1d70940bd9d7c6c862304c66d64233726dc30342ae7032a4636939e8249cbf46; error transferring ""bd17c3a98f7651b4e7ee54d875c47ec12e18b75daf79b3744a2590ddb0d6b44d"": [0] remote missing object bd17c3a98f7651b4e7ee54d875c47ec12e18b75daf79b3744a2590ddb0d6b44d; error transferring ""6f663a2fdbcde0addc5cb755f7af5d4c19bed92dccfd20e25b2acf2bc8c2ca7c"": [0] remote missing object 6f663a2fdbcde0addc5cb755f7af5d4c19bed92dccfd20e25b2acf2bc8c2ca7c; error transferring ""e38e09cfe7b7ffbc80dce4972bc9c382148520147d46738a3f6f3235b2d876c6"": [0] remote missing object e38e09cfe7b7ffbc80dce4972bc9c382148520147d46738a3f6f3235b2d876c6; error transferring ""4ed7feb0343e9ac03135b1456b2c8d2edab1b359c4950908c4d44152c0634a89"": [0] remote missing object 4ed7feb0343e9ac03135b1456b2c8d2edab1b359c4950908c4d44152c0634a89; error transferring ""eda2517817b23238c2b28f69a1fa39e9b85b45985854f0a5d5508280e76da39e"": [0] remote missing object eda2517817b23238c2b28f69a1fa39e9b85b45985854f0a5d5508280e76da39e; error transferring ""5e69a86f301ab9ab0d507ad7659abb4ad3732382ccbeb714db497e51eb3cf87b"": [0] remote missing object 5e69a86f301ab9ab0d507ad7659abb4ad3732382ccbeb714db497e51eb3cf87b; Failed to fetch some objects from 'file:///startdir/gatk'. ```. #### Expected behavior; can be compiled. #### Actual behavior; see above",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8320
https://github.com/broadinstitute/gatk/issues/8320:4102,Availability,error,error,4102,"r/share/man/man1/git-lfs-smudge.1.gz; git-lfs usr/share/man/man1/git-lfs-standalone-file.1.gz; git-lfs usr/share/man/man1/git-lfs-status.1.gz; git-lfs usr/share/man/man1/git-lfs-track.1.gz; git-lfs usr/share/man/man1/git-lfs-uninstall.1.gz; git-lfs usr/share/man/man1/git-lfs-unlock.1.gz; git-lfs usr/share/man/man1/git-lfs-untrack.1.gz; git-lfs usr/share/man/man1/git-lfs-update.1.gz; git-lfs usr/share/man/man1/git-lfs.1.gz; git-lfs usr/share/man/man5/; git-lfs usr/share/man/man5/git-lfs-config.5.gz; ```. Then I run ; ```; git lfs pull --include src/main/resources/large; ./gradle localJar; ```; then; ```; error transferring ""1d70940bd9d7c6c862304c66d64233726dc30342ae7032a4636939e8249cbf46"": [0] remote missing object 1d70940bd9d7c6c862304c66d64233726dc30342ae7032a4636939e8249cbf46; error transferring ""bd17c3a98f7651b4e7ee54d875c47ec12e18b75daf79b3744a2590ddb0d6b44d"": [0] remote missing object bd17c3a98f7651b4e7ee54d875c47ec12e18b75daf79b3744a2590ddb0d6b44d; error transferring ""6f663a2fdbcde0addc5cb755f7af5d4c19bed92dccfd20e25b2acf2bc8c2ca7c"": [0] remote missing object 6f663a2fdbcde0addc5cb755f7af5d4c19bed92dccfd20e25b2acf2bc8c2ca7c; error transferring ""e38e09cfe7b7ffbc80dce4972bc9c382148520147d46738a3f6f3235b2d876c6"": [0] remote missing object e38e09cfe7b7ffbc80dce4972bc9c382148520147d46738a3f6f3235b2d876c6; error transferring ""4ed7feb0343e9ac03135b1456b2c8d2edab1b359c4950908c4d44152c0634a89"": [0] remote missing object 4ed7feb0343e9ac03135b1456b2c8d2edab1b359c4950908c4d44152c0634a89; error transferring ""eda2517817b23238c2b28f69a1fa39e9b85b45985854f0a5d5508280e76da39e"": [0] remote missing object eda2517817b23238c2b28f69a1fa39e9b85b45985854f0a5d5508280e76da39e; error transferring ""5e69a86f301ab9ab0d507ad7659abb4ad3732382ccbeb714db497e51eb3cf87b"": [0] remote missing object 5e69a86f301ab9ab0d507ad7659abb4ad3732382ccbeb714db497e51eb3cf87b; Failed to fetch some objects from 'file:///startdir/gatk'. ```. #### Expected behavior; can be compiled. #### Actual behavior; see above",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8320
https://github.com/broadinstitute/gatk/issues/8320:4281,Availability,error,error,4281,"r/share/man/man1/git-lfs-smudge.1.gz; git-lfs usr/share/man/man1/git-lfs-standalone-file.1.gz; git-lfs usr/share/man/man1/git-lfs-status.1.gz; git-lfs usr/share/man/man1/git-lfs-track.1.gz; git-lfs usr/share/man/man1/git-lfs-uninstall.1.gz; git-lfs usr/share/man/man1/git-lfs-unlock.1.gz; git-lfs usr/share/man/man1/git-lfs-untrack.1.gz; git-lfs usr/share/man/man1/git-lfs-update.1.gz; git-lfs usr/share/man/man1/git-lfs.1.gz; git-lfs usr/share/man/man5/; git-lfs usr/share/man/man5/git-lfs-config.5.gz; ```. Then I run ; ```; git lfs pull --include src/main/resources/large; ./gradle localJar; ```; then; ```; error transferring ""1d70940bd9d7c6c862304c66d64233726dc30342ae7032a4636939e8249cbf46"": [0] remote missing object 1d70940bd9d7c6c862304c66d64233726dc30342ae7032a4636939e8249cbf46; error transferring ""bd17c3a98f7651b4e7ee54d875c47ec12e18b75daf79b3744a2590ddb0d6b44d"": [0] remote missing object bd17c3a98f7651b4e7ee54d875c47ec12e18b75daf79b3744a2590ddb0d6b44d; error transferring ""6f663a2fdbcde0addc5cb755f7af5d4c19bed92dccfd20e25b2acf2bc8c2ca7c"": [0] remote missing object 6f663a2fdbcde0addc5cb755f7af5d4c19bed92dccfd20e25b2acf2bc8c2ca7c; error transferring ""e38e09cfe7b7ffbc80dce4972bc9c382148520147d46738a3f6f3235b2d876c6"": [0] remote missing object e38e09cfe7b7ffbc80dce4972bc9c382148520147d46738a3f6f3235b2d876c6; error transferring ""4ed7feb0343e9ac03135b1456b2c8d2edab1b359c4950908c4d44152c0634a89"": [0] remote missing object 4ed7feb0343e9ac03135b1456b2c8d2edab1b359c4950908c4d44152c0634a89; error transferring ""eda2517817b23238c2b28f69a1fa39e9b85b45985854f0a5d5508280e76da39e"": [0] remote missing object eda2517817b23238c2b28f69a1fa39e9b85b45985854f0a5d5508280e76da39e; error transferring ""5e69a86f301ab9ab0d507ad7659abb4ad3732382ccbeb714db497e51eb3cf87b"": [0] remote missing object 5e69a86f301ab9ab0d507ad7659abb4ad3732382ccbeb714db497e51eb3cf87b; Failed to fetch some objects from 'file:///startdir/gatk'. ```. #### Expected behavior; can be compiled. #### Actual behavior; see above",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8320
https://github.com/broadinstitute/gatk/issues/8320:4460,Availability,error,error,4460,"r/share/man/man1/git-lfs-smudge.1.gz; git-lfs usr/share/man/man1/git-lfs-standalone-file.1.gz; git-lfs usr/share/man/man1/git-lfs-status.1.gz; git-lfs usr/share/man/man1/git-lfs-track.1.gz; git-lfs usr/share/man/man1/git-lfs-uninstall.1.gz; git-lfs usr/share/man/man1/git-lfs-unlock.1.gz; git-lfs usr/share/man/man1/git-lfs-untrack.1.gz; git-lfs usr/share/man/man1/git-lfs-update.1.gz; git-lfs usr/share/man/man1/git-lfs.1.gz; git-lfs usr/share/man/man5/; git-lfs usr/share/man/man5/git-lfs-config.5.gz; ```. Then I run ; ```; git lfs pull --include src/main/resources/large; ./gradle localJar; ```; then; ```; error transferring ""1d70940bd9d7c6c862304c66d64233726dc30342ae7032a4636939e8249cbf46"": [0] remote missing object 1d70940bd9d7c6c862304c66d64233726dc30342ae7032a4636939e8249cbf46; error transferring ""bd17c3a98f7651b4e7ee54d875c47ec12e18b75daf79b3744a2590ddb0d6b44d"": [0] remote missing object bd17c3a98f7651b4e7ee54d875c47ec12e18b75daf79b3744a2590ddb0d6b44d; error transferring ""6f663a2fdbcde0addc5cb755f7af5d4c19bed92dccfd20e25b2acf2bc8c2ca7c"": [0] remote missing object 6f663a2fdbcde0addc5cb755f7af5d4c19bed92dccfd20e25b2acf2bc8c2ca7c; error transferring ""e38e09cfe7b7ffbc80dce4972bc9c382148520147d46738a3f6f3235b2d876c6"": [0] remote missing object e38e09cfe7b7ffbc80dce4972bc9c382148520147d46738a3f6f3235b2d876c6; error transferring ""4ed7feb0343e9ac03135b1456b2c8d2edab1b359c4950908c4d44152c0634a89"": [0] remote missing object 4ed7feb0343e9ac03135b1456b2c8d2edab1b359c4950908c4d44152c0634a89; error transferring ""eda2517817b23238c2b28f69a1fa39e9b85b45985854f0a5d5508280e76da39e"": [0] remote missing object eda2517817b23238c2b28f69a1fa39e9b85b45985854f0a5d5508280e76da39e; error transferring ""5e69a86f301ab9ab0d507ad7659abb4ad3732382ccbeb714db497e51eb3cf87b"": [0] remote missing object 5e69a86f301ab9ab0d507ad7659abb4ad3732382ccbeb714db497e51eb3cf87b; Failed to fetch some objects from 'file:///startdir/gatk'. ```. #### Expected behavior; can be compiled. #### Actual behavior; see above",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8320
https://github.com/broadinstitute/gatk/issues/8320:4639,Availability,error,error,4639,"r/share/man/man1/git-lfs-smudge.1.gz; git-lfs usr/share/man/man1/git-lfs-standalone-file.1.gz; git-lfs usr/share/man/man1/git-lfs-status.1.gz; git-lfs usr/share/man/man1/git-lfs-track.1.gz; git-lfs usr/share/man/man1/git-lfs-uninstall.1.gz; git-lfs usr/share/man/man1/git-lfs-unlock.1.gz; git-lfs usr/share/man/man1/git-lfs-untrack.1.gz; git-lfs usr/share/man/man1/git-lfs-update.1.gz; git-lfs usr/share/man/man1/git-lfs.1.gz; git-lfs usr/share/man/man5/; git-lfs usr/share/man/man5/git-lfs-config.5.gz; ```. Then I run ; ```; git lfs pull --include src/main/resources/large; ./gradle localJar; ```; then; ```; error transferring ""1d70940bd9d7c6c862304c66d64233726dc30342ae7032a4636939e8249cbf46"": [0] remote missing object 1d70940bd9d7c6c862304c66d64233726dc30342ae7032a4636939e8249cbf46; error transferring ""bd17c3a98f7651b4e7ee54d875c47ec12e18b75daf79b3744a2590ddb0d6b44d"": [0] remote missing object bd17c3a98f7651b4e7ee54d875c47ec12e18b75daf79b3744a2590ddb0d6b44d; error transferring ""6f663a2fdbcde0addc5cb755f7af5d4c19bed92dccfd20e25b2acf2bc8c2ca7c"": [0] remote missing object 6f663a2fdbcde0addc5cb755f7af5d4c19bed92dccfd20e25b2acf2bc8c2ca7c; error transferring ""e38e09cfe7b7ffbc80dce4972bc9c382148520147d46738a3f6f3235b2d876c6"": [0] remote missing object e38e09cfe7b7ffbc80dce4972bc9c382148520147d46738a3f6f3235b2d876c6; error transferring ""4ed7feb0343e9ac03135b1456b2c8d2edab1b359c4950908c4d44152c0634a89"": [0] remote missing object 4ed7feb0343e9ac03135b1456b2c8d2edab1b359c4950908c4d44152c0634a89; error transferring ""eda2517817b23238c2b28f69a1fa39e9b85b45985854f0a5d5508280e76da39e"": [0] remote missing object eda2517817b23238c2b28f69a1fa39e9b85b45985854f0a5d5508280e76da39e; error transferring ""5e69a86f301ab9ab0d507ad7659abb4ad3732382ccbeb714db497e51eb3cf87b"": [0] remote missing object 5e69a86f301ab9ab0d507ad7659abb4ad3732382ccbeb714db497e51eb3cf87b; Failed to fetch some objects from 'file:///startdir/gatk'. ```. #### Expected behavior; can be compiled. #### Actual behavior; see above",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8320
https://github.com/broadinstitute/gatk/issues/8320:90,Deployability,release,release,90,"## Bug Report. - OS: Arch Linux; - Java: 17. ### Affected version(s); - [x] Latest public release version [version?]. ### Description . Firstly, I run `./gradle localJar`. ```; Downloading https://services.gradle.org/distributions/gradle-7.5.1-bin.zip; ...........10%............20%...........30%............40%...........50%............60%...........70%............80%...........90%............100%. Welcome to Gradle 7.5.1!. Here are the highlights of this release:; - Support for Java 18; - Support for building with Groovy 4; - Much more responsive continuous builds; - Improved diagnostics for dependency resolution. For more details see https://docs.gradle.org/7.5.1/release-notes.html. Starting a Gradle Daemon (subsequent builds will be faster). > Configure project :; Executing: git lfs pull --include src/main/resources/large. FAILURE: Build failed with an exception. * Where:; Build file '/build/gatk/src/gatk/build.gradle' line: 104. * What went wrong:; A problem occurred evaluating root project 'gatk'.; > Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 2. git-lfs is required to build GATK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK. * Try:; > Run with --stacktrace option to get the stack trace.; > Run with --info or --debug option to get more log output.; > Run with --scan to get full insights. * Get more help at https://help.gradle.org. BUILD FAILED in 17s; ```; However, I already install git-lfs; ```; git-lfs usr/; git-lfs usr/bin/; git-lfs usr/bin/git-lfs; git-lfs usr/share/; git-lfs usr/share/licenses/; git-lfs usr/share/licenses/git-lfs/; git-lfs usr/share/licenses/git-lfs/LICENSE; git-lfs usr/share/man/; git-lfs usr/share/man/man1/; git-lfs usr/share/man/man1/git-lfs-checkout.1.gz; git-lfs usr/share/man/man1/git-lfs-clean.1.gz; git-lfs usr/share/man/man1/git-lfs-clone.1.gz; git-lfs usr/share/man/man1/git-lfs-dedup.1.gz; git-lfs usr/share/man/man1/git-lfs-",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8320
https://github.com/broadinstitute/gatk/issues/8320:459,Deployability,release,release,459,"## Bug Report. - OS: Arch Linux; - Java: 17. ### Affected version(s); - [x] Latest public release version [version?]. ### Description . Firstly, I run `./gradle localJar`. ```; Downloading https://services.gradle.org/distributions/gradle-7.5.1-bin.zip; ...........10%............20%...........30%............40%...........50%............60%...........70%............80%...........90%............100%. Welcome to Gradle 7.5.1!. Here are the highlights of this release:; - Support for Java 18; - Support for building with Groovy 4; - Much more responsive continuous builds; - Improved diagnostics for dependency resolution. For more details see https://docs.gradle.org/7.5.1/release-notes.html. Starting a Gradle Daemon (subsequent builds will be faster). > Configure project :; Executing: git lfs pull --include src/main/resources/large. FAILURE: Build failed with an exception. * Where:; Build file '/build/gatk/src/gatk/build.gradle' line: 104. * What went wrong:; A problem occurred evaluating root project 'gatk'.; > Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 2. git-lfs is required to build GATK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK. * Try:; > Run with --stacktrace option to get the stack trace.; > Run with --info or --debug option to get more log output.; > Run with --scan to get full insights. * Get more help at https://help.gradle.org. BUILD FAILED in 17s; ```; However, I already install git-lfs; ```; git-lfs usr/; git-lfs usr/bin/; git-lfs usr/bin/git-lfs; git-lfs usr/share/; git-lfs usr/share/licenses/; git-lfs usr/share/licenses/git-lfs/; git-lfs usr/share/licenses/git-lfs/LICENSE; git-lfs usr/share/man/; git-lfs usr/share/man/man1/; git-lfs usr/share/man/man1/git-lfs-checkout.1.gz; git-lfs usr/share/man/man1/git-lfs-clean.1.gz; git-lfs usr/share/man/man1/git-lfs-clone.1.gz; git-lfs usr/share/man/man1/git-lfs-dedup.1.gz; git-lfs usr/share/man/man1/git-lfs-",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8320
https://github.com/broadinstitute/gatk/issues/8320:553,Deployability,continuous,continuous,553,"## Bug Report. - OS: Arch Linux; - Java: 17. ### Affected version(s); - [x] Latest public release version [version?]. ### Description . Firstly, I run `./gradle localJar`. ```; Downloading https://services.gradle.org/distributions/gradle-7.5.1-bin.zip; ...........10%............20%...........30%............40%...........50%............60%...........70%............80%...........90%............100%. Welcome to Gradle 7.5.1!. Here are the highlights of this release:; - Support for Java 18; - Support for building with Groovy 4; - Much more responsive continuous builds; - Improved diagnostics for dependency resolution. For more details see https://docs.gradle.org/7.5.1/release-notes.html. Starting a Gradle Daemon (subsequent builds will be faster). > Configure project :; Executing: git lfs pull --include src/main/resources/large. FAILURE: Build failed with an exception. * Where:; Build file '/build/gatk/src/gatk/build.gradle' line: 104. * What went wrong:; A problem occurred evaluating root project 'gatk'.; > Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 2. git-lfs is required to build GATK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK. * Try:; > Run with --stacktrace option to get the stack trace.; > Run with --info or --debug option to get more log output.; > Run with --scan to get full insights. * Get more help at https://help.gradle.org. BUILD FAILED in 17s; ```; However, I already install git-lfs; ```; git-lfs usr/; git-lfs usr/bin/; git-lfs usr/bin/git-lfs; git-lfs usr/share/; git-lfs usr/share/licenses/; git-lfs usr/share/licenses/git-lfs/; git-lfs usr/share/licenses/git-lfs/LICENSE; git-lfs usr/share/man/; git-lfs usr/share/man/man1/; git-lfs usr/share/man/man1/git-lfs-checkout.1.gz; git-lfs usr/share/man/man1/git-lfs-clean.1.gz; git-lfs usr/share/man/man1/git-lfs-clone.1.gz; git-lfs usr/share/man/man1/git-lfs-dedup.1.gz; git-lfs usr/share/man/man1/git-lfs-",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8320
https://github.com/broadinstitute/gatk/issues/8320:673,Deployability,release,release-notes,673,"## Bug Report. - OS: Arch Linux; - Java: 17. ### Affected version(s); - [x] Latest public release version [version?]. ### Description . Firstly, I run `./gradle localJar`. ```; Downloading https://services.gradle.org/distributions/gradle-7.5.1-bin.zip; ...........10%............20%...........30%............40%...........50%............60%...........70%............80%...........90%............100%. Welcome to Gradle 7.5.1!. Here are the highlights of this release:; - Support for Java 18; - Support for building with Groovy 4; - Much more responsive continuous builds; - Improved diagnostics for dependency resolution. For more details see https://docs.gradle.org/7.5.1/release-notes.html. Starting a Gradle Daemon (subsequent builds will be faster). > Configure project :; Executing: git lfs pull --include src/main/resources/large. FAILURE: Build failed with an exception. * Where:; Build file '/build/gatk/src/gatk/build.gradle' line: 104. * What went wrong:; A problem occurred evaluating root project 'gatk'.; > Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 2. git-lfs is required to build GATK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK. * Try:; > Run with --stacktrace option to get the stack trace.; > Run with --info or --debug option to get more log output.; > Run with --scan to get full insights. * Get more help at https://help.gradle.org. BUILD FAILED in 17s; ```; However, I already install git-lfs; ```; git-lfs usr/; git-lfs usr/bin/; git-lfs usr/bin/git-lfs; git-lfs usr/share/; git-lfs usr/share/licenses/; git-lfs usr/share/licenses/git-lfs/; git-lfs usr/share/licenses/git-lfs/LICENSE; git-lfs usr/share/man/; git-lfs usr/share/man/man1/; git-lfs usr/share/man/man1/git-lfs-checkout.1.gz; git-lfs usr/share/man/man1/git-lfs-clean.1.gz; git-lfs usr/share/man/man1/git-lfs-clone.1.gz; git-lfs usr/share/man/man1/git-lfs-dedup.1.gz; git-lfs usr/share/man/man1/git-lfs-",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8320
https://github.com/broadinstitute/gatk/issues/8320:1158,Deployability,install,installed,1158,"rstly, I run `./gradle localJar`. ```; Downloading https://services.gradle.org/distributions/gradle-7.5.1-bin.zip; ...........10%............20%...........30%............40%...........50%............60%...........70%............80%...........90%............100%. Welcome to Gradle 7.5.1!. Here are the highlights of this release:; - Support for Java 18; - Support for building with Groovy 4; - Much more responsive continuous builds; - Improved diagnostics for dependency resolution. For more details see https://docs.gradle.org/7.5.1/release-notes.html. Starting a Gradle Daemon (subsequent builds will be faster). > Configure project :; Executing: git lfs pull --include src/main/resources/large. FAILURE: Build failed with an exception. * Where:; Build file '/build/gatk/src/gatk/build.gradle' line: 104. * What went wrong:; A problem occurred evaluating root project 'gatk'.; > Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 2. git-lfs is required to build GATK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK. * Try:; > Run with --stacktrace option to get the stack trace.; > Run with --info or --debug option to get more log output.; > Run with --scan to get full insights. * Get more help at https://help.gradle.org. BUILD FAILED in 17s; ```; However, I already install git-lfs; ```; git-lfs usr/; git-lfs usr/bin/; git-lfs usr/bin/git-lfs; git-lfs usr/share/; git-lfs usr/share/licenses/; git-lfs usr/share/licenses/git-lfs/; git-lfs usr/share/licenses/git-lfs/LICENSE; git-lfs usr/share/man/; git-lfs usr/share/man/man1/; git-lfs usr/share/man/man1/git-lfs-checkout.1.gz; git-lfs usr/share/man/man1/git-lfs-clean.1.gz; git-lfs usr/share/man/man1/git-lfs-clone.1.gz; git-lfs usr/share/man/man1/git-lfs-dedup.1.gz; git-lfs usr/share/man/man1/git-lfs-env.1.gz; git-lfs usr/share/man/man1/git-lfs-ext.1.gz; git-lfs usr/share/man/man1/git-lfs-fetch.1.gz; git-lfs usr/share/man/man1/git-lfs-",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8320
https://github.com/broadinstitute/gatk/issues/8320:1513,Deployability,install,install,1513,"https://docs.gradle.org/7.5.1/release-notes.html. Starting a Gradle Daemon (subsequent builds will be faster). > Configure project :; Executing: git lfs pull --include src/main/resources/large. FAILURE: Build failed with an exception. * Where:; Build file '/build/gatk/src/gatk/build.gradle' line: 104. * What went wrong:; A problem occurred evaluating root project 'gatk'.; > Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 2. git-lfs is required to build GATK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK. * Try:; > Run with --stacktrace option to get the stack trace.; > Run with --info or --debug option to get more log output.; > Run with --scan to get full insights. * Get more help at https://help.gradle.org. BUILD FAILED in 17s; ```; However, I already install git-lfs; ```; git-lfs usr/; git-lfs usr/bin/; git-lfs usr/bin/git-lfs; git-lfs usr/share/; git-lfs usr/share/licenses/; git-lfs usr/share/licenses/git-lfs/; git-lfs usr/share/licenses/git-lfs/LICENSE; git-lfs usr/share/man/; git-lfs usr/share/man/man1/; git-lfs usr/share/man/man1/git-lfs-checkout.1.gz; git-lfs usr/share/man/man1/git-lfs-clean.1.gz; git-lfs usr/share/man/man1/git-lfs-clone.1.gz; git-lfs usr/share/man/man1/git-lfs-dedup.1.gz; git-lfs usr/share/man/man1/git-lfs-env.1.gz; git-lfs usr/share/man/man1/git-lfs-ext.1.gz; git-lfs usr/share/man/man1/git-lfs-fetch.1.gz; git-lfs usr/share/man/man1/git-lfs-filter-process.1.gz; git-lfs usr/share/man/man1/git-lfs-fsck.1.gz; git-lfs usr/share/man/man1/git-lfs-install.1.gz; git-lfs usr/share/man/man1/git-lfs-lock.1.gz; git-lfs usr/share/man/man1/git-lfs-locks.1.gz; git-lfs usr/share/man/man1/git-lfs-logs.1.gz; git-lfs usr/share/man/man1/git-lfs-ls-files.1.gz; git-lfs usr/share/man/man1/git-lfs-merge-driver.1.gz; git-lfs usr/share/man/man1/git-lfs-migrate.1.gz; git-lfs usr/share/man/man1/git-lfs-pointer.1.gz; git-lfs usr/share/man/man1/git-lfs-post-checkout",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8320
https://github.com/broadinstitute/gatk/issues/8320:2240,Deployability,install,install,2240," information on how to build GATK. * Try:; > Run with --stacktrace option to get the stack trace.; > Run with --info or --debug option to get more log output.; > Run with --scan to get full insights. * Get more help at https://help.gradle.org. BUILD FAILED in 17s; ```; However, I already install git-lfs; ```; git-lfs usr/; git-lfs usr/bin/; git-lfs usr/bin/git-lfs; git-lfs usr/share/; git-lfs usr/share/licenses/; git-lfs usr/share/licenses/git-lfs/; git-lfs usr/share/licenses/git-lfs/LICENSE; git-lfs usr/share/man/; git-lfs usr/share/man/man1/; git-lfs usr/share/man/man1/git-lfs-checkout.1.gz; git-lfs usr/share/man/man1/git-lfs-clean.1.gz; git-lfs usr/share/man/man1/git-lfs-clone.1.gz; git-lfs usr/share/man/man1/git-lfs-dedup.1.gz; git-lfs usr/share/man/man1/git-lfs-env.1.gz; git-lfs usr/share/man/man1/git-lfs-ext.1.gz; git-lfs usr/share/man/man1/git-lfs-fetch.1.gz; git-lfs usr/share/man/man1/git-lfs-filter-process.1.gz; git-lfs usr/share/man/man1/git-lfs-fsck.1.gz; git-lfs usr/share/man/man1/git-lfs-install.1.gz; git-lfs usr/share/man/man1/git-lfs-lock.1.gz; git-lfs usr/share/man/man1/git-lfs-locks.1.gz; git-lfs usr/share/man/man1/git-lfs-logs.1.gz; git-lfs usr/share/man/man1/git-lfs-ls-files.1.gz; git-lfs usr/share/man/man1/git-lfs-merge-driver.1.gz; git-lfs usr/share/man/man1/git-lfs-migrate.1.gz; git-lfs usr/share/man/man1/git-lfs-pointer.1.gz; git-lfs usr/share/man/man1/git-lfs-post-checkout.1.gz; git-lfs usr/share/man/man1/git-lfs-post-commit.1.gz; git-lfs usr/share/man/man1/git-lfs-post-merge.1.gz; git-lfs usr/share/man/man1/git-lfs-pre-push.1.gz; git-lfs usr/share/man/man1/git-lfs-prune.1.gz; git-lfs usr/share/man/man1/git-lfs-pull.1.gz; git-lfs usr/share/man/man1/git-lfs-push.1.gz; git-lfs usr/share/man/man1/git-lfs-smudge.1.gz; git-lfs usr/share/man/man1/git-lfs-standalone-file.1.gz; git-lfs usr/share/man/man1/git-lfs-status.1.gz; git-lfs usr/share/man/man1/git-lfs-track.1.gz; git-lfs usr/share/man/man1/git-lfs-uninstall.1.gz; git-lfs usr/share/man/man1/git",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8320
https://github.com/broadinstitute/gatk/issues/8320:3327,Deployability,update,update,3327,"/share/man/man1/git-lfs-locks.1.gz; git-lfs usr/share/man/man1/git-lfs-logs.1.gz; git-lfs usr/share/man/man1/git-lfs-ls-files.1.gz; git-lfs usr/share/man/man1/git-lfs-merge-driver.1.gz; git-lfs usr/share/man/man1/git-lfs-migrate.1.gz; git-lfs usr/share/man/man1/git-lfs-pointer.1.gz; git-lfs usr/share/man/man1/git-lfs-post-checkout.1.gz; git-lfs usr/share/man/man1/git-lfs-post-commit.1.gz; git-lfs usr/share/man/man1/git-lfs-post-merge.1.gz; git-lfs usr/share/man/man1/git-lfs-pre-push.1.gz; git-lfs usr/share/man/man1/git-lfs-prune.1.gz; git-lfs usr/share/man/man1/git-lfs-pull.1.gz; git-lfs usr/share/man/man1/git-lfs-push.1.gz; git-lfs usr/share/man/man1/git-lfs-smudge.1.gz; git-lfs usr/share/man/man1/git-lfs-standalone-file.1.gz; git-lfs usr/share/man/man1/git-lfs-status.1.gz; git-lfs usr/share/man/man1/git-lfs-track.1.gz; git-lfs usr/share/man/man1/git-lfs-uninstall.1.gz; git-lfs usr/share/man/man1/git-lfs-unlock.1.gz; git-lfs usr/share/man/man1/git-lfs-untrack.1.gz; git-lfs usr/share/man/man1/git-lfs-update.1.gz; git-lfs usr/share/man/man1/git-lfs.1.gz; git-lfs usr/share/man/man5/; git-lfs usr/share/man/man5/git-lfs-config.5.gz; ```. Then I run ; ```; git lfs pull --include src/main/resources/large; ./gradle localJar; ```; then; ```; error transferring ""1d70940bd9d7c6c862304c66d64233726dc30342ae7032a4636939e8249cbf46"": [0] remote missing object 1d70940bd9d7c6c862304c66d64233726dc30342ae7032a4636939e8249cbf46; error transferring ""bd17c3a98f7651b4e7ee54d875c47ec12e18b75daf79b3744a2590ddb0d6b44d"": [0] remote missing object bd17c3a98f7651b4e7ee54d875c47ec12e18b75daf79b3744a2590ddb0d6b44d; error transferring ""6f663a2fdbcde0addc5cb755f7af5d4c19bed92dccfd20e25b2acf2bc8c2ca7c"": [0] remote missing object 6f663a2fdbcde0addc5cb755f7af5d4c19bed92dccfd20e25b2acf2bc8c2ca7c; error transferring ""e38e09cfe7b7ffbc80dce4972bc9c382148520147d46738a3f6f3235b2d876c6"": [0] remote missing object e38e09cfe7b7ffbc80dce4972bc9c382148520147d46738a3f6f3235b2d876c6; error transferring ""4ed7feb034",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8320
https://github.com/broadinstitute/gatk/issues/8320:599,Integrability,depend,dependency,599,"## Bug Report. - OS: Arch Linux; - Java: 17. ### Affected version(s); - [x] Latest public release version [version?]. ### Description . Firstly, I run `./gradle localJar`. ```; Downloading https://services.gradle.org/distributions/gradle-7.5.1-bin.zip; ...........10%............20%...........30%............40%...........50%............60%...........70%............80%...........90%............100%. Welcome to Gradle 7.5.1!. Here are the highlights of this release:; - Support for Java 18; - Support for building with Groovy 4; - Much more responsive continuous builds; - Improved diagnostics for dependency resolution. For more details see https://docs.gradle.org/7.5.1/release-notes.html. Starting a Gradle Daemon (subsequent builds will be faster). > Configure project :; Executing: git lfs pull --include src/main/resources/large. FAILURE: Build failed with an exception. * Where:; Build file '/build/gatk/src/gatk/build.gradle' line: 104. * What went wrong:; A problem occurred evaluating root project 'gatk'.; > Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 2. git-lfs is required to build GATK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK. * Try:; > Run with --stacktrace option to get the stack trace.; > Run with --info or --debug option to get more log output.; > Run with --scan to get full insights. * Get more help at https://help.gradle.org. BUILD FAILED in 17s; ```; However, I already install git-lfs; ```; git-lfs usr/; git-lfs usr/bin/; git-lfs usr/bin/git-lfs; git-lfs usr/share/; git-lfs usr/share/licenses/; git-lfs usr/share/licenses/git-lfs/; git-lfs usr/share/licenses/git-lfs/LICENSE; git-lfs usr/share/man/; git-lfs usr/share/man/man1/; git-lfs usr/share/man/man1/git-lfs-checkout.1.gz; git-lfs usr/share/man/man1/git-lfs-clean.1.gz; git-lfs usr/share/man/man1/git-lfs-clone.1.gz; git-lfs usr/share/man/man1/git-lfs-dedup.1.gz; git-lfs usr/share/man/man1/git-lfs-",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8320
https://github.com/broadinstitute/gatk/issues/8320:756,Modifiability,Config,Configure,756,"## Bug Report. - OS: Arch Linux; - Java: 17. ### Affected version(s); - [x] Latest public release version [version?]. ### Description . Firstly, I run `./gradle localJar`. ```; Downloading https://services.gradle.org/distributions/gradle-7.5.1-bin.zip; ...........10%............20%...........30%............40%...........50%............60%...........70%............80%...........90%............100%. Welcome to Gradle 7.5.1!. Here are the highlights of this release:; - Support for Java 18; - Support for building with Groovy 4; - Much more responsive continuous builds; - Improved diagnostics for dependency resolution. For more details see https://docs.gradle.org/7.5.1/release-notes.html. Starting a Gradle Daemon (subsequent builds will be faster). > Configure project :; Executing: git lfs pull --include src/main/resources/large. FAILURE: Build failed with an exception. * Where:; Build file '/build/gatk/src/gatk/build.gradle' line: 104. * What went wrong:; A problem occurred evaluating root project 'gatk'.; > Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 2. git-lfs is required to build GATK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK. * Try:; > Run with --stacktrace option to get the stack trace.; > Run with --info or --debug option to get more log output.; > Run with --scan to get full insights. * Get more help at https://help.gradle.org. BUILD FAILED in 17s; ```; However, I already install git-lfs; ```; git-lfs usr/; git-lfs usr/bin/; git-lfs usr/bin/git-lfs; git-lfs usr/share/; git-lfs usr/share/licenses/; git-lfs usr/share/licenses/git-lfs/; git-lfs usr/share/licenses/git-lfs/LICENSE; git-lfs usr/share/man/; git-lfs usr/share/man/man1/; git-lfs usr/share/man/man1/git-lfs-checkout.1.gz; git-lfs usr/share/man/man1/git-lfs-clean.1.gz; git-lfs usr/share/man/man1/git-lfs-clone.1.gz; git-lfs usr/share/man/man1/git-lfs-dedup.1.gz; git-lfs usr/share/man/man1/git-lfs-",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8320
https://github.com/broadinstitute/gatk/issues/8320:3445,Modifiability,config,config,3445,"/man1/git-lfs-ls-files.1.gz; git-lfs usr/share/man/man1/git-lfs-merge-driver.1.gz; git-lfs usr/share/man/man1/git-lfs-migrate.1.gz; git-lfs usr/share/man/man1/git-lfs-pointer.1.gz; git-lfs usr/share/man/man1/git-lfs-post-checkout.1.gz; git-lfs usr/share/man/man1/git-lfs-post-commit.1.gz; git-lfs usr/share/man/man1/git-lfs-post-merge.1.gz; git-lfs usr/share/man/man1/git-lfs-pre-push.1.gz; git-lfs usr/share/man/man1/git-lfs-prune.1.gz; git-lfs usr/share/man/man1/git-lfs-pull.1.gz; git-lfs usr/share/man/man1/git-lfs-push.1.gz; git-lfs usr/share/man/man1/git-lfs-smudge.1.gz; git-lfs usr/share/man/man1/git-lfs-standalone-file.1.gz; git-lfs usr/share/man/man1/git-lfs-status.1.gz; git-lfs usr/share/man/man1/git-lfs-track.1.gz; git-lfs usr/share/man/man1/git-lfs-uninstall.1.gz; git-lfs usr/share/man/man1/git-lfs-unlock.1.gz; git-lfs usr/share/man/man1/git-lfs-untrack.1.gz; git-lfs usr/share/man/man1/git-lfs-update.1.gz; git-lfs usr/share/man/man1/git-lfs.1.gz; git-lfs usr/share/man/man5/; git-lfs usr/share/man/man5/git-lfs-config.5.gz; ```. Then I run ; ```; git lfs pull --include src/main/resources/large; ./gradle localJar; ```; then; ```; error transferring ""1d70940bd9d7c6c862304c66d64233726dc30342ae7032a4636939e8249cbf46"": [0] remote missing object 1d70940bd9d7c6c862304c66d64233726dc30342ae7032a4636939e8249cbf46; error transferring ""bd17c3a98f7651b4e7ee54d875c47ec12e18b75daf79b3744a2590ddb0d6b44d"": [0] remote missing object bd17c3a98f7651b4e7ee54d875c47ec12e18b75daf79b3744a2590ddb0d6b44d; error transferring ""6f663a2fdbcde0addc5cb755f7af5d4c19bed92dccfd20e25b2acf2bc8c2ca7c"": [0] remote missing object 6f663a2fdbcde0addc5cb755f7af5d4c19bed92dccfd20e25b2acf2bc8c2ca7c; error transferring ""e38e09cfe7b7ffbc80dce4972bc9c382148520147d46738a3f6f3235b2d876c6"": [0] remote missing object e38e09cfe7b7ffbc80dce4972bc9c382148520147d46738a3f6f3235b2d876c6; error transferring ""4ed7feb0343e9ac03135b1456b2c8d2edab1b359c4950908c4d44152c0634a89"": [0] remote missing object 4ed7feb0343e9ac03135b",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8320
https://github.com/broadinstitute/gatk/issues/8320:1371,Testability,log,log,1371,"70%............80%...........90%............100%. Welcome to Gradle 7.5.1!. Here are the highlights of this release:; - Support for Java 18; - Support for building with Groovy 4; - Much more responsive continuous builds; - Improved diagnostics for dependency resolution. For more details see https://docs.gradle.org/7.5.1/release-notes.html. Starting a Gradle Daemon (subsequent builds will be faster). > Configure project :; Executing: git lfs pull --include src/main/resources/large. FAILURE: Build failed with an exception. * Where:; Build file '/build/gatk/src/gatk/build.gradle' line: 104. * What went wrong:; A problem occurred evaluating root project 'gatk'.; > Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 2. git-lfs is required to build GATK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK. * Try:; > Run with --stacktrace option to get the stack trace.; > Run with --info or --debug option to get more log output.; > Run with --scan to get full insights. * Get more help at https://help.gradle.org. BUILD FAILED in 17s; ```; However, I already install git-lfs; ```; git-lfs usr/; git-lfs usr/bin/; git-lfs usr/bin/git-lfs; git-lfs usr/share/; git-lfs usr/share/licenses/; git-lfs usr/share/licenses/git-lfs/; git-lfs usr/share/licenses/git-lfs/LICENSE; git-lfs usr/share/man/; git-lfs usr/share/man/man1/; git-lfs usr/share/man/man1/git-lfs-checkout.1.gz; git-lfs usr/share/man/man1/git-lfs-clean.1.gz; git-lfs usr/share/man/man1/git-lfs-clone.1.gz; git-lfs usr/share/man/man1/git-lfs-dedup.1.gz; git-lfs usr/share/man/man1/git-lfs-env.1.gz; git-lfs usr/share/man/man1/git-lfs-ext.1.gz; git-lfs usr/share/man/man1/git-lfs-fetch.1.gz; git-lfs usr/share/man/man1/git-lfs-filter-process.1.gz; git-lfs usr/share/man/man1/git-lfs-fsck.1.gz; git-lfs usr/share/man/man1/git-lfs-install.1.gz; git-lfs usr/share/man/man1/git-lfs-lock.1.gz; git-lfs usr/share/man/man1/git-lfs-locks.1.gz; git-l",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8320
https://github.com/broadinstitute/gatk/issues/8320:2382,Testability,log,logs,2382," more log output.; > Run with --scan to get full insights. * Get more help at https://help.gradle.org. BUILD FAILED in 17s; ```; However, I already install git-lfs; ```; git-lfs usr/; git-lfs usr/bin/; git-lfs usr/bin/git-lfs; git-lfs usr/share/; git-lfs usr/share/licenses/; git-lfs usr/share/licenses/git-lfs/; git-lfs usr/share/licenses/git-lfs/LICENSE; git-lfs usr/share/man/; git-lfs usr/share/man/man1/; git-lfs usr/share/man/man1/git-lfs-checkout.1.gz; git-lfs usr/share/man/man1/git-lfs-clean.1.gz; git-lfs usr/share/man/man1/git-lfs-clone.1.gz; git-lfs usr/share/man/man1/git-lfs-dedup.1.gz; git-lfs usr/share/man/man1/git-lfs-env.1.gz; git-lfs usr/share/man/man1/git-lfs-ext.1.gz; git-lfs usr/share/man/man1/git-lfs-fetch.1.gz; git-lfs usr/share/man/man1/git-lfs-filter-process.1.gz; git-lfs usr/share/man/man1/git-lfs-fsck.1.gz; git-lfs usr/share/man/man1/git-lfs-install.1.gz; git-lfs usr/share/man/man1/git-lfs-lock.1.gz; git-lfs usr/share/man/man1/git-lfs-locks.1.gz; git-lfs usr/share/man/man1/git-lfs-logs.1.gz; git-lfs usr/share/man/man1/git-lfs-ls-files.1.gz; git-lfs usr/share/man/man1/git-lfs-merge-driver.1.gz; git-lfs usr/share/man/man1/git-lfs-migrate.1.gz; git-lfs usr/share/man/man1/git-lfs-pointer.1.gz; git-lfs usr/share/man/man1/git-lfs-post-checkout.1.gz; git-lfs usr/share/man/man1/git-lfs-post-commit.1.gz; git-lfs usr/share/man/man1/git-lfs-post-merge.1.gz; git-lfs usr/share/man/man1/git-lfs-pre-push.1.gz; git-lfs usr/share/man/man1/git-lfs-prune.1.gz; git-lfs usr/share/man/man1/git-lfs-pull.1.gz; git-lfs usr/share/man/man1/git-lfs-push.1.gz; git-lfs usr/share/man/man1/git-lfs-smudge.1.gz; git-lfs usr/share/man/man1/git-lfs-standalone-file.1.gz; git-lfs usr/share/man/man1/git-lfs-status.1.gz; git-lfs usr/share/man/man1/git-lfs-track.1.gz; git-lfs usr/share/man/man1/git-lfs-uninstall.1.gz; git-lfs usr/share/man/man1/git-lfs-unlock.1.gz; git-lfs usr/share/man/man1/git-lfs-untrack.1.gz; git-lfs usr/share/man/man1/git-lfs-update.1.gz; git-lfs usr/share/man/man",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8320
https://github.com/broadinstitute/gatk/issues/8320:542,Usability,responsiv,responsive,542,"## Bug Report. - OS: Arch Linux; - Java: 17. ### Affected version(s); - [x] Latest public release version [version?]. ### Description . Firstly, I run `./gradle localJar`. ```; Downloading https://services.gradle.org/distributions/gradle-7.5.1-bin.zip; ...........10%............20%...........30%............40%...........50%............60%...........70%............80%...........90%............100%. Welcome to Gradle 7.5.1!. Here are the highlights of this release:; - Support for Java 18; - Support for building with Groovy 4; - Much more responsive continuous builds; - Improved diagnostics for dependency resolution. For more details see https://docs.gradle.org/7.5.1/release-notes.html. Starting a Gradle Daemon (subsequent builds will be faster). > Configure project :; Executing: git lfs pull --include src/main/resources/large. FAILURE: Build failed with an exception. * Where:; Build file '/build/gatk/src/gatk/build.gradle' line: 104. * What went wrong:; A problem occurred evaluating root project 'gatk'.; > Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 2. git-lfs is required to build GATK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK. * Try:; > Run with --stacktrace option to get the stack trace.; > Run with --info or --debug option to get more log output.; > Run with --scan to get full insights. * Get more help at https://help.gradle.org. BUILD FAILED in 17s; ```; However, I already install git-lfs; ```; git-lfs usr/; git-lfs usr/bin/; git-lfs usr/bin/git-lfs; git-lfs usr/share/; git-lfs usr/share/licenses/; git-lfs usr/share/licenses/git-lfs/; git-lfs usr/share/licenses/git-lfs/LICENSE; git-lfs usr/share/man/; git-lfs usr/share/man/man1/; git-lfs usr/share/man/man1/git-lfs-checkout.1.gz; git-lfs usr/share/man/man1/git-lfs-clean.1.gz; git-lfs usr/share/man/man1/git-lfs-clone.1.gz; git-lfs usr/share/man/man1/git-lfs-dedup.1.gz; git-lfs usr/share/man/man1/git-lfs-",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8320
https://github.com/broadinstitute/gatk/pull/8321:162,Performance,Load,LoadData,162,"- adds three new tables: vcf_header_lines_temp, vcf_header_lines, and sample_vcf_header; - populates vcf_header_lines_temp table during CreateVariantIngestFiles (LoadData WDL task); - parses data in vcf_header_lines_temp table in python script and populates vcf_header_lines and sample_vcf_header tables, then cleans up (ProcessVCFHeaders WDL task); - fixed ""bug"" where BigQueryUtils.doRowsExistFor() assumed value was a String that was really an Long, did not work for actual String values; - all behind feature flag (set to false by default) so to not break Beta. Successful run of `GvsJointVariantCalling`: https://job-manager.dsde-prod.broadinstitute.org/jobs/7d5e7b30-7b7c-475c-bf4e-86d6d38cfc8d; Successful run of `GvsQuickstartVcfIntegration`: https://job-manager.dsde-prod.broadinstitute.org/jobs/a17e171e-f8a7-465b-8214-52630f9ec9d1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8321
https://github.com/broadinstitute/gatk/pull/8325:53,Security,validat,validation,53,"Successful run on the quickstart with the new python validation; https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20cremer/job_history/475e425d-5be5-47a7-a2ed-ebbdcb3746d5. Successful run on the 3k callset with samples sets; https://app.terra.bio/#workspaces/allofus-drc-wgs-dev/NHGRI_AnVIL_3K%20Cremer/job_history/1db17d59-c221-4345-be15-7fed27358d6f. Sample set with 3202 samples; <img width=""435"" alt=""Screen Shot 2023-05-22 at 11 48 11 PM"" src=""https://github.com/broadinstitute/gatk/assets/6863459/de5683e3-b927-4fb3-bb06-ff4adc7b5435"">. <img width=""1078"" alt=""Screen Shot 2023-05-23 at 12 23 05 AM"" src=""https://github.com/broadinstitute/gatk/assets/6863459/976a382a-9953-42e7-a53a-0a7a4d15a9b8"">",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8325
https://github.com/broadinstitute/gatk/pull/8326:162,Deployability,patch,patch,162,"The empty alts that results from overtrimming in an alt allele UDF correlate strongly with VAT / alt allele mismatches, but from trial runs with and without this patch they don't seem to actually be causing the mismatches. Nonetheless I think this is a desirable change as it keeps VAT VIDs from getting weird. VIDs have the format `<contig>-<position>-<ref>-<alt>`; if the alt is empty the VID just ends in a dash which just seems like an edge case waiting to break something.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8326
https://github.com/broadinstitute/gatk/issues/8327:1201,Performance,cache,cacheCopy,1201,In the discussion in this branch https://github.com/broadinstitute/gatk/pull/6351#pullrequestreview-1430841832 we were tripped up by the fact that the Carrot tests were showing a slight (between 5 and 7% on the aggregated `$ time` command output across 50 shards) runtime regression in the current version of HaplotypeCaller compared with a misconfigured older version of the tool. Specifically the faster older version was broadinstitute/gatk-nightly:2022-03-04-4.2.5.0-9-gb097f75c5-NIGHTLY-SNAPSHOT which was before the Java 17 migration (which is a high likelihood culprit form the past year). . Somebody should spend a few hours with a profiler to make sure there isn't some obvious culprit. . Here is the command that Carrot was running:; ```HaplotypeCaller \; -R /cromwell_root/dsp-methods-carrot-data/test_data/haplotypecaller_tests/Homo_sapiens_assembly38.fasta \; -I gs://dsp-methods-carrot-data/test_data/haplotypecaller_tests/chm1_chm13_hiseqx_sm_hf3mo.bam \; -L /cromwell_root/dsde-methods-carrot-prod-cromwell/VariantCallingCarrotOrchestrated/9886a710-334a-41eb-a495-6968d322730a/call-CHMSampleHeadToHead/VariantCallingCarrot/63594353-145d-4c4a-a713-352ad41ff3e6/call-ScatterIntervalList/cacheCopy/glob-cb4648beeaff920acb03de7603c06f98/10scattered.interval_list \; -O CHM113.g.vcf.gz \; -contamination 0.0 \; -G StandardAnnotation -G StandardHCAnnotation -G AS_StandardAnnotation \; \; \; \; -GQB 10 -GQB 20 -GQB 30 -GQB 40 -GQB 50 -GQB 60 -GQB 70 -GQB 80 -GQB 90 \; -ERC GVCF \; ```; And a shard where a significant slowdown was observed spanned the region `chr3:55313816` -> `chr3:113699078` which should hopefully provide a good starting point for anybody investigating this.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8327
https://github.com/broadinstitute/gatk/issues/8327:158,Testability,test,tests,158,In the discussion in this branch https://github.com/broadinstitute/gatk/pull/6351#pullrequestreview-1430841832 we were tripped up by the fact that the Carrot tests were showing a slight (between 5 and 7% on the aggregated `$ time` command output across 50 shards) runtime regression in the current version of HaplotypeCaller compared with a misconfigured older version of the tool. Specifically the faster older version was broadinstitute/gatk-nightly:2022-03-04-4.2.5.0-9-gb097f75c5-NIGHTLY-SNAPSHOT which was before the Java 17 migration (which is a high likelihood culprit form the past year). . Somebody should spend a few hours with a profiler to make sure there isn't some obvious culprit. . Here is the command that Carrot was running:; ```HaplotypeCaller \; -R /cromwell_root/dsp-methods-carrot-data/test_data/haplotypecaller_tests/Homo_sapiens_assembly38.fasta \; -I gs://dsp-methods-carrot-data/test_data/haplotypecaller_tests/chm1_chm13_hiseqx_sm_hf3mo.bam \; -L /cromwell_root/dsde-methods-carrot-prod-cromwell/VariantCallingCarrotOrchestrated/9886a710-334a-41eb-a495-6968d322730a/call-CHMSampleHeadToHead/VariantCallingCarrot/63594353-145d-4c4a-a713-352ad41ff3e6/call-ScatterIntervalList/cacheCopy/glob-cb4648beeaff920acb03de7603c06f98/10scattered.interval_list \; -O CHM113.g.vcf.gz \; -contamination 0.0 \; -G StandardAnnotation -G StandardHCAnnotation -G AS_StandardAnnotation \; \; \; \; -GQB 10 -GQB 20 -GQB 30 -GQB 40 -GQB 50 -GQB 60 -GQB 70 -GQB 80 -GQB 90 \; -ERC GVCF \; ```; And a shard where a significant slowdown was observed spanned the region `chr3:55313816` -> `chr3:113699078` which should hopefully provide a good starting point for anybody investigating this.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8327
https://github.com/broadinstitute/gatk/pull/8330:11,Deployability,integrat,integration,11,"Successful integration run; https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20cremer/job_history/4caeb3e1-6c8f-4547-a334-b3264f2aed95. We initially changed the name of the method (since import_gvs is a bit misleading inside our repo) but because it looks like there are still external changes being made, we decided to keep the name consistent with Tim/Hail's chosen one.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8330
https://github.com/broadinstitute/gatk/pull/8330:11,Integrability,integrat,integration,11,"Successful integration run; https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20cremer/job_history/4caeb3e1-6c8f-4547-a334-b3264f2aed95. We initially changed the name of the method (since import_gvs is a bit misleading inside our repo) but because it looks like there are still external changes being made, we decided to keep the name consistent with Tim/Hail's chosen one.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8330
https://github.com/broadinstitute/gatk/pull/8332:296,Safety,safe,safe,296,"* VariantContexts from an assembled haplotype's EventMap, or found from pileups, that are really containers for a single alt allele, are explicitly marked as such. This way we don't have to keep tracing back the source of a biallelic variant context and putting in little comments about why it's safe to assume it has only one alt allele.; * Some methods that remove or add haplotypes based on alleles found in pileups have been made void methods of AssemblyResultSet, which to mind mind is the appropriate way to encapsulate transformations acting on that class.; * other random simplification of code surrounding pileup haplotypes. @jamesemery This is a warmup PR for DRAGEN stuff, a bit of housekeeping of code at the margins of partially determined haplotype logic.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8332
https://github.com/broadinstitute/gatk/pull/8332:763,Testability,log,logic,763,"* VariantContexts from an assembled haplotype's EventMap, or found from pileups, that are really containers for a single alt allele, are explicitly marked as such. This way we don't have to keep tracing back the source of a biallelic variant context and putting in little comments about why it's safe to assume it has only one alt allele.; * Some methods that remove or add haplotypes based on alleles found in pileups have been made void methods of AssemblyResultSet, which to mind mind is the appropriate way to encapsulate transformations acting on that class.; * other random simplification of code surrounding pileup haplotypes. @jamesemery This is a warmup PR for DRAGEN stuff, a bit of housekeeping of code at the margins of partially determined haplotype logic.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8332
https://github.com/broadinstitute/gatk/pull/8332:580,Usability,simpl,simplification,580,"* VariantContexts from an assembled haplotype's EventMap, or found from pileups, that are really containers for a single alt allele, are explicitly marked as such. This way we don't have to keep tracing back the source of a biallelic variant context and putting in little comments about why it's safe to assume it has only one alt allele.; * Some methods that remove or add haplotypes based on alleles found in pileups have been made void methods of AssemblyResultSet, which to mind mind is the appropriate way to encapsulate transformations acting on that class.; * other random simplification of code surrounding pileup haplotypes. @jamesemery This is a warmup PR for DRAGEN stuff, a bit of housekeeping of code at the margins of partially determined haplotype logic.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8332
https://github.com/broadinstitute/gatk/pull/8334:234,Deployability,pipeline,pipeline,234,"This handles the case we saw in Shriner's beta files where variants on chrX (and presumably chrY) were represented with a call_GT of ""1."". NOTE: SOME of these changes fixed our code to work for any ploidy, but others only changed our pipeline to work for examples with a ploidy of 1. Specifically, the changes made to . `scripts/variantstore/wdl/extract/populate_alt_allele_table.py` and ; `src/main/resources/org/broadinstitute/hellbender/tools/gvs/filtering/feature_extract.sql`. have made it work for haploid and diploid values, but we'd need to generalize the code that explicitly lists the potential values for GT. Given that we're not seeing cases with a ploidy above 2 yet, doing that can be for a later ticket. Doc with steps I went through to test this:; https://docs.google.com/document/d/1F194j7OQh9ehs5pSdt5yHcsSWrm3WmqDlVVkDFkULuw/edit#heading=h.464spie271ew. Successful extract here:; https://app.terra.bio/#workspaces/gvs-dev/GVS%20Tiny%20Quickstart%20hatcher/job_history/a7cc6ffb-fd98-4142-a211-8235dea10b35. Successful integration run here:; https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/8d09d70e-a6f3-42a8-9c81-95065c653f4d",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8334
https://github.com/broadinstitute/gatk/pull/8334:1036,Deployability,integrat,integration,1036,"This handles the case we saw in Shriner's beta files where variants on chrX (and presumably chrY) were represented with a call_GT of ""1."". NOTE: SOME of these changes fixed our code to work for any ploidy, but others only changed our pipeline to work for examples with a ploidy of 1. Specifically, the changes made to . `scripts/variantstore/wdl/extract/populate_alt_allele_table.py` and ; `src/main/resources/org/broadinstitute/hellbender/tools/gvs/filtering/feature_extract.sql`. have made it work for haploid and diploid values, but we'd need to generalize the code that explicitly lists the potential values for GT. Given that we're not seeing cases with a ploidy above 2 yet, doing that can be for a later ticket. Doc with steps I went through to test this:; https://docs.google.com/document/d/1F194j7OQh9ehs5pSdt5yHcsSWrm3WmqDlVVkDFkULuw/edit#heading=h.464spie271ew. Successful extract here:; https://app.terra.bio/#workspaces/gvs-dev/GVS%20Tiny%20Quickstart%20hatcher/job_history/a7cc6ffb-fd98-4142-a211-8235dea10b35. Successful integration run here:; https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/8d09d70e-a6f3-42a8-9c81-95065c653f4d",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8334
https://github.com/broadinstitute/gatk/pull/8334:1036,Integrability,integrat,integration,1036,"This handles the case we saw in Shriner's beta files where variants on chrX (and presumably chrY) were represented with a call_GT of ""1."". NOTE: SOME of these changes fixed our code to work for any ploidy, but others only changed our pipeline to work for examples with a ploidy of 1. Specifically, the changes made to . `scripts/variantstore/wdl/extract/populate_alt_allele_table.py` and ; `src/main/resources/org/broadinstitute/hellbender/tools/gvs/filtering/feature_extract.sql`. have made it work for haploid and diploid values, but we'd need to generalize the code that explicitly lists the potential values for GT. Given that we're not seeing cases with a ploidy above 2 yet, doing that can be for a later ticket. Doc with steps I went through to test this:; https://docs.google.com/document/d/1F194j7OQh9ehs5pSdt5yHcsSWrm3WmqDlVVkDFkULuw/edit#heading=h.464spie271ew. Successful extract here:; https://app.terra.bio/#workspaces/gvs-dev/GVS%20Tiny%20Quickstart%20hatcher/job_history/a7cc6ffb-fd98-4142-a211-8235dea10b35. Successful integration run here:; https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/8d09d70e-a6f3-42a8-9c81-95065c653f4d",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8334
https://github.com/broadinstitute/gatk/pull/8334:752,Testability,test,test,752,"This handles the case we saw in Shriner's beta files where variants on chrX (and presumably chrY) were represented with a call_GT of ""1."". NOTE: SOME of these changes fixed our code to work for any ploidy, but others only changed our pipeline to work for examples with a ploidy of 1. Specifically, the changes made to . `scripts/variantstore/wdl/extract/populate_alt_allele_table.py` and ; `src/main/resources/org/broadinstitute/hellbender/tools/gvs/filtering/feature_extract.sql`. have made it work for haploid and diploid values, but we'd need to generalize the code that explicitly lists the potential values for GT. Given that we're not seeing cases with a ploidy above 2 yet, doing that can be for a later ticket. Doc with steps I went through to test this:; https://docs.google.com/document/d/1F194j7OQh9ehs5pSdt5yHcsSWrm3WmqDlVVkDFkULuw/edit#heading=h.464spie271ew. Successful extract here:; https://app.terra.bio/#workspaces/gvs-dev/GVS%20Tiny%20Quickstart%20hatcher/job_history/a7cc6ffb-fd98-4142-a211-8235dea10b35. Successful integration run here:; https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/8d09d70e-a6f3-42a8-9c81-95065c653f4d",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8334
https://github.com/broadinstitute/gatk/issues/8335:989,Availability,down,down,989,"It seems that a [few users](https://gatk.broadinstitute.org/hc/en-us/community/posts/11440622639387-Unable-to-trim-uncertain-bases-without-flow-order-information?page=1#community_comment_12925222020763) have complained when they try to use LongReads that their minimap2 bams are failing with the message: ; `org.broadinstitute.hellbender.exceptions.GATKException: Unable to trim uncertain bases without flow order information`. It looks like in `AssemblyBasedCallerUtils.java:147` we are calling `FlowBasedReadUtils.isFlow(originalRead)` on every read and the presence of the `tp` read tag in reads is considered sufficient to flag reads as being flow based which finally causes them to fail. This check was probably misguided, we should really be checking flow-based identity (at this stage anyway) from the readgroup in the header to prevent any spurious read tags from god knows what aligners don't cause problems like this again. Alternatively we should thread the ""isFlowBased"" check down into this part of the code so its opt-in to treat flow based reads specially when clipping here.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8335
https://github.com/broadinstitute/gatk/issues/8335:296,Integrability,message,message,296,"It seems that a [few users](https://gatk.broadinstitute.org/hc/en-us/community/posts/11440622639387-Unable-to-trim-uncertain-bases-without-flow-order-information?page=1#community_comment_12925222020763) have complained when they try to use LongReads that their minimap2 bams are failing with the message: ; `org.broadinstitute.hellbender.exceptions.GATKException: Unable to trim uncertain bases without flow order information`. It looks like in `AssemblyBasedCallerUtils.java:147` we are calling `FlowBasedReadUtils.isFlow(originalRead)` on every read and the presence of the `tp` read tag in reads is considered sufficient to flag reads as being flow based which finally causes them to fail. This check was probably misguided, we should really be checking flow-based identity (at this stage anyway) from the readgroup in the header to prevent any spurious read tags from god knows what aligners don't cause problems like this again. Alternatively we should thread the ""isFlowBased"" check down into this part of the code so its opt-in to treat flow based reads specially when clipping here.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8335
https://github.com/broadinstitute/gatk/issues/8338:1265,Safety,detect,detected,1265,"## Feature request. ### Tool(s) or class(es) involved; Log10Cache class. ### Description; Dear GATK developers,; I found different results of HaplotypeCaller with different CPUs (x64 or arm).; That differences are caused by log10 method of Math class in Log10Cache.java.; https://docs.oracle.com/en/java/javase/17/docs/api/java.base/java/lang/Math.html; I got different values of Math class log10 between x64 CPU + OpenJDK and arm CPU + OpenJDK.; StrictMath class Log10 outputs consistent values in different environments. I suggest changing from Math class to StrictMath class in Log10Cache.java.; This change has a small impact on speed, but should improve consistency of results in different environments. I placed a bam file to get different variant call results due to log10 value difference in following URL. I executed HaplotypeCaller of gatk-4.3.0.0 with openjdk-1.8.0. ```; $java -jar $gatk_jar HaplotypeCaller \; --reference /data/ref/GRCh38/Homo_sapiens_assembly38.fasta \; --input PFDATCV2HG002.pz_pipeline.GRCh38.chr20-29520758-29522758.bam \; --output $vcf \; --pcr-indel-model NONE \; -pairHMM FASTEST_AVAILABLE \; --smith-waterman FASTEST_AVAILABLE \; --native-pair-hmm-threads $n_threads \; -L chr20:29520758-29522758; ```. A following variant was detected or not due to different Log10 value. ; `chr20 29521758 . A G 55.64 . AC=1;AF=0.500;AN=2;BaseQRankSum=0.311;DP=41;ExcessHet=0.0000;FS=8.502;MLEAC=1;MLEAF=0.500;MQ=56.14;MQRankSum=-4.689;QD=1.36;ReadPosRankSum=0.020;SOR=1.886 GT:AD:DP:GQ:PL 0/1:36,5:41:63:63,0,1373`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8338
https://github.com/broadinstitute/gatk/issues/8339:104,Availability,down,download,104,"### Instructions; I use PathSeqPipelineSpark to analyze 10x Visium spatial transcribed data.; I did not download the data from the database on the GATK official website. But I prepared the database according to the tutorial [https://gatk.broadinstitute.org/hc/en-us/articles/360035889911--How-to-Run-the-Pathseq-pipeline] by myself.; The analysis has no results, and I don't know the reason for the lack of results. ## software / environment / log file informations; Using GATK jar /mnt/icfs/work/singlecelldevelopment/software/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx750g -jar /mnt/icfs/work/singlecelldevelopment/software/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar PathSeqPipelineSpark --input CRC_16/outs/possorted_genome_bam.bam --filter-bwa-image hsa_GRCh38/genome.fa.img --kmer-file hsa_GRCh38/genome.hss --min-clipped-read-length 60 --microbe-dict 16SrRNA/bacteria.16SrRNA.dict --microbe-bwa-image 16SrRNA/bacteria.16SrRNA.fa.img --taxonomy-file 16SrRNA/16SrRNA.db --output pathseq/CRC_16.pathseq.complete.bam --scores-output pathseq/CRC_16.pathseq.complete.csv --is-host-aligned false --filter-duplicates false --min-score-identity .7 --tmp-dir pathseq/tmp; 13:19:23.776 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/mnt/icfs/work/singlecelldevelopment/software/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 13:19:28.982 INFO PathSeqPipelineSpark - ------------------------------------------------------------; 13:19:28.982 INFO PathSeqPipelineSpark - The Genome Analysis Toolkit (GATK) v4.3.0.0; 13:19:28.982 INFO PathSeqPipelineSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 13:19:28.983 INFO PathSeqPipelineSpark - Executing as singlecellproject@d01.capitalbiotech.local on Linux v3.10.0-514.16.1.el7.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8339
https://github.com/broadinstitute/gatk/issues/8339:48305,Availability,failure,failures,48305,"O MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 679.0 B, free 399.8 GB); 23/05/23 13:20:18 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on d01.capitalbiotech.local:41352 (size: 679.0 B, free: 399.8 GB); 23/05/23 13:20:18 INFO SparkContext: Created broadcast 17 from broadcast at ReadsSparkSink.java:146; 23/05/23 13:20:18 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 7.3 KB, free 399.8 GB); 23/05/23 13:20:18 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 679.0 B, free 399.8 GB); 23/05/23 13:20:18 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on d01.capitalbiotech.local:41352 (size: 679.0 B, free: 399.8 GB); 23/05/23 13:20:18 INFO SparkContext: Created broadcast 18 from broadcast at BamSink.java:76; 23/05/23 13:20:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 2; 23/05/23 13:20:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 23/05/23 13:20:18 INFO SparkContext: Starting job: runJob at SparkHadoopWriter.scala:78; 23/05/23 13:20:18 INFO DAGScheduler: Registering RDD 68 (mapToPair at SparkUtils.java:161) as input to shuffle 7; 23/05/23 13:20:18 INFO DAGScheduler: Got job 6 (runJob at SparkHadoopWriter.scala:78) with 1 output partitions; 23/05/23 13:20:18 INFO DAGScheduler: Final stage: ResultStage 30 (runJob at SparkHadoopWriter.scala:78); 23/05/23 13:20:18 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 29); 23/05/23 13:20:18 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 29); 23/05/23 13:20:18 INFO DAGScheduler: Submitting ShuffleMapStage 29 (MapPartitionsRDD[68] at mapToPair at SparkUtils.java:161), which has no missing parents; 23/05/23 13:20:18 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 14.8 KB, free 399.8 GB); 23/05/23 13:20:18 INFO MemoryStore: Block br",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8339
https://github.com/broadinstitute/gatk/issues/8339:52464,Availability,failure,failures,52464,"/05/23 13:20:18 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1163; 23/05/23 13:20:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 30 (MapPartitionsRDD[73] at mapToPair at BamSink.java:91) (first 15 tasks are for partitions Vector(0)); 23/05/23 13:20:18 INFO TaskSchedulerImpl: Adding task set 30.0 with 1 tasks; 23/05/23 13:20:18 INFO TaskSetManager: Starting task 0.0 in stage 30.0 (TID 1973, localhost, executor driver, partition 0, PROCESS_LOCAL, 7662 bytes); 23/05/23 13:20:18 INFO Executor: Running task 0.0 in stage 30.0 (TID 1973); 23/05/23 13:20:18 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks; 23/05/23 13:20:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms; 23/05/23 13:20:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 2; 23/05/23 13:20:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 23/05/23 13:20:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 2; 23/05/23 13:20:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 23/05/23 13:20:18 INFO FileOutputCommitter: Saved output of task 'attempt_20230523132018_0073_r_000000_0' to file:pathseq/CRC_16.pathseq.complete.bam.parts; 23/05/23 13:20:18 INFO SparkHadoopMapRedUtil: attempt_20230523132018_0073_r_000000_0: Committed; 23/05/23 13:20:18 INFO Executor: Finished task 0.0 in stage 30.0 (TID 1973). 1149 bytes result sent to driver; 23/05/23 13:20:18 INFO TaskSetManager: Finished task 0.0 in stage 30.0 (TID 1973) in 184 ms on localhost (executor driver) (1/1); 23/05/23 13:20:18 INFO TaskSchedulerImpl: Removed TaskSet 30.0, whose tasks have all completed, from pool ; 23/05/23 13:20:18 INFO DAGScheduler: ResultStage 30 (runJob at SparkHadoopWriter.scala:78) finished i",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8339
https://github.com/broadinstitute/gatk/issues/8339:52712,Availability,failure,failures,52712,"/05/23 13:20:18 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1163; 23/05/23 13:20:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 30 (MapPartitionsRDD[73] at mapToPair at BamSink.java:91) (first 15 tasks are for partitions Vector(0)); 23/05/23 13:20:18 INFO TaskSchedulerImpl: Adding task set 30.0 with 1 tasks; 23/05/23 13:20:18 INFO TaskSetManager: Starting task 0.0 in stage 30.0 (TID 1973, localhost, executor driver, partition 0, PROCESS_LOCAL, 7662 bytes); 23/05/23 13:20:18 INFO Executor: Running task 0.0 in stage 30.0 (TID 1973); 23/05/23 13:20:18 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks; 23/05/23 13:20:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms; 23/05/23 13:20:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 2; 23/05/23 13:20:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 23/05/23 13:20:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 2; 23/05/23 13:20:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 23/05/23 13:20:18 INFO FileOutputCommitter: Saved output of task 'attempt_20230523132018_0073_r_000000_0' to file:pathseq/CRC_16.pathseq.complete.bam.parts; 23/05/23 13:20:18 INFO SparkHadoopMapRedUtil: attempt_20230523132018_0073_r_000000_0: Committed; 23/05/23 13:20:18 INFO Executor: Finished task 0.0 in stage 30.0 (TID 1973). 1149 bytes result sent to driver; 23/05/23 13:20:18 INFO TaskSetManager: Finished task 0.0 in stage 30.0 (TID 1973) in 184 ms on localhost (executor driver) (1/1); 23/05/23 13:20:18 INFO TaskSchedulerImpl: Removed TaskSet 30.0, whose tasks have all completed, from pool ; 23/05/23 13:20:18 INFO DAGScheduler: ResultStage 30 (runJob at SparkHadoopWriter.scala:78) finished i",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8339
https://github.com/broadinstitute/gatk/issues/8339:58018,Availability,down,down,58018,"ID 1976). 667 bytes result sent to driver; 23/05/23 13:20:19 INFO TaskSetManager: Finished task 66.0 in stage 31.0 (TID 2040) in 160 ms on localhost (executor driver) (1/128); 23/05/23 13:20:19 INFO TaskSetManager: Finished task 2.0 in stage 31.0 (TID 1976) in 330 ms on localhost (executor driver) (2/128); 23/05/23 13:20:19 INFO Executor: Finished task 3.0 in stage 31.0 (TID 1977). 667 bytes result sent to driver; ...; 23/05/23 13:20:19 INFO TaskSetManager: Finished task 97.0 in stage 31.0 (TID 2071) in 123 ms on localhost (executor driver) (127/128); 23/05/23 13:20:19 INFO TaskSetManager: Finished task 112.0 in stage 31.0 (TID 2086) in 88 ms on localhost (executor driver) (128/128); 23/05/23 13:20:19 INFO TaskSchedulerImpl: Removed TaskSet 31.0, whose tasks have all completed, from pool ; 23/05/23 13:20:19 INFO DAGScheduler: ResultStage 31 (foreach at BwaMemIndexCache.java:84) finished in 0.389 s; 23/05/23 13:20:19 INFO DAGScheduler: Job 7 finished: foreach at BwaMemIndexCache.java:84, took 0.392269 s; 23/05/23 13:20:19 INFO SparkUI: Stopped Spark web UI at http://d01.capitalbiotech.local:4040; 23/05/23 13:20:19 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 23/05/23 13:20:26 INFO MemoryStore: MemoryStore cleared; 23/05/23 13:20:26 INFO BlockManager: BlockManager stopped; 23/05/23 13:20:26 INFO BlockManagerMaster: BlockManagerMaster stopped; 23/05/23 13:20:26 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 23/05/23 13:20:26 INFO SparkContext: Successfully stopped SparkContext; 13:20:26.099 INFO PathSeqPipelineSpark - Shutting down engine; [May 23, 2023 1:20:26 PM CST] org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark done. Elapsed time: 1.04 minutes.; Runtime.totalMemory()=156475326464; 23/05/23 13:20:26 INFO ShutdownHookManager: Shutdown hook called; 23/05/23 13:20:26 INFO ShutdownHookManager: Deleting directory pathseq/tmp/spark-2042a18b-a4af-4a86-a236-c4914f0407a1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8339
https://github.com/broadinstitute/gatk/issues/8339:312,Deployability,pipeline,pipeline,312,"### Instructions; I use PathSeqPipelineSpark to analyze 10x Visium spatial transcribed data.; I did not download the data from the database on the GATK official website. But I prepared the database according to the tutorial [https://gatk.broadinstitute.org/hc/en-us/articles/360035889911--How-to-Run-the-Pathseq-pipeline] by myself.; The analysis has no results, and I don't know the reason for the lack of results. ## software / environment / log file informations; Using GATK jar /mnt/icfs/work/singlecelldevelopment/software/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx750g -jar /mnt/icfs/work/singlecelldevelopment/software/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar PathSeqPipelineSpark --input CRC_16/outs/possorted_genome_bam.bam --filter-bwa-image hsa_GRCh38/genome.fa.img --kmer-file hsa_GRCh38/genome.hss --min-clipped-read-length 60 --microbe-dict 16SrRNA/bacteria.16SrRNA.dict --microbe-bwa-image 16SrRNA/bacteria.16SrRNA.fa.img --taxonomy-file 16SrRNA/16SrRNA.db --output pathseq/CRC_16.pathseq.complete.bam --scores-output pathseq/CRC_16.pathseq.complete.csv --is-host-aligned false --filter-duplicates false --min-score-identity .7 --tmp-dir pathseq/tmp; 13:19:23.776 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/mnt/icfs/work/singlecelldevelopment/software/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 13:19:28.982 INFO PathSeqPipelineSpark - ------------------------------------------------------------; 13:19:28.982 INFO PathSeqPipelineSpark - The Genome Analysis Toolkit (GATK) v4.3.0.0; 13:19:28.982 INFO PathSeqPipelineSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 13:19:28.983 INFO PathSeqPipelineSpark - Executing as singlecellproject@d01.capitalbiotech.local on Linux v3.10.0-514.16.1.el7.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8339
https://github.com/broadinstitute/gatk/issues/8339:38995,Energy Efficiency,reduce,reduceByKey,38995,"20:18 INFO BlockManagerInfo: Added rdd_53_0 in memory on d01.capitalbiotech.local:41352 (size: 0.0 B, free: 399.8 GB); 23/05/23 13:20:18 INFO Executor: Finished task 0.0 in stage 15.0 (TID 1964). 1226 bytes result sent to driver; 23/05/23 13:20:18 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 1964) in 147 ms on localhost (executor driver) (1/2); 23/05/23 13:20:18 INFO Executor: Finished task 1.0 in stage 15.0 (TID 1965). 1183 bytes result sent to driver; 23/05/23 13:20:18 INFO TaskSetManager: Finished task 1.0 in stage 15.0 (TID 1965) in 146 ms on localhost (executor driver) (2/2); 23/05/23 13:20:18 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool ; 23/05/23 13:20:18 INFO DAGScheduler: ShuffleMapStage 15 (mapPartitionsToPair at PSScorer.java:68) finished in 0.185 s; 23/05/23 13:20:18 INFO DAGScheduler: looking for newly runnable stages; 23/05/23 13:20:18 INFO DAGScheduler: running: Set(); 23/05/23 13:20:18 INFO DAGScheduler: waiting: Set(ResultStage 16); 23/05/23 13:20:18 INFO DAGScheduler: failed: Set(); 23/05/23 13:20:18 INFO DAGScheduler: Submitting ResultStage 16 (ShuffledRDD[61] at reduceByKey at PSScorer.java:71), which has no missing parents; 23/05/23 13:20:18 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 4.7 KB, free 399.8 GB); 23/05/23 13:20:18 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 2.6 KB, free 399.8 GB); 23/05/23 13:20:18 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on d01.capitalbiotech.local:41352 (size: 2.6 KB, free: 399.8 GB); 23/05/23 13:20:18 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1163; 23/05/23 13:20:18 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 16 (ShuffledRDD[61] at reduceByKey at PSScorer.java:71) (first 15 tasks are for partitions Vector(0, 1)); 23/05/23 13:20:18 INFO TaskSchedulerImpl: Adding task set 16.0 with 2 tasks; 23/05/23 13:20:18 INFO ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8339
https://github.com/broadinstitute/gatk/issues/8339:39657,Energy Efficiency,reduce,reduceByKey,39657,"rtitionsToPair at PSScorer.java:68) finished in 0.185 s; 23/05/23 13:20:18 INFO DAGScheduler: looking for newly runnable stages; 23/05/23 13:20:18 INFO DAGScheduler: running: Set(); 23/05/23 13:20:18 INFO DAGScheduler: waiting: Set(ResultStage 16); 23/05/23 13:20:18 INFO DAGScheduler: failed: Set(); 23/05/23 13:20:18 INFO DAGScheduler: Submitting ResultStage 16 (ShuffledRDD[61] at reduceByKey at PSScorer.java:71), which has no missing parents; 23/05/23 13:20:18 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 4.7 KB, free 399.8 GB); 23/05/23 13:20:18 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 2.6 KB, free 399.8 GB); 23/05/23 13:20:18 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on d01.capitalbiotech.local:41352 (size: 2.6 KB, free: 399.8 GB); 23/05/23 13:20:18 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1163; 23/05/23 13:20:18 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 16 (ShuffledRDD[61] at reduceByKey at PSScorer.java:71) (first 15 tasks are for partitions Vector(0, 1)); 23/05/23 13:20:18 INFO TaskSchedulerImpl: Adding task set 16.0 with 2 tasks; 23/05/23 13:20:18 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 1966, localhost, executor driver, partition 0, PROCESS_LOCAL, 7662 bytes); 23/05/23 13:20:18 INFO TaskSetManager: Starting task 1.0 in stage 16.0 (TID 1967, localhost, executor driver, partition 1, PROCESS_LOCAL, 7662 bytes); 23/05/23 13:20:18 INFO Executor: Running task 0.0 in stage 16.0 (TID 1966); 23/05/23 13:20:18 INFO Executor: Running task 1.0 in stage 16.0 (TID 1967); 23/05/23 13:20:18 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks; 23/05/23 13:20:18 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks; 23/05/23 13:20:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms; 23/0",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8339
https://github.com/broadinstitute/gatk/issues/8339:1386,Performance,Load,Loading,1386,"reason for the lack of results. ## software / environment / log file informations; Using GATK jar /mnt/icfs/work/singlecelldevelopment/software/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx750g -jar /mnt/icfs/work/singlecelldevelopment/software/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar PathSeqPipelineSpark --input CRC_16/outs/possorted_genome_bam.bam --filter-bwa-image hsa_GRCh38/genome.fa.img --kmer-file hsa_GRCh38/genome.hss --min-clipped-read-length 60 --microbe-dict 16SrRNA/bacteria.16SrRNA.dict --microbe-bwa-image 16SrRNA/bacteria.16SrRNA.fa.img --taxonomy-file 16SrRNA/16SrRNA.db --output pathseq/CRC_16.pathseq.complete.bam --scores-output pathseq/CRC_16.pathseq.complete.csv --is-host-aligned false --filter-duplicates false --min-score-identity .7 --tmp-dir pathseq/tmp; 13:19:23.776 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/mnt/icfs/work/singlecelldevelopment/software/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 13:19:28.982 INFO PathSeqPipelineSpark - ------------------------------------------------------------; 13:19:28.982 INFO PathSeqPipelineSpark - The Genome Analysis Toolkit (GATK) v4.3.0.0; 13:19:28.982 INFO PathSeqPipelineSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 13:19:28.983 INFO PathSeqPipelineSpark - Executing as singlecellproject@d01.capitalbiotech.local on Linux v3.10.0-514.16.1.el7.x86_64 amd64; 13:19:28.983 INFO PathSeqPipelineSpark - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_151-b12; 13:19:28.983 INFO PathSeqPipelineSpark - Start Date/Time: May 23, 2023 1:19:23 PM CST; 13:19:28.983 INFO PathSeqPipelineSpark - ------------------------------------------------------------; 13:19:28.983 INFO PathSeqPipelineSpark - ------------------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8339
https://github.com/broadinstitute/gatk/issues/8339:3573,Performance,load,load,3573,ark - Built for Spark Version: 2.4.5; 13:19:28.984 INFO PathSeqPipelineSpark - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 13:19:28.984 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 13:19:28.984 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 13:19:28.984 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 13:19:28.985 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 13:19:28.985 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 13:19:28.985 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 13:19:28.985 INFO PathSeqPipelineSpark - Requester pays: disabled; 13:19:28.985 INFO PathSeqPipelineSpark - Initializing engine; 13:19:28.985 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 23/05/23 13:19:29 INFO SparkContext: Running Spark version 2.4.5; 23/05/23 13:19:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 23/05/23 13:19:29 INFO SparkContext: Submitted application: PathSeqPipelineSpark; 23/05/23 13:19:29 INFO SecurityManager: Changing view acls to: singlecellproject; 23/05/23 13:19:29 INFO SecurityManager: Changing modify acls to: singlecellproject; 23/05/23 13:19:29 INFO SecurityManager: Changing view acls groups to: ; 23/05/23 13:19:29 INFO SecurityManager: Changing modify acls groups to: ; 23/05/23 13:19:29 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(singlecellproject); groups with view permissions: Set(); users with modify permissions: Set(singlecellproject); groups with modify permissions: Set(); 23/05/23 13:19:29 INFO Utils: Successfully started service 'sparkDriver' on port 40471.; 23/05/23 13:19:29 INFO SparkEnv: Registering MapOutputTracker; 23/05/23 13:19:29 INFO SparkEnv: Registering BlockManagerMaster; 23/05/23,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8339
https://github.com/broadinstitute/gatk/issues/8339:3771,Security,Secur,SecurityManager,3771,r: IntelDeflater; 13:19:28.985 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 13:19:28.985 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 13:19:28.985 INFO PathSeqPipelineSpark - Requester pays: disabled; 13:19:28.985 INFO PathSeqPipelineSpark - Initializing engine; 13:19:28.985 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 23/05/23 13:19:29 INFO SparkContext: Running Spark version 2.4.5; 23/05/23 13:19:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 23/05/23 13:19:29 INFO SparkContext: Submitted application: PathSeqPipelineSpark; 23/05/23 13:19:29 INFO SecurityManager: Changing view acls to: singlecellproject; 23/05/23 13:19:29 INFO SecurityManager: Changing modify acls to: singlecellproject; 23/05/23 13:19:29 INFO SecurityManager: Changing view acls groups to: ; 23/05/23 13:19:29 INFO SecurityManager: Changing modify acls groups to: ; 23/05/23 13:19:29 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(singlecellproject); groups with view permissions: Set(); users with modify permissions: Set(singlecellproject); groups with modify permissions: Set(); 23/05/23 13:19:29 INFO Utils: Successfully started service 'sparkDriver' on port 40471.; 23/05/23 13:19:29 INFO SparkEnv: Registering MapOutputTracker; 23/05/23 13:19:29 INFO SparkEnv: Registering BlockManagerMaster; 23/05/23 13:19:29 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 23/05/23 13:19:29 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up; 23/05/23 13:19:29 INFO DiskBlockManager: Created local directory at pathseq/tmp/blockmgr-11fec4b1-0808-4f7e-9ab9-a87799853aee; 23/05/23 13:19:29 INFO MemoryStore: MemoryStore started with capacity 399.8 GB; 23/05/23 13:19:29 INFO SparkEnv: Regis,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8339
https://github.com/broadinstitute/gatk/issues/8339:3853,Security,Secur,SecurityManager,3853,r: IntelDeflater; 13:19:28.985 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 13:19:28.985 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 13:19:28.985 INFO PathSeqPipelineSpark - Requester pays: disabled; 13:19:28.985 INFO PathSeqPipelineSpark - Initializing engine; 13:19:28.985 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 23/05/23 13:19:29 INFO SparkContext: Running Spark version 2.4.5; 23/05/23 13:19:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 23/05/23 13:19:29 INFO SparkContext: Submitted application: PathSeqPipelineSpark; 23/05/23 13:19:29 INFO SecurityManager: Changing view acls to: singlecellproject; 23/05/23 13:19:29 INFO SecurityManager: Changing modify acls to: singlecellproject; 23/05/23 13:19:29 INFO SecurityManager: Changing view acls groups to: ; 23/05/23 13:19:29 INFO SecurityManager: Changing modify acls groups to: ; 23/05/23 13:19:29 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(singlecellproject); groups with view permissions: Set(); users with modify permissions: Set(singlecellproject); groups with modify permissions: Set(); 23/05/23 13:19:29 INFO Utils: Successfully started service 'sparkDriver' on port 40471.; 23/05/23 13:19:29 INFO SparkEnv: Registering MapOutputTracker; 23/05/23 13:19:29 INFO SparkEnv: Registering BlockManagerMaster; 23/05/23 13:19:29 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 23/05/23 13:19:29 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up; 23/05/23 13:19:29 INFO DiskBlockManager: Created local directory at pathseq/tmp/blockmgr-11fec4b1-0808-4f7e-9ab9-a87799853aee; 23/05/23 13:19:29 INFO MemoryStore: MemoryStore started with capacity 399.8 GB; 23/05/23 13:19:29 INFO SparkEnv: Regis,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8339
https://github.com/broadinstitute/gatk/issues/8339:3937,Security,Secur,SecurityManager,3937,r: IntelDeflater; 13:19:28.985 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 13:19:28.985 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 13:19:28.985 INFO PathSeqPipelineSpark - Requester pays: disabled; 13:19:28.985 INFO PathSeqPipelineSpark - Initializing engine; 13:19:28.985 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 23/05/23 13:19:29 INFO SparkContext: Running Spark version 2.4.5; 23/05/23 13:19:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 23/05/23 13:19:29 INFO SparkContext: Submitted application: PathSeqPipelineSpark; 23/05/23 13:19:29 INFO SecurityManager: Changing view acls to: singlecellproject; 23/05/23 13:19:29 INFO SecurityManager: Changing modify acls to: singlecellproject; 23/05/23 13:19:29 INFO SecurityManager: Changing view acls groups to: ; 23/05/23 13:19:29 INFO SecurityManager: Changing modify acls groups to: ; 23/05/23 13:19:29 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(singlecellproject); groups with view permissions: Set(); users with modify permissions: Set(singlecellproject); groups with modify permissions: Set(); 23/05/23 13:19:29 INFO Utils: Successfully started service 'sparkDriver' on port 40471.; 23/05/23 13:19:29 INFO SparkEnv: Registering MapOutputTracker; 23/05/23 13:19:29 INFO SparkEnv: Registering BlockManagerMaster; 23/05/23 13:19:29 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 23/05/23 13:19:29 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up; 23/05/23 13:19:29 INFO DiskBlockManager: Created local directory at pathseq/tmp/blockmgr-11fec4b1-0808-4f7e-9ab9-a87799853aee; 23/05/23 13:19:29 INFO MemoryStore: MemoryStore started with capacity 399.8 GB; 23/05/23 13:19:29 INFO SparkEnv: Regis,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8339
https://github.com/broadinstitute/gatk/issues/8339:4009,Security,Secur,SecurityManager,4009,r: IntelDeflater; 13:19:28.985 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 13:19:28.985 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 13:19:28.985 INFO PathSeqPipelineSpark - Requester pays: disabled; 13:19:28.985 INFO PathSeqPipelineSpark - Initializing engine; 13:19:28.985 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 23/05/23 13:19:29 INFO SparkContext: Running Spark version 2.4.5; 23/05/23 13:19:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 23/05/23 13:19:29 INFO SparkContext: Submitted application: PathSeqPipelineSpark; 23/05/23 13:19:29 INFO SecurityManager: Changing view acls to: singlecellproject; 23/05/23 13:19:29 INFO SecurityManager: Changing modify acls to: singlecellproject; 23/05/23 13:19:29 INFO SecurityManager: Changing view acls groups to: ; 23/05/23 13:19:29 INFO SecurityManager: Changing modify acls groups to: ; 23/05/23 13:19:29 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(singlecellproject); groups with view permissions: Set(); users with modify permissions: Set(singlecellproject); groups with modify permissions: Set(); 23/05/23 13:19:29 INFO Utils: Successfully started service 'sparkDriver' on port 40471.; 23/05/23 13:19:29 INFO SparkEnv: Registering MapOutputTracker; 23/05/23 13:19:29 INFO SparkEnv: Registering BlockManagerMaster; 23/05/23 13:19:29 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 23/05/23 13:19:29 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up; 23/05/23 13:19:29 INFO DiskBlockManager: Created local directory at pathseq/tmp/blockmgr-11fec4b1-0808-4f7e-9ab9-a87799853aee; 23/05/23 13:19:29 INFO MemoryStore: MemoryStore started with capacity 399.8 GB; 23/05/23 13:19:29 INFO SparkEnv: Regis,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8339
https://github.com/broadinstitute/gatk/issues/8339:4083,Security,Secur,SecurityManager,4083,r: IntelDeflater; 13:19:28.985 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 13:19:28.985 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 13:19:28.985 INFO PathSeqPipelineSpark - Requester pays: disabled; 13:19:28.985 INFO PathSeqPipelineSpark - Initializing engine; 13:19:28.985 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 23/05/23 13:19:29 INFO SparkContext: Running Spark version 2.4.5; 23/05/23 13:19:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 23/05/23 13:19:29 INFO SparkContext: Submitted application: PathSeqPipelineSpark; 23/05/23 13:19:29 INFO SecurityManager: Changing view acls to: singlecellproject; 23/05/23 13:19:29 INFO SecurityManager: Changing modify acls to: singlecellproject; 23/05/23 13:19:29 INFO SecurityManager: Changing view acls groups to: ; 23/05/23 13:19:29 INFO SecurityManager: Changing modify acls groups to: ; 23/05/23 13:19:29 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(singlecellproject); groups with view permissions: Set(); users with modify permissions: Set(singlecellproject); groups with modify permissions: Set(); 23/05/23 13:19:29 INFO Utils: Successfully started service 'sparkDriver' on port 40471.; 23/05/23 13:19:29 INFO SparkEnv: Registering MapOutputTracker; 23/05/23 13:19:29 INFO SparkEnv: Registering BlockManagerMaster; 23/05/23 13:19:29 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 23/05/23 13:19:29 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up; 23/05/23 13:19:29 INFO DiskBlockManager: Created local directory at pathseq/tmp/blockmgr-11fec4b1-0808-4f7e-9ab9-a87799853aee; 23/05/23 13:19:29 INFO MemoryStore: MemoryStore started with capacity 399.8 GB; 23/05/23 13:19:29 INFO SparkEnv: Regis,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8339
https://github.com/broadinstitute/gatk/issues/8339:4100,Security,Secur,SecurityManager,4100,r: IntelDeflater; 13:19:28.985 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 13:19:28.985 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 13:19:28.985 INFO PathSeqPipelineSpark - Requester pays: disabled; 13:19:28.985 INFO PathSeqPipelineSpark - Initializing engine; 13:19:28.985 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 23/05/23 13:19:29 INFO SparkContext: Running Spark version 2.4.5; 23/05/23 13:19:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 23/05/23 13:19:29 INFO SparkContext: Submitted application: PathSeqPipelineSpark; 23/05/23 13:19:29 INFO SecurityManager: Changing view acls to: singlecellproject; 23/05/23 13:19:29 INFO SecurityManager: Changing modify acls to: singlecellproject; 23/05/23 13:19:29 INFO SecurityManager: Changing view acls groups to: ; 23/05/23 13:19:29 INFO SecurityManager: Changing modify acls groups to: ; 23/05/23 13:19:29 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(singlecellproject); groups with view permissions: Set(); users with modify permissions: Set(singlecellproject); groups with modify permissions: Set(); 23/05/23 13:19:29 INFO Utils: Successfully started service 'sparkDriver' on port 40471.; 23/05/23 13:19:29 INFO SparkEnv: Registering MapOutputTracker; 23/05/23 13:19:29 INFO SparkEnv: Registering BlockManagerMaster; 23/05/23 13:19:29 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 23/05/23 13:19:29 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up; 23/05/23 13:19:29 INFO DiskBlockManager: Created local directory at pathseq/tmp/blockmgr-11fec4b1-0808-4f7e-9ab9-a87799853aee; 23/05/23 13:19:29 INFO MemoryStore: MemoryStore started with capacity 399.8 GB; 23/05/23 13:19:29 INFO SparkEnv: Regis,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8339
https://github.com/broadinstitute/gatk/issues/8339:4117,Security,authenticat,authentication,4117,r: IntelDeflater; 13:19:28.985 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 13:19:28.985 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 13:19:28.985 INFO PathSeqPipelineSpark - Requester pays: disabled; 13:19:28.985 INFO PathSeqPipelineSpark - Initializing engine; 13:19:28.985 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 23/05/23 13:19:29 INFO SparkContext: Running Spark version 2.4.5; 23/05/23 13:19:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 23/05/23 13:19:29 INFO SparkContext: Submitted application: PathSeqPipelineSpark; 23/05/23 13:19:29 INFO SecurityManager: Changing view acls to: singlecellproject; 23/05/23 13:19:29 INFO SecurityManager: Changing modify acls to: singlecellproject; 23/05/23 13:19:29 INFO SecurityManager: Changing view acls groups to: ; 23/05/23 13:19:29 INFO SecurityManager: Changing modify acls groups to: ; 23/05/23 13:19:29 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(singlecellproject); groups with view permissions: Set(); users with modify permissions: Set(singlecellproject); groups with modify permissions: Set(); 23/05/23 13:19:29 INFO Utils: Successfully started service 'sparkDriver' on port 40471.; 23/05/23 13:19:29 INFO SparkEnv: Registering MapOutputTracker; 23/05/23 13:19:29 INFO SparkEnv: Registering BlockManagerMaster; 23/05/23 13:19:29 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 23/05/23 13:19:29 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up; 23/05/23 13:19:29 INFO DiskBlockManager: Created local directory at pathseq/tmp/blockmgr-11fec4b1-0808-4f7e-9ab9-a87799853aee; 23/05/23 13:19:29 INFO MemoryStore: MemoryStore started with capacity 399.8 GB; 23/05/23 13:19:29 INFO SparkEnv: Regis,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8339
https://github.com/broadinstitute/gatk/issues/8339:444,Testability,log,log,444,"### Instructions; I use PathSeqPipelineSpark to analyze 10x Visium spatial transcribed data.; I did not download the data from the database on the GATK official website. But I prepared the database according to the tutorial [https://gatk.broadinstitute.org/hc/en-us/articles/360035889911--How-to-Run-the-Pathseq-pipeline] by myself.; The analysis has no results, and I don't know the reason for the lack of results. ## software / environment / log file informations; Using GATK jar /mnt/icfs/work/singlecelldevelopment/software/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx750g -jar /mnt/icfs/work/singlecelldevelopment/software/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar PathSeqPipelineSpark --input CRC_16/outs/possorted_genome_bam.bam --filter-bwa-image hsa_GRCh38/genome.fa.img --kmer-file hsa_GRCh38/genome.hss --min-clipped-read-length 60 --microbe-dict 16SrRNA/bacteria.16SrRNA.dict --microbe-bwa-image 16SrRNA/bacteria.16SrRNA.fa.img --taxonomy-file 16SrRNA/16SrRNA.db --output pathseq/CRC_16.pathseq.complete.bam --scores-output pathseq/CRC_16.pathseq.complete.csv --is-host-aligned false --filter-duplicates false --min-score-identity .7 --tmp-dir pathseq/tmp; 13:19:23.776 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/mnt/icfs/work/singlecelldevelopment/software/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 13:19:28.982 INFO PathSeqPipelineSpark - ------------------------------------------------------------; 13:19:28.982 INFO PathSeqPipelineSpark - The Genome Analysis Toolkit (GATK) v4.3.0.0; 13:19:28.982 INFO PathSeqPipelineSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 13:19:28.983 INFO PathSeqPipelineSpark - Executing as singlecellproject@d01.capitalbiotech.local on Linux v3.10.0-514.16.1.el7.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8339
https://github.com/broadinstitute/gatk/issues/8339:57643,Usability,clear,cleared,57643,"ID 1976). 667 bytes result sent to driver; 23/05/23 13:20:19 INFO TaskSetManager: Finished task 66.0 in stage 31.0 (TID 2040) in 160 ms on localhost (executor driver) (1/128); 23/05/23 13:20:19 INFO TaskSetManager: Finished task 2.0 in stage 31.0 (TID 1976) in 330 ms on localhost (executor driver) (2/128); 23/05/23 13:20:19 INFO Executor: Finished task 3.0 in stage 31.0 (TID 1977). 667 bytes result sent to driver; ...; 23/05/23 13:20:19 INFO TaskSetManager: Finished task 97.0 in stage 31.0 (TID 2071) in 123 ms on localhost (executor driver) (127/128); 23/05/23 13:20:19 INFO TaskSetManager: Finished task 112.0 in stage 31.0 (TID 2086) in 88 ms on localhost (executor driver) (128/128); 23/05/23 13:20:19 INFO TaskSchedulerImpl: Removed TaskSet 31.0, whose tasks have all completed, from pool ; 23/05/23 13:20:19 INFO DAGScheduler: ResultStage 31 (foreach at BwaMemIndexCache.java:84) finished in 0.389 s; 23/05/23 13:20:19 INFO DAGScheduler: Job 7 finished: foreach at BwaMemIndexCache.java:84, took 0.392269 s; 23/05/23 13:20:19 INFO SparkUI: Stopped Spark web UI at http://d01.capitalbiotech.local:4040; 23/05/23 13:20:19 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 23/05/23 13:20:26 INFO MemoryStore: MemoryStore cleared; 23/05/23 13:20:26 INFO BlockManager: BlockManager stopped; 23/05/23 13:20:26 INFO BlockManagerMaster: BlockManagerMaster stopped; 23/05/23 13:20:26 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 23/05/23 13:20:26 INFO SparkContext: Successfully stopped SparkContext; 13:20:26.099 INFO PathSeqPipelineSpark - Shutting down engine; [May 23, 2023 1:20:26 PM CST] org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark done. Elapsed time: 1.04 minutes.; Runtime.totalMemory()=156475326464; 23/05/23 13:20:26 INFO ShutdownHookManager: Shutdown hook called; 23/05/23 13:20:26 INFO ShutdownHookManager: Deleting directory pathseq/tmp/spark-2042a18b-a4af-4a86-a236-c4914f0407a1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8339
https://github.com/broadinstitute/gatk/issues/8340:450,Availability,reliab,reliability,450,"## Feature request. ### Mutect2. ### Description ; In general, Mutet2 can correctly determine whether a mutation is located in the STR region, but if a mutation is not in the str region strictly, it will not be labeled as STR or any other meaningful label. In practical use, we hope that a mutation located next to the str region can also be labeled as STR, or other labels that can be used to judge, as this is crucial for correctly determining the reliability of the mutation. ; In the following example, if the mutation is determined to be STR, our filtering script will filter it out because it is obvious that the mutation is located after a series of T, which may be caused by sequencing errors. But Mutect2 did not determine it as STR, and it was determined as PASS. Eventually, the mutation was reported in our product report, and we manually deleted the site after discovering it.; ![image](https://github.com/broadinstitute/gatk/assets/66426093/4264e2a0-cefd-4a9d-92e0-69a45f26857c); ![a1d7589a482f73dfa528dd11bb24a305](https://github.com/broadinstitute/gatk/assets/66426093/f1e72d0b-eec7-4cfb-aa88-4b3077cdeab5). #### Steps to reproduce; Here are the bam, bed, shell and output files:; [chr11_108121426.zip](https://github.com/broadinstitute/gatk/files/11549460/chr11_108121426.zip). #### Expected behavior; We hope that a mutation located next to the str region can also be labeled as STR, or other labels that can be used to determine it.; Please evaluate whether it can be implemented in a later version, which is very important for us. Thank you very much!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8340
https://github.com/broadinstitute/gatk/issues/8340:694,Availability,error,errors,694,"## Feature request. ### Mutect2. ### Description ; In general, Mutet2 can correctly determine whether a mutation is located in the STR region, but if a mutation is not in the str region strictly, it will not be labeled as STR or any other meaningful label. In practical use, we hope that a mutation located next to the str region can also be labeled as STR, or other labels that can be used to judge, as this is crucial for correctly determining the reliability of the mutation. ; In the following example, if the mutation is determined to be STR, our filtering script will filter it out because it is obvious that the mutation is located after a series of T, which may be caused by sequencing errors. But Mutect2 did not determine it as STR, and it was determined as PASS. Eventually, the mutation was reported in our product report, and we manually deleted the site after discovering it.; ![image](https://github.com/broadinstitute/gatk/assets/66426093/4264e2a0-cefd-4a9d-92e0-69a45f26857c); ![a1d7589a482f73dfa528dd11bb24a305](https://github.com/broadinstitute/gatk/assets/66426093/f1e72d0b-eec7-4cfb-aa88-4b3077cdeab5). #### Steps to reproduce; Here are the bam, bed, shell and output files:; [chr11_108121426.zip](https://github.com/broadinstitute/gatk/files/11549460/chr11_108121426.zip). #### Expected behavior; We hope that a mutation located next to the str region can also be labeled as STR, or other labels that can be used to determine it.; Please evaluate whether it can be implemented in a later version, which is very important for us. Thank you very much!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8340
https://github.com/broadinstitute/gatk/pull/8343:95,Availability,down,down,95,"ok well this was a draft, but Miguel got here first, so.....; do we want to just shut this all down and skip Validate VDS?. https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20cremer/job_history/91bc9190-d623-4ce6-8184-b20e5cb622e5. ok I hate this pr---I dont think it makes sense to have a VDS validation script that only produces a VDS if the VDS matches the VCF--that makes it very hard to debug. https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20cremer/job_history/c27c9bbe-6a01-4639-bdf9-14b00d5dc252",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8343
https://github.com/broadinstitute/gatk/pull/8343:109,Security,Validat,Validate,109,"ok well this was a draft, but Miguel got here first, so.....; do we want to just shut this all down and skip Validate VDS?. https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20cremer/job_history/91bc9190-d623-4ce6-8184-b20e5cb622e5. ok I hate this pr---I dont think it makes sense to have a VDS validation script that only produces a VDS if the VDS matches the VCF--that makes it very hard to debug. https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20cremer/job_history/c27c9bbe-6a01-4639-bdf9-14b00d5dc252",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8343
https://github.com/broadinstitute/gatk/pull/8343:309,Security,validat,validation,309,"ok well this was a draft, but Miguel got here first, so.....; do we want to just shut this all down and skip Validate VDS?. https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20cremer/job_history/91bc9190-d623-4ce6-8184-b20e5cb622e5. ok I hate this pr---I dont think it makes sense to have a VDS validation script that only produces a VDS if the VDS matches the VCF--that makes it very hard to debug. https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20cremer/job_history/c27c9bbe-6a01-4639-bdf9-14b00d5dc252",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8343
https://github.com/broadinstitute/gatk/pull/8345:105,Energy Efficiency,monitor,monitoring,105,VS-943.; Fixes a bug in summarize_monitoring_logs script where it didn't recognize the format of certain monitoring log files.; This PR also migrates the changes to support VQSR Lite in gvs_avros_to_vds.py to import_gvs.py and removes the former since it is no longer used.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8345
https://github.com/broadinstitute/gatk/pull/8345:116,Testability,log,log,116,VS-943.; Fixes a bug in summarize_monitoring_logs script where it didn't recognize the format of certain monitoring log files.; This PR also migrates the changes to support VQSR Lite in gvs_avros_to_vds.py to import_gvs.py and removes the former since it is no longer used.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8345
https://github.com/broadinstitute/gatk/issues/8346:123,Availability,down,downloaded,123,"Hi. I failed to build GATK4. . I am a very beginner of bioinformatics and data science. ; I am using google VM ubuntu. ; I downloaded gatk-4.4.0.0. Step by step, I tried to build GATK4. (https://github.com/broadinstitute/gatk/blob/master/README.md#building). I made a gitclone using ; wget https://github.com/broadinstitute/gatk. and entered gatk folder. ; there was a gradlew.; and I entered ; ./gradlew bundle ; or; ./gradlew. but it failed to build GATK4 with following errors. . ====================================; OpenJDK 64-Bit Server VM warning: Insufficient space for shared memory file:; 30934; Try using the -Djava.io.tmpdir= option to select an alternate temp location. FAILURE: Build failed with an exception. * What went wrong:; Gradle could not start your build.; > Cannot create service of type DependencyLockingHandler using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createDependencyLockingHandler() as there is a problem with parameter #2 of type ConfigurationContainerInternal.; > Cannot create service of type ConfigurationContainerInternal using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createConfigurationContainer() as there is a problem with parameter #13 of type DefaultConfigurationFactory.; > Cannot create service of type DefaultConfigurationFactory using DefaultConfigurationFactory constructor as there is a problem with parameter #2 of type ConfigurationResolver.; > Cannot create service of type ConfigurationResolver using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createDependencyResolver() as there is a problem with parameter #1 of type ArtifactDependencyResolver.; > Cannot create service of type ArtifactDependencyResolver using method DependencyManagementBuildScopeServices.createArtifactDependencyResolver() as there is a problem with parameter #4 of type List<ResolverProviderFactory>.; > Could not create service of type VersionControlRepositoryConnect",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8346
https://github.com/broadinstitute/gatk/issues/8346:473,Availability,error,errors,473,"Hi. I failed to build GATK4. . I am a very beginner of bioinformatics and data science. ; I am using google VM ubuntu. ; I downloaded gatk-4.4.0.0. Step by step, I tried to build GATK4. (https://github.com/broadinstitute/gatk/blob/master/README.md#building). I made a gitclone using ; wget https://github.com/broadinstitute/gatk. and entered gatk folder. ; there was a gradlew.; and I entered ; ./gradlew bundle ; or; ./gradlew. but it failed to build GATK4 with following errors. . ====================================; OpenJDK 64-Bit Server VM warning: Insufficient space for shared memory file:; 30934; Try using the -Djava.io.tmpdir= option to select an alternate temp location. FAILURE: Build failed with an exception. * What went wrong:; Gradle could not start your build.; > Cannot create service of type DependencyLockingHandler using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createDependencyLockingHandler() as there is a problem with parameter #2 of type ConfigurationContainerInternal.; > Cannot create service of type ConfigurationContainerInternal using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createConfigurationContainer() as there is a problem with parameter #13 of type DefaultConfigurationFactory.; > Cannot create service of type DefaultConfigurationFactory using DefaultConfigurationFactory constructor as there is a problem with parameter #2 of type ConfigurationResolver.; > Cannot create service of type ConfigurationResolver using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createDependencyResolver() as there is a problem with parameter #1 of type ArtifactDependencyResolver.; > Cannot create service of type ArtifactDependencyResolver using method DependencyManagementBuildScopeServices.createArtifactDependencyResolver() as there is a problem with parameter #4 of type List<ResolverProviderFactory>.; > Could not create service of type VersionControlRepositoryConnect",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8346
https://github.com/broadinstitute/gatk/issues/8346:683,Availability,FAILURE,FAILURE,683,"Hi. I failed to build GATK4. . I am a very beginner of bioinformatics and data science. ; I am using google VM ubuntu. ; I downloaded gatk-4.4.0.0. Step by step, I tried to build GATK4. (https://github.com/broadinstitute/gatk/blob/master/README.md#building). I made a gitclone using ; wget https://github.com/broadinstitute/gatk. and entered gatk folder. ; there was a gradlew.; and I entered ; ./gradlew bundle ; or; ./gradlew. but it failed to build GATK4 with following errors. . ====================================; OpenJDK 64-Bit Server VM warning: Insufficient space for shared memory file:; 30934; Try using the -Djava.io.tmpdir= option to select an alternate temp location. FAILURE: Build failed with an exception. * What went wrong:; Gradle could not start your build.; > Cannot create service of type DependencyLockingHandler using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createDependencyLockingHandler() as there is a problem with parameter #2 of type ConfigurationContainerInternal.; > Cannot create service of type ConfigurationContainerInternal using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createConfigurationContainer() as there is a problem with parameter #13 of type DefaultConfigurationFactory.; > Cannot create service of type DefaultConfigurationFactory using DefaultConfigurationFactory constructor as there is a problem with parameter #2 of type ConfigurationResolver.; > Cannot create service of type ConfigurationResolver using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createDependencyResolver() as there is a problem with parameter #1 of type ArtifactDependencyResolver.; > Cannot create service of type ArtifactDependencyResolver using method DependencyManagementBuildScopeServices.createArtifactDependencyResolver() as there is a problem with parameter #4 of type List<ResolverProviderFactory>.; > Could not create service of type VersionControlRepositoryConnect",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8346
https://github.com/broadinstitute/gatk/issues/8346:2452,Availability,FAILURE,FAILURE,2452,pe ConfigurationContainerInternal.; > Cannot create service of type ConfigurationContainerInternal using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createConfigurationContainer() as there is a problem with parameter #13 of type DefaultConfigurationFactory.; > Cannot create service of type DefaultConfigurationFactory using DefaultConfigurationFactory constructor as there is a problem with parameter #2 of type ConfigurationResolver.; > Cannot create service of type ConfigurationResolver using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createDependencyResolver() as there is a problem with parameter #1 of type ArtifactDependencyResolver.; > Cannot create service of type ArtifactDependencyResolver using method DependencyManagementBuildScopeServices.createArtifactDependencyResolver() as there is a problem with parameter #4 of type List<ResolverProviderFactory>.; > Could not create service of type VersionControlRepositoryConnectionFactory using VersionControlBuildSessionServices.createVersionControlSystemFactory().; > Failed to create parent directory '/home/jdjdj0202/gatk/.gradle' when creating directory '/home/jdjdj0202/gatk/.gradle/vcs-1'. * Try:; > Run with --stacktrace option to get the stack trace.; > Run with --info or --debug option to get more log output.; > Run with --scan to get full insights. * Get more help at https://help.gradle.org. BUILD FAILED in 754ms. FAILURE: Build failed with an exception. * What went wrong:; Could not update /home/jdjdj0202/gatk/.gradle/7.5.1/fileChanges/last-build.bin; > /home/jdjdj0202/gatk/.gradle/7.5.1/fileChanges/last-build.bin (No such file or directory). * Try:; > Run with --stacktrace option to get the stack trace.; > Run with --info or --debug option to get more log output.; > Run with --scan to get full insights. * Get more help at https://help.gradle.org; * ; BUILD FAILED in 761ms; ====================================. How can I build GATK4? . Thanks a lot.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8346
https://github.com/broadinstitute/gatk/issues/8346:1001,Deployability,Configurat,ConfigurationContainerInternal,1001,"Hi. I failed to build GATK4. . I am a very beginner of bioinformatics and data science. ; I am using google VM ubuntu. ; I downloaded gatk-4.4.0.0. Step by step, I tried to build GATK4. (https://github.com/broadinstitute/gatk/blob/master/README.md#building). I made a gitclone using ; wget https://github.com/broadinstitute/gatk. and entered gatk folder. ; there was a gradlew.; and I entered ; ./gradlew bundle ; or; ./gradlew. but it failed to build GATK4 with following errors. . ====================================; OpenJDK 64-Bit Server VM warning: Insufficient space for shared memory file:; 30934; Try using the -Djava.io.tmpdir= option to select an alternate temp location. FAILURE: Build failed with an exception. * What went wrong:; Gradle could not start your build.; > Cannot create service of type DependencyLockingHandler using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createDependencyLockingHandler() as there is a problem with parameter #2 of type ConfigurationContainerInternal.; > Cannot create service of type ConfigurationContainerInternal using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createConfigurationContainer() as there is a problem with parameter #13 of type DefaultConfigurationFactory.; > Cannot create service of type DefaultConfigurationFactory using DefaultConfigurationFactory constructor as there is a problem with parameter #2 of type ConfigurationResolver.; > Cannot create service of type ConfigurationResolver using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createDependencyResolver() as there is a problem with parameter #1 of type ArtifactDependencyResolver.; > Cannot create service of type ArtifactDependencyResolver using method DependencyManagementBuildScopeServices.createArtifactDependencyResolver() as there is a problem with parameter #4 of type List<ResolverProviderFactory>.; > Could not create service of type VersionControlRepositoryConnect",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8346
https://github.com/broadinstitute/gatk/issues/8346:1066,Deployability,Configurat,ConfigurationContainerInternal,1066,"e VM ubuntu. ; I downloaded gatk-4.4.0.0. Step by step, I tried to build GATK4. (https://github.com/broadinstitute/gatk/blob/master/README.md#building). I made a gitclone using ; wget https://github.com/broadinstitute/gatk. and entered gatk folder. ; there was a gradlew.; and I entered ; ./gradlew bundle ; or; ./gradlew. but it failed to build GATK4 with following errors. . ====================================; OpenJDK 64-Bit Server VM warning: Insufficient space for shared memory file:; 30934; Try using the -Djava.io.tmpdir= option to select an alternate temp location. FAILURE: Build failed with an exception. * What went wrong:; Gradle could not start your build.; > Cannot create service of type DependencyLockingHandler using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createDependencyLockingHandler() as there is a problem with parameter #2 of type ConfigurationContainerInternal.; > Cannot create service of type ConfigurationContainerInternal using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createConfigurationContainer() as there is a problem with parameter #13 of type DefaultConfigurationFactory.; > Cannot create service of type DefaultConfigurationFactory using DefaultConfigurationFactory constructor as there is a problem with parameter #2 of type ConfigurationResolver.; > Cannot create service of type ConfigurationResolver using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createDependencyResolver() as there is a problem with parameter #1 of type ArtifactDependencyResolver.; > Cannot create service of type ArtifactDependencyResolver using method DependencyManagementBuildScopeServices.createArtifactDependencyResolver() as there is a problem with parameter #4 of type List<ResolverProviderFactory>.; > Could not create service of type VersionControlRepositoryConnectionFactory using VersionControlBuildSessionServices.createVersionControlSystemFactory().; > Failed to cre",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8346
https://github.com/broadinstitute/gatk/issues/8346:1444,Deployability,Configurat,ConfigurationResolver,1444,; and I entered ; ./gradlew bundle ; or; ./gradlew. but it failed to build GATK4 with following errors. . ====================================; OpenJDK 64-Bit Server VM warning: Insufficient space for shared memory file:; 30934; Try using the -Djava.io.tmpdir= option to select an alternate temp location. FAILURE: Build failed with an exception. * What went wrong:; Gradle could not start your build.; > Cannot create service of type DependencyLockingHandler using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createDependencyLockingHandler() as there is a problem with parameter #2 of type ConfigurationContainerInternal.; > Cannot create service of type ConfigurationContainerInternal using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createConfigurationContainer() as there is a problem with parameter #13 of type DefaultConfigurationFactory.; > Cannot create service of type DefaultConfigurationFactory using DefaultConfigurationFactory constructor as there is a problem with parameter #2 of type ConfigurationResolver.; > Cannot create service of type ConfigurationResolver using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createDependencyResolver() as there is a problem with parameter #1 of type ArtifactDependencyResolver.; > Cannot create service of type ArtifactDependencyResolver using method DependencyManagementBuildScopeServices.createArtifactDependencyResolver() as there is a problem with parameter #4 of type List<ResolverProviderFactory>.; > Could not create service of type VersionControlRepositoryConnectionFactory using VersionControlBuildSessionServices.createVersionControlSystemFactory().; > Failed to create parent directory '/home/jdjdj0202/gatk/.gradle' when creating directory '/home/jdjdj0202/gatk/.gradle/vcs-1'. * Try:; > Run with --stacktrace option to get the stack trace.; > Run with --info or --debug option to get more log output.; > Run with --scan to get full in,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8346
https://github.com/broadinstitute/gatk/issues/8346:1500,Deployability,Configurat,ConfigurationResolver,1500, Server VM warning: Insufficient space for shared memory file:; 30934; Try using the -Djava.io.tmpdir= option to select an alternate temp location. FAILURE: Build failed with an exception. * What went wrong:; Gradle could not start your build.; > Cannot create service of type DependencyLockingHandler using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createDependencyLockingHandler() as there is a problem with parameter #2 of type ConfigurationContainerInternal.; > Cannot create service of type ConfigurationContainerInternal using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createConfigurationContainer() as there is a problem with parameter #13 of type DefaultConfigurationFactory.; > Cannot create service of type DefaultConfigurationFactory using DefaultConfigurationFactory constructor as there is a problem with parameter #2 of type ConfigurationResolver.; > Cannot create service of type ConfigurationResolver using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createDependencyResolver() as there is a problem with parameter #1 of type ArtifactDependencyResolver.; > Cannot create service of type ArtifactDependencyResolver using method DependencyManagementBuildScopeServices.createArtifactDependencyResolver() as there is a problem with parameter #4 of type List<ResolverProviderFactory>.; > Could not create service of type VersionControlRepositoryConnectionFactory using VersionControlBuildSessionServices.createVersionControlSystemFactory().; > Failed to create parent directory '/home/jdjdj0202/gatk/.gradle' when creating directory '/home/jdjdj0202/gatk/.gradle/vcs-1'. * Try:; > Run with --stacktrace option to get the stack trace.; > Run with --info or --debug option to get more log output.; > Run with --scan to get full insights. * Get more help at https://help.gradle.org. BUILD FAILED in 754ms. FAILURE: Build failed with an exception. * What went wrong:; Could not update /home/,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8346
https://github.com/broadinstitute/gatk/issues/8346:2523,Deployability,update,update,2523,pe ConfigurationContainerInternal.; > Cannot create service of type ConfigurationContainerInternal using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createConfigurationContainer() as there is a problem with parameter #13 of type DefaultConfigurationFactory.; > Cannot create service of type DefaultConfigurationFactory using DefaultConfigurationFactory constructor as there is a problem with parameter #2 of type ConfigurationResolver.; > Cannot create service of type ConfigurationResolver using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createDependencyResolver() as there is a problem with parameter #1 of type ArtifactDependencyResolver.; > Cannot create service of type ArtifactDependencyResolver using method DependencyManagementBuildScopeServices.createArtifactDependencyResolver() as there is a problem with parameter #4 of type List<ResolverProviderFactory>.; > Could not create service of type VersionControlRepositoryConnectionFactory using VersionControlBuildSessionServices.createVersionControlSystemFactory().; > Failed to create parent directory '/home/jdjdj0202/gatk/.gradle' when creating directory '/home/jdjdj0202/gatk/.gradle/vcs-1'. * Try:; > Run with --stacktrace option to get the stack trace.; > Run with --info or --debug option to get more log output.; > Run with --scan to get full insights. * Get more help at https://help.gradle.org. BUILD FAILED in 754ms. FAILURE: Build failed with an exception. * What went wrong:; Could not update /home/jdjdj0202/gatk/.gradle/7.5.1/fileChanges/last-build.bin; > /home/jdjdj0202/gatk/.gradle/7.5.1/fileChanges/last-build.bin (No such file or directory). * Try:; > Run with --stacktrace option to get the stack trace.; > Run with --info or --debug option to get more log output.; > Run with --scan to get full insights. * Get more help at https://help.gradle.org; * ; BUILD FAILED in 761ms; ====================================. How can I build GATK4? . Thanks a lot.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8346
https://github.com/broadinstitute/gatk/issues/8346:812,Integrability,Depend,DependencyLockingHandler,812,"Hi. I failed to build GATK4. . I am a very beginner of bioinformatics and data science. ; I am using google VM ubuntu. ; I downloaded gatk-4.4.0.0. Step by step, I tried to build GATK4. (https://github.com/broadinstitute/gatk/blob/master/README.md#building). I made a gitclone using ; wget https://github.com/broadinstitute/gatk. and entered gatk folder. ; there was a gradlew.; and I entered ; ./gradlew bundle ; or; ./gradlew. but it failed to build GATK4 with following errors. . ====================================; OpenJDK 64-Bit Server VM warning: Insufficient space for shared memory file:; 30934; Try using the -Djava.io.tmpdir= option to select an alternate temp location. FAILURE: Build failed with an exception. * What went wrong:; Gradle could not start your build.; > Cannot create service of type DependencyLockingHandler using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createDependencyLockingHandler() as there is a problem with parameter #2 of type ConfigurationContainerInternal.; > Cannot create service of type ConfigurationContainerInternal using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createConfigurationContainer() as there is a problem with parameter #13 of type DefaultConfigurationFactory.; > Cannot create service of type DefaultConfigurationFactory using DefaultConfigurationFactory constructor as there is a problem with parameter #2 of type ConfigurationResolver.; > Cannot create service of type ConfigurationResolver using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createDependencyResolver() as there is a problem with parameter #1 of type ArtifactDependencyResolver.; > Cannot create service of type ArtifactDependencyResolver using method DependencyManagementBuildScopeServices.createArtifactDependencyResolver() as there is a problem with parameter #4 of type List<ResolverProviderFactory>.; > Could not create service of type VersionControlRepositoryConnect",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8346
https://github.com/broadinstitute/gatk/issues/8346:886,Integrability,Depend,DependencyResolutionScopeServices,886,"Hi. I failed to build GATK4. . I am a very beginner of bioinformatics and data science. ; I am using google VM ubuntu. ; I downloaded gatk-4.4.0.0. Step by step, I tried to build GATK4. (https://github.com/broadinstitute/gatk/blob/master/README.md#building). I made a gitclone using ; wget https://github.com/broadinstitute/gatk. and entered gatk folder. ; there was a gradlew.; and I entered ; ./gradlew bundle ; or; ./gradlew. but it failed to build GATK4 with following errors. . ====================================; OpenJDK 64-Bit Server VM warning: Insufficient space for shared memory file:; 30934; Try using the -Djava.io.tmpdir= option to select an alternate temp location. FAILURE: Build failed with an exception. * What went wrong:; Gradle could not start your build.; > Cannot create service of type DependencyLockingHandler using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createDependencyLockingHandler() as there is a problem with parameter #2 of type ConfigurationContainerInternal.; > Cannot create service of type ConfigurationContainerInternal using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createConfigurationContainer() as there is a problem with parameter #13 of type DefaultConfigurationFactory.; > Cannot create service of type DefaultConfigurationFactory using DefaultConfigurationFactory constructor as there is a problem with parameter #2 of type ConfigurationResolver.; > Cannot create service of type ConfigurationResolver using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createDependencyResolver() as there is a problem with parameter #1 of type ArtifactDependencyResolver.; > Cannot create service of type ArtifactDependencyResolver using method DependencyManagementBuildScopeServices.createArtifactDependencyResolver() as there is a problem with parameter #4 of type List<ResolverProviderFactory>.; > Could not create service of type VersionControlRepositoryConnect",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8346
https://github.com/broadinstitute/gatk/issues/8346:1146,Integrability,Depend,DependencyResolutionScopeServices,1146,"e VM ubuntu. ; I downloaded gatk-4.4.0.0. Step by step, I tried to build GATK4. (https://github.com/broadinstitute/gatk/blob/master/README.md#building). I made a gitclone using ; wget https://github.com/broadinstitute/gatk. and entered gatk folder. ; there was a gradlew.; and I entered ; ./gradlew bundle ; or; ./gradlew. but it failed to build GATK4 with following errors. . ====================================; OpenJDK 64-Bit Server VM warning: Insufficient space for shared memory file:; 30934; Try using the -Djava.io.tmpdir= option to select an alternate temp location. FAILURE: Build failed with an exception. * What went wrong:; Gradle could not start your build.; > Cannot create service of type DependencyLockingHandler using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createDependencyLockingHandler() as there is a problem with parameter #2 of type ConfigurationContainerInternal.; > Cannot create service of type ConfigurationContainerInternal using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createConfigurationContainer() as there is a problem with parameter #13 of type DefaultConfigurationFactory.; > Cannot create service of type DefaultConfigurationFactory using DefaultConfigurationFactory constructor as there is a problem with parameter #2 of type ConfigurationResolver.; > Cannot create service of type ConfigurationResolver using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createDependencyResolver() as there is a problem with parameter #1 of type ArtifactDependencyResolver.; > Cannot create service of type ArtifactDependencyResolver using method DependencyManagementBuildScopeServices.createArtifactDependencyResolver() as there is a problem with parameter #4 of type List<ResolverProviderFactory>.; > Could not create service of type VersionControlRepositoryConnectionFactory using VersionControlBuildSessionServices.createVersionControlSystemFactory().; > Failed to cre",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8346
https://github.com/broadinstitute/gatk/issues/8346:1571,Integrability,Depend,DependencyResolutionScopeServices,1571, Server VM warning: Insufficient space for shared memory file:; 30934; Try using the -Djava.io.tmpdir= option to select an alternate temp location. FAILURE: Build failed with an exception. * What went wrong:; Gradle could not start your build.; > Cannot create service of type DependencyLockingHandler using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createDependencyLockingHandler() as there is a problem with parameter #2 of type ConfigurationContainerInternal.; > Cannot create service of type ConfigurationContainerInternal using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createConfigurationContainer() as there is a problem with parameter #13 of type DefaultConfigurationFactory.; > Cannot create service of type DefaultConfigurationFactory using DefaultConfigurationFactory constructor as there is a problem with parameter #2 of type ConfigurationResolver.; > Cannot create service of type ConfigurationResolver using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createDependencyResolver() as there is a problem with parameter #1 of type ArtifactDependencyResolver.; > Cannot create service of type ArtifactDependencyResolver using method DependencyManagementBuildScopeServices.createArtifactDependencyResolver() as there is a problem with parameter #4 of type List<ResolverProviderFactory>.; > Could not create service of type VersionControlRepositoryConnectionFactory using VersionControlBuildSessionServices.createVersionControlSystemFactory().; > Failed to create parent directory '/home/jdjdj0202/gatk/.gradle' when creating directory '/home/jdjdj0202/gatk/.gradle/vcs-1'. * Try:; > Run with --stacktrace option to get the stack trace.; > Run with --info or --debug option to get more log output.; > Run with --scan to get full insights. * Get more help at https://help.gradle.org. BUILD FAILED in 754ms. FAILURE: Build failed with an exception. * What went wrong:; Could not update /home/,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8346
https://github.com/broadinstitute/gatk/issues/8346:1781,Integrability,Depend,DependencyManagementBuildScopeServices,1781,art your build.; > Cannot create service of type DependencyLockingHandler using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createDependencyLockingHandler() as there is a problem with parameter #2 of type ConfigurationContainerInternal.; > Cannot create service of type ConfigurationContainerInternal using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createConfigurationContainer() as there is a problem with parameter #13 of type DefaultConfigurationFactory.; > Cannot create service of type DefaultConfigurationFactory using DefaultConfigurationFactory constructor as there is a problem with parameter #2 of type ConfigurationResolver.; > Cannot create service of type ConfigurationResolver using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createDependencyResolver() as there is a problem with parameter #1 of type ArtifactDependencyResolver.; > Cannot create service of type ArtifactDependencyResolver using method DependencyManagementBuildScopeServices.createArtifactDependencyResolver() as there is a problem with parameter #4 of type List<ResolverProviderFactory>.; > Could not create service of type VersionControlRepositoryConnectionFactory using VersionControlBuildSessionServices.createVersionControlSystemFactory().; > Failed to create parent directory '/home/jdjdj0202/gatk/.gradle' when creating directory '/home/jdjdj0202/gatk/.gradle/vcs-1'. * Try:; > Run with --stacktrace option to get the stack trace.; > Run with --info or --debug option to get more log output.; > Run with --scan to get full insights. * Get more help at https://help.gradle.org. BUILD FAILED in 754ms. FAILURE: Build failed with an exception. * What went wrong:; Could not update /home/jdjdj0202/gatk/.gradle/7.5.1/fileChanges/last-build.bin; > /home/jdjdj0202/gatk/.gradle/7.5.1/fileChanges/last-build.bin (No such file or directory). * Try:; > Run with --stacktrace option to get the stack trace.; > Run with --i,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8346
https://github.com/broadinstitute/gatk/issues/8346:1001,Modifiability,Config,ConfigurationContainerInternal,1001,"Hi. I failed to build GATK4. . I am a very beginner of bioinformatics and data science. ; I am using google VM ubuntu. ; I downloaded gatk-4.4.0.0. Step by step, I tried to build GATK4. (https://github.com/broadinstitute/gatk/blob/master/README.md#building). I made a gitclone using ; wget https://github.com/broadinstitute/gatk. and entered gatk folder. ; there was a gradlew.; and I entered ; ./gradlew bundle ; or; ./gradlew. but it failed to build GATK4 with following errors. . ====================================; OpenJDK 64-Bit Server VM warning: Insufficient space for shared memory file:; 30934; Try using the -Djava.io.tmpdir= option to select an alternate temp location. FAILURE: Build failed with an exception. * What went wrong:; Gradle could not start your build.; > Cannot create service of type DependencyLockingHandler using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createDependencyLockingHandler() as there is a problem with parameter #2 of type ConfigurationContainerInternal.; > Cannot create service of type ConfigurationContainerInternal using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createConfigurationContainer() as there is a problem with parameter #13 of type DefaultConfigurationFactory.; > Cannot create service of type DefaultConfigurationFactory using DefaultConfigurationFactory constructor as there is a problem with parameter #2 of type ConfigurationResolver.; > Cannot create service of type ConfigurationResolver using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createDependencyResolver() as there is a problem with parameter #1 of type ArtifactDependencyResolver.; > Cannot create service of type ArtifactDependencyResolver using method DependencyManagementBuildScopeServices.createArtifactDependencyResolver() as there is a problem with parameter #4 of type List<ResolverProviderFactory>.; > Could not create service of type VersionControlRepositoryConnect",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8346
https://github.com/broadinstitute/gatk/issues/8346:1066,Modifiability,Config,ConfigurationContainerInternal,1066,"e VM ubuntu. ; I downloaded gatk-4.4.0.0. Step by step, I tried to build GATK4. (https://github.com/broadinstitute/gatk/blob/master/README.md#building). I made a gitclone using ; wget https://github.com/broadinstitute/gatk. and entered gatk folder. ; there was a gradlew.; and I entered ; ./gradlew bundle ; or; ./gradlew. but it failed to build GATK4 with following errors. . ====================================; OpenJDK 64-Bit Server VM warning: Insufficient space for shared memory file:; 30934; Try using the -Djava.io.tmpdir= option to select an alternate temp location. FAILURE: Build failed with an exception. * What went wrong:; Gradle could not start your build.; > Cannot create service of type DependencyLockingHandler using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createDependencyLockingHandler() as there is a problem with parameter #2 of type ConfigurationContainerInternal.; > Cannot create service of type ConfigurationContainerInternal using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createConfigurationContainer() as there is a problem with parameter #13 of type DefaultConfigurationFactory.; > Cannot create service of type DefaultConfigurationFactory using DefaultConfigurationFactory constructor as there is a problem with parameter #2 of type ConfigurationResolver.; > Cannot create service of type ConfigurationResolver using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createDependencyResolver() as there is a problem with parameter #1 of type ArtifactDependencyResolver.; > Cannot create service of type ArtifactDependencyResolver using method DependencyManagementBuildScopeServices.createArtifactDependencyResolver() as there is a problem with parameter #4 of type List<ResolverProviderFactory>.; > Could not create service of type VersionControlRepositoryConnectionFactory using VersionControlBuildSessionServices.createVersionControlSystemFactory().; > Failed to cre",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8346
https://github.com/broadinstitute/gatk/issues/8346:1444,Modifiability,Config,ConfigurationResolver,1444,; and I entered ; ./gradlew bundle ; or; ./gradlew. but it failed to build GATK4 with following errors. . ====================================; OpenJDK 64-Bit Server VM warning: Insufficient space for shared memory file:; 30934; Try using the -Djava.io.tmpdir= option to select an alternate temp location. FAILURE: Build failed with an exception. * What went wrong:; Gradle could not start your build.; > Cannot create service of type DependencyLockingHandler using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createDependencyLockingHandler() as there is a problem with parameter #2 of type ConfigurationContainerInternal.; > Cannot create service of type ConfigurationContainerInternal using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createConfigurationContainer() as there is a problem with parameter #13 of type DefaultConfigurationFactory.; > Cannot create service of type DefaultConfigurationFactory using DefaultConfigurationFactory constructor as there is a problem with parameter #2 of type ConfigurationResolver.; > Cannot create service of type ConfigurationResolver using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createDependencyResolver() as there is a problem with parameter #1 of type ArtifactDependencyResolver.; > Cannot create service of type ArtifactDependencyResolver using method DependencyManagementBuildScopeServices.createArtifactDependencyResolver() as there is a problem with parameter #4 of type List<ResolverProviderFactory>.; > Could not create service of type VersionControlRepositoryConnectionFactory using VersionControlBuildSessionServices.createVersionControlSystemFactory().; > Failed to create parent directory '/home/jdjdj0202/gatk/.gradle' when creating directory '/home/jdjdj0202/gatk/.gradle/vcs-1'. * Try:; > Run with --stacktrace option to get the stack trace.; > Run with --info or --debug option to get more log output.; > Run with --scan to get full in,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8346
https://github.com/broadinstitute/gatk/issues/8346:1500,Modifiability,Config,ConfigurationResolver,1500, Server VM warning: Insufficient space for shared memory file:; 30934; Try using the -Djava.io.tmpdir= option to select an alternate temp location. FAILURE: Build failed with an exception. * What went wrong:; Gradle could not start your build.; > Cannot create service of type DependencyLockingHandler using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createDependencyLockingHandler() as there is a problem with parameter #2 of type ConfigurationContainerInternal.; > Cannot create service of type ConfigurationContainerInternal using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createConfigurationContainer() as there is a problem with parameter #13 of type DefaultConfigurationFactory.; > Cannot create service of type DefaultConfigurationFactory using DefaultConfigurationFactory constructor as there is a problem with parameter #2 of type ConfigurationResolver.; > Cannot create service of type ConfigurationResolver using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createDependencyResolver() as there is a problem with parameter #1 of type ArtifactDependencyResolver.; > Cannot create service of type ArtifactDependencyResolver using method DependencyManagementBuildScopeServices.createArtifactDependencyResolver() as there is a problem with parameter #4 of type List<ResolverProviderFactory>.; > Could not create service of type VersionControlRepositoryConnectionFactory using VersionControlBuildSessionServices.createVersionControlSystemFactory().; > Failed to create parent directory '/home/jdjdj0202/gatk/.gradle' when creating directory '/home/jdjdj0202/gatk/.gradle/vcs-1'. * Try:; > Run with --stacktrace option to get the stack trace.; > Run with --info or --debug option to get more log output.; > Run with --scan to get full insights. * Get more help at https://help.gradle.org. BUILD FAILED in 754ms. FAILURE: Build failed with an exception. * What went wrong:; Could not update /home/,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8346
https://github.com/broadinstitute/gatk/issues/8346:2332,Testability,log,log,2332,pe ConfigurationContainerInternal.; > Cannot create service of type ConfigurationContainerInternal using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createConfigurationContainer() as there is a problem with parameter #13 of type DefaultConfigurationFactory.; > Cannot create service of type DefaultConfigurationFactory using DefaultConfigurationFactory constructor as there is a problem with parameter #2 of type ConfigurationResolver.; > Cannot create service of type ConfigurationResolver using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createDependencyResolver() as there is a problem with parameter #1 of type ArtifactDependencyResolver.; > Cannot create service of type ArtifactDependencyResolver using method DependencyManagementBuildScopeServices.createArtifactDependencyResolver() as there is a problem with parameter #4 of type List<ResolverProviderFactory>.; > Could not create service of type VersionControlRepositoryConnectionFactory using VersionControlBuildSessionServices.createVersionControlSystemFactory().; > Failed to create parent directory '/home/jdjdj0202/gatk/.gradle' when creating directory '/home/jdjdj0202/gatk/.gradle/vcs-1'. * Try:; > Run with --stacktrace option to get the stack trace.; > Run with --info or --debug option to get more log output.; > Run with --scan to get full insights. * Get more help at https://help.gradle.org. BUILD FAILED in 754ms. FAILURE: Build failed with an exception. * What went wrong:; Could not update /home/jdjdj0202/gatk/.gradle/7.5.1/fileChanges/last-build.bin; > /home/jdjdj0202/gatk/.gradle/7.5.1/fileChanges/last-build.bin (No such file or directory). * Try:; > Run with --stacktrace option to get the stack trace.; > Run with --info or --debug option to get more log output.; > Run with --scan to get full insights. * Get more help at https://help.gradle.org; * ; BUILD FAILED in 761ms; ====================================. How can I build GATK4? . Thanks a lot.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8346
https://github.com/broadinstitute/gatk/issues/8346:2798,Testability,log,log,2798,pe ConfigurationContainerInternal.; > Cannot create service of type ConfigurationContainerInternal using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createConfigurationContainer() as there is a problem with parameter #13 of type DefaultConfigurationFactory.; > Cannot create service of type DefaultConfigurationFactory using DefaultConfigurationFactory constructor as there is a problem with parameter #2 of type ConfigurationResolver.; > Cannot create service of type ConfigurationResolver using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createDependencyResolver() as there is a problem with parameter #1 of type ArtifactDependencyResolver.; > Cannot create service of type ArtifactDependencyResolver using method DependencyManagementBuildScopeServices.createArtifactDependencyResolver() as there is a problem with parameter #4 of type List<ResolverProviderFactory>.; > Could not create service of type VersionControlRepositoryConnectionFactory using VersionControlBuildSessionServices.createVersionControlSystemFactory().; > Failed to create parent directory '/home/jdjdj0202/gatk/.gradle' when creating directory '/home/jdjdj0202/gatk/.gradle/vcs-1'. * Try:; > Run with --stacktrace option to get the stack trace.; > Run with --info or --debug option to get more log output.; > Run with --scan to get full insights. * Get more help at https://help.gradle.org. BUILD FAILED in 754ms. FAILURE: Build failed with an exception. * What went wrong:; Could not update /home/jdjdj0202/gatk/.gradle/7.5.1/fileChanges/last-build.bin; > /home/jdjdj0202/gatk/.gradle/7.5.1/fileChanges/last-build.bin (No such file or directory). * Try:; > Run with --stacktrace option to get the stack trace.; > Run with --info or --debug option to get more log output.; > Run with --scan to get full insights. * Get more help at https://help.gradle.org; * ; BUILD FAILED in 761ms; ====================================. How can I build GATK4? . Thanks a lot.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8346
https://github.com/broadinstitute/gatk/pull/8349:12,Deployability,update,update,12,quick pr to update the beta workflow to use the latest ploidy changes,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8349
https://github.com/broadinstitute/gatk/pull/8351:188,Modifiability,refactor,refactoring,188,"Fixes #7166 #7385 . This doesn't contain any of the necessary work to support Gencode GFF3 files yet #, that will (probably) come in a subsequent PR as it requires a much more substantial refactoring effort of the Gencode datasources code.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8351
https://github.com/broadinstitute/gatk/issues/8356:427,Availability,recover,recover-all-dangling-branches,427,"Hello,; There's a question about haplotypecaller that's been bothering me for a long time.; GT is 0/1 in haplotypecaller's out vcf, but 1/1 in IGV. Can you help me explain?; This is cmd:; `gatk-4.2.6.1/gatk HaplotypeCaller -R hg19.fasta -I bam -A QualByDepth -A FisherStrand -A ReadPosRankSumTest -A StrandOddsRatio -A MappingQualityRankSumTest -A RMSMappingQuality --max-reads-per-alignment-start 0 --linked-de-bruijn-graph --recover-all-dangling-branches --max-mnp-distance 2 -O vcf`; IGV picture as below:; ![image](https://github.com/broadinstitute/gatk/assets/35715828/de85a9ed-6d80-43fb-9ce6-a6fec79fca67)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8356
https://github.com/broadinstitute/gatk/issues/8356:427,Safety,recover,recover-all-dangling-branches,427,"Hello,; There's a question about haplotypecaller that's been bothering me for a long time.; GT is 0/1 in haplotypecaller's out vcf, but 1/1 in IGV. Can you help me explain?; This is cmd:; `gatk-4.2.6.1/gatk HaplotypeCaller -R hg19.fasta -I bam -A QualByDepth -A FisherStrand -A ReadPosRankSumTest -A StrandOddsRatio -A MappingQualityRankSumTest -A RMSMappingQuality --max-reads-per-alignment-start 0 --linked-de-bruijn-graph --recover-all-dangling-branches --max-mnp-distance 2 -O vcf`; IGV picture as below:; ![image](https://github.com/broadinstitute/gatk/assets/35715828/de85a9ed-6d80-43fb-9ce6-a6fec79fca67)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8356
https://github.com/broadinstitute/gatk/issues/8357:338,Availability,error,error,338,"Hi!. I am using the gatk tool AnalyzeSaturationMutagenesis to analyze some data produced with the MITE-seq technology. It works perfectly for my purpose, so I decided to include it as a step in my pipeline written in Nextflow. ; Strangely enough, when I try to run the SAME EXACT command line inside a Nextflow module, it gives a generic error for most of the samples (sometimes all of them, sometimes some of them). ; It looks like a random issue, because if I run the same code outside of Nextflow, it works perfectly on every sample. . I would really appreciate if someone may give me some hints on why this is occurring and, eventually, how to fix it. ## Bug Report. ### Affected tool(s) or class(es); AnalyzeSaturationMutagenesis . ### Affected version(s); gatk4-4.3.0.0. ### Description ; Here it follows the output from Nextflow that appears on screen:. ```; Error executing process > 'gatk_count (gatk)'. Caused by:; Process `gatk_count (gatk)` terminated with an error exit status (247). Command executed:. gatk AnalyzeSaturationMutagenesis -I MITE6_P1_out.sam -R /home/tigem/f.panariello/Scratch/Cacchiarelli/MITE/QC_1804//i ndex/genome.fa --orf 1-5610 -O ./MITE6_P1. Command exit status:; 247. Command output:; (empty). Command error:; WARNING: Not mounting requested bind point (already mounted in container): /home/tigem/f.panariello; Using GATK jar /usr/local/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=f alse -Dsamjdk.compression_level=2 -jar /usr/local/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar AnalyzeSaturationMutagenesis -I MITE6_P1_out.sam -R /home/tigem/f.panariello/Scratch/Cacchiarelli/MITE/QC_1804//index/genome.fa --orf 1-5610 -O ./MIT E6_P1; 09:36:03.173 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/usr/local/share/gatk4-4.3.0.0-0/gatk-package -4.3.0.0-local.jar!/com/intel/gkl/native/libgk",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8357
https://github.com/broadinstitute/gatk/issues/8357:866,Availability,Error,Error,866,"Hi!. I am using the gatk tool AnalyzeSaturationMutagenesis to analyze some data produced with the MITE-seq technology. It works perfectly for my purpose, so I decided to include it as a step in my pipeline written in Nextflow. ; Strangely enough, when I try to run the SAME EXACT command line inside a Nextflow module, it gives a generic error for most of the samples (sometimes all of them, sometimes some of them). ; It looks like a random issue, because if I run the same code outside of Nextflow, it works perfectly on every sample. . I would really appreciate if someone may give me some hints on why this is occurring and, eventually, how to fix it. ## Bug Report. ### Affected tool(s) or class(es); AnalyzeSaturationMutagenesis . ### Affected version(s); gatk4-4.3.0.0. ### Description ; Here it follows the output from Nextflow that appears on screen:. ```; Error executing process > 'gatk_count (gatk)'. Caused by:; Process `gatk_count (gatk)` terminated with an error exit status (247). Command executed:. gatk AnalyzeSaturationMutagenesis -I MITE6_P1_out.sam -R /home/tigem/f.panariello/Scratch/Cacchiarelli/MITE/QC_1804//i ndex/genome.fa --orf 1-5610 -O ./MITE6_P1. Command exit status:; 247. Command output:; (empty). Command error:; WARNING: Not mounting requested bind point (already mounted in container): /home/tigem/f.panariello; Using GATK jar /usr/local/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=f alse -Dsamjdk.compression_level=2 -jar /usr/local/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar AnalyzeSaturationMutagenesis -I MITE6_P1_out.sam -R /home/tigem/f.panariello/Scratch/Cacchiarelli/MITE/QC_1804//index/genome.fa --orf 1-5610 -O ./MIT E6_P1; 09:36:03.173 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/usr/local/share/gatk4-4.3.0.0-0/gatk-package -4.3.0.0-local.jar!/com/intel/gkl/native/libgk",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8357
https://github.com/broadinstitute/gatk/issues/8357:972,Availability,error,error,972,"Hi!. I am using the gatk tool AnalyzeSaturationMutagenesis to analyze some data produced with the MITE-seq technology. It works perfectly for my purpose, so I decided to include it as a step in my pipeline written in Nextflow. ; Strangely enough, when I try to run the SAME EXACT command line inside a Nextflow module, it gives a generic error for most of the samples (sometimes all of them, sometimes some of them). ; It looks like a random issue, because if I run the same code outside of Nextflow, it works perfectly on every sample. . I would really appreciate if someone may give me some hints on why this is occurring and, eventually, how to fix it. ## Bug Report. ### Affected tool(s) or class(es); AnalyzeSaturationMutagenesis . ### Affected version(s); gatk4-4.3.0.0. ### Description ; Here it follows the output from Nextflow that appears on screen:. ```; Error executing process > 'gatk_count (gatk)'. Caused by:; Process `gatk_count (gatk)` terminated with an error exit status (247). Command executed:. gatk AnalyzeSaturationMutagenesis -I MITE6_P1_out.sam -R /home/tigem/f.panariello/Scratch/Cacchiarelli/MITE/QC_1804//i ndex/genome.fa --orf 1-5610 -O ./MITE6_P1. Command exit status:; 247. Command output:; (empty). Command error:; WARNING: Not mounting requested bind point (already mounted in container): /home/tigem/f.panariello; Using GATK jar /usr/local/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=f alse -Dsamjdk.compression_level=2 -jar /usr/local/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar AnalyzeSaturationMutagenesis -I MITE6_P1_out.sam -R /home/tigem/f.panariello/Scratch/Cacchiarelli/MITE/QC_1804//index/genome.fa --orf 1-5610 -O ./MIT E6_P1; 09:36:03.173 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/usr/local/share/gatk4-4.3.0.0-0/gatk-package -4.3.0.0-local.jar!/com/intel/gkl/native/libgk",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8357
https://github.com/broadinstitute/gatk/issues/8357:1239,Availability,error,error,1239,"mand line inside a Nextflow module, it gives a generic error for most of the samples (sometimes all of them, sometimes some of them). ; It looks like a random issue, because if I run the same code outside of Nextflow, it works perfectly on every sample. . I would really appreciate if someone may give me some hints on why this is occurring and, eventually, how to fix it. ## Bug Report. ### Affected tool(s) or class(es); AnalyzeSaturationMutagenesis . ### Affected version(s); gatk4-4.3.0.0. ### Description ; Here it follows the output from Nextflow that appears on screen:. ```; Error executing process > 'gatk_count (gatk)'. Caused by:; Process `gatk_count (gatk)` terminated with an error exit status (247). Command executed:. gatk AnalyzeSaturationMutagenesis -I MITE6_P1_out.sam -R /home/tigem/f.panariello/Scratch/Cacchiarelli/MITE/QC_1804//i ndex/genome.fa --orf 1-5610 -O ./MITE6_P1. Command exit status:; 247. Command output:; (empty). Command error:; WARNING: Not mounting requested bind point (already mounted in container): /home/tigem/f.panariello; Using GATK jar /usr/local/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=f alse -Dsamjdk.compression_level=2 -jar /usr/local/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar AnalyzeSaturationMutagenesis -I MITE6_P1_out.sam -R /home/tigem/f.panariello/Scratch/Cacchiarelli/MITE/QC_1804//index/genome.fa --orf 1-5610 -O ./MIT E6_P1; 09:36:03.173 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/usr/local/share/gatk4-4.3.0.0-0/gatk-package -4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 09:36:03.397 INFO AnalyzeSaturationMutagenesis - ------------------------------------------------------------; 09:36:03.398 INFO AnalyzeSaturationMutagenesis - The Genome Analysis Toolkit (GATK) v4.3.0.0; 09:36:03.398 INFO AnalyzeSaturationMutagenesis - For suppor",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8357
https://github.com/broadinstitute/gatk/issues/8357:197,Deployability,pipeline,pipeline,197,"Hi!. I am using the gatk tool AnalyzeSaturationMutagenesis to analyze some data produced with the MITE-seq technology. It works perfectly for my purpose, so I decided to include it as a step in my pipeline written in Nextflow. ; Strangely enough, when I try to run the SAME EXACT command line inside a Nextflow module, it gives a generic error for most of the samples (sometimes all of them, sometimes some of them). ; It looks like a random issue, because if I run the same code outside of Nextflow, it works perfectly on every sample. . I would really appreciate if someone may give me some hints on why this is occurring and, eventually, how to fix it. ## Bug Report. ### Affected tool(s) or class(es); AnalyzeSaturationMutagenesis . ### Affected version(s); gatk4-4.3.0.0. ### Description ; Here it follows the output from Nextflow that appears on screen:. ```; Error executing process > 'gatk_count (gatk)'. Caused by:; Process `gatk_count (gatk)` terminated with an error exit status (247). Command executed:. gatk AnalyzeSaturationMutagenesis -I MITE6_P1_out.sam -R /home/tigem/f.panariello/Scratch/Cacchiarelli/MITE/QC_1804//i ndex/genome.fa --orf 1-5610 -O ./MITE6_P1. Command exit status:; 247. Command output:; (empty). Command error:; WARNING: Not mounting requested bind point (already mounted in container): /home/tigem/f.panariello; Using GATK jar /usr/local/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=f alse -Dsamjdk.compression_level=2 -jar /usr/local/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar AnalyzeSaturationMutagenesis -I MITE6_P1_out.sam -R /home/tigem/f.panariello/Scratch/Cacchiarelli/MITE/QC_1804//index/genome.fa --orf 1-5610 -O ./MIT E6_P1; 09:36:03.173 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/usr/local/share/gatk4-4.3.0.0-0/gatk-package -4.3.0.0-local.jar!/com/intel/gkl/native/libgk",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8357
https://github.com/broadinstitute/gatk/issues/8357:1865,Performance,Load,Loading,1865,"`; Error executing process > 'gatk_count (gatk)'. Caused by:; Process `gatk_count (gatk)` terminated with an error exit status (247). Command executed:. gatk AnalyzeSaturationMutagenesis -I MITE6_P1_out.sam -R /home/tigem/f.panariello/Scratch/Cacchiarelli/MITE/QC_1804//i ndex/genome.fa --orf 1-5610 -O ./MITE6_P1. Command exit status:; 247. Command output:; (empty). Command error:; WARNING: Not mounting requested bind point (already mounted in container): /home/tigem/f.panariello; Using GATK jar /usr/local/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=f alse -Dsamjdk.compression_level=2 -jar /usr/local/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar AnalyzeSaturationMutagenesis -I MITE6_P1_out.sam -R /home/tigem/f.panariello/Scratch/Cacchiarelli/MITE/QC_1804//index/genome.fa --orf 1-5610 -O ./MIT E6_P1; 09:36:03.173 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/usr/local/share/gatk4-4.3.0.0-0/gatk-package -4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 09:36:03.397 INFO AnalyzeSaturationMutagenesis - ------------------------------------------------------------; 09:36:03.398 INFO AnalyzeSaturationMutagenesis - The Genome Analysis Toolkit (GATK) v4.3.0.0; 09:36:03.398 INFO AnalyzeSaturationMutagenesis - For support and documentation go to https://software.broadinstitute.org/gatk/; 09:36:03.398 INFO AnalyzeSaturationMutagenesis - Executing as f.panariello@biocluster2 on Linux v5.10.0-9-amd64 amd64; 09:36:03.398 INFO AnalyzeSaturationMutagenesis - Java runtime: OpenJDK 64-Bit Server VM v11.0.15-internal+0-adhoc..src; 09:36:03.399 INFO AnalyzeSaturationMutagenesis - Start Date/Time: June 9, 2023 at 9:36:03 AM GMT; 09:36:03.399 INFO AnalyzeSaturationMutagenesis - ------------------------------------------------------------; 09:36:03.399 INFO AnalyzeSaturationMutagenesis - --------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8357
https://github.com/broadinstitute/gatk/pull/8358:26,Deployability,Release,Release,26,Move to using [GenomicsDB Release 1.5.0](https://github.com/GenomicsDB/GenomicsDB/releases/v1.5.0). ; Highlights in the release relevant to gatk are; - [readthedocs](https://genomicsdb.readthedocs.io/en/latest/) for GenomicsDB design/usage/functionality - GenomicsDB/GenomicsDB#265.; - GenomicsDB/GenomicsDB#284; - GenomicsDB/GenomicsDB#271; - Exclude spark from genomicsdb core jar GenomicsDB/GenomicsDB#281; - General improved performance/logging.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8358
https://github.com/broadinstitute/gatk/pull/8358:82,Deployability,release,releases,82,Move to using [GenomicsDB Release 1.5.0](https://github.com/GenomicsDB/GenomicsDB/releases/v1.5.0). ; Highlights in the release relevant to gatk are; - [readthedocs](https://genomicsdb.readthedocs.io/en/latest/) for GenomicsDB design/usage/functionality - GenomicsDB/GenomicsDB#265.; - GenomicsDB/GenomicsDB#284; - GenomicsDB/GenomicsDB#271; - Exclude spark from genomicsdb core jar GenomicsDB/GenomicsDB#281; - General improved performance/logging.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8358
https://github.com/broadinstitute/gatk/pull/8358:120,Deployability,release,release,120,Move to using [GenomicsDB Release 1.5.0](https://github.com/GenomicsDB/GenomicsDB/releases/v1.5.0). ; Highlights in the release relevant to gatk are; - [readthedocs](https://genomicsdb.readthedocs.io/en/latest/) for GenomicsDB design/usage/functionality - GenomicsDB/GenomicsDB#265.; - GenomicsDB/GenomicsDB#284; - GenomicsDB/GenomicsDB#271; - Exclude spark from genomicsdb core jar GenomicsDB/GenomicsDB#281; - General improved performance/logging.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8358
https://github.com/broadinstitute/gatk/pull/8358:429,Performance,perform,performance,429,Move to using [GenomicsDB Release 1.5.0](https://github.com/GenomicsDB/GenomicsDB/releases/v1.5.0). ; Highlights in the release relevant to gatk are; - [readthedocs](https://genomicsdb.readthedocs.io/en/latest/) for GenomicsDB design/usage/functionality - GenomicsDB/GenomicsDB#265.; - GenomicsDB/GenomicsDB#284; - GenomicsDB/GenomicsDB#271; - Exclude spark from genomicsdb core jar GenomicsDB/GenomicsDB#281; - General improved performance/logging.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8358
https://github.com/broadinstitute/gatk/pull/8358:441,Testability,log,logging,441,Move to using [GenomicsDB Release 1.5.0](https://github.com/GenomicsDB/GenomicsDB/releases/v1.5.0). ; Highlights in the release relevant to gatk are; - [readthedocs](https://genomicsdb.readthedocs.io/en/latest/) for GenomicsDB design/usage/functionality - GenomicsDB/GenomicsDB#265.; - GenomicsDB/GenomicsDB#284; - GenomicsDB/GenomicsDB#271; - Exclude spark from genomicsdb core jar GenomicsDB/GenomicsDB#281; - General improved performance/logging.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8358
https://github.com/broadinstitute/gatk/pull/8360:108,Deployability,integrat,integration,108,Mostly changing `use_classic_VQSR` to `use_VQSR_lite` with a couple of minor bugfixes thrown in. Successful integration run [here](https://app.terra.bio/#workspaces/gvs-dev/mlc%20GVS%20Quickstart%20v3/job_history/3e60bf43-5cc3-4e5b-97ad-1732b4c543be).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8360
https://github.com/broadinstitute/gatk/pull/8360:108,Integrability,integrat,integration,108,Mostly changing `use_classic_VQSR` to `use_VQSR_lite` with a couple of minor bugfixes thrown in. Successful integration run [here](https://app.terra.bio/#workspaces/gvs-dev/mlc%20GVS%20Quickstart%20v3/job_history/3e60bf43-5cc3-4e5b-97ad-1732b4c543be).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8360
https://github.com/broadinstitute/gatk/pull/8362:236,Deployability,update,updated,236,"A collection of changes in non-GVS packages required to build a working version of GVS against master:. 1. Support for an optional monitoring script for VQSR Lite `JointVcfFiltering.wdl`.; 2. `VQS_SENS_FAILURE_PREFIX ` VCF header value updated for correctness.; 3. Moved all BigQuery classes under a `gvs` package to make clear these are currently considered to be GVS specific.; 4. Added method to BigQueryUtils.; 5. ~ExcessHet calculation fixes for the case of no PLs.~ Removed, no longer required with Annotation changes in `ExtractTool`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8362
https://github.com/broadinstitute/gatk/pull/8362:131,Energy Efficiency,monitor,monitoring,131,"A collection of changes in non-GVS packages required to build a working version of GVS against master:. 1. Support for an optional monitoring script for VQSR Lite `JointVcfFiltering.wdl`.; 2. `VQS_SENS_FAILURE_PREFIX ` VCF header value updated for correctness.; 3. Moved all BigQuery classes under a `gvs` package to make clear these are currently considered to be GVS specific.; 4. Added method to BigQueryUtils.; 5. ~ExcessHet calculation fixes for the case of no PLs.~ Removed, no longer required with Annotation changes in `ExtractTool`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8362
https://github.com/broadinstitute/gatk/pull/8362:322,Usability,clear,clear,322,"A collection of changes in non-GVS packages required to build a working version of GVS against master:. 1. Support for an optional monitoring script for VQSR Lite `JointVcfFiltering.wdl`.; 2. `VQS_SENS_FAILURE_PREFIX ` VCF header value updated for correctness.; 3. Moved all BigQuery classes under a `gvs` package to make clear these are currently considered to be GVS specific.; 4. Added method to BigQueryUtils.; 5. ~ExcessHet calculation fixes for the case of no PLs.~ Removed, no longer required with Annotation changes in `ExtractTool`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8362
https://github.com/broadinstitute/gatk/pull/8365:0,Testability,Test,Test,0,Test runs to (1) show no loss of functionality and (2) addition of new functionality:. - Baseline (`ah_var_store`) run with only chr20: https://app.terra.bio/#workspaces/gvs-dev/RSA%20-%20GVS%20Quickstart%20V2%20/job_history/a4216ae6-2ae3-40da-a149-cd13d2874cf0.; - Baseline (`ah_var_store`) run with all chromosomes: https://app.terra.bio/#workspaces/gvs-dev/RSA%20-%20GVS%20Quickstart%20V2%20/job_history/c061273c-6426-49b2-9b2d-036f201cb4cd.; - Branch run with only chr20: https://app.terra.bio/#workspaces/gvs-dev/RSA%20-%20GVS%20Quickstart%20V2%20/job_history/e0b5297d-fae8-4406-81f7-fa488fd7c835.; - Branch run with all chromosomes: https://app.terra.bio/#workspaces/gvs-dev/RSA%20-%20GVS%20Quickstart%20V2%20/job_history/fbc106a1-fdd5-4a26-97e6-e23bdf6a2096.; - Branch run with chr1 - chr22 (no sex chromosomes): https://app.terra.bio/#workspaces/gvs-dev/RSA%20-%20GVS%20Quickstart%20V2%20/job_history/4c1e82bc-533f-41ef-8fb3-05d8d359a0a6. - Branch run with only chr20 after cleanup: https://app.terra.bio/#workspaces/gvs-dev/RSA%20-%20GVS%20Quickstart%20V2%20/job_history/064f8014-ae72-4a38-8230-676b8a096179,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8365
https://github.com/broadinstitute/gatk/pull/8366:484,Availability,failure,failures,484,"@jamesemery The fun begins. No change in output yet, but a non-trivial change in implementation. Instead of making preliminary event groups according to overlap, then merging them according to mutually excluded events, this PR does it all in one step while automatically handling transitivity by treating as a matter of finding connected components of a graph whose vertices are events and whose edges are reasons (overlap and mutex) for events to be in the same event group. All the failures are from WDL tests. I assume those are not related.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8366
https://github.com/broadinstitute/gatk/pull/8366:506,Testability,test,tests,506,"@jamesemery The fun begins. No change in output yet, but a non-trivial change in implementation. Instead of making preliminary event groups according to overlap, then merging them according to mutually excluded events, this PR does it all in one step while automatically handling transitivity by treating as a matter of finding connected components of a graph whose vertices are events and whose edges are reasons (overlap and mutex) for events to be in the same event group. All the failures are from WDL tests. I assume those are not related.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8366
https://github.com/broadinstitute/gatk/pull/8367:78,Modifiability,refactor,refactoring,78,"@jamesemery Here's another fun one, again no change in output but significant refactoring of `constructHaplotypeFromVariants` and `createNewPDHaplotypeFromEvents`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8367
https://github.com/broadinstitute/gatk/issues/8372:138,Deployability,pipeline,pipeline,138,"Hello,. I am trying to use gatk/4.1.4.1 and picard/2.22.0 to do joint variant calling of PacBio HiFi reads and Illumina short-reads. ; My pipeline is basically the recommended one (without base recalibration) where after HaplotypeCaller, I use GenomicsDBImport and GenotypeGVCFs and end up with the vcf file. The HiFI reads have normal hifi phred scores up to 93 and the illumina are encoded with phred33 quality scores. The output of the joint variant calling is strange and it causes issues when I try to use it with tools like plink and it makes me wonder if the joint variant calling went badly as well. This is even after variant filtration where I filter for QUAL<30 and QD<2. Here is an example of what the first few lines of my vcf calls look like. Notice the QUAL column that looks like Num,Num:. ```; #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT Illumina_sample0 sample1 sample2 sample_3 sample4 sample5 sample6 hifi_sample; Chr1 10608 . GCA G 174,34 PASS AC=2;AF=0.143;AN=14;BaseQRankSum=0.00;DP=55;ExcessHet=3.3579;FS=7.404;MLEAC=2;MLEAF=0.143;MQ=57.97;MQRankSum=0.00;QD=15.85;ReadPosRankSum=1.00;SOR=3.611 GT:AD:DP:GQ:PL ./.:0,0:0:.:0,0,0 0/1:3,3:6:99:117,0,117 0/0:5,0:5:15:0,15,141 0/1:3,2:5:71:71,0,120 0/0:3,0:3:9:0,9,113 0/0:7,0:7:21:0,21,190 0/0:5,0:5:15:0,15,175 0/0:20,0:20:60:0,60,900; Chr1 10616 . G A 903,42 PASS AC=8;AF=0.571;AN=14;BaseQRankSum=0.00;DP=66;ExcessHet=0.7136;FS=9.277;MLEAC=9;MLEAF=0.643;MQ=59.80;MQRankSum=0.00;QD=20.08;ReadPosRankSum=1.00;SOR=0.251 GT:AD:DP:GQ:PL ./.:0,0:0:.:0,0,0 0/0:10,0:10:30:0,30,367 1/1:0,5:5:15:141,15,0 0/1:2,3:5:75:110,0,75 0/0:3,0:3:9:0,9,113 1/1:0,8:8:24:232,24,0 1/1:0,7:7:21:224,21,0 0/1:14,6:20:99:210,0,570. ```. I believe this can be reproduced with any illumina+hifi samples. Are you aware of this problem? Could the snp calling be wrong because of the different phred score encoding? What do you suggest to do in these cases?. I have read your response in this issue: ; https://gatk.broadinstitute.org/hc/en-us/community/pos",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8372
https://github.com/broadinstitute/gatk/pull/8374:306,Availability,failure,failure,306,Passing workflow here: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Exome%20Data%2049k/job_history/38d22351-33cd-4c2c-abf9-feccda71d40a. Mostly passing integration test here: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/4a7e6628-6c19-442d-90b8-202da267d8bb; (the failure was a bq time out.),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8374
https://github.com/broadinstitute/gatk/pull/8374:155,Deployability,integrat,integration,155,Passing workflow here: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Exome%20Data%2049k/job_history/38d22351-33cd-4c2c-abf9-feccda71d40a. Mostly passing integration test here: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/4a7e6628-6c19-442d-90b8-202da267d8bb; (the failure was a bq time out.),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8374
https://github.com/broadinstitute/gatk/pull/8374:155,Integrability,integrat,integration,155,Passing workflow here: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Exome%20Data%2049k/job_history/38d22351-33cd-4c2c-abf9-feccda71d40a. Mostly passing integration test here: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/4a7e6628-6c19-442d-90b8-202da267d8bb; (the failure was a bq time out.),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8374
https://github.com/broadinstitute/gatk/pull/8374:167,Testability,test,test,167,Passing workflow here: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Exome%20Data%2049k/job_history/38d22351-33cd-4c2c-abf9-feccda71d40a. Mostly passing integration test here: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/4a7e6628-6c19-442d-90b8-202da267d8bb; (the failure was a bq time out.),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8374
https://github.com/broadinstitute/gatk/pull/8375:144,Usability,clear,clear,144,Move BigQuery classes that are only used by GVS into a `gvs` package. This should facilitate the alignment of GVS with GATK master by making it clear that these classes were created specifically for GVS and are not necessarily more generally usable in their current forms.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8375
https://github.com/broadinstitute/gatk/pull/8375:242,Usability,usab,usable,242,Move BigQuery classes that are only used by GVS into a `gvs` package. This should facilitate the alignment of GVS with GATK master by making it clear that these classes were created specifically for GVS and are not necessarily more generally usable in their current forms.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8375
https://github.com/broadinstitute/gatk/pull/8378:354,Testability,test,tests,354,"Adds some flexibility to the allowed split-read strand annotations on SV records:. - Allow INS -+ strands; - Allow INV null strands; - When clustering, only require that strands match for INV/BND records. The changes affecting INS variants are commensurate with changing going into gatk-sv (see https://github.com/broadinstitute/gatk-sv/pull/553). Added tests for the new functionality and improved some unit test coverage for a few related cases.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8378
https://github.com/broadinstitute/gatk/pull/8378:409,Testability,test,test,409,"Adds some flexibility to the allowed split-read strand annotations on SV records:. - Allow INS -+ strands; - Allow INV null strands; - When clustering, only require that strands match for INV/BND records. The changes affecting INS variants are commensurate with changing going into gatk-sv (see https://github.com/broadinstitute/gatk-sv/pull/553). Added tests for the new functionality and improved some unit test coverage for a few related cases.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8378
https://github.com/broadinstitute/gatk/issues/8381:678,Performance,Load,Loading,678,"Using GATK jar /share/home/yzbs/.conda/envs/WGS/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx256g -XX:ParallelGCThreads=48 -Djava.io.tmpdir=./tmp -jar /share/home/yzbs/.conda/envs/WGS/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar CombineGVCFs -R /share/home/yzbs/common_data/osativa.genome_bwa/IRGSP-1.0.dna.toplevel.fasta --variant /share/home/yzbs/bio_project/WGS_2023/Gvcf/gvcf.list -O /share/home/yzbs/bio_project/WGS_2023/Gvcf/combined.g.vcf; 14:13:43.565 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/home/yzbs/.conda/envs/WGS/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 14:13:43.884 INFO CombineGVCFs - ------------------------------------------------------------; 14:13:43.884 INFO CombineGVCFs - The Genome Analysis Toolkit (GATK) v4.3.0.0; 14:13:43.885 INFO CombineGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:13:43.885 INFO CombineGVCFs - Executing as yzbsl_xupeng@h3c42 on Linux v3.10.0-1160.el7.x86_64 amd64; 14:13:43.885 INFO CombineGVCFs - Java runtime: OpenJDK 64-Bit Server VM v11.0.13+7-b1751.21; 14:13:43.885 INFO CombineGVCFs - Start Date/Time: June 26, 2023 at 2:13:43 PM CST; 14:13:43.885 INFO CombineGVCFs - ------------------------------------------------------------; 14:13:43.885 INFO CombineGVCFs - ------------------------------------------------------------; 14:13:43.886 INFO CombineGVCFs - HTSJDK Version: 3.0.1; 14:13:43.886 INFO CombineGVCFs - Picard Version: 2.27.5; 14:13:43.886 INFO CombineGVCFs - Built for Spark Version: 2.4.5; 14:13:43.886 INFO CombineGVCFs - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 14:13:43.886 INFO CombineGVCFs - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 14:13:43.886 INFO CombineGVCFs - HTSJDK Defaults.USE_",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8381
https://github.com/broadinstitute/gatk/issues/8381:3011,Testability,log,log,3011,"a runtime: OpenJDK 64-Bit Server VM v11.0.13+7-b1751.21; 14:13:43.885 INFO CombineGVCFs - Start Date/Time: June 26, 2023 at 2:13:43 PM CST; 14:13:43.885 INFO CombineGVCFs - ------------------------------------------------------------; 14:13:43.885 INFO CombineGVCFs - ------------------------------------------------------------; 14:13:43.886 INFO CombineGVCFs - HTSJDK Version: 3.0.1; 14:13:43.886 INFO CombineGVCFs - Picard Version: 2.27.5; 14:13:43.886 INFO CombineGVCFs - Built for Spark Version: 2.4.5; 14:13:43.886 INFO CombineGVCFs - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 14:13:43.886 INFO CombineGVCFs - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 14:13:43.886 INFO CombineGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 14:13:43.886 INFO CombineGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 14:13:43.886 INFO CombineGVCFs - Deflater: IntelDeflater; 14:13:43.887 INFO CombineGVCFs - Inflater: IntelInflater; 14:13:43.887 INFO CombineGVCFs - GCS max retries/reopens: 20; 14:13:43.887 INFO CombineGVCFs - Requester pays: disabled; 14:13:43.887 INFO CombineGVCFs - Initializing engine; 14:13:44.274 INFO FeatureManager - Using codec VCFCodec to read file file:///share/home/yzbs/bio_project/WGS_2023/Gvcf/S1.g.vcf; 14:13:44.688 INFO FeatureManager - Using codec VCFCodec to read file file:///share/home/yzbs/bio_project/WGS_2023/Gvcf/S2.g.vcf; 14:13:45.207 INFO FeatureManager - Using codec VCFCodec to read file file:///share/home/yzbs/bio_project/WGS_2023/Gvcf/S3.g.vcf; .....; .....; .....; .....; 14:13:45.207 INFO FeatureManager - Using codec VCFCodec to read file file:///share/home/yzbs/bio_project/WGS_2023/Gvcf/S887.g.vcf\. It suddendly quit,when it print last samples's log S887.gvcf “INFO FeatureManager - Using codec VCFCodec to read file file:///share/home/yzbs/bio_project/WGS_2023/Gvcf/S887.g.vcf\”. Can you give me some advice to fix this problem? When I pick three samples to test script, It is success to create combine.gvcf. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8381
https://github.com/broadinstitute/gatk/issues/8381:3224,Testability,test,test,3224,"a runtime: OpenJDK 64-Bit Server VM v11.0.13+7-b1751.21; 14:13:43.885 INFO CombineGVCFs - Start Date/Time: June 26, 2023 at 2:13:43 PM CST; 14:13:43.885 INFO CombineGVCFs - ------------------------------------------------------------; 14:13:43.885 INFO CombineGVCFs - ------------------------------------------------------------; 14:13:43.886 INFO CombineGVCFs - HTSJDK Version: 3.0.1; 14:13:43.886 INFO CombineGVCFs - Picard Version: 2.27.5; 14:13:43.886 INFO CombineGVCFs - Built for Spark Version: 2.4.5; 14:13:43.886 INFO CombineGVCFs - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 14:13:43.886 INFO CombineGVCFs - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 14:13:43.886 INFO CombineGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 14:13:43.886 INFO CombineGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 14:13:43.886 INFO CombineGVCFs - Deflater: IntelDeflater; 14:13:43.887 INFO CombineGVCFs - Inflater: IntelInflater; 14:13:43.887 INFO CombineGVCFs - GCS max retries/reopens: 20; 14:13:43.887 INFO CombineGVCFs - Requester pays: disabled; 14:13:43.887 INFO CombineGVCFs - Initializing engine; 14:13:44.274 INFO FeatureManager - Using codec VCFCodec to read file file:///share/home/yzbs/bio_project/WGS_2023/Gvcf/S1.g.vcf; 14:13:44.688 INFO FeatureManager - Using codec VCFCodec to read file file:///share/home/yzbs/bio_project/WGS_2023/Gvcf/S2.g.vcf; 14:13:45.207 INFO FeatureManager - Using codec VCFCodec to read file file:///share/home/yzbs/bio_project/WGS_2023/Gvcf/S3.g.vcf; .....; .....; .....; .....; 14:13:45.207 INFO FeatureManager - Using codec VCFCodec to read file file:///share/home/yzbs/bio_project/WGS_2023/Gvcf/S887.g.vcf\. It suddendly quit,when it print last samples's log S887.gvcf “INFO FeatureManager - Using codec VCFCodec to read file file:///share/home/yzbs/bio_project/WGS_2023/Gvcf/S887.g.vcf\”. Can you give me some advice to fix this problem? When I pick three samples to test script, It is success to create combine.gvcf. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8381
https://github.com/broadinstitute/gatk/issues/8385:532,Availability,toler,tolerable,532,It was noticed while doing #8351 that the `GencodeFunctotation.equals()` method has the following line in it; ``` ; if (geneTranscriptType != that.geneTranscriptType) return false; ; ```. Unfortunately the geneTranscriptType is stored as a Sting and thus this should NOT be expected to succeed in almost any case. As it stands fixing this innocuous oversight seems to break several of the combinatorial funcotator tests and an integration test. Somebody should fix this behavior (easy) and validate that the test changes are within tolerable levels (hard).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8385
https://github.com/broadinstitute/gatk/issues/8385:427,Deployability,integrat,integration,427,It was noticed while doing #8351 that the `GencodeFunctotation.equals()` method has the following line in it; ``` ; if (geneTranscriptType != that.geneTranscriptType) return false; ; ```. Unfortunately the geneTranscriptType is stored as a Sting and thus this should NOT be expected to succeed in almost any case. As it stands fixing this innocuous oversight seems to break several of the combinatorial funcotator tests and an integration test. Somebody should fix this behavior (easy) and validate that the test changes are within tolerable levels (hard).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8385
https://github.com/broadinstitute/gatk/issues/8385:427,Integrability,integrat,integration,427,It was noticed while doing #8351 that the `GencodeFunctotation.equals()` method has the following line in it; ``` ; if (geneTranscriptType != that.geneTranscriptType) return false; ; ```. Unfortunately the geneTranscriptType is stored as a Sting and thus this should NOT be expected to succeed in almost any case. As it stands fixing this innocuous oversight seems to break several of the combinatorial funcotator tests and an integration test. Somebody should fix this behavior (easy) and validate that the test changes are within tolerable levels (hard).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8385
https://github.com/broadinstitute/gatk/issues/8385:490,Security,validat,validate,490,It was noticed while doing #8351 that the `GencodeFunctotation.equals()` method has the following line in it; ``` ; if (geneTranscriptType != that.geneTranscriptType) return false; ; ```. Unfortunately the geneTranscriptType is stored as a Sting and thus this should NOT be expected to succeed in almost any case. As it stands fixing this innocuous oversight seems to break several of the combinatorial funcotator tests and an integration test. Somebody should fix this behavior (easy) and validate that the test changes are within tolerable levels (hard).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8385
https://github.com/broadinstitute/gatk/issues/8385:414,Testability,test,tests,414,It was noticed while doing #8351 that the `GencodeFunctotation.equals()` method has the following line in it; ``` ; if (geneTranscriptType != that.geneTranscriptType) return false; ; ```. Unfortunately the geneTranscriptType is stored as a Sting and thus this should NOT be expected to succeed in almost any case. As it stands fixing this innocuous oversight seems to break several of the combinatorial funcotator tests and an integration test. Somebody should fix this behavior (easy) and validate that the test changes are within tolerable levels (hard).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8385
https://github.com/broadinstitute/gatk/issues/8385:439,Testability,test,test,439,It was noticed while doing #8351 that the `GencodeFunctotation.equals()` method has the following line in it; ``` ; if (geneTranscriptType != that.geneTranscriptType) return false; ; ```. Unfortunately the geneTranscriptType is stored as a Sting and thus this should NOT be expected to succeed in almost any case. As it stands fixing this innocuous oversight seems to break several of the combinatorial funcotator tests and an integration test. Somebody should fix this behavior (easy) and validate that the test changes are within tolerable levels (hard).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8385
https://github.com/broadinstitute/gatk/issues/8385:508,Testability,test,test,508,It was noticed while doing #8351 that the `GencodeFunctotation.equals()` method has the following line in it; ``` ; if (geneTranscriptType != that.geneTranscriptType) return false; ; ```. Unfortunately the geneTranscriptType is stored as a Sting and thus this should NOT be expected to succeed in almost any case. As it stands fixing this innocuous oversight seems to break several of the combinatorial funcotator tests and an integration test. Somebody should fix this behavior (easy) and validate that the test changes are within tolerable levels (hard).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8385
https://github.com/broadinstitute/gatk/issues/8387:409,Availability,error,error,409,"Hello,. I am trying to set up a python environment to use gatk DetermineGermlineContigPloidy module. I cannot use conda. I have tried to install in a virtual python environment the dependencies found in these two files:. gatk/scripts/gatkcondaenv.yml.template ; gatk/src/main/python/org/broadinstitute/hellbender/setup_gcnvkernel.py. I have installed gcnvkernel in my virtual environment. . ----. This is the error message I get when I try to import gcnvkernel: python -c ""import gcnvkernel"". Traceback (most recent call last):; File ""/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.6.10/lib/python3.6/configparser.py"", line 1138, in _unify_values; sectiondict = self._sections[section]; KeyError: 'blas'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configparser.py"", line 168, in fetch_val_for_key; return theano_cfg.get(section, option); File ""/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.6.10/lib/python3.6/configparser.py"", line 781, in get; d = self._unify_values(section, vars); File ""/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.6.10/lib/python3.6/configparser.py"", line 1141, in _unify_values; raise NoSectionError(section); configparser.NoSectionError: No section: 'blas'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configparser.py"", line 328, in __get__; delete_key=delete_key); File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configparser.py"", line 172, in fetch_val_for_key; raise KeyError(key); KeyError: 'blas.ldflags'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File """,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8387
https://github.com/broadinstitute/gatk/issues/8387:137,Deployability,install,install,137,"Hello,. I am trying to set up a python environment to use gatk DetermineGermlineContigPloidy module. I cannot use conda. I have tried to install in a virtual python environment the dependencies found in these two files:. gatk/scripts/gatkcondaenv.yml.template ; gatk/src/main/python/org/broadinstitute/hellbender/setup_gcnvkernel.py. I have installed gcnvkernel in my virtual environment. . ----. This is the error message I get when I try to import gcnvkernel: python -c ""import gcnvkernel"". Traceback (most recent call last):; File ""/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.6.10/lib/python3.6/configparser.py"", line 1138, in _unify_values; sectiondict = self._sections[section]; KeyError: 'blas'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configparser.py"", line 168, in fetch_val_for_key; return theano_cfg.get(section, option); File ""/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.6.10/lib/python3.6/configparser.py"", line 781, in get; d = self._unify_values(section, vars); File ""/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.6.10/lib/python3.6/configparser.py"", line 1141, in _unify_values; raise NoSectionError(section); configparser.NoSectionError: No section: 'blas'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configparser.py"", line 328, in __get__; delete_key=delete_key); File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configparser.py"", line 172, in fetch_val_for_key; raise KeyError(key); KeyError: 'blas.ldflags'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File """,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8387
https://github.com/broadinstitute/gatk/issues/8387:341,Deployability,install,installed,341,"Hello,. I am trying to set up a python environment to use gatk DetermineGermlineContigPloidy module. I cannot use conda. I have tried to install in a virtual python environment the dependencies found in these two files:. gatk/scripts/gatkcondaenv.yml.template ; gatk/src/main/python/org/broadinstitute/hellbender/setup_gcnvkernel.py. I have installed gcnvkernel in my virtual environment. . ----. This is the error message I get when I try to import gcnvkernel: python -c ""import gcnvkernel"". Traceback (most recent call last):; File ""/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.6.10/lib/python3.6/configparser.py"", line 1138, in _unify_values; sectiondict = self._sections[section]; KeyError: 'blas'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configparser.py"", line 168, in fetch_val_for_key; return theano_cfg.get(section, option); File ""/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.6.10/lib/python3.6/configparser.py"", line 781, in get; d = self._unify_values(section, vars); File ""/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.6.10/lib/python3.6/configparser.py"", line 1141, in _unify_values; raise NoSectionError(section); configparser.NoSectionError: No section: 'blas'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configparser.py"", line 328, in __get__; delete_key=delete_key); File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configparser.py"", line 172, in fetch_val_for_key; raise KeyError(key); KeyError: 'blas.ldflags'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File """,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8387
https://github.com/broadinstitute/gatk/issues/8387:4730,Deployability,update,update,4730,"tensor import blas; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/tensor/blas.py"", line 155, in <module>; from theano.tensor.blas_headers import blas_header_text; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/tensor/blas_headers.py"", line 987, in <module>; if not config.blas.ldflags:; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configparser.py"", line 332, in __get__; val_str = self.default(); File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configdefaults.py"", line 1451, in default_blas_ldflags; check_mkl_openmp(); File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configdefaults.py"", line 1273, in check_mkl_openmp; """"""); RuntimeError: ; Could not import 'mkl'. If you are using conda, update the numpy; packages to the latest build otherwise, set MKL_THREADING_LAYER=GNU in; your environment for MKL 2018. If you have MKL 2017 install and are not in a conda environment you; can set the Theano flag blas.check_openmp to False. Be warned that if; you set this flag and don't set the appropriate environment or make; sure you have the right version you *will* get wrong results. ----. Here is the pip list from my environment:. cached-property 1.5.2+computecanada ; cycler 0.11.0+computecanada ; enum34 1.1.10+computecanada ; gatkpythonpackages 0.1 ; gcnvkernel 0.8 ; h5py 3.1.0+computecanada ; intel-openmp 2021.1.1+computecanada; joblib 0.14.1+computecanada ; kiwisolver 1.3.1+computecanada ; matplotlib 3.3.4+computecanada ; mkl 2021.1.1+computecanada; numpy 1.17.3+computecanada ; pandas 1.0.3+computecanada ; patsy 0.5.3+computecanada ; Pillow 8.1.2+computecanada ; pip 20.0.2 ; pymc3 3.1 ; pyparsing 3.1.0 ; python-dateutil 2.8.2+computecanada ; pytz 2023.3+computecanada ; scipy 1.1.0+computecanada ; setuptools 46.1.3 ; six 1.1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8387
https://github.com/broadinstitute/gatk/issues/8387:4872,Deployability,install,install,4872,"sor/blas.py"", line 155, in <module>; from theano.tensor.blas_headers import blas_header_text; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/tensor/blas_headers.py"", line 987, in <module>; if not config.blas.ldflags:; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configparser.py"", line 332, in __get__; val_str = self.default(); File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configdefaults.py"", line 1451, in default_blas_ldflags; check_mkl_openmp(); File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configdefaults.py"", line 1273, in check_mkl_openmp; """"""); RuntimeError: ; Could not import 'mkl'. If you are using conda, update the numpy; packages to the latest build otherwise, set MKL_THREADING_LAYER=GNU in; your environment for MKL 2018. If you have MKL 2017 install and are not in a conda environment you; can set the Theano flag blas.check_openmp to False. Be warned that if; you set this flag and don't set the appropriate environment or make; sure you have the right version you *will* get wrong results. ----. Here is the pip list from my environment:. cached-property 1.5.2+computecanada ; cycler 0.11.0+computecanada ; enum34 1.1.10+computecanada ; gatkpythonpackages 0.1 ; gcnvkernel 0.8 ; h5py 3.1.0+computecanada ; intel-openmp 2021.1.1+computecanada; joblib 0.14.1+computecanada ; kiwisolver 1.3.1+computecanada ; matplotlib 3.3.4+computecanada ; mkl 2021.1.1+computecanada; numpy 1.17.3+computecanada ; pandas 1.0.3+computecanada ; patsy 0.5.3+computecanada ; Pillow 8.1.2+computecanada ; pip 20.0.2 ; pymc3 3.1 ; pyparsing 3.1.0 ; python-dateutil 2.8.2+computecanada ; pytz 2023.3+computecanada ; scipy 1.1.0+computecanada ; setuptools 46.1.3 ; six 1.16.0+computecanada ; tbb 2021.1.1+computecanada; Theano 1.0.4 ; tqdm 4.19.5+computecanada ; wheel 0.34.2 ; ----. I used pyt",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8387
https://github.com/broadinstitute/gatk/issues/8387:6129,Deployability,install,installed,6129," pip list from my environment:. cached-property 1.5.2+computecanada ; cycler 0.11.0+computecanada ; enum34 1.1.10+computecanada ; gatkpythonpackages 0.1 ; gcnvkernel 0.8 ; h5py 3.1.0+computecanada ; intel-openmp 2021.1.1+computecanada; joblib 0.14.1+computecanada ; kiwisolver 1.3.1+computecanada ; matplotlib 3.3.4+computecanada ; mkl 2021.1.1+computecanada; numpy 1.17.3+computecanada ; pandas 1.0.3+computecanada ; patsy 0.5.3+computecanada ; Pillow 8.1.2+computecanada ; pip 20.0.2 ; pymc3 3.1 ; pyparsing 3.1.0 ; python-dateutil 2.8.2+computecanada ; pytz 2023.3+computecanada ; scipy 1.1.0+computecanada ; setuptools 46.1.3 ; six 1.16.0+computecanada ; tbb 2021.1.1+computecanada; Theano 1.0.4 ; tqdm 4.19.5+computecanada ; wheel 0.34.2 ; ----. I used python 3.6.10 as suggested in gatkcondaenv.yml.template and respecting these dependencies found here setup_gcnvkernel.py:. ""theano == 1.0.4"",; ""pymc3 == 3.1"",; ""numpy >= 1.13.1"",; ""scipy >= 0.19.1"",; ""tqdm >= 4.15.0"" . ----. mkl is installed in my environment.; When I do : python -c ""import numpy ; numpy.show_config()"". I get this message:. blas_mkl_info:; libraries = ['mkl_rt', 'pthread']; library_dirs = ['/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64']; define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]; include_dirs = ['/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl', '/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/include', '/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib']; blas_opt_info:; libraries = ['mkl_rt', 'pthread']; library_dirs = ['/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64']; define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]; include_dirs = ['/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl', '/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/inc",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8387
https://github.com/broadinstitute/gatk/issues/8387:3223,Energy Efficiency,reduce,reduce,3223,"ception occurred:. Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/gcnvkernel/__init__.py"", line 1, in <module>; from pymc3 import __version__ as pymc3_version; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/pymc3/__init__.py"", line 5, in <module>; from .distributions import *; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/pymc3/distributions/__init__.py"", line 1, in <module>; from . import timeseries; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/pymc3/distributions/timeseries.py"", line 1, in <module>; import theano.tensor as tt; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/__init__.py"", line 124, in <module>; from theano.scan_module import (scan, map, reduce, foldl, foldr, clone,; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/scan_module/__init__.py"", line 41, in <module>; from theano.scan_module import scan_opt; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/scan_module/scan_opt.py"", line 60, in <module>; from theano import tensor, scalar; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/tensor/__init__.py"", line 17, in <module>; from theano.tensor import blas; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/tensor/blas.py"", line 155, in <module>; from theano.tensor.blas_headers import blas_header_text; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/tensor/blas_headers.py"", line 987, in <module>; if not config.blas.ldflags:; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8387
https://github.com/broadinstitute/gatk/issues/8387:181,Integrability,depend,dependencies,181,"Hello,. I am trying to set up a python environment to use gatk DetermineGermlineContigPloidy module. I cannot use conda. I have tried to install in a virtual python environment the dependencies found in these two files:. gatk/scripts/gatkcondaenv.yml.template ; gatk/src/main/python/org/broadinstitute/hellbender/setup_gcnvkernel.py. I have installed gcnvkernel in my virtual environment. . ----. This is the error message I get when I try to import gcnvkernel: python -c ""import gcnvkernel"". Traceback (most recent call last):; File ""/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.6.10/lib/python3.6/configparser.py"", line 1138, in _unify_values; sectiondict = self._sections[section]; KeyError: 'blas'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configparser.py"", line 168, in fetch_val_for_key; return theano_cfg.get(section, option); File ""/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.6.10/lib/python3.6/configparser.py"", line 781, in get; d = self._unify_values(section, vars); File ""/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.6.10/lib/python3.6/configparser.py"", line 1141, in _unify_values; raise NoSectionError(section); configparser.NoSectionError: No section: 'blas'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configparser.py"", line 328, in __get__; delete_key=delete_key); File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configparser.py"", line 172, in fetch_val_for_key; raise KeyError(key); KeyError: 'blas.ldflags'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File """,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8387
https://github.com/broadinstitute/gatk/issues/8387:415,Integrability,message,message,415,"Hello,. I am trying to set up a python environment to use gatk DetermineGermlineContigPloidy module. I cannot use conda. I have tried to install in a virtual python environment the dependencies found in these two files:. gatk/scripts/gatkcondaenv.yml.template ; gatk/src/main/python/org/broadinstitute/hellbender/setup_gcnvkernel.py. I have installed gcnvkernel in my virtual environment. . ----. This is the error message I get when I try to import gcnvkernel: python -c ""import gcnvkernel"". Traceback (most recent call last):; File ""/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.6.10/lib/python3.6/configparser.py"", line 1138, in _unify_values; sectiondict = self._sections[section]; KeyError: 'blas'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configparser.py"", line 168, in fetch_val_for_key; return theano_cfg.get(section, option); File ""/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.6.10/lib/python3.6/configparser.py"", line 781, in get; d = self._unify_values(section, vars); File ""/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.6.10/lib/python3.6/configparser.py"", line 1141, in _unify_values; raise NoSectionError(section); configparser.NoSectionError: No section: 'blas'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configparser.py"", line 328, in __get__; delete_key=delete_key); File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configparser.py"", line 172, in fetch_val_for_key; raise KeyError(key); KeyError: 'blas.ldflags'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File """,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8387
https://github.com/broadinstitute/gatk/issues/8387:5974,Integrability,depend,dependencies,5974,"ed that if; you set this flag and don't set the appropriate environment or make; sure you have the right version you *will* get wrong results. ----. Here is the pip list from my environment:. cached-property 1.5.2+computecanada ; cycler 0.11.0+computecanada ; enum34 1.1.10+computecanada ; gatkpythonpackages 0.1 ; gcnvkernel 0.8 ; h5py 3.1.0+computecanada ; intel-openmp 2021.1.1+computecanada; joblib 0.14.1+computecanada ; kiwisolver 1.3.1+computecanada ; matplotlib 3.3.4+computecanada ; mkl 2021.1.1+computecanada; numpy 1.17.3+computecanada ; pandas 1.0.3+computecanada ; patsy 0.5.3+computecanada ; Pillow 8.1.2+computecanada ; pip 20.0.2 ; pymc3 3.1 ; pyparsing 3.1.0 ; python-dateutil 2.8.2+computecanada ; pytz 2023.3+computecanada ; scipy 1.1.0+computecanada ; setuptools 46.1.3 ; six 1.16.0+computecanada ; tbb 2021.1.1+computecanada; Theano 1.0.4 ; tqdm 4.19.5+computecanada ; wheel 0.34.2 ; ----. I used python 3.6.10 as suggested in gatkcondaenv.yml.template and respecting these dependencies found here setup_gcnvkernel.py:. ""theano == 1.0.4"",; ""pymc3 == 3.1"",; ""numpy >= 1.13.1"",; ""scipy >= 0.19.1"",; ""tqdm >= 4.15.0"" . ----. mkl is installed in my environment.; When I do : python -c ""import numpy ; numpy.show_config()"". I get this message:. blas_mkl_info:; libraries = ['mkl_rt', 'pthread']; library_dirs = ['/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64']; define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]; include_dirs = ['/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl', '/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/include', '/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib']; blas_opt_info:; libraries = ['mkl_rt', 'pthread']; library_dirs = ['/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64']; define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]; include_dirs = ['/cv",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8387
https://github.com/broadinstitute/gatk/issues/8387:6230,Integrability,message,message,6230,"tecanada ; enum34 1.1.10+computecanada ; gatkpythonpackages 0.1 ; gcnvkernel 0.8 ; h5py 3.1.0+computecanada ; intel-openmp 2021.1.1+computecanada; joblib 0.14.1+computecanada ; kiwisolver 1.3.1+computecanada ; matplotlib 3.3.4+computecanada ; mkl 2021.1.1+computecanada; numpy 1.17.3+computecanada ; pandas 1.0.3+computecanada ; patsy 0.5.3+computecanada ; Pillow 8.1.2+computecanada ; pip 20.0.2 ; pymc3 3.1 ; pyparsing 3.1.0 ; python-dateutil 2.8.2+computecanada ; pytz 2023.3+computecanada ; scipy 1.1.0+computecanada ; setuptools 46.1.3 ; six 1.16.0+computecanada ; tbb 2021.1.1+computecanada; Theano 1.0.4 ; tqdm 4.19.5+computecanada ; wheel 0.34.2 ; ----. I used python 3.6.10 as suggested in gatkcondaenv.yml.template and respecting these dependencies found here setup_gcnvkernel.py:. ""theano == 1.0.4"",; ""pymc3 == 3.1"",; ""numpy >= 1.13.1"",; ""scipy >= 0.19.1"",; ""tqdm >= 4.15.0"" . ----. mkl is installed in my environment.; When I do : python -c ""import numpy ; numpy.show_config()"". I get this message:. blas_mkl_info:; libraries = ['mkl_rt', 'pthread']; library_dirs = ['/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64']; define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]; include_dirs = ['/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl', '/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/include', '/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib']; blas_opt_info:; libraries = ['mkl_rt', 'pthread']; library_dirs = ['/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64']; define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]; include_dirs = ['/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl', '/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/include', '/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib'",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8387
https://github.com/broadinstitute/gatk/issues/8387:8272,Integrability,depend,dependency,8272,"/imkl/2020.1.217/mkl/lib/intel64']; define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]; include_dirs = ['/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl', '/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/include', '/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib']; blas_opt_info:; libraries = ['mkl_rt', 'pthread']; library_dirs = ['/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64']; define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]; include_dirs = ['/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl', '/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/include', '/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib']; lapack_mkl_info:; libraries = ['mkl_rt', 'pthread']; library_dirs = ['/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64']; define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]; include_dirs = ['/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl', '/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/include', '/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib']; lapack_opt_info:; libraries = ['mkl_rt', 'pthread']; library_dirs = ['/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64']; define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]; include_dirs = ['/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl', '/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/include', '/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib']. -----. Is it possible to have an up-to-date dependency list for me to have a functional gcnvkernel module?. Thanks for your help,. Hélène",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8387
https://github.com/broadinstitute/gatk/issues/8387:628,Modifiability,config,configparser,628,"Hello,. I am trying to set up a python environment to use gatk DetermineGermlineContigPloidy module. I cannot use conda. I have tried to install in a virtual python environment the dependencies found in these two files:. gatk/scripts/gatkcondaenv.yml.template ; gatk/src/main/python/org/broadinstitute/hellbender/setup_gcnvkernel.py. I have installed gcnvkernel in my virtual environment. . ----. This is the error message I get when I try to import gcnvkernel: python -c ""import gcnvkernel"". Traceback (most recent call last):; File ""/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.6.10/lib/python3.6/configparser.py"", line 1138, in _unify_values; sectiondict = self._sections[section]; KeyError: 'blas'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configparser.py"", line 168, in fetch_val_for_key; return theano_cfg.get(section, option); File ""/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.6.10/lib/python3.6/configparser.py"", line 781, in get; d = self._unify_values(section, vars); File ""/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.6.10/lib/python3.6/configparser.py"", line 1141, in _unify_values; raise NoSectionError(section); configparser.NoSectionError: No section: 'blas'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configparser.py"", line 328, in __get__; delete_key=delete_key); File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configparser.py"", line 172, in fetch_val_for_key; raise KeyError(key); KeyError: 'blas.ldflags'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File """,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8387
https://github.com/broadinstitute/gatk/issues/8387:936,Modifiability,config,configparser,936,"Hello,. I am trying to set up a python environment to use gatk DetermineGermlineContigPloidy module. I cannot use conda. I have tried to install in a virtual python environment the dependencies found in these two files:. gatk/scripts/gatkcondaenv.yml.template ; gatk/src/main/python/org/broadinstitute/hellbender/setup_gcnvkernel.py. I have installed gcnvkernel in my virtual environment. . ----. This is the error message I get when I try to import gcnvkernel: python -c ""import gcnvkernel"". Traceback (most recent call last):; File ""/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.6.10/lib/python3.6/configparser.py"", line 1138, in _unify_values; sectiondict = self._sections[section]; KeyError: 'blas'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configparser.py"", line 168, in fetch_val_for_key; return theano_cfg.get(section, option); File ""/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.6.10/lib/python3.6/configparser.py"", line 781, in get; d = self._unify_values(section, vars); File ""/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.6.10/lib/python3.6/configparser.py"", line 1141, in _unify_values; raise NoSectionError(section); configparser.NoSectionError: No section: 'blas'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configparser.py"", line 328, in __get__; delete_key=delete_key); File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configparser.py"", line 172, in fetch_val_for_key; raise KeyError(key); KeyError: 'blas.ldflags'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File """,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8387
https://github.com/broadinstitute/gatk/issues/8387:1125,Modifiability,config,configparser,1125,"ied to install in a virtual python environment the dependencies found in these two files:. gatk/scripts/gatkcondaenv.yml.template ; gatk/src/main/python/org/broadinstitute/hellbender/setup_gcnvkernel.py. I have installed gcnvkernel in my virtual environment. . ----. This is the error message I get when I try to import gcnvkernel: python -c ""import gcnvkernel"". Traceback (most recent call last):; File ""/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.6.10/lib/python3.6/configparser.py"", line 1138, in _unify_values; sectiondict = self._sections[section]; KeyError: 'blas'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configparser.py"", line 168, in fetch_val_for_key; return theano_cfg.get(section, option); File ""/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.6.10/lib/python3.6/configparser.py"", line 781, in get; d = self._unify_values(section, vars); File ""/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.6.10/lib/python3.6/configparser.py"", line 1141, in _unify_values; raise NoSectionError(section); configparser.NoSectionError: No section: 'blas'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configparser.py"", line 328, in __get__; delete_key=delete_key); File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configparser.py"", line 172, in fetch_val_for_key; raise KeyError(key); KeyError: 'blas.ldflags'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configdefaults.py"", line 1256, in che",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8387
https://github.com/broadinstitute/gatk/issues/8387:1299,Modifiability,config,configparser,1299,"llbender/setup_gcnvkernel.py. I have installed gcnvkernel in my virtual environment. . ----. This is the error message I get when I try to import gcnvkernel: python -c ""import gcnvkernel"". Traceback (most recent call last):; File ""/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.6.10/lib/python3.6/configparser.py"", line 1138, in _unify_values; sectiondict = self._sections[section]; KeyError: 'blas'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configparser.py"", line 168, in fetch_val_for_key; return theano_cfg.get(section, option); File ""/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.6.10/lib/python3.6/configparser.py"", line 781, in get; d = self._unify_values(section, vars); File ""/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.6.10/lib/python3.6/configparser.py"", line 1141, in _unify_values; raise NoSectionError(section); configparser.NoSectionError: No section: 'blas'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configparser.py"", line 328, in __get__; delete_key=delete_key); File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configparser.py"", line 172, in fetch_val_for_key; raise KeyError(key); KeyError: 'blas.ldflags'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configdefaults.py"", line 1256, in check_mkl_openmp; import mkl; ModuleNotFoundError: No module named 'mkl'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8387
https://github.com/broadinstitute/gatk/issues/8387:1377,Modifiability,config,configparser,1377,"gcnvkernel in my virtual environment. . ----. This is the error message I get when I try to import gcnvkernel: python -c ""import gcnvkernel"". Traceback (most recent call last):; File ""/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.6.10/lib/python3.6/configparser.py"", line 1138, in _unify_values; sectiondict = self._sections[section]; KeyError: 'blas'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configparser.py"", line 168, in fetch_val_for_key; return theano_cfg.get(section, option); File ""/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.6.10/lib/python3.6/configparser.py"", line 781, in get; d = self._unify_values(section, vars); File ""/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.6.10/lib/python3.6/configparser.py"", line 1141, in _unify_values; raise NoSectionError(section); configparser.NoSectionError: No section: 'blas'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configparser.py"", line 328, in __get__; delete_key=delete_key); File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configparser.py"", line 172, in fetch_val_for_key; raise KeyError(key); KeyError: 'blas.ldflags'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configdefaults.py"", line 1256, in check_mkl_openmp; import mkl; ModuleNotFoundError: No module named 'mkl'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File """,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8387
https://github.com/broadinstitute/gatk/issues/8387:1630,Modifiability,config,configparser,1630,".6/configparser.py"", line 1138, in _unify_values; sectiondict = self._sections[section]; KeyError: 'blas'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configparser.py"", line 168, in fetch_val_for_key; return theano_cfg.get(section, option); File ""/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.6.10/lib/python3.6/configparser.py"", line 781, in get; d = self._unify_values(section, vars); File ""/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.6.10/lib/python3.6/configparser.py"", line 1141, in _unify_values; raise NoSectionError(section); configparser.NoSectionError: No section: 'blas'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configparser.py"", line 328, in __get__; delete_key=delete_key); File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configparser.py"", line 172, in fetch_val_for_key; raise KeyError(key); KeyError: 'blas.ldflags'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configdefaults.py"", line 1256, in check_mkl_openmp; import mkl; ModuleNotFoundError: No module named 'mkl'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/gcnvkernel/__init__.py"", line 1, in <module>; from pymc3 import __version__ as pymc3_version; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/py",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8387
https://github.com/broadinstitute/gatk/issues/8387:1793,Modifiability,config,configparser,1793,"n occurred:. Traceback (most recent call last):; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configparser.py"", line 168, in fetch_val_for_key; return theano_cfg.get(section, option); File ""/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.6.10/lib/python3.6/configparser.py"", line 781, in get; d = self._unify_values(section, vars); File ""/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.6.10/lib/python3.6/configparser.py"", line 1141, in _unify_values; raise NoSectionError(section); configparser.NoSectionError: No section: 'blas'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configparser.py"", line 328, in __get__; delete_key=delete_key); File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configparser.py"", line 172, in fetch_val_for_key; raise KeyError(key); KeyError: 'blas.ldflags'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configdefaults.py"", line 1256, in check_mkl_openmp; import mkl; ModuleNotFoundError: No module named 'mkl'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/gcnvkernel/__init__.py"", line 1, in <module>; from pymc3 import __version__ as pymc3_version; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/pymc3/__init__.py"", line 5, in <module>; from .distributions import *; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/py",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8387
https://github.com/broadinstitute/gatk/issues/8387:2094,Modifiability,config,configdefaults,2094,"2/Core/python/3.6.10/lib/python3.6/configparser.py"", line 781, in get; d = self._unify_values(section, vars); File ""/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.6.10/lib/python3.6/configparser.py"", line 1141, in _unify_values; raise NoSectionError(section); configparser.NoSectionError: No section: 'blas'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configparser.py"", line 328, in __get__; delete_key=delete_key); File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configparser.py"", line 172, in fetch_val_for_key; raise KeyError(key); KeyError: 'blas.ldflags'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configdefaults.py"", line 1256, in check_mkl_openmp; import mkl; ModuleNotFoundError: No module named 'mkl'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/gcnvkernel/__init__.py"", line 1, in <module>; from pymc3 import __version__ as pymc3_version; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/pymc3/__init__.py"", line 5, in <module>; from .distributions import *; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/pymc3/distributions/__init__.py"", line 1, in <module>; from . import timeseries; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/pymc3/distributions/timeseries.py"", line 1, in <module>; import theano.tensor as tt; File ""/lustre04/scratch/helene/Ticket/0196857/",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8387
https://github.com/broadinstitute/gatk/issues/8387:4147,Modifiability,config,config,4147,"es/theano/__init__.py"", line 124, in <module>; from theano.scan_module import (scan, map, reduce, foldl, foldr, clone,; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/scan_module/__init__.py"", line 41, in <module>; from theano.scan_module import scan_opt; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/scan_module/scan_opt.py"", line 60, in <module>; from theano import tensor, scalar; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/tensor/__init__.py"", line 17, in <module>; from theano.tensor import blas; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/tensor/blas.py"", line 155, in <module>; from theano.tensor.blas_headers import blas_header_text; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/tensor/blas_headers.py"", line 987, in <module>; if not config.blas.ldflags:; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configparser.py"", line 332, in __get__; val_str = self.default(); File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configdefaults.py"", line 1451, in default_blas_ldflags; check_mkl_openmp(); File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configdefaults.py"", line 1273, in check_mkl_openmp; """"""); RuntimeError: ; Could not import 'mkl'. If you are using conda, update the numpy; packages to the latest build otherwise, set MKL_THREADING_LAYER=GNU in; your environment for MKL 2018. If you have MKL 2017 install and are not in a conda environment you; can set the Theano flag blas.check_openmp to False. Be warned that if; you set this flag and don't set the appropriate environment or make; sure you have the right version you *will* get wrong results. ----. Here ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8387
https://github.com/broadinstitute/gatk/issues/8387:4268,Modifiability,config,configparser,4268,"tre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/scan_module/__init__.py"", line 41, in <module>; from theano.scan_module import scan_opt; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/scan_module/scan_opt.py"", line 60, in <module>; from theano import tensor, scalar; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/tensor/__init__.py"", line 17, in <module>; from theano.tensor import blas; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/tensor/blas.py"", line 155, in <module>; from theano.tensor.blas_headers import blas_header_text; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/tensor/blas_headers.py"", line 987, in <module>; if not config.blas.ldflags:; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configparser.py"", line 332, in __get__; val_str = self.default(); File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configdefaults.py"", line 1451, in default_blas_ldflags; check_mkl_openmp(); File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configdefaults.py"", line 1273, in check_mkl_openmp; """"""); RuntimeError: ; Could not import 'mkl'. If you are using conda, update the numpy; packages to the latest build otherwise, set MKL_THREADING_LAYER=GNU in; your environment for MKL 2018. If you have MKL 2017 install and are not in a conda environment you; can set the Theano flag blas.check_openmp to False. Be warned that if; you set this flag and don't set the appropriate environment or make; sure you have the right version you *will* get wrong results. ----. Here is the pip list from my environment:. cached-property 1.5.2+computecanada ; cycler 0.11.0+computecanada ; enum34 1.1.10+computecan",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8387
https://github.com/broadinstitute/gatk/issues/8387:4433,Modifiability,config,configdefaults,4433,"t scan_opt; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/scan_module/scan_opt.py"", line 60, in <module>; from theano import tensor, scalar; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/tensor/__init__.py"", line 17, in <module>; from theano.tensor import blas; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/tensor/blas.py"", line 155, in <module>; from theano.tensor.blas_headers import blas_header_text; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/tensor/blas_headers.py"", line 987, in <module>; if not config.blas.ldflags:; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configparser.py"", line 332, in __get__; val_str = self.default(); File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configdefaults.py"", line 1451, in default_blas_ldflags; check_mkl_openmp(); File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configdefaults.py"", line 1273, in check_mkl_openmp; """"""); RuntimeError: ; Could not import 'mkl'. If you are using conda, update the numpy; packages to the latest build otherwise, set MKL_THREADING_LAYER=GNU in; your environment for MKL 2018. If you have MKL 2017 install and are not in a conda environment you; can set the Theano flag blas.check_openmp to False. Be warned that if; you set this flag and don't set the appropriate environment or make; sure you have the right version you *will* get wrong results. ----. Here is the pip list from my environment:. cached-property 1.5.2+computecanada ; cycler 0.11.0+computecanada ; enum34 1.1.10+computecanada ; gatkpythonpackages 0.1 ; gcnvkernel 0.8 ; h5py 3.1.0+computecanada ; intel-openmp 2021.1.1+computecanada; joblib 0.14.1+computecanada ; kiwisolver 1.3.1+compute",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8387
https://github.com/broadinstitute/gatk/issues/8387:4608,Modifiability,config,configdefaults,4608,"rt tensor, scalar; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/tensor/__init__.py"", line 17, in <module>; from theano.tensor import blas; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/tensor/blas.py"", line 155, in <module>; from theano.tensor.blas_headers import blas_header_text; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/tensor/blas_headers.py"", line 987, in <module>; if not config.blas.ldflags:; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configparser.py"", line 332, in __get__; val_str = self.default(); File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configdefaults.py"", line 1451, in default_blas_ldflags; check_mkl_openmp(); File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configdefaults.py"", line 1273, in check_mkl_openmp; """"""); RuntimeError: ; Could not import 'mkl'. If you are using conda, update the numpy; packages to the latest build otherwise, set MKL_THREADING_LAYER=GNU in; your environment for MKL 2018. If you have MKL 2017 install and are not in a conda environment you; can set the Theano flag blas.check_openmp to False. Be warned that if; you set this flag and don't set the appropriate environment or make; sure you have the right version you *will* get wrong results. ----. Here is the pip list from my environment:. cached-property 1.5.2+computecanada ; cycler 0.11.0+computecanada ; enum34 1.1.10+computecanada ; gatkpythonpackages 0.1 ; gcnvkernel 0.8 ; h5py 3.1.0+computecanada ; intel-openmp 2021.1.1+computecanada; joblib 0.14.1+computecanada ; kiwisolver 1.3.1+computecanada ; matplotlib 3.3.4+computecanada ; mkl 2021.1.1+computecanada; numpy 1.17.3+computecanada ; pandas 1.0.3+computecanada ; patsy 0.5.3+computecanada ; Pillow 8.1.2+comput",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8387
https://github.com/broadinstitute/gatk/issues/8387:5171,Performance,cache,cached-property,5171,"tre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configparser.py"", line 332, in __get__; val_str = self.default(); File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configdefaults.py"", line 1451, in default_blas_ldflags; check_mkl_openmp(); File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configdefaults.py"", line 1273, in check_mkl_openmp; """"""); RuntimeError: ; Could not import 'mkl'. If you are using conda, update the numpy; packages to the latest build otherwise, set MKL_THREADING_LAYER=GNU in; your environment for MKL 2018. If you have MKL 2017 install and are not in a conda environment you; can set the Theano flag blas.check_openmp to False. Be warned that if; you set this flag and don't set the appropriate environment or make; sure you have the right version you *will* get wrong results. ----. Here is the pip list from my environment:. cached-property 1.5.2+computecanada ; cycler 0.11.0+computecanada ; enum34 1.1.10+computecanada ; gatkpythonpackages 0.1 ; gcnvkernel 0.8 ; h5py 3.1.0+computecanada ; intel-openmp 2021.1.1+computecanada; joblib 0.14.1+computecanada ; kiwisolver 1.3.1+computecanada ; matplotlib 3.3.4+computecanada ; mkl 2021.1.1+computecanada; numpy 1.17.3+computecanada ; pandas 1.0.3+computecanada ; patsy 0.5.3+computecanada ; Pillow 8.1.2+computecanada ; pip 20.0.2 ; pymc3 3.1 ; pyparsing 3.1.0 ; python-dateutil 2.8.2+computecanada ; pytz 2023.3+computecanada ; scipy 1.1.0+computecanada ; setuptools 46.1.3 ; six 1.16.0+computecanada ; tbb 2021.1.1+computecanada; Theano 1.0.4 ; tqdm 4.19.5+computecanada ; wheel 0.34.2 ; ----. I used python 3.6.10 as suggested in gatkcondaenv.yml.template and respecting these dependencies found here setup_gcnvkernel.py:. ""theano == 1.0.4"",; ""pymc3 == 3.1"",; ""numpy >= 1.13.1"",; ""scipy >= 0.19.1"",; ""tqdm >= 4.15.0"" . ----. mkl is installed in my environment.; When I do : python -c",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8387
https://github.com/broadinstitute/gatk/pull/8388:662,Deployability,Integrat,Integration,662,"This PR fixes a bug I found in testing. I was extracting all the samples for an Exome run and it was widely scattered. So there occurred a situation where there no VET entries in one of the shards and a NPE happened.; This PR fixes that and makes it tool generate an empty (well, has a header) VCF, which the GVS workflow can handle. Failing workflow showing the problem [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Exome%20Data%2049k/job_history/9f821329-f2bd-487c-a9af-4a81d0716072). Passing workflow (after the fix) [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Exome%20Data%2049k/job_history/52ecbbaa-199d-413b-95fe-2a3285462b43); Passing Integration Test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/a2f67baa-9613-4c4e-be3b-85a1b25a3b3b)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8388
https://github.com/broadinstitute/gatk/pull/8388:662,Integrability,Integrat,Integration,662,"This PR fixes a bug I found in testing. I was extracting all the samples for an Exome run and it was widely scattered. So there occurred a situation where there no VET entries in one of the shards and a NPE happened.; This PR fixes that and makes it tool generate an empty (well, has a header) VCF, which the GVS workflow can handle. Failing workflow showing the problem [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Exome%20Data%2049k/job_history/9f821329-f2bd-487c-a9af-4a81d0716072). Passing workflow (after the fix) [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Exome%20Data%2049k/job_history/52ecbbaa-199d-413b-95fe-2a3285462b43); Passing Integration Test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/a2f67baa-9613-4c4e-be3b-85a1b25a3b3b)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8388
https://github.com/broadinstitute/gatk/pull/8388:31,Testability,test,testing,31,"This PR fixes a bug I found in testing. I was extracting all the samples for an Exome run and it was widely scattered. So there occurred a situation where there no VET entries in one of the shards and a NPE happened.; This PR fixes that and makes it tool generate an empty (well, has a header) VCF, which the GVS workflow can handle. Failing workflow showing the problem [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Exome%20Data%2049k/job_history/9f821329-f2bd-487c-a9af-4a81d0716072). Passing workflow (after the fix) [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Exome%20Data%2049k/job_history/52ecbbaa-199d-413b-95fe-2a3285462b43); Passing Integration Test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/a2f67baa-9613-4c4e-be3b-85a1b25a3b3b)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8388
https://github.com/broadinstitute/gatk/pull/8388:674,Testability,Test,Test,674,"This PR fixes a bug I found in testing. I was extracting all the samples for an Exome run and it was widely scattered. So there occurred a situation where there no VET entries in one of the shards and a NPE happened.; This PR fixes that and makes it tool generate an empty (well, has a header) VCF, which the GVS workflow can handle. Failing workflow showing the problem [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Exome%20Data%2049k/job_history/9f821329-f2bd-487c-a9af-4a81d0716072). Passing workflow (after the fix) [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Exome%20Data%2049k/job_history/52ecbbaa-199d-413b-95fe-2a3285462b43); Passing Integration Test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/a2f67baa-9613-4c4e-be3b-85a1b25a3b3b)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8388
https://github.com/broadinstitute/gatk/issues/8394:241,Availability,error,error,241,"I was trying to use SVAnnotate to generate annotation for my own `vcf` file:; ```; $java17 -jar $gatkjar SVAnnotate -V 31354420/250000.vcf --protein-coding-gtf $newgtf_path -O 31354420/annotated_250000.vcf; ```; However, I got the following error message:; ```bash; java.lang.NullPointerException: Cannot invoke ""org.broadinstitute.hellbender.utils.codecs.gtf.GencodeGtfGeneFeature.addTranscript(org.banscriptFeature)"" because ""gene"" is null; at org.broadinstitute.hellbender.utils.codecs.gtf.AbstractGtfCodec.aggregateRecordsIntoGeneFeature(AbstractGtfCodec.java:339); at org.broadinstitute.hellbender.utils.codecs.gtf.AbstractGtfCodec.decode(AbstractGtfCodec.java:170); at org.broadinstitute.hellbender.utils.codecs.gtf.AbstractGtfCodec.decode(AbstractGtfCodec.java:23); at htsjdk.tribble.TribbleIndexedFeatureReader$WFIterator.readNextRecord(TribbleIndexedFeatureReader.java:376); at htsjdk.tribble.TribbleIndexedFeatureReader$WFIterator.<init>(TribbleIndexedFeatureReader.java:343); at htsjdk.tribble.TribbleIndexedFeatureReader.iterator(TribbleIndexedFeatureReader.java:310); at org.broadinstitute.hellbender.engine.FeatureDataSource.iterator(FeatureDataSource.java:531); at org.broadinstitute.hellbender.tools.walkers.sv.SVAnnotate.buildIntervalTreesFromGTF(SVAnnotate.java:297); at org.broadinstitute.hellbender.tools.walkers.sv.SVAnnotate.onTraversalStart(SVAnnotate.java:227); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1096); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:149); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:198); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:217); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. Seems like these ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8394
https://github.com/broadinstitute/gatk/issues/8394:2001,Availability,error,errors,2001,"e ""org.broadinstitute.hellbender.utils.codecs.gtf.GencodeGtfGeneFeature.addTranscript(org.banscriptFeature)"" because ""gene"" is null; at org.broadinstitute.hellbender.utils.codecs.gtf.AbstractGtfCodec.aggregateRecordsIntoGeneFeature(AbstractGtfCodec.java:339); at org.broadinstitute.hellbender.utils.codecs.gtf.AbstractGtfCodec.decode(AbstractGtfCodec.java:170); at org.broadinstitute.hellbender.utils.codecs.gtf.AbstractGtfCodec.decode(AbstractGtfCodec.java:23); at htsjdk.tribble.TribbleIndexedFeatureReader$WFIterator.readNextRecord(TribbleIndexedFeatureReader.java:376); at htsjdk.tribble.TribbleIndexedFeatureReader$WFIterator.<init>(TribbleIndexedFeatureReader.java:343); at htsjdk.tribble.TribbleIndexedFeatureReader.iterator(TribbleIndexedFeatureReader.java:310); at org.broadinstitute.hellbender.engine.FeatureDataSource.iterator(FeatureDataSource.java:531); at org.broadinstitute.hellbender.tools.walkers.sv.SVAnnotate.buildIntervalTreesFromGTF(SVAnnotate.java:297); at org.broadinstitute.hellbender.tools.walkers.sv.SVAnnotate.onTraversalStart(SVAnnotate.java:227); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1096); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:149); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:198); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:217); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. Seems like these errors are from compiled code and i couldn't locate the exact script. ; It will be great if you could help me with the following questions so i can fix these issues myself:; **what does the `gene` refer to? Could you please provide a template input `.gtf` file so i can better know how to call `SVAnnotate`?**",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8394
https://github.com/broadinstitute/gatk/issues/8394:247,Integrability,message,message,247,"I was trying to use SVAnnotate to generate annotation for my own `vcf` file:; ```; $java17 -jar $gatkjar SVAnnotate -V 31354420/250000.vcf --protein-coding-gtf $newgtf_path -O 31354420/annotated_250000.vcf; ```; However, I got the following error message:; ```bash; java.lang.NullPointerException: Cannot invoke ""org.broadinstitute.hellbender.utils.codecs.gtf.GencodeGtfGeneFeature.addTranscript(org.banscriptFeature)"" because ""gene"" is null; at org.broadinstitute.hellbender.utils.codecs.gtf.AbstractGtfCodec.aggregateRecordsIntoGeneFeature(AbstractGtfCodec.java:339); at org.broadinstitute.hellbender.utils.codecs.gtf.AbstractGtfCodec.decode(AbstractGtfCodec.java:170); at org.broadinstitute.hellbender.utils.codecs.gtf.AbstractGtfCodec.decode(AbstractGtfCodec.java:23); at htsjdk.tribble.TribbleIndexedFeatureReader$WFIterator.readNextRecord(TribbleIndexedFeatureReader.java:376); at htsjdk.tribble.TribbleIndexedFeatureReader$WFIterator.<init>(TribbleIndexedFeatureReader.java:343); at htsjdk.tribble.TribbleIndexedFeatureReader.iterator(TribbleIndexedFeatureReader.java:310); at org.broadinstitute.hellbender.engine.FeatureDataSource.iterator(FeatureDataSource.java:531); at org.broadinstitute.hellbender.tools.walkers.sv.SVAnnotate.buildIntervalTreesFromGTF(SVAnnotate.java:297); at org.broadinstitute.hellbender.tools.walkers.sv.SVAnnotate.onTraversalStart(SVAnnotate.java:227); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1096); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:149); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:198); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:217); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. Seems like these ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8394
https://github.com/broadinstitute/gatk/pull/8395:7,Modifiability,variab,variable,7,create variable for java xmx size in picard,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8395
https://github.com/broadinstitute/gatk/pull/8397:140,Deployability,update,updated,140,A PR updating beta documentation to reflect the changes in user experience with bulk ingestion. Do not merge until workspace is ready to be updated to reflect this documentation.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8397
https://github.com/broadinstitute/gatk/pull/8397:59,Usability,user experience,user experience,59,A PR updating beta documentation to reflect the changes in user experience with bulk ingestion. Do not merge until workspace is ready to be updated to reflect this documentation.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8397
https://github.com/broadinstitute/gatk/pull/8401:28,Deployability,update,updates,28,Updating dockers to include updates to Google cloud storage libraries,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8401
https://github.com/broadinstitute/gatk/issues/8402:319,Availability,avail,available,319,"Dear *,; I just tested the Docker Container 4.4.0.0 (upgrade from 4.3.0.0) and my pipeline tried to execute BaseRecalibrator and ApplyBQSR. It crashed with a message:; `/usr/bin/env: 'python': No such file or directory`. A simple test with a python script, using ""/usr/bin/python"" in shebang revealed that is truly not available in this path.; A test with the 4.3.0.0 Container worked. I solved the issue by typing ; `ln -s /usr/bin/python3 /usr/bin/python`. Kind regards,; Daniel",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8402
https://github.com/broadinstitute/gatk/issues/8402:53,Deployability,upgrade,upgrade,53,"Dear *,; I just tested the Docker Container 4.4.0.0 (upgrade from 4.3.0.0) and my pipeline tried to execute BaseRecalibrator and ApplyBQSR. It crashed with a message:; `/usr/bin/env: 'python': No such file or directory`. A simple test with a python script, using ""/usr/bin/python"" in shebang revealed that is truly not available in this path.; A test with the 4.3.0.0 Container worked. I solved the issue by typing ; `ln -s /usr/bin/python3 /usr/bin/python`. Kind regards,; Daniel",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8402
https://github.com/broadinstitute/gatk/issues/8402:82,Deployability,pipeline,pipeline,82,"Dear *,; I just tested the Docker Container 4.4.0.0 (upgrade from 4.3.0.0) and my pipeline tried to execute BaseRecalibrator and ApplyBQSR. It crashed with a message:; `/usr/bin/env: 'python': No such file or directory`. A simple test with a python script, using ""/usr/bin/python"" in shebang revealed that is truly not available in this path.; A test with the 4.3.0.0 Container worked. I solved the issue by typing ; `ln -s /usr/bin/python3 /usr/bin/python`. Kind regards,; Daniel",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8402
https://github.com/broadinstitute/gatk/issues/8402:158,Integrability,message,message,158,"Dear *,; I just tested the Docker Container 4.4.0.0 (upgrade from 4.3.0.0) and my pipeline tried to execute BaseRecalibrator and ApplyBQSR. It crashed with a message:; `/usr/bin/env: 'python': No such file or directory`. A simple test with a python script, using ""/usr/bin/python"" in shebang revealed that is truly not available in this path.; A test with the 4.3.0.0 Container worked. I solved the issue by typing ; `ln -s /usr/bin/python3 /usr/bin/python`. Kind regards,; Daniel",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8402
https://github.com/broadinstitute/gatk/issues/8402:16,Testability,test,tested,16,"Dear *,; I just tested the Docker Container 4.4.0.0 (upgrade from 4.3.0.0) and my pipeline tried to execute BaseRecalibrator and ApplyBQSR. It crashed with a message:; `/usr/bin/env: 'python': No such file or directory`. A simple test with a python script, using ""/usr/bin/python"" in shebang revealed that is truly not available in this path.; A test with the 4.3.0.0 Container worked. I solved the issue by typing ; `ln -s /usr/bin/python3 /usr/bin/python`. Kind regards,; Daniel",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8402
https://github.com/broadinstitute/gatk/issues/8402:230,Testability,test,test,230,"Dear *,; I just tested the Docker Container 4.4.0.0 (upgrade from 4.3.0.0) and my pipeline tried to execute BaseRecalibrator and ApplyBQSR. It crashed with a message:; `/usr/bin/env: 'python': No such file or directory`. A simple test with a python script, using ""/usr/bin/python"" in shebang revealed that is truly not available in this path.; A test with the 4.3.0.0 Container worked. I solved the issue by typing ; `ln -s /usr/bin/python3 /usr/bin/python`. Kind regards,; Daniel",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8402
https://github.com/broadinstitute/gatk/issues/8402:346,Testability,test,test,346,"Dear *,; I just tested the Docker Container 4.4.0.0 (upgrade from 4.3.0.0) and my pipeline tried to execute BaseRecalibrator and ApplyBQSR. It crashed with a message:; `/usr/bin/env: 'python': No such file or directory`. A simple test with a python script, using ""/usr/bin/python"" in shebang revealed that is truly not available in this path.; A test with the 4.3.0.0 Container worked. I solved the issue by typing ; `ln -s /usr/bin/python3 /usr/bin/python`. Kind regards,; Daniel",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8402
https://github.com/broadinstitute/gatk/issues/8402:223,Usability,simpl,simple,223,"Dear *,; I just tested the Docker Container 4.4.0.0 (upgrade from 4.3.0.0) and my pipeline tried to execute BaseRecalibrator and ApplyBQSR. It crashed with a message:; `/usr/bin/env: 'python': No such file or directory`. A simple test with a python script, using ""/usr/bin/python"" in shebang revealed that is truly not available in this path.; A test with the 4.3.0.0 Container worked. I solved the issue by typing ; `ln -s /usr/bin/python3 /usr/bin/python`. Kind regards,; Daniel",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8402
https://github.com/broadinstitute/gatk/issues/8403:16,Deployability,update,updated,16,"Dear *,; I just updated the funcotator database for hg19 from 1.6 to 1.7.; Here I observed for EGFR a wrong ENST assignment.; I wanted funcotator to use ENST00000275493, which corresponds to NM_005228.3 ([oncokb.org](https://www.oncokb.org/gene/EGFR)).; I therefore listed the ENST in --transcript-list file, when calling funcotator with --ref-version hg19.; For some reason it works with the 1.6 .database, but not with the 1.7 database.; Here I receive the correct NM_ID in the MAF file, but another ENST number and therefore other protein variants.; The ENST I now got is ENST00000455089. Kind regards,; Daniel",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8403
https://github.com/broadinstitute/gatk/pull/8404:142,Deployability,integrat,integration,142,"This ticket set out to do VS-962 (one FOFN to rule them all) only, but along the way I found I also needed to do VS-984 (use Bulk Ingest from integration test) as the interface to `GvsAssignIds` had changed. After making these changes I realized I had basically done VS-982 as well (use Bulk Ingest in Beta WDL) due to the beta WDL picking up the changes to the unified WDL both it and Quickstart call. - [Successful integration run](https://job-manager.dsde-prod.broadinstitute.org/jobs/ab07dffd-a2ea-4b69-9c4c-eec0019e5b3b); - [Pending beta run](https://job-manager.dsde-prod.broadinstitute.org/jobs/37f6be29-5aae-42c4-a86b-c6d00c3caec5)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8404
https://github.com/broadinstitute/gatk/pull/8404:417,Deployability,integrat,integration,417,"This ticket set out to do VS-962 (one FOFN to rule them all) only, but along the way I found I also needed to do VS-984 (use Bulk Ingest from integration test) as the interface to `GvsAssignIds` had changed. After making these changes I realized I had basically done VS-982 as well (use Bulk Ingest in Beta WDL) due to the beta WDL picking up the changes to the unified WDL both it and Quickstart call. - [Successful integration run](https://job-manager.dsde-prod.broadinstitute.org/jobs/ab07dffd-a2ea-4b69-9c4c-eec0019e5b3b); - [Pending beta run](https://job-manager.dsde-prod.broadinstitute.org/jobs/37f6be29-5aae-42c4-a86b-c6d00c3caec5)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8404
https://github.com/broadinstitute/gatk/pull/8404:142,Integrability,integrat,integration,142,"This ticket set out to do VS-962 (one FOFN to rule them all) only, but along the way I found I also needed to do VS-984 (use Bulk Ingest from integration test) as the interface to `GvsAssignIds` had changed. After making these changes I realized I had basically done VS-982 as well (use Bulk Ingest in Beta WDL) due to the beta WDL picking up the changes to the unified WDL both it and Quickstart call. - [Successful integration run](https://job-manager.dsde-prod.broadinstitute.org/jobs/ab07dffd-a2ea-4b69-9c4c-eec0019e5b3b); - [Pending beta run](https://job-manager.dsde-prod.broadinstitute.org/jobs/37f6be29-5aae-42c4-a86b-c6d00c3caec5)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8404
https://github.com/broadinstitute/gatk/pull/8404:167,Integrability,interface,interface,167,"This ticket set out to do VS-962 (one FOFN to rule them all) only, but along the way I found I also needed to do VS-984 (use Bulk Ingest from integration test) as the interface to `GvsAssignIds` had changed. After making these changes I realized I had basically done VS-982 as well (use Bulk Ingest in Beta WDL) due to the beta WDL picking up the changes to the unified WDL both it and Quickstart call. - [Successful integration run](https://job-manager.dsde-prod.broadinstitute.org/jobs/ab07dffd-a2ea-4b69-9c4c-eec0019e5b3b); - [Pending beta run](https://job-manager.dsde-prod.broadinstitute.org/jobs/37f6be29-5aae-42c4-a86b-c6d00c3caec5)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8404
https://github.com/broadinstitute/gatk/pull/8404:417,Integrability,integrat,integration,417,"This ticket set out to do VS-962 (one FOFN to rule them all) only, but along the way I found I also needed to do VS-984 (use Bulk Ingest from integration test) as the interface to `GvsAssignIds` had changed. After making these changes I realized I had basically done VS-982 as well (use Bulk Ingest in Beta WDL) due to the beta WDL picking up the changes to the unified WDL both it and Quickstart call. - [Successful integration run](https://job-manager.dsde-prod.broadinstitute.org/jobs/ab07dffd-a2ea-4b69-9c4c-eec0019e5b3b); - [Pending beta run](https://job-manager.dsde-prod.broadinstitute.org/jobs/37f6be29-5aae-42c4-a86b-c6d00c3caec5)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8404
https://github.com/broadinstitute/gatk/pull/8404:154,Testability,test,test,154,"This ticket set out to do VS-962 (one FOFN to rule them all) only, but along the way I found I also needed to do VS-984 (use Bulk Ingest from integration test) as the interface to `GvsAssignIds` had changed. After making these changes I realized I had basically done VS-982 as well (use Bulk Ingest in Beta WDL) due to the beta WDL picking up the changes to the unified WDL both it and Quickstart call. - [Successful integration run](https://job-manager.dsde-prod.broadinstitute.org/jobs/ab07dffd-a2ea-4b69-9c4c-eec0019e5b3b); - [Pending beta run](https://job-manager.dsde-prod.broadinstitute.org/jobs/37f6be29-5aae-42c4-a86b-c6d00c3caec5)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8404
https://github.com/broadinstitute/gatk/pull/8406:7,Availability,error,error,7,"due to error `Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space: failed reallocation of scalar replaced objects` during RevertSam step in MitochondriaPipeline.wdl @meganshand",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8406
https://github.com/broadinstitute/gatk/pull/8407:0,Deployability,Update,Update,0,Update documentation to point to newly released BGE exome calling list.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8407
https://github.com/broadinstitute/gatk/pull/8407:39,Deployability,release,released,39,Update documentation to point to newly released BGE exome calling list.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8407
https://github.com/broadinstitute/gatk/pull/8408:179,Security,Validat,Validates,179,"Implements allele collapsing for ""breakend replacement"" BND alleles, as described in section 5.4 of the [VCFv4.2 spec](https://samtools.github.io/hts-specs/VCFv4.2.pdf). Also:; - Validates symbolic alt allele for non-BND SV classes when attempting to collapse multiple alt alleles.; - Greatly improves unit test coverage for `CanonicalSVCollapser`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8408
https://github.com/broadinstitute/gatk/pull/8408:307,Testability,test,test,307,"Implements allele collapsing for ""breakend replacement"" BND alleles, as described in section 5.4 of the [VCFv4.2 spec](https://samtools.github.io/hts-specs/VCFv4.2.pdf). Also:; - Validates symbolic alt allele for non-BND SV classes when attempting to collapse multiple alt alleles.; - Greatly improves unit test coverage for `CanonicalSVCollapser`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8408
https://github.com/broadinstitute/gatk/pull/8409:127,Availability,failure,failure,127,"This release contains important bugfixes, including a fix for https://github.com/broadinstitute/gatk/issues/8141 (intermittent failure to properly compress outputs)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8409
https://github.com/broadinstitute/gatk/pull/8409:5,Deployability,release,release,5,"This release contains important bugfixes, including a fix for https://github.com/broadinstitute/gatk/issues/8141 (intermittent failure to properly compress outputs)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8409
https://github.com/broadinstitute/gatk/pull/8412:173,Deployability,integrat,integration,173,- Remove some unused VCF header fields from ExtractFeatures; - Renamed VQSR Lite fields to their original naming (e.g. AS_VQS_SENS becomes CALIBRATION_SENSITIVITY). Passing integration test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/67a00690-5b74-40fd-a0fb-5ab2b0407a4d) - uses updated truth. Example outputs can be found in [this](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/22134bb6-e4b5-4252-b674-860a1168fb6c) Extract run.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8412
https://github.com/broadinstitute/gatk/pull/8412:327,Deployability,update,updated,327,- Remove some unused VCF header fields from ExtractFeatures; - Renamed VQSR Lite fields to their original naming (e.g. AS_VQS_SENS becomes CALIBRATION_SENSITIVITY). Passing integration test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/67a00690-5b74-40fd-a0fb-5ab2b0407a4d) - uses updated truth. Example outputs can be found in [this](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/22134bb6-e4b5-4252-b674-860a1168fb6c) Extract run.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8412
https://github.com/broadinstitute/gatk/pull/8412:173,Integrability,integrat,integration,173,- Remove some unused VCF header fields from ExtractFeatures; - Renamed VQSR Lite fields to their original naming (e.g. AS_VQS_SENS becomes CALIBRATION_SENSITIVITY). Passing integration test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/67a00690-5b74-40fd-a0fb-5ab2b0407a4d) - uses updated truth. Example outputs can be found in [this](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/22134bb6-e4b5-4252-b674-860a1168fb6c) Extract run.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8412
https://github.com/broadinstitute/gatk/pull/8412:185,Testability,test,test,185,- Remove some unused VCF header fields from ExtractFeatures; - Renamed VQSR Lite fields to their original naming (e.g. AS_VQS_SENS becomes CALIBRATION_SENSITIVITY). Passing integration test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/67a00690-5b74-40fd-a0fb-5ab2b0407a4d) - uses updated truth. Example outputs can be found in [this](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/22134bb6-e4b5-4252-b674-860a1168fb6c) Extract run.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8412
https://github.com/broadinstitute/gatk/issues/8414:295,Deployability,pipeline,pipeline,295,"Hi everyone,. I am going to use GATK4 to carry out population-based snp-calling for around 500 samples with the ploidy level from 2 to 6.; The sequence depth varied from 2x to 80x. Since there are many samples with low sequence depth (<5x), I was advised to use the population-based snp-calling pipeline. ; Recently, I met some problems in the flowchart of this analysis. Firstly, I found the pipelines that detailedly explained the flowchart of BQSR and variant calling for single-sample snp-calling. But I have no idea about the pipeline of the BQSR in population-based snp-calling. Should I apply the BQSR sample by sample following the flowchart menthioned above (the command line below), and then used these corrected bam files to identify the variants based on population-based method? Or there is another correct pipeline for this analysis. **Command lines:**. ```; for i in sample1.bam sample2.bam sample3.bam; do; java -jar picard.jar MarkDuplicates I=$i O=$i_bam CREATE_INDEX=true M=$i_metrics; gatk HaplotypeCaller -R $reference -I $i.sorted.dedup.bam -O ""$i"".g.vcf.gz --tmp-dir ./tmp -ERC GVCF; #SNP; gatk GenotypeGVCFs -R $reference -V ""$i"".g.vcf.gz -O ""$i"".vcf.gz; gatk SelectVariants -R $reference -V ""$i"".vcf.gz -select-type SNP -O ""$i"".snp.vcf.gz; gatk VariantFiltration -R $reference -V $path_BQSR/bqsr1.snp.vcf.gz -filter ""QD < 2.0"" --filter-name ""QD2"" -filter ""FS > 60.0"" --filter-name ""FS60"" -filter ""MQ < 40.0"" --filter-name ""MQ40"" -filter ""MQRankSum < -12.5"" --filter-name ""MQRankSum-12.5"" -filter ""ReadPosRankSum < -8.0"" --filter-name ""ReadPosRankSum-8.0"" -O $""i"".filtered.snp.vcf.gz; gatk SelectVariants -R $reference -V $""i"".filtered.snp.vcf.gz --exclude-filtered -O ""$i"".select.filtered.snp.vcf.gz; #INDEL; gatk SelectVariants -R $reference -V ""$i"".vcf.gz -select-type INDEL -O ""$i"".indel.vcf.gz; gatk VariantFiltration -R $reference -V ""$i"".indel.vcf.gz -filter ""QD < 2.0"" --filter-name ""QD2"" -filter ""FS > 200.0"" --filter-name ""FS200"" -filter ""ReadPosRankSum < -20.0"" --fi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8414
https://github.com/broadinstitute/gatk/issues/8414:393,Deployability,pipeline,pipelines,393,"Hi everyone,. I am going to use GATK4 to carry out population-based snp-calling for around 500 samples with the ploidy level from 2 to 6.; The sequence depth varied from 2x to 80x. Since there are many samples with low sequence depth (<5x), I was advised to use the population-based snp-calling pipeline. ; Recently, I met some problems in the flowchart of this analysis. Firstly, I found the pipelines that detailedly explained the flowchart of BQSR and variant calling for single-sample snp-calling. But I have no idea about the pipeline of the BQSR in population-based snp-calling. Should I apply the BQSR sample by sample following the flowchart menthioned above (the command line below), and then used these corrected bam files to identify the variants based on population-based method? Or there is another correct pipeline for this analysis. **Command lines:**. ```; for i in sample1.bam sample2.bam sample3.bam; do; java -jar picard.jar MarkDuplicates I=$i O=$i_bam CREATE_INDEX=true M=$i_metrics; gatk HaplotypeCaller -R $reference -I $i.sorted.dedup.bam -O ""$i"".g.vcf.gz --tmp-dir ./tmp -ERC GVCF; #SNP; gatk GenotypeGVCFs -R $reference -V ""$i"".g.vcf.gz -O ""$i"".vcf.gz; gatk SelectVariants -R $reference -V ""$i"".vcf.gz -select-type SNP -O ""$i"".snp.vcf.gz; gatk VariantFiltration -R $reference -V $path_BQSR/bqsr1.snp.vcf.gz -filter ""QD < 2.0"" --filter-name ""QD2"" -filter ""FS > 60.0"" --filter-name ""FS60"" -filter ""MQ < 40.0"" --filter-name ""MQ40"" -filter ""MQRankSum < -12.5"" --filter-name ""MQRankSum-12.5"" -filter ""ReadPosRankSum < -8.0"" --filter-name ""ReadPosRankSum-8.0"" -O $""i"".filtered.snp.vcf.gz; gatk SelectVariants -R $reference -V $""i"".filtered.snp.vcf.gz --exclude-filtered -O ""$i"".select.filtered.snp.vcf.gz; #INDEL; gatk SelectVariants -R $reference -V ""$i"".vcf.gz -select-type INDEL -O ""$i"".indel.vcf.gz; gatk VariantFiltration -R $reference -V ""$i"".indel.vcf.gz -filter ""QD < 2.0"" --filter-name ""QD2"" -filter ""FS > 200.0"" --filter-name ""FS200"" -filter ""ReadPosRankSum < -20.0"" --fi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8414
https://github.com/broadinstitute/gatk/issues/8414:531,Deployability,pipeline,pipeline,531,"Hi everyone,. I am going to use GATK4 to carry out population-based snp-calling for around 500 samples with the ploidy level from 2 to 6.; The sequence depth varied from 2x to 80x. Since there are many samples with low sequence depth (<5x), I was advised to use the population-based snp-calling pipeline. ; Recently, I met some problems in the flowchart of this analysis. Firstly, I found the pipelines that detailedly explained the flowchart of BQSR and variant calling for single-sample snp-calling. But I have no idea about the pipeline of the BQSR in population-based snp-calling. Should I apply the BQSR sample by sample following the flowchart menthioned above (the command line below), and then used these corrected bam files to identify the variants based on population-based method? Or there is another correct pipeline for this analysis. **Command lines:**. ```; for i in sample1.bam sample2.bam sample3.bam; do; java -jar picard.jar MarkDuplicates I=$i O=$i_bam CREATE_INDEX=true M=$i_metrics; gatk HaplotypeCaller -R $reference -I $i.sorted.dedup.bam -O ""$i"".g.vcf.gz --tmp-dir ./tmp -ERC GVCF; #SNP; gatk GenotypeGVCFs -R $reference -V ""$i"".g.vcf.gz -O ""$i"".vcf.gz; gatk SelectVariants -R $reference -V ""$i"".vcf.gz -select-type SNP -O ""$i"".snp.vcf.gz; gatk VariantFiltration -R $reference -V $path_BQSR/bqsr1.snp.vcf.gz -filter ""QD < 2.0"" --filter-name ""QD2"" -filter ""FS > 60.0"" --filter-name ""FS60"" -filter ""MQ < 40.0"" --filter-name ""MQ40"" -filter ""MQRankSum < -12.5"" --filter-name ""MQRankSum-12.5"" -filter ""ReadPosRankSum < -8.0"" --filter-name ""ReadPosRankSum-8.0"" -O $""i"".filtered.snp.vcf.gz; gatk SelectVariants -R $reference -V $""i"".filtered.snp.vcf.gz --exclude-filtered -O ""$i"".select.filtered.snp.vcf.gz; #INDEL; gatk SelectVariants -R $reference -V ""$i"".vcf.gz -select-type INDEL -O ""$i"".indel.vcf.gz; gatk VariantFiltration -R $reference -V ""$i"".indel.vcf.gz -filter ""QD < 2.0"" --filter-name ""QD2"" -filter ""FS > 200.0"" --filter-name ""FS200"" -filter ""ReadPosRankSum < -20.0"" --fi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8414
https://github.com/broadinstitute/gatk/issues/8414:820,Deployability,pipeline,pipeline,820,"Hi everyone,. I am going to use GATK4 to carry out population-based snp-calling for around 500 samples with the ploidy level from 2 to 6.; The sequence depth varied from 2x to 80x. Since there are many samples with low sequence depth (<5x), I was advised to use the population-based snp-calling pipeline. ; Recently, I met some problems in the flowchart of this analysis. Firstly, I found the pipelines that detailedly explained the flowchart of BQSR and variant calling for single-sample snp-calling. But I have no idea about the pipeline of the BQSR in population-based snp-calling. Should I apply the BQSR sample by sample following the flowchart menthioned above (the command line below), and then used these corrected bam files to identify the variants based on population-based method? Or there is another correct pipeline for this analysis. **Command lines:**. ```; for i in sample1.bam sample2.bam sample3.bam; do; java -jar picard.jar MarkDuplicates I=$i O=$i_bam CREATE_INDEX=true M=$i_metrics; gatk HaplotypeCaller -R $reference -I $i.sorted.dedup.bam -O ""$i"".g.vcf.gz --tmp-dir ./tmp -ERC GVCF; #SNP; gatk GenotypeGVCFs -R $reference -V ""$i"".g.vcf.gz -O ""$i"".vcf.gz; gatk SelectVariants -R $reference -V ""$i"".vcf.gz -select-type SNP -O ""$i"".snp.vcf.gz; gatk VariantFiltration -R $reference -V $path_BQSR/bqsr1.snp.vcf.gz -filter ""QD < 2.0"" --filter-name ""QD2"" -filter ""FS > 60.0"" --filter-name ""FS60"" -filter ""MQ < 40.0"" --filter-name ""MQ40"" -filter ""MQRankSum < -12.5"" --filter-name ""MQRankSum-12.5"" -filter ""ReadPosRankSum < -8.0"" --filter-name ""ReadPosRankSum-8.0"" -O $""i"".filtered.snp.vcf.gz; gatk SelectVariants -R $reference -V $""i"".filtered.snp.vcf.gz --exclude-filtered -O ""$i"".select.filtered.snp.vcf.gz; #INDEL; gatk SelectVariants -R $reference -V ""$i"".vcf.gz -select-type INDEL -O ""$i"".indel.vcf.gz; gatk VariantFiltration -R $reference -V ""$i"".indel.vcf.gz -filter ""QD < 2.0"" --filter-name ""QD2"" -filter ""FS > 200.0"" --filter-name ""FS200"" -filter ""ReadPosRankSum < -20.0"" --fi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8414
https://github.com/broadinstitute/gatk/issues/8414:3217,Deployability,pipeline,pipeline,3217,"O ""$i"".snp.vcf.gz; gatk VariantFiltration -R $reference -V $path_BQSR/bqsr1.snp.vcf.gz -filter ""QD < 2.0"" --filter-name ""QD2"" -filter ""FS > 60.0"" --filter-name ""FS60"" -filter ""MQ < 40.0"" --filter-name ""MQ40"" -filter ""MQRankSum < -12.5"" --filter-name ""MQRankSum-12.5"" -filter ""ReadPosRankSum < -8.0"" --filter-name ""ReadPosRankSum-8.0"" -O $""i"".filtered.snp.vcf.gz; gatk SelectVariants -R $reference -V $""i"".filtered.snp.vcf.gz --exclude-filtered -O ""$i"".select.filtered.snp.vcf.gz; #INDEL; gatk SelectVariants -R $reference -V ""$i"".vcf.gz -select-type INDEL -O ""$i"".indel.vcf.gz; gatk VariantFiltration -R $reference -V ""$i"".indel.vcf.gz -filter ""QD < 2.0"" --filter-name ""QD2"" -filter ""FS > 200.0"" --filter-name ""FS200"" -filter ""ReadPosRankSum < -20.0"" --filter-name ""ReadPosRankSum-20"" -O ""$i"".filtered.indel.vcf.gz; gatk SelectVariants -R $reference -V ""$i"".filtered.indel.vcf.gz --exclude-filtered -O ""$i"".selected.filtered.indel.vcf.gz. gatk BaseRecalibrator -R $reference -I $i_bam -O grp1 --use-original-qualities --known-sites ""$i"".select.filtered.snp.vcf.gz --known-sites ""$i"".selected.filtered.indel.vcf.gz; gatk ApplyBQSR -R $reference -I $i_bam -O ""$i"".sorted.dedup.BQSR1.bam -bqsr grp1 --static-quantized-quals 10 --static-quantized-quals 20 --static-quantized-quals 30 --add-output-sam-program-record --create-output-bam-md5 --use-original-qualities; gatk ValidateSamFile -I ""$i"".sorted.dedup.BQSR.bam -O ""$i""_validateSamFile_of_bqsr_bam_file.out; samtools index ""$i"".sorted.dedup.BQSR.bam; done; gatk --java-options ""-Xmx30G"" HaplotypeCaller -R $reference -I sample1.sorted.dedup.BQSR.bam sample2.sorted.dedup.BQSR.bam sample3.sorted.dedup.BQSR.bam -O sample1_sample2_sample4.g.vcf.gz --tmp-dir tmp -ERC GVCF; ```. Secondly, how do I set the ploidy parameter for different samples in the population-based snp-calling?; Finally, for each taxa, there are some samples with relatively high sequence depth (> 10x). Is there any better choices for the snp-calling pipeline ??. Sincerely.; Jing",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8414
https://github.com/broadinstitute/gatk/issues/8414:2613,Security,Validat,ValidateSamFile,2613,"O ""$i"".snp.vcf.gz; gatk VariantFiltration -R $reference -V $path_BQSR/bqsr1.snp.vcf.gz -filter ""QD < 2.0"" --filter-name ""QD2"" -filter ""FS > 60.0"" --filter-name ""FS60"" -filter ""MQ < 40.0"" --filter-name ""MQ40"" -filter ""MQRankSum < -12.5"" --filter-name ""MQRankSum-12.5"" -filter ""ReadPosRankSum < -8.0"" --filter-name ""ReadPosRankSum-8.0"" -O $""i"".filtered.snp.vcf.gz; gatk SelectVariants -R $reference -V $""i"".filtered.snp.vcf.gz --exclude-filtered -O ""$i"".select.filtered.snp.vcf.gz; #INDEL; gatk SelectVariants -R $reference -V ""$i"".vcf.gz -select-type INDEL -O ""$i"".indel.vcf.gz; gatk VariantFiltration -R $reference -V ""$i"".indel.vcf.gz -filter ""QD < 2.0"" --filter-name ""QD2"" -filter ""FS > 200.0"" --filter-name ""FS200"" -filter ""ReadPosRankSum < -20.0"" --filter-name ""ReadPosRankSum-20"" -O ""$i"".filtered.indel.vcf.gz; gatk SelectVariants -R $reference -V ""$i"".filtered.indel.vcf.gz --exclude-filtered -O ""$i"".selected.filtered.indel.vcf.gz. gatk BaseRecalibrator -R $reference -I $i_bam -O grp1 --use-original-qualities --known-sites ""$i"".select.filtered.snp.vcf.gz --known-sites ""$i"".selected.filtered.indel.vcf.gz; gatk ApplyBQSR -R $reference -I $i_bam -O ""$i"".sorted.dedup.BQSR1.bam -bqsr grp1 --static-quantized-quals 10 --static-quantized-quals 20 --static-quantized-quals 30 --add-output-sam-program-record --create-output-bam-md5 --use-original-qualities; gatk ValidateSamFile -I ""$i"".sorted.dedup.BQSR.bam -O ""$i""_validateSamFile_of_bqsr_bam_file.out; samtools index ""$i"".sorted.dedup.BQSR.bam; done; gatk --java-options ""-Xmx30G"" HaplotypeCaller -R $reference -I sample1.sorted.dedup.BQSR.bam sample2.sorted.dedup.BQSR.bam sample3.sorted.dedup.BQSR.bam -O sample1_sample2_sample4.g.vcf.gz --tmp-dir tmp -ERC GVCF; ```. Secondly, how do I set the ploidy parameter for different samples in the population-based snp-calling?; Finally, for each taxa, there are some samples with relatively high sequence depth (> 10x). Is there any better choices for the snp-calling pipeline ??. Sincerely.; Jing",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8414
https://github.com/broadinstitute/gatk/issues/8415:4097,Availability,down,down,4097,"03:51:59.222 INFO GenotypeGVCFs - Initializing engine; 03:51:59.571 INFO GenomicsDBLibLoader - GenomicsDB native library version : 1.3.2-e18fa63; 11:51:59.957 info NativeGenomicsDB - pid=1480681 tid=1480682 No valid combination operation found for INFO field InbreedingCoeff - the field will NOT be part of INFO fields in the generated VCF records; 11:51:59.957 info NativeGenomicsDB - pid=1480681 tid=1480682 No valid combination operation found for INFO field MLEAC - the field will NOT be part of INFO fields in the generated VCF records; 11:51:59.957 info NativeGenomicsDB - pid=1480681 tid=1480682 No valid combination operation found for INFO field MLEAF - the field will NOT be part of INFO fields in the generated VCF records; 03:52:00.341 INFO GenotypeGVCFs - Done initializing engine; 03:52:00.369 INFO ProgressMeter - Starting traversal; 03:52:00.369 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 03:52:00.377 INFO GenotypeGVCFs - Shutting down engine; [July 13, 2023 at 3:52:00 AM UTC] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 0.02 minutes.; Runtime.totalMemory()=2583691264; java.lang.IllegalStateException: There are no sources based on those query parameters; at org.genomicsdb.reader.GenomicsDBFeatureIterator.<init>(GenomicsDBFeatureIterator.java:167); at org.genomicsdb.reader.GenomicsDBFeatureReader.query(GenomicsDBFeatureReader.java:152); at org.broadinstitute.hellbender.engine.FeatureDataSource.refillQueryCache(FeatureDataSource.java:569); at org.broadinstitute.hellbender.engine.FeatureDataSource.queryAndPrefetch(FeatureDataSource.java:538); at org.broadinstitute.hellbender.engine.FeatureDataSource.query(FeatureDataSource.java:504); at org.broadinstitute.hellbender.engine.VariantLocusWalker.lambda$traverse$2(VariantLocusWalker.java:149); at java.base/java.util.Iterator.forEachRemaining(Iterator.java:133); at java.base/java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8415
https://github.com/broadinstitute/gatk/issues/8415:6453,Deployability,release,release,6453,"gram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); Using GATK jar /opt/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx56g -Djava.io.tmpdir=./tmp -jar /opt/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar GenotypeGVCFs -R ../../01.ref/MS/genome.fasta -V gendb://genomeDB.Chr23 -all-sites -O Chr23.raw.vcf.gz. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_. ### Affected version(s); - [ ] Latest public release version [version?] V4.2.0.0; - [ ] Latest master branch as of [date of test?]. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._; There are no results generated.; ![image](https://github.com/broadinstitute/gatk/assets/103233242/2c505328-c0a0-473a-bd64-63b2137e0f06). #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_. 1:singularity exec ~/biosoft/resequence/Reseq_genek.sif gatk --java-options ""-Xmx48g -Djava.io.tmpdir=./tmp"" HaplotypeCaller -R ../../01.ref/MS/genome.fasta -I ../../02.mapping/MS/F10.sort.markdup.bam -L Chr01 -ERC GVCF -O F10/F10.Chr01.g.vcf.gz ; 2:singularity exec ~/biosoft/resequence/Reseq_genek.sif gatk --java-options ""-Xmx56g -Djava.io.tmpdir=./tmp -DGATK_STACKTRACE_ON_USER_EXCEPTION=true"" GenomicsDBImport --sample-name-map gvcf",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8415
https://github.com/broadinstitute/gatk/issues/8415:1223,Performance,Load,Loading,1223,"ial support forum](http://gatkforums.broadinstitute.org/gatk).; - Search the existing github issues to see if your issue (or something similar) has already been reported. If the issue already exists, you may comment there to inquire about the progress.; - Determine whether your issue is a **bug report**, a **feature request**, or a **documentation request** (for tool/class javadoc only -- for forum docs please post there); - Consider if your ""issue"" is better addressed on the GATK forum: http://gatkforums.broadinstitute.org/gatk Post there if you have questions about expected tool behavior, output format, unexpected results, or generally any question that does not fit into the categories above; - Use a **concise** yet **descriptive** title; - Choose the corresponding template block below and fill it in, replacing or deleting text in italics (surrounded by `_`) as appropriate; - Delete the other template blocks and this header. ----. ## Bug Report. 03:51:59.035 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/opt/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jul 13, 2023 3:51:59 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 03:51:59.220 INFO GenotypeGVCFs - ------------------------------------------------------------; 03:51:59.220 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.0.0; 03:51:59.220 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 03:51:59.220 INFO GenotypeGVCFs - Executing as dingrj@localhost.localdomain on Linux v4.18.0-348.7.1.el8_5.x86_64 amd64; 03:51:59.220 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v11.0.11+9-Ubuntu-0ubuntu2.18.04; 03:51:59.220 INFO GenotypeGVCFs - Start Date/Time: July 13, 2023 at 3:51:58 AM UTC; 03:51:59.221 INFO GenotypeGVCFs - ----------------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8415
https://github.com/broadinstitute/gatk/issues/8415:1490,Safety,detect,detect,1490,"e whether your issue is a **bug report**, a **feature request**, or a **documentation request** (for tool/class javadoc only -- for forum docs please post there); - Consider if your ""issue"" is better addressed on the GATK forum: http://gatkforums.broadinstitute.org/gatk Post there if you have questions about expected tool behavior, output format, unexpected results, or generally any question that does not fit into the categories above; - Use a **concise** yet **descriptive** title; - Choose the corresponding template block below and fill it in, replacing or deleting text in italics (surrounded by `_`) as appropriate; - Delete the other template blocks and this header. ----. ## Bug Report. 03:51:59.035 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/opt/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jul 13, 2023 3:51:59 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 03:51:59.220 INFO GenotypeGVCFs - ------------------------------------------------------------; 03:51:59.220 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.0.0; 03:51:59.220 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 03:51:59.220 INFO GenotypeGVCFs - Executing as dingrj@localhost.localdomain on Linux v4.18.0-348.7.1.el8_5.x86_64 amd64; 03:51:59.220 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v11.0.11+9-Ubuntu-0ubuntu2.18.04; 03:51:59.220 INFO GenotypeGVCFs - Start Date/Time: July 13, 2023 at 3:51:58 AM UTC; 03:51:59.221 INFO GenotypeGVCFs - ------------------------------------------------------------; 03:51:59.221 INFO GenotypeGVCFs - ------------------------------------------------------------; 03:51:59.221 INFO GenotypeGVCFs - HTSJDK Version: 2.24.0; 03:51:59.221 INFO GenotypeGVCFs - Picard Version: 2.25.0; 03:51:59.221 INFO GenotypeGVCF",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8415
https://github.com/broadinstitute/gatk/issues/8415:6532,Testability,test,test,6532,"lbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); Using GATK jar /opt/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx56g -Djava.io.tmpdir=./tmp -jar /opt/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar GenotypeGVCFs -R ../../01.ref/MS/genome.fasta -V gendb://genomeDB.Chr23 -all-sites -O Chr23.raw.vcf.gz. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_. ### Affected version(s); - [ ] Latest public release version [version?] V4.2.0.0; - [ ] Latest master branch as of [date of test?]. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._; There are no results generated.; ![image](https://github.com/broadinstitute/gatk/assets/103233242/2c505328-c0a0-473a-bd64-63b2137e0f06). #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_. 1:singularity exec ~/biosoft/resequence/Reseq_genek.sif gatk --java-options ""-Xmx48g -Djava.io.tmpdir=./tmp"" HaplotypeCaller -R ../../01.ref/MS/genome.fasta -I ../../02.mapping/MS/F10.sort.markdup.bam -L Chr01 -ERC GVCF -O F10/F10.Chr01.g.vcf.gz ; 2:singularity exec ~/biosoft/resequence/Reseq_genek.sif gatk --java-options ""-Xmx56g -Djava.io.tmpdir=./tmp -DGATK_STACKTRACE_ON_USER_EXCEPTION=true"" GenomicsDBImport --sample-name-map gvcf.Chr01.map --genomicsdb-workspace-path genomeDB.Chr01 -L Chr01 --rea",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8415
https://github.com/broadinstitute/gatk/issues/8415:6632,Testability,log,logs,6632,"tute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); Using GATK jar /opt/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx56g -Djava.io.tmpdir=./tmp -jar /opt/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar GenotypeGVCFs -R ../../01.ref/MS/genome.fasta -V gendb://genomeDB.Chr23 -all-sites -O Chr23.raw.vcf.gz. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_. ### Affected version(s); - [ ] Latest public release version [version?] V4.2.0.0; - [ ] Latest master branch as of [date of test?]. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._; There are no results generated.; ![image](https://github.com/broadinstitute/gatk/assets/103233242/2c505328-c0a0-473a-bd64-63b2137e0f06). #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_. 1:singularity exec ~/biosoft/resequence/Reseq_genek.sif gatk --java-options ""-Xmx48g -Djava.io.tmpdir=./tmp"" HaplotypeCaller -R ../../01.ref/MS/genome.fasta -I ../../02.mapping/MS/F10.sort.markdup.bam -L Chr01 -ERC GVCF -O F10/F10.Chr01.g.vcf.gz ; 2:singularity exec ~/biosoft/resequence/Reseq_genek.sif gatk --java-options ""-Xmx56g -Djava.io.tmpdir=./tmp -DGATK_STACKTRACE_ON_USER_EXCEPTION=true"" GenomicsDBImport --sample-name-map gvcf.Chr01.map --genomicsdb-workspace-path genomeDB.Chr01 -L Chr01 --reader-threads 56 --batch-size 50 --tmp-dir ./tmp ; 3：singularity exec ~/biosoft/resequence/Reseq_genek.sif gatk",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8415
https://github.com/broadinstitute/gatk/pull/8418:17,Availability,down,downloads,17,- Now will count downloads of all artifacts in github releases instead of just the first one.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8418
https://github.com/broadinstitute/gatk/pull/8418:54,Deployability,release,releases,54,- Now will count downloads of all artifacts in github releases instead of just the first one.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8418
https://github.com/broadinstitute/gatk/pull/8422:5,Deployability,update,updated,5,"Also updated GATK docker to latest. Latest run with missing columns [here](https://app.terra.bio/#workspaces/gvs-dev/RSA%20-%20GVS%20Quickstart%20V2%20/job_history/d9e5ec21-da22-42de-944c-a43ccf83e220) to show what ""failing"" looks like.; Quickstart integration failed because of https://broadworkbench.atlassian.net/browse/VS-1010.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8422
https://github.com/broadinstitute/gatk/pull/8422:249,Deployability,integrat,integration,249,"Also updated GATK docker to latest. Latest run with missing columns [here](https://app.terra.bio/#workspaces/gvs-dev/RSA%20-%20GVS%20Quickstart%20V2%20/job_history/d9e5ec21-da22-42de-944c-a43ccf83e220) to show what ""failing"" looks like.; Quickstart integration failed because of https://broadworkbench.atlassian.net/browse/VS-1010.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8422
https://github.com/broadinstitute/gatk/pull/8422:249,Integrability,integrat,integration,249,"Also updated GATK docker to latest. Latest run with missing columns [here](https://app.terra.bio/#workspaces/gvs-dev/RSA%20-%20GVS%20Quickstart%20V2%20/job_history/d9e5ec21-da22-42de-944c-a43ccf83e220) to show what ""failing"" looks like.; Quickstart integration failed because of https://broadworkbench.atlassian.net/browse/VS-1010.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8422
https://github.com/broadinstitute/gatk/pull/8423:274,Deployability,Integrat,Integration,274,"This PR adds the 'SCORE' field as an output in the VQSR-Lite derived VCFs; Score is the value from which the `CALIBRATION_SENSITIVITY` is derived. The latter is what we use for filtering based on sensitivity, but Sam and Laura also want the SCORE stored in the VCF. Passing Integration test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/721bc470-a968-4fe4-9be3-a1ddddc9a792)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8423
https://github.com/broadinstitute/gatk/pull/8423:274,Integrability,Integrat,Integration,274,"This PR adds the 'SCORE' field as an output in the VQSR-Lite derived VCFs; Score is the value from which the `CALIBRATION_SENSITIVITY` is derived. The latter is what we use for filtering based on sensitivity, but Sam and Laura also want the SCORE stored in the VCF. Passing Integration test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/721bc470-a968-4fe4-9be3-a1ddddc9a792)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8423
https://github.com/broadinstitute/gatk/pull/8423:286,Testability,test,test,286,"This PR adds the 'SCORE' field as an output in the VQSR-Lite derived VCFs; Score is the value from which the `CALIBRATION_SENSITIVITY` is derived. The latter is what we use for filtering based on sensitivity, but Sam and Laura also want the SCORE stored in the VCF. Passing Integration test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/721bc470-a968-4fe4-9be3-a1ddddc9a792)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8423
https://github.com/broadinstitute/gatk/pull/8426:315,Deployability,Integrat,Integration,315,Defaulting to WGS.; Note that I had to thread the weighted bed file through (and a flag to use it). As this will also be needed to run exomes. Successful Joint Calling run [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/32d9dfcc-a5c8-4f95-9a5f-f0fbf7294bcf).; Successful Integration test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/65973408-77d4-41a4-9277-6697efdc88fc),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8426
https://github.com/broadinstitute/gatk/pull/8426:315,Integrability,Integrat,Integration,315,Defaulting to WGS.; Note that I had to thread the weighted bed file through (and a flag to use it). As this will also be needed to run exomes. Successful Joint Calling run [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/32d9dfcc-a5c8-4f95-9a5f-f0fbf7294bcf).; Successful Integration test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/65973408-77d4-41a4-9277-6697efdc88fc),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8426
https://github.com/broadinstitute/gatk/pull/8426:327,Testability,test,test,327,Defaulting to WGS.; Note that I had to thread the weighted bed file through (and a flag to use it). As this will also be needed to run exomes. Successful Joint Calling run [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/32d9dfcc-a5c8-4f95-9a5f-f0fbf7294bcf).; Successful Integration test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/65973408-77d4-41a4-9277-6697efdc88fc),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8426
https://github.com/broadinstitute/gatk/issues/8427:214,Availability,error,error,214,"I am attempting to create fasta files from specific regions in approximately 270 VCF files. For every other region/gene I've looked at, I have not had this issue. For one particular region (mrr1), I am getting the error seen below. I checked the coverage of the bam file and viewed the vcf in IGV viewer, but notice no problems. Can you please advise? Thank you. Similar to this issue, but am still not sure how to approach it?; https://github.com/broadinstitute/gatk/issues/6260#issue-521418442. Bash script:; ```; #!/bin/bash --login; #SBATCH --time=1:00:00 # limit of wall clock time - how long the job will run; #SBATCH --ntasks=1 # number of tasks - how many tasks (nodes) that you requir; #SBATCH --cpus-per-task=1 # number of CPUs (or cores) per task (same as -c); #SBATCH --mem=50G # memory required per node - amount of memory (in bytes); #SBATCH --job-name=VCF_FastaNEP_CCR; #SBATCH --mail-user=lukaskon@msu.edu; #SBATCH --mail-type=ALL; #SBATCH -o SpeciesID_CCR7_slurm. cd /mnt/research/Hausbeck_group/Lukasko/BotrytisDNASeq/. module load Java/JDK12. for sample in AI7 W18 B5 BU9 I9 R23 Y1; do; base=$(basename ${sample}). gatk-4.2.5.0/gatk SelectVariants -R /mnt/research/Hausbeck_group/Lukasko/BotrytisDNASeq/0_DNAscripts/ReferenceGenome/Botrytis_cinerea.ASM83294v1.dna.toplevel.fa -V /mn; t/research/Hausbeck_group/Lukasko/BotrytisDNASeq/10_FilteredVCF/Plates123/BcinereaP123.SNVonly.filteredPASS_renamed.vcf -sn ${sample} --remove-unused-alternates --exclu; de-sample-name /mnt/research/Hausbeck_group/Lukasko/BotrytisDNASeq/CCR7/ConservedGenes/ExcludeList.args -O /mnt/research/Hausbeck_group/Lukasko/BotrytisDNASeq/CCR7/Cons; ervedGenes/VCFs/${base}.vcf. gatk-4.2.5.0/gatk FastaAlternateReferenceMaker -R /mnt/research/Hausbeck_group/Lukasko/BotrytisDNASeq/0_DNAscripts/ReferenceGenome/Botrytis_cinerea.ASM83294v1.dna.tople; vel.fa -O /mnt/research/Hausbeck_group/Lukasko/BotrytisDNASeq/CCR7/ConservedGenes/mrr1/${base}_mrr1.fasta -L 5:680219-684662 -V /mnt/research/Hausbeck_group/Lu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8427
https://github.com/broadinstitute/gatk/issues/8427:4633,Availability,down,down,4633,":33:06.617 INFO FastaAlternateReferenceMaker - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 15:33:06.618 INFO FastaAlternateReferenceMaker - Deflater: IntelDeflater; 15:33:06.618 INFO FastaAlternateReferenceMaker - Inflater: IntelInflater; 15:33:06.618 INFO FastaAlternateReferenceMaker - GCS max retries/reopens: 20; 15:33:06.619 INFO FastaAlternateReferenceMaker - Requester pays: disabled; 15:33:06.619 INFO FastaAlternateReferenceMaker - Initializing engine; 15:33:06.870 INFO FeatureManager - Using codec VCFCodec to read file file:///mnt/research/Hausbeck_group/Lukasko/BotrytisDNASeq/CCR7/ConservedGenes/VCFs/AI7.vcf; 15:33:06.936 INFO IntervalArgumentCollection - Processing 4444 bp from intervals; 15:33:06.939 INFO FastaAlternateReferenceMaker - Done initializing engine; 15:33:06.949 INFO ProgressMeter - Starting traversal; 15:33:06.949 INFO ProgressMeter - Current Locus Elapsed Minutes Bases Processed Bases/Minute; 15:33:07.194 INFO FastaAlternateReferenceMaker - Shutting down engine; [July 18, 2023 at 3:33:07 PM EDT] org.broadinstitute.hellbender.tools.walkers.fasta.FastaAlternateReferenceMaker done. Elapsed time: 0.02 minutes.; Runtime.totalMemory()=2076049408; java.lang.IllegalArgumentException: Illegal base [ ] seen in the allele; at htsjdk.variant.variantcontext.Allele.create(Allele.java:251); at htsjdk.variant.variantcontext.Allele.create(Allele.java:402); at org.broadinstitute.hellbender.tools.walkers.fasta.FastaAlternateReferenceMaker.lambda$handlePosition$0(FastaAlternateReferenceMaker.java:176); at java.base/java.util.Optional.orElseGet(Optional.java:369); at org.broadinstitute.hellbender.tools.walkers.fasta.FastaAlternateReferenceMaker.handlePosition(FastaAlternateReferenceMaker.java:176); at org.broadinstitute.hellbender.tools.walkers.fasta.FastaAlternateReferenceMaker.apply(FastaAlternateReferenceMaker.java:141); at org.broadinstitute.hellbender.engine.ReferenceWalker.traverse(ReferenceWalker.java:55); at org.broadinstitute.hellbender.engine",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8427
https://github.com/broadinstitute/gatk/issues/8427:1045,Performance,load,load,1045,"ic regions in approximately 270 VCF files. For every other region/gene I've looked at, I have not had this issue. For one particular region (mrr1), I am getting the error seen below. I checked the coverage of the bam file and viewed the vcf in IGV viewer, but notice no problems. Can you please advise? Thank you. Similar to this issue, but am still not sure how to approach it?; https://github.com/broadinstitute/gatk/issues/6260#issue-521418442. Bash script:; ```; #!/bin/bash --login; #SBATCH --time=1:00:00 # limit of wall clock time - how long the job will run; #SBATCH --ntasks=1 # number of tasks - how many tasks (nodes) that you requir; #SBATCH --cpus-per-task=1 # number of CPUs (or cores) per task (same as -c); #SBATCH --mem=50G # memory required per node - amount of memory (in bytes); #SBATCH --job-name=VCF_FastaNEP_CCR; #SBATCH --mail-user=lukaskon@msu.edu; #SBATCH --mail-type=ALL; #SBATCH -o SpeciesID_CCR7_slurm. cd /mnt/research/Hausbeck_group/Lukasko/BotrytisDNASeq/. module load Java/JDK12. for sample in AI7 W18 B5 BU9 I9 R23 Y1; do; base=$(basename ${sample}). gatk-4.2.5.0/gatk SelectVariants -R /mnt/research/Hausbeck_group/Lukasko/BotrytisDNASeq/0_DNAscripts/ReferenceGenome/Botrytis_cinerea.ASM83294v1.dna.toplevel.fa -V /mn; t/research/Hausbeck_group/Lukasko/BotrytisDNASeq/10_FilteredVCF/Plates123/BcinereaP123.SNVonly.filteredPASS_renamed.vcf -sn ${sample} --remove-unused-alternates --exclu; de-sample-name /mnt/research/Hausbeck_group/Lukasko/BotrytisDNASeq/CCR7/ConservedGenes/ExcludeList.args -O /mnt/research/Hausbeck_group/Lukasko/BotrytisDNASeq/CCR7/Cons; ervedGenes/VCFs/${base}.vcf. gatk-4.2.5.0/gatk FastaAlternateReferenceMaker -R /mnt/research/Hausbeck_group/Lukasko/BotrytisDNASeq/0_DNAscripts/ReferenceGenome/Botrytis_cinerea.ASM83294v1.dna.tople; vel.fa -O /mnt/research/Hausbeck_group/Lukasko/BotrytisDNASeq/CCR7/ConservedGenes/mrr1/${base}_mrr1.fasta -L 5:680219-684662 -V /mnt/research/Hausbeck_group/Lukasko; /BotrytisDNASeq/CCR7/ConservedGenes/VCFs/",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8427
https://github.com/broadinstitute/gatk/issues/8427:530,Testability,log,login,530,"I am attempting to create fasta files from specific regions in approximately 270 VCF files. For every other region/gene I've looked at, I have not had this issue. For one particular region (mrr1), I am getting the error seen below. I checked the coverage of the bam file and viewed the vcf in IGV viewer, but notice no problems. Can you please advise? Thank you. Similar to this issue, but am still not sure how to approach it?; https://github.com/broadinstitute/gatk/issues/6260#issue-521418442. Bash script:; ```; #!/bin/bash --login; #SBATCH --time=1:00:00 # limit of wall clock time - how long the job will run; #SBATCH --ntasks=1 # number of tasks - how many tasks (nodes) that you requir; #SBATCH --cpus-per-task=1 # number of CPUs (or cores) per task (same as -c); #SBATCH --mem=50G # memory required per node - amount of memory (in bytes); #SBATCH --job-name=VCF_FastaNEP_CCR; #SBATCH --mail-user=lukaskon@msu.edu; #SBATCH --mail-type=ALL; #SBATCH -o SpeciesID_CCR7_slurm. cd /mnt/research/Hausbeck_group/Lukasko/BotrytisDNASeq/. module load Java/JDK12. for sample in AI7 W18 B5 BU9 I9 R23 Y1; do; base=$(basename ${sample}). gatk-4.2.5.0/gatk SelectVariants -R /mnt/research/Hausbeck_group/Lukasko/BotrytisDNASeq/0_DNAscripts/ReferenceGenome/Botrytis_cinerea.ASM83294v1.dna.toplevel.fa -V /mn; t/research/Hausbeck_group/Lukasko/BotrytisDNASeq/10_FilteredVCF/Plates123/BcinereaP123.SNVonly.filteredPASS_renamed.vcf -sn ${sample} --remove-unused-alternates --exclu; de-sample-name /mnt/research/Hausbeck_group/Lukasko/BotrytisDNASeq/CCR7/ConservedGenes/ExcludeList.args -O /mnt/research/Hausbeck_group/Lukasko/BotrytisDNASeq/CCR7/Cons; ervedGenes/VCFs/${base}.vcf. gatk-4.2.5.0/gatk FastaAlternateReferenceMaker -R /mnt/research/Hausbeck_group/Lukasko/BotrytisDNASeq/0_DNAscripts/ReferenceGenome/Botrytis_cinerea.ASM83294v1.dna.tople; vel.fa -O /mnt/research/Hausbeck_group/Lukasko/BotrytisDNASeq/CCR7/ConservedGenes/mrr1/${base}_mrr1.fasta -L 5:680219-684662 -V /mnt/research/Hausbeck_group/Lu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8427
https://github.com/broadinstitute/gatk/pull/8429:162,Deployability,Integrat,Integration,162,Defaulting to WGS.; Note that I had to thread the weighted bed file through (and a flag to use it). As this will also be needed to run exomes. Passing (finally!) Integration Test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/199f2e66-4b4b-478c-9370-47a760d3fab2).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8429
https://github.com/broadinstitute/gatk/pull/8429:162,Integrability,Integrat,Integration,162,Defaulting to WGS.; Note that I had to thread the weighted bed file through (and a flag to use it). As this will also be needed to run exomes. Passing (finally!) Integration Test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/199f2e66-4b4b-478c-9370-47a760d3fab2).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8429
https://github.com/broadinstitute/gatk/pull/8429:174,Testability,Test,Test,174,Defaulting to WGS.; Note that I had to thread the weighted bed file through (and a flag to use it). As this will also be needed to run exomes. Passing (finally!) Integration Test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/199f2e66-4b4b-478c-9370-47a760d3fab2).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8429
https://github.com/broadinstitute/gatk/issues/8432:31,Availability,down,downloaded,31,"## Bug Report. Hi there,; So I downloaded the gatk-4.4-0.0.zip and unzipped it for using gatk. I also created the conda env using the gatkcondaenv.yml and used conda to install java ""1.7.0_91"". But when I run ./gatk --list I got this error message: . `; ./gatk --list; Using GATK jar /home/athchu/bin/gatk-4.4.0.0/gatk-package-4.4.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/athchu/bin/gatk-4.4.0.0/gatk-package-4.4.0.0-local.jar --help; Error: Invalid or corrupt jarfile /home/athchu/bin/gatk-4.4.0.0/gatk-package-4.4.0.0-local.jar; `; Next, I moved on to git clone the gatk repository, trying to build gatk. Again, I stay in the java ""1.7.0_91"" gatk env that I already created. But I got this error msg this time:; `; ./gradlew localJar; Gradle 7.5.1 requires Java 1.8 or later to run. You are currently using Java 1.7.; `; When I switch back to the server default java (1.8.0_292-b10), i got another error msg.; `; java -version; openjdk version ""1.8.0_292""; OpenJDK Runtime Environment (build 1.8.0_292-b10); OpenJDK 64-Bit Server VM (build 25.292-b10, mixed mode). ./gradlew localJar. > Configure project :; Warning: using Java 1.8 but only Java 17 has been tested. FAILURE: Build failed with an exception. * Where:; Build file '/home/athchu/bin/gatk/build.gradle' line: 141. * What went wrong:; A problem occurred evaluating root project 'gatk'.; > A Java 17 compatible (Java 17 or later) version is required to build GATK, but 1.8 was found. See https://github.com/broadinstitute/gatk#building for information on how to build GATK. * Try:; > Run with --stacktrace option to get the stack trace.; > Run with --info or --debug option to get more log output.; > Run with --scan to get full insights. * Get more help at https://help.gradle.org; `; So to sum up, my issues are :; 1) downloaded gatk-4.4.0.0 but it contained invalid jar file and i ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8432
https://github.com/broadinstitute/gatk/issues/8432:234,Availability,error,error,234,"## Bug Report. Hi there,; So I downloaded the gatk-4.4-0.0.zip and unzipped it for using gatk. I also created the conda env using the gatkcondaenv.yml and used conda to install java ""1.7.0_91"". But when I run ./gatk --list I got this error message: . `; ./gatk --list; Using GATK jar /home/athchu/bin/gatk-4.4.0.0/gatk-package-4.4.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/athchu/bin/gatk-4.4.0.0/gatk-package-4.4.0.0-local.jar --help; Error: Invalid or corrupt jarfile /home/athchu/bin/gatk-4.4.0.0/gatk-package-4.4.0.0-local.jar; `; Next, I moved on to git clone the gatk repository, trying to build gatk. Again, I stay in the java ""1.7.0_91"" gatk env that I already created. But I got this error msg this time:; `; ./gradlew localJar; Gradle 7.5.1 requires Java 1.8 or later to run. You are currently using Java 1.7.; `; When I switch back to the server default java (1.8.0_292-b10), i got another error msg.; `; java -version; openjdk version ""1.8.0_292""; OpenJDK Runtime Environment (build 1.8.0_292-b10); OpenJDK 64-Bit Server VM (build 25.292-b10, mixed mode). ./gradlew localJar. > Configure project :; Warning: using Java 1.8 but only Java 17 has been tested. FAILURE: Build failed with an exception. * Where:; Build file '/home/athchu/bin/gatk/build.gradle' line: 141. * What went wrong:; A problem occurred evaluating root project 'gatk'.; > A Java 17 compatible (Java 17 or later) version is required to build GATK, but 1.8 was found. See https://github.com/broadinstitute/gatk#building for information on how to build GATK. * Try:; > Run with --stacktrace option to get the stack trace.; > Run with --info or --debug option to get more log output.; > Run with --scan to get full insights. * Get more help at https://help.gradle.org; `; So to sum up, my issues are :; 1) downloaded gatk-4.4.0.0 but it contained invalid jar file and i ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8432
https://github.com/broadinstitute/gatk/issues/8432:590,Availability,Error,Error,590,"## Bug Report. Hi there,; So I downloaded the gatk-4.4-0.0.zip and unzipped it for using gatk. I also created the conda env using the gatkcondaenv.yml and used conda to install java ""1.7.0_91"". But when I run ./gatk --list I got this error message: . `; ./gatk --list; Using GATK jar /home/athchu/bin/gatk-4.4.0.0/gatk-package-4.4.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/athchu/bin/gatk-4.4.0.0/gatk-package-4.4.0.0-local.jar --help; Error: Invalid or corrupt jarfile /home/athchu/bin/gatk-4.4.0.0/gatk-package-4.4.0.0-local.jar; `; Next, I moved on to git clone the gatk repository, trying to build gatk. Again, I stay in the java ""1.7.0_91"" gatk env that I already created. But I got this error msg this time:; `; ./gradlew localJar; Gradle 7.5.1 requires Java 1.8 or later to run. You are currently using Java 1.7.; `; When I switch back to the server default java (1.8.0_292-b10), i got another error msg.; `; java -version; openjdk version ""1.8.0_292""; OpenJDK Runtime Environment (build 1.8.0_292-b10); OpenJDK 64-Bit Server VM (build 25.292-b10, mixed mode). ./gradlew localJar. > Configure project :; Warning: using Java 1.8 but only Java 17 has been tested. FAILURE: Build failed with an exception. * Where:; Build file '/home/athchu/bin/gatk/build.gradle' line: 141. * What went wrong:; A problem occurred evaluating root project 'gatk'.; > A Java 17 compatible (Java 17 or later) version is required to build GATK, but 1.8 was found. See https://github.com/broadinstitute/gatk#building for information on how to build GATK. * Try:; > Run with --stacktrace option to get the stack trace.; > Run with --info or --debug option to get more log output.; > Run with --scan to get full insights. * Get more help at https://help.gradle.org; `; So to sum up, my issues are :; 1) downloaded gatk-4.4.0.0 but it contained invalid jar file and i ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8432
https://github.com/broadinstitute/gatk/issues/8432:847,Availability,error,error,847,"## Bug Report. Hi there,; So I downloaded the gatk-4.4-0.0.zip and unzipped it for using gatk. I also created the conda env using the gatkcondaenv.yml and used conda to install java ""1.7.0_91"". But when I run ./gatk --list I got this error message: . `; ./gatk --list; Using GATK jar /home/athchu/bin/gatk-4.4.0.0/gatk-package-4.4.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/athchu/bin/gatk-4.4.0.0/gatk-package-4.4.0.0-local.jar --help; Error: Invalid or corrupt jarfile /home/athchu/bin/gatk-4.4.0.0/gatk-package-4.4.0.0-local.jar; `; Next, I moved on to git clone the gatk repository, trying to build gatk. Again, I stay in the java ""1.7.0_91"" gatk env that I already created. But I got this error msg this time:; `; ./gradlew localJar; Gradle 7.5.1 requires Java 1.8 or later to run. You are currently using Java 1.7.; `; When I switch back to the server default java (1.8.0_292-b10), i got another error msg.; `; java -version; openjdk version ""1.8.0_292""; OpenJDK Runtime Environment (build 1.8.0_292-b10); OpenJDK 64-Bit Server VM (build 25.292-b10, mixed mode). ./gradlew localJar. > Configure project :; Warning: using Java 1.8 but only Java 17 has been tested. FAILURE: Build failed with an exception. * Where:; Build file '/home/athchu/bin/gatk/build.gradle' line: 141. * What went wrong:; A problem occurred evaluating root project 'gatk'.; > A Java 17 compatible (Java 17 or later) version is required to build GATK, but 1.8 was found. See https://github.com/broadinstitute/gatk#building for information on how to build GATK. * Try:; > Run with --stacktrace option to get the stack trace.; > Run with --info or --debug option to get more log output.; > Run with --scan to get full insights. * Get more help at https://help.gradle.org; `; So to sum up, my issues are :; 1) downloaded gatk-4.4.0.0 but it contained invalid jar file and i ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8432
https://github.com/broadinstitute/gatk/issues/8432:1055,Availability,error,error,1055,"atk-4.4-0.0.zip and unzipped it for using gatk. I also created the conda env using the gatkcondaenv.yml and used conda to install java ""1.7.0_91"". But when I run ./gatk --list I got this error message: . `; ./gatk --list; Using GATK jar /home/athchu/bin/gatk-4.4.0.0/gatk-package-4.4.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/athchu/bin/gatk-4.4.0.0/gatk-package-4.4.0.0-local.jar --help; Error: Invalid or corrupt jarfile /home/athchu/bin/gatk-4.4.0.0/gatk-package-4.4.0.0-local.jar; `; Next, I moved on to git clone the gatk repository, trying to build gatk. Again, I stay in the java ""1.7.0_91"" gatk env that I already created. But I got this error msg this time:; `; ./gradlew localJar; Gradle 7.5.1 requires Java 1.8 or later to run. You are currently using Java 1.7.; `; When I switch back to the server default java (1.8.0_292-b10), i got another error msg.; `; java -version; openjdk version ""1.8.0_292""; OpenJDK Runtime Environment (build 1.8.0_292-b10); OpenJDK 64-Bit Server VM (build 25.292-b10, mixed mode). ./gradlew localJar. > Configure project :; Warning: using Java 1.8 but only Java 17 has been tested. FAILURE: Build failed with an exception. * Where:; Build file '/home/athchu/bin/gatk/build.gradle' line: 141. * What went wrong:; A problem occurred evaluating root project 'gatk'.; > A Java 17 compatible (Java 17 or later) version is required to build GATK, but 1.8 was found. See https://github.com/broadinstitute/gatk#building for information on how to build GATK. * Try:; > Run with --stacktrace option to get the stack trace.; > Run with --info or --debug option to get more log output.; > Run with --scan to get full insights. * Get more help at https://help.gradle.org; `; So to sum up, my issues are :; 1) downloaded gatk-4.4.0.0 but it contained invalid jar file and i cannot run GATK; 2) following the github instr",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8432
https://github.com/broadinstitute/gatk/issues/8432:1323,Availability,FAILURE,FAILURE,1323,"gatk-package-4.4.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/athchu/bin/gatk-4.4.0.0/gatk-package-4.4.0.0-local.jar --help; Error: Invalid or corrupt jarfile /home/athchu/bin/gatk-4.4.0.0/gatk-package-4.4.0.0-local.jar; `; Next, I moved on to git clone the gatk repository, trying to build gatk. Again, I stay in the java ""1.7.0_91"" gatk env that I already created. But I got this error msg this time:; `; ./gradlew localJar; Gradle 7.5.1 requires Java 1.8 or later to run. You are currently using Java 1.7.; `; When I switch back to the server default java (1.8.0_292-b10), i got another error msg.; `; java -version; openjdk version ""1.8.0_292""; OpenJDK Runtime Environment (build 1.8.0_292-b10); OpenJDK 64-Bit Server VM (build 25.292-b10, mixed mode). ./gradlew localJar. > Configure project :; Warning: using Java 1.8 but only Java 17 has been tested. FAILURE: Build failed with an exception. * Where:; Build file '/home/athchu/bin/gatk/build.gradle' line: 141. * What went wrong:; A problem occurred evaluating root project 'gatk'.; > A Java 17 compatible (Java 17 or later) version is required to build GATK, but 1.8 was found. See https://github.com/broadinstitute/gatk#building for information on how to build GATK. * Try:; > Run with --stacktrace option to get the stack trace.; > Run with --info or --debug option to get more log output.; > Run with --scan to get full insights. * Get more help at https://help.gradle.org; `; So to sum up, my issues are :; 1) downloaded gatk-4.4.0.0 but it contained invalid jar file and i cannot run GATK; 2) following the github instruction, I cannot build gatk under java1.7.0_91, because it is incompatible to GRADLE7.5.1.; 3) built gatk using java 1.8.0_292 failed because gatk is java 17 compatible. Would you please advise on what I shall do? The docker installed in my sever doesn't not work....",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8432
https://github.com/broadinstitute/gatk/issues/8432:1937,Availability,down,downloaded,1937,"gatk-package-4.4.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/athchu/bin/gatk-4.4.0.0/gatk-package-4.4.0.0-local.jar --help; Error: Invalid or corrupt jarfile /home/athchu/bin/gatk-4.4.0.0/gatk-package-4.4.0.0-local.jar; `; Next, I moved on to git clone the gatk repository, trying to build gatk. Again, I stay in the java ""1.7.0_91"" gatk env that I already created. But I got this error msg this time:; `; ./gradlew localJar; Gradle 7.5.1 requires Java 1.8 or later to run. You are currently using Java 1.7.; `; When I switch back to the server default java (1.8.0_292-b10), i got another error msg.; `; java -version; openjdk version ""1.8.0_292""; OpenJDK Runtime Environment (build 1.8.0_292-b10); OpenJDK 64-Bit Server VM (build 25.292-b10, mixed mode). ./gradlew localJar. > Configure project :; Warning: using Java 1.8 but only Java 17 has been tested. FAILURE: Build failed with an exception. * Where:; Build file '/home/athchu/bin/gatk/build.gradle' line: 141. * What went wrong:; A problem occurred evaluating root project 'gatk'.; > A Java 17 compatible (Java 17 or later) version is required to build GATK, but 1.8 was found. See https://github.com/broadinstitute/gatk#building for information on how to build GATK. * Try:; > Run with --stacktrace option to get the stack trace.; > Run with --info or --debug option to get more log output.; > Run with --scan to get full insights. * Get more help at https://help.gradle.org; `; So to sum up, my issues are :; 1) downloaded gatk-4.4.0.0 but it contained invalid jar file and i cannot run GATK; 2) following the github instruction, I cannot build gatk under java1.7.0_91, because it is incompatible to GRADLE7.5.1.; 3) built gatk using java 1.8.0_292 failed because gatk is java 17 compatible. Would you please advise on what I shall do? The docker installed in my sever doesn't not work....",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8432
https://github.com/broadinstitute/gatk/issues/8432:169,Deployability,install,install,169,"## Bug Report. Hi there,; So I downloaded the gatk-4.4-0.0.zip and unzipped it for using gatk. I also created the conda env using the gatkcondaenv.yml and used conda to install java ""1.7.0_91"". But when I run ./gatk --list I got this error message: . `; ./gatk --list; Using GATK jar /home/athchu/bin/gatk-4.4.0.0/gatk-package-4.4.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/athchu/bin/gatk-4.4.0.0/gatk-package-4.4.0.0-local.jar --help; Error: Invalid or corrupt jarfile /home/athchu/bin/gatk-4.4.0.0/gatk-package-4.4.0.0-local.jar; `; Next, I moved on to git clone the gatk repository, trying to build gatk. Again, I stay in the java ""1.7.0_91"" gatk env that I already created. But I got this error msg this time:; `; ./gradlew localJar; Gradle 7.5.1 requires Java 1.8 or later to run. You are currently using Java 1.7.; `; When I switch back to the server default java (1.8.0_292-b10), i got another error msg.; `; java -version; openjdk version ""1.8.0_292""; OpenJDK Runtime Environment (build 1.8.0_292-b10); OpenJDK 64-Bit Server VM (build 25.292-b10, mixed mode). ./gradlew localJar. > Configure project :; Warning: using Java 1.8 but only Java 17 has been tested. FAILURE: Build failed with an exception. * Where:; Build file '/home/athchu/bin/gatk/build.gradle' line: 141. * What went wrong:; A problem occurred evaluating root project 'gatk'.; > A Java 17 compatible (Java 17 or later) version is required to build GATK, but 1.8 was found. See https://github.com/broadinstitute/gatk#building for information on how to build GATK. * Try:; > Run with --stacktrace option to get the stack trace.; > Run with --info or --debug option to get more log output.; > Run with --scan to get full insights. * Get more help at https://help.gradle.org; `; So to sum up, my issues are :; 1) downloaded gatk-4.4.0.0 but it contained invalid jar file and i ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8432
https://github.com/broadinstitute/gatk/issues/8432:2272,Deployability,install,installed,2272,"gatk-package-4.4.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/athchu/bin/gatk-4.4.0.0/gatk-package-4.4.0.0-local.jar --help; Error: Invalid or corrupt jarfile /home/athchu/bin/gatk-4.4.0.0/gatk-package-4.4.0.0-local.jar; `; Next, I moved on to git clone the gatk repository, trying to build gatk. Again, I stay in the java ""1.7.0_91"" gatk env that I already created. But I got this error msg this time:; `; ./gradlew localJar; Gradle 7.5.1 requires Java 1.8 or later to run. You are currently using Java 1.7.; `; When I switch back to the server default java (1.8.0_292-b10), i got another error msg.; `; java -version; openjdk version ""1.8.0_292""; OpenJDK Runtime Environment (build 1.8.0_292-b10); OpenJDK 64-Bit Server VM (build 25.292-b10, mixed mode). ./gradlew localJar. > Configure project :; Warning: using Java 1.8 but only Java 17 has been tested. FAILURE: Build failed with an exception. * Where:; Build file '/home/athchu/bin/gatk/build.gradle' line: 141. * What went wrong:; A problem occurred evaluating root project 'gatk'.; > A Java 17 compatible (Java 17 or later) version is required to build GATK, but 1.8 was found. See https://github.com/broadinstitute/gatk#building for information on how to build GATK. * Try:; > Run with --stacktrace option to get the stack trace.; > Run with --info or --debug option to get more log output.; > Run with --scan to get full insights. * Get more help at https://help.gradle.org; `; So to sum up, my issues are :; 1) downloaded gatk-4.4.0.0 but it contained invalid jar file and i cannot run GATK; 2) following the github instruction, I cannot build gatk under java1.7.0_91, because it is incompatible to GRADLE7.5.1.; 3) built gatk using java 1.8.0_292 failed because gatk is java 17 compatible. Would you please advise on what I shall do? The docker installed in my sever doesn't not work....",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8432
https://github.com/broadinstitute/gatk/issues/8432:240,Integrability,message,message,240,"## Bug Report. Hi there,; So I downloaded the gatk-4.4-0.0.zip and unzipped it for using gatk. I also created the conda env using the gatkcondaenv.yml and used conda to install java ""1.7.0_91"". But when I run ./gatk --list I got this error message: . `; ./gatk --list; Using GATK jar /home/athchu/bin/gatk-4.4.0.0/gatk-package-4.4.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/athchu/bin/gatk-4.4.0.0/gatk-package-4.4.0.0-local.jar --help; Error: Invalid or corrupt jarfile /home/athchu/bin/gatk-4.4.0.0/gatk-package-4.4.0.0-local.jar; `; Next, I moved on to git clone the gatk repository, trying to build gatk. Again, I stay in the java ""1.7.0_91"" gatk env that I already created. But I got this error msg this time:; `; ./gradlew localJar; Gradle 7.5.1 requires Java 1.8 or later to run. You are currently using Java 1.7.; `; When I switch back to the server default java (1.8.0_292-b10), i got another error msg.; `; java -version; openjdk version ""1.8.0_292""; OpenJDK Runtime Environment (build 1.8.0_292-b10); OpenJDK 64-Bit Server VM (build 25.292-b10, mixed mode). ./gradlew localJar. > Configure project :; Warning: using Java 1.8 but only Java 17 has been tested. FAILURE: Build failed with an exception. * Where:; Build file '/home/athchu/bin/gatk/build.gradle' line: 141. * What went wrong:; A problem occurred evaluating root project 'gatk'.; > A Java 17 compatible (Java 17 or later) version is required to build GATK, but 1.8 was found. See https://github.com/broadinstitute/gatk#building for information on how to build GATK. * Try:; > Run with --stacktrace option to get the stack trace.; > Run with --info or --debug option to get more log output.; > Run with --scan to get full insights. * Get more help at https://help.gradle.org; `; So to sum up, my issues are :; 1) downloaded gatk-4.4.0.0 but it contained invalid jar file and i ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8432
https://github.com/broadinstitute/gatk/issues/8432:1244,Modifiability,Config,Configure,1244,"ist; Using GATK jar /home/athchu/bin/gatk-4.4.0.0/gatk-package-4.4.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/athchu/bin/gatk-4.4.0.0/gatk-package-4.4.0.0-local.jar --help; Error: Invalid or corrupt jarfile /home/athchu/bin/gatk-4.4.0.0/gatk-package-4.4.0.0-local.jar; `; Next, I moved on to git clone the gatk repository, trying to build gatk. Again, I stay in the java ""1.7.0_91"" gatk env that I already created. But I got this error msg this time:; `; ./gradlew localJar; Gradle 7.5.1 requires Java 1.8 or later to run. You are currently using Java 1.7.; `; When I switch back to the server default java (1.8.0_292-b10), i got another error msg.; `; java -version; openjdk version ""1.8.0_292""; OpenJDK Runtime Environment (build 1.8.0_292-b10); OpenJDK 64-Bit Server VM (build 25.292-b10, mixed mode). ./gradlew localJar. > Configure project :; Warning: using Java 1.8 but only Java 17 has been tested. FAILURE: Build failed with an exception. * Where:; Build file '/home/athchu/bin/gatk/build.gradle' line: 141. * What went wrong:; A problem occurred evaluating root project 'gatk'.; > A Java 17 compatible (Java 17 or later) version is required to build GATK, but 1.8 was found. See https://github.com/broadinstitute/gatk#building for information on how to build GATK. * Try:; > Run with --stacktrace option to get the stack trace.; > Run with --info or --debug option to get more log output.; > Run with --scan to get full insights. * Get more help at https://help.gradle.org; `; So to sum up, my issues are :; 1) downloaded gatk-4.4.0.0 but it contained invalid jar file and i cannot run GATK; 2) following the github instruction, I cannot build gatk under java1.7.0_91, because it is incompatible to GRADLE7.5.1.; 3) built gatk using java 1.8.0_292 failed because gatk is java 17 compatible. Would you please advise on what I shall do? The",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8432
https://github.com/broadinstitute/gatk/issues/8432:1315,Testability,test,tested,1315,"k-4.4.0.0/gatk-package-4.4.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/athchu/bin/gatk-4.4.0.0/gatk-package-4.4.0.0-local.jar --help; Error: Invalid or corrupt jarfile /home/athchu/bin/gatk-4.4.0.0/gatk-package-4.4.0.0-local.jar; `; Next, I moved on to git clone the gatk repository, trying to build gatk. Again, I stay in the java ""1.7.0_91"" gatk env that I already created. But I got this error msg this time:; `; ./gradlew localJar; Gradle 7.5.1 requires Java 1.8 or later to run. You are currently using Java 1.7.; `; When I switch back to the server default java (1.8.0_292-b10), i got another error msg.; `; java -version; openjdk version ""1.8.0_292""; OpenJDK Runtime Environment (build 1.8.0_292-b10); OpenJDK 64-Bit Server VM (build 25.292-b10, mixed mode). ./gradlew localJar. > Configure project :; Warning: using Java 1.8 but only Java 17 has been tested. FAILURE: Build failed with an exception. * Where:; Build file '/home/athchu/bin/gatk/build.gradle' line: 141. * What went wrong:; A problem occurred evaluating root project 'gatk'.; > A Java 17 compatible (Java 17 or later) version is required to build GATK, but 1.8 was found. See https://github.com/broadinstitute/gatk#building for information on how to build GATK. * Try:; > Run with --stacktrace option to get the stack trace.; > Run with --info or --debug option to get more log output.; > Run with --scan to get full insights. * Get more help at https://help.gradle.org; `; So to sum up, my issues are :; 1) downloaded gatk-4.4.0.0 but it contained invalid jar file and i cannot run GATK; 2) following the github instruction, I cannot build gatk under java1.7.0_91, because it is incompatible to GRADLE7.5.1.; 3) built gatk using java 1.8.0_292 failed because gatk is java 17 compatible. Would you please advise on what I shall do? The docker installed in my sever doesn't not",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8432
https://github.com/broadinstitute/gatk/issues/8432:1803,Testability,log,log,1803,"gatk-package-4.4.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/athchu/bin/gatk-4.4.0.0/gatk-package-4.4.0.0-local.jar --help; Error: Invalid or corrupt jarfile /home/athchu/bin/gatk-4.4.0.0/gatk-package-4.4.0.0-local.jar; `; Next, I moved on to git clone the gatk repository, trying to build gatk. Again, I stay in the java ""1.7.0_91"" gatk env that I already created. But I got this error msg this time:; `; ./gradlew localJar; Gradle 7.5.1 requires Java 1.8 or later to run. You are currently using Java 1.7.; `; When I switch back to the server default java (1.8.0_292-b10), i got another error msg.; `; java -version; openjdk version ""1.8.0_292""; OpenJDK Runtime Environment (build 1.8.0_292-b10); OpenJDK 64-Bit Server VM (build 25.292-b10, mixed mode). ./gradlew localJar. > Configure project :; Warning: using Java 1.8 but only Java 17 has been tested. FAILURE: Build failed with an exception. * Where:; Build file '/home/athchu/bin/gatk/build.gradle' line: 141. * What went wrong:; A problem occurred evaluating root project 'gatk'.; > A Java 17 compatible (Java 17 or later) version is required to build GATK, but 1.8 was found. See https://github.com/broadinstitute/gatk#building for information on how to build GATK. * Try:; > Run with --stacktrace option to get the stack trace.; > Run with --info or --debug option to get more log output.; > Run with --scan to get full insights. * Get more help at https://help.gradle.org; `; So to sum up, my issues are :; 1) downloaded gatk-4.4.0.0 but it contained invalid jar file and i cannot run GATK; 2) following the github instruction, I cannot build gatk under java1.7.0_91, because it is incompatible to GRADLE7.5.1.; 3) built gatk using java 1.8.0_292 failed because gatk is java 17 compatible. Would you please advise on what I shall do? The docker installed in my sever doesn't not work....",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8432
https://github.com/broadinstitute/gatk/pull/8433:16,Deployability,integrat,integration,16,This PR adds an integration test for Exomes.; It also adds an optional input to the test to allow you to use the default dockers (and NOT build the gatk override jar) if you so desire. Passing Integration test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/e16edc16-92a7-4a52-834a-1b45e1a2f92c).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8433
https://github.com/broadinstitute/gatk/pull/8433:193,Deployability,Integrat,Integration,193,This PR adds an integration test for Exomes.; It also adds an optional input to the test to allow you to use the default dockers (and NOT build the gatk override jar) if you so desire. Passing Integration test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/e16edc16-92a7-4a52-834a-1b45e1a2f92c).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8433
https://github.com/broadinstitute/gatk/pull/8433:16,Integrability,integrat,integration,16,This PR adds an integration test for Exomes.; It also adds an optional input to the test to allow you to use the default dockers (and NOT build the gatk override jar) if you so desire. Passing Integration test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/e16edc16-92a7-4a52-834a-1b45e1a2f92c).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8433
https://github.com/broadinstitute/gatk/pull/8433:193,Integrability,Integrat,Integration,193,This PR adds an integration test for Exomes.; It also adds an optional input to the test to allow you to use the default dockers (and NOT build the gatk override jar) if you so desire. Passing Integration test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/e16edc16-92a7-4a52-834a-1b45e1a2f92c).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8433
https://github.com/broadinstitute/gatk/pull/8433:28,Testability,test,test,28,This PR adds an integration test for Exomes.; It also adds an optional input to the test to allow you to use the default dockers (and NOT build the gatk override jar) if you so desire. Passing Integration test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/e16edc16-92a7-4a52-834a-1b45e1a2f92c).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8433
https://github.com/broadinstitute/gatk/pull/8433:84,Testability,test,test,84,This PR adds an integration test for Exomes.; It also adds an optional input to the test to allow you to use the default dockers (and NOT build the gatk override jar) if you so desire. Passing Integration test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/e16edc16-92a7-4a52-834a-1b45e1a2f92c).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8433
https://github.com/broadinstitute/gatk/pull/8433:205,Testability,test,test,205,This PR adds an integration test for Exomes.; It also adds an optional input to the test to allow you to use the default dockers (and NOT build the gatk override jar) if you so desire. Passing Integration test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/e16edc16-92a7-4a52-834a-1b45e1a2f92c).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8433
https://github.com/broadinstitute/gatk/pull/8434:82,Deployability,integrat,integration,82,"Rebasing on the most recent commit to `ah_var_store` with some fixups, successful integration run [here](https://app.terra.bio/#workspaces/gvs-dev/mlc%20GVS%20Quickstart%203%20samples/job_history/1156657f-5c31-446a-92e1-5e39ae012ce2). The Docker CI breakages appear to be affecting all Java 8 based branches, not just this one (I currently have no idea what's wrong there).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8434
https://github.com/broadinstitute/gatk/pull/8434:82,Integrability,integrat,integration,82,"Rebasing on the most recent commit to `ah_var_store` with some fixups, successful integration run [here](https://app.terra.bio/#workspaces/gvs-dev/mlc%20GVS%20Quickstart%203%20samples/job_history/1156657f-5c31-446a-92e1-5e39ae012ce2). The Docker CI breakages appear to be affecting all Java 8 based branches, not just this one (I currently have no idea what's wrong there).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8434
https://github.com/broadinstitute/gatk/issues/8435:29,Deployability,pipeline,pipeline,29,"Hi,. I'm working on the GATK pipeline with aim to find the best pipeline before start my project.; I never do VQSR before. I need your suggestion according to my result.; I using the NA12878 Exome data from SureSelect7 version with 4 difference main pipelines. All pipeline had follow the best practice option.; 1. BWA7.15-GATK3.7; 2. BWA7.15-GATK4.4; 3. DRAMAP1.2.1-GATK4.4; 4. DRAMAP1.2.1-GATK4.4-dragen. I have questions about the number of variant output and VQSR plot from each pipeline; BWA-GATK3.7 pipeline gave me 100694 variants; BWA-GATK4.4 pipeline gave me 45861 variants; DRAMAP-GATK4.4 gave me 44835 variants; DRAMAP-GATK4.4-dragen gave me 42969 variants. For the VQSR tranchs plot; 1. BWA-GATK3.7; ![image](https://github.com/broadinstitute/gatk/assets/15682256/7d158c0b-d0af-4e5e-a605-993fad38cc0b). 2. BWA-GATK4.4; ![image](https://github.com/broadinstitute/gatk/assets/15682256/1898f5f0-cfa4-476c-8637-9a4ea3333f49). 3. DRAMAP-GATK4.4; ![image](https://github.com/broadinstitute/gatk/assets/15682256/52f6e56c-7dc5-4047-9086-2a9fb37f478f). 4. DRAMAP-GATK4.4-dragen; ![image](https://github.com/broadinstitute/gatk/assets/15682256/20bfc543-ad76-4e3d-a2ba-693cbed46a92). From what I see it far from Ti/Tv ratio that suitable for whole exome sequencing as ~ 2.8. Please give me some suggestion. ; Which pipeline I should selected as the best pipeline for my project. Thanks,; GM",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8435
https://github.com/broadinstitute/gatk/issues/8435:64,Deployability,pipeline,pipeline,64,"Hi,. I'm working on the GATK pipeline with aim to find the best pipeline before start my project.; I never do VQSR before. I need your suggestion according to my result.; I using the NA12878 Exome data from SureSelect7 version with 4 difference main pipelines. All pipeline had follow the best practice option.; 1. BWA7.15-GATK3.7; 2. BWA7.15-GATK4.4; 3. DRAMAP1.2.1-GATK4.4; 4. DRAMAP1.2.1-GATK4.4-dragen. I have questions about the number of variant output and VQSR plot from each pipeline; BWA-GATK3.7 pipeline gave me 100694 variants; BWA-GATK4.4 pipeline gave me 45861 variants; DRAMAP-GATK4.4 gave me 44835 variants; DRAMAP-GATK4.4-dragen gave me 42969 variants. For the VQSR tranchs plot; 1. BWA-GATK3.7; ![image](https://github.com/broadinstitute/gatk/assets/15682256/7d158c0b-d0af-4e5e-a605-993fad38cc0b). 2. BWA-GATK4.4; ![image](https://github.com/broadinstitute/gatk/assets/15682256/1898f5f0-cfa4-476c-8637-9a4ea3333f49). 3. DRAMAP-GATK4.4; ![image](https://github.com/broadinstitute/gatk/assets/15682256/52f6e56c-7dc5-4047-9086-2a9fb37f478f). 4. DRAMAP-GATK4.4-dragen; ![image](https://github.com/broadinstitute/gatk/assets/15682256/20bfc543-ad76-4e3d-a2ba-693cbed46a92). From what I see it far from Ti/Tv ratio that suitable for whole exome sequencing as ~ 2.8. Please give me some suggestion. ; Which pipeline I should selected as the best pipeline for my project. Thanks,; GM",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8435
https://github.com/broadinstitute/gatk/issues/8435:250,Deployability,pipeline,pipelines,250,"Hi,. I'm working on the GATK pipeline with aim to find the best pipeline before start my project.; I never do VQSR before. I need your suggestion according to my result.; I using the NA12878 Exome data from SureSelect7 version with 4 difference main pipelines. All pipeline had follow the best practice option.; 1. BWA7.15-GATK3.7; 2. BWA7.15-GATK4.4; 3. DRAMAP1.2.1-GATK4.4; 4. DRAMAP1.2.1-GATK4.4-dragen. I have questions about the number of variant output and VQSR plot from each pipeline; BWA-GATK3.7 pipeline gave me 100694 variants; BWA-GATK4.4 pipeline gave me 45861 variants; DRAMAP-GATK4.4 gave me 44835 variants; DRAMAP-GATK4.4-dragen gave me 42969 variants. For the VQSR tranchs plot; 1. BWA-GATK3.7; ![image](https://github.com/broadinstitute/gatk/assets/15682256/7d158c0b-d0af-4e5e-a605-993fad38cc0b). 2. BWA-GATK4.4; ![image](https://github.com/broadinstitute/gatk/assets/15682256/1898f5f0-cfa4-476c-8637-9a4ea3333f49). 3. DRAMAP-GATK4.4; ![image](https://github.com/broadinstitute/gatk/assets/15682256/52f6e56c-7dc5-4047-9086-2a9fb37f478f). 4. DRAMAP-GATK4.4-dragen; ![image](https://github.com/broadinstitute/gatk/assets/15682256/20bfc543-ad76-4e3d-a2ba-693cbed46a92). From what I see it far from Ti/Tv ratio that suitable for whole exome sequencing as ~ 2.8. Please give me some suggestion. ; Which pipeline I should selected as the best pipeline for my project. Thanks,; GM",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8435
https://github.com/broadinstitute/gatk/issues/8435:265,Deployability,pipeline,pipeline,265,"Hi,. I'm working on the GATK pipeline with aim to find the best pipeline before start my project.; I never do VQSR before. I need your suggestion according to my result.; I using the NA12878 Exome data from SureSelect7 version with 4 difference main pipelines. All pipeline had follow the best practice option.; 1. BWA7.15-GATK3.7; 2. BWA7.15-GATK4.4; 3. DRAMAP1.2.1-GATK4.4; 4. DRAMAP1.2.1-GATK4.4-dragen. I have questions about the number of variant output and VQSR plot from each pipeline; BWA-GATK3.7 pipeline gave me 100694 variants; BWA-GATK4.4 pipeline gave me 45861 variants; DRAMAP-GATK4.4 gave me 44835 variants; DRAMAP-GATK4.4-dragen gave me 42969 variants. For the VQSR tranchs plot; 1. BWA-GATK3.7; ![image](https://github.com/broadinstitute/gatk/assets/15682256/7d158c0b-d0af-4e5e-a605-993fad38cc0b). 2. BWA-GATK4.4; ![image](https://github.com/broadinstitute/gatk/assets/15682256/1898f5f0-cfa4-476c-8637-9a4ea3333f49). 3. DRAMAP-GATK4.4; ![image](https://github.com/broadinstitute/gatk/assets/15682256/52f6e56c-7dc5-4047-9086-2a9fb37f478f). 4. DRAMAP-GATK4.4-dragen; ![image](https://github.com/broadinstitute/gatk/assets/15682256/20bfc543-ad76-4e3d-a2ba-693cbed46a92). From what I see it far from Ti/Tv ratio that suitable for whole exome sequencing as ~ 2.8. Please give me some suggestion. ; Which pipeline I should selected as the best pipeline for my project. Thanks,; GM",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8435
https://github.com/broadinstitute/gatk/issues/8435:483,Deployability,pipeline,pipeline,483,"Hi,. I'm working on the GATK pipeline with aim to find the best pipeline before start my project.; I never do VQSR before. I need your suggestion according to my result.; I using the NA12878 Exome data from SureSelect7 version with 4 difference main pipelines. All pipeline had follow the best practice option.; 1. BWA7.15-GATK3.7; 2. BWA7.15-GATK4.4; 3. DRAMAP1.2.1-GATK4.4; 4. DRAMAP1.2.1-GATK4.4-dragen. I have questions about the number of variant output and VQSR plot from each pipeline; BWA-GATK3.7 pipeline gave me 100694 variants; BWA-GATK4.4 pipeline gave me 45861 variants; DRAMAP-GATK4.4 gave me 44835 variants; DRAMAP-GATK4.4-dragen gave me 42969 variants. For the VQSR tranchs plot; 1. BWA-GATK3.7; ![image](https://github.com/broadinstitute/gatk/assets/15682256/7d158c0b-d0af-4e5e-a605-993fad38cc0b). 2. BWA-GATK4.4; ![image](https://github.com/broadinstitute/gatk/assets/15682256/1898f5f0-cfa4-476c-8637-9a4ea3333f49). 3. DRAMAP-GATK4.4; ![image](https://github.com/broadinstitute/gatk/assets/15682256/52f6e56c-7dc5-4047-9086-2a9fb37f478f). 4. DRAMAP-GATK4.4-dragen; ![image](https://github.com/broadinstitute/gatk/assets/15682256/20bfc543-ad76-4e3d-a2ba-693cbed46a92). From what I see it far from Ti/Tv ratio that suitable for whole exome sequencing as ~ 2.8. Please give me some suggestion. ; Which pipeline I should selected as the best pipeline for my project. Thanks,; GM",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8435
https://github.com/broadinstitute/gatk/issues/8435:505,Deployability,pipeline,pipeline,505,"Hi,. I'm working on the GATK pipeline with aim to find the best pipeline before start my project.; I never do VQSR before. I need your suggestion according to my result.; I using the NA12878 Exome data from SureSelect7 version with 4 difference main pipelines. All pipeline had follow the best practice option.; 1. BWA7.15-GATK3.7; 2. BWA7.15-GATK4.4; 3. DRAMAP1.2.1-GATK4.4; 4. DRAMAP1.2.1-GATK4.4-dragen. I have questions about the number of variant output and VQSR plot from each pipeline; BWA-GATK3.7 pipeline gave me 100694 variants; BWA-GATK4.4 pipeline gave me 45861 variants; DRAMAP-GATK4.4 gave me 44835 variants; DRAMAP-GATK4.4-dragen gave me 42969 variants. For the VQSR tranchs plot; 1. BWA-GATK3.7; ![image](https://github.com/broadinstitute/gatk/assets/15682256/7d158c0b-d0af-4e5e-a605-993fad38cc0b). 2. BWA-GATK4.4; ![image](https://github.com/broadinstitute/gatk/assets/15682256/1898f5f0-cfa4-476c-8637-9a4ea3333f49). 3. DRAMAP-GATK4.4; ![image](https://github.com/broadinstitute/gatk/assets/15682256/52f6e56c-7dc5-4047-9086-2a9fb37f478f). 4. DRAMAP-GATK4.4-dragen; ![image](https://github.com/broadinstitute/gatk/assets/15682256/20bfc543-ad76-4e3d-a2ba-693cbed46a92). From what I see it far from Ti/Tv ratio that suitable for whole exome sequencing as ~ 2.8. Please give me some suggestion. ; Which pipeline I should selected as the best pipeline for my project. Thanks,; GM",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8435
https://github.com/broadinstitute/gatk/issues/8435:551,Deployability,pipeline,pipeline,551,"Hi,. I'm working on the GATK pipeline with aim to find the best pipeline before start my project.; I never do VQSR before. I need your suggestion according to my result.; I using the NA12878 Exome data from SureSelect7 version with 4 difference main pipelines. All pipeline had follow the best practice option.; 1. BWA7.15-GATK3.7; 2. BWA7.15-GATK4.4; 3. DRAMAP1.2.1-GATK4.4; 4. DRAMAP1.2.1-GATK4.4-dragen. I have questions about the number of variant output and VQSR plot from each pipeline; BWA-GATK3.7 pipeline gave me 100694 variants; BWA-GATK4.4 pipeline gave me 45861 variants; DRAMAP-GATK4.4 gave me 44835 variants; DRAMAP-GATK4.4-dragen gave me 42969 variants. For the VQSR tranchs plot; 1. BWA-GATK3.7; ![image](https://github.com/broadinstitute/gatk/assets/15682256/7d158c0b-d0af-4e5e-a605-993fad38cc0b). 2. BWA-GATK4.4; ![image](https://github.com/broadinstitute/gatk/assets/15682256/1898f5f0-cfa4-476c-8637-9a4ea3333f49). 3. DRAMAP-GATK4.4; ![image](https://github.com/broadinstitute/gatk/assets/15682256/52f6e56c-7dc5-4047-9086-2a9fb37f478f). 4. DRAMAP-GATK4.4-dragen; ![image](https://github.com/broadinstitute/gatk/assets/15682256/20bfc543-ad76-4e3d-a2ba-693cbed46a92). From what I see it far from Ti/Tv ratio that suitable for whole exome sequencing as ~ 2.8. Please give me some suggestion. ; Which pipeline I should selected as the best pipeline for my project. Thanks,; GM",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8435
https://github.com/broadinstitute/gatk/issues/8435:1316,Deployability,pipeline,pipeline,1316,"Hi,. I'm working on the GATK pipeline with aim to find the best pipeline before start my project.; I never do VQSR before. I need your suggestion according to my result.; I using the NA12878 Exome data from SureSelect7 version with 4 difference main pipelines. All pipeline had follow the best practice option.; 1. BWA7.15-GATK3.7; 2. BWA7.15-GATK4.4; 3. DRAMAP1.2.1-GATK4.4; 4. DRAMAP1.2.1-GATK4.4-dragen. I have questions about the number of variant output and VQSR plot from each pipeline; BWA-GATK3.7 pipeline gave me 100694 variants; BWA-GATK4.4 pipeline gave me 45861 variants; DRAMAP-GATK4.4 gave me 44835 variants; DRAMAP-GATK4.4-dragen gave me 42969 variants. For the VQSR tranchs plot; 1. BWA-GATK3.7; ![image](https://github.com/broadinstitute/gatk/assets/15682256/7d158c0b-d0af-4e5e-a605-993fad38cc0b). 2. BWA-GATK4.4; ![image](https://github.com/broadinstitute/gatk/assets/15682256/1898f5f0-cfa4-476c-8637-9a4ea3333f49). 3. DRAMAP-GATK4.4; ![image](https://github.com/broadinstitute/gatk/assets/15682256/52f6e56c-7dc5-4047-9086-2a9fb37f478f). 4. DRAMAP-GATK4.4-dragen; ![image](https://github.com/broadinstitute/gatk/assets/15682256/20bfc543-ad76-4e3d-a2ba-693cbed46a92). From what I see it far from Ti/Tv ratio that suitable for whole exome sequencing as ~ 2.8. Please give me some suggestion. ; Which pipeline I should selected as the best pipeline for my project. Thanks,; GM",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8435
https://github.com/broadinstitute/gatk/issues/8435:1355,Deployability,pipeline,pipeline,1355,"Hi,. I'm working on the GATK pipeline with aim to find the best pipeline before start my project.; I never do VQSR before. I need your suggestion according to my result.; I using the NA12878 Exome data from SureSelect7 version with 4 difference main pipelines. All pipeline had follow the best practice option.; 1. BWA7.15-GATK3.7; 2. BWA7.15-GATK4.4; 3. DRAMAP1.2.1-GATK4.4; 4. DRAMAP1.2.1-GATK4.4-dragen. I have questions about the number of variant output and VQSR plot from each pipeline; BWA-GATK3.7 pipeline gave me 100694 variants; BWA-GATK4.4 pipeline gave me 45861 variants; DRAMAP-GATK4.4 gave me 44835 variants; DRAMAP-GATK4.4-dragen gave me 42969 variants. For the VQSR tranchs plot; 1. BWA-GATK3.7; ![image](https://github.com/broadinstitute/gatk/assets/15682256/7d158c0b-d0af-4e5e-a605-993fad38cc0b). 2. BWA-GATK4.4; ![image](https://github.com/broadinstitute/gatk/assets/15682256/1898f5f0-cfa4-476c-8637-9a4ea3333f49). 3. DRAMAP-GATK4.4; ![image](https://github.com/broadinstitute/gatk/assets/15682256/52f6e56c-7dc5-4047-9086-2a9fb37f478f). 4. DRAMAP-GATK4.4-dragen; ![image](https://github.com/broadinstitute/gatk/assets/15682256/20bfc543-ad76-4e3d-a2ba-693cbed46a92). From what I see it far from Ti/Tv ratio that suitable for whole exome sequencing as ~ 2.8. Please give me some suggestion. ; Which pipeline I should selected as the best pipeline for my project. Thanks,; GM",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8435
https://github.com/broadinstitute/gatk/pull/8438:255,Safety,avoid,avoid-nio,255,adding two new parameters which work together to allow passing through files from azure too genomicsDB; `--header <vcf>` which lets you specify a vcf file to use the header from as your merged header. Do not mess this up or you will likely be doomed.; `--avoid-nio` which disables GATK sanity checks that involve reading the files since this would require opening them on azure. This needs tests but I wanted to put it here for @meganshand to try.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8438
https://github.com/broadinstitute/gatk/pull/8438:286,Safety,sanity check,sanity checks,286,adding two new parameters which work together to allow passing through files from azure too genomicsDB; `--header <vcf>` which lets you specify a vcf file to use the header from as your merged header. Do not mess this up or you will likely be doomed.; `--avoid-nio` which disables GATK sanity checks that involve reading the files since this would require opening them on azure. This needs tests but I wanted to put it here for @meganshand to try.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8438
https://github.com/broadinstitute/gatk/pull/8438:390,Testability,test,tests,390,adding two new parameters which work together to allow passing through files from azure too genomicsDB; `--header <vcf>` which lets you specify a vcf file to use the header from as your merged header. Do not mess this up or you will likely be doomed.; `--avoid-nio` which disables GATK sanity checks that involve reading the files since this would require opening them on azure. This needs tests but I wanted to put it here for @meganshand to try.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8438
https://github.com/broadinstitute/gatk/issues/8440:92,Availability,error,error,92,I'm working on some Plasmodium falciparum callsets in GATK and I have come across a curious error:. ```; Using GATK wrapper script /juffowup/gatk/build/install/gatk/bin/gatk; Running:; /juffowup/gatk/build/install/gatk/bin/gatk HaplotypeCaller -R /juffowup2/malaria/references/PlasmoDB-61_Pfalciparum3D7_Genome.fasta -I /juffowup2/malaria/haplotypecaller_arg_testing/fixed_bam/PG0004-CW.aligned.merged.markDuplicates.sorted.BQSR.bam -O /juffowup2/malaria/haplotypecaller_arg_testing/PG0004-CW.haplotype_caller.fixed_bam_file.with_pileup.g.vcf.gz --bam-output /juffowup2/malaria/haplotypecaller_arg_testing/PG0004-CW.haplotype_caller.fixed_bam_file.with_pileup.bamout.bam -contamination 0 --sample-ploidy 2 --linked-de-bruijn-graph --pileup-detection true --pileup-detection-enable-indel-pileup-calling true --max-reads-per-alignment-start 20 --annotate-with-num-discovered-alleles -GQB 10 -GQB 20 -GQB 30 -GQB 40 -GQB 50 -GQB 60 -GQB 70 -GQB 80 -GQB 90 -G StandardAnnotation -G StandardHCAnnotation -ERC GVCF --verbosity INFO; 14:14:15.323 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 14:14:15.328 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardHCAnnotation) is enabled for this tool by default; 14:14:15.388 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/juffowup/gatk/build/install/gatk/lib/gkl-0.8.11.jar!/com/intel/gkl/native/libgkl_compression.so; 14:14:15.435 INFO HaplotypeCaller - ------------------------------------------------------------; 14:14:15.439 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.4.0.0-44-g1529aa1-SNAPSHOT; 14:14:15.439 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:14:15.439 INFO HaplotypeCaller - Executing as jonn@dsde-methods-jonn-juffowup on Linux v5.4.0-1104-gcp amd64; 14:14:15.439 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8440
https://github.com/broadinstitute/gatk/issues/8440:1078,Availability,Redundant,Redundant,1078,"ng GATK wrapper script /juffowup/gatk/build/install/gatk/bin/gatk; Running:; /juffowup/gatk/build/install/gatk/bin/gatk HaplotypeCaller -R /juffowup2/malaria/references/PlasmoDB-61_Pfalciparum3D7_Genome.fasta -I /juffowup2/malaria/haplotypecaller_arg_testing/fixed_bam/PG0004-CW.aligned.merged.markDuplicates.sorted.BQSR.bam -O /juffowup2/malaria/haplotypecaller_arg_testing/PG0004-CW.haplotype_caller.fixed_bam_file.with_pileup.g.vcf.gz --bam-output /juffowup2/malaria/haplotypecaller_arg_testing/PG0004-CW.haplotype_caller.fixed_bam_file.with_pileup.bamout.bam -contamination 0 --sample-ploidy 2 --linked-de-bruijn-graph --pileup-detection true --pileup-detection-enable-indel-pileup-calling true --max-reads-per-alignment-start 20 --annotate-with-num-discovered-alleles -GQB 10 -GQB 20 -GQB 30 -GQB 40 -GQB 50 -GQB 60 -GQB 70 -GQB 80 -GQB 90 -G StandardAnnotation -G StandardHCAnnotation -ERC GVCF --verbosity INFO; 14:14:15.323 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 14:14:15.328 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardHCAnnotation) is enabled for this tool by default; 14:14:15.388 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/juffowup/gatk/build/install/gatk/lib/gkl-0.8.11.jar!/com/intel/gkl/native/libgkl_compression.so; 14:14:15.435 INFO HaplotypeCaller - ------------------------------------------------------------; 14:14:15.439 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.4.0.0-44-g1529aa1-SNAPSHOT; 14:14:15.439 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:14:15.439 INFO HaplotypeCaller - Executing as jonn@dsde-methods-jonn-juffowup on Linux v5.4.0-1104-gcp amd64; 14:14:15.439 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v17.0.7+7; 14:14:15.440 INFO HaplotypeCaller - Start Date/Time: July 26, 2023 at 2:14:15 PM UTC; ...; 22:1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8440
https://github.com/broadinstitute/gatk/issues/8440:1222,Availability,Redundant,Redundant,1222,"wup2/malaria/references/PlasmoDB-61_Pfalciparum3D7_Genome.fasta -I /juffowup2/malaria/haplotypecaller_arg_testing/fixed_bam/PG0004-CW.aligned.merged.markDuplicates.sorted.BQSR.bam -O /juffowup2/malaria/haplotypecaller_arg_testing/PG0004-CW.haplotype_caller.fixed_bam_file.with_pileup.g.vcf.gz --bam-output /juffowup2/malaria/haplotypecaller_arg_testing/PG0004-CW.haplotype_caller.fixed_bam_file.with_pileup.bamout.bam -contamination 0 --sample-ploidy 2 --linked-de-bruijn-graph --pileup-detection true --pileup-detection-enable-indel-pileup-calling true --max-reads-per-alignment-start 20 --annotate-with-num-discovered-alleles -GQB 10 -GQB 20 -GQB 30 -GQB 40 -GQB 50 -GQB 60 -GQB 70 -GQB 80 -GQB 90 -G StandardAnnotation -G StandardHCAnnotation -ERC GVCF --verbosity INFO; 14:14:15.323 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 14:14:15.328 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardHCAnnotation) is enabled for this tool by default; 14:14:15.388 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/juffowup/gatk/build/install/gatk/lib/gkl-0.8.11.jar!/com/intel/gkl/native/libgkl_compression.so; 14:14:15.435 INFO HaplotypeCaller - ------------------------------------------------------------; 14:14:15.439 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.4.0.0-44-g1529aa1-SNAPSHOT; 14:14:15.439 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:14:15.439 INFO HaplotypeCaller - Executing as jonn@dsde-methods-jonn-juffowup on Linux v5.4.0-1104-gcp amd64; 14:14:15.439 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v17.0.7+7; 14:14:15.440 INFO HaplotypeCaller - Start Date/Time: July 26, 2023 at 2:14:15 PM UTC; ...; 22:15:34.977 INFO HaplotypeCaller - Shutting down engine; [July 26, 2023 at 10:15:34 PM UTC] org.broadinstitute.hellbender.tools.walkers.haplotypecal",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8440
https://github.com/broadinstitute/gatk/issues/8440:2149,Availability,down,down,2149,"l by default; 14:14:15.328 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardHCAnnotation) is enabled for this tool by default; 14:14:15.388 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/juffowup/gatk/build/install/gatk/lib/gkl-0.8.11.jar!/com/intel/gkl/native/libgkl_compression.so; 14:14:15.435 INFO HaplotypeCaller - ------------------------------------------------------------; 14:14:15.439 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.4.0.0-44-g1529aa1-SNAPSHOT; 14:14:15.439 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:14:15.439 INFO HaplotypeCaller - Executing as jonn@dsde-methods-jonn-juffowup on Linux v5.4.0-1104-gcp amd64; 14:14:15.439 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v17.0.7+7; 14:14:15.440 INFO HaplotypeCaller - Start Date/Time: July 26, 2023 at 2:14:15 PM UTC; ...; 22:15:34.977 INFO HaplotypeCaller - Shutting down engine; [July 26, 2023 at 10:15:34 PM UTC] org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller done. Elapsed time: 481.33 minutes.; Runtime.totalMemory()=47982837760; java.lang.NegativeArraySizeException: -896617256; at org.broadinstitute.hellbender.utils.pairhmm.VectorLoglessPairHMM.computeLog10Likelihoods(VectorLoglessPairHMM.java:131); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngine.java:272); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngine.java:197); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngine.java:177); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:790); at org.b",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8440
https://github.com/broadinstitute/gatk/issues/8440:4327,Availability,error,error,4327,"itute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:200); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:173); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1098); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:149); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:198); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:217); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:166); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:209); at org.broadinstitute.hellbender.Main.main(Main.java:306). real 481m24.418s; user 581m54.752s; sys 2m49.965s. ```. This run did not complete successfully - the Exception caused it to fail prematurely. . Previously I had seen HaplotypeCaller run out of memory and fail in almost as much time, so I think this and the OOM error are related. The only difference in invocation was that with the OOM failure, I was running with the default for `--max-reads-per-alignment-start` (`50`). This also works just fine with that setting at 15. The failure seems to occur around the same place in the data each time (the end of `chr13`). At that point in the data, there is a very large pileup which is probably instigating this. Additionally, if I remove the `--linked-de-bruijn-graph` argument, this runs just fine with the default setting of `--max-reads-per-alignment-start`. I have a minimally reproductive dataset that I can share which reproduces the OOM error for sure (I'm 99% sure it reproduces this one as well). For the OOM failures, the final logs from HaplotypeCaller look like this:. ```; ./gatk HaplotypeCaller ...; ...; 15:56:23.205 INFO ProgressMeter - Pf3D7_13_v3:2603234 100.5 114070 1134.5; 15:56:33.443 INFO ProgressMeter - Pf3D7_13_v3:2661462 100.7 114420 1136",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8440
https://github.com/broadinstitute/gatk/issues/8440:4402,Availability,failure,failure,4402,"blyRegionWalker.traverse(AssemblyRegionWalker.java:173); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1098); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:149); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:198); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:217); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:166); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:209); at org.broadinstitute.hellbender.Main.main(Main.java:306). real 481m24.418s; user 581m54.752s; sys 2m49.965s. ```. This run did not complete successfully - the Exception caused it to fail prematurely. . Previously I had seen HaplotypeCaller run out of memory and fail in almost as much time, so I think this and the OOM error are related. The only difference in invocation was that with the OOM failure, I was running with the default for `--max-reads-per-alignment-start` (`50`). This also works just fine with that setting at 15. The failure seems to occur around the same place in the data each time (the end of `chr13`). At that point in the data, there is a very large pileup which is probably instigating this. Additionally, if I remove the `--linked-de-bruijn-graph` argument, this runs just fine with the default setting of `--max-reads-per-alignment-start`. I have a minimally reproductive dataset that I can share which reproduces the OOM error for sure (I'm 99% sure it reproduces this one as well). For the OOM failures, the final logs from HaplotypeCaller look like this:. ```; ./gatk HaplotypeCaller ...; ...; 15:56:23.205 INFO ProgressMeter - Pf3D7_13_v3:2603234 100.5 114070 1134.5; 15:56:33.443 INFO ProgressMeter - Pf3D7_13_v3:2661462 100.7 114420 1136.1; Dangling End recovery killed because of a loop (getReferencePathForwardFromKmer); 15:56:43.998 INFO ProgressMeter - Pf3D7_13_v3:2730055",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8440
https://github.com/broadinstitute/gatk/issues/8440:4543,Availability,failure,failure,4543,"mdline.CommandLineProgram.runTool(CommandLineProgram.java:149); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:198); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:217); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:166); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:209); at org.broadinstitute.hellbender.Main.main(Main.java:306). real 481m24.418s; user 581m54.752s; sys 2m49.965s. ```. This run did not complete successfully - the Exception caused it to fail prematurely. . Previously I had seen HaplotypeCaller run out of memory and fail in almost as much time, so I think this and the OOM error are related. The only difference in invocation was that with the OOM failure, I was running with the default for `--max-reads-per-alignment-start` (`50`). This also works just fine with that setting at 15. The failure seems to occur around the same place in the data each time (the end of `chr13`). At that point in the data, there is a very large pileup which is probably instigating this. Additionally, if I remove the `--linked-de-bruijn-graph` argument, this runs just fine with the default setting of `--max-reads-per-alignment-start`. I have a minimally reproductive dataset that I can share which reproduces the OOM error for sure (I'm 99% sure it reproduces this one as well). For the OOM failures, the final logs from HaplotypeCaller look like this:. ```; ./gatk HaplotypeCaller ...; ...; 15:56:23.205 INFO ProgressMeter - Pf3D7_13_v3:2603234 100.5 114070 1134.5; 15:56:33.443 INFO ProgressMeter - Pf3D7_13_v3:2661462 100.7 114420 1136.1; Dangling End recovery killed because of a loop (getReferencePathForwardFromKmer); 15:56:43.998 INFO ProgressMeter - Pf3D7_13_v3:2730055 100.9 114840 1138.3; 15:56:59.911 INFO ProgressMeter - Pf3D7_13_v3:2798281 101.2 115210 1139.0; 15:59:27.062 INFO ProgressMeter - Pf3D7_13_v3:2861780 103.6 115460 1114.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8440
https://github.com/broadinstitute/gatk/issues/8440:4956,Availability,error,error,4956," org.broadinstitute.hellbender.Main.mainEntry(Main.java:209); at org.broadinstitute.hellbender.Main.main(Main.java:306). real 481m24.418s; user 581m54.752s; sys 2m49.965s. ```. This run did not complete successfully - the Exception caused it to fail prematurely. . Previously I had seen HaplotypeCaller run out of memory and fail in almost as much time, so I think this and the OOM error are related. The only difference in invocation was that with the OOM failure, I was running with the default for `--max-reads-per-alignment-start` (`50`). This also works just fine with that setting at 15. The failure seems to occur around the same place in the data each time (the end of `chr13`). At that point in the data, there is a very large pileup which is probably instigating this. Additionally, if I remove the `--linked-de-bruijn-graph` argument, this runs just fine with the default setting of `--max-reads-per-alignment-start`. I have a minimally reproductive dataset that I can share which reproduces the OOM error for sure (I'm 99% sure it reproduces this one as well). For the OOM failures, the final logs from HaplotypeCaller look like this:. ```; ./gatk HaplotypeCaller ...; ...; 15:56:23.205 INFO ProgressMeter - Pf3D7_13_v3:2603234 100.5 114070 1134.5; 15:56:33.443 INFO ProgressMeter - Pf3D7_13_v3:2661462 100.7 114420 1136.1; Dangling End recovery killed because of a loop (getReferencePathForwardFromKmer); 15:56:43.998 INFO ProgressMeter - Pf3D7_13_v3:2730055 100.9 114840 1138.3; 15:56:59.911 INFO ProgressMeter - Pf3D7_13_v3:2798281 101.2 115210 1139.0; 15:59:27.062 INFO ProgressMeter - Pf3D7_13_v3:2861780 103.6 115460 1114.4; Dangling End recovery killed because of a loop (getReferencePathForwardFromKmer); Dangling End recovery killed because of a loop (getReferencePathForwardFromKmer); 15:59:37.457 INFO ProgressMeter - Pf3D7_13_v3:2869697 103.8 115500 1112.9. real 671m24.770s; user 777m30.923s; sys 6m13.682s. $ echo $?; 247; ```. Here is my command-line invocation:; ```; ./ga",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8440
https://github.com/broadinstitute/gatk/issues/8440:5030,Availability,failure,failures,5030,".java:306). real 481m24.418s; user 581m54.752s; sys 2m49.965s. ```. This run did not complete successfully - the Exception caused it to fail prematurely. . Previously I had seen HaplotypeCaller run out of memory and fail in almost as much time, so I think this and the OOM error are related. The only difference in invocation was that with the OOM failure, I was running with the default for `--max-reads-per-alignment-start` (`50`). This also works just fine with that setting at 15. The failure seems to occur around the same place in the data each time (the end of `chr13`). At that point in the data, there is a very large pileup which is probably instigating this. Additionally, if I remove the `--linked-de-bruijn-graph` argument, this runs just fine with the default setting of `--max-reads-per-alignment-start`. I have a minimally reproductive dataset that I can share which reproduces the OOM error for sure (I'm 99% sure it reproduces this one as well). For the OOM failures, the final logs from HaplotypeCaller look like this:. ```; ./gatk HaplotypeCaller ...; ...; 15:56:23.205 INFO ProgressMeter - Pf3D7_13_v3:2603234 100.5 114070 1134.5; 15:56:33.443 INFO ProgressMeter - Pf3D7_13_v3:2661462 100.7 114420 1136.1; Dangling End recovery killed because of a loop (getReferencePathForwardFromKmer); 15:56:43.998 INFO ProgressMeter - Pf3D7_13_v3:2730055 100.9 114840 1138.3; 15:56:59.911 INFO ProgressMeter - Pf3D7_13_v3:2798281 101.2 115210 1139.0; 15:59:27.062 INFO ProgressMeter - Pf3D7_13_v3:2861780 103.6 115460 1114.4; Dangling End recovery killed because of a loop (getReferencePathForwardFromKmer); Dangling End recovery killed because of a loop (getReferencePathForwardFromKmer); 15:59:37.457 INFO ProgressMeter - Pf3D7_13_v3:2869697 103.8 115500 1112.9. real 671m24.770s; user 777m30.923s; sys 6m13.682s. $ echo $?; 247; ```. Here is my command-line invocation:; ```; ./gatk --java-options ""-Xmx100000m -Xms25000m"" \; HaplotypeCaller \; -R /juffowup2/malaria/references/PlasmoDB-61_",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8440
https://github.com/broadinstitute/gatk/issues/8440:5294,Availability,recover,recovery,5294,"M error are related. The only difference in invocation was that with the OOM failure, I was running with the default for `--max-reads-per-alignment-start` (`50`). This also works just fine with that setting at 15. The failure seems to occur around the same place in the data each time (the end of `chr13`). At that point in the data, there is a very large pileup which is probably instigating this. Additionally, if I remove the `--linked-de-bruijn-graph` argument, this runs just fine with the default setting of `--max-reads-per-alignment-start`. I have a minimally reproductive dataset that I can share which reproduces the OOM error for sure (I'm 99% sure it reproduces this one as well). For the OOM failures, the final logs from HaplotypeCaller look like this:. ```; ./gatk HaplotypeCaller ...; ...; 15:56:23.205 INFO ProgressMeter - Pf3D7_13_v3:2603234 100.5 114070 1134.5; 15:56:33.443 INFO ProgressMeter - Pf3D7_13_v3:2661462 100.7 114420 1136.1; Dangling End recovery killed because of a loop (getReferencePathForwardFromKmer); 15:56:43.998 INFO ProgressMeter - Pf3D7_13_v3:2730055 100.9 114840 1138.3; 15:56:59.911 INFO ProgressMeter - Pf3D7_13_v3:2798281 101.2 115210 1139.0; 15:59:27.062 INFO ProgressMeter - Pf3D7_13_v3:2861780 103.6 115460 1114.4; Dangling End recovery killed because of a loop (getReferencePathForwardFromKmer); Dangling End recovery killed because of a loop (getReferencePathForwardFromKmer); 15:59:37.457 INFO ProgressMeter - Pf3D7_13_v3:2869697 103.8 115500 1112.9. real 671m24.770s; user 777m30.923s; sys 6m13.682s. $ echo $?; 247; ```. Here is my command-line invocation:; ```; ./gatk --java-options ""-Xmx100000m -Xms25000m"" \; HaplotypeCaller \; -R /juffowup2/malaria/references/PlasmoDB-61_Pfalciparum3D7_Genome.fasta \; -I ${WORKING_DIR}/fixed_bam/PG0004-CW.aligned.merged.markDuplicates.sorted.BQSR.bam \; -O ${WORKING_DIR}/PG0004-CW.haplotype_caller.fixed_bam_file.with_pileup.g.vcf.gz \; --bam-output ${WORKING_DIR}/PG0004-CW.haplotype_caller.fixed_bam_fil",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8440
https://github.com/broadinstitute/gatk/issues/8440:5601,Availability,recover,recovery,5601,"y large pileup which is probably instigating this. Additionally, if I remove the `--linked-de-bruijn-graph` argument, this runs just fine with the default setting of `--max-reads-per-alignment-start`. I have a minimally reproductive dataset that I can share which reproduces the OOM error for sure (I'm 99% sure it reproduces this one as well). For the OOM failures, the final logs from HaplotypeCaller look like this:. ```; ./gatk HaplotypeCaller ...; ...; 15:56:23.205 INFO ProgressMeter - Pf3D7_13_v3:2603234 100.5 114070 1134.5; 15:56:33.443 INFO ProgressMeter - Pf3D7_13_v3:2661462 100.7 114420 1136.1; Dangling End recovery killed because of a loop (getReferencePathForwardFromKmer); 15:56:43.998 INFO ProgressMeter - Pf3D7_13_v3:2730055 100.9 114840 1138.3; 15:56:59.911 INFO ProgressMeter - Pf3D7_13_v3:2798281 101.2 115210 1139.0; 15:59:27.062 INFO ProgressMeter - Pf3D7_13_v3:2861780 103.6 115460 1114.4; Dangling End recovery killed because of a loop (getReferencePathForwardFromKmer); Dangling End recovery killed because of a loop (getReferencePathForwardFromKmer); 15:59:37.457 INFO ProgressMeter - Pf3D7_13_v3:2869697 103.8 115500 1112.9. real 671m24.770s; user 777m30.923s; sys 6m13.682s. $ echo $?; 247; ```. Here is my command-line invocation:; ```; ./gatk --java-options ""-Xmx100000m -Xms25000m"" \; HaplotypeCaller \; -R /juffowup2/malaria/references/PlasmoDB-61_Pfalciparum3D7_Genome.fasta \; -I ${WORKING_DIR}/fixed_bam/PG0004-CW.aligned.merged.markDuplicates.sorted.BQSR.bam \; -O ${WORKING_DIR}/PG0004-CW.haplotype_caller.fixed_bam_file.with_pileup.g.vcf.gz \; --bam-output ${WORKING_DIR}/PG0004-CW.haplotype_caller.fixed_bam_file.with_pileup.bamout.bam \; -contamination 0 \; --sample-ploidy 2 \; --linked-de-bruijn-graph \; --pileup-detection true \; --pileup-detection-enable-indel-pileup-calling true \; --max-reads-per-alignment-start 20 \; --annotate-with-num-discovered-alleles \; -GQB 10 -GQB 20 -GQB 30 -GQB 40 -GQB 50 -GQB 60 -GQB 70 -GQB 80 -GQB 90 \; -G StandardAnn",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8440
https://github.com/broadinstitute/gatk/issues/8440:5683,Availability,recover,recovery,5683,"y large pileup which is probably instigating this. Additionally, if I remove the `--linked-de-bruijn-graph` argument, this runs just fine with the default setting of `--max-reads-per-alignment-start`. I have a minimally reproductive dataset that I can share which reproduces the OOM error for sure (I'm 99% sure it reproduces this one as well). For the OOM failures, the final logs from HaplotypeCaller look like this:. ```; ./gatk HaplotypeCaller ...; ...; 15:56:23.205 INFO ProgressMeter - Pf3D7_13_v3:2603234 100.5 114070 1134.5; 15:56:33.443 INFO ProgressMeter - Pf3D7_13_v3:2661462 100.7 114420 1136.1; Dangling End recovery killed because of a loop (getReferencePathForwardFromKmer); 15:56:43.998 INFO ProgressMeter - Pf3D7_13_v3:2730055 100.9 114840 1138.3; 15:56:59.911 INFO ProgressMeter - Pf3D7_13_v3:2798281 101.2 115210 1139.0; 15:59:27.062 INFO ProgressMeter - Pf3D7_13_v3:2861780 103.6 115460 1114.4; Dangling End recovery killed because of a loop (getReferencePathForwardFromKmer); Dangling End recovery killed because of a loop (getReferencePathForwardFromKmer); 15:59:37.457 INFO ProgressMeter - Pf3D7_13_v3:2869697 103.8 115500 1112.9. real 671m24.770s; user 777m30.923s; sys 6m13.682s. $ echo $?; 247; ```. Here is my command-line invocation:; ```; ./gatk --java-options ""-Xmx100000m -Xms25000m"" \; HaplotypeCaller \; -R /juffowup2/malaria/references/PlasmoDB-61_Pfalciparum3D7_Genome.fasta \; -I ${WORKING_DIR}/fixed_bam/PG0004-CW.aligned.merged.markDuplicates.sorted.BQSR.bam \; -O ${WORKING_DIR}/PG0004-CW.haplotype_caller.fixed_bam_file.with_pileup.g.vcf.gz \; --bam-output ${WORKING_DIR}/PG0004-CW.haplotype_caller.fixed_bam_file.with_pileup.bamout.bam \; -contamination 0 \; --sample-ploidy 2 \; --linked-de-bruijn-graph \; --pileup-detection true \; --pileup-detection-enable-indel-pileup-calling true \; --max-reads-per-alignment-start 20 \; --annotate-with-num-discovered-alleles \; -GQB 10 -GQB 20 -GQB 30 -GQB 40 -GQB 50 -GQB 60 -GQB 70 -GQB 80 -GQB 90 \; -G StandardAnn",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8440
https://github.com/broadinstitute/gatk/issues/8440:5880,Availability,echo,echo,5880,"emove the `--linked-de-bruijn-graph` argument, this runs just fine with the default setting of `--max-reads-per-alignment-start`. I have a minimally reproductive dataset that I can share which reproduces the OOM error for sure (I'm 99% sure it reproduces this one as well). For the OOM failures, the final logs from HaplotypeCaller look like this:. ```; ./gatk HaplotypeCaller ...; ...; 15:56:23.205 INFO ProgressMeter - Pf3D7_13_v3:2603234 100.5 114070 1134.5; 15:56:33.443 INFO ProgressMeter - Pf3D7_13_v3:2661462 100.7 114420 1136.1; Dangling End recovery killed because of a loop (getReferencePathForwardFromKmer); 15:56:43.998 INFO ProgressMeter - Pf3D7_13_v3:2730055 100.9 114840 1138.3; 15:56:59.911 INFO ProgressMeter - Pf3D7_13_v3:2798281 101.2 115210 1139.0; 15:59:27.062 INFO ProgressMeter - Pf3D7_13_v3:2861780 103.6 115460 1114.4; Dangling End recovery killed because of a loop (getReferencePathForwardFromKmer); Dangling End recovery killed because of a loop (getReferencePathForwardFromKmer); 15:59:37.457 INFO ProgressMeter - Pf3D7_13_v3:2869697 103.8 115500 1112.9. real 671m24.770s; user 777m30.923s; sys 6m13.682s. $ echo $?; 247; ```. Here is my command-line invocation:; ```; ./gatk --java-options ""-Xmx100000m -Xms25000m"" \; HaplotypeCaller \; -R /juffowup2/malaria/references/PlasmoDB-61_Pfalciparum3D7_Genome.fasta \; -I ${WORKING_DIR}/fixed_bam/PG0004-CW.aligned.merged.markDuplicates.sorted.BQSR.bam \; -O ${WORKING_DIR}/PG0004-CW.haplotype_caller.fixed_bam_file.with_pileup.g.vcf.gz \; --bam-output ${WORKING_DIR}/PG0004-CW.haplotype_caller.fixed_bam_file.with_pileup.bamout.bam \; -contamination 0 \; --sample-ploidy 2 \; --linked-de-bruijn-graph \; --pileup-detection true \; --pileup-detection-enable-indel-pileup-calling true \; --max-reads-per-alignment-start 20 \; --annotate-with-num-discovered-alleles \; -GQB 10 -GQB 20 -GQB 30 -GQB 40 -GQB 50 -GQB 60 -GQB 70 -GQB 80 -GQB 90 \; -G StandardAnnotation -G StandardHCAnnotation \; -ERC GVCF \; --verbosity INFO \; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8440
https://github.com/broadinstitute/gatk/issues/8440:152,Deployability,install,install,152,I'm working on some Plasmodium falciparum callsets in GATK and I have come across a curious error:. ```; Using GATK wrapper script /juffowup/gatk/build/install/gatk/bin/gatk; Running:; /juffowup/gatk/build/install/gatk/bin/gatk HaplotypeCaller -R /juffowup2/malaria/references/PlasmoDB-61_Pfalciparum3D7_Genome.fasta -I /juffowup2/malaria/haplotypecaller_arg_testing/fixed_bam/PG0004-CW.aligned.merged.markDuplicates.sorted.BQSR.bam -O /juffowup2/malaria/haplotypecaller_arg_testing/PG0004-CW.haplotype_caller.fixed_bam_file.with_pileup.g.vcf.gz --bam-output /juffowup2/malaria/haplotypecaller_arg_testing/PG0004-CW.haplotype_caller.fixed_bam_file.with_pileup.bamout.bam -contamination 0 --sample-ploidy 2 --linked-de-bruijn-graph --pileup-detection true --pileup-detection-enable-indel-pileup-calling true --max-reads-per-alignment-start 20 --annotate-with-num-discovered-alleles -GQB 10 -GQB 20 -GQB 30 -GQB 40 -GQB 50 -GQB 60 -GQB 70 -GQB 80 -GQB 90 -G StandardAnnotation -G StandardHCAnnotation -ERC GVCF --verbosity INFO; 14:14:15.323 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 14:14:15.328 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardHCAnnotation) is enabled for this tool by default; 14:14:15.388 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/juffowup/gatk/build/install/gatk/lib/gkl-0.8.11.jar!/com/intel/gkl/native/libgkl_compression.so; 14:14:15.435 INFO HaplotypeCaller - ------------------------------------------------------------; 14:14:15.439 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.4.0.0-44-g1529aa1-SNAPSHOT; 14:14:15.439 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:14:15.439 INFO HaplotypeCaller - Executing as jonn@dsde-methods-jonn-juffowup on Linux v5.4.0-1104-gcp amd64; 14:14:15.439 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8440
https://github.com/broadinstitute/gatk/issues/8440:206,Deployability,install,install,206,I'm working on some Plasmodium falciparum callsets in GATK and I have come across a curious error:. ```; Using GATK wrapper script /juffowup/gatk/build/install/gatk/bin/gatk; Running:; /juffowup/gatk/build/install/gatk/bin/gatk HaplotypeCaller -R /juffowup2/malaria/references/PlasmoDB-61_Pfalciparum3D7_Genome.fasta -I /juffowup2/malaria/haplotypecaller_arg_testing/fixed_bam/PG0004-CW.aligned.merged.markDuplicates.sorted.BQSR.bam -O /juffowup2/malaria/haplotypecaller_arg_testing/PG0004-CW.haplotype_caller.fixed_bam_file.with_pileup.g.vcf.gz --bam-output /juffowup2/malaria/haplotypecaller_arg_testing/PG0004-CW.haplotype_caller.fixed_bam_file.with_pileup.bamout.bam -contamination 0 --sample-ploidy 2 --linked-de-bruijn-graph --pileup-detection true --pileup-detection-enable-indel-pileup-calling true --max-reads-per-alignment-start 20 --annotate-with-num-discovered-alleles -GQB 10 -GQB 20 -GQB 30 -GQB 40 -GQB 50 -GQB 60 -GQB 70 -GQB 80 -GQB 90 -G StandardAnnotation -G StandardHCAnnotation -ERC GVCF --verbosity INFO; 14:14:15.323 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 14:14:15.328 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardHCAnnotation) is enabled for this tool by default; 14:14:15.388 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/juffowup/gatk/build/install/gatk/lib/gkl-0.8.11.jar!/com/intel/gkl/native/libgkl_compression.so; 14:14:15.435 INFO HaplotypeCaller - ------------------------------------------------------------; 14:14:15.439 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.4.0.0-44-g1529aa1-SNAPSHOT; 14:14:15.439 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:14:15.439 INFO HaplotypeCaller - Executing as jonn@dsde-methods-jonn-juffowup on Linux v5.4.0-1104-gcp amd64; 14:14:15.439 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8440
https://github.com/broadinstitute/gatk/issues/8440:1422,Deployability,install,install,1422,"es.sorted.BQSR.bam -O /juffowup2/malaria/haplotypecaller_arg_testing/PG0004-CW.haplotype_caller.fixed_bam_file.with_pileup.g.vcf.gz --bam-output /juffowup2/malaria/haplotypecaller_arg_testing/PG0004-CW.haplotype_caller.fixed_bam_file.with_pileup.bamout.bam -contamination 0 --sample-ploidy 2 --linked-de-bruijn-graph --pileup-detection true --pileup-detection-enable-indel-pileup-calling true --max-reads-per-alignment-start 20 --annotate-with-num-discovered-alleles -GQB 10 -GQB 20 -GQB 30 -GQB 40 -GQB 50 -GQB 60 -GQB 70 -GQB 80 -GQB 90 -G StandardAnnotation -G StandardHCAnnotation -ERC GVCF --verbosity INFO; 14:14:15.323 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 14:14:15.328 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardHCAnnotation) is enabled for this tool by default; 14:14:15.388 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/juffowup/gatk/build/install/gatk/lib/gkl-0.8.11.jar!/com/intel/gkl/native/libgkl_compression.so; 14:14:15.435 INFO HaplotypeCaller - ------------------------------------------------------------; 14:14:15.439 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.4.0.0-44-g1529aa1-SNAPSHOT; 14:14:15.439 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:14:15.439 INFO HaplotypeCaller - Executing as jonn@dsde-methods-jonn-juffowup on Linux v5.4.0-1104-gcp amd64; 14:14:15.439 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v17.0.7+7; 14:14:15.440 INFO HaplotypeCaller - Start Date/Time: July 26, 2023 at 2:14:15 PM UTC; ...; 22:15:34.977 INFO HaplotypeCaller - Shutting down engine; [July 26, 2023 at 10:15:34 PM UTC] org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller done. Elapsed time: 481.33 minutes.; Runtime.totalMemory()=47982837760; java.lang.NegativeArraySizeException: -896617256; at org.broadinstitut",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8440
https://github.com/broadinstitute/gatk/issues/8440:116,Integrability,wrap,wrapper,116,I'm working on some Plasmodium falciparum callsets in GATK and I have come across a curious error:. ```; Using GATK wrapper script /juffowup/gatk/build/install/gatk/bin/gatk; Running:; /juffowup/gatk/build/install/gatk/bin/gatk HaplotypeCaller -R /juffowup2/malaria/references/PlasmoDB-61_Pfalciparum3D7_Genome.fasta -I /juffowup2/malaria/haplotypecaller_arg_testing/fixed_bam/PG0004-CW.aligned.merged.markDuplicates.sorted.BQSR.bam -O /juffowup2/malaria/haplotypecaller_arg_testing/PG0004-CW.haplotype_caller.fixed_bam_file.with_pileup.g.vcf.gz --bam-output /juffowup2/malaria/haplotypecaller_arg_testing/PG0004-CW.haplotype_caller.fixed_bam_file.with_pileup.bamout.bam -contamination 0 --sample-ploidy 2 --linked-de-bruijn-graph --pileup-detection true --pileup-detection-enable-indel-pileup-calling true --max-reads-per-alignment-start 20 --annotate-with-num-discovered-alleles -GQB 10 -GQB 20 -GQB 30 -GQB 40 -GQB 50 -GQB 60 -GQB 70 -GQB 80 -GQB 90 -G StandardAnnotation -G StandardHCAnnotation -ERC GVCF --verbosity INFO; 14:14:15.323 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 14:14:15.328 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardHCAnnotation) is enabled for this tool by default; 14:14:15.388 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/juffowup/gatk/build/install/gatk/lib/gkl-0.8.11.jar!/com/intel/gkl/native/libgkl_compression.so; 14:14:15.435 INFO HaplotypeCaller - ------------------------------------------------------------; 14:14:15.439 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.4.0.0-44-g1529aa1-SNAPSHOT; 14:14:15.439 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:14:15.439 INFO HaplotypeCaller - Executing as jonn@dsde-methods-jonn-juffowup on Linux v5.4.0-1104-gcp amd64; 14:14:15.439 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8440
https://github.com/broadinstitute/gatk/issues/8440:1357,Performance,Load,Loading,1357,"arg_testing/fixed_bam/PG0004-CW.aligned.merged.markDuplicates.sorted.BQSR.bam -O /juffowup2/malaria/haplotypecaller_arg_testing/PG0004-CW.haplotype_caller.fixed_bam_file.with_pileup.g.vcf.gz --bam-output /juffowup2/malaria/haplotypecaller_arg_testing/PG0004-CW.haplotype_caller.fixed_bam_file.with_pileup.bamout.bam -contamination 0 --sample-ploidy 2 --linked-de-bruijn-graph --pileup-detection true --pileup-detection-enable-indel-pileup-calling true --max-reads-per-alignment-start 20 --annotate-with-num-discovered-alleles -GQB 10 -GQB 20 -GQB 30 -GQB 40 -GQB 50 -GQB 60 -GQB 70 -GQB 80 -GQB 90 -G StandardAnnotation -G StandardHCAnnotation -ERC GVCF --verbosity INFO; 14:14:15.323 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 14:14:15.328 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardHCAnnotation) is enabled for this tool by default; 14:14:15.388 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/juffowup/gatk/build/install/gatk/lib/gkl-0.8.11.jar!/com/intel/gkl/native/libgkl_compression.so; 14:14:15.435 INFO HaplotypeCaller - ------------------------------------------------------------; 14:14:15.439 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.4.0.0-44-g1529aa1-SNAPSHOT; 14:14:15.439 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:14:15.439 INFO HaplotypeCaller - Executing as jonn@dsde-methods-jonn-juffowup on Linux v5.4.0-1104-gcp amd64; 14:14:15.439 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v17.0.7+7; 14:14:15.440 INFO HaplotypeCaller - Start Date/Time: July 26, 2023 at 2:14:15 PM UTC; ...; 22:15:34.977 INFO HaplotypeCaller - Shutting down engine; [July 26, 2023 at 10:15:34 PM UTC] org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller done. Elapsed time: 481.33 minutes.; Runtime.totalMemory()=47982837760; java.lang.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8440
https://github.com/broadinstitute/gatk/issues/8440:740,Safety,detect,detection,740,I'm working on some Plasmodium falciparum callsets in GATK and I have come across a curious error:. ```; Using GATK wrapper script /juffowup/gatk/build/install/gatk/bin/gatk; Running:; /juffowup/gatk/build/install/gatk/bin/gatk HaplotypeCaller -R /juffowup2/malaria/references/PlasmoDB-61_Pfalciparum3D7_Genome.fasta -I /juffowup2/malaria/haplotypecaller_arg_testing/fixed_bam/PG0004-CW.aligned.merged.markDuplicates.sorted.BQSR.bam -O /juffowup2/malaria/haplotypecaller_arg_testing/PG0004-CW.haplotype_caller.fixed_bam_file.with_pileup.g.vcf.gz --bam-output /juffowup2/malaria/haplotypecaller_arg_testing/PG0004-CW.haplotype_caller.fixed_bam_file.with_pileup.bamout.bam -contamination 0 --sample-ploidy 2 --linked-de-bruijn-graph --pileup-detection true --pileup-detection-enable-indel-pileup-calling true --max-reads-per-alignment-start 20 --annotate-with-num-discovered-alleles -GQB 10 -GQB 20 -GQB 30 -GQB 40 -GQB 50 -GQB 60 -GQB 70 -GQB 80 -GQB 90 -G StandardAnnotation -G StandardHCAnnotation -ERC GVCF --verbosity INFO; 14:14:15.323 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 14:14:15.328 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardHCAnnotation) is enabled for this tool by default; 14:14:15.388 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/juffowup/gatk/build/install/gatk/lib/gkl-0.8.11.jar!/com/intel/gkl/native/libgkl_compression.so; 14:14:15.435 INFO HaplotypeCaller - ------------------------------------------------------------; 14:14:15.439 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.4.0.0-44-g1529aa1-SNAPSHOT; 14:14:15.439 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:14:15.439 INFO HaplotypeCaller - Executing as jonn@dsde-methods-jonn-juffowup on Linux v5.4.0-1104-gcp amd64; 14:14:15.439 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8440
https://github.com/broadinstitute/gatk/issues/8440:764,Safety,detect,detection-enable-indel-pileup-calling,764,I'm working on some Plasmodium falciparum callsets in GATK and I have come across a curious error:. ```; Using GATK wrapper script /juffowup/gatk/build/install/gatk/bin/gatk; Running:; /juffowup/gatk/build/install/gatk/bin/gatk HaplotypeCaller -R /juffowup2/malaria/references/PlasmoDB-61_Pfalciparum3D7_Genome.fasta -I /juffowup2/malaria/haplotypecaller_arg_testing/fixed_bam/PG0004-CW.aligned.merged.markDuplicates.sorted.BQSR.bam -O /juffowup2/malaria/haplotypecaller_arg_testing/PG0004-CW.haplotype_caller.fixed_bam_file.with_pileup.g.vcf.gz --bam-output /juffowup2/malaria/haplotypecaller_arg_testing/PG0004-CW.haplotype_caller.fixed_bam_file.with_pileup.bamout.bam -contamination 0 --sample-ploidy 2 --linked-de-bruijn-graph --pileup-detection true --pileup-detection-enable-indel-pileup-calling true --max-reads-per-alignment-start 20 --annotate-with-num-discovered-alleles -GQB 10 -GQB 20 -GQB 30 -GQB 40 -GQB 50 -GQB 60 -GQB 70 -GQB 80 -GQB 90 -G StandardAnnotation -G StandardHCAnnotation -ERC GVCF --verbosity INFO; 14:14:15.323 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 14:14:15.328 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardHCAnnotation) is enabled for this tool by default; 14:14:15.388 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/juffowup/gatk/build/install/gatk/lib/gkl-0.8.11.jar!/com/intel/gkl/native/libgkl_compression.so; 14:14:15.435 INFO HaplotypeCaller - ------------------------------------------------------------; 14:14:15.439 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.4.0.0-44-g1529aa1-SNAPSHOT; 14:14:15.439 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:14:15.439 INFO HaplotypeCaller - Executing as jonn@dsde-methods-jonn-juffowup on Linux v5.4.0-1104-gcp amd64; 14:14:15.439 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8440
https://github.com/broadinstitute/gatk/issues/8440:1078,Safety,Redund,Redundant,1078,"ng GATK wrapper script /juffowup/gatk/build/install/gatk/bin/gatk; Running:; /juffowup/gatk/build/install/gatk/bin/gatk HaplotypeCaller -R /juffowup2/malaria/references/PlasmoDB-61_Pfalciparum3D7_Genome.fasta -I /juffowup2/malaria/haplotypecaller_arg_testing/fixed_bam/PG0004-CW.aligned.merged.markDuplicates.sorted.BQSR.bam -O /juffowup2/malaria/haplotypecaller_arg_testing/PG0004-CW.haplotype_caller.fixed_bam_file.with_pileup.g.vcf.gz --bam-output /juffowup2/malaria/haplotypecaller_arg_testing/PG0004-CW.haplotype_caller.fixed_bam_file.with_pileup.bamout.bam -contamination 0 --sample-ploidy 2 --linked-de-bruijn-graph --pileup-detection true --pileup-detection-enable-indel-pileup-calling true --max-reads-per-alignment-start 20 --annotate-with-num-discovered-alleles -GQB 10 -GQB 20 -GQB 30 -GQB 40 -GQB 50 -GQB 60 -GQB 70 -GQB 80 -GQB 90 -G StandardAnnotation -G StandardHCAnnotation -ERC GVCF --verbosity INFO; 14:14:15.323 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 14:14:15.328 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardHCAnnotation) is enabled for this tool by default; 14:14:15.388 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/juffowup/gatk/build/install/gatk/lib/gkl-0.8.11.jar!/com/intel/gkl/native/libgkl_compression.so; 14:14:15.435 INFO HaplotypeCaller - ------------------------------------------------------------; 14:14:15.439 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.4.0.0-44-g1529aa1-SNAPSHOT; 14:14:15.439 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:14:15.439 INFO HaplotypeCaller - Executing as jonn@dsde-methods-jonn-juffowup on Linux v5.4.0-1104-gcp amd64; 14:14:15.439 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v17.0.7+7; 14:14:15.440 INFO HaplotypeCaller - Start Date/Time: July 26, 2023 at 2:14:15 PM UTC; ...; 22:1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8440
https://github.com/broadinstitute/gatk/issues/8440:1222,Safety,Redund,Redundant,1222,"wup2/malaria/references/PlasmoDB-61_Pfalciparum3D7_Genome.fasta -I /juffowup2/malaria/haplotypecaller_arg_testing/fixed_bam/PG0004-CW.aligned.merged.markDuplicates.sorted.BQSR.bam -O /juffowup2/malaria/haplotypecaller_arg_testing/PG0004-CW.haplotype_caller.fixed_bam_file.with_pileup.g.vcf.gz --bam-output /juffowup2/malaria/haplotypecaller_arg_testing/PG0004-CW.haplotype_caller.fixed_bam_file.with_pileup.bamout.bam -contamination 0 --sample-ploidy 2 --linked-de-bruijn-graph --pileup-detection true --pileup-detection-enable-indel-pileup-calling true --max-reads-per-alignment-start 20 --annotate-with-num-discovered-alleles -GQB 10 -GQB 20 -GQB 30 -GQB 40 -GQB 50 -GQB 60 -GQB 70 -GQB 80 -GQB 90 -G StandardAnnotation -G StandardHCAnnotation -ERC GVCF --verbosity INFO; 14:14:15.323 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 14:14:15.328 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardHCAnnotation) is enabled for this tool by default; 14:14:15.388 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/juffowup/gatk/build/install/gatk/lib/gkl-0.8.11.jar!/com/intel/gkl/native/libgkl_compression.so; 14:14:15.435 INFO HaplotypeCaller - ------------------------------------------------------------; 14:14:15.439 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.4.0.0-44-g1529aa1-SNAPSHOT; 14:14:15.439 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:14:15.439 INFO HaplotypeCaller - Executing as jonn@dsde-methods-jonn-juffowup on Linux v5.4.0-1104-gcp amd64; 14:14:15.439 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v17.0.7+7; 14:14:15.440 INFO HaplotypeCaller - Start Date/Time: July 26, 2023 at 2:14:15 PM UTC; ...; 22:15:34.977 INFO HaplotypeCaller - Shutting down engine; [July 26, 2023 at 10:15:34 PM UTC] org.broadinstitute.hellbender.tools.walkers.haplotypecal",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8440
https://github.com/broadinstitute/gatk/issues/8440:5294,Safety,recover,recovery,5294,"M error are related. The only difference in invocation was that with the OOM failure, I was running with the default for `--max-reads-per-alignment-start` (`50`). This also works just fine with that setting at 15. The failure seems to occur around the same place in the data each time (the end of `chr13`). At that point in the data, there is a very large pileup which is probably instigating this. Additionally, if I remove the `--linked-de-bruijn-graph` argument, this runs just fine with the default setting of `--max-reads-per-alignment-start`. I have a minimally reproductive dataset that I can share which reproduces the OOM error for sure (I'm 99% sure it reproduces this one as well). For the OOM failures, the final logs from HaplotypeCaller look like this:. ```; ./gatk HaplotypeCaller ...; ...; 15:56:23.205 INFO ProgressMeter - Pf3D7_13_v3:2603234 100.5 114070 1134.5; 15:56:33.443 INFO ProgressMeter - Pf3D7_13_v3:2661462 100.7 114420 1136.1; Dangling End recovery killed because of a loop (getReferencePathForwardFromKmer); 15:56:43.998 INFO ProgressMeter - Pf3D7_13_v3:2730055 100.9 114840 1138.3; 15:56:59.911 INFO ProgressMeter - Pf3D7_13_v3:2798281 101.2 115210 1139.0; 15:59:27.062 INFO ProgressMeter - Pf3D7_13_v3:2861780 103.6 115460 1114.4; Dangling End recovery killed because of a loop (getReferencePathForwardFromKmer); Dangling End recovery killed because of a loop (getReferencePathForwardFromKmer); 15:59:37.457 INFO ProgressMeter - Pf3D7_13_v3:2869697 103.8 115500 1112.9. real 671m24.770s; user 777m30.923s; sys 6m13.682s. $ echo $?; 247; ```. Here is my command-line invocation:; ```; ./gatk --java-options ""-Xmx100000m -Xms25000m"" \; HaplotypeCaller \; -R /juffowup2/malaria/references/PlasmoDB-61_Pfalciparum3D7_Genome.fasta \; -I ${WORKING_DIR}/fixed_bam/PG0004-CW.aligned.merged.markDuplicates.sorted.BQSR.bam \; -O ${WORKING_DIR}/PG0004-CW.haplotype_caller.fixed_bam_file.with_pileup.g.vcf.gz \; --bam-output ${WORKING_DIR}/PG0004-CW.haplotype_caller.fixed_bam_fil",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8440
https://github.com/broadinstitute/gatk/issues/8440:5601,Safety,recover,recovery,5601,"y large pileup which is probably instigating this. Additionally, if I remove the `--linked-de-bruijn-graph` argument, this runs just fine with the default setting of `--max-reads-per-alignment-start`. I have a minimally reproductive dataset that I can share which reproduces the OOM error for sure (I'm 99% sure it reproduces this one as well). For the OOM failures, the final logs from HaplotypeCaller look like this:. ```; ./gatk HaplotypeCaller ...; ...; 15:56:23.205 INFO ProgressMeter - Pf3D7_13_v3:2603234 100.5 114070 1134.5; 15:56:33.443 INFO ProgressMeter - Pf3D7_13_v3:2661462 100.7 114420 1136.1; Dangling End recovery killed because of a loop (getReferencePathForwardFromKmer); 15:56:43.998 INFO ProgressMeter - Pf3D7_13_v3:2730055 100.9 114840 1138.3; 15:56:59.911 INFO ProgressMeter - Pf3D7_13_v3:2798281 101.2 115210 1139.0; 15:59:27.062 INFO ProgressMeter - Pf3D7_13_v3:2861780 103.6 115460 1114.4; Dangling End recovery killed because of a loop (getReferencePathForwardFromKmer); Dangling End recovery killed because of a loop (getReferencePathForwardFromKmer); 15:59:37.457 INFO ProgressMeter - Pf3D7_13_v3:2869697 103.8 115500 1112.9. real 671m24.770s; user 777m30.923s; sys 6m13.682s. $ echo $?; 247; ```. Here is my command-line invocation:; ```; ./gatk --java-options ""-Xmx100000m -Xms25000m"" \; HaplotypeCaller \; -R /juffowup2/malaria/references/PlasmoDB-61_Pfalciparum3D7_Genome.fasta \; -I ${WORKING_DIR}/fixed_bam/PG0004-CW.aligned.merged.markDuplicates.sorted.BQSR.bam \; -O ${WORKING_DIR}/PG0004-CW.haplotype_caller.fixed_bam_file.with_pileup.g.vcf.gz \; --bam-output ${WORKING_DIR}/PG0004-CW.haplotype_caller.fixed_bam_file.with_pileup.bamout.bam \; -contamination 0 \; --sample-ploidy 2 \; --linked-de-bruijn-graph \; --pileup-detection true \; --pileup-detection-enable-indel-pileup-calling true \; --max-reads-per-alignment-start 20 \; --annotate-with-num-discovered-alleles \; -GQB 10 -GQB 20 -GQB 30 -GQB 40 -GQB 50 -GQB 60 -GQB 70 -GQB 80 -GQB 90 \; -G StandardAnn",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8440
https://github.com/broadinstitute/gatk/issues/8440:5683,Safety,recover,recovery,5683,"y large pileup which is probably instigating this. Additionally, if I remove the `--linked-de-bruijn-graph` argument, this runs just fine with the default setting of `--max-reads-per-alignment-start`. I have a minimally reproductive dataset that I can share which reproduces the OOM error for sure (I'm 99% sure it reproduces this one as well). For the OOM failures, the final logs from HaplotypeCaller look like this:. ```; ./gatk HaplotypeCaller ...; ...; 15:56:23.205 INFO ProgressMeter - Pf3D7_13_v3:2603234 100.5 114070 1134.5; 15:56:33.443 INFO ProgressMeter - Pf3D7_13_v3:2661462 100.7 114420 1136.1; Dangling End recovery killed because of a loop (getReferencePathForwardFromKmer); 15:56:43.998 INFO ProgressMeter - Pf3D7_13_v3:2730055 100.9 114840 1138.3; 15:56:59.911 INFO ProgressMeter - Pf3D7_13_v3:2798281 101.2 115210 1139.0; 15:59:27.062 INFO ProgressMeter - Pf3D7_13_v3:2861780 103.6 115460 1114.4; Dangling End recovery killed because of a loop (getReferencePathForwardFromKmer); Dangling End recovery killed because of a loop (getReferencePathForwardFromKmer); 15:59:37.457 INFO ProgressMeter - Pf3D7_13_v3:2869697 103.8 115500 1112.9. real 671m24.770s; user 777m30.923s; sys 6m13.682s. $ echo $?; 247; ```. Here is my command-line invocation:; ```; ./gatk --java-options ""-Xmx100000m -Xms25000m"" \; HaplotypeCaller \; -R /juffowup2/malaria/references/PlasmoDB-61_Pfalciparum3D7_Genome.fasta \; -I ${WORKING_DIR}/fixed_bam/PG0004-CW.aligned.merged.markDuplicates.sorted.BQSR.bam \; -O ${WORKING_DIR}/PG0004-CW.haplotype_caller.fixed_bam_file.with_pileup.g.vcf.gz \; --bam-output ${WORKING_DIR}/PG0004-CW.haplotype_caller.fixed_bam_file.with_pileup.bamout.bam \; -contamination 0 \; --sample-ploidy 2 \; --linked-de-bruijn-graph \; --pileup-detection true \; --pileup-detection-enable-indel-pileup-calling true \; --max-reads-per-alignment-start 20 \; --annotate-with-num-discovered-alleles \; -GQB 10 -GQB 20 -GQB 30 -GQB 40 -GQB 50 -GQB 60 -GQB 70 -GQB 80 -GQB 90 \; -G StandardAnn",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8440
https://github.com/broadinstitute/gatk/issues/8440:6431,Safety,detect,detection,6431,"emove the `--linked-de-bruijn-graph` argument, this runs just fine with the default setting of `--max-reads-per-alignment-start`. I have a minimally reproductive dataset that I can share which reproduces the OOM error for sure (I'm 99% sure it reproduces this one as well). For the OOM failures, the final logs from HaplotypeCaller look like this:. ```; ./gatk HaplotypeCaller ...; ...; 15:56:23.205 INFO ProgressMeter - Pf3D7_13_v3:2603234 100.5 114070 1134.5; 15:56:33.443 INFO ProgressMeter - Pf3D7_13_v3:2661462 100.7 114420 1136.1; Dangling End recovery killed because of a loop (getReferencePathForwardFromKmer); 15:56:43.998 INFO ProgressMeter - Pf3D7_13_v3:2730055 100.9 114840 1138.3; 15:56:59.911 INFO ProgressMeter - Pf3D7_13_v3:2798281 101.2 115210 1139.0; 15:59:27.062 INFO ProgressMeter - Pf3D7_13_v3:2861780 103.6 115460 1114.4; Dangling End recovery killed because of a loop (getReferencePathForwardFromKmer); Dangling End recovery killed because of a loop (getReferencePathForwardFromKmer); 15:59:37.457 INFO ProgressMeter - Pf3D7_13_v3:2869697 103.8 115500 1112.9. real 671m24.770s; user 777m30.923s; sys 6m13.682s. $ echo $?; 247; ```. Here is my command-line invocation:; ```; ./gatk --java-options ""-Xmx100000m -Xms25000m"" \; HaplotypeCaller \; -R /juffowup2/malaria/references/PlasmoDB-61_Pfalciparum3D7_Genome.fasta \; -I ${WORKING_DIR}/fixed_bam/PG0004-CW.aligned.merged.markDuplicates.sorted.BQSR.bam \; -O ${WORKING_DIR}/PG0004-CW.haplotype_caller.fixed_bam_file.with_pileup.g.vcf.gz \; --bam-output ${WORKING_DIR}/PG0004-CW.haplotype_caller.fixed_bam_file.with_pileup.bamout.bam \; -contamination 0 \; --sample-ploidy 2 \; --linked-de-bruijn-graph \; --pileup-detection true \; --pileup-detection-enable-indel-pileup-calling true \; --max-reads-per-alignment-start 20 \; --annotate-with-num-discovered-alleles \; -GQB 10 -GQB 20 -GQB 30 -GQB 40 -GQB 50 -GQB 60 -GQB 70 -GQB 80 -GQB 90 \; -G StandardAnnotation -G StandardHCAnnotation \; -ERC GVCF \; --verbosity INFO \; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8440
https://github.com/broadinstitute/gatk/issues/8440:6458,Safety,detect,detection-enable-indel-pileup-calling,6458,"emove the `--linked-de-bruijn-graph` argument, this runs just fine with the default setting of `--max-reads-per-alignment-start`. I have a minimally reproductive dataset that I can share which reproduces the OOM error for sure (I'm 99% sure it reproduces this one as well). For the OOM failures, the final logs from HaplotypeCaller look like this:. ```; ./gatk HaplotypeCaller ...; ...; 15:56:23.205 INFO ProgressMeter - Pf3D7_13_v3:2603234 100.5 114070 1134.5; 15:56:33.443 INFO ProgressMeter - Pf3D7_13_v3:2661462 100.7 114420 1136.1; Dangling End recovery killed because of a loop (getReferencePathForwardFromKmer); 15:56:43.998 INFO ProgressMeter - Pf3D7_13_v3:2730055 100.9 114840 1138.3; 15:56:59.911 INFO ProgressMeter - Pf3D7_13_v3:2798281 101.2 115210 1139.0; 15:59:27.062 INFO ProgressMeter - Pf3D7_13_v3:2861780 103.6 115460 1114.4; Dangling End recovery killed because of a loop (getReferencePathForwardFromKmer); Dangling End recovery killed because of a loop (getReferencePathForwardFromKmer); 15:59:37.457 INFO ProgressMeter - Pf3D7_13_v3:2869697 103.8 115500 1112.9. real 671m24.770s; user 777m30.923s; sys 6m13.682s. $ echo $?; 247; ```. Here is my command-line invocation:; ```; ./gatk --java-options ""-Xmx100000m -Xms25000m"" \; HaplotypeCaller \; -R /juffowup2/malaria/references/PlasmoDB-61_Pfalciparum3D7_Genome.fasta \; -I ${WORKING_DIR}/fixed_bam/PG0004-CW.aligned.merged.markDuplicates.sorted.BQSR.bam \; -O ${WORKING_DIR}/PG0004-CW.haplotype_caller.fixed_bam_file.with_pileup.g.vcf.gz \; --bam-output ${WORKING_DIR}/PG0004-CW.haplotype_caller.fixed_bam_file.with_pileup.bamout.bam \; -contamination 0 \; --sample-ploidy 2 \; --linked-de-bruijn-graph \; --pileup-detection true \; --pileup-detection-enable-indel-pileup-calling true \; --max-reads-per-alignment-start 20 \; --annotate-with-num-discovered-alleles \; -GQB 10 -GQB 20 -GQB 30 -GQB 40 -GQB 50 -GQB 60 -GQB 70 -GQB 80 -GQB 90 \; -G StandardAnnotation -G StandardHCAnnotation \; -ERC GVCF \; --verbosity INFO \; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8440
https://github.com/broadinstitute/gatk/issues/8440:5050,Testability,log,logs,5050,".java:306). real 481m24.418s; user 581m54.752s; sys 2m49.965s. ```. This run did not complete successfully - the Exception caused it to fail prematurely. . Previously I had seen HaplotypeCaller run out of memory and fail in almost as much time, so I think this and the OOM error are related. The only difference in invocation was that with the OOM failure, I was running with the default for `--max-reads-per-alignment-start` (`50`). This also works just fine with that setting at 15. The failure seems to occur around the same place in the data each time (the end of `chr13`). At that point in the data, there is a very large pileup which is probably instigating this. Additionally, if I remove the `--linked-de-bruijn-graph` argument, this runs just fine with the default setting of `--max-reads-per-alignment-start`. I have a minimally reproductive dataset that I can share which reproduces the OOM error for sure (I'm 99% sure it reproduces this one as well). For the OOM failures, the final logs from HaplotypeCaller look like this:. ```; ./gatk HaplotypeCaller ...; ...; 15:56:23.205 INFO ProgressMeter - Pf3D7_13_v3:2603234 100.5 114070 1134.5; 15:56:33.443 INFO ProgressMeter - Pf3D7_13_v3:2661462 100.7 114420 1136.1; Dangling End recovery killed because of a loop (getReferencePathForwardFromKmer); 15:56:43.998 INFO ProgressMeter - Pf3D7_13_v3:2730055 100.9 114840 1138.3; 15:56:59.911 INFO ProgressMeter - Pf3D7_13_v3:2798281 101.2 115210 1139.0; 15:59:27.062 INFO ProgressMeter - Pf3D7_13_v3:2861780 103.6 115460 1114.4; Dangling End recovery killed because of a loop (getReferencePathForwardFromKmer); Dangling End recovery killed because of a loop (getReferencePathForwardFromKmer); 15:59:37.457 INFO ProgressMeter - Pf3D7_13_v3:2869697 103.8 115500 1112.9. real 671m24.770s; user 777m30.923s; sys 6m13.682s. $ echo $?; 247; ```. Here is my command-line invocation:; ```; ./gatk --java-options ""-Xmx100000m -Xms25000m"" \; HaplotypeCaller \; -R /juffowup2/malaria/references/PlasmoDB-61_",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8440
https://github.com/broadinstitute/gatk/pull/8441:315,Availability,echo,echo,315,"Integration run here: https://app.terra.bio/#workspaces/gvs-dev/mlc%20GVS%20Quickstart%203%20samples/job_history/acb5b878-af45-443d-8139-0f0044cbcb38. The basic problem: https://news.ycombinator.com/item?id=9255830. Repro:. ```; % # make a file shaped like what was failing in ingest; % for i in $(seq 50000); do ; echo foo,${i} >> file.csv; done; % # repro the pipeline that was failing; % set -o pipefail; % cat file.csv | cut -d, -f2 | sort -r -n | head -1; 50000; % echo $?; 141; % # repeat with temp file construct; % head -1 <(cat file.csv | cut -d, -f2 | sort -r -n) ; 50000; % echo $?; 0; %; ```; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8441
https://github.com/broadinstitute/gatk/pull/8441:470,Availability,echo,echo,470,"Integration run here: https://app.terra.bio/#workspaces/gvs-dev/mlc%20GVS%20Quickstart%203%20samples/job_history/acb5b878-af45-443d-8139-0f0044cbcb38. The basic problem: https://news.ycombinator.com/item?id=9255830. Repro:. ```; % # make a file shaped like what was failing in ingest; % for i in $(seq 50000); do ; echo foo,${i} >> file.csv; done; % # repro the pipeline that was failing; % set -o pipefail; % cat file.csv | cut -d, -f2 | sort -r -n | head -1; 50000; % echo $?; 141; % # repeat with temp file construct; % head -1 <(cat file.csv | cut -d, -f2 | sort -r -n) ; 50000; % echo $?; 0; %; ```; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8441
https://github.com/broadinstitute/gatk/pull/8441:585,Availability,echo,echo,585,"Integration run here: https://app.terra.bio/#workspaces/gvs-dev/mlc%20GVS%20Quickstart%203%20samples/job_history/acb5b878-af45-443d-8139-0f0044cbcb38. The basic problem: https://news.ycombinator.com/item?id=9255830. Repro:. ```; % # make a file shaped like what was failing in ingest; % for i in $(seq 50000); do ; echo foo,${i} >> file.csv; done; % # repro the pipeline that was failing; % set -o pipefail; % cat file.csv | cut -d, -f2 | sort -r -n | head -1; 50000; % echo $?; 141; % # repeat with temp file construct; % head -1 <(cat file.csv | cut -d, -f2 | sort -r -n) ; 50000; % echo $?; 0; %; ```; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8441
https://github.com/broadinstitute/gatk/pull/8441:0,Deployability,Integrat,Integration,0,"Integration run here: https://app.terra.bio/#workspaces/gvs-dev/mlc%20GVS%20Quickstart%203%20samples/job_history/acb5b878-af45-443d-8139-0f0044cbcb38. The basic problem: https://news.ycombinator.com/item?id=9255830. Repro:. ```; % # make a file shaped like what was failing in ingest; % for i in $(seq 50000); do ; echo foo,${i} >> file.csv; done; % # repro the pipeline that was failing; % set -o pipefail; % cat file.csv | cut -d, -f2 | sort -r -n | head -1; 50000; % echo $?; 141; % # repeat with temp file construct; % head -1 <(cat file.csv | cut -d, -f2 | sort -r -n) ; 50000; % echo $?; 0; %; ```; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8441
https://github.com/broadinstitute/gatk/pull/8441:362,Deployability,pipeline,pipeline,362,"Integration run here: https://app.terra.bio/#workspaces/gvs-dev/mlc%20GVS%20Quickstart%203%20samples/job_history/acb5b878-af45-443d-8139-0f0044cbcb38. The basic problem: https://news.ycombinator.com/item?id=9255830. Repro:. ```; % # make a file shaped like what was failing in ingest; % for i in $(seq 50000); do ; echo foo,${i} >> file.csv; done; % # repro the pipeline that was failing; % set -o pipefail; % cat file.csv | cut -d, -f2 | sort -r -n | head -1; 50000; % echo $?; 141; % # repeat with temp file construct; % head -1 <(cat file.csv | cut -d, -f2 | sort -r -n) ; 50000; % echo $?; 0; %; ```; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8441
https://github.com/broadinstitute/gatk/pull/8441:0,Integrability,Integrat,Integration,0,"Integration run here: https://app.terra.bio/#workspaces/gvs-dev/mlc%20GVS%20Quickstart%203%20samples/job_history/acb5b878-af45-443d-8139-0f0044cbcb38. The basic problem: https://news.ycombinator.com/item?id=9255830. Repro:. ```; % # make a file shaped like what was failing in ingest; % for i in $(seq 50000); do ; echo foo,${i} >> file.csv; done; % # repro the pipeline that was failing; % set -o pipefail; % cat file.csv | cut -d, -f2 | sort -r -n | head -1; 50000; % echo $?; 141; % # repeat with temp file construct; % head -1 <(cat file.csv | cut -d, -f2 | sort -r -n) ; 50000; % echo $?; 0; %; ```; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8441
https://github.com/broadinstitute/gatk/issues/8443:581,Usability,simpl,simplest,581,"Hello,. When using JEXL expressions, is there a good built-in solution for handling both single and multi-allelic sites? A good example is trying to filter on AF, which could be either a single number or a list. Perhaps I'm missing something now, but it seems like there are two possibilities:. 1) Split multi-allelic sites into multiple variants (like VariantsToTable can do), and output/filter them independently.; 2) Support functions, like perhaps min() and max(). One would need to think about the desired result, but filtering on ""min(AF) < 0.05"" might be reasonable. In the simplest implementation, the entire site would be in or out (as opposed to trying to be smart about filtering specific alt alleles). . Just curious if there is something built-in i'm missing, or if ways to support this have already been discussed.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8443
https://github.com/broadinstitute/gatk/pull/8447:32,Availability,error,error,32,Allow users to turn off Mutect2 error model fix from 4.1.9.0,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8447
https://github.com/broadinstitute/gatk/pull/8448:6,Deployability,integrat,integration,6,Exome integration test. Passing integration test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/894d12a1-35b8-4a9b-850e-a11dca4b4257),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8448
https://github.com/broadinstitute/gatk/pull/8448:32,Deployability,integrat,integration,32,Exome integration test. Passing integration test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/894d12a1-35b8-4a9b-850e-a11dca4b4257),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8448
https://github.com/broadinstitute/gatk/pull/8448:6,Integrability,integrat,integration,6,Exome integration test. Passing integration test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/894d12a1-35b8-4a9b-850e-a11dca4b4257),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8448
https://github.com/broadinstitute/gatk/pull/8448:32,Integrability,integrat,integration,32,Exome integration test. Passing integration test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/894d12a1-35b8-4a9b-850e-a11dca4b4257),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8448
https://github.com/broadinstitute/gatk/pull/8448:18,Testability,test,test,18,Exome integration test. Passing integration test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/894d12a1-35b8-4a9b-850e-a11dca4b4257),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8448
https://github.com/broadinstitute/gatk/pull/8448:44,Testability,test,test,44,Exome integration test. Passing integration test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/894d12a1-35b8-4a9b-850e-a11dca4b4257),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8448
https://github.com/broadinstitute/gatk/pull/8452:205,Deployability,integrat,integration,205,Allow VQSR Classic Memory overrides to be passed from GvsJointVariantCalling.wdl to GvsCreateFilterSet.wdl.; Increase memory overhead on a couple of tasks in GvsVQSRClassic.wdl. @RoriCremer is running the integration test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/8a8b5553-d9d4-47f5-80fb-ec5992172143).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8452
https://github.com/broadinstitute/gatk/pull/8452:205,Integrability,integrat,integration,205,Allow VQSR Classic Memory overrides to be passed from GvsJointVariantCalling.wdl to GvsCreateFilterSet.wdl.; Increase memory overhead on a couple of tasks in GvsVQSRClassic.wdl. @RoriCremer is running the integration test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/8a8b5553-d9d4-47f5-80fb-ec5992172143).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8452
https://github.com/broadinstitute/gatk/pull/8452:217,Testability,test,test,217,Allow VQSR Classic Memory overrides to be passed from GvsJointVariantCalling.wdl to GvsCreateFilterSet.wdl.; Increase memory overhead on a couple of tasks in GvsVQSRClassic.wdl. @RoriCremer is running the integration test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/8a8b5553-d9d4-47f5-80fb-ec5992172143).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8452
https://github.com/broadinstitute/gatk/issues/8453:326,Availability,error,error,326,"As reported recently by Eric Jones:. ""Many newer Linux distributions will mount /tmp with the noexec mount option. If /tmp is noexec, then any JNA code (Java code that extracts native binary components into shareable libraries and then executes that code) will fail. There are a number of GATK methods that do this. A typical error looks like:. java.lang.UnsatisfiedLinkError: /tmp/libbwa.2929202181066681888.jnilib: /tmp/libbwa.2929202181066681888.jnilib: failed to map segment from shared object. There's an easy fix for it: you can use --tmpdir or one of the typical java methods that reset java.io.tmpdir to name a directory that isn't noexec. But it's amazingly hard to find clues about that being necessary. I found no references to noexec on the forum nor in the help section of the gatk site"". We should address this by explicitly checking on GATK startup whether the selected temp dir is marked noexec, and warn the user in that case.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8453
https://github.com/broadinstitute/gatk/pull/8454:1,Deployability,Integrat,Integration,1,[Integration run](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/dfeb564d-5b4d-4b6b-9ffd-88e618acf5e0) in progress,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8454
https://github.com/broadinstitute/gatk/pull/8454:1,Integrability,Integrat,Integration,1,[Integration run](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/dfeb564d-5b4d-4b6b-9ffd-88e618acf5e0) in progress,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8454
https://github.com/broadinstitute/gatk/issues/8455:287,Availability,error,error,287,"I'm using GATK 4.2.1.0-0 tool `Mutect2` to call mutations in a mitochondrion genome, and later processing the VCFs with `FilterMutectCalls` enabling as well the mitochondria mode (`--mitochondria-mode true`). For some reason, this results in **some** of the VCFs to return the following error:. > java.lang.IllegalArgumentException: log10p: Log10-probability must be 0 or less; > 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:798); > 	at org.broadinstitute.hellbender.utils.MathUtils.log10BinomialProbability(MathUtils.java:646); > 	at org.broadinstitute.hellbender.utils.MathUtils.binomialProbability(MathUtils.java:639); > 	at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.lambda$calculateQuantileBackgroundResponsibilities$10(SomaticClusteringModel.java:271); > 	at org.broadinstitute.hellbender.utils.MathUtils.applyToArray(MathUtils.java:1035); > 	at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.calculateQuantileBackgroundResponsibilities(SomaticClusteringModel.java:271); > 	at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.initializeClusters(SomaticClusteringModel.java:165); > 	at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.learnAndClearAccumulatedData(SomaticClusteringModel.java:325); > 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.learnParameters(Mutect2FilteringEngine.java:153); > 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls.afterNthPass(FilterMutectCalls.java:165); > 	at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.traverse(MultiplePassVariantWalker.java:44); > 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1085); > 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); > 	at org.broadinstitute.hellbender.cmdline.CommandLineProgr",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8455
https://github.com/broadinstitute/gatk/issues/8455:2407,Availability,error,error,2407,"mbda$calculateQuantileBackgroundResponsibilities$10(SomaticClusteringModel.java:271); > 	at org.broadinstitute.hellbender.utils.MathUtils.applyToArray(MathUtils.java:1035); > 	at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.calculateQuantileBackgroundResponsibilities(SomaticClusteringModel.java:271); > 	at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.initializeClusters(SomaticClusteringModel.java:165); > 	at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.learnAndClearAccumulatedData(SomaticClusteringModel.java:325); > 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.learnParameters(Mutect2FilteringEngine.java:153); > 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls.afterNthPass(FilterMutectCalls.java:165); > 	at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.traverse(MultiplePassVariantWalker.java:44); > 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1085); > 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); > 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); > 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); > 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); > 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); > 	at org.broadinstitute.hellbender.Main.main(Main.java:289). For those files experiencing the error, it disappears when disabling the mitochondria mode for `FilterMutectCalls`. I wonder how this problem could be solved so that all VCFs can be filtering in a consistent way, enabling the mitochondria mode. I am happy to share the VCF and reference sequence used it that helps reproducing/solving the issue. Thank you,; Eugenio",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8455
https://github.com/broadinstitute/gatk/issues/8455:426,Security,validat,validateArg,426,"I'm using GATK 4.2.1.0-0 tool `Mutect2` to call mutations in a mitochondrion genome, and later processing the VCFs with `FilterMutectCalls` enabling as well the mitochondria mode (`--mitochondria-mode true`). For some reason, this results in **some** of the VCFs to return the following error:. > java.lang.IllegalArgumentException: log10p: Log10-probability must be 0 or less; > 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:798); > 	at org.broadinstitute.hellbender.utils.MathUtils.log10BinomialProbability(MathUtils.java:646); > 	at org.broadinstitute.hellbender.utils.MathUtils.binomialProbability(MathUtils.java:639); > 	at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.lambda$calculateQuantileBackgroundResponsibilities$10(SomaticClusteringModel.java:271); > 	at org.broadinstitute.hellbender.utils.MathUtils.applyToArray(MathUtils.java:1035); > 	at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.calculateQuantileBackgroundResponsibilities(SomaticClusteringModel.java:271); > 	at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.initializeClusters(SomaticClusteringModel.java:165); > 	at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.learnAndClearAccumulatedData(SomaticClusteringModel.java:325); > 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.learnParameters(Mutect2FilteringEngine.java:153); > 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls.afterNthPass(FilterMutectCalls.java:165); > 	at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.traverse(MultiplePassVariantWalker.java:44); > 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1085); > 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); > 	at org.broadinstitute.hellbender.cmdline.CommandLineProgr",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8455
https://github.com/broadinstitute/gatk/issues/8455:1316,Usability,learn,learnAndClearAccumulatedData,1316,og10-probability must be 0 or less; > 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:798); > 	at org.broadinstitute.hellbender.utils.MathUtils.log10BinomialProbability(MathUtils.java:646); > 	at org.broadinstitute.hellbender.utils.MathUtils.binomialProbability(MathUtils.java:639); > 	at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.lambda$calculateQuantileBackgroundResponsibilities$10(SomaticClusteringModel.java:271); > 	at org.broadinstitute.hellbender.utils.MathUtils.applyToArray(MathUtils.java:1035); > 	at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.calculateQuantileBackgroundResponsibilities(SomaticClusteringModel.java:271); > 	at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.initializeClusters(SomaticClusteringModel.java:165); > 	at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.learnAndClearAccumulatedData(SomaticClusteringModel.java:325); > 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.learnParameters(Mutect2FilteringEngine.java:153); > 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls.afterNthPass(FilterMutectCalls.java:165); > 	at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.traverse(MultiplePassVariantWalker.java:44); > 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1085); > 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); > 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); > 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); > 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); > 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); > 	at org.broadinstitute.hellb,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8455
https://github.com/broadinstitute/gatk/issues/8455:1469,Usability,learn,learnParameters,1469,"er.utils.MathUtils.log10BinomialProbability(MathUtils.java:646); > 	at org.broadinstitute.hellbender.utils.MathUtils.binomialProbability(MathUtils.java:639); > 	at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.lambda$calculateQuantileBackgroundResponsibilities$10(SomaticClusteringModel.java:271); > 	at org.broadinstitute.hellbender.utils.MathUtils.applyToArray(MathUtils.java:1035); > 	at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.calculateQuantileBackgroundResponsibilities(SomaticClusteringModel.java:271); > 	at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.initializeClusters(SomaticClusteringModel.java:165); > 	at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.learnAndClearAccumulatedData(SomaticClusteringModel.java:325); > 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.learnParameters(Mutect2FilteringEngine.java:153); > 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls.afterNthPass(FilterMutectCalls.java:165); > 	at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.traverse(MultiplePassVariantWalker.java:44); > 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1085); > 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); > 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); > 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); > 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); > 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); > 	at org.broadinstitute.hellbender.Main.main(Main.java:289). For those files experiencing the error, it disappears when disabling the mitochondria mode for `FilterMutectCalls`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8455
https://github.com/broadinstitute/gatk/issues/8456:2824,Availability,down,down,2824,"iates - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 22:13:29.982 INFO AnalyzeCovariates - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 22:13:29.983 INFO AnalyzeCovariates - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 22:13:29.983 INFO AnalyzeCovariates - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 22:13:29.983 INFO AnalyzeCovariates - Deflater: IntelDeflater; 22:13:29.983 INFO AnalyzeCovariates - Inflater: IntelInflater; 22:13:29.983 INFO AnalyzeCovariates - GCS max retries/reopens: 20; 22:13:29.983 INFO AnalyzeCovariates - Requester pays: disabled; 22:13:29.984 INFO AnalyzeCovariates - Initializing engine; 22:13:29.984 INFO AnalyzeCovariates - Done initializing engine; 22:13:30.002 INFO AnalyzeCovariates - Generating csv file '/tmp/AnalyzeCovariates13996065741193890473.csv'; 22:13:30.002 INFO AnalyzeCovariates - Generating plots file './sample_analysis/SRR25308851/SRR25308851_recalibration_plots.pdf'; 22:13:30.518 INFO AnalyzeCovariates - Shutting down engine; [August 7, 2023 at 10:13:30 PM UTC] org.broadinstitute.hellbender.tools.walkers.bqsr.AnalyzeCovariates done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=113246208; org.broadinstitute.hellbender.utils.R.RScriptExecutorException:; Rscript exited with 1; Command Line: Rscript -e tempLibDir = '/tmp/Rlib.10708586791705723928';source('/tmp/BQSR.12372590345390592260.R'); /tmp/AnalyzeCovariates13996065741193890473.csv /attach/data/vinit/human_exome/test/./sample_analysis/SRR25308851/SRR25308851_before_recal_data.table /attach/data/vinit/human_exome/test/./sample_analysis/SRR25308851/SRR25308851_recalibration_plots.pdf; Stdout:; Stderr:; Attaching package: ‘gplots’. The following object is masked from ‘package:stats’:. lowess. Error in names(x) <- value :; 'names' attribute [6] must be the same length as the vector [1]; Calls: source ... finishTable -> .gsa.assignGATKTableToEnvironment -> colnames<-; In addition: Warning messages:; 1: In type.convert.default(d[, i]) :; 'as.is' shoul",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8456
https://github.com/broadinstitute/gatk/issues/8456:3534,Availability,mask,masked,3534,"ne; 22:13:30.002 INFO AnalyzeCovariates - Generating csv file '/tmp/AnalyzeCovariates13996065741193890473.csv'; 22:13:30.002 INFO AnalyzeCovariates - Generating plots file './sample_analysis/SRR25308851/SRR25308851_recalibration_plots.pdf'; 22:13:30.518 INFO AnalyzeCovariates - Shutting down engine; [August 7, 2023 at 10:13:30 PM UTC] org.broadinstitute.hellbender.tools.walkers.bqsr.AnalyzeCovariates done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=113246208; org.broadinstitute.hellbender.utils.R.RScriptExecutorException:; Rscript exited with 1; Command Line: Rscript -e tempLibDir = '/tmp/Rlib.10708586791705723928';source('/tmp/BQSR.12372590345390592260.R'); /tmp/AnalyzeCovariates13996065741193890473.csv /attach/data/vinit/human_exome/test/./sample_analysis/SRR25308851/SRR25308851_before_recal_data.table /attach/data/vinit/human_exome/test/./sample_analysis/SRR25308851/SRR25308851_recalibration_plots.pdf; Stdout:; Stderr:; Attaching package: ‘gplots’. The following object is masked from ‘package:stats’:. lowess. Error in names(x) <- value :; 'names' attribute [6] must be the same length as the vector [1]; Calls: source ... finishTable -> .gsa.assignGATKTableToEnvironment -> colnames<-; In addition: Warning messages:; 1: In type.convert.default(d[, i]) :; 'as.is' should be specified by the caller; using TRUE; 2: In type.convert.default(d[, i]) :; 'as.is' should be specified by the caller; using TRUE; 3: In type.convert.default(d[, i]) :; 'as.is' should be specified by the caller; using TRUE; 4: In type.convert.default(d[, i]) :; 'as.is' should be specified by the caller; using TRUE; 5: In type.convert.default(d[, i]) :; 'as.is' should be specified by the caller; using TRUE; Execution halted. at org.broadinstitute.hellbender.utils.R.RScriptExecutor.getScriptException(RScriptExecutor.java:79); at org.broadinstitute.hellbender.utils.R.RScriptExecutor.getScriptException(RScriptExecutor.java:18); at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.execut",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8456
https://github.com/broadinstitute/gatk/issues/8456:3572,Availability,Error,Error,3572,"065741193890473.csv'; 22:13:30.002 INFO AnalyzeCovariates - Generating plots file './sample_analysis/SRR25308851/SRR25308851_recalibration_plots.pdf'; 22:13:30.518 INFO AnalyzeCovariates - Shutting down engine; [August 7, 2023 at 10:13:30 PM UTC] org.broadinstitute.hellbender.tools.walkers.bqsr.AnalyzeCovariates done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=113246208; org.broadinstitute.hellbender.utils.R.RScriptExecutorException:; Rscript exited with 1; Command Line: Rscript -e tempLibDir = '/tmp/Rlib.10708586791705723928';source('/tmp/BQSR.12372590345390592260.R'); /tmp/AnalyzeCovariates13996065741193890473.csv /attach/data/vinit/human_exome/test/./sample_analysis/SRR25308851/SRR25308851_before_recal_data.table /attach/data/vinit/human_exome/test/./sample_analysis/SRR25308851/SRR25308851_recalibration_plots.pdf; Stdout:; Stderr:; Attaching package: ‘gplots’. The following object is masked from ‘package:stats’:. lowess. Error in names(x) <- value :; 'names' attribute [6] must be the same length as the vector [1]; Calls: source ... finishTable -> .gsa.assignGATKTableToEnvironment -> colnames<-; In addition: Warning messages:; 1: In type.convert.default(d[, i]) :; 'as.is' should be specified by the caller; using TRUE; 2: In type.convert.default(d[, i]) :; 'as.is' should be specified by the caller; using TRUE; 3: In type.convert.default(d[, i]) :; 'as.is' should be specified by the caller; using TRUE; 4: In type.convert.default(d[, i]) :; 'as.is' should be specified by the caller; using TRUE; 5: In type.convert.default(d[, i]) :; 'as.is' should be specified by the caller; using TRUE; Execution halted. at org.broadinstitute.hellbender.utils.R.RScriptExecutor.getScriptException(RScriptExecutor.java:79); at org.broadinstitute.hellbender.utils.R.RScriptExecutor.getScriptException(RScriptExecutor.java:18); at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCuratedArgs(ScriptExecutor.java:112); at org.broadinstitute.hellbender.utils.R.RScriptExec",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8456
https://github.com/broadinstitute/gatk/issues/8456:5509,Availability,error,error,5509,"by the caller; using TRUE; 3: In type.convert.default(d[, i]) :; 'as.is' should be specified by the caller; using TRUE; 4: In type.convert.default(d[, i]) :; 'as.is' should be specified by the caller; using TRUE; 5: In type.convert.default(d[, i]) :; 'as.is' should be specified by the caller; using TRUE; Execution halted. at org.broadinstitute.hellbender.utils.R.RScriptExecutor.getScriptException(RScriptExecutor.java:79); at org.broadinstitute.hellbender.utils.R.RScriptExecutor.getScriptException(RScriptExecutor.java:18); at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCuratedArgs(ScriptExecutor.java:112); at org.broadinstitute.hellbender.utils.R.RScriptExecutor.exec(RScriptExecutor.java:125); at org.broadinstitute.hellbender.utils.recalibration.RecalUtils.generatePlots(RecalUtils.java:360); at org.broadinstitute.hellbender.tools.walkers.bqsr.AnalyzeCovariates.generatePlots(AnalyzeCovariates.java:329); at org.broadinstitute.hellbender.tools.walkers.bqsr.AnalyzeCovariates.doWork(AnalyzeCovariates.java:341); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:149); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:198); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:217); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289). The above error occurs when generating plot using:; ```; gatk AnalyzeCovariates \; -before /sample_analysis/SRR25308851/SRR25308851_before_recal_data.table \; -after /sample_analysis/SRR25308851/SRR25308851_after_recal_data.table \; -plots /sample_analysis/SRR25308851/SRR25308851_recalibration_plots.pdf; ```. SRR25308851_before_recal_data.table and SRR25308851_after_recal_data.table files are proper, generated using gatk BaseRecalibrator",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8456
https://github.com/broadinstitute/gatk/issues/8456:3770,Integrability,message,messages,3770,"51_recalibration_plots.pdf'; 22:13:30.518 INFO AnalyzeCovariates - Shutting down engine; [August 7, 2023 at 10:13:30 PM UTC] org.broadinstitute.hellbender.tools.walkers.bqsr.AnalyzeCovariates done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=113246208; org.broadinstitute.hellbender.utils.R.RScriptExecutorException:; Rscript exited with 1; Command Line: Rscript -e tempLibDir = '/tmp/Rlib.10708586791705723928';source('/tmp/BQSR.12372590345390592260.R'); /tmp/AnalyzeCovariates13996065741193890473.csv /attach/data/vinit/human_exome/test/./sample_analysis/SRR25308851/SRR25308851_before_recal_data.table /attach/data/vinit/human_exome/test/./sample_analysis/SRR25308851/SRR25308851_recalibration_plots.pdf; Stdout:; Stderr:; Attaching package: ‘gplots’. The following object is masked from ‘package:stats’:. lowess. Error in names(x) <- value :; 'names' attribute [6] must be the same length as the vector [1]; Calls: source ... finishTable -> .gsa.assignGATKTableToEnvironment -> colnames<-; In addition: Warning messages:; 1: In type.convert.default(d[, i]) :; 'as.is' should be specified by the caller; using TRUE; 2: In type.convert.default(d[, i]) :; 'as.is' should be specified by the caller; using TRUE; 3: In type.convert.default(d[, i]) :; 'as.is' should be specified by the caller; using TRUE; 4: In type.convert.default(d[, i]) :; 'as.is' should be specified by the caller; using TRUE; 5: In type.convert.default(d[, i]) :; 'as.is' should be specified by the caller; using TRUE; Execution halted. at org.broadinstitute.hellbender.utils.R.RScriptExecutor.getScriptException(RScriptExecutor.java:79); at org.broadinstitute.hellbender.utils.R.RScriptExecutor.getScriptException(RScriptExecutor.java:18); at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCuratedArgs(ScriptExecutor.java:112); at org.broadinstitute.hellbender.utils.R.RScriptExecutor.exec(RScriptExecutor.java:125); at org.broadinstitute.hellbender.utils.recalibration.RecalUtils.generatePlots(RecalUt",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8456
https://github.com/broadinstitute/gatk/issues/8456:633,Performance,Load,Loading,633,"Using GATK jar /root/miniconda3/envs/denovo/share/gatk4-4.4.0.0-0/gatk-package-4.4.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /root/miniconda3/envs/denovo/share/gatk4-4.4.0.0-0/gatk-package-4.4.0.0-local.jar AnalyzeCovariates -before ./sample_analysis/SRR25308851/SRR25308851_before_recal_data.table -after ./sample_analysis/SRR25308851/SRR25308851_after_recal_data.table -plots ./sample_analysis/SRR25308851/SRR25308851_recalibration_plots.pdf; 22:13:29.955 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/root/miniconda3/envs/denovo/share/gatk4-4.4.0.0-0/gatk-package-4.4.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 22:13:29.979 INFO AnalyzeCovariates - ------------------------------------------------------------; 22:13:29.981 INFO AnalyzeCovariates - The Genome Analysis Toolkit (GATK) v4.4.0.0; 22:13:29.981 INFO AnalyzeCovariates - For support and documentation go to https://software.broadinstitute.org/gatk/; 22:13:29.981 INFO AnalyzeCovariates - Executing as root@ip-172-31-23-160 on Linux v5.19.0-1029-aws amd64; 22:13:29.981 INFO AnalyzeCovariates - Java runtime: OpenJDK 64-Bit Server VM v19.0.2+7-Ubuntu-0ubuntu322.04; 22:13:29.981 INFO AnalyzeCovariates - Start Date/Time: August 7, 2023 at 10:13:29 PM UTC; 22:13:29.981 INFO AnalyzeCovariates - ------------------------------------------------------------; 22:13:29.981 INFO AnalyzeCovariates - ------------------------------------------------------------; 22:13:29.982 INFO AnalyzeCovariates - HTSJDK Version: 3.0.5; 22:13:29.982 INFO AnalyzeCovariates - Picard Version: 3.0.0; 22:13:29.982 INFO AnalyzeCovariates - Built for Spark Version: 3.3.1; 22:13:29.982 INFO AnalyzeCovariates - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 22:13:29.982 INFO AnalyzeCovariates - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 22:13:29.983 INFO AnalyzeCova",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8456
https://github.com/broadinstitute/gatk/issues/8456:3289,Testability,test,test,3289,"elInflater; 22:13:29.983 INFO AnalyzeCovariates - GCS max retries/reopens: 20; 22:13:29.983 INFO AnalyzeCovariates - Requester pays: disabled; 22:13:29.984 INFO AnalyzeCovariates - Initializing engine; 22:13:29.984 INFO AnalyzeCovariates - Done initializing engine; 22:13:30.002 INFO AnalyzeCovariates - Generating csv file '/tmp/AnalyzeCovariates13996065741193890473.csv'; 22:13:30.002 INFO AnalyzeCovariates - Generating plots file './sample_analysis/SRR25308851/SRR25308851_recalibration_plots.pdf'; 22:13:30.518 INFO AnalyzeCovariates - Shutting down engine; [August 7, 2023 at 10:13:30 PM UTC] org.broadinstitute.hellbender.tools.walkers.bqsr.AnalyzeCovariates done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=113246208; org.broadinstitute.hellbender.utils.R.RScriptExecutorException:; Rscript exited with 1; Command Line: Rscript -e tempLibDir = '/tmp/Rlib.10708586791705723928';source('/tmp/BQSR.12372590345390592260.R'); /tmp/AnalyzeCovariates13996065741193890473.csv /attach/data/vinit/human_exome/test/./sample_analysis/SRR25308851/SRR25308851_before_recal_data.table /attach/data/vinit/human_exome/test/./sample_analysis/SRR25308851/SRR25308851_recalibration_plots.pdf; Stdout:; Stderr:; Attaching package: ‘gplots’. The following object is masked from ‘package:stats’:. lowess. Error in names(x) <- value :; 'names' attribute [6] must be the same length as the vector [1]; Calls: source ... finishTable -> .gsa.assignGATKTableToEnvironment -> colnames<-; In addition: Warning messages:; 1: In type.convert.default(d[, i]) :; 'as.is' should be specified by the caller; using TRUE; 2: In type.convert.default(d[, i]) :; 'as.is' should be specified by the caller; using TRUE; 3: In type.convert.default(d[, i]) :; 'as.is' should be specified by the caller; using TRUE; 4: In type.convert.default(d[, i]) :; 'as.is' should be specified by the caller; using TRUE; 5: In type.convert.default(d[, i]) :; 'as.is' should be specified by the caller; using TRUE; Execution halted. at org.broa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8456
https://github.com/broadinstitute/gatk/issues/8456:3391,Testability,test,test,3391,"yzeCovariates - Requester pays: disabled; 22:13:29.984 INFO AnalyzeCovariates - Initializing engine; 22:13:29.984 INFO AnalyzeCovariates - Done initializing engine; 22:13:30.002 INFO AnalyzeCovariates - Generating csv file '/tmp/AnalyzeCovariates13996065741193890473.csv'; 22:13:30.002 INFO AnalyzeCovariates - Generating plots file './sample_analysis/SRR25308851/SRR25308851_recalibration_plots.pdf'; 22:13:30.518 INFO AnalyzeCovariates - Shutting down engine; [August 7, 2023 at 10:13:30 PM UTC] org.broadinstitute.hellbender.tools.walkers.bqsr.AnalyzeCovariates done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=113246208; org.broadinstitute.hellbender.utils.R.RScriptExecutorException:; Rscript exited with 1; Command Line: Rscript -e tempLibDir = '/tmp/Rlib.10708586791705723928';source('/tmp/BQSR.12372590345390592260.R'); /tmp/AnalyzeCovariates13996065741193890473.csv /attach/data/vinit/human_exome/test/./sample_analysis/SRR25308851/SRR25308851_before_recal_data.table /attach/data/vinit/human_exome/test/./sample_analysis/SRR25308851/SRR25308851_recalibration_plots.pdf; Stdout:; Stderr:; Attaching package: ‘gplots’. The following object is masked from ‘package:stats’:. lowess. Error in names(x) <- value :; 'names' attribute [6] must be the same length as the vector [1]; Calls: source ... finishTable -> .gsa.assignGATKTableToEnvironment -> colnames<-; In addition: Warning messages:; 1: In type.convert.default(d[, i]) :; 'as.is' should be specified by the caller; using TRUE; 2: In type.convert.default(d[, i]) :; 'as.is' should be specified by the caller; using TRUE; 3: In type.convert.default(d[, i]) :; 'as.is' should be specified by the caller; using TRUE; 4: In type.convert.default(d[, i]) :; 'as.is' should be specified by the caller; using TRUE; 5: In type.convert.default(d[, i]) :; 'as.is' should be specified by the caller; using TRUE; Execution halted. at org.broadinstitute.hellbender.utils.R.RScriptExecutor.getScriptException(RScriptExecutor.java:79); at org.bro",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8456
https://github.com/broadinstitute/gatk/pull/8457:112,Deployability,integrat,integration,112,* Centralizes Docker image versioning to top-level WDLs; * Does away with GATK override jar in all cases except integration tests (override jar can still be specified during feature development and/or for emergencies); * Docker image versions can be captured as the inputs to tasks; * Freshens Variants Docker image. Integration run [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/815ef8ea-8cfe-47b6-be80-54250d1f180b),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8457
https://github.com/broadinstitute/gatk/pull/8457:317,Deployability,Integrat,Integration,317,* Centralizes Docker image versioning to top-level WDLs; * Does away with GATK override jar in all cases except integration tests (override jar can still be specified during feature development and/or for emergencies); * Docker image versions can be captured as the inputs to tasks; * Freshens Variants Docker image. Integration run [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/815ef8ea-8cfe-47b6-be80-54250d1f180b),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8457
https://github.com/broadinstitute/gatk/pull/8457:112,Integrability,integrat,integration,112,* Centralizes Docker image versioning to top-level WDLs; * Does away with GATK override jar in all cases except integration tests (override jar can still be specified during feature development and/or for emergencies); * Docker image versions can be captured as the inputs to tasks; * Freshens Variants Docker image. Integration run [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/815ef8ea-8cfe-47b6-be80-54250d1f180b),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8457
https://github.com/broadinstitute/gatk/pull/8457:317,Integrability,Integrat,Integration,317,* Centralizes Docker image versioning to top-level WDLs; * Does away with GATK override jar in all cases except integration tests (override jar can still be specified during feature development and/or for emergencies); * Docker image versions can be captured as the inputs to tasks; * Freshens Variants Docker image. Integration run [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/815ef8ea-8cfe-47b6-be80-54250d1f180b),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8457
https://github.com/broadinstitute/gatk/pull/8457:124,Testability,test,tests,124,* Centralizes Docker image versioning to top-level WDLs; * Does away with GATK override jar in all cases except integration tests (override jar can still be specified during feature development and/or for emergencies); * Docker image versions can be captured as the inputs to tasks; * Freshens Variants Docker image. Integration run [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/815ef8ea-8cfe-47b6-be80-54250d1f180b),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8457
https://github.com/broadinstitute/gatk/pull/8458:196,Availability,error,error,196,"When using GVCFs with Mutect2 (for example with the Mitochondria mode), in the filtering step ADs for symbolic alleles are set to 0 so it doesn't contribute to overall AD. There was an off-by-one error that removed the alt allele AD rather than the <NON_REF> allele AD. This led to `NaN`s and errors when a site had no ref reads (for example a GT of [ref,alt,<NON_REF>] and AD of [0,300,0] would accidentally be changed to an AD of [0,0,0] if the alt index was removed instead of the <NON_REF> index). . The test changes the AD in one of the sites of the NA12878.MT.g.vcf to have 0 ref reads which fails without the fix in `SomaticClusteringModel`. This addresses #8455.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8458
https://github.com/broadinstitute/gatk/pull/8458:293,Availability,error,errors,293,"When using GVCFs with Mutect2 (for example with the Mitochondria mode), in the filtering step ADs for symbolic alleles are set to 0 so it doesn't contribute to overall AD. There was an off-by-one error that removed the alt allele AD rather than the <NON_REF> allele AD. This led to `NaN`s and errors when a site had no ref reads (for example a GT of [ref,alt,<NON_REF>] and AD of [0,300,0] would accidentally be changed to an AD of [0,0,0] if the alt index was removed instead of the <NON_REF> index). . The test changes the AD in one of the sites of the NA12878.MT.g.vcf to have 0 ref reads which fails without the fix in `SomaticClusteringModel`. This addresses #8455.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8458
https://github.com/broadinstitute/gatk/pull/8458:508,Testability,test,test,508,"When using GVCFs with Mutect2 (for example with the Mitochondria mode), in the filtering step ADs for symbolic alleles are set to 0 so it doesn't contribute to overall AD. There was an off-by-one error that removed the alt allele AD rather than the <NON_REF> allele AD. This led to `NaN`s and errors when a site had no ref reads (for example a GT of [ref,alt,<NON_REF>] and AD of [0,300,0] would accidentally be changed to an AD of [0,0,0] if the alt index was removed instead of the <NON_REF> index). . The test changes the AD in one of the sites of the NA12878.MT.g.vcf to have 0 ref reads which fails without the fix in `SomaticClusteringModel`. This addresses #8455.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8458
https://github.com/broadinstitute/gatk/pull/8459:202,Deployability,update,updates,202,"Changes are; - fixes to `add_max_as_vqs_score.py` to correspond to changes from https://github.com/broadinstitute/gatk/pull/8412 that changes ""AS_VQS_SENS"" to ""CALIBRATION_SENSITIVITY""; - documentation updates. Run to create filter set: https://app.terra.bio/#workspaces/gvs-dev/GVS_190k_Exomes/job_history/cd08d3fa-26a8-4e00-bbeb-0b458dff11ac; Run to extract control VCFs: https://app.terra.bio/#workspaces/gvs-dev/GVS_190k_Exomes/job_history/b351b3d4-4fb0-45be-a7fb-138f2ad0191e; Run to calculate Precision and Sensitivity: https://app.terra.bio/#workspaces/gvs-dev/GVS_190k_Exomes/job_history/34be5132-7f17-42e8-a44f-8c8ed37745e3",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8459
https://github.com/broadinstitute/gatk/issues/8460:152,Deployability,update,update,152,"The GATK docker image is on samtools 1.7, which is ancient and has several known issues that users have run into (especially with CRAM files). We shoud update to a modern version.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8460
https://github.com/broadinstitute/gatk/issues/8462:1862,Availability,avail,available,1862,"GRCh38_no_alt_analysis_set.fna.gz --MINIMUM_QUALITY_SCORE 20 --MINIMUM_MAPPING_QUALITY 30 --MINIMUM_INSERT_SIZE 60 --MAXIMUM_INSERT_SIZE 600 --INCLUDE_UNPAIRED false --INCLUDE_DUPLICATES false --INCLUDE_NON_PF_READS false --TANDEM_READS false --USE_OQ true --CONTEXT_SIZE 1 --ASSUME_SORTED true --STOP_AFTER 0 --VERBOSITY INFO --QUIET false --VALIDATION_STRINGENCY STRICT --COMPRESSION_LEVEL 2 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX false --CREATE_MD5_FILE false --GA4GH_CLIENT_SECRETS client_secrets.json --help false --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATER false; Aug 10, 2023 12:49:43 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; [Thu Aug 10 12:49:43 UTC 2023] Executing as root@34684eaa046e on Linux 4.15.0-208-generic amd64; OpenJDK 64-Bit Server VM 1.8.0_191-8u191-b12-0ubuntu0.16.04.1-b12; Deflater: Intel; Inflater: Intel; Provider GCS is available; Picard version: Version:4.1.3.0; [Thu Aug 10 12:49:43 UTC 2023] picard.analysis.artifacts.CollectSequencingArtifactMetrics done. Elapsed time: 0.03 minutes.; Runtime.totalMemory()=2076049408; To get help, see http://broadinstitute.github.io/picard/index.html#GettingHelp; htsjdk.samtools.SAMException: Cannot read non-existent file: file:///gatk/data/Continuum/WES/vcf/NG-27280_CLTSS_LTS_001A_lib506241_7636_2_MarkedDup.bam; at htsjdk.samtools.util.IOUtil.assertFileIsReadable(IOUtil.java:483); at htsjdk.samtools.util.IOUtil.assertFileIsReadable(IOUtil.java:470); at picard.analysis.SinglePassSamProgram.makeItSo(SinglePassSamProgram.java:95); at picard.analysis.SinglePassSamProgram.doWork(SinglePassSamProgram.java:84); at picard.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:305); at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgramExecutor.instanceMain(PicardCommandLineProgramExecutor.java:25); at org.broadinstitute.hellbender.Main.runComma",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8462
https://github.com/broadinstitute/gatk/issues/8462:479,Performance,Load,Loading,479,"Hi. I have pulled your docker and I am certain bam files are in the path but gate can not locate them. ```; (gatk) root@34684eaa046e:/gatk/data/Continuum/WES/vcf# java -d64 -XX:+UseSerialGC -Xmx3G -jar /gatk/gatk.jar CollectSequencingArtifactMetrics -I NG-27280_CLTSS_LTS_001A_lib506241_7636_2_MarkedDup.bam -O NG-27280_CLTSS_LTS_001A_lib506241_7636_2_MarkedDup --FILE_EXTENSION .txt -R GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz; 12:49:41.698 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.3.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; [Thu Aug 10 12:49:41 UTC 2023] CollectSequencingArtifactMetrics --FILE_EXTENSION .txt --INPUT NG-27280_CLTSS_LTS_001A_lib506241_7636_2_MarkedDup.bam --OUTPUT NG-27280_CLTSS_LTS_001A_lib506241_7636_2_MarkedDup --REFERENCE_SEQUENCE GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz --MINIMUM_QUALITY_SCORE 20 --MINIMUM_MAPPING_QUALITY 30 --MINIMUM_INSERT_SIZE 60 --MAXIMUM_INSERT_SIZE 600 --INCLUDE_UNPAIRED false --INCLUDE_DUPLICATES false --INCLUDE_NON_PF_READS false --TANDEM_READS false --USE_OQ true --CONTEXT_SIZE 1 --ASSUME_SORTED true --STOP_AFTER 0 --VERBOSITY INFO --QUIET false --VALIDATION_STRINGENCY STRICT --COMPRESSION_LEVEL 2 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX false --CREATE_MD5_FILE false --GA4GH_CLIENT_SECRETS client_secrets.json --help false --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATER false; Aug 10, 2023 12:49:43 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; [Thu Aug 10 12:49:43 UTC 2023] Executing as root@34684eaa046e on Linux 4.15.0-208-generic amd64; OpenJDK 64-Bit Server VM 1.8.0_191-8u191-b12-0ubuntu0.16.04.1-b12; Deflater: Intel; Inflater: Intel; Provider GCS is available; Picard version: Version:4.1.3.0; [Thu Aug 10 12:49:43 UTC 2023] picard.analysis.artifacts.CollectSequencingArtifactMetrics done.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8462
https://github.com/broadinstitute/gatk/issues/8462:1591,Safety,detect,detect,1591,"kl_compression.so; [Thu Aug 10 12:49:41 UTC 2023] CollectSequencingArtifactMetrics --FILE_EXTENSION .txt --INPUT NG-27280_CLTSS_LTS_001A_lib506241_7636_2_MarkedDup.bam --OUTPUT NG-27280_CLTSS_LTS_001A_lib506241_7636_2_MarkedDup --REFERENCE_SEQUENCE GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz --MINIMUM_QUALITY_SCORE 20 --MINIMUM_MAPPING_QUALITY 30 --MINIMUM_INSERT_SIZE 60 --MAXIMUM_INSERT_SIZE 600 --INCLUDE_UNPAIRED false --INCLUDE_DUPLICATES false --INCLUDE_NON_PF_READS false --TANDEM_READS false --USE_OQ true --CONTEXT_SIZE 1 --ASSUME_SORTED true --STOP_AFTER 0 --VERBOSITY INFO --QUIET false --VALIDATION_STRINGENCY STRICT --COMPRESSION_LEVEL 2 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX false --CREATE_MD5_FILE false --GA4GH_CLIENT_SECRETS client_secrets.json --help false --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATER false; Aug 10, 2023 12:49:43 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; [Thu Aug 10 12:49:43 UTC 2023] Executing as root@34684eaa046e on Linux 4.15.0-208-generic amd64; OpenJDK 64-Bit Server VM 1.8.0_191-8u191-b12-0ubuntu0.16.04.1-b12; Deflater: Intel; Inflater: Intel; Provider GCS is available; Picard version: Version:4.1.3.0; [Thu Aug 10 12:49:43 UTC 2023] picard.analysis.artifacts.CollectSequencingArtifactMetrics done. Elapsed time: 0.03 minutes.; Runtime.totalMemory()=2076049408; To get help, see http://broadinstitute.github.io/picard/index.html#GettingHelp; htsjdk.samtools.SAMException: Cannot read non-existent file: file:///gatk/data/Continuum/WES/vcf/NG-27280_CLTSS_LTS_001A_lib506241_7636_2_MarkedDup.bam; at htsjdk.samtools.util.IOUtil.assertFileIsReadable(IOUtil.java:483); at htsjdk.samtools.util.IOUtil.assertFileIsReadable(IOUtil.java:470); at picard.analysis.SinglePassSamProgram.makeItSo(SinglePassSamProgram.java:95); at picard.analysis.SinglePassSamProgram.doWork(SinglePassSamProgram.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8462
https://github.com/broadinstitute/gatk/issues/8462:2329,Testability,assert,assertFileIsReadable,2329,"ATION_STRINGENCY STRICT --COMPRESSION_LEVEL 2 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX false --CREATE_MD5_FILE false --GA4GH_CLIENT_SECRETS client_secrets.json --help false --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATER false; Aug 10, 2023 12:49:43 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; [Thu Aug 10 12:49:43 UTC 2023] Executing as root@34684eaa046e on Linux 4.15.0-208-generic amd64; OpenJDK 64-Bit Server VM 1.8.0_191-8u191-b12-0ubuntu0.16.04.1-b12; Deflater: Intel; Inflater: Intel; Provider GCS is available; Picard version: Version:4.1.3.0; [Thu Aug 10 12:49:43 UTC 2023] picard.analysis.artifacts.CollectSequencingArtifactMetrics done. Elapsed time: 0.03 minutes.; Runtime.totalMemory()=2076049408; To get help, see http://broadinstitute.github.io/picard/index.html#GettingHelp; htsjdk.samtools.SAMException: Cannot read non-existent file: file:///gatk/data/Continuum/WES/vcf/NG-27280_CLTSS_LTS_001A_lib506241_7636_2_MarkedDup.bam; at htsjdk.samtools.util.IOUtil.assertFileIsReadable(IOUtil.java:483); at htsjdk.samtools.util.IOUtil.assertFileIsReadable(IOUtil.java:470); at picard.analysis.SinglePassSamProgram.makeItSo(SinglePassSamProgram.java:95); at picard.analysis.SinglePassSamProgram.doWork(SinglePassSamProgram.java:84); at picard.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:305); at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgramExecutor.instanceMain(PicardCommandLineProgramExecutor.java:25); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); at org.broadinstitute.hellbender.Main.main(Main.java:291); (gatk) root@34684eaa046e:/gatk/data/Continuum/WES/vcf# ls; GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz; (gatk) root@34684eaa046e:/gatk/data/Continuum/WES/vcf# ; ```; Please help me; Thanks",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8462
https://github.com/broadinstitute/gatk/issues/8462:2399,Testability,assert,assertFileIsReadable,2399,"ATION_STRINGENCY STRICT --COMPRESSION_LEVEL 2 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX false --CREATE_MD5_FILE false --GA4GH_CLIENT_SECRETS client_secrets.json --help false --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATER false; Aug 10, 2023 12:49:43 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; [Thu Aug 10 12:49:43 UTC 2023] Executing as root@34684eaa046e on Linux 4.15.0-208-generic amd64; OpenJDK 64-Bit Server VM 1.8.0_191-8u191-b12-0ubuntu0.16.04.1-b12; Deflater: Intel; Inflater: Intel; Provider GCS is available; Picard version: Version:4.1.3.0; [Thu Aug 10 12:49:43 UTC 2023] picard.analysis.artifacts.CollectSequencingArtifactMetrics done. Elapsed time: 0.03 minutes.; Runtime.totalMemory()=2076049408; To get help, see http://broadinstitute.github.io/picard/index.html#GettingHelp; htsjdk.samtools.SAMException: Cannot read non-existent file: file:///gatk/data/Continuum/WES/vcf/NG-27280_CLTSS_LTS_001A_lib506241_7636_2_MarkedDup.bam; at htsjdk.samtools.util.IOUtil.assertFileIsReadable(IOUtil.java:483); at htsjdk.samtools.util.IOUtil.assertFileIsReadable(IOUtil.java:470); at picard.analysis.SinglePassSamProgram.makeItSo(SinglePassSamProgram.java:95); at picard.analysis.SinglePassSamProgram.doWork(SinglePassSamProgram.java:84); at picard.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:305); at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgramExecutor.instanceMain(PicardCommandLineProgramExecutor.java:25); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); at org.broadinstitute.hellbender.Main.main(Main.java:291); (gatk) root@34684eaa046e:/gatk/data/Continuum/WES/vcf# ls; GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz; (gatk) root@34684eaa046e:/gatk/data/Continuum/WES/vcf# ; ```; Please help me; Thanks",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8462
https://github.com/broadinstitute/gatk/pull/8464:2424,Deployability,release,release,2424,"will still provide the background default (or the built-in ploidy of 2 for humans), but the user input value will supersede these in overlapping regions. Note that the overlap is checked against the active region, meaning variants near the boundary of the `--ploidy-regions` file may end up with GT fields having ploidy slightly differently than expected, for example if your custom region overlaps a given active region but the variant ends up being written to a location outside that interval. In this case the ploidy from the user input would be used rather than any other default. # Implementation Details. The key idea is to allow `HaplotypeCallerEngine` to initialize multiple genotyping engines based on the `--ploidy-regions` input. The intervals are first parsed to check for positive integer ploidy values, and then used to create hashmaps of ploidy -> genotyper. The engine uses two types of genotypers: one for active region determination and one for doing the actual genotyping. Both admit a ploidy paramter passed via `hcArgs`. This PR modifies the `HaplotypeCallerArgumentCollection` class to include a method for creating copies of this object with differing ploidy amounts. These then get fed to the constructors of the appropriate genotyper classes, which are organized into two hashmaps. In every situation where one of these genotypers is used, we instead begin the scope by calling a ""get local genotyper"" method that performs the logic of checking whether the region of interest overlaps any of the user-provided regions, and then selects the appropriate `localEngine` genotyper for the task, ensuring the user-provided ploidy supersedes any other defaults. # A Note on Dependency. The flexibility of using either .bed or .interval_list files to specify this information depends on [this](https://github.com/samtools/htsjdk/pull/1680) PR in htsjdk being made into a full release, and then bumping the dependency of GATK. The code in this PR would not compile until this happens.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8464
https://github.com/broadinstitute/gatk/pull/8464:398,Integrability,interface,interface,398,"# Summary. This PR adds a new flag to `HaplotypeCaller` called `--ploidy-regions` which allows the user to input a .bed or .interval_list with ""name"" column equal to a positive integer for the ploidy to use when calling variants in that region. The main use case is for calling haploid variants outside the PAR for XY individuals as required by the VCF spec, but this provides a much more flexible interface for other similar niche applications, like genotyping individuals with other known aneuploidies. The global `-ploidy` flag will still provide the background default (or the built-in ploidy of 2 for humans), but the user input value will supersede these in overlapping regions. Note that the overlap is checked against the active region, meaning variants near the boundary of the `--ploidy-regions` file may end up with GT fields having ploidy slightly differently than expected, for example if your custom region overlaps a given active region but the variant ends up being written to a location outside that interval. In this case the ploidy from the user input would be used rather than any other default. # Implementation Details. The key idea is to allow `HaplotypeCallerEngine` to initialize multiple genotyping engines based on the `--ploidy-regions` input. The intervals are first parsed to check for positive integer ploidy values, and then used to create hashmaps of ploidy -> genotyper. The engine uses two types of genotypers: one for active region determination and one for doing the actual genotyping. Both admit a ploidy paramter passed via `hcArgs`. This PR modifies the `HaplotypeCallerArgumentCollection` class to include a method for creating copies of this object with differing ploidy amounts. These then get fed to the constructors of the appropriate genotyper classes, which are organized into two hashmaps. In every situation where one of these genotypers is used, we instead begin the scope by calling a ""get local genotyper"" method that performs the logic of checking ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8464
https://github.com/broadinstitute/gatk/pull/8464:2223,Integrability,Depend,Dependency,2223,"will still provide the background default (or the built-in ploidy of 2 for humans), but the user input value will supersede these in overlapping regions. Note that the overlap is checked against the active region, meaning variants near the boundary of the `--ploidy-regions` file may end up with GT fields having ploidy slightly differently than expected, for example if your custom region overlaps a given active region but the variant ends up being written to a location outside that interval. In this case the ploidy from the user input would be used rather than any other default. # Implementation Details. The key idea is to allow `HaplotypeCallerEngine` to initialize multiple genotyping engines based on the `--ploidy-regions` input. The intervals are first parsed to check for positive integer ploidy values, and then used to create hashmaps of ploidy -> genotyper. The engine uses two types of genotypers: one for active region determination and one for doing the actual genotyping. Both admit a ploidy paramter passed via `hcArgs`. This PR modifies the `HaplotypeCallerArgumentCollection` class to include a method for creating copies of this object with differing ploidy amounts. These then get fed to the constructors of the appropriate genotyper classes, which are organized into two hashmaps. In every situation where one of these genotypers is used, we instead begin the scope by calling a ""get local genotyper"" method that performs the logic of checking whether the region of interest overlaps any of the user-provided regions, and then selects the appropriate `localEngine` genotyper for the task, ensuring the user-provided ploidy supersedes any other defaults. # A Note on Dependency. The flexibility of using either .bed or .interval_list files to specify this information depends on [this](https://github.com/samtools/htsjdk/pull/1680) PR in htsjdk being made into a full release, and then bumping the dependency of GATK. The code in this PR would not compile until this happens.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8464
https://github.com/broadinstitute/gatk/pull/8464:2324,Integrability,depend,depends,2324,"will still provide the background default (or the built-in ploidy of 2 for humans), but the user input value will supersede these in overlapping regions. Note that the overlap is checked against the active region, meaning variants near the boundary of the `--ploidy-regions` file may end up with GT fields having ploidy slightly differently than expected, for example if your custom region overlaps a given active region but the variant ends up being written to a location outside that interval. In this case the ploidy from the user input would be used rather than any other default. # Implementation Details. The key idea is to allow `HaplotypeCallerEngine` to initialize multiple genotyping engines based on the `--ploidy-regions` input. The intervals are first parsed to check for positive integer ploidy values, and then used to create hashmaps of ploidy -> genotyper. The engine uses two types of genotypers: one for active region determination and one for doing the actual genotyping. Both admit a ploidy paramter passed via `hcArgs`. This PR modifies the `HaplotypeCallerArgumentCollection` class to include a method for creating copies of this object with differing ploidy amounts. These then get fed to the constructors of the appropriate genotyper classes, which are organized into two hashmaps. In every situation where one of these genotypers is used, we instead begin the scope by calling a ""get local genotyper"" method that performs the logic of checking whether the region of interest overlaps any of the user-provided regions, and then selects the appropriate `localEngine` genotyper for the task, ensuring the user-provided ploidy supersedes any other defaults. # A Note on Dependency. The flexibility of using either .bed or .interval_list files to specify this information depends on [this](https://github.com/samtools/htsjdk/pull/1680) PR in htsjdk being made into a full release, and then bumping the dependency of GATK. The code in this PR would not compile until this happens.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8464
https://github.com/broadinstitute/gatk/pull/8464:2454,Integrability,depend,dependency,2454,"will still provide the background default (or the built-in ploidy of 2 for humans), but the user input value will supersede these in overlapping regions. Note that the overlap is checked against the active region, meaning variants near the boundary of the `--ploidy-regions` file may end up with GT fields having ploidy slightly differently than expected, for example if your custom region overlaps a given active region but the variant ends up being written to a location outside that interval. In this case the ploidy from the user input would be used rather than any other default. # Implementation Details. The key idea is to allow `HaplotypeCallerEngine` to initialize multiple genotyping engines based on the `--ploidy-regions` input. The intervals are first parsed to check for positive integer ploidy values, and then used to create hashmaps of ploidy -> genotyper. The engine uses two types of genotypers: one for active region determination and one for doing the actual genotyping. Both admit a ploidy paramter passed via `hcArgs`. This PR modifies the `HaplotypeCallerArgumentCollection` class to include a method for creating copies of this object with differing ploidy amounts. These then get fed to the constructors of the appropriate genotyper classes, which are organized into two hashmaps. In every situation where one of these genotypers is used, we instead begin the scope by calling a ""get local genotyper"" method that performs the logic of checking whether the region of interest overlaps any of the user-provided regions, and then selects the appropriate `localEngine` genotyper for the task, ensuring the user-provided ploidy supersedes any other defaults. # A Note on Dependency. The flexibility of using either .bed or .interval_list files to specify this information depends on [this](https://github.com/samtools/htsjdk/pull/1680) PR in htsjdk being made into a full release, and then bumping the dependency of GATK. The code in this PR would not compile until this happens.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8464
https://github.com/broadinstitute/gatk/pull/8464:389,Modifiability,flexible,flexible,389,"# Summary. This PR adds a new flag to `HaplotypeCaller` called `--ploidy-regions` which allows the user to input a .bed or .interval_list with ""name"" column equal to a positive integer for the ploidy to use when calling variants in that region. The main use case is for calling haploid variants outside the PAR for XY individuals as required by the VCF spec, but this provides a much more flexible interface for other similar niche applications, like genotyping individuals with other known aneuploidies. The global `-ploidy` flag will still provide the background default (or the built-in ploidy of 2 for humans), but the user input value will supersede these in overlapping regions. Note that the overlap is checked against the active region, meaning variants near the boundary of the `--ploidy-regions` file may end up with GT fields having ploidy slightly differently than expected, for example if your custom region overlaps a given active region but the variant ends up being written to a location outside that interval. In this case the ploidy from the user input would be used rather than any other default. # Implementation Details. The key idea is to allow `HaplotypeCallerEngine` to initialize multiple genotyping engines based on the `--ploidy-regions` input. The intervals are first parsed to check for positive integer ploidy values, and then used to create hashmaps of ploidy -> genotyper. The engine uses two types of genotypers: one for active region determination and one for doing the actual genotyping. Both admit a ploidy paramter passed via `hcArgs`. This PR modifies the `HaplotypeCallerArgumentCollection` class to include a method for creating copies of this object with differing ploidy amounts. These then get fed to the constructors of the appropriate genotyper classes, which are organized into two hashmaps. In every situation where one of these genotypers is used, we instead begin the scope by calling a ""get local genotyper"" method that performs the logic of checking ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8464
https://github.com/broadinstitute/gatk/pull/8464:1970,Performance,perform,performs,1970,"will still provide the background default (or the built-in ploidy of 2 for humans), but the user input value will supersede these in overlapping regions. Note that the overlap is checked against the active region, meaning variants near the boundary of the `--ploidy-regions` file may end up with GT fields having ploidy slightly differently than expected, for example if your custom region overlaps a given active region but the variant ends up being written to a location outside that interval. In this case the ploidy from the user input would be used rather than any other default. # Implementation Details. The key idea is to allow `HaplotypeCallerEngine` to initialize multiple genotyping engines based on the `--ploidy-regions` input. The intervals are first parsed to check for positive integer ploidy values, and then used to create hashmaps of ploidy -> genotyper. The engine uses two types of genotypers: one for active region determination and one for doing the actual genotyping. Both admit a ploidy paramter passed via `hcArgs`. This PR modifies the `HaplotypeCallerArgumentCollection` class to include a method for creating copies of this object with differing ploidy amounts. These then get fed to the constructors of the appropriate genotyper classes, which are organized into two hashmaps. In every situation where one of these genotypers is used, we instead begin the scope by calling a ""get local genotyper"" method that performs the logic of checking whether the region of interest overlaps any of the user-provided regions, and then selects the appropriate `localEngine` genotyper for the task, ensuring the user-provided ploidy supersedes any other defaults. # A Note on Dependency. The flexibility of using either .bed or .interval_list files to specify this information depends on [this](https://github.com/samtools/htsjdk/pull/1680) PR in htsjdk being made into a full release, and then bumping the dependency of GATK. The code in this PR would not compile until this happens.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8464
https://github.com/broadinstitute/gatk/pull/8464:1372,Security,hash,hashmaps,1372,"ired by the VCF spec, but this provides a much more flexible interface for other similar niche applications, like genotyping individuals with other known aneuploidies. The global `-ploidy` flag will still provide the background default (or the built-in ploidy of 2 for humans), but the user input value will supersede these in overlapping regions. Note that the overlap is checked against the active region, meaning variants near the boundary of the `--ploidy-regions` file may end up with GT fields having ploidy slightly differently than expected, for example if your custom region overlaps a given active region but the variant ends up being written to a location outside that interval. In this case the ploidy from the user input would be used rather than any other default. # Implementation Details. The key idea is to allow `HaplotypeCallerEngine` to initialize multiple genotyping engines based on the `--ploidy-regions` input. The intervals are first parsed to check for positive integer ploidy values, and then used to create hashmaps of ploidy -> genotyper. The engine uses two types of genotypers: one for active region determination and one for doing the actual genotyping. Both admit a ploidy paramter passed via `hcArgs`. This PR modifies the `HaplotypeCallerArgumentCollection` class to include a method for creating copies of this object with differing ploidy amounts. These then get fed to the constructors of the appropriate genotyper classes, which are organized into two hashmaps. In every situation where one of these genotypers is used, we instead begin the scope by calling a ""get local genotyper"" method that performs the logic of checking whether the region of interest overlaps any of the user-provided regions, and then selects the appropriate `localEngine` genotyper for the task, ensuring the user-provided ploidy supersedes any other defaults. # A Note on Dependency. The flexibility of using either .bed or .interval_list files to specify this information depends on [th",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8464
https://github.com/broadinstitute/gatk/pull/8464:1828,Security,hash,hashmaps,1828,"will still provide the background default (or the built-in ploidy of 2 for humans), but the user input value will supersede these in overlapping regions. Note that the overlap is checked against the active region, meaning variants near the boundary of the `--ploidy-regions` file may end up with GT fields having ploidy slightly differently than expected, for example if your custom region overlaps a given active region but the variant ends up being written to a location outside that interval. In this case the ploidy from the user input would be used rather than any other default. # Implementation Details. The key idea is to allow `HaplotypeCallerEngine` to initialize multiple genotyping engines based on the `--ploidy-regions` input. The intervals are first parsed to check for positive integer ploidy values, and then used to create hashmaps of ploidy -> genotyper. The engine uses two types of genotypers: one for active region determination and one for doing the actual genotyping. Both admit a ploidy paramter passed via `hcArgs`. This PR modifies the `HaplotypeCallerArgumentCollection` class to include a method for creating copies of this object with differing ploidy amounts. These then get fed to the constructors of the appropriate genotyper classes, which are organized into two hashmaps. In every situation where one of these genotypers is used, we instead begin the scope by calling a ""get local genotyper"" method that performs the logic of checking whether the region of interest overlaps any of the user-provided regions, and then selects the appropriate `localEngine` genotyper for the task, ensuring the user-provided ploidy supersedes any other defaults. # A Note on Dependency. The flexibility of using either .bed or .interval_list files to specify this information depends on [this](https://github.com/samtools/htsjdk/pull/1680) PR in htsjdk being made into a full release, and then bumping the dependency of GATK. The code in this PR would not compile until this happens.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8464
https://github.com/broadinstitute/gatk/pull/8464:1983,Testability,log,logic,1983,"will still provide the background default (or the built-in ploidy of 2 for humans), but the user input value will supersede these in overlapping regions. Note that the overlap is checked against the active region, meaning variants near the boundary of the `--ploidy-regions` file may end up with GT fields having ploidy slightly differently than expected, for example if your custom region overlaps a given active region but the variant ends up being written to a location outside that interval. In this case the ploidy from the user input would be used rather than any other default. # Implementation Details. The key idea is to allow `HaplotypeCallerEngine` to initialize multiple genotyping engines based on the `--ploidy-regions` input. The intervals are first parsed to check for positive integer ploidy values, and then used to create hashmaps of ploidy -> genotyper. The engine uses two types of genotypers: one for active region determination and one for doing the actual genotyping. Both admit a ploidy paramter passed via `hcArgs`. This PR modifies the `HaplotypeCallerArgumentCollection` class to include a method for creating copies of this object with differing ploidy amounts. These then get fed to the constructors of the appropriate genotyper classes, which are organized into two hashmaps. In every situation where one of these genotypers is used, we instead begin the scope by calling a ""get local genotyper"" method that performs the logic of checking whether the region of interest overlaps any of the user-provided regions, and then selects the appropriate `localEngine` genotyper for the task, ensuring the user-provided ploidy supersedes any other defaults. # A Note on Dependency. The flexibility of using either .bed or .interval_list files to specify this information depends on [this](https://github.com/samtools/htsjdk/pull/1680) PR in htsjdk being made into a full release, and then bumping the dependency of GATK. The code in this PR would not compile until this happens.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8464
https://github.com/broadinstitute/gatk/pull/8466:13,Deployability,release,release,13,Added PDF of release note about VETS in the context of GVS. Updated documentation to reference VETS and the relevant GATK tools instead of VQSR. Updated png of gvs diagram to be more generic and not mention VQSR.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8466
https://github.com/broadinstitute/gatk/pull/8466:60,Deployability,Update,Updated,60,Added PDF of release note about VETS in the context of GVS. Updated documentation to reference VETS and the relevant GATK tools instead of VQSR. Updated png of gvs diagram to be more generic and not mention VQSR.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8466
https://github.com/broadinstitute/gatk/pull/8466:145,Deployability,Update,Updated,145,Added PDF of release note about VETS in the context of GVS. Updated documentation to reference VETS and the relevant GATK tools instead of VQSR. Updated png of gvs diagram to be more generic and not mention VQSR.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8466
https://github.com/broadinstitute/gatk/pull/8470:24,Deployability,release,release,24,"@meganshand, the [1.5.1 release](https://github.com/GenomicsDB/GenomicsDB/releases/tag/v1.5.1) should contain all the changes we made for azure to support your use case. There is no need to use `TILEDB_NUM_THREADS=1` env anymore as that is the default now.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8470
https://github.com/broadinstitute/gatk/pull/8470:74,Deployability,release,releases,74,"@meganshand, the [1.5.1 release](https://github.com/GenomicsDB/GenomicsDB/releases/tag/v1.5.1) should contain all the changes we made for azure to support your use case. There is no need to use `TILEDB_NUM_THREADS=1` env anymore as that is the default now.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8470
https://github.com/broadinstitute/gatk/pull/8474:0,Deployability,Integrat,Integration,0,Integration run in progress https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/474c06f0-1c6a-41d0-bc1f-7a22054153fe,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8474
https://github.com/broadinstitute/gatk/pull/8474:0,Integrability,Integrat,Integration,0,Integration run in progress https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/474c06f0-1c6a-41d0-bc1f-7a22054153fe,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8474
https://github.com/broadinstitute/gatk/issues/8476:664,Availability,Error,Error,664,"GATK version: 4.4.0.0. Crashing in FilterAlignmentArtifacts. Not clear why. Command; ```; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx30g -jar /gpfs/data/lab/bin/gatk/gatk-package-4.4.0.0-local.jar FilterAlignmentArtifacts -R /gpfs/data/lab/reference-files/hg38-gatk/Homo_sapiens_assembly38.fasta -V 60603-bulk.filtered.vcf.gz -I /gpfs/data/lab/projects/Mini/analysis/STR/60603-bulk_results/60603-bulk.cram --bwa-mem-index-image /gpfs/data/lab/reference-files/hg38-gatk/Homo_sapiens_assembly38.fasta.img -O 60603-bulk.filtered.FAA.vcf.gz; ```. Error:; ```; 11:02:16.087 INFO ProgressMeter - chrX:144247387 619.0 145000 234.3; 11:05:08.297 WARN IntelInflater - Zero Bytes Written : 0; 12:29:39.297 INFO FilterAlignmentArtifacts - Shutting down engine; [August 15, 2023 at 12:29:39 PM EDT] org.broadinstitute.hellbender.tools.walkers.realignmentfilter.FilterAlignmentArtifacts done. Elapsed time: 710.24 minutes.; Runtime.totalMemory()=4345298944; java.lang.IllegalStateException: Padded span must contain active span.; at org.broadinstitute.hellbender.utils.Utils.validate(Utils.java:814); at org.broadinstitute.hellbender.engine.AssemblyRegion.<init>(AssemblyRegion.java:109); at org.broadinstitute.hellbender.engine.AssemblyRegion.<init>(AssemblyRegion.java:85); at org.broadinstitute.hellbender.engine.AssemblyRegion.<init>(AssemblyRegion.java:120); at org.broadinstitute.hellbender.tools.walkers.realignmentfilter.FilterAlignmentArtifacts.makeAssemblyRegionFromVariantReads(FilterAlignmentArtifacts.java:280); at org.broadinstitute.hellbender.tools.walkers.realignmentfilter.FilterAlignmentArtifacts.apply(FilterAlignmentArtifacts.java:212); at org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart.apply(MultiVariantWalkerGroupedOnStart.java:133); at org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart.afterTraverse(MultiVariantWalkerGroupedO",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8476
https://github.com/broadinstitute/gatk/issues/8476:858,Availability,down,down,858,"GATK version: 4.4.0.0. Crashing in FilterAlignmentArtifacts. Not clear why. Command; ```; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx30g -jar /gpfs/data/lab/bin/gatk/gatk-package-4.4.0.0-local.jar FilterAlignmentArtifacts -R /gpfs/data/lab/reference-files/hg38-gatk/Homo_sapiens_assembly38.fasta -V 60603-bulk.filtered.vcf.gz -I /gpfs/data/lab/projects/Mini/analysis/STR/60603-bulk_results/60603-bulk.cram --bwa-mem-index-image /gpfs/data/lab/reference-files/hg38-gatk/Homo_sapiens_assembly38.fasta.img -O 60603-bulk.filtered.FAA.vcf.gz; ```. Error:; ```; 11:02:16.087 INFO ProgressMeter - chrX:144247387 619.0 145000 234.3; 11:05:08.297 WARN IntelInflater - Zero Bytes Written : 0; 12:29:39.297 INFO FilterAlignmentArtifacts - Shutting down engine; [August 15, 2023 at 12:29:39 PM EDT] org.broadinstitute.hellbender.tools.walkers.realignmentfilter.FilterAlignmentArtifacts done. Elapsed time: 710.24 minutes.; Runtime.totalMemory()=4345298944; java.lang.IllegalStateException: Padded span must contain active span.; at org.broadinstitute.hellbender.utils.Utils.validate(Utils.java:814); at org.broadinstitute.hellbender.engine.AssemblyRegion.<init>(AssemblyRegion.java:109); at org.broadinstitute.hellbender.engine.AssemblyRegion.<init>(AssemblyRegion.java:85); at org.broadinstitute.hellbender.engine.AssemblyRegion.<init>(AssemblyRegion.java:120); at org.broadinstitute.hellbender.tools.walkers.realignmentfilter.FilterAlignmentArtifacts.makeAssemblyRegionFromVariantReads(FilterAlignmentArtifacts.java:280); at org.broadinstitute.hellbender.tools.walkers.realignmentfilter.FilterAlignmentArtifacts.apply(FilterAlignmentArtifacts.java:212); at org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart.apply(MultiVariantWalkerGroupedOnStart.java:133); at org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart.afterTraverse(MultiVariantWalkerGroupedO",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8476
https://github.com/broadinstitute/gatk/issues/8476:1183,Security,validat,validate,1183,"e_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx30g -jar /gpfs/data/lab/bin/gatk/gatk-package-4.4.0.0-local.jar FilterAlignmentArtifacts -R /gpfs/data/lab/reference-files/hg38-gatk/Homo_sapiens_assembly38.fasta -V 60603-bulk.filtered.vcf.gz -I /gpfs/data/lab/projects/Mini/analysis/STR/60603-bulk_results/60603-bulk.cram --bwa-mem-index-image /gpfs/data/lab/reference-files/hg38-gatk/Homo_sapiens_assembly38.fasta.img -O 60603-bulk.filtered.FAA.vcf.gz; ```. Error:; ```; 11:02:16.087 INFO ProgressMeter - chrX:144247387 619.0 145000 234.3; 11:05:08.297 WARN IntelInflater - Zero Bytes Written : 0; 12:29:39.297 INFO FilterAlignmentArtifacts - Shutting down engine; [August 15, 2023 at 12:29:39 PM EDT] org.broadinstitute.hellbender.tools.walkers.realignmentfilter.FilterAlignmentArtifacts done. Elapsed time: 710.24 minutes.; Runtime.totalMemory()=4345298944; java.lang.IllegalStateException: Padded span must contain active span.; at org.broadinstitute.hellbender.utils.Utils.validate(Utils.java:814); at org.broadinstitute.hellbender.engine.AssemblyRegion.<init>(AssemblyRegion.java:109); at org.broadinstitute.hellbender.engine.AssemblyRegion.<init>(AssemblyRegion.java:85); at org.broadinstitute.hellbender.engine.AssemblyRegion.<init>(AssemblyRegion.java:120); at org.broadinstitute.hellbender.tools.walkers.realignmentfilter.FilterAlignmentArtifacts.makeAssemblyRegionFromVariantReads(FilterAlignmentArtifacts.java:280); at org.broadinstitute.hellbender.tools.walkers.realignmentfilter.FilterAlignmentArtifacts.apply(FilterAlignmentArtifacts.java:212); at org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart.apply(MultiVariantWalkerGroupedOnStart.java:133); at org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart.afterTraverse(MultiVariantWalkerGroupedOnStart.java:193); at org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart.traverse(MultiVariantWalkerGroupedOnStart.java:166); at org.broadinstitute.hellbender.engine.GATKTo",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8476
https://github.com/broadinstitute/gatk/issues/8476:65,Usability,clear,clear,65,"GATK version: 4.4.0.0. Crashing in FilterAlignmentArtifacts. Not clear why. Command; ```; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx30g -jar /gpfs/data/lab/bin/gatk/gatk-package-4.4.0.0-local.jar FilterAlignmentArtifacts -R /gpfs/data/lab/reference-files/hg38-gatk/Homo_sapiens_assembly38.fasta -V 60603-bulk.filtered.vcf.gz -I /gpfs/data/lab/projects/Mini/analysis/STR/60603-bulk_results/60603-bulk.cram --bwa-mem-index-image /gpfs/data/lab/reference-files/hg38-gatk/Homo_sapiens_assembly38.fasta.img -O 60603-bulk.filtered.FAA.vcf.gz; ```. Error:; ```; 11:02:16.087 INFO ProgressMeter - chrX:144247387 619.0 145000 234.3; 11:05:08.297 WARN IntelInflater - Zero Bytes Written : 0; 12:29:39.297 INFO FilterAlignmentArtifacts - Shutting down engine; [August 15, 2023 at 12:29:39 PM EDT] org.broadinstitute.hellbender.tools.walkers.realignmentfilter.FilterAlignmentArtifacts done. Elapsed time: 710.24 minutes.; Runtime.totalMemory()=4345298944; java.lang.IllegalStateException: Padded span must contain active span.; at org.broadinstitute.hellbender.utils.Utils.validate(Utils.java:814); at org.broadinstitute.hellbender.engine.AssemblyRegion.<init>(AssemblyRegion.java:109); at org.broadinstitute.hellbender.engine.AssemblyRegion.<init>(AssemblyRegion.java:85); at org.broadinstitute.hellbender.engine.AssemblyRegion.<init>(AssemblyRegion.java:120); at org.broadinstitute.hellbender.tools.walkers.realignmentfilter.FilterAlignmentArtifacts.makeAssemblyRegionFromVariantReads(FilterAlignmentArtifacts.java:280); at org.broadinstitute.hellbender.tools.walkers.realignmentfilter.FilterAlignmentArtifacts.apply(FilterAlignmentArtifacts.java:212); at org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart.apply(MultiVariantWalkerGroupedOnStart.java:133); at org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart.afterTraverse(MultiVariantWalkerGroupedO",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8476
https://github.com/broadinstitute/gatk/issues/8477:612,Availability,ERROR,ERROR,612,"Thanks a lot. I am trying to run `mutect2` in **tumour-only** mode for which I need a panel of normal (PON). I have tried **somatic-hg38_1000g_pon.hg38.vcf.vcf** which gives . `./gatk Mutect2 -R resources_broad_hg38_v0_Homo_sapiens_assembly38.fasta -I /data/Continuum/WES/testAlignmentBROADGenome/results/NG-27280_CLTSS_LTS_001A_lib506241_7636_2_MarkedDup_PicMD.bam -O 3.mt2.vcf -tumor NG-27280_CLTSS_LTS_001A_lib506241_7636_2_MarkedDup_PicMD.targeted_sequencing.sample_name --af-of-alleles-not-in-resource 2.5e-06 --germline-resource af-only-gnomad.hg38.vcf.gz -pon somatic-hg38_1000g_pon.hg38.vcf.vcf`. A USER ERROR has occurred: Cannot read file:///data/somatic-hg38_1000g_pon.hg38.vcf.vcf because no suitable codecs found. I know **gatk4_mutect2_4136_pon.vcf.gz** locates here for which I should register in GDC but because I am a postdoctoral researcher, I can not register. [1]: https://gdc.cancer.gov/about-data/gdc-data-processing/gdc-reference-files. Could you please help me to run mutect2 in tumour-only mode using another publicly available PON?. Thanks for any help",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8477
https://github.com/broadinstitute/gatk/issues/8477:1043,Availability,avail,available,1043,"Thanks a lot. I am trying to run `mutect2` in **tumour-only** mode for which I need a panel of normal (PON). I have tried **somatic-hg38_1000g_pon.hg38.vcf.vcf** which gives . `./gatk Mutect2 -R resources_broad_hg38_v0_Homo_sapiens_assembly38.fasta -I /data/Continuum/WES/testAlignmentBROADGenome/results/NG-27280_CLTSS_LTS_001A_lib506241_7636_2_MarkedDup_PicMD.bam -O 3.mt2.vcf -tumor NG-27280_CLTSS_LTS_001A_lib506241_7636_2_MarkedDup_PicMD.targeted_sequencing.sample_name --af-of-alleles-not-in-resource 2.5e-06 --germline-resource af-only-gnomad.hg38.vcf.gz -pon somatic-hg38_1000g_pon.hg38.vcf.vcf`. A USER ERROR has occurred: Cannot read file:///data/somatic-hg38_1000g_pon.hg38.vcf.vcf because no suitable codecs found. I know **gatk4_mutect2_4136_pon.vcf.gz** locates here for which I should register in GDC but because I am a postdoctoral researcher, I can not register. [1]: https://gdc.cancer.gov/about-data/gdc-data-processing/gdc-reference-files. Could you please help me to run mutect2 in tumour-only mode using another publicly available PON?. Thanks for any help",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8477
https://github.com/broadinstitute/gatk/issues/8477:272,Testability,test,testAlignmentBROADGenome,272,"Thanks a lot. I am trying to run `mutect2` in **tumour-only** mode for which I need a panel of normal (PON). I have tried **somatic-hg38_1000g_pon.hg38.vcf.vcf** which gives . `./gatk Mutect2 -R resources_broad_hg38_v0_Homo_sapiens_assembly38.fasta -I /data/Continuum/WES/testAlignmentBROADGenome/results/NG-27280_CLTSS_LTS_001A_lib506241_7636_2_MarkedDup_PicMD.bam -O 3.mt2.vcf -tumor NG-27280_CLTSS_LTS_001A_lib506241_7636_2_MarkedDup_PicMD.targeted_sequencing.sample_name --af-of-alleles-not-in-resource 2.5e-06 --germline-resource af-only-gnomad.hg38.vcf.gz -pon somatic-hg38_1000g_pon.hg38.vcf.vcf`. A USER ERROR has occurred: Cannot read file:///data/somatic-hg38_1000g_pon.hg38.vcf.vcf because no suitable codecs found. I know **gatk4_mutect2_4136_pon.vcf.gz** locates here for which I should register in GDC but because I am a postdoctoral researcher, I can not register. [1]: https://gdc.cancer.gov/about-data/gdc-data-processing/gdc-reference-files. Could you please help me to run mutect2 in tumour-only mode using another publicly available PON?. Thanks for any help",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8477
https://github.com/broadinstitute/gatk/issues/8482:445,Modifiability,rewrite,rewrite,445,"In working on #8296 we have discovered that in the `MafOutputRendererConstants.java` there are myriad constants that hard code aliases with the pattern ""Gencode_34_hugoSymbol"". This can lead to bad behavior if a non-bundled Gencode version is used in Funcotator, specifically it can cause the MAF file to be missing most of its hard-coded fields as they will be mis-identified by the output-renderer resulting in mostly empty outputs. We should rewrite the logic in the MAF code to be completely agnostic to Gencode versions used to generate the Funcotations to drop this hard-coding all-together.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8482
https://github.com/broadinstitute/gatk/issues/8482:457,Testability,log,logic,457,"In working on #8296 we have discovered that in the `MafOutputRendererConstants.java` there are myriad constants that hard code aliases with the pattern ""Gencode_34_hugoSymbol"". This can lead to bad behavior if a non-bundled Gencode version is used in Funcotator, specifically it can cause the MAF file to be missing most of its hard-coded fields as they will be mis-identified by the output-renderer resulting in mostly empty outputs. We should rewrite the logic in the MAF code to be completely agnostic to Gencode versions used to generate the Funcotations to drop this hard-coding all-together.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8482
https://github.com/broadinstitute/gatk/pull/8483:25,Performance,optimiz,optimization,25,Added the following Adam optimization / learning parameters to the command-line:. - learning rate; - beta1; - beta2; - epsilon; - clipnorm,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8483
https://github.com/broadinstitute/gatk/pull/8483:40,Usability,learn,learning,40,Added the following Adam optimization / learning parameters to the command-line:. - learning rate; - beta1; - beta2; - epsilon; - clipnorm,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8483
https://github.com/broadinstitute/gatk/pull/8483:84,Usability,learn,learning,84,Added the following Adam optimization / learning parameters to the command-line:. - learning rate; - beta1; - beta2; - epsilon; - clipnorm,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8483
https://github.com/broadinstitute/gatk/pull/8487:11,Availability,error,errors,11,Fix naming errors in script and added the ability to specify the temp directory for Hail.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8487
https://github.com/broadinstitute/gatk/pull/8488:544,Availability,failure,failure,544,"successful run:; https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20cremer/job_history/649ee4c9-1afc-473b-b460-2fc88d5f49d4. failing run with the bug:; https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20cremer/job_history/f1c952fc-7f05-4468-ae20-1c1cc5b9bf38. AC is:. Cohort builder subcohort extract in AoU and our extract workflow work with both VETS and VQSR callsets, including past callsets. (Note--I did not test in AoU, just on quickstart since the issue doesn't seem to be permission or scale related--see failure reproduced above). Full extract with past callset & VQSR; https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20cremer/job_history/649ee4c9-1afc-473b-b460-2fc88d5f49d4. Subcohort extract with past callset & VQSR. Full extract with new callset & VETS; https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/50ef3073-f618-42ee-b207-73712a783a8a; (note this failed but only on one of the 4 runs and it's based on query cost). <img width=""1202"" alt=""Screenshot 2023-08-25 at 1 22 58 PM"" src=""https://github.com/broadinstitute/gatk/assets/6863459/39468ed8-fe2b-4bf8-9326-3bfcf6dabbb1"">. Kevin is able to run latest extract on Delta (still waiting on Kevin, but otherwise the above are all set). note that there was briefly no ""score"" col but I dont _think_ we need to be backwards compatible for that as there was no release",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8488
https://github.com/broadinstitute/gatk/pull/8488:1393,Deployability,release,release,1393,"successful run:; https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20cremer/job_history/649ee4c9-1afc-473b-b460-2fc88d5f49d4. failing run with the bug:; https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20cremer/job_history/f1c952fc-7f05-4468-ae20-1c1cc5b9bf38. AC is:. Cohort builder subcohort extract in AoU and our extract workflow work with both VETS and VQSR callsets, including past callsets. (Note--I did not test in AoU, just on quickstart since the issue doesn't seem to be permission or scale related--see failure reproduced above). Full extract with past callset & VQSR; https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20cremer/job_history/649ee4c9-1afc-473b-b460-2fc88d5f49d4. Subcohort extract with past callset & VQSR. Full extract with new callset & VETS; https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/50ef3073-f618-42ee-b207-73712a783a8a; (note this failed but only on one of the 4 runs and it's based on query cost). <img width=""1202"" alt=""Screenshot 2023-08-25 at 1 22 58 PM"" src=""https://github.com/broadinstitute/gatk/assets/6863459/39468ed8-fe2b-4bf8-9326-3bfcf6dabbb1"">. Kevin is able to run latest extract on Delta (still waiting on Kevin, but otherwise the above are all set). note that there was briefly no ""score"" col but I dont _think_ we need to be backwards compatible for that as there was no release",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8488
https://github.com/broadinstitute/gatk/pull/8488:444,Testability,test,test,444,"successful run:; https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20cremer/job_history/649ee4c9-1afc-473b-b460-2fc88d5f49d4. failing run with the bug:; https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20cremer/job_history/f1c952fc-7f05-4468-ae20-1c1cc5b9bf38. AC is:. Cohort builder subcohort extract in AoU and our extract workflow work with both VETS and VQSR callsets, including past callsets. (Note--I did not test in AoU, just on quickstart since the issue doesn't seem to be permission or scale related--see failure reproduced above). Full extract with past callset & VQSR; https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20cremer/job_history/649ee4c9-1afc-473b-b460-2fc88d5f49d4. Subcohort extract with past callset & VQSR. Full extract with new callset & VETS; https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/50ef3073-f618-42ee-b207-73712a783a8a; (note this failed but only on one of the 4 runs and it's based on query cost). <img width=""1202"" alt=""Screenshot 2023-08-25 at 1 22 58 PM"" src=""https://github.com/broadinstitute/gatk/assets/6863459/39468ed8-fe2b-4bf8-9326-3bfcf6dabbb1"">. Kevin is able to run latest extract on Delta (still waiting on Kevin, but otherwise the above are all set). note that there was briefly no ""score"" col but I dont _think_ we need to be backwards compatible for that as there was no release",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8488
https://github.com/broadinstitute/gatk/pull/8489:154,Deployability,patch,patch,154,Recent refactoring seems to have introduced a bug in pileup mode that failed to enforce the limit on the number of haplotypes to be considered. With this patch:. - HaplotypeCaller once again respects the limit on haplotypes before genotyping.; - Changed some `HashSet`s to `LinkedHashSets` to preserve determinism.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8489
https://github.com/broadinstitute/gatk/pull/8489:7,Modifiability,refactor,refactoring,7,Recent refactoring seems to have introduced a bug in pileup mode that failed to enforce the limit on the number of haplotypes to be considered. With this patch:. - HaplotypeCaller once again respects the limit on haplotypes before genotyping.; - Changed some `HashSet`s to `LinkedHashSets` to preserve determinism.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8489
https://github.com/broadinstitute/gatk/pull/8489:260,Security,Hash,HashSet,260,Recent refactoring seems to have introduced a bug in pileup mode that failed to enforce the limit on the number of haplotypes to be considered. With this patch:. - HaplotypeCaller once again respects the limit on haplotypes before genotyping.; - Changed some `HashSet`s to `LinkedHashSets` to preserve determinism.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8489
https://github.com/broadinstitute/gatk/issues/8490:146,Availability,error,error,146,"Hi ; This is an issue derived from the GATK forum. . https://gatk.broadinstitute.org/hc/en-us/community/posts/17996976455067-HaplotypecallerSpark-error?page=1#community_comment_18161302676123. User is having an issue with HaplotypeCallerSpark possibly a traversal issue. This problem was also present in the past in another github issue. https://github.com/broadinstitute/gatk/issues/7199. User tried with the latest version as well and even providing a bed file as in the former issue did not help solve the problem. HaplotypeCaller works fine with or without the bed file but Spark version does not. . Here is the error message. ```; **Caused by: java.lang.IllegalArgumentException: Sequence [VC HC_call @ NW_020555792.1:138 Q78.32 of type=SNP alleles=[T*, G] attr={AC=2, AF=1.0, AN=2, DP=2, ExcessHet=0.0000, FS=0.000, MLEAC=[1], MLEAF=[0.5], MQ=47.00, QD=29.09, SOR=0.693} GT=[[AR0111 G/G GQ 6 DP 2 AD 0,2 PL 90,6,0]] filters= added out of order currentReferenceIndex: 9, referenceIndex:11; at htsjdk.tribble.index.tabix.AllRefsTabixIndexCreator.addFeature(AllRefsTabixIndexCreator.java:79); at htsjdk.variant.variantcontext.writer.IndexingVariantContextWriter.add(IndexingVariantContextWriter.java:203); at htsjdk.variant.variantcontext.writer.VCFWriter.add(VCFWriter.java:242); at org.disq_bio.disq.impl.formats.vcf.HeaderlessVcfOutputFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.java:93); at org.disq_bio.disq.impl.formats.vcf.HeaderlessVcfOutputFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.java:56); at org.apache.spark.internal.io.HadoopMapReduceWriteConfigUtil.write(SparkHadoopWriter.scala:368); at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:138); at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538); at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:135); ... 9 more; 01:50:24.007 INFO ShutdownHookManager - Shutdown hook called**; ```; User wa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8490
https://github.com/broadinstitute/gatk/issues/8490:616,Availability,error,error,616,"Hi ; This is an issue derived from the GATK forum. . https://gatk.broadinstitute.org/hc/en-us/community/posts/17996976455067-HaplotypecallerSpark-error?page=1#community_comment_18161302676123. User is having an issue with HaplotypeCallerSpark possibly a traversal issue. This problem was also present in the past in another github issue. https://github.com/broadinstitute/gatk/issues/7199. User tried with the latest version as well and even providing a bed file as in the former issue did not help solve the problem. HaplotypeCaller works fine with or without the bed file but Spark version does not. . Here is the error message. ```; **Caused by: java.lang.IllegalArgumentException: Sequence [VC HC_call @ NW_020555792.1:138 Q78.32 of type=SNP alleles=[T*, G] attr={AC=2, AF=1.0, AN=2, DP=2, ExcessHet=0.0000, FS=0.000, MLEAC=[1], MLEAF=[0.5], MQ=47.00, QD=29.09, SOR=0.693} GT=[[AR0111 G/G GQ 6 DP 2 AD 0,2 PL 90,6,0]] filters= added out of order currentReferenceIndex: 9, referenceIndex:11; at htsjdk.tribble.index.tabix.AllRefsTabixIndexCreator.addFeature(AllRefsTabixIndexCreator.java:79); at htsjdk.variant.variantcontext.writer.IndexingVariantContextWriter.add(IndexingVariantContextWriter.java:203); at htsjdk.variant.variantcontext.writer.VCFWriter.add(VCFWriter.java:242); at org.disq_bio.disq.impl.formats.vcf.HeaderlessVcfOutputFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.java:93); at org.disq_bio.disq.impl.formats.vcf.HeaderlessVcfOutputFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.java:56); at org.apache.spark.internal.io.HadoopMapReduceWriteConfigUtil.write(SparkHadoopWriter.scala:368); at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:138); at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538); at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:135); ... 9 more; 01:50:24.007 INFO ShutdownHookManager - Shutdown hook called**; ```; User wa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8490
https://github.com/broadinstitute/gatk/issues/8490:622,Integrability,message,message,622,"Hi ; This is an issue derived from the GATK forum. . https://gatk.broadinstitute.org/hc/en-us/community/posts/17996976455067-HaplotypecallerSpark-error?page=1#community_comment_18161302676123. User is having an issue with HaplotypeCallerSpark possibly a traversal issue. This problem was also present in the past in another github issue. https://github.com/broadinstitute/gatk/issues/7199. User tried with the latest version as well and even providing a bed file as in the former issue did not help solve the problem. HaplotypeCaller works fine with or without the bed file but Spark version does not. . Here is the error message. ```; **Caused by: java.lang.IllegalArgumentException: Sequence [VC HC_call @ NW_020555792.1:138 Q78.32 of type=SNP alleles=[T*, G] attr={AC=2, AF=1.0, AN=2, DP=2, ExcessHet=0.0000, FS=0.000, MLEAC=[1], MLEAF=[0.5], MQ=47.00, QD=29.09, SOR=0.693} GT=[[AR0111 G/G GQ 6 DP 2 AD 0,2 PL 90,6,0]] filters= added out of order currentReferenceIndex: 9, referenceIndex:11; at htsjdk.tribble.index.tabix.AllRefsTabixIndexCreator.addFeature(AllRefsTabixIndexCreator.java:79); at htsjdk.variant.variantcontext.writer.IndexingVariantContextWriter.add(IndexingVariantContextWriter.java:203); at htsjdk.variant.variantcontext.writer.VCFWriter.add(VCFWriter.java:242); at org.disq_bio.disq.impl.formats.vcf.HeaderlessVcfOutputFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.java:93); at org.disq_bio.disq.impl.formats.vcf.HeaderlessVcfOutputFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.java:56); at org.apache.spark.internal.io.HadoopMapReduceWriteConfigUtil.write(SparkHadoopWriter.scala:368); at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:138); at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538); at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:135); ... 9 more; 01:50:24.007 INFO ShutdownHookManager - Shutdown hook called**; ```; User wa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8490
https://github.com/broadinstitute/gatk/issues/8494:687,Availability,redundant,redundant,687,"The recent branch #8489 has demonstrated that there are some problematic edge cases in the pileup allele merging code that could cause pathological numbers of haplotypes to be handed to the genotyper. In updating the bug in that branch it was observed that it is very common that there are score ties at the 5th haplotype level for the pileupcaller as illustrated by the noise in the updated tests. This algorithm is not a good heuristic and we should replace it with something better, some ideas from that branch that might fix a few of its shortcomings:. 1) Increase/decouple the kmer size used with the reads from the assembly graph kmer size to prevent the filtering step from being redundant with assembly; 2) Normalize the scores to the haplotype lengths to deal with haplotype size bias.; 3) Change the scores to instead reflect the absolute count of unsupported kmers from the graph to also deal with hapotype size bias. ; 4) Iteratively expand the kmer size used for filtering to pare down the number of haplotypes in a more principled fashion.; 5) Utilize the read kmer occurrence counts to construct the scores in order to reduce the risk of spurious reads being sufficient support for a given haplotype. . We have observed that there can be significant changes to the actual genotyping engine output from the pileup engine from even relatively minor changes to the pileupcalling merging code. We should strive to find a more principled solution for merging haplotypes than the one we have currently.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8494
https://github.com/broadinstitute/gatk/issues/8494:994,Availability,down,down,994,"The recent branch #8489 has demonstrated that there are some problematic edge cases in the pileup allele merging code that could cause pathological numbers of haplotypes to be handed to the genotyper. In updating the bug in that branch it was observed that it is very common that there are score ties at the 5th haplotype level for the pileupcaller as illustrated by the noise in the updated tests. This algorithm is not a good heuristic and we should replace it with something better, some ideas from that branch that might fix a few of its shortcomings:. 1) Increase/decouple the kmer size used with the reads from the assembly graph kmer size to prevent the filtering step from being redundant with assembly; 2) Normalize the scores to the haplotype lengths to deal with haplotype size bias.; 3) Change the scores to instead reflect the absolute count of unsupported kmers from the graph to also deal with hapotype size bias. ; 4) Iteratively expand the kmer size used for filtering to pare down the number of haplotypes in a more principled fashion.; 5) Utilize the read kmer occurrence counts to construct the scores in order to reduce the risk of spurious reads being sufficient support for a given haplotype. . We have observed that there can be significant changes to the actual genotyping engine output from the pileup engine from even relatively minor changes to the pileupcalling merging code. We should strive to find a more principled solution for merging haplotypes than the one we have currently.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8494
https://github.com/broadinstitute/gatk/issues/8494:384,Deployability,update,updated,384,"The recent branch #8489 has demonstrated that there are some problematic edge cases in the pileup allele merging code that could cause pathological numbers of haplotypes to be handed to the genotyper. In updating the bug in that branch it was observed that it is very common that there are score ties at the 5th haplotype level for the pileupcaller as illustrated by the noise in the updated tests. This algorithm is not a good heuristic and we should replace it with something better, some ideas from that branch that might fix a few of its shortcomings:. 1) Increase/decouple the kmer size used with the reads from the assembly graph kmer size to prevent the filtering step from being redundant with assembly; 2) Normalize the scores to the haplotype lengths to deal with haplotype size bias.; 3) Change the scores to instead reflect the absolute count of unsupported kmers from the graph to also deal with hapotype size bias. ; 4) Iteratively expand the kmer size used for filtering to pare down the number of haplotypes in a more principled fashion.; 5) Utilize the read kmer occurrence counts to construct the scores in order to reduce the risk of spurious reads being sufficient support for a given haplotype. . We have observed that there can be significant changes to the actual genotyping engine output from the pileup engine from even relatively minor changes to the pileupcalling merging code. We should strive to find a more principled solution for merging haplotypes than the one we have currently.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8494
https://github.com/broadinstitute/gatk/issues/8494:1134,Energy Efficiency,reduce,reduce,1134,"The recent branch #8489 has demonstrated that there are some problematic edge cases in the pileup allele merging code that could cause pathological numbers of haplotypes to be handed to the genotyper. In updating the bug in that branch it was observed that it is very common that there are score ties at the 5th haplotype level for the pileupcaller as illustrated by the noise in the updated tests. This algorithm is not a good heuristic and we should replace it with something better, some ideas from that branch that might fix a few of its shortcomings:. 1) Increase/decouple the kmer size used with the reads from the assembly graph kmer size to prevent the filtering step from being redundant with assembly; 2) Normalize the scores to the haplotype lengths to deal with haplotype size bias.; 3) Change the scores to instead reflect the absolute count of unsupported kmers from the graph to also deal with hapotype size bias. ; 4) Iteratively expand the kmer size used for filtering to pare down the number of haplotypes in a more principled fashion.; 5) Utilize the read kmer occurrence counts to construct the scores in order to reduce the risk of spurious reads being sufficient support for a given haplotype. . We have observed that there can be significant changes to the actual genotyping engine output from the pileup engine from even relatively minor changes to the pileupcalling merging code. We should strive to find a more principled solution for merging haplotypes than the one we have currently.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8494
https://github.com/broadinstitute/gatk/issues/8494:687,Safety,redund,redundant,687,"The recent branch #8489 has demonstrated that there are some problematic edge cases in the pileup allele merging code that could cause pathological numbers of haplotypes to be handed to the genotyper. In updating the bug in that branch it was observed that it is very common that there are score ties at the 5th haplotype level for the pileupcaller as illustrated by the noise in the updated tests. This algorithm is not a good heuristic and we should replace it with something better, some ideas from that branch that might fix a few of its shortcomings:. 1) Increase/decouple the kmer size used with the reads from the assembly graph kmer size to prevent the filtering step from being redundant with assembly; 2) Normalize the scores to the haplotype lengths to deal with haplotype size bias.; 3) Change the scores to instead reflect the absolute count of unsupported kmers from the graph to also deal with hapotype size bias. ; 4) Iteratively expand the kmer size used for filtering to pare down the number of haplotypes in a more principled fashion.; 5) Utilize the read kmer occurrence counts to construct the scores in order to reduce the risk of spurious reads being sufficient support for a given haplotype. . We have observed that there can be significant changes to the actual genotyping engine output from the pileup engine from even relatively minor changes to the pileupcalling merging code. We should strive to find a more principled solution for merging haplotypes than the one we have currently.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8494
https://github.com/broadinstitute/gatk/issues/8494:1145,Safety,risk,risk,1145,"The recent branch #8489 has demonstrated that there are some problematic edge cases in the pileup allele merging code that could cause pathological numbers of haplotypes to be handed to the genotyper. In updating the bug in that branch it was observed that it is very common that there are score ties at the 5th haplotype level for the pileupcaller as illustrated by the noise in the updated tests. This algorithm is not a good heuristic and we should replace it with something better, some ideas from that branch that might fix a few of its shortcomings:. 1) Increase/decouple the kmer size used with the reads from the assembly graph kmer size to prevent the filtering step from being redundant with assembly; 2) Normalize the scores to the haplotype lengths to deal with haplotype size bias.; 3) Change the scores to instead reflect the absolute count of unsupported kmers from the graph to also deal with hapotype size bias. ; 4) Iteratively expand the kmer size used for filtering to pare down the number of haplotypes in a more principled fashion.; 5) Utilize the read kmer occurrence counts to construct the scores in order to reduce the risk of spurious reads being sufficient support for a given haplotype. . We have observed that there can be significant changes to the actual genotyping engine output from the pileup engine from even relatively minor changes to the pileupcalling merging code. We should strive to find a more principled solution for merging haplotypes than the one we have currently.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8494
https://github.com/broadinstitute/gatk/issues/8494:392,Testability,test,tests,392,"The recent branch #8489 has demonstrated that there are some problematic edge cases in the pileup allele merging code that could cause pathological numbers of haplotypes to be handed to the genotyper. In updating the bug in that branch it was observed that it is very common that there are score ties at the 5th haplotype level for the pileupcaller as illustrated by the noise in the updated tests. This algorithm is not a good heuristic and we should replace it with something better, some ideas from that branch that might fix a few of its shortcomings:. 1) Increase/decouple the kmer size used with the reads from the assembly graph kmer size to prevent the filtering step from being redundant with assembly; 2) Normalize the scores to the haplotype lengths to deal with haplotype size bias.; 3) Change the scores to instead reflect the absolute count of unsupported kmers from the graph to also deal with hapotype size bias. ; 4) Iteratively expand the kmer size used for filtering to pare down the number of haplotypes in a more principled fashion.; 5) Utilize the read kmer occurrence counts to construct the scores in order to reduce the risk of spurious reads being sufficient support for a given haplotype. . We have observed that there can be significant changes to the actual genotyping engine output from the pileup engine from even relatively minor changes to the pileupcalling merging code. We should strive to find a more principled solution for merging haplotypes than the one we have currently.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8494
https://github.com/broadinstitute/gatk/pull/8495:104,Integrability,depend,depending,104,"Bundled jdk path was set to . `scriptpath + ""/gatkbundle/jdk/bin/java""`. arbitrarily. It may be changed depending on the consensus of how to bundle a jdk with gatk.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8495
https://github.com/broadinstitute/gatk/issues/8497:10,Deployability,install,install,10,We should install the `python-is-python3` package during our docker build to create a symlink from `/usr/bin/python` to `/usr/bin/python3`. This would help avoid problems such as https://github.com/broadinstitute/gatk/issues/8402,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8497
https://github.com/broadinstitute/gatk/issues/8497:156,Safety,avoid,avoid,156,We should install the `python-is-python3` package during our docker build to create a symlink from `/usr/bin/python` to `/usr/bin/python3`. This would help avoid problems such as https://github.com/broadinstitute/gatk/issues/8402,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8497
https://github.com/broadinstitute/gatk/pull/8499:66,Safety,avoid,avoid,66,"This symlinks /usr/bin/python to /usr/bin/python3, which can help avoid issues with scripts that reference the python binary directly. Resolves #8402",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8499
https://github.com/broadinstitute/gatk/issues/8500:324,Availability,error,error,324,"Hi,; I need to include the SnpCluster filter in VariantFiltration but the filtration is not working for me. I have included the other filters but I'm not sure about the usage of this particular filter. The command that I have used:; --genotype-filter-name ""SnpCluster"" \; --genotype-filter-expression ""clusterSize=3"" \. The error that I'm facing:; 12:52:33.595 WARN JexlEngine - ![0,15]: 'clusterSize = 3;' context is readonly. ![image](https://github.com/broadinstitute/gatk/assets/125788800/9cd32db5-0010-489f-be8c-091b53e02c99)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8500
https://github.com/broadinstitute/gatk/issues/8501:2278,Availability,down,down,2278,"riants - Built for Spark Version: 2.4.5; 08:37:42.996 INFO NVScoreVariants - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 08:37:42.996 INFO NVScoreVariants - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 08:37:42.996 INFO NVScoreVariants - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 08:37:42.996 INFO NVScoreVariants - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 08:37:42.996 INFO NVScoreVariants - Deflater: IntelDeflater; 08:37:42.996 INFO NVScoreVariants - Inflater: IntelInflater; 08:37:42.996 INFO NVScoreVariants - GCS max retries/reopens: 20; 08:37:42.996 INFO NVScoreVariants - Requester pays: disabled; 08:37:42.996 WARN NVScoreVariants - . !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: NVScoreVariants is an EXPERIMENTAL tool and should not be used for production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 08:37:42.996 INFO NVScoreVariants - Initializing engine; 08:37:43.031 INFO NVScoreVariants - Shutting down engine; [August 29, 2023 8:37:43 AM GMT] org.broadinstitute.hellbender.tools.walkers.vqsr.NVScoreVariants done. Elapsed time: 0.00 minutes.; Runtime.totalMemory()=2123366400; java.lang.RuntimeException: A required Python package (""scorevariants"") could not be imported into the Python environment. This tool requires that the GATK Python environment is properly established and activated. Please refer to GATK README.md file for instructions on setting up the GATK Python environment.; 	at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.checkPythonEnvironmentForPackage(PythonScriptExecutor.java:228); 	at org.broadinstitute.hellbender.tools.walkers.vqsr.NVScoreVariants.onStartup(NVScoreVariants.java:108); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8501
https://github.com/broadinstitute/gatk/issues/8501:170,Performance,Load,Loading,170,"### code; docker run -it -v /media/sj/14t1:/data2 broadinstitute/gatk-dev:NVSCOREVARIANTS-PREVIEW-SNAPSHOT /bin/bash; ####result; 08:37:42.884 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-NVSCOREVARIANTS-PREVIEW-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_compression.so; 08:37:42.995 INFO NVScoreVariants - ------------------------------------------------------------; 08:37:42.995 INFO NVScoreVariants - The Genome Analysis Toolkit (GATK) vNVSCOREVARIANTS-PREVIEW-SNAPSHOT; 08:37:42.995 INFO NVScoreVariants - For support and documentation go to https://software.broadinstitute.org/gatk/; 08:37:42.995 INFO NVScoreVariants - Executing as root@0e48fe56d3ce on Linux v5.15.0-79-generic amd64; 08:37:42.995 INFO NVScoreVariants - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_242-8u242-b08-0ubuntu3~18.04-b08; 08:37:42.995 INFO NVScoreVariants - Start Date/Time: August 29, 2023 8:37:42 AM GMT; 08:37:42.995 INFO NVScoreVariants - ------------------------------------------------------------; 08:37:42.995 INFO NVScoreVariants - ------------------------------------------------------------; 08:37:42.996 INFO NVScoreVariants - HTSJDK Version: 3.0.1; 08:37:42.996 INFO NVScoreVariants - Picard Version: 2.27.5; 08:37:42.996 INFO NVScoreVariants - Built for Spark Version: 2.4.5; 08:37:42.996 INFO NVScoreVariants - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 08:37:42.996 INFO NVScoreVariants - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 08:37:42.996 INFO NVScoreVariants - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 08:37:42.996 INFO NVScoreVariants - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 08:37:42.996 INFO NVScoreVariants - Deflater: IntelDeflater; 08:37:42.996 INFO NVScoreVariants - Inflater: IntelInflater; 08:37:42.996 INFO NVScoreVariants - GCS max retries/reopens: 20; 08:37:42.996 INFO NVScoreVariants - Requester pays: disabled; 08:37:42.996 WARN NVScoreVariants - . !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8501
https://github.com/broadinstitute/gatk/pull/8502:389,Deployability,Integrat,Integration,389,"AC:. While in a Notebook, I can run Hail 120 and I can specifically run the new vcf_combiner code; merge_alleles() works in the 120; and we must use array_elements_required=False in import_VCF to get around the missing data issue Dan pointed out; ![image](https://github.com/broadinstitute/gatk/assets/6863459/3da27122-8b6b-4bec-b785-55846e671cff). Hail version 120 has been pinned in the Integration test -- pinned!. Integration test with above pin has succeeded.; https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/a1c5e4c4-2058-4dad-8261-87b23c8bb0f3",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8502
https://github.com/broadinstitute/gatk/pull/8502:418,Deployability,Integrat,Integration,418,"AC:. While in a Notebook, I can run Hail 120 and I can specifically run the new vcf_combiner code; merge_alleles() works in the 120; and we must use array_elements_required=False in import_VCF to get around the missing data issue Dan pointed out; ![image](https://github.com/broadinstitute/gatk/assets/6863459/3da27122-8b6b-4bec-b785-55846e671cff). Hail version 120 has been pinned in the Integration test -- pinned!. Integration test with above pin has succeeded.; https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/a1c5e4c4-2058-4dad-8261-87b23c8bb0f3",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8502
https://github.com/broadinstitute/gatk/pull/8502:389,Integrability,Integrat,Integration,389,"AC:. While in a Notebook, I can run Hail 120 and I can specifically run the new vcf_combiner code; merge_alleles() works in the 120; and we must use array_elements_required=False in import_VCF to get around the missing data issue Dan pointed out; ![image](https://github.com/broadinstitute/gatk/assets/6863459/3da27122-8b6b-4bec-b785-55846e671cff). Hail version 120 has been pinned in the Integration test -- pinned!. Integration test with above pin has succeeded.; https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/a1c5e4c4-2058-4dad-8261-87b23c8bb0f3",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8502
https://github.com/broadinstitute/gatk/pull/8502:418,Integrability,Integrat,Integration,418,"AC:. While in a Notebook, I can run Hail 120 and I can specifically run the new vcf_combiner code; merge_alleles() works in the 120; and we must use array_elements_required=False in import_VCF to get around the missing data issue Dan pointed out; ![image](https://github.com/broadinstitute/gatk/assets/6863459/3da27122-8b6b-4bec-b785-55846e671cff). Hail version 120 has been pinned in the Integration test -- pinned!. Integration test with above pin has succeeded.; https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/a1c5e4c4-2058-4dad-8261-87b23c8bb0f3",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8502
https://github.com/broadinstitute/gatk/pull/8502:401,Testability,test,test,401,"AC:. While in a Notebook, I can run Hail 120 and I can specifically run the new vcf_combiner code; merge_alleles() works in the 120; and we must use array_elements_required=False in import_VCF to get around the missing data issue Dan pointed out; ![image](https://github.com/broadinstitute/gatk/assets/6863459/3da27122-8b6b-4bec-b785-55846e671cff). Hail version 120 has been pinned in the Integration test -- pinned!. Integration test with above pin has succeeded.; https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/a1c5e4c4-2058-4dad-8261-87b23c8bb0f3",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8502
https://github.com/broadinstitute/gatk/pull/8502:430,Testability,test,test,430,"AC:. While in a Notebook, I can run Hail 120 and I can specifically run the new vcf_combiner code; merge_alleles() works in the 120; and we must use array_elements_required=False in import_VCF to get around the missing data issue Dan pointed out; ![image](https://github.com/broadinstitute/gatk/assets/6863459/3da27122-8b6b-4bec-b785-55846e671cff). Hail version 120 has been pinned in the Integration test -- pinned!. Integration test with above pin has succeeded.; https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/a1c5e4c4-2058-4dad-8261-87b23c8bb0f3",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8502
https://github.com/broadinstitute/gatk/issues/8504:81,Deployability,install,installed,81,Hi ; We have a forum post asking help for getting GATK 4.1.0.0 conda environment installed using the yml file. ; [https://gatk.broadinstitute.org/hc/en-us/community/posts/18332470602523-Install-GATK-version-4-1-0-0-using-Conda-](url). Looks like restructuring of the default repository under conda took out some of these packages and they are no longer directly accessible. They can be accessed from the forge repo with certain flags. This issue seems to deprecate some of the older but still usable versions of GATK (due to various reasons). Directing people to use the docker version or upgrading to the latest GATK version seems to be the only solution left for now. Any other ideas of how we should pursue this issue? @lbergelson @droazen ?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8504
https://github.com/broadinstitute/gatk/issues/8504:186,Deployability,Install,Install-GATK-version-,186,Hi ; We have a forum post asking help for getting GATK 4.1.0.0 conda environment installed using the yml file. ; [https://gatk.broadinstitute.org/hc/en-us/community/posts/18332470602523-Install-GATK-version-4-1-0-0-using-Conda-](url). Looks like restructuring of the default repository under conda took out some of these packages and they are no longer directly accessible. They can be accessed from the forge repo with certain flags. This issue seems to deprecate some of the older but still usable versions of GATK (due to various reasons). Directing people to use the docker version or upgrading to the latest GATK version seems to be the only solution left for now. Any other ideas of how we should pursue this issue? @lbergelson @droazen ?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8504
https://github.com/broadinstitute/gatk/issues/8504:362,Security,access,accessible,362,Hi ; We have a forum post asking help for getting GATK 4.1.0.0 conda environment installed using the yml file. ; [https://gatk.broadinstitute.org/hc/en-us/community/posts/18332470602523-Install-GATK-version-4-1-0-0-using-Conda-](url). Looks like restructuring of the default repository under conda took out some of these packages and they are no longer directly accessible. They can be accessed from the forge repo with certain flags. This issue seems to deprecate some of the older but still usable versions of GATK (due to various reasons). Directing people to use the docker version or upgrading to the latest GATK version seems to be the only solution left for now. Any other ideas of how we should pursue this issue? @lbergelson @droazen ?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8504
https://github.com/broadinstitute/gatk/issues/8504:386,Security,access,accessed,386,Hi ; We have a forum post asking help for getting GATK 4.1.0.0 conda environment installed using the yml file. ; [https://gatk.broadinstitute.org/hc/en-us/community/posts/18332470602523-Install-GATK-version-4-1-0-0-using-Conda-](url). Looks like restructuring of the default repository under conda took out some of these packages and they are no longer directly accessible. They can be accessed from the forge repo with certain flags. This issue seems to deprecate some of the older but still usable versions of GATK (due to various reasons). Directing people to use the docker version or upgrading to the latest GATK version seems to be the only solution left for now. Any other ideas of how we should pursue this issue? @lbergelson @droazen ?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8504
https://github.com/broadinstitute/gatk/issues/8504:493,Usability,usab,usable,493,Hi ; We have a forum post asking help for getting GATK 4.1.0.0 conda environment installed using the yml file. ; [https://gatk.broadinstitute.org/hc/en-us/community/posts/18332470602523-Install-GATK-version-4-1-0-0-using-Conda-](url). Looks like restructuring of the default repository under conda took out some of these packages and they are no longer directly accessible. They can be accessed from the forge repo with certain flags. This issue seems to deprecate some of the older but still usable versions of GATK (due to various reasons). Directing people to use the docker version or upgrading to the latest GATK version seems to be the only solution left for now. Any other ideas of how we should pursue this issue? @lbergelson @droazen ?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8504
https://github.com/broadinstitute/gatk/pull/8505:11,Deployability,integrat,integration,11,"Successful integration run [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/f7f0131f-96b8-424e-b022-9cb08fd4b39e). Only the ~9 newest commits are actually new, the rest comes from GATK master.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8505
https://github.com/broadinstitute/gatk/pull/8505:11,Integrability,integrat,integration,11,"Successful integration run [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/f7f0131f-96b8-424e-b022-9cb08fd4b39e). Only the ~9 newest commits are actually new, the rest comes from GATK master.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8505
https://github.com/broadinstitute/gatk/issues/8506:3342,Availability,down,down,3342,"46 INFO CNNVariantWriteTensors - Picard Version: 3.0.0; 02:02:31.346 INFO CNNVariantWriteTensors - Built for Spark Version: 3.3.1; 02:02:31.346 INFO CNNVariantWriteTensors - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 02:02:31.346 INFO CNNVariantWriteTensors - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 02:02:31.346 INFO CNNVariantWriteTensors - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 02:02:31.347 INFO CNNVariantWriteTensors - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 02:02:31.347 INFO CNNVariantWriteTensors - Deflater: IntelDeflater; 02:02:31.347 INFO CNNVariantWriteTensors - Inflater: IntelInflater; 02:02:31.347 INFO CNNVariantWriteTensors - GCS max retries/reopens: 20; 02:02:31.347 INFO CNNVariantWriteTensors - Requester pays: disabled; 02:02:31.347 WARN CNNVariantWriteTensors - . !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: CNNVariantWriteTensors is an EXPERIMENTAL tool and should not be used for production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 02:02:31.347 INFO CNNVariantWriteTensors - Initializing engine; 02:02:33.899 INFO CNNVariantWriteTensors - Done initializing engine; 02:02:33.899 INFO CNNVariantWriteTensors - Args are:[--reference_fasta, /data2/example/1/hg19.fa, --input_vcf, /data2/example/NA12877.vcf.gz, --bam_file, , --train_vcf, /data2/example/hg19.hybrid.vcf.gz, --bed_file, /data2/example/hg19.hybrid.bed, --tensor_name, reference, --annotation_set, best_practices, --samples, 1000000, --downsample_snps, 0.05, --downsample_indels, 0.5, --data_dir, /data2/example/results, --channels_last, --mode, write_reference_and_annotation_tensors]; 02:38:41.806 INFO CNNVariantWriteTensors - Shutting down engine; [August 30, 2023 at 2:38:41 AM GMT] org.broadinstitute.hellbender.tools.walkers.vqsr.CNNVariantWriteTensors done. Elapsed time: 36.18 minutes.; Runtime.totalMemory()=285212672; Tool returned:; true. ####issue:; /data2/example/results is empty, and no file was generated.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8506
https://github.com/broadinstitute/gatk/issues/8506:577,Performance,Load,Loading,577,"########code:; docker run -it -v /media/sj/14t1:/data2 broadinstitute/gatk:latest. java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.4.0.0-local.jar CNNVariantWriteTensors --output-tensor-dir /data2/example/results --reference /data2/example/1/hg19.fa --truth-bed /data2/example/hg19.hybrid.bed --truth-vcf /data2/example/hg19.hybrid.vcf.gz --variant /data2/example/NA12877.vcf.gz. ##########result:; 02:02:31.316 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.4.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 02:02:31.342 INFO CNNVariantWriteTensors - ------------------------------------------------------------; 02:02:31.345 INFO CNNVariantWriteTensors - The Genome Analysis Toolkit (GATK) v4.4.0.0; 02:02:31.345 INFO CNNVariantWriteTensors - For support and documentation go to https://software.broadinstitute.org/gatk/; 02:02:31.345 INFO CNNVariantWriteTensors - Executing as root@d768be9a3fc5 on Linux v5.15.0-79-generic amd64; 02:02:31.345 INFO CNNVariantWriteTensors - Java runtime: OpenJDK 64-Bit Server VM v17.0.6+10-Ubuntu-0ubuntu118.04.1; 02:02:31.345 INFO CNNVariantWriteTensors - Start Date/Time: August 30, 2023 at 2:02:31 AM GMT; 02:02:31.345 INFO CNNVariantWriteTensors - ------------------------------------------------------------; 02:02:31.345 INFO CNNVariantWriteTensors - ------------------------------------------------------------; 02:02:31.346 INFO CNNVariantWriteTensors - HTSJDK Version: 3.0.5; 02:02:31.346 INFO CNNVariantWriteTensors - Picard Version: 3.0.0; 02:02:31.346 INFO CNNVariantWriteTensors - Built for Spark Version: 3.3.1; 02:02:31.346 INFO CNNVariantWriteTensors - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 02:02:31.346 INFO CNNVariantWriteTensors - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 02:02:31.346 INFO CNNVariantWriteTensors - HTSJDK Defaults.USE_A",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8506
https://github.com/broadinstitute/gatk/pull/8507:337,Availability,down,down,337,"This PR reimplements the overlap detector used in WeighedSplitIntervals in a much faster form for our particular use case. It also involved preprocessing the weighted bed input file in a new way, so the previous weights files will no longer work. As such, there's a new weights file uploaded and referred to as part of this pr. I pulled down the documentation and rationale for the original process from the git issue to a markdown file that can live in our repo, and made python scripts out of the necessary bits of python logic there (as well as a new one to do the further preprocessing step that I added). The motivation for this was the inability of the previous WeightedSplitIntervals task to complete when run against an exome interval list. This new one does, and it does so quickly. The link referenced below is not a ""successful"" run in the Terra sense because it was 190k exomes and that was simply too much for Terra to handle, but it DOES show a successful WeightedSplitIntervals run before the real extract started and I believe that is sufficient to merge. Delaying while ticket VS-189 gets figured out will create an unnecessary delay. Successful integration run: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/294fd6a8-15ed-4722-a63e-bdf089c1c52a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8507
https://github.com/broadinstitute/gatk/pull/8507:1163,Deployability,integrat,integration,1163,"This PR reimplements the overlap detector used in WeighedSplitIntervals in a much faster form for our particular use case. It also involved preprocessing the weighted bed input file in a new way, so the previous weights files will no longer work. As such, there's a new weights file uploaded and referred to as part of this pr. I pulled down the documentation and rationale for the original process from the git issue to a markdown file that can live in our repo, and made python scripts out of the necessary bits of python logic there (as well as a new one to do the further preprocessing step that I added). The motivation for this was the inability of the previous WeightedSplitIntervals task to complete when run against an exome interval list. This new one does, and it does so quickly. The link referenced below is not a ""successful"" run in the Terra sense because it was 190k exomes and that was simply too much for Terra to handle, but it DOES show a successful WeightedSplitIntervals run before the real extract started and I believe that is sufficient to merge. Delaying while ticket VS-189 gets figured out will create an unnecessary delay. Successful integration run: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/294fd6a8-15ed-4722-a63e-bdf089c1c52a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8507
https://github.com/broadinstitute/gatk/pull/8507:1163,Integrability,integrat,integration,1163,"This PR reimplements the overlap detector used in WeighedSplitIntervals in a much faster form for our particular use case. It also involved preprocessing the weighted bed input file in a new way, so the previous weights files will no longer work. As such, there's a new weights file uploaded and referred to as part of this pr. I pulled down the documentation and rationale for the original process from the git issue to a markdown file that can live in our repo, and made python scripts out of the necessary bits of python logic there (as well as a new one to do the further preprocessing step that I added). The motivation for this was the inability of the previous WeightedSplitIntervals task to complete when run against an exome interval list. This new one does, and it does so quickly. The link referenced below is not a ""successful"" run in the Terra sense because it was 190k exomes and that was simply too much for Terra to handle, but it DOES show a successful WeightedSplitIntervals run before the real extract started and I believe that is sufficient to merge. Delaying while ticket VS-189 gets figured out will create an unnecessary delay. Successful integration run: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/294fd6a8-15ed-4722-a63e-bdf089c1c52a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8507
https://github.com/broadinstitute/gatk/pull/8507:33,Safety,detect,detector,33,"This PR reimplements the overlap detector used in WeighedSplitIntervals in a much faster form for our particular use case. It also involved preprocessing the weighted bed input file in a new way, so the previous weights files will no longer work. As such, there's a new weights file uploaded and referred to as part of this pr. I pulled down the documentation and rationale for the original process from the git issue to a markdown file that can live in our repo, and made python scripts out of the necessary bits of python logic there (as well as a new one to do the further preprocessing step that I added). The motivation for this was the inability of the previous WeightedSplitIntervals task to complete when run against an exome interval list. This new one does, and it does so quickly. The link referenced below is not a ""successful"" run in the Terra sense because it was 190k exomes and that was simply too much for Terra to handle, but it DOES show a successful WeightedSplitIntervals run before the real extract started and I believe that is sufficient to merge. Delaying while ticket VS-189 gets figured out will create an unnecessary delay. Successful integration run: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/294fd6a8-15ed-4722-a63e-bdf089c1c52a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8507
https://github.com/broadinstitute/gatk/pull/8507:524,Testability,log,logic,524,"This PR reimplements the overlap detector used in WeighedSplitIntervals in a much faster form for our particular use case. It also involved preprocessing the weighted bed input file in a new way, so the previous weights files will no longer work. As such, there's a new weights file uploaded and referred to as part of this pr. I pulled down the documentation and rationale for the original process from the git issue to a markdown file that can live in our repo, and made python scripts out of the necessary bits of python logic there (as well as a new one to do the further preprocessing step that I added). The motivation for this was the inability of the previous WeightedSplitIntervals task to complete when run against an exome interval list. This new one does, and it does so quickly. The link referenced below is not a ""successful"" run in the Terra sense because it was 190k exomes and that was simply too much for Terra to handle, but it DOES show a successful WeightedSplitIntervals run before the real extract started and I believe that is sufficient to merge. Delaying while ticket VS-189 gets figured out will create an unnecessary delay. Successful integration run: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/294fd6a8-15ed-4722-a63e-bdf089c1c52a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8507
https://github.com/broadinstitute/gatk/pull/8507:903,Usability,simpl,simply,903,"This PR reimplements the overlap detector used in WeighedSplitIntervals in a much faster form for our particular use case. It also involved preprocessing the weighted bed input file in a new way, so the previous weights files will no longer work. As such, there's a new weights file uploaded and referred to as part of this pr. I pulled down the documentation and rationale for the original process from the git issue to a markdown file that can live in our repo, and made python scripts out of the necessary bits of python logic there (as well as a new one to do the further preprocessing step that I added). The motivation for this was the inability of the previous WeightedSplitIntervals task to complete when run against an exome interval list. This new one does, and it does so quickly. The link referenced below is not a ""successful"" run in the Terra sense because it was 190k exomes and that was simply too much for Terra to handle, but it DOES show a successful WeightedSplitIntervals run before the real extract started and I believe that is sufficient to merge. Delaying while ticket VS-189 gets figured out will create an unnecessary delay. Successful integration run: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/294fd6a8-15ed-4722-a63e-bdf089c1c52a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8507
https://github.com/broadinstitute/gatk/pull/8509:132,Deployability,update,updated,132,In releasing bulk ingest I noticed a few places we could add some clarification to the workspace description. This has already been updated in the workspace and can go out in github in the next release.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8509
https://github.com/broadinstitute/gatk/pull/8509:194,Deployability,release,release,194,In releasing bulk ingest I noticed a few places we could add some clarification to the workspace description. This has already been updated in the workspace and can go out in github in the next release.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8509
https://github.com/broadinstitute/gatk/pull/8512:606,Availability,down,down,606,"Accompanying this branch is and will be an official new release of the Funcotator Datasource bundles: ; gs://broad-public-datasets/funcotator/funcotator_dataSources.v1.8.hg19.20230908g.tar.gz; gs://broad-public-datasets/funcotator/funcotator_dataSources.v1.8.hg19.20230908s.tar.gz; gs://broad-public-datasets/funcotator/funcotator_dataSources.v1.8.hg38.20230908g.tar.gz; gs://broad-public-datasets/funcotator/funcotator_dataSources.v1.8.hg38.20230908s.tar.gz; ; Note that the format of the datasources bundles has changed somewhat, importantly they are now split into separate hg19 and hg38 bundles to cut down on size. In this branch are:; - The necessary changes to the FuncotatorDownloaderScript to accomidate the new bundles; - Changes to the various Funcotator datasource downloader scripts to point to newer releases of bundled sources used in this release; - A fix for the MAF output renderer to handle Gencodev43 datasources. Fixes #8296",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8512
https://github.com/broadinstitute/gatk/pull/8512:777,Availability,down,downloader,777,"Accompanying this branch is and will be an official new release of the Funcotator Datasource bundles: ; gs://broad-public-datasets/funcotator/funcotator_dataSources.v1.8.hg19.20230908g.tar.gz; gs://broad-public-datasets/funcotator/funcotator_dataSources.v1.8.hg19.20230908s.tar.gz; gs://broad-public-datasets/funcotator/funcotator_dataSources.v1.8.hg38.20230908g.tar.gz; gs://broad-public-datasets/funcotator/funcotator_dataSources.v1.8.hg38.20230908s.tar.gz; ; Note that the format of the datasources bundles has changed somewhat, importantly they are now split into separate hg19 and hg38 bundles to cut down on size. In this branch are:; - The necessary changes to the FuncotatorDownloaderScript to accomidate the new bundles; - Changes to the various Funcotator datasource downloader scripts to point to newer releases of bundled sources used in this release; - A fix for the MAF output renderer to handle Gencodev43 datasources. Fixes #8296",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8512
https://github.com/broadinstitute/gatk/pull/8512:56,Deployability,release,release,56,"Accompanying this branch is and will be an official new release of the Funcotator Datasource bundles: ; gs://broad-public-datasets/funcotator/funcotator_dataSources.v1.8.hg19.20230908g.tar.gz; gs://broad-public-datasets/funcotator/funcotator_dataSources.v1.8.hg19.20230908s.tar.gz; gs://broad-public-datasets/funcotator/funcotator_dataSources.v1.8.hg38.20230908g.tar.gz; gs://broad-public-datasets/funcotator/funcotator_dataSources.v1.8.hg38.20230908s.tar.gz; ; Note that the format of the datasources bundles has changed somewhat, importantly they are now split into separate hg19 and hg38 bundles to cut down on size. In this branch are:; - The necessary changes to the FuncotatorDownloaderScript to accomidate the new bundles; - Changes to the various Funcotator datasource downloader scripts to point to newer releases of bundled sources used in this release; - A fix for the MAF output renderer to handle Gencodev43 datasources. Fixes #8296",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8512
https://github.com/broadinstitute/gatk/pull/8512:814,Deployability,release,releases,814,"Accompanying this branch is and will be an official new release of the Funcotator Datasource bundles: ; gs://broad-public-datasets/funcotator/funcotator_dataSources.v1.8.hg19.20230908g.tar.gz; gs://broad-public-datasets/funcotator/funcotator_dataSources.v1.8.hg19.20230908s.tar.gz; gs://broad-public-datasets/funcotator/funcotator_dataSources.v1.8.hg38.20230908g.tar.gz; gs://broad-public-datasets/funcotator/funcotator_dataSources.v1.8.hg38.20230908s.tar.gz; ; Note that the format of the datasources bundles has changed somewhat, importantly they are now split into separate hg19 and hg38 bundles to cut down on size. In this branch are:; - The necessary changes to the FuncotatorDownloaderScript to accomidate the new bundles; - Changes to the various Funcotator datasource downloader scripts to point to newer releases of bundled sources used in this release; - A fix for the MAF output renderer to handle Gencodev43 datasources. Fixes #8296",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8512
https://github.com/broadinstitute/gatk/pull/8512:855,Deployability,release,release,855,"Accompanying this branch is and will be an official new release of the Funcotator Datasource bundles: ; gs://broad-public-datasets/funcotator/funcotator_dataSources.v1.8.hg19.20230908g.tar.gz; gs://broad-public-datasets/funcotator/funcotator_dataSources.v1.8.hg19.20230908s.tar.gz; gs://broad-public-datasets/funcotator/funcotator_dataSources.v1.8.hg38.20230908g.tar.gz; gs://broad-public-datasets/funcotator/funcotator_dataSources.v1.8.hg38.20230908s.tar.gz; ; Note that the format of the datasources bundles has changed somewhat, importantly they are now split into separate hg19 and hg38 bundles to cut down on size. In this branch are:; - The necessary changes to the FuncotatorDownloaderScript to accomidate the new bundles; - Changes to the various Funcotator datasource downloader scripts to point to newer releases of bundled sources used in this release; - A fix for the MAF output renderer to handle Gencodev43 datasources. Fixes #8296",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8512
https://github.com/broadinstitute/gatk/pull/8513:313,Deployability,pipeline,pipeline,313,"Notebook documenting the _actual_ creation of this VDS here:; https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/analysis/launch/Make%20a%20VDS%20with%20Quickstart.ipynb?mode=edit. This VDS (well I made two, one with VQSR and one with VETS) can then be used for a tie out in the near future with the WDL pipeline for VDS creation. gs://fc-eada2674-7c2b-42a6-8db3-0246872596dc/quickstart-vds-for-wdl-tieout/VQSR-Classic/vds/. gs://fc-eada2674-7c2b-42a6-8db3-0246872596dc/quickstart-vds-for-wdl-tieout/VETS/vds/",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8513
https://github.com/broadinstitute/gatk/pull/8514:0,Testability,test,test,0,test run here: https://app.terra.bio/#workspaces/gvs-dev/RSA%20-%20GVS%20Quickstart%20V2%20/job_history/c381ebc7-4378-4550-8647-0e787ff61d95,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8514
https://github.com/broadinstitute/gatk/pull/8515:20,Security,expose,exposes,20,This PR Creates and exposes the `is_wgs` parameter in the GvsJointVariantCalling wdl.; It follows the rules defined in the ticket VS-1020 as to how to set `is_wgs` and the `optional interval_list` and `interval_weights_bed` inputs,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8515
https://github.com/broadinstitute/gatk/pull/8516:0,Deployability,Update,Updates,0,"Updates SVAnnotate's functional consequence annotation of complex SVs.; * Introduce PREDICTED_PARTIAL_DISPERSED_DUP annotation to describe dispersed duplications of coding sequence that are not expected to behave the same way as tandem duplications; * Ignore INV intervals in dDUP events; * Modify INV intervals in dupINV and similar events to more accurately capture the overall impact of the complex SV; * Ignore complex DUP segments for promoter, noncoding, and nearest TSS annotations because these DUPs are never in tandem; * Merge relevant intervals before annotating nearest TSS for complex events containing DELs; * Update documentation. Testing; * Add unit test for CPX SV segment determination; * Add CPX SV unit test cases; * Update unit/integration test expected outputs; * All unit & integration tests for SVAnnotate ran successfully",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8516
https://github.com/broadinstitute/gatk/pull/8516:624,Deployability,Update,Update,624,"Updates SVAnnotate's functional consequence annotation of complex SVs.; * Introduce PREDICTED_PARTIAL_DISPERSED_DUP annotation to describe dispersed duplications of coding sequence that are not expected to behave the same way as tandem duplications; * Ignore INV intervals in dDUP events; * Modify INV intervals in dupINV and similar events to more accurately capture the overall impact of the complex SV; * Ignore complex DUP segments for promoter, noncoding, and nearest TSS annotations because these DUPs are never in tandem; * Merge relevant intervals before annotating nearest TSS for complex events containing DELs; * Update documentation. Testing; * Add unit test for CPX SV segment determination; * Add CPX SV unit test cases; * Update unit/integration test expected outputs; * All unit & integration tests for SVAnnotate ran successfully",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8516
https://github.com/broadinstitute/gatk/pull/8516:737,Deployability,Update,Update,737,"Updates SVAnnotate's functional consequence annotation of complex SVs.; * Introduce PREDICTED_PARTIAL_DISPERSED_DUP annotation to describe dispersed duplications of coding sequence that are not expected to behave the same way as tandem duplications; * Ignore INV intervals in dDUP events; * Modify INV intervals in dupINV and similar events to more accurately capture the overall impact of the complex SV; * Ignore complex DUP segments for promoter, noncoding, and nearest TSS annotations because these DUPs are never in tandem; * Merge relevant intervals before annotating nearest TSS for complex events containing DELs; * Update documentation. Testing; * Add unit test for CPX SV segment determination; * Add CPX SV unit test cases; * Update unit/integration test expected outputs; * All unit & integration tests for SVAnnotate ran successfully",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8516
https://github.com/broadinstitute/gatk/pull/8516:749,Deployability,integrat,integration,749,"Updates SVAnnotate's functional consequence annotation of complex SVs.; * Introduce PREDICTED_PARTIAL_DISPERSED_DUP annotation to describe dispersed duplications of coding sequence that are not expected to behave the same way as tandem duplications; * Ignore INV intervals in dDUP events; * Modify INV intervals in dupINV and similar events to more accurately capture the overall impact of the complex SV; * Ignore complex DUP segments for promoter, noncoding, and nearest TSS annotations because these DUPs are never in tandem; * Merge relevant intervals before annotating nearest TSS for complex events containing DELs; * Update documentation. Testing; * Add unit test for CPX SV segment determination; * Add CPX SV unit test cases; * Update unit/integration test expected outputs; * All unit & integration tests for SVAnnotate ran successfully",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8516
https://github.com/broadinstitute/gatk/pull/8516:797,Deployability,integrat,integration,797,"Updates SVAnnotate's functional consequence annotation of complex SVs.; * Introduce PREDICTED_PARTIAL_DISPERSED_DUP annotation to describe dispersed duplications of coding sequence that are not expected to behave the same way as tandem duplications; * Ignore INV intervals in dDUP events; * Modify INV intervals in dupINV and similar events to more accurately capture the overall impact of the complex SV; * Ignore complex DUP segments for promoter, noncoding, and nearest TSS annotations because these DUPs are never in tandem; * Merge relevant intervals before annotating nearest TSS for complex events containing DELs; * Update documentation. Testing; * Add unit test for CPX SV segment determination; * Add CPX SV unit test cases; * Update unit/integration test expected outputs; * All unit & integration tests for SVAnnotate ran successfully",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8516
https://github.com/broadinstitute/gatk/pull/8516:749,Integrability,integrat,integration,749,"Updates SVAnnotate's functional consequence annotation of complex SVs.; * Introduce PREDICTED_PARTIAL_DISPERSED_DUP annotation to describe dispersed duplications of coding sequence that are not expected to behave the same way as tandem duplications; * Ignore INV intervals in dDUP events; * Modify INV intervals in dupINV and similar events to more accurately capture the overall impact of the complex SV; * Ignore complex DUP segments for promoter, noncoding, and nearest TSS annotations because these DUPs are never in tandem; * Merge relevant intervals before annotating nearest TSS for complex events containing DELs; * Update documentation. Testing; * Add unit test for CPX SV segment determination; * Add CPX SV unit test cases; * Update unit/integration test expected outputs; * All unit & integration tests for SVAnnotate ran successfully",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8516
https://github.com/broadinstitute/gatk/pull/8516:797,Integrability,integrat,integration,797,"Updates SVAnnotate's functional consequence annotation of complex SVs.; * Introduce PREDICTED_PARTIAL_DISPERSED_DUP annotation to describe dispersed duplications of coding sequence that are not expected to behave the same way as tandem duplications; * Ignore INV intervals in dDUP events; * Modify INV intervals in dupINV and similar events to more accurately capture the overall impact of the complex SV; * Ignore complex DUP segments for promoter, noncoding, and nearest TSS annotations because these DUPs are never in tandem; * Merge relevant intervals before annotating nearest TSS for complex events containing DELs; * Update documentation. Testing; * Add unit test for CPX SV segment determination; * Add CPX SV unit test cases; * Update unit/integration test expected outputs; * All unit & integration tests for SVAnnotate ran successfully",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8516
https://github.com/broadinstitute/gatk/pull/8516:646,Testability,Test,Testing,646,"Updates SVAnnotate's functional consequence annotation of complex SVs.; * Introduce PREDICTED_PARTIAL_DISPERSED_DUP annotation to describe dispersed duplications of coding sequence that are not expected to behave the same way as tandem duplications; * Ignore INV intervals in dDUP events; * Modify INV intervals in dupINV and similar events to more accurately capture the overall impact of the complex SV; * Ignore complex DUP segments for promoter, noncoding, and nearest TSS annotations because these DUPs are never in tandem; * Merge relevant intervals before annotating nearest TSS for complex events containing DELs; * Update documentation. Testing; * Add unit test for CPX SV segment determination; * Add CPX SV unit test cases; * Update unit/integration test expected outputs; * All unit & integration tests for SVAnnotate ran successfully",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8516
https://github.com/broadinstitute/gatk/pull/8516:666,Testability,test,test,666,"Updates SVAnnotate's functional consequence annotation of complex SVs.; * Introduce PREDICTED_PARTIAL_DISPERSED_DUP annotation to describe dispersed duplications of coding sequence that are not expected to behave the same way as tandem duplications; * Ignore INV intervals in dDUP events; * Modify INV intervals in dupINV and similar events to more accurately capture the overall impact of the complex SV; * Ignore complex DUP segments for promoter, noncoding, and nearest TSS annotations because these DUPs are never in tandem; * Merge relevant intervals before annotating nearest TSS for complex events containing DELs; * Update documentation. Testing; * Add unit test for CPX SV segment determination; * Add CPX SV unit test cases; * Update unit/integration test expected outputs; * All unit & integration tests for SVAnnotate ran successfully",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8516
https://github.com/broadinstitute/gatk/pull/8516:723,Testability,test,test,723,"Updates SVAnnotate's functional consequence annotation of complex SVs.; * Introduce PREDICTED_PARTIAL_DISPERSED_DUP annotation to describe dispersed duplications of coding sequence that are not expected to behave the same way as tandem duplications; * Ignore INV intervals in dDUP events; * Modify INV intervals in dupINV and similar events to more accurately capture the overall impact of the complex SV; * Ignore complex DUP segments for promoter, noncoding, and nearest TSS annotations because these DUPs are never in tandem; * Merge relevant intervals before annotating nearest TSS for complex events containing DELs; * Update documentation. Testing; * Add unit test for CPX SV segment determination; * Add CPX SV unit test cases; * Update unit/integration test expected outputs; * All unit & integration tests for SVAnnotate ran successfully",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8516
https://github.com/broadinstitute/gatk/pull/8516:761,Testability,test,test,761,"Updates SVAnnotate's functional consequence annotation of complex SVs.; * Introduce PREDICTED_PARTIAL_DISPERSED_DUP annotation to describe dispersed duplications of coding sequence that are not expected to behave the same way as tandem duplications; * Ignore INV intervals in dDUP events; * Modify INV intervals in dupINV and similar events to more accurately capture the overall impact of the complex SV; * Ignore complex DUP segments for promoter, noncoding, and nearest TSS annotations because these DUPs are never in tandem; * Merge relevant intervals before annotating nearest TSS for complex events containing DELs; * Update documentation. Testing; * Add unit test for CPX SV segment determination; * Add CPX SV unit test cases; * Update unit/integration test expected outputs; * All unit & integration tests for SVAnnotate ran successfully",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8516
https://github.com/broadinstitute/gatk/pull/8516:809,Testability,test,tests,809,"Updates SVAnnotate's functional consequence annotation of complex SVs.; * Introduce PREDICTED_PARTIAL_DISPERSED_DUP annotation to describe dispersed duplications of coding sequence that are not expected to behave the same way as tandem duplications; * Ignore INV intervals in dDUP events; * Modify INV intervals in dupINV and similar events to more accurately capture the overall impact of the complex SV; * Ignore complex DUP segments for promoter, noncoding, and nearest TSS annotations because these DUPs are never in tandem; * Merge relevant intervals before annotating nearest TSS for complex events containing DELs; * Update documentation. Testing; * Add unit test for CPX SV segment determination; * Add CPX SV unit test cases; * Update unit/integration test expected outputs; * All unit & integration tests for SVAnnotate ran successfully",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8516
https://github.com/broadinstitute/gatk/issues/8517:3029,Availability,down,down,3029," - GCS max retries/reopens: 20; 10:19:39.338 INFO GenomicsDBImport - Requester pays: disabled; 10:19:39.338 INFO GenomicsDBImport - Initializing engine; 10:19:39.489 INFO IntervalArgumentCollection - Processing 100 bp from intervals; 10:19:39.490 INFO GenomicsDBImport - Done initializing engine; 10:19:39.948 INFO GenomicsDBLibLoader - GenomicsDB native library version : 1.4.4-ce4e1b9; 10:19:39.951 INFO GenomicsDBImport - Vid Map JSON file will be written to /home/test/Software/gatk-4.4.0.0/test/./02/vidmap.json; 10:19:39.951 INFO GenomicsDBImport - Callset Map JSON file will be written to /home/test/Software/gatk-4.4.0.0/test/./02/callset.json; 10:19:39.951 INFO GenomicsDBImport - Complete VCF Header will be written to /home/test/Software/gatk-4.4.0.0/test/./02/vcfheader.vcf; 10:19:39.951 INFO GenomicsDBImport - Importing to workspace - /home/test/Software/gatk-4.4.0.0/test/./02; 10:19:40.060 INFO GenomicsDBImport - Importing batch 1 with 2 samples; 10:19:40.075 INFO GenomicsDBImport - Shutting down engine; org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=285212672; java.lang.NumberFormatException: For input string: ""G""; 	at java.base/java.lang.NumberFormatException.forInputString(NumberFormatException.java:67); 	at java.base/java.lang.Integer.parseInt(Integer.java:668); 	at java.base/java.lang.Integer.parseInt(Integer.java:786); 	at htsjdk.tribble.readers.TabixReader.getIntv(TabixReader.java:337); 	at htsjdk.tribble.readers.TabixReader.access$500(TabixReader.java:48); 	at htsjdk.tribble.readers.TabixReader$IteratorImpl.next(TabixReader.java:438); 	at htsjdk.tribble.readers.TabixIteratorLineReader.readLine(TabixIteratorLineReader.java:46); 	at htsjdk.tribble.TabixFeatureReader$FeatureIterator.readNextRecord(TabixFeatureReader.java:170); 	at htsjdk.tribble.TabixFeatureReader$FeatureIterator.<init>(TabixFeatureReader.java:159); 	at htsjdk.tribble.TabixFeatureReader.query(TabixFeatureReader.java:13",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8517
https://github.com/broadinstitute/gatk/issues/8517:20,Deployability,install,installed,20,"Dear developer:; We installed the gatk4.4 and we want to used the GenomicsDBImport. However, we encountered a problem that made me feel very confused and could not be solved. We had not encountered this problem before, because I was relatively familiar with GATK.; Best day!. Code:; /home/test/Software/gatk-4.4.0.0/gatk GenomicsDBImport \; > -V /home/test/Software/gatk-4.4.0.0/test/SRR11306995.gvcf.gz \; > -V /home/test/Software/gatk-4.4.0.0/test/SRR11306996.gvcf.gz \; > --intervals NC_010443.5 \; > --genomicsdb-workspace-path /home/test/Software/gatk-4.4.0.0/test/02. Erro:; 10:19:39.332 INFO GenomicsDBImport - ------------------------------------------------------------; 10:19:39.334 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.4.0.0; 10:19:39.335 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:19:39.335 INFO GenomicsDBImport - Executing as test@yang301 on Linux v5.15.0-71-generic amd64; 10:19:39.335 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v17.0.3+7; 10:19:39.335 INFO GenomicsDBImport - Start Date/Time: ; 10:19:39.335 INFO GenomicsDBImport - ------------------------------------------------------------; 10:19:39.335 INFO GenomicsDBImport - ------------------------------------------------------------; 10:19:39.336 INFO GenomicsDBImport - HTSJDK Version: 3.0.5; 10:19:39.336 INFO GenomicsDBImport - Picard Version: 3.0.0; 10:19:39.336 INFO GenomicsDBImport - Built for Spark Version: 3.3.1; 10:19:39.336 INFO GenomicsDBImport - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 10:19:39.337 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 10:19:39.337 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 10:19:39.337 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 10:19:39.337 INFO GenomicsDBImport - Deflater: IntelDeflater; 10:19:39.337 INFO GenomicsDBImport - Inflater: IntelInflater; 10:19:39.337 INF",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8517
https://github.com/broadinstitute/gatk/issues/8517:4326,Performance,concurren,concurrent,4326,"4.0.0/test/./02/callset.json; 10:19:39.951 INFO GenomicsDBImport - Complete VCF Header will be written to /home/test/Software/gatk-4.4.0.0/test/./02/vcfheader.vcf; 10:19:39.951 INFO GenomicsDBImport - Importing to workspace - /home/test/Software/gatk-4.4.0.0/test/./02; 10:19:40.060 INFO GenomicsDBImport - Importing batch 1 with 2 samples; 10:19:40.075 INFO GenomicsDBImport - Shutting down engine; org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=285212672; java.lang.NumberFormatException: For input string: ""G""; 	at java.base/java.lang.NumberFormatException.forInputString(NumberFormatException.java:67); 	at java.base/java.lang.Integer.parseInt(Integer.java:668); 	at java.base/java.lang.Integer.parseInt(Integer.java:786); 	at htsjdk.tribble.readers.TabixReader.getIntv(TabixReader.java:337); 	at htsjdk.tribble.readers.TabixReader.access$500(TabixReader.java:48); 	at htsjdk.tribble.readers.TabixReader$IteratorImpl.next(TabixReader.java:438); 	at htsjdk.tribble.readers.TabixIteratorLineReader.readLine(TabixIteratorLineReader.java:46); 	at htsjdk.tribble.TabixFeatureReader$FeatureIterator.readNextRecord(TabixFeatureReader.java:170); 	at htsjdk.tribble.TabixFeatureReader$FeatureIterator.<init>(TabixFeatureReader.java:159); 	at htsjdk.tribble.TabixFeatureReader.query(TabixFeatureReader.java:133); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport$1.query(GenomicsDBImport.java:971); 	at org.genomicsdb.importer.GenomicsDBImporter.<init>(GenomicsDBImporter.java:167); 	at org.genomicsdb.importer.GenomicsDBImporter.lambda$null$4(GenomicsDBImporter.java:732); 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1768); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635); 	at java.base/java.lang.Thread.run(Thread.java:833)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8517
https://github.com/broadinstitute/gatk/issues/8517:4425,Performance,concurren,concurrent,4425,"4.0.0/test/./02/callset.json; 10:19:39.951 INFO GenomicsDBImport - Complete VCF Header will be written to /home/test/Software/gatk-4.4.0.0/test/./02/vcfheader.vcf; 10:19:39.951 INFO GenomicsDBImport - Importing to workspace - /home/test/Software/gatk-4.4.0.0/test/./02; 10:19:40.060 INFO GenomicsDBImport - Importing batch 1 with 2 samples; 10:19:40.075 INFO GenomicsDBImport - Shutting down engine; org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=285212672; java.lang.NumberFormatException: For input string: ""G""; 	at java.base/java.lang.NumberFormatException.forInputString(NumberFormatException.java:67); 	at java.base/java.lang.Integer.parseInt(Integer.java:668); 	at java.base/java.lang.Integer.parseInt(Integer.java:786); 	at htsjdk.tribble.readers.TabixReader.getIntv(TabixReader.java:337); 	at htsjdk.tribble.readers.TabixReader.access$500(TabixReader.java:48); 	at htsjdk.tribble.readers.TabixReader$IteratorImpl.next(TabixReader.java:438); 	at htsjdk.tribble.readers.TabixIteratorLineReader.readLine(TabixIteratorLineReader.java:46); 	at htsjdk.tribble.TabixFeatureReader$FeatureIterator.readNextRecord(TabixFeatureReader.java:170); 	at htsjdk.tribble.TabixFeatureReader$FeatureIterator.<init>(TabixFeatureReader.java:159); 	at htsjdk.tribble.TabixFeatureReader.query(TabixFeatureReader.java:133); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport$1.query(GenomicsDBImport.java:971); 	at org.genomicsdb.importer.GenomicsDBImporter.<init>(GenomicsDBImporter.java:167); 	at org.genomicsdb.importer.GenomicsDBImporter.lambda$null$4(GenomicsDBImporter.java:732); 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1768); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635); 	at java.base/java.lang.Thread.run(Thread.java:833)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8517
https://github.com/broadinstitute/gatk/issues/8517:4520,Performance,concurren,concurrent,4520,"4.0.0/test/./02/callset.json; 10:19:39.951 INFO GenomicsDBImport - Complete VCF Header will be written to /home/test/Software/gatk-4.4.0.0/test/./02/vcfheader.vcf; 10:19:39.951 INFO GenomicsDBImport - Importing to workspace - /home/test/Software/gatk-4.4.0.0/test/./02; 10:19:40.060 INFO GenomicsDBImport - Importing batch 1 with 2 samples; 10:19:40.075 INFO GenomicsDBImport - Shutting down engine; org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=285212672; java.lang.NumberFormatException: For input string: ""G""; 	at java.base/java.lang.NumberFormatException.forInputString(NumberFormatException.java:67); 	at java.base/java.lang.Integer.parseInt(Integer.java:668); 	at java.base/java.lang.Integer.parseInt(Integer.java:786); 	at htsjdk.tribble.readers.TabixReader.getIntv(TabixReader.java:337); 	at htsjdk.tribble.readers.TabixReader.access$500(TabixReader.java:48); 	at htsjdk.tribble.readers.TabixReader$IteratorImpl.next(TabixReader.java:438); 	at htsjdk.tribble.readers.TabixIteratorLineReader.readLine(TabixIteratorLineReader.java:46); 	at htsjdk.tribble.TabixFeatureReader$FeatureIterator.readNextRecord(TabixFeatureReader.java:170); 	at htsjdk.tribble.TabixFeatureReader$FeatureIterator.<init>(TabixFeatureReader.java:159); 	at htsjdk.tribble.TabixFeatureReader.query(TabixFeatureReader.java:133); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport$1.query(GenomicsDBImport.java:971); 	at org.genomicsdb.importer.GenomicsDBImporter.<init>(GenomicsDBImporter.java:167); 	at org.genomicsdb.importer.GenomicsDBImporter.lambda$null$4(GenomicsDBImporter.java:732); 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1768); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635); 	at java.base/java.lang.Thread.run(Thread.java:833)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8517
https://github.com/broadinstitute/gatk/issues/8517:3552,Security,access,access,3552,"DBImport - Callset Map JSON file will be written to /home/test/Software/gatk-4.4.0.0/test/./02/callset.json; 10:19:39.951 INFO GenomicsDBImport - Complete VCF Header will be written to /home/test/Software/gatk-4.4.0.0/test/./02/vcfheader.vcf; 10:19:39.951 INFO GenomicsDBImport - Importing to workspace - /home/test/Software/gatk-4.4.0.0/test/./02; 10:19:40.060 INFO GenomicsDBImport - Importing batch 1 with 2 samples; 10:19:40.075 INFO GenomicsDBImport - Shutting down engine; org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=285212672; java.lang.NumberFormatException: For input string: ""G""; 	at java.base/java.lang.NumberFormatException.forInputString(NumberFormatException.java:67); 	at java.base/java.lang.Integer.parseInt(Integer.java:668); 	at java.base/java.lang.Integer.parseInt(Integer.java:786); 	at htsjdk.tribble.readers.TabixReader.getIntv(TabixReader.java:337); 	at htsjdk.tribble.readers.TabixReader.access$500(TabixReader.java:48); 	at htsjdk.tribble.readers.TabixReader$IteratorImpl.next(TabixReader.java:438); 	at htsjdk.tribble.readers.TabixIteratorLineReader.readLine(TabixIteratorLineReader.java:46); 	at htsjdk.tribble.TabixFeatureReader$FeatureIterator.readNextRecord(TabixFeatureReader.java:170); 	at htsjdk.tribble.TabixFeatureReader$FeatureIterator.<init>(TabixFeatureReader.java:159); 	at htsjdk.tribble.TabixFeatureReader.query(TabixFeatureReader.java:133); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport$1.query(GenomicsDBImport.java:971); 	at org.genomicsdb.importer.GenomicsDBImporter.<init>(GenomicsDBImporter.java:167); 	at org.genomicsdb.importer.GenomicsDBImporter.lambda$null$4(GenomicsDBImporter.java:732); 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1768); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Thr",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8517
https://github.com/broadinstitute/gatk/issues/8517:289,Testability,test,test,289,"Dear developer:; We installed the gatk4.4 and we want to used the GenomicsDBImport. However, we encountered a problem that made me feel very confused and could not be solved. We had not encountered this problem before, because I was relatively familiar with GATK.; Best day!. Code:; /home/test/Software/gatk-4.4.0.0/gatk GenomicsDBImport \; > -V /home/test/Software/gatk-4.4.0.0/test/SRR11306995.gvcf.gz \; > -V /home/test/Software/gatk-4.4.0.0/test/SRR11306996.gvcf.gz \; > --intervals NC_010443.5 \; > --genomicsdb-workspace-path /home/test/Software/gatk-4.4.0.0/test/02. Erro:; 10:19:39.332 INFO GenomicsDBImport - ------------------------------------------------------------; 10:19:39.334 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.4.0.0; 10:19:39.335 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:19:39.335 INFO GenomicsDBImport - Executing as test@yang301 on Linux v5.15.0-71-generic amd64; 10:19:39.335 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v17.0.3+7; 10:19:39.335 INFO GenomicsDBImport - Start Date/Time: ; 10:19:39.335 INFO GenomicsDBImport - ------------------------------------------------------------; 10:19:39.335 INFO GenomicsDBImport - ------------------------------------------------------------; 10:19:39.336 INFO GenomicsDBImport - HTSJDK Version: 3.0.5; 10:19:39.336 INFO GenomicsDBImport - Picard Version: 3.0.0; 10:19:39.336 INFO GenomicsDBImport - Built for Spark Version: 3.3.1; 10:19:39.336 INFO GenomicsDBImport - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 10:19:39.337 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 10:19:39.337 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 10:19:39.337 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 10:19:39.337 INFO GenomicsDBImport - Deflater: IntelDeflater; 10:19:39.337 INFO GenomicsDBImport - Inflater: IntelInflater; 10:19:39.337 INF",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8517
https://github.com/broadinstitute/gatk/issues/8517:352,Testability,test,test,352,"Dear developer:; We installed the gatk4.4 and we want to used the GenomicsDBImport. However, we encountered a problem that made me feel very confused and could not be solved. We had not encountered this problem before, because I was relatively familiar with GATK.; Best day!. Code:; /home/test/Software/gatk-4.4.0.0/gatk GenomicsDBImport \; > -V /home/test/Software/gatk-4.4.0.0/test/SRR11306995.gvcf.gz \; > -V /home/test/Software/gatk-4.4.0.0/test/SRR11306996.gvcf.gz \; > --intervals NC_010443.5 \; > --genomicsdb-workspace-path /home/test/Software/gatk-4.4.0.0/test/02. Erro:; 10:19:39.332 INFO GenomicsDBImport - ------------------------------------------------------------; 10:19:39.334 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.4.0.0; 10:19:39.335 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:19:39.335 INFO GenomicsDBImport - Executing as test@yang301 on Linux v5.15.0-71-generic amd64; 10:19:39.335 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v17.0.3+7; 10:19:39.335 INFO GenomicsDBImport - Start Date/Time: ; 10:19:39.335 INFO GenomicsDBImport - ------------------------------------------------------------; 10:19:39.335 INFO GenomicsDBImport - ------------------------------------------------------------; 10:19:39.336 INFO GenomicsDBImport - HTSJDK Version: 3.0.5; 10:19:39.336 INFO GenomicsDBImport - Picard Version: 3.0.0; 10:19:39.336 INFO GenomicsDBImport - Built for Spark Version: 3.3.1; 10:19:39.336 INFO GenomicsDBImport - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 10:19:39.337 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 10:19:39.337 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 10:19:39.337 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 10:19:39.337 INFO GenomicsDBImport - Deflater: IntelDeflater; 10:19:39.337 INFO GenomicsDBImport - Inflater: IntelInflater; 10:19:39.337 INF",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8517
https://github.com/broadinstitute/gatk/issues/8517:379,Testability,test,test,379,"Dear developer:; We installed the gatk4.4 and we want to used the GenomicsDBImport. However, we encountered a problem that made me feel very confused and could not be solved. We had not encountered this problem before, because I was relatively familiar with GATK.; Best day!. Code:; /home/test/Software/gatk-4.4.0.0/gatk GenomicsDBImport \; > -V /home/test/Software/gatk-4.4.0.0/test/SRR11306995.gvcf.gz \; > -V /home/test/Software/gatk-4.4.0.0/test/SRR11306996.gvcf.gz \; > --intervals NC_010443.5 \; > --genomicsdb-workspace-path /home/test/Software/gatk-4.4.0.0/test/02. Erro:; 10:19:39.332 INFO GenomicsDBImport - ------------------------------------------------------------; 10:19:39.334 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.4.0.0; 10:19:39.335 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:19:39.335 INFO GenomicsDBImport - Executing as test@yang301 on Linux v5.15.0-71-generic amd64; 10:19:39.335 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v17.0.3+7; 10:19:39.335 INFO GenomicsDBImport - Start Date/Time: ; 10:19:39.335 INFO GenomicsDBImport - ------------------------------------------------------------; 10:19:39.335 INFO GenomicsDBImport - ------------------------------------------------------------; 10:19:39.336 INFO GenomicsDBImport - HTSJDK Version: 3.0.5; 10:19:39.336 INFO GenomicsDBImport - Picard Version: 3.0.0; 10:19:39.336 INFO GenomicsDBImport - Built for Spark Version: 3.3.1; 10:19:39.336 INFO GenomicsDBImport - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 10:19:39.337 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 10:19:39.337 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 10:19:39.337 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 10:19:39.337 INFO GenomicsDBImport - Deflater: IntelDeflater; 10:19:39.337 INFO GenomicsDBImport - Inflater: IntelInflater; 10:19:39.337 INF",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8517
https://github.com/broadinstitute/gatk/issues/8517:418,Testability,test,test,418,"Dear developer:; We installed the gatk4.4 and we want to used the GenomicsDBImport. However, we encountered a problem that made me feel very confused and could not be solved. We had not encountered this problem before, because I was relatively familiar with GATK.; Best day!. Code:; /home/test/Software/gatk-4.4.0.0/gatk GenomicsDBImport \; > -V /home/test/Software/gatk-4.4.0.0/test/SRR11306995.gvcf.gz \; > -V /home/test/Software/gatk-4.4.0.0/test/SRR11306996.gvcf.gz \; > --intervals NC_010443.5 \; > --genomicsdb-workspace-path /home/test/Software/gatk-4.4.0.0/test/02. Erro:; 10:19:39.332 INFO GenomicsDBImport - ------------------------------------------------------------; 10:19:39.334 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.4.0.0; 10:19:39.335 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:19:39.335 INFO GenomicsDBImport - Executing as test@yang301 on Linux v5.15.0-71-generic amd64; 10:19:39.335 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v17.0.3+7; 10:19:39.335 INFO GenomicsDBImport - Start Date/Time: ; 10:19:39.335 INFO GenomicsDBImport - ------------------------------------------------------------; 10:19:39.335 INFO GenomicsDBImport - ------------------------------------------------------------; 10:19:39.336 INFO GenomicsDBImport - HTSJDK Version: 3.0.5; 10:19:39.336 INFO GenomicsDBImport - Picard Version: 3.0.0; 10:19:39.336 INFO GenomicsDBImport - Built for Spark Version: 3.3.1; 10:19:39.336 INFO GenomicsDBImport - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 10:19:39.337 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 10:19:39.337 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 10:19:39.337 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 10:19:39.337 INFO GenomicsDBImport - Deflater: IntelDeflater; 10:19:39.337 INFO GenomicsDBImport - Inflater: IntelInflater; 10:19:39.337 INF",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8517
https://github.com/broadinstitute/gatk/issues/8517:445,Testability,test,test,445,"Dear developer:; We installed the gatk4.4 and we want to used the GenomicsDBImport. However, we encountered a problem that made me feel very confused and could not be solved. We had not encountered this problem before, because I was relatively familiar with GATK.; Best day!. Code:; /home/test/Software/gatk-4.4.0.0/gatk GenomicsDBImport \; > -V /home/test/Software/gatk-4.4.0.0/test/SRR11306995.gvcf.gz \; > -V /home/test/Software/gatk-4.4.0.0/test/SRR11306996.gvcf.gz \; > --intervals NC_010443.5 \; > --genomicsdb-workspace-path /home/test/Software/gatk-4.4.0.0/test/02. Erro:; 10:19:39.332 INFO GenomicsDBImport - ------------------------------------------------------------; 10:19:39.334 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.4.0.0; 10:19:39.335 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:19:39.335 INFO GenomicsDBImport - Executing as test@yang301 on Linux v5.15.0-71-generic amd64; 10:19:39.335 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v17.0.3+7; 10:19:39.335 INFO GenomicsDBImport - Start Date/Time: ; 10:19:39.335 INFO GenomicsDBImport - ------------------------------------------------------------; 10:19:39.335 INFO GenomicsDBImport - ------------------------------------------------------------; 10:19:39.336 INFO GenomicsDBImport - HTSJDK Version: 3.0.5; 10:19:39.336 INFO GenomicsDBImport - Picard Version: 3.0.0; 10:19:39.336 INFO GenomicsDBImport - Built for Spark Version: 3.3.1; 10:19:39.336 INFO GenomicsDBImport - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 10:19:39.337 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 10:19:39.337 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 10:19:39.337 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 10:19:39.337 INFO GenomicsDBImport - Deflater: IntelDeflater; 10:19:39.337 INFO GenomicsDBImport - Inflater: IntelInflater; 10:19:39.337 INF",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8517
https://github.com/broadinstitute/gatk/issues/8517:538,Testability,test,test,538,"Dear developer:; We installed the gatk4.4 and we want to used the GenomicsDBImport. However, we encountered a problem that made me feel very confused and could not be solved. We had not encountered this problem before, because I was relatively familiar with GATK.; Best day!. Code:; /home/test/Software/gatk-4.4.0.0/gatk GenomicsDBImport \; > -V /home/test/Software/gatk-4.4.0.0/test/SRR11306995.gvcf.gz \; > -V /home/test/Software/gatk-4.4.0.0/test/SRR11306996.gvcf.gz \; > --intervals NC_010443.5 \; > --genomicsdb-workspace-path /home/test/Software/gatk-4.4.0.0/test/02. Erro:; 10:19:39.332 INFO GenomicsDBImport - ------------------------------------------------------------; 10:19:39.334 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.4.0.0; 10:19:39.335 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:19:39.335 INFO GenomicsDBImport - Executing as test@yang301 on Linux v5.15.0-71-generic amd64; 10:19:39.335 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v17.0.3+7; 10:19:39.335 INFO GenomicsDBImport - Start Date/Time: ; 10:19:39.335 INFO GenomicsDBImport - ------------------------------------------------------------; 10:19:39.335 INFO GenomicsDBImport - ------------------------------------------------------------; 10:19:39.336 INFO GenomicsDBImport - HTSJDK Version: 3.0.5; 10:19:39.336 INFO GenomicsDBImport - Picard Version: 3.0.0; 10:19:39.336 INFO GenomicsDBImport - Built for Spark Version: 3.3.1; 10:19:39.336 INFO GenomicsDBImport - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 10:19:39.337 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 10:19:39.337 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 10:19:39.337 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 10:19:39.337 INFO GenomicsDBImport - Deflater: IntelDeflater; 10:19:39.337 INFO GenomicsDBImport - Inflater: IntelInflater; 10:19:39.337 INF",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8517
https://github.com/broadinstitute/gatk/issues/8517:565,Testability,test,test,565,"Dear developer:; We installed the gatk4.4 and we want to used the GenomicsDBImport. However, we encountered a problem that made me feel very confused and could not be solved. We had not encountered this problem before, because I was relatively familiar with GATK.; Best day!. Code:; /home/test/Software/gatk-4.4.0.0/gatk GenomicsDBImport \; > -V /home/test/Software/gatk-4.4.0.0/test/SRR11306995.gvcf.gz \; > -V /home/test/Software/gatk-4.4.0.0/test/SRR11306996.gvcf.gz \; > --intervals NC_010443.5 \; > --genomicsdb-workspace-path /home/test/Software/gatk-4.4.0.0/test/02. Erro:; 10:19:39.332 INFO GenomicsDBImport - ------------------------------------------------------------; 10:19:39.334 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.4.0.0; 10:19:39.335 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:19:39.335 INFO GenomicsDBImport - Executing as test@yang301 on Linux v5.15.0-71-generic amd64; 10:19:39.335 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v17.0.3+7; 10:19:39.335 INFO GenomicsDBImport - Start Date/Time: ; 10:19:39.335 INFO GenomicsDBImport - ------------------------------------------------------------; 10:19:39.335 INFO GenomicsDBImport - ------------------------------------------------------------; 10:19:39.336 INFO GenomicsDBImport - HTSJDK Version: 3.0.5; 10:19:39.336 INFO GenomicsDBImport - Picard Version: 3.0.0; 10:19:39.336 INFO GenomicsDBImport - Built for Spark Version: 3.3.1; 10:19:39.336 INFO GenomicsDBImport - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 10:19:39.337 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 10:19:39.337 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 10:19:39.337 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 10:19:39.337 INFO GenomicsDBImport - Deflater: IntelDeflater; 10:19:39.337 INFO GenomicsDBImport - Inflater: IntelInflater; 10:19:39.337 INF",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8517
https://github.com/broadinstitute/gatk/issues/8517:928,Testability,test,test,928,"Dear developer:; We installed the gatk4.4 and we want to used the GenomicsDBImport. However, we encountered a problem that made me feel very confused and could not be solved. We had not encountered this problem before, because I was relatively familiar with GATK.; Best day!. Code:; /home/test/Software/gatk-4.4.0.0/gatk GenomicsDBImport \; > -V /home/test/Software/gatk-4.4.0.0/test/SRR11306995.gvcf.gz \; > -V /home/test/Software/gatk-4.4.0.0/test/SRR11306996.gvcf.gz \; > --intervals NC_010443.5 \; > --genomicsdb-workspace-path /home/test/Software/gatk-4.4.0.0/test/02. Erro:; 10:19:39.332 INFO GenomicsDBImport - ------------------------------------------------------------; 10:19:39.334 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.4.0.0; 10:19:39.335 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:19:39.335 INFO GenomicsDBImport - Executing as test@yang301 on Linux v5.15.0-71-generic amd64; 10:19:39.335 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v17.0.3+7; 10:19:39.335 INFO GenomicsDBImport - Start Date/Time: ; 10:19:39.335 INFO GenomicsDBImport - ------------------------------------------------------------; 10:19:39.335 INFO GenomicsDBImport - ------------------------------------------------------------; 10:19:39.336 INFO GenomicsDBImport - HTSJDK Version: 3.0.5; 10:19:39.336 INFO GenomicsDBImport - Picard Version: 3.0.0; 10:19:39.336 INFO GenomicsDBImport - Built for Spark Version: 3.3.1; 10:19:39.336 INFO GenomicsDBImport - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 10:19:39.337 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 10:19:39.337 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 10:19:39.337 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 10:19:39.337 INFO GenomicsDBImport - Deflater: IntelDeflater; 10:19:39.337 INFO GenomicsDBImport - Inflater: IntelInflater; 10:19:39.337 INF",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8517
https://github.com/broadinstitute/gatk/issues/8517:2487,Testability,test,test,2487,"sDBImport - Built for Spark Version: 3.3.1; 10:19:39.336 INFO GenomicsDBImport - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 10:19:39.337 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 10:19:39.337 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 10:19:39.337 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 10:19:39.337 INFO GenomicsDBImport - Deflater: IntelDeflater; 10:19:39.337 INFO GenomicsDBImport - Inflater: IntelInflater; 10:19:39.337 INFO GenomicsDBImport - GCS max retries/reopens: 20; 10:19:39.338 INFO GenomicsDBImport - Requester pays: disabled; 10:19:39.338 INFO GenomicsDBImport - Initializing engine; 10:19:39.489 INFO IntervalArgumentCollection - Processing 100 bp from intervals; 10:19:39.490 INFO GenomicsDBImport - Done initializing engine; 10:19:39.948 INFO GenomicsDBLibLoader - GenomicsDB native library version : 1.4.4-ce4e1b9; 10:19:39.951 INFO GenomicsDBImport - Vid Map JSON file will be written to /home/test/Software/gatk-4.4.0.0/test/./02/vidmap.json; 10:19:39.951 INFO GenomicsDBImport - Callset Map JSON file will be written to /home/test/Software/gatk-4.4.0.0/test/./02/callset.json; 10:19:39.951 INFO GenomicsDBImport - Complete VCF Header will be written to /home/test/Software/gatk-4.4.0.0/test/./02/vcfheader.vcf; 10:19:39.951 INFO GenomicsDBImport - Importing to workspace - /home/test/Software/gatk-4.4.0.0/test/./02; 10:19:40.060 INFO GenomicsDBImport - Importing batch 1 with 2 samples; 10:19:40.075 INFO GenomicsDBImport - Shutting down engine; org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=285212672; java.lang.NumberFormatException: For input string: ""G""; 	at java.base/java.lang.NumberFormatException.forInputString(NumberFormatException.java:67); 	at java.base/java.lang.Integer.parseInt(Integer.java:668); 	at java.base/java.lang.Integer.parseInt(Integer.java:786); 	at htsjdk.tribble.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8517
https://github.com/broadinstitute/gatk/issues/8517:2514,Testability,test,test,2514,"36 INFO GenomicsDBImport - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 10:19:39.337 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 10:19:39.337 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 10:19:39.337 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 10:19:39.337 INFO GenomicsDBImport - Deflater: IntelDeflater; 10:19:39.337 INFO GenomicsDBImport - Inflater: IntelInflater; 10:19:39.337 INFO GenomicsDBImport - GCS max retries/reopens: 20; 10:19:39.338 INFO GenomicsDBImport - Requester pays: disabled; 10:19:39.338 INFO GenomicsDBImport - Initializing engine; 10:19:39.489 INFO IntervalArgumentCollection - Processing 100 bp from intervals; 10:19:39.490 INFO GenomicsDBImport - Done initializing engine; 10:19:39.948 INFO GenomicsDBLibLoader - GenomicsDB native library version : 1.4.4-ce4e1b9; 10:19:39.951 INFO GenomicsDBImport - Vid Map JSON file will be written to /home/test/Software/gatk-4.4.0.0/test/./02/vidmap.json; 10:19:39.951 INFO GenomicsDBImport - Callset Map JSON file will be written to /home/test/Software/gatk-4.4.0.0/test/./02/callset.json; 10:19:39.951 INFO GenomicsDBImport - Complete VCF Header will be written to /home/test/Software/gatk-4.4.0.0/test/./02/vcfheader.vcf; 10:19:39.951 INFO GenomicsDBImport - Importing to workspace - /home/test/Software/gatk-4.4.0.0/test/./02; 10:19:40.060 INFO GenomicsDBImport - Importing batch 1 with 2 samples; 10:19:40.075 INFO GenomicsDBImport - Shutting down engine; org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=285212672; java.lang.NumberFormatException: For input string: ""G""; 	at java.base/java.lang.NumberFormatException.forInputString(NumberFormatException.java:67); 	at java.base/java.lang.Integer.parseInt(Integer.java:668); 	at java.base/java.lang.Integer.parseInt(Integer.java:786); 	at htsjdk.tribble.readers.TabixReader.getIntv(TabixReader.java:337); 	at",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8517
https://github.com/broadinstitute/gatk/issues/8517:2621,Testability,test,test,2621," INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 10:19:39.337 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 10:19:39.337 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 10:19:39.337 INFO GenomicsDBImport - Deflater: IntelDeflater; 10:19:39.337 INFO GenomicsDBImport - Inflater: IntelInflater; 10:19:39.337 INFO GenomicsDBImport - GCS max retries/reopens: 20; 10:19:39.338 INFO GenomicsDBImport - Requester pays: disabled; 10:19:39.338 INFO GenomicsDBImport - Initializing engine; 10:19:39.489 INFO IntervalArgumentCollection - Processing 100 bp from intervals; 10:19:39.490 INFO GenomicsDBImport - Done initializing engine; 10:19:39.948 INFO GenomicsDBLibLoader - GenomicsDB native library version : 1.4.4-ce4e1b9; 10:19:39.951 INFO GenomicsDBImport - Vid Map JSON file will be written to /home/test/Software/gatk-4.4.0.0/test/./02/vidmap.json; 10:19:39.951 INFO GenomicsDBImport - Callset Map JSON file will be written to /home/test/Software/gatk-4.4.0.0/test/./02/callset.json; 10:19:39.951 INFO GenomicsDBImport - Complete VCF Header will be written to /home/test/Software/gatk-4.4.0.0/test/./02/vcfheader.vcf; 10:19:39.951 INFO GenomicsDBImport - Importing to workspace - /home/test/Software/gatk-4.4.0.0/test/./02; 10:19:40.060 INFO GenomicsDBImport - Importing batch 1 with 2 samples; 10:19:40.075 INFO GenomicsDBImport - Shutting down engine; org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=285212672; java.lang.NumberFormatException: For input string: ""G""; 	at java.base/java.lang.NumberFormatException.forInputString(NumberFormatException.java:67); 	at java.base/java.lang.Integer.parseInt(Integer.java:668); 	at java.base/java.lang.Integer.parseInt(Integer.java:786); 	at htsjdk.tribble.readers.TabixReader.getIntv(TabixReader.java:337); 	at htsjdk.tribble.readers.TabixReader.access$500(TabixReader.java:48); 	at htsjd",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8517
https://github.com/broadinstitute/gatk/issues/8517:2648,Testability,test,test,2648,"AD_FOR_SAMTOOLS : false; 10:19:39.337 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 10:19:39.337 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 10:19:39.337 INFO GenomicsDBImport - Deflater: IntelDeflater; 10:19:39.337 INFO GenomicsDBImport - Inflater: IntelInflater; 10:19:39.337 INFO GenomicsDBImport - GCS max retries/reopens: 20; 10:19:39.338 INFO GenomicsDBImport - Requester pays: disabled; 10:19:39.338 INFO GenomicsDBImport - Initializing engine; 10:19:39.489 INFO IntervalArgumentCollection - Processing 100 bp from intervals; 10:19:39.490 INFO GenomicsDBImport - Done initializing engine; 10:19:39.948 INFO GenomicsDBLibLoader - GenomicsDB native library version : 1.4.4-ce4e1b9; 10:19:39.951 INFO GenomicsDBImport - Vid Map JSON file will be written to /home/test/Software/gatk-4.4.0.0/test/./02/vidmap.json; 10:19:39.951 INFO GenomicsDBImport - Callset Map JSON file will be written to /home/test/Software/gatk-4.4.0.0/test/./02/callset.json; 10:19:39.951 INFO GenomicsDBImport - Complete VCF Header will be written to /home/test/Software/gatk-4.4.0.0/test/./02/vcfheader.vcf; 10:19:39.951 INFO GenomicsDBImport - Importing to workspace - /home/test/Software/gatk-4.4.0.0/test/./02; 10:19:40.060 INFO GenomicsDBImport - Importing batch 1 with 2 samples; 10:19:40.075 INFO GenomicsDBImport - Shutting down engine; org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=285212672; java.lang.NumberFormatException: For input string: ""G""; 	at java.base/java.lang.NumberFormatException.forInputString(NumberFormatException.java:67); 	at java.base/java.lang.Integer.parseInt(Integer.java:668); 	at java.base/java.lang.Integer.parseInt(Integer.java:786); 	at htsjdk.tribble.readers.TabixReader.getIntv(TabixReader.java:337); 	at htsjdk.tribble.readers.TabixReader.access$500(TabixReader.java:48); 	at htsjdk.tribble.readers.TabixReader$IteratorImpl.next(TabixRea",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8517
https://github.com/broadinstitute/gatk/issues/8517:2754,Testability,test,test,2754,"USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 10:19:39.337 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 10:19:39.337 INFO GenomicsDBImport - Deflater: IntelDeflater; 10:19:39.337 INFO GenomicsDBImport - Inflater: IntelInflater; 10:19:39.337 INFO GenomicsDBImport - GCS max retries/reopens: 20; 10:19:39.338 INFO GenomicsDBImport - Requester pays: disabled; 10:19:39.338 INFO GenomicsDBImport - Initializing engine; 10:19:39.489 INFO IntervalArgumentCollection - Processing 100 bp from intervals; 10:19:39.490 INFO GenomicsDBImport - Done initializing engine; 10:19:39.948 INFO GenomicsDBLibLoader - GenomicsDB native library version : 1.4.4-ce4e1b9; 10:19:39.951 INFO GenomicsDBImport - Vid Map JSON file will be written to /home/test/Software/gatk-4.4.0.0/test/./02/vidmap.json; 10:19:39.951 INFO GenomicsDBImport - Callset Map JSON file will be written to /home/test/Software/gatk-4.4.0.0/test/./02/callset.json; 10:19:39.951 INFO GenomicsDBImport - Complete VCF Header will be written to /home/test/Software/gatk-4.4.0.0/test/./02/vcfheader.vcf; 10:19:39.951 INFO GenomicsDBImport - Importing to workspace - /home/test/Software/gatk-4.4.0.0/test/./02; 10:19:40.060 INFO GenomicsDBImport - Importing batch 1 with 2 samples; 10:19:40.075 INFO GenomicsDBImport - Shutting down engine; org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=285212672; java.lang.NumberFormatException: For input string: ""G""; 	at java.base/java.lang.NumberFormatException.forInputString(NumberFormatException.java:67); 	at java.base/java.lang.Integer.parseInt(Integer.java:668); 	at java.base/java.lang.Integer.parseInt(Integer.java:786); 	at htsjdk.tribble.readers.TabixReader.getIntv(TabixReader.java:337); 	at htsjdk.tribble.readers.TabixReader.access$500(TabixReader.java:48); 	at htsjdk.tribble.readers.TabixReader$IteratorImpl.next(TabixReader.java:438); 	at htsjdk.tribble.readers.TabixIteratorLineReader.readLine(Tab",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8517
https://github.com/broadinstitute/gatk/issues/8517:2781,Testability,test,test,2781,"FO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 10:19:39.337 INFO GenomicsDBImport - Deflater: IntelDeflater; 10:19:39.337 INFO GenomicsDBImport - Inflater: IntelInflater; 10:19:39.337 INFO GenomicsDBImport - GCS max retries/reopens: 20; 10:19:39.338 INFO GenomicsDBImport - Requester pays: disabled; 10:19:39.338 INFO GenomicsDBImport - Initializing engine; 10:19:39.489 INFO IntervalArgumentCollection - Processing 100 bp from intervals; 10:19:39.490 INFO GenomicsDBImport - Done initializing engine; 10:19:39.948 INFO GenomicsDBLibLoader - GenomicsDB native library version : 1.4.4-ce4e1b9; 10:19:39.951 INFO GenomicsDBImport - Vid Map JSON file will be written to /home/test/Software/gatk-4.4.0.0/test/./02/vidmap.json; 10:19:39.951 INFO GenomicsDBImport - Callset Map JSON file will be written to /home/test/Software/gatk-4.4.0.0/test/./02/callset.json; 10:19:39.951 INFO GenomicsDBImport - Complete VCF Header will be written to /home/test/Software/gatk-4.4.0.0/test/./02/vcfheader.vcf; 10:19:39.951 INFO GenomicsDBImport - Importing to workspace - /home/test/Software/gatk-4.4.0.0/test/./02; 10:19:40.060 INFO GenomicsDBImport - Importing batch 1 with 2 samples; 10:19:40.075 INFO GenomicsDBImport - Shutting down engine; org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=285212672; java.lang.NumberFormatException: For input string: ""G""; 	at java.base/java.lang.NumberFormatException.forInputString(NumberFormatException.java:67); 	at java.base/java.lang.Integer.parseInt(Integer.java:668); 	at java.base/java.lang.Integer.parseInt(Integer.java:786); 	at htsjdk.tribble.readers.TabixReader.getIntv(TabixReader.java:337); 	at htsjdk.tribble.readers.TabixReader.access$500(TabixReader.java:48); 	at htsjdk.tribble.readers.TabixReader$IteratorImpl.next(TabixReader.java:438); 	at htsjdk.tribble.readers.TabixIteratorLineReader.readLine(TabixIteratorLineReader.java:46); 	at htsjdk.tribble.Tabix",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8517
https://github.com/broadinstitute/gatk/issues/8517:2874,Testability,test,test,2874,"alse; 10:19:39.337 INFO GenomicsDBImport - Deflater: IntelDeflater; 10:19:39.337 INFO GenomicsDBImport - Inflater: IntelInflater; 10:19:39.337 INFO GenomicsDBImport - GCS max retries/reopens: 20; 10:19:39.338 INFO GenomicsDBImport - Requester pays: disabled; 10:19:39.338 INFO GenomicsDBImport - Initializing engine; 10:19:39.489 INFO IntervalArgumentCollection - Processing 100 bp from intervals; 10:19:39.490 INFO GenomicsDBImport - Done initializing engine; 10:19:39.948 INFO GenomicsDBLibLoader - GenomicsDB native library version : 1.4.4-ce4e1b9; 10:19:39.951 INFO GenomicsDBImport - Vid Map JSON file will be written to /home/test/Software/gatk-4.4.0.0/test/./02/vidmap.json; 10:19:39.951 INFO GenomicsDBImport - Callset Map JSON file will be written to /home/test/Software/gatk-4.4.0.0/test/./02/callset.json; 10:19:39.951 INFO GenomicsDBImport - Complete VCF Header will be written to /home/test/Software/gatk-4.4.0.0/test/./02/vcfheader.vcf; 10:19:39.951 INFO GenomicsDBImport - Importing to workspace - /home/test/Software/gatk-4.4.0.0/test/./02; 10:19:40.060 INFO GenomicsDBImport - Importing batch 1 with 2 samples; 10:19:40.075 INFO GenomicsDBImport - Shutting down engine; org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=285212672; java.lang.NumberFormatException: For input string: ""G""; 	at java.base/java.lang.NumberFormatException.forInputString(NumberFormatException.java:67); 	at java.base/java.lang.Integer.parseInt(Integer.java:668); 	at java.base/java.lang.Integer.parseInt(Integer.java:786); 	at htsjdk.tribble.readers.TabixReader.getIntv(TabixReader.java:337); 	at htsjdk.tribble.readers.TabixReader.access$500(TabixReader.java:48); 	at htsjdk.tribble.readers.TabixReader$IteratorImpl.next(TabixReader.java:438); 	at htsjdk.tribble.readers.TabixIteratorLineReader.readLine(TabixIteratorLineReader.java:46); 	at htsjdk.tribble.TabixFeatureReader$FeatureIterator.readNextRecord(TabixFeatureReader.java:170",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8517
https://github.com/broadinstitute/gatk/issues/8517:2901,Testability,test,test,2901,"ter: IntelDeflater; 10:19:39.337 INFO GenomicsDBImport - Inflater: IntelInflater; 10:19:39.337 INFO GenomicsDBImport - GCS max retries/reopens: 20; 10:19:39.338 INFO GenomicsDBImport - Requester pays: disabled; 10:19:39.338 INFO GenomicsDBImport - Initializing engine; 10:19:39.489 INFO IntervalArgumentCollection - Processing 100 bp from intervals; 10:19:39.490 INFO GenomicsDBImport - Done initializing engine; 10:19:39.948 INFO GenomicsDBLibLoader - GenomicsDB native library version : 1.4.4-ce4e1b9; 10:19:39.951 INFO GenomicsDBImport - Vid Map JSON file will be written to /home/test/Software/gatk-4.4.0.0/test/./02/vidmap.json; 10:19:39.951 INFO GenomicsDBImport - Callset Map JSON file will be written to /home/test/Software/gatk-4.4.0.0/test/./02/callset.json; 10:19:39.951 INFO GenomicsDBImport - Complete VCF Header will be written to /home/test/Software/gatk-4.4.0.0/test/./02/vcfheader.vcf; 10:19:39.951 INFO GenomicsDBImport - Importing to workspace - /home/test/Software/gatk-4.4.0.0/test/./02; 10:19:40.060 INFO GenomicsDBImport - Importing batch 1 with 2 samples; 10:19:40.075 INFO GenomicsDBImport - Shutting down engine; org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=285212672; java.lang.NumberFormatException: For input string: ""G""; 	at java.base/java.lang.NumberFormatException.forInputString(NumberFormatException.java:67); 	at java.base/java.lang.Integer.parseInt(Integer.java:668); 	at java.base/java.lang.Integer.parseInt(Integer.java:786); 	at htsjdk.tribble.readers.TabixReader.getIntv(TabixReader.java:337); 	at htsjdk.tribble.readers.TabixReader.access$500(TabixReader.java:48); 	at htsjdk.tribble.readers.TabixReader$IteratorImpl.next(TabixReader.java:438); 	at htsjdk.tribble.readers.TabixIteratorLineReader.readLine(TabixIteratorLineReader.java:46); 	at htsjdk.tribble.TabixFeatureReader$FeatureIterator.readNextRecord(TabixFeatureReader.java:170); 	at htsjdk.tribble.TabixFeatureReader$Feature",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8517
https://github.com/broadinstitute/gatk/pull/8518:0,Deployability,Update,Update,0,Update the change log in preparation for releasing version 0.3.2. of the Beta workflow.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8518
https://github.com/broadinstitute/gatk/pull/8518:18,Testability,log,log,18,Update the change log in preparation for releasing version 0.3.2. of the Beta workflow.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8518
https://github.com/broadinstitute/gatk/issues/8520:2708,Availability,avail,available,2708,"-SORTING_COLLECTION_SIZE_RATIO 0.25 --TAG_DUPLICATE_SET_MEMBERS false --REMOVE_SEQUENCING_DUPLICATES false --TAGGING_POLICY DontTag --CLEAR_DT true --DUPLEX_UMI false --ADD_PG_TAG_TO_READS true --ASSUME_SORTED false --DUPLICATE_SCORING_STRATEGY SUM_OF_BASE_QUALITIES --PROGRAM_RECORD_ID MarkDuplicates --PROGRAM_GROUP_NAME MarkDuplicates --READ_NAME_REGEX <optimized capture of last three ':' separated fields as numeric values> --OPTICAL_DUPLICATE_PIXEL_DISTANCE 100 --MAX_OPTICAL_DUPLICATE_SET_SIZE 300000 --VERBOSITY INFO --QUIET false --COMPRESSION_LEVEL 2 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX false --CREATE_MD5_FILE false --GA4GH_CLIENT_SECRETS client_secrets.json --help false --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATER false; Sep 14, 2023 1:41:23 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; [Thu Sep 14 01:41:23 PDT 2023] Executing as ionadmin@proton-torrent-server on Linux 2.6.32-21-server amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_201-b09; Deflater: Intel; Inflater: Intel; Provider GCS is available; Picard version: Version:4.1.2.0; INFO 2023-09-14 01:41:23 MarkDuplicates Start of doWork freeMemory: 2396610552; totalMemory: 2423259136; maxMemory: 61084270592; INFO 2023-09-14 01:41:23 MarkDuplicates Reading input file and constructing read end information.; INFO 2023-09-14 01:41:23 MarkDuplicates Will retain up to 221319820 data points before spilling to disk. ### Affected version(s); gatk 4.1.2.0. ### Description ; the output information is just stopped at ""INFO 2023-09-14 01:41:23 MarkDuplicates Will retain up to 221319820 data points before spilling to disk."", it should runs more information out. and there is no output for the rmdup bam. #### Expected behavior; it should finish it running and output the result. ## Feature request. ### Tool(s) or class(es) involved; JAVA_HOME = /usr/lib/jvm/jdk1.8.0_201/",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8520
https://github.com/broadinstitute/gatk/issues/8520:795,Performance,Load,Loading,795,### Instructions; I have run java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx64g -jar /rawdata/software-wes/software/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar MarkDuplicates -I /rawdata/2023result/0914/chip5868/R5868.2023.09.10.161.PT50493WR2/1_BAM/R5868.2023.09.10.161.PT50493WR2.sorted.bam --REMOVE_DUPLICATES TRUE --VALIDATION_STRINGENCY SILENT -O /rawdata/2023result/0914/chip5868/R5868.2023.09.10.161.PT50493WR2/1_BAM/R5868.2023.09.10.161.PT50493WR2.sorted.rmdup.bam -M /rawdata/2023result/0914/chip5868/R5868.2023.09.10.161.PT50493WR2/1_BAM/R5868.2023.09.10.161.PT50493WR2.sorted.rmdup.stat. ----. ## Bug Report; it just returns ; 01:41:21.972 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/rawdata/software-wes/software/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; [Thu Sep 14 01:41:21 PDT 2023] MarkDuplicates --INPUT /rawdata/2023result/0914/chip5868/R5868.2023.09.10.161.PT50493WR2/1_BAM/R5868.2023.09.10.161.PT50493WR2.sorted.bam --OUTPUT /rawdata/2023result/0914/chip5868/R5868.2023.09.10.161.PT50493WR2/1_BAM/R5868.2023.09.10.161.PT50493WR2.sorted.rmdup.bam --METRICS_FILE /rawdata/2023result/0914/chip5868/R5868.2023.09.10.161.PT50493WR2/1_BAM/R5868.2023.09.10.161.PT50493WR2.sorted.rmdup.stat --REMOVE_DUPLICATES true --VALIDATION_STRINGENCY SILENT --MAX_SEQUENCES_FOR_DISK_READ_ENDS_MAP 50000 --MAX_FILE_HANDLES_FOR_READ_ENDS_MAP 8000 --SORTING_COLLECTION_SIZE_RATIO 0.25 --TAG_DUPLICATE_SET_MEMBERS false --REMOVE_SEQUENCING_DUPLICATES false --TAGGING_POLICY DontTag --CLEAR_DT true --DUPLEX_UMI false --ADD_PG_TAG_TO_READS true --ASSUME_SORTED false --DUPLICATE_SCORING_STRATEGY SUM_OF_BASE_QUALITIES --PROGRAM_RECORD_ID MarkDuplicates --PROGRAM_GROUP_NAME MarkDuplicates --READ_NAME_REGEX <optimized capture of last three ':' separated fields as numeric values> --OPTICAL_DUPLICATE_PIXEL_DISTANC,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8520
https://github.com/broadinstitute/gatk/issues/8520:1896,Performance,optimiz,optimized,1896,"kage-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; [Thu Sep 14 01:41:21 PDT 2023] MarkDuplicates --INPUT /rawdata/2023result/0914/chip5868/R5868.2023.09.10.161.PT50493WR2/1_BAM/R5868.2023.09.10.161.PT50493WR2.sorted.bam --OUTPUT /rawdata/2023result/0914/chip5868/R5868.2023.09.10.161.PT50493WR2/1_BAM/R5868.2023.09.10.161.PT50493WR2.sorted.rmdup.bam --METRICS_FILE /rawdata/2023result/0914/chip5868/R5868.2023.09.10.161.PT50493WR2/1_BAM/R5868.2023.09.10.161.PT50493WR2.sorted.rmdup.stat --REMOVE_DUPLICATES true --VALIDATION_STRINGENCY SILENT --MAX_SEQUENCES_FOR_DISK_READ_ENDS_MAP 50000 --MAX_FILE_HANDLES_FOR_READ_ENDS_MAP 8000 --SORTING_COLLECTION_SIZE_RATIO 0.25 --TAG_DUPLICATE_SET_MEMBERS false --REMOVE_SEQUENCING_DUPLICATES false --TAGGING_POLICY DontTag --CLEAR_DT true --DUPLEX_UMI false --ADD_PG_TAG_TO_READS true --ASSUME_SORTED false --DUPLICATE_SCORING_STRATEGY SUM_OF_BASE_QUALITIES --PROGRAM_RECORD_ID MarkDuplicates --PROGRAM_GROUP_NAME MarkDuplicates --READ_NAME_REGEX <optimized capture of last three ':' separated fields as numeric values> --OPTICAL_DUPLICATE_PIXEL_DISTANCE 100 --MAX_OPTICAL_DUPLICATE_SET_SIZE 300000 --VERBOSITY INFO --QUIET false --COMPRESSION_LEVEL 2 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX false --CREATE_MD5_FILE false --GA4GH_CLIENT_SECRETS client_secrets.json --help false --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATER false; Sep 14, 2023 1:41:23 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; [Thu Sep 14 01:41:23 PDT 2023] Executing as ionadmin@proton-torrent-server on Linux 2.6.32-21-server amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_201-b09; Deflater: Intel; Inflater: Intel; Provider GCS is available; Picard version: Version:4.1.2.0; INFO 2023-09-14 01:41:23 MarkDuplicates Start of doWork freeMemory: 2396610552; totalMemory: 2423259136; maxMemory: 61084270592; INFO 2023-0",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8520
https://github.com/broadinstitute/gatk/issues/8520:2444,Safety,detect,detect,2444,"ENCY SILENT --MAX_SEQUENCES_FOR_DISK_READ_ENDS_MAP 50000 --MAX_FILE_HANDLES_FOR_READ_ENDS_MAP 8000 --SORTING_COLLECTION_SIZE_RATIO 0.25 --TAG_DUPLICATE_SET_MEMBERS false --REMOVE_SEQUENCING_DUPLICATES false --TAGGING_POLICY DontTag --CLEAR_DT true --DUPLEX_UMI false --ADD_PG_TAG_TO_READS true --ASSUME_SORTED false --DUPLICATE_SCORING_STRATEGY SUM_OF_BASE_QUALITIES --PROGRAM_RECORD_ID MarkDuplicates --PROGRAM_GROUP_NAME MarkDuplicates --READ_NAME_REGEX <optimized capture of last three ':' separated fields as numeric values> --OPTICAL_DUPLICATE_PIXEL_DISTANCE 100 --MAX_OPTICAL_DUPLICATE_SET_SIZE 300000 --VERBOSITY INFO --QUIET false --COMPRESSION_LEVEL 2 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX false --CREATE_MD5_FILE false --GA4GH_CLIENT_SECRETS client_secrets.json --help false --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATER false; Sep 14, 2023 1:41:23 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; [Thu Sep 14 01:41:23 PDT 2023] Executing as ionadmin@proton-torrent-server on Linux 2.6.32-21-server amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_201-b09; Deflater: Intel; Inflater: Intel; Provider GCS is available; Picard version: Version:4.1.2.0; INFO 2023-09-14 01:41:23 MarkDuplicates Start of doWork freeMemory: 2396610552; totalMemory: 2423259136; maxMemory: 61084270592; INFO 2023-09-14 01:41:23 MarkDuplicates Reading input file and constructing read end information.; INFO 2023-09-14 01:41:23 MarkDuplicates Will retain up to 221319820 data points before spilling to disk. ### Affected version(s); gatk 4.1.2.0. ### Description ; the output information is just stopped at ""INFO 2023-09-14 01:41:23 MarkDuplicates Will retain up to 221319820 data points before spilling to disk."", it should runs more information out. and there is no output for the rmdup bam. #### Expected behavior; it should finish it running and output the r",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8520
https://github.com/broadinstitute/gatk/pull/8521:649,Testability,test,tests,649,"- Complex (CPX) SV records no longer use `END2`/`CHR2` INFO fields and instead are defined by `CPX_INTERVALS` in addition to normal coordinates.; - Adds new class `SVCallRecord.ComplexEventInterval` for storing complex event intervals, which are associated with a particular SV type.; - SV clustering now takes into account CPX subtype and intervals, requiring intervals to match on type and interval similarity as defined by the clustering parameters (e.g. minimum reciprocal overlap).; - Improved reciprocal translocation (CTX) record handling. In particular, these records are now treated similarly to BNDs for clustering purposes.; - Added unit tests for new CPX cases.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8521
https://github.com/broadinstitute/gatk/issues/8522:8675,Availability,down,down,8675,"essMeter - chr1:145584694 70.1 174825000 2494654.9; 20:50:40.338 INFO ProgressMeter - chr1:146077591 70.2 175095000 2492576.7; ...; 21:17:56.047 INFO ProgressMeter - chr6:31828873 97.5 222011000 2276839.7; 21:18:06.108 INFO ProgressMeter - chr6:31829365 97.7 222402000 2276934.0; 21:18:16.447 INFO ProgressMeter - chr6:31829459 97.8 222828000 2277277.9; 21:18:26.537 INFO ProgressMeter - chr6:31871533 98.0 223264000 2277819.0; 21:18:36.548 INFO ProgressMeter - chr6:32031575 98.2 223609000 2277462.0; 21:18:46.550 INFO ProgressMeter - chr6:33694905 98.4 223890000 2276459.3; 21:18:56.882 INFO ProgressMeter - chr6:42652593 98.5 224124000 2274855.2; 21:19:06.922 INFO ProgressMeter - chr6:46150821 98.7 224356000 2273348.8; 21:19:16.925 INFO ProgressMeter - chr6:56607518 98.9 224691000 2272903.7; 21:19:26.951 INFO ProgressMeter - chr6:72182472 99.0 224990000 2272087.7; 21:19:36.956 INFO ProgressMeter - chr6:73519441 99.2 225334000 2271736.1; 21:19:44.136 INFO SplitNCigarReads - Shutting down engine; [September 14, 2023 9:19:44 PM BST] org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads done. Elapsed time: 99.33 minutes.; Runtime.totalMemory()=5453119488; htsjdk.samtools.util.RuntimeIOException: Attempt to add record to closed writer.; at htsjdk.samtools.util.AbstractAsyncWriter.write(AbstractAsyncWriter.java:57); at htsjdk.samtools.AsyncSAMFileWriter.addAlignment(AsyncSAMFileWriter.java:53); at org.broadinstitute.hellbender.utils.read.SAMFileGATKReadWriter.addRead(SAMFileGATKReadWriter.java:21); at org.broadinstitute.hellbender.tools.walkers.rnaseq.OverhangFixingManager.writeReads(OverhangFixingManager.java:358); at org.broadinstitute.hellbender.tools.walkers.rnaseq.OverhangFixingManager.flush(OverhangFixingManager.java:338); at org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads.closeTool(SplitNCigarReads.java:192); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1091); at org.broadinstitute.hellbender.cmdline.CommandLine",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8522
https://github.com/broadinstitute/gatk/issues/8522:10556,Deployability,pipeline,pipeline,10556,"igarReads done. Elapsed time: 99.33 minutes.; Runtime.totalMemory()=5453119488; htsjdk.samtools.util.RuntimeIOException: Attempt to add record to closed writer.; at htsjdk.samtools.util.AbstractAsyncWriter.write(AbstractAsyncWriter.java:57); at htsjdk.samtools.AsyncSAMFileWriter.addAlignment(AsyncSAMFileWriter.java:53); at org.broadinstitute.hellbender.utils.read.SAMFileGATKReadWriter.addRead(SAMFileGATKReadWriter.java:21); at org.broadinstitute.hellbender.tools.walkers.rnaseq.OverhangFixingManager.writeReads(OverhangFixingManager.java:358); at org.broadinstitute.hellbender.tools.walkers.rnaseq.OverhangFixingManager.flush(OverhangFixingManager.java:338); at org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads.closeTool(SplitNCigarReads.java:192); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1091); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289). This looks similar to issue #8232, but I've not been able to solve this using any of the fixes suggested on that page. I'm using Java version 8, the input BAMs were mapped using STAR via nfcore rnaseq without issue and I have plenty of space on my disk drive, even when specifying a temp directory. It also doesn't matter if I run the tool independently using the command above or as part of a pre-configured pipeline, and I get the same issue with SplitNCigarReads acting as if it has running out of space. . How do I fix this? I need this step to run variant calling on my rnaseq samples (I'm using the GATK best practices pipeline).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8522
https://github.com/broadinstitute/gatk/issues/8522:10772,Deployability,pipeline,pipeline,10772,"igarReads done. Elapsed time: 99.33 minutes.; Runtime.totalMemory()=5453119488; htsjdk.samtools.util.RuntimeIOException: Attempt to add record to closed writer.; at htsjdk.samtools.util.AbstractAsyncWriter.write(AbstractAsyncWriter.java:57); at htsjdk.samtools.AsyncSAMFileWriter.addAlignment(AsyncSAMFileWriter.java:53); at org.broadinstitute.hellbender.utils.read.SAMFileGATKReadWriter.addRead(SAMFileGATKReadWriter.java:21); at org.broadinstitute.hellbender.tools.walkers.rnaseq.OverhangFixingManager.writeReads(OverhangFixingManager.java:358); at org.broadinstitute.hellbender.tools.walkers.rnaseq.OverhangFixingManager.flush(OverhangFixingManager.java:338); at org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads.closeTool(SplitNCigarReads.java:192); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1091); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289). This looks similar to issue #8232, but I've not been able to solve this using any of the fixes suggested on that page. I'm using Java version 8, the input BAMs were mapped using STAR via nfcore rnaseq without issue and I have plenty of space on my disk drive, even when specifying a temp directory. It also doesn't matter if I run the tool independently using the command above or as part of a pre-configured pipeline, and I get the same issue with SplitNCigarReads acting as if it has running out of space. . How do I fix this? I need this step to run variant calling on my rnaseq samples (I'm using the GATK best practices pipeline).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8522
https://github.com/broadinstitute/gatk/issues/8522:433,Integrability,depend,dependencies,433,"Hi All,. ### Affected tool(s) or class(es); gatk SplitNCigarReads. ### Affected version(s); Version 4.2.5.0. I'm having a issue with running SplitNCigarReads on my local cluster. When I try to run the following command:. #!/bin/bash -l. #$ -l h_rt=48:00:0; #$ -l mem=12G; #$ -l tmpfs=20G; #$ -N bam_recalibration_rna; #$ -wd /home/regmvcr/Scratch/workspace/JSBF; #$ -pe smp 2; #$ -t 1-128. shopt -s expand_aliases. cd $TMPDIR. #Load dependencies on Myriad. module load java/temurin-8/8u322-b06; module load gatk/4.2.5.0. #Parse parameter files for inputs. number=$SGE_TASK_ID. paramfile=/home/regmvcr/Scratch/jobscripts/JSBF/file_names.txt. SAMPLE=""`sed -n ${number}p $paramfile | awk '{print $1}'`"". #run SplitNCigarReads. gatk SplitNCigarReads \; -R /home/regmvcr/Scratch/reference/sarek/resources_broad_hg38_v0_Homo_sapiens_assembly38.fasta \; -I ""/home/regmvcr/Scratch/workspace/JSBF/star_salmon/""$SAMPLE"".markdup.sorted.bam"" \; -O ""/home/regmvcr/Scratch/workspace/JSBF/SplitNCigarReads/""$SAMPLE""_split.bam"". I get the following output:. GATK: Some GATK tools require conda and associated libraries.; To use them run:; module load python/miniconda3/4.10.3; source $UCL_CONDA_PATH/etc/profile.d/conda.sh; conda activate $GATK_CONDA; Using GATK jar /shared/ucl/apps/gatk-bsd/4.2.5.0/gatk-4.2.5.0/gatk-package-4.2.5.0-local.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /shar; ed/ucl/apps/gatk-bsd/4.2.5.0/gatk-4.2.5.0/gatk-package-4.2.5.0-local.jar SplitNCigarReads -R /home/regmvcr/Scratch/reference/sarek/resources_broad_hg38_v0_Homo_sapiens_assemb; ly38.fasta -I /home/regmvcr/Scratch/workspace/JSBF/star_salmon/I3O-MC-JSBF-100-1003.markdup.sorted.bam -O /home/regmvcr/Scratch/workspace/JSBF/SplitNCigarReads/I3O-MC-JSBF-10; 0-1003_split.bam; 19:40:24.551 INFO NativeLibraryLoader - Loading libgkl_compression.so fr",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8522
https://github.com/broadinstitute/gatk/issues/8522:1352,Modifiability,variab,variable,1352,"#$ -pe smp 2; #$ -t 1-128. shopt -s expand_aliases. cd $TMPDIR. #Load dependencies on Myriad. module load java/temurin-8/8u322-b06; module load gatk/4.2.5.0. #Parse parameter files for inputs. number=$SGE_TASK_ID. paramfile=/home/regmvcr/Scratch/jobscripts/JSBF/file_names.txt. SAMPLE=""`sed -n ${number}p $paramfile | awk '{print $1}'`"". #run SplitNCigarReads. gatk SplitNCigarReads \; -R /home/regmvcr/Scratch/reference/sarek/resources_broad_hg38_v0_Homo_sapiens_assembly38.fasta \; -I ""/home/regmvcr/Scratch/workspace/JSBF/star_salmon/""$SAMPLE"".markdup.sorted.bam"" \; -O ""/home/regmvcr/Scratch/workspace/JSBF/SplitNCigarReads/""$SAMPLE""_split.bam"". I get the following output:. GATK: Some GATK tools require conda and associated libraries.; To use them run:; module load python/miniconda3/4.10.3; source $UCL_CONDA_PATH/etc/profile.d/conda.sh; conda activate $GATK_CONDA; Using GATK jar /shared/ucl/apps/gatk-bsd/4.2.5.0/gatk-4.2.5.0/gatk-package-4.2.5.0-local.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /shar; ed/ucl/apps/gatk-bsd/4.2.5.0/gatk-4.2.5.0/gatk-package-4.2.5.0-local.jar SplitNCigarReads -R /home/regmvcr/Scratch/reference/sarek/resources_broad_hg38_v0_Homo_sapiens_assemb; ly38.fasta -I /home/regmvcr/Scratch/workspace/JSBF/star_salmon/I3O-MC-JSBF-100-1003.markdup.sorted.bam -O /home/regmvcr/Scratch/workspace/JSBF/SplitNCigarReads/I3O-MC-JSBF-10; 0-1003_split.bam; 19:40:24.551 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/lustre/shared/ucl/apps/gatk-bsd/4.2.5.0/gatk-4.2.5.0/gatk-package-4.2.5.0-local.jar!/com; /intel/gkl/native/libgkl_compression.so; Sep 14, 2023 7:40:24 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 19:40:24.716 INFO SplitNCigarRea",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8522
https://github.com/broadinstitute/gatk/issues/8522:10545,Modifiability,config,configured,10545,"igarReads done. Elapsed time: 99.33 minutes.; Runtime.totalMemory()=5453119488; htsjdk.samtools.util.RuntimeIOException: Attempt to add record to closed writer.; at htsjdk.samtools.util.AbstractAsyncWriter.write(AbstractAsyncWriter.java:57); at htsjdk.samtools.AsyncSAMFileWriter.addAlignment(AsyncSAMFileWriter.java:53); at org.broadinstitute.hellbender.utils.read.SAMFileGATKReadWriter.addRead(SAMFileGATKReadWriter.java:21); at org.broadinstitute.hellbender.tools.walkers.rnaseq.OverhangFixingManager.writeReads(OverhangFixingManager.java:358); at org.broadinstitute.hellbender.tools.walkers.rnaseq.OverhangFixingManager.flush(OverhangFixingManager.java:338); at org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads.closeTool(SplitNCigarReads.java:192); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1091); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289). This looks similar to issue #8232, but I've not been able to solve this using any of the fixes suggested on that page. I'm using Java version 8, the input BAMs were mapped using STAR via nfcore rnaseq without issue and I have plenty of space on my disk drive, even when specifying a temp directory. It also doesn't matter if I run the tool independently using the command above or as part of a pre-configured pipeline, and I get the same issue with SplitNCigarReads acting as if it has running out of space. . How do I fix this? I need this step to run variant calling on my rnaseq samples (I'm using the GATK best practices pipeline).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8522
https://github.com/broadinstitute/gatk/issues/8522:428,Performance,Load,Load,428,"Hi All,. ### Affected tool(s) or class(es); gatk SplitNCigarReads. ### Affected version(s); Version 4.2.5.0. I'm having a issue with running SplitNCigarReads on my local cluster. When I try to run the following command:. #!/bin/bash -l. #$ -l h_rt=48:00:0; #$ -l mem=12G; #$ -l tmpfs=20G; #$ -N bam_recalibration_rna; #$ -wd /home/regmvcr/Scratch/workspace/JSBF; #$ -pe smp 2; #$ -t 1-128. shopt -s expand_aliases. cd $TMPDIR. #Load dependencies on Myriad. module load java/temurin-8/8u322-b06; module load gatk/4.2.5.0. #Parse parameter files for inputs. number=$SGE_TASK_ID. paramfile=/home/regmvcr/Scratch/jobscripts/JSBF/file_names.txt. SAMPLE=""`sed -n ${number}p $paramfile | awk '{print $1}'`"". #run SplitNCigarReads. gatk SplitNCigarReads \; -R /home/regmvcr/Scratch/reference/sarek/resources_broad_hg38_v0_Homo_sapiens_assembly38.fasta \; -I ""/home/regmvcr/Scratch/workspace/JSBF/star_salmon/""$SAMPLE"".markdup.sorted.bam"" \; -O ""/home/regmvcr/Scratch/workspace/JSBF/SplitNCigarReads/""$SAMPLE""_split.bam"". I get the following output:. GATK: Some GATK tools require conda and associated libraries.; To use them run:; module load python/miniconda3/4.10.3; source $UCL_CONDA_PATH/etc/profile.d/conda.sh; conda activate $GATK_CONDA; Using GATK jar /shared/ucl/apps/gatk-bsd/4.2.5.0/gatk-4.2.5.0/gatk-package-4.2.5.0-local.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /shar; ed/ucl/apps/gatk-bsd/4.2.5.0/gatk-4.2.5.0/gatk-package-4.2.5.0-local.jar SplitNCigarReads -R /home/regmvcr/Scratch/reference/sarek/resources_broad_hg38_v0_Homo_sapiens_assemb; ly38.fasta -I /home/regmvcr/Scratch/workspace/JSBF/star_salmon/I3O-MC-JSBF-100-1003.markdup.sorted.bam -O /home/regmvcr/Scratch/workspace/JSBF/SplitNCigarReads/I3O-MC-JSBF-10; 0-1003_split.bam; 19:40:24.551 INFO NativeLibraryLoader - Loading libgkl_compression.so fr",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8522
https://github.com/broadinstitute/gatk/issues/8522:464,Performance,load,load,464,"Hi All,. ### Affected tool(s) or class(es); gatk SplitNCigarReads. ### Affected version(s); Version 4.2.5.0. I'm having a issue with running SplitNCigarReads on my local cluster. When I try to run the following command:. #!/bin/bash -l. #$ -l h_rt=48:00:0; #$ -l mem=12G; #$ -l tmpfs=20G; #$ -N bam_recalibration_rna; #$ -wd /home/regmvcr/Scratch/workspace/JSBF; #$ -pe smp 2; #$ -t 1-128. shopt -s expand_aliases. cd $TMPDIR. #Load dependencies on Myriad. module load java/temurin-8/8u322-b06; module load gatk/4.2.5.0. #Parse parameter files for inputs. number=$SGE_TASK_ID. paramfile=/home/regmvcr/Scratch/jobscripts/JSBF/file_names.txt. SAMPLE=""`sed -n ${number}p $paramfile | awk '{print $1}'`"". #run SplitNCigarReads. gatk SplitNCigarReads \; -R /home/regmvcr/Scratch/reference/sarek/resources_broad_hg38_v0_Homo_sapiens_assembly38.fasta \; -I ""/home/regmvcr/Scratch/workspace/JSBF/star_salmon/""$SAMPLE"".markdup.sorted.bam"" \; -O ""/home/regmvcr/Scratch/workspace/JSBF/SplitNCigarReads/""$SAMPLE""_split.bam"". I get the following output:. GATK: Some GATK tools require conda and associated libraries.; To use them run:; module load python/miniconda3/4.10.3; source $UCL_CONDA_PATH/etc/profile.d/conda.sh; conda activate $GATK_CONDA; Using GATK jar /shared/ucl/apps/gatk-bsd/4.2.5.0/gatk-4.2.5.0/gatk-package-4.2.5.0-local.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /shar; ed/ucl/apps/gatk-bsd/4.2.5.0/gatk-4.2.5.0/gatk-package-4.2.5.0-local.jar SplitNCigarReads -R /home/regmvcr/Scratch/reference/sarek/resources_broad_hg38_v0_Homo_sapiens_assemb; ly38.fasta -I /home/regmvcr/Scratch/workspace/JSBF/star_salmon/I3O-MC-JSBF-100-1003.markdup.sorted.bam -O /home/regmvcr/Scratch/workspace/JSBF/SplitNCigarReads/I3O-MC-JSBF-10; 0-1003_split.bam; 19:40:24.551 INFO NativeLibraryLoader - Loading libgkl_compression.so fr",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8522
https://github.com/broadinstitute/gatk/issues/8522:502,Performance,load,load,502,"Hi All,. ### Affected tool(s) or class(es); gatk SplitNCigarReads. ### Affected version(s); Version 4.2.5.0. I'm having a issue with running SplitNCigarReads on my local cluster. When I try to run the following command:. #!/bin/bash -l. #$ -l h_rt=48:00:0; #$ -l mem=12G; #$ -l tmpfs=20G; #$ -N bam_recalibration_rna; #$ -wd /home/regmvcr/Scratch/workspace/JSBF; #$ -pe smp 2; #$ -t 1-128. shopt -s expand_aliases. cd $TMPDIR. #Load dependencies on Myriad. module load java/temurin-8/8u322-b06; module load gatk/4.2.5.0. #Parse parameter files for inputs. number=$SGE_TASK_ID. paramfile=/home/regmvcr/Scratch/jobscripts/JSBF/file_names.txt. SAMPLE=""`sed -n ${number}p $paramfile | awk '{print $1}'`"". #run SplitNCigarReads. gatk SplitNCigarReads \; -R /home/regmvcr/Scratch/reference/sarek/resources_broad_hg38_v0_Homo_sapiens_assembly38.fasta \; -I ""/home/regmvcr/Scratch/workspace/JSBF/star_salmon/""$SAMPLE"".markdup.sorted.bam"" \; -O ""/home/regmvcr/Scratch/workspace/JSBF/SplitNCigarReads/""$SAMPLE""_split.bam"". I get the following output:. GATK: Some GATK tools require conda and associated libraries.; To use them run:; module load python/miniconda3/4.10.3; source $UCL_CONDA_PATH/etc/profile.d/conda.sh; conda activate $GATK_CONDA; Using GATK jar /shared/ucl/apps/gatk-bsd/4.2.5.0/gatk-4.2.5.0/gatk-package-4.2.5.0-local.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /shar; ed/ucl/apps/gatk-bsd/4.2.5.0/gatk-4.2.5.0/gatk-package-4.2.5.0-local.jar SplitNCigarReads -R /home/regmvcr/Scratch/reference/sarek/resources_broad_hg38_v0_Homo_sapiens_assemb; ly38.fasta -I /home/regmvcr/Scratch/workspace/JSBF/star_salmon/I3O-MC-JSBF-100-1003.markdup.sorted.bam -O /home/regmvcr/Scratch/workspace/JSBF/SplitNCigarReads/I3O-MC-JSBF-10; 0-1003_split.bam; 19:40:24.551 INFO NativeLibraryLoader - Loading libgkl_compression.so fr",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8522
https://github.com/broadinstitute/gatk/issues/8522:1130,Performance,load,load,1130,"ith running SplitNCigarReads on my local cluster. When I try to run the following command:. #!/bin/bash -l. #$ -l h_rt=48:00:0; #$ -l mem=12G; #$ -l tmpfs=20G; #$ -N bam_recalibration_rna; #$ -wd /home/regmvcr/Scratch/workspace/JSBF; #$ -pe smp 2; #$ -t 1-128. shopt -s expand_aliases. cd $TMPDIR. #Load dependencies on Myriad. module load java/temurin-8/8u322-b06; module load gatk/4.2.5.0. #Parse parameter files for inputs. number=$SGE_TASK_ID. paramfile=/home/regmvcr/Scratch/jobscripts/JSBF/file_names.txt. SAMPLE=""`sed -n ${number}p $paramfile | awk '{print $1}'`"". #run SplitNCigarReads. gatk SplitNCigarReads \; -R /home/regmvcr/Scratch/reference/sarek/resources_broad_hg38_v0_Homo_sapiens_assembly38.fasta \; -I ""/home/regmvcr/Scratch/workspace/JSBF/star_salmon/""$SAMPLE"".markdup.sorted.bam"" \; -O ""/home/regmvcr/Scratch/workspace/JSBF/SplitNCigarReads/""$SAMPLE""_split.bam"". I get the following output:. GATK: Some GATK tools require conda and associated libraries.; To use them run:; module load python/miniconda3/4.10.3; source $UCL_CONDA_PATH/etc/profile.d/conda.sh; conda activate $GATK_CONDA; Using GATK jar /shared/ucl/apps/gatk-bsd/4.2.5.0/gatk-4.2.5.0/gatk-package-4.2.5.0-local.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /shar; ed/ucl/apps/gatk-bsd/4.2.5.0/gatk-4.2.5.0/gatk-package-4.2.5.0-local.jar SplitNCigarReads -R /home/regmvcr/Scratch/reference/sarek/resources_broad_hg38_v0_Homo_sapiens_assemb; ly38.fasta -I /home/regmvcr/Scratch/workspace/JSBF/star_salmon/I3O-MC-JSBF-100-1003.markdup.sorted.bam -O /home/regmvcr/Scratch/workspace/JSBF/SplitNCigarReads/I3O-MC-JSBF-10; 0-1003_split.bam; 19:40:24.551 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/lustre/shared/ucl/apps/gatk-bsd/4.2.5.0/gatk-4.2.5.0/gatk-package-4.2.5.0-local.jar!/com; /intel/gkl/native/libgkl_",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8522
https://github.com/broadinstitute/gatk/issues/8522:1969,Performance,Load,Loading,1969,"e/JSBF/SplitNCigarReads/""$SAMPLE""_split.bam"". I get the following output:. GATK: Some GATK tools require conda and associated libraries.; To use them run:; module load python/miniconda3/4.10.3; source $UCL_CONDA_PATH/etc/profile.d/conda.sh; conda activate $GATK_CONDA; Using GATK jar /shared/ucl/apps/gatk-bsd/4.2.5.0/gatk-4.2.5.0/gatk-package-4.2.5.0-local.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /shar; ed/ucl/apps/gatk-bsd/4.2.5.0/gatk-4.2.5.0/gatk-package-4.2.5.0-local.jar SplitNCigarReads -R /home/regmvcr/Scratch/reference/sarek/resources_broad_hg38_v0_Homo_sapiens_assemb; ly38.fasta -I /home/regmvcr/Scratch/workspace/JSBF/star_salmon/I3O-MC-JSBF-100-1003.markdup.sorted.bam -O /home/regmvcr/Scratch/workspace/JSBF/SplitNCigarReads/I3O-MC-JSBF-10; 0-1003_split.bam; 19:40:24.551 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/lustre/shared/ucl/apps/gatk-bsd/4.2.5.0/gatk-4.2.5.0/gatk-package-4.2.5.0-local.jar!/com; /intel/gkl/native/libgkl_compression.so; Sep 14, 2023 7:40:24 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 19:40:24.716 INFO SplitNCigarReads - ------------------------------------------------------------; 19:40:24.716 INFO SplitNCigarReads - The Genome Analysis Toolkit (GATK) v4.2.5.0; 19:40:24.716 INFO SplitNCigarReads - For support and documentation go to https://software.broadinstitute.org/gatk/; 19:40:24.717 INFO SplitNCigarReads - Executing as regmvcr@node-h00a-012.myriad.ucl.ac.uk on Linux v3.10.0-1160.53.1.el7.x86_64 amd64; 19:40:24.717 INFO SplitNCigarReads - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_322-b06; 19:40:24.717 INFO SplitNCigarReads - Start Date/Time: September 14, 2023 7:40:24 PM BST; 19:40:24.717 INFO SplitN",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8522
https://github.com/broadinstitute/gatk/issues/8522:2274,Safety,detect,detect,2274,"atk-bsd/4.2.5.0/gatk-4.2.5.0/gatk-package-4.2.5.0-local.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /shar; ed/ucl/apps/gatk-bsd/4.2.5.0/gatk-4.2.5.0/gatk-package-4.2.5.0-local.jar SplitNCigarReads -R /home/regmvcr/Scratch/reference/sarek/resources_broad_hg38_v0_Homo_sapiens_assemb; ly38.fasta -I /home/regmvcr/Scratch/workspace/JSBF/star_salmon/I3O-MC-JSBF-100-1003.markdup.sorted.bam -O /home/regmvcr/Scratch/workspace/JSBF/SplitNCigarReads/I3O-MC-JSBF-10; 0-1003_split.bam; 19:40:24.551 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/lustre/shared/ucl/apps/gatk-bsd/4.2.5.0/gatk-4.2.5.0/gatk-package-4.2.5.0-local.jar!/com; /intel/gkl/native/libgkl_compression.so; Sep 14, 2023 7:40:24 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 19:40:24.716 INFO SplitNCigarReads - ------------------------------------------------------------; 19:40:24.716 INFO SplitNCigarReads - The Genome Analysis Toolkit (GATK) v4.2.5.0; 19:40:24.716 INFO SplitNCigarReads - For support and documentation go to https://software.broadinstitute.org/gatk/; 19:40:24.717 INFO SplitNCigarReads - Executing as regmvcr@node-h00a-012.myriad.ucl.ac.uk on Linux v3.10.0-1160.53.1.el7.x86_64 amd64; 19:40:24.717 INFO SplitNCigarReads - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_322-b06; 19:40:24.717 INFO SplitNCigarReads - Start Date/Time: September 14, 2023 7:40:24 PM BST; 19:40:24.717 INFO SplitNCigarReads - ------------------------------------------------------------; 19:40:24.717 INFO SplitNCigarReads - ------------------------------------------------------------; 19:40:24.718 INFO SplitNCigarReads - HTSJDK Version: 2.24.1; 19:40:24.718 INFO SplitNCigarReads - Picard Version: 2.25.4; 19:40:",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8522
https://github.com/broadinstitute/gatk/issues/8523:635,Availability,error,error,635,"gatk --java-options ""-Xmx4g -DGATK_STACKTRACE_ON_USER_EXCEPTION=true"" BaseRecalibrator -I /mnt/fq2bam/sample1.markdup.sorted.bam \; -R /mnt/fq2bam/inputs/reference/files/Homo_sapiens_assembly38.fasta \; --known-sites /mnt/fq2bam/inputs/resources/files/Homo_sapiens_assembly38.known_indels.vcf.gz \; --known-sites /mnt/fq2bam/inputs/resources/files/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz \; --known-sites /mnt/fq2bam/inputs/resources/files/Homo_sapiens_assembly38.dbsnp138.vcf.gz \; -L chr1 \; -DF MappingQualityNotZeroReadFilter \; -DF MappedReadFilter \; -O /mnt/fq2bam/sample1_BQSR001.recal_data.table. I got the following error. java.lang.IllegalStateException: No cigar elements left after removing leading and trailing deletions.; at org.broadinstitute.hellbender.utils.Utils.validate(Utils.java:814); at org.broadinstitute.hellbender.utils.read.CigarBuilder.make(CigarBuilder.java:138); at org.broadinstitute.hellbender.utils.read.CigarBuilder.make(CigarBuilder.java:143); at org.broadinstitute.hellbender.utils.recalibration.BaseRecalibrationEngine.consolidateCigar(BaseRecalibrationEngine.java:293); at org.broadinstitute.hellbender.transformers.ReadTransformer.lambda$andThen$f85d1091$1(ReadTransformer.java:20); at org.broadinstitute.hellbender.transformers.ReadTransformer.lambda$andThen$f85d1091$1(ReadTransformer.java:20); at org.broadinstitute.hellbender.transformers.ReadTransformer.lambda$andThen$f85d1091$1(ReadTransformer.java:20); at org.broadinstitute.hellbender.transformers.ReadTransformer.lambda$andThen$f85d1091$1(ReadTransformer.java:20); at org.broadinstitute.hellbender.utils.recalibration.BaseRecalibrationEngine.processRead(BaseRecalibrationEngine.java:118); at org.broadinstitute.hellbender.tools.walkers.bqsr.BaseRecalibrator.apply(BaseRecalibrator.java:189); at org.broadinstitute.hellbender.engine.ReadWalker.lambda$traverse$0(ReadWalker.java:100); at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183); at java.util.stream.ReferencePipe",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8523
https://github.com/broadinstitute/gatk/issues/8523:2460,Integrability,wrap,wrapAndCopyInto,2460,itute.hellbender.transformers.ReadTransformer.lambda$andThen$f85d1091$1(ReadTransformer.java:20); at org.broadinstitute.hellbender.utils.recalibration.BaseRecalibrationEngine.processRead(BaseRecalibrationEngine.java:118); at org.broadinstitute.hellbender.tools.walkers.bqsr.BaseRecalibrator.apply(BaseRecalibrator.java:189); at org.broadinstitute.hellbender.engine.ReadWalker.lambda$traverse$0(ReadWalker.java:100); at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.Iterator.forEachRemaining(Iterator.java:116); at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472); at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150); at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485); at org.broadinstitute.hellbender.engine.ReadWalker.traverse(ReadWalker.java:98); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1085); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.m,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8523
https://github.com/broadinstitute/gatk/issues/8523:791,Security,validat,validate,791,"gatk --java-options ""-Xmx4g -DGATK_STACKTRACE_ON_USER_EXCEPTION=true"" BaseRecalibrator -I /mnt/fq2bam/sample1.markdup.sorted.bam \; -R /mnt/fq2bam/inputs/reference/files/Homo_sapiens_assembly38.fasta \; --known-sites /mnt/fq2bam/inputs/resources/files/Homo_sapiens_assembly38.known_indels.vcf.gz \; --known-sites /mnt/fq2bam/inputs/resources/files/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz \; --known-sites /mnt/fq2bam/inputs/resources/files/Homo_sapiens_assembly38.dbsnp138.vcf.gz \; -L chr1 \; -DF MappingQualityNotZeroReadFilter \; -DF MappedReadFilter \; -O /mnt/fq2bam/sample1_BQSR001.recal_data.table. I got the following error. java.lang.IllegalStateException: No cigar elements left after removing leading and trailing deletions.; at org.broadinstitute.hellbender.utils.Utils.validate(Utils.java:814); at org.broadinstitute.hellbender.utils.read.CigarBuilder.make(CigarBuilder.java:138); at org.broadinstitute.hellbender.utils.read.CigarBuilder.make(CigarBuilder.java:143); at org.broadinstitute.hellbender.utils.recalibration.BaseRecalibrationEngine.consolidateCigar(BaseRecalibrationEngine.java:293); at org.broadinstitute.hellbender.transformers.ReadTransformer.lambda$andThen$f85d1091$1(ReadTransformer.java:20); at org.broadinstitute.hellbender.transformers.ReadTransformer.lambda$andThen$f85d1091$1(ReadTransformer.java:20); at org.broadinstitute.hellbender.transformers.ReadTransformer.lambda$andThen$f85d1091$1(ReadTransformer.java:20); at org.broadinstitute.hellbender.transformers.ReadTransformer.lambda$andThen$f85d1091$1(ReadTransformer.java:20); at org.broadinstitute.hellbender.utils.recalibration.BaseRecalibrationEngine.processRead(BaseRecalibrationEngine.java:118); at org.broadinstitute.hellbender.tools.walkers.bqsr.BaseRecalibrator.apply(BaseRecalibrator.java:189); at org.broadinstitute.hellbender.engine.ReadWalker.lambda$traverse$0(ReadWalker.java:100); at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183); at java.util.stream.ReferencePipe",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8523
https://github.com/broadinstitute/gatk/pull/8524:498,Security,hash,hashed,498,"successful run; https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20cremer/job_history/cb0d2b70-64e8-455c-955e-a960db264804. Look at this beaut!!!; <img width=""1163"" alt=""Screenshot 2023-09-21 at 1 47 06 PM"" src=""https://github.com/broadinstitute/gatk/assets/6863459/c5b39555-e78e-4e54-b91f-3e1702086fb2"">. And we can add an autoscaling policy (currently here with a yaml that I shoved in the workspace bucket ahead of time---in the future we will want this to be a param or at least hashed out with Hail). <img width=""1144"" alt=""Screenshot 2023-09-22 at 1 36 07 PM"" src=""https://github.com/broadinstitute/gatk/assets/6863459/b9bf05ac-1a98-42cc-a117-24f6940bd764"">. And link to said policy (remember the ""="" sign!!!!). <img width=""1389"" alt=""Screenshot 2023-09-25 at 1 42 37 PM"" src=""https://github.com/broadinstitute/gatk/assets/6863459/5ed3e472-e4f8-4ed7-8dac-aeb917575c86"">",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8524
https://github.com/broadinstitute/gatk/pull/8525:741,Availability,down,down,741,"Largely taken from Lee's sample code, see JIRA ticket for details. Spins up a Hail cluster and runs a script to extract from a VDS to VCF files on a per-chromosome basis. Includes some refactoring to move some of the workspace-sniffing that was part of bulk ingest into more generic utility code. In terms of cluster tracking:. - Cluster name is calculated in shell script and visible in the logs; - Cluster name is written to a file which is delocalized even if the workload script fails. . Unintended but useful example [here](https://job-manager.dsde-prod.broadinstitute.org/jobs/a96667a7-e08c-43f4-abad-b55fbe7f0c06) where not only is the cluster name logged and written to an output file which is delocalized, but the cluster gets shut down anyway by cleanup code.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8525
https://github.com/broadinstitute/gatk/pull/8525:185,Modifiability,refactor,refactoring,185,"Largely taken from Lee's sample code, see JIRA ticket for details. Spins up a Hail cluster and runs a script to extract from a VDS to VCF files on a per-chromosome basis. Includes some refactoring to move some of the workspace-sniffing that was part of bulk ingest into more generic utility code. In terms of cluster tracking:. - Cluster name is calculated in shell script and visible in the logs; - Cluster name is written to a file which is delocalized even if the workload script fails. . Unintended but useful example [here](https://job-manager.dsde-prod.broadinstitute.org/jobs/a96667a7-e08c-43f4-abad-b55fbe7f0c06) where not only is the cluster name logged and written to an output file which is delocalized, but the cluster gets shut down anyway by cleanup code.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8525
https://github.com/broadinstitute/gatk/pull/8525:392,Testability,log,logs,392,"Largely taken from Lee's sample code, see JIRA ticket for details. Spins up a Hail cluster and runs a script to extract from a VDS to VCF files on a per-chromosome basis. Includes some refactoring to move some of the workspace-sniffing that was part of bulk ingest into more generic utility code. In terms of cluster tracking:. - Cluster name is calculated in shell script and visible in the logs; - Cluster name is written to a file which is delocalized even if the workload script fails. . Unintended but useful example [here](https://job-manager.dsde-prod.broadinstitute.org/jobs/a96667a7-e08c-43f4-abad-b55fbe7f0c06) where not only is the cluster name logged and written to an output file which is delocalized, but the cluster gets shut down anyway by cleanup code.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8525
https://github.com/broadinstitute/gatk/pull/8525:656,Testability,log,logged,656,"Largely taken from Lee's sample code, see JIRA ticket for details. Spins up a Hail cluster and runs a script to extract from a VDS to VCF files on a per-chromosome basis. Includes some refactoring to move some of the workspace-sniffing that was part of bulk ingest into more generic utility code. In terms of cluster tracking:. - Cluster name is calculated in shell script and visible in the logs; - Cluster name is written to a file which is delocalized even if the workload script fails. . Unintended but useful example [here](https://job-manager.dsde-prod.broadinstitute.org/jobs/a96667a7-e08c-43f4-abad-b55fbe7f0c06) where not only is the cluster name logged and written to an output file which is delocalized, but the cluster gets shut down anyway by cleanup code.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8525
https://github.com/broadinstitute/gatk/issues/8527:941,Availability,Redundant,Redundant,941,"### Instructions. I'm running on :; gatk 4.3.0.0. ; 88 cpu ; 128G mem ; 538 samples. Chrom 11-22 don't have problems, but 1-11 don't work.; What should I do?; thanks. ```; java -DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/solivehong/miniconda3/envs/bio_base/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar GenotypeGVCFs -R /data/reference/update_gatk_v0/Homo_sapiens_assembly38.fasta -V gendb://Genomicsdb.2 -O /storage/project/collaborators/UH_Burdentest/1.running/genotypeGvcf/UH_Burdentest2222.vcf --tmp-dir /storage/GenomesDbimport/Agilent_WES/tmp -L chr2 -G StandardAnnotation --only-output-calls-starting-in-intervals --use-new-qual-calculator -D /data/reference/update_gatk_v0//Homo_sapiens_assembly38.dbsnp138.vcf; 20:09:21.335 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 20:09:21.383 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/solivehong/miniconda3/envs/bio_base/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 20:09:21.521 INFO GenotypeGVCFs - ------------------------------------------------------------; 20:09:21.521 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.3.0.0; 20:09:21.521 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 20:09:21.521 INFO GenotypeGVCFs - Executing as solivehong@solivehong on Linux v5.10.0-25-amd64 amd64; 20:09:21.521 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v11.0.13+7-b1751.21; 20:09:21.522 INFO GenotypeGVCFs - Start Date/Time: September 23, 2023 at 8:09:21 PM CST; 20:09:21.522 INFO GenotypeGVCFs - ------------------------------------------------------------; 20:09:21.522 INFO GenotypeGVCFs - ----------------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8527
https://github.com/broadinstitute/gatk/issues/8527:5730,Availability,down,down,5730,"veGenomicsDB - pid=1332903 tid=1332904 No valid combination operation found for INFO field AS_SOR - the field will NOT be part of INFO fields in the generated VCF records; 20:09:23.524 info NativeGenomicsDB - pid=1332903 tid=1332904 No valid combination operation found for INFO field FS - the field will NOT be part of INFO fields in the generated VCF records; 20:09:23.524 info NativeGenomicsDB - pid=1332903 tid=1332904 No valid combination operation found for INFO field InbreedingCoeff - the field will NOT be part of INFO fields in the generated VCF records; 20:09:23.524 info NativeGenomicsDB - pid=1332903 tid=1332904 No valid combination operation found for INFO field QD - the field will NOT be part of INFO fields in the generated VCF records; 20:09:23.524 info NativeGenomicsDB - pid=1332903 tid=1332904 No valid combination operation found for INFO field SOR - the field will NOT be part of INFO fields in the generated VCF records; 20:09:23.528 INFO GenotypeGVCFs - Shutting down engine; [September 23, 2023 at 8:09:23 PM CST] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 0.04 minutes.; Runtime.totalMemory()=2801795072; ***********************************************************************. A USER ERROR has occurred: Couldn't create GenomicsDBFeatureReader. ***********************************************************************; org.broadinstitute.hellbender.exceptions.UserException: Couldn't create GenomicsDBFeatureReader; at org.broadinstitute.hellbender.engine.FeatureDataSource.getGenomicsDBFeatureReader(FeatureDataSource.java:463); at org.broadinstitute.hellbender.engine.FeatureDataSource.getFeatureReader(FeatureDataSource.java:365); at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:319); at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:291); at org.broadinstitute.hellbender.engine.VariantLocusWalker.initialize at org.broadinstitute.hellbender.engine.Varia",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8527
https://github.com/broadinstitute/gatk/issues/8527:5989,Availability,ERROR,ERROR,5989,"INFO field FS - the field will NOT be part of INFO fields in the generated VCF records; 20:09:23.524 info NativeGenomicsDB - pid=1332903 tid=1332904 No valid combination operation found for INFO field InbreedingCoeff - the field will NOT be part of INFO fields in the generated VCF records; 20:09:23.524 info NativeGenomicsDB - pid=1332903 tid=1332904 No valid combination operation found for INFO field QD - the field will NOT be part of INFO fields in the generated VCF records; 20:09:23.524 info NativeGenomicsDB - pid=1332903 tid=1332904 No valid combination operation found for INFO field SOR - the field will NOT be part of INFO fields in the generated VCF records; 20:09:23.528 INFO GenotypeGVCFs - Shutting down engine; [September 23, 2023 at 8:09:23 PM CST] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 0.04 minutes.; Runtime.totalMemory()=2801795072; ***********************************************************************. A USER ERROR has occurred: Couldn't create GenomicsDBFeatureReader. ***********************************************************************; org.broadinstitute.hellbender.exceptions.UserException: Couldn't create GenomicsDBFeatureReader; at org.broadinstitute.hellbender.engine.FeatureDataSource.getGenomicsDBFeatureReader(FeatureDataSource.java:463); at org.broadinstitute.hellbender.engine.FeatureDataSource.getFeatureReader(FeatureDataSource.java:365); at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:319); at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:291); at org.broadinstitute.hellbender.engine.VariantLocusWalker.initialize at org.broadinstitute.hellbender.engine.VariantWalkerBase.initializeFava:726); at org.broadinstitute.hellbender.engine.VariantLocusWalker.onStartup(VariantLocusWalker.java:63); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); at org.broadinstitute.hellbender.cmdline.Com",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8527
https://github.com/broadinstitute/gatk/issues/8527:7436,Availability,Error,Error,7436,Reader; at org.broadinstitute.hellbender.engine.FeatureDataSource.getGenomicsDBFeatureReader(FeatureDataSource.java:463); at org.broadinstitute.hellbender.engine.FeatureDataSource.getFeatureReader(FeatureDataSource.java:365); at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:319); at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:291); at org.broadinstitute.hellbender.engine.VariantLocusWalker.initialize at org.broadinstitute.hellbender.engine.VariantWalkerBase.initializeFava:726); at org.broadinstitute.hellbender.engine.VariantLocusWalker.onStartup(VariantLocusWalker.java:63); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: java.io.IOException: GenomicsDB JNI Error: Broad combine GVCFs exception : No sample/CallSet name specified in JSON file/Protobuf object for TileDB row 72; at org.genomicsdb.reader.GenomicsDBQueryStream.jniGenomicsDBInit(Native Method); at org.genomicsdb.reader.GenomicsDBQueryStream.<init>(GenomicsDBQueryStream.java:209); at org.genomicsdb.reader.GenomicsDBQueryStream.<init>(GenomicsDBQueryStream.java:182); at org.genomicsdb.reader.GenomicsDBQueryStream.<init>(GenomicsDBQueryStream.java:91); at org.genomicsdb.reader.GenomicsDBFeatureReader.generateHeadersForQuery(GenomicsDBFeatureReader.java:200); at org.genomicsdb.reader.GenomicsDBFeatureReader.<init>(GenomicsDBFeatureReader.java:85); at org.broadinstitute.hellbender.engine.FeatureDataSource.getGenomicsDBFeatureReader(FeatureDataSource.java:460). ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8527
https://github.com/broadinstitute/gatk/issues/8527:1074,Performance,Load,Loading,1074,"538 samples. Chrom 11-22 don't have problems, but 1-11 don't work.; What should I do?; thanks. ```; java -DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/solivehong/miniconda3/envs/bio_base/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar GenotypeGVCFs -R /data/reference/update_gatk_v0/Homo_sapiens_assembly38.fasta -V gendb://Genomicsdb.2 -O /storage/project/collaborators/UH_Burdentest/1.running/genotypeGvcf/UH_Burdentest2222.vcf --tmp-dir /storage/GenomesDbimport/Agilent_WES/tmp -L chr2 -G StandardAnnotation --only-output-calls-starting-in-intervals --use-new-qual-calculator -D /data/reference/update_gatk_v0//Homo_sapiens_assembly38.dbsnp138.vcf; 20:09:21.335 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 20:09:21.383 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/solivehong/miniconda3/envs/bio_base/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 20:09:21.521 INFO GenotypeGVCFs - ------------------------------------------------------------; 20:09:21.521 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.3.0.0; 20:09:21.521 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 20:09:21.521 INFO GenotypeGVCFs - Executing as solivehong@solivehong on Linux v5.10.0-25-amd64 amd64; 20:09:21.521 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v11.0.13+7-b1751.21; 20:09:21.522 INFO GenotypeGVCFs - Start Date/Time: September 23, 2023 at 8:09:21 PM CST; 20:09:21.522 INFO GenotypeGVCFs - ------------------------------------------------------------; 20:09:21.522 INFO GenotypeGVCFs - ------------------------------------------------------------; 20:09:21.522 INFO GenotypeGVCFs - HTSJDK Version:",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8527
https://github.com/broadinstitute/gatk/issues/8527:941,Safety,Redund,Redundant,941,"### Instructions. I'm running on :; gatk 4.3.0.0. ; 88 cpu ; 128G mem ; 538 samples. Chrom 11-22 don't have problems, but 1-11 don't work.; What should I do?; thanks. ```; java -DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/solivehong/miniconda3/envs/bio_base/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar GenotypeGVCFs -R /data/reference/update_gatk_v0/Homo_sapiens_assembly38.fasta -V gendb://Genomicsdb.2 -O /storage/project/collaborators/UH_Burdentest/1.running/genotypeGvcf/UH_Burdentest2222.vcf --tmp-dir /storage/GenomesDbimport/Agilent_WES/tmp -L chr2 -G StandardAnnotation --only-output-calls-starting-in-intervals --use-new-qual-calculator -D /data/reference/update_gatk_v0//Homo_sapiens_assembly38.dbsnp138.vcf; 20:09:21.335 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 20:09:21.383 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/solivehong/miniconda3/envs/bio_base/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 20:09:21.521 INFO GenotypeGVCFs - ------------------------------------------------------------; 20:09:21.521 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.3.0.0; 20:09:21.521 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 20:09:21.521 INFO GenotypeGVCFs - Executing as solivehong@solivehong on Linux v5.10.0-25-amd64 amd64; 20:09:21.521 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v11.0.13+7-b1751.21; 20:09:21.522 INFO GenotypeGVCFs - Start Date/Time: September 23, 2023 at 8:09:21 PM CST; 20:09:21.522 INFO GenotypeGVCFs - ------------------------------------------------------------; 20:09:21.522 INFO GenotypeGVCFs - ----------------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8527
https://github.com/broadinstitute/gatk/pull/8530:74,Availability,error,error,74,Changed so that PopulateFilterSetInfo now explicitly prints to STDERR any error it encounters. . Example run [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/2f86bfa0-bf1b-451a-8217-a0ab4cba5834).; stderr has this error now:. > Error loading combined TSV into gvs-internal.gg_VS_1056.filter_set_info:; > BigQuery error in load operation: Provided Schema does not match Table gvs-; > internal:gg_VS_1056.filter_set_info. Cannot add fields (field: score2),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8530
https://github.com/broadinstitute/gatk/pull/8530:257,Availability,error,error,257,Changed so that PopulateFilterSetInfo now explicitly prints to STDERR any error it encounters. . Example run [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/2f86bfa0-bf1b-451a-8217-a0ab4cba5834).; stderr has this error now:. > Error loading combined TSV into gvs-internal.gg_VS_1056.filter_set_info:; > BigQuery error in load operation: Provided Schema does not match Table gvs-; > internal:gg_VS_1056.filter_set_info. Cannot add fields (field: score2),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8530
https://github.com/broadinstitute/gatk/pull/8530:271,Availability,Error,Error,271,Changed so that PopulateFilterSetInfo now explicitly prints to STDERR any error it encounters. . Example run [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/2f86bfa0-bf1b-451a-8217-a0ab4cba5834).; stderr has this error now:. > Error loading combined TSV into gvs-internal.gg_VS_1056.filter_set_info:; > BigQuery error in load operation: Provided Schema does not match Table gvs-; > internal:gg_VS_1056.filter_set_info. Cannot add fields (field: score2),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8530
https://github.com/broadinstitute/gatk/pull/8530:356,Availability,error,error,356,Changed so that PopulateFilterSetInfo now explicitly prints to STDERR any error it encounters. . Example run [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/2f86bfa0-bf1b-451a-8217-a0ab4cba5834).; stderr has this error now:. > Error loading combined TSV into gvs-internal.gg_VS_1056.filter_set_info:; > BigQuery error in load operation: Provided Schema does not match Table gvs-; > internal:gg_VS_1056.filter_set_info. Cannot add fields (field: score2),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8530
https://github.com/broadinstitute/gatk/pull/8530:277,Performance,load,loading,277,Changed so that PopulateFilterSetInfo now explicitly prints to STDERR any error it encounters. . Example run [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/2f86bfa0-bf1b-451a-8217-a0ab4cba5834).; stderr has this error now:. > Error loading combined TSV into gvs-internal.gg_VS_1056.filter_set_info:; > BigQuery error in load operation: Provided Schema does not match Table gvs-; > internal:gg_VS_1056.filter_set_info. Cannot add fields (field: score2),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8530
https://github.com/broadinstitute/gatk/pull/8530:365,Performance,load,load,365,Changed so that PopulateFilterSetInfo now explicitly prints to STDERR any error it encounters. . Example run [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/2f86bfa0-bf1b-451a-8217-a0ab4cba5834).; stderr has this error now:. > Error loading combined TSV into gvs-internal.gg_VS_1056.filter_set_info:; > BigQuery error in load operation: Provided Schema does not match Table gvs-; > internal:gg_VS_1056.filter_set_info. Cannot add fields (field: score2),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8530
https://github.com/broadinstitute/gatk/pull/8531:0,Deployability,Update,Updates,0,"Updates the code to pull the PS field out of gVCFs and stores it in the VET_* table(s).; Note schema change on the VET table; Successful BulkIngest [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Exome%20Beta%20Test%20ggrant/job_history/5d1f17a9-eeb2-4db3-a681-32d36d9e567e) (run on Exomes as they have PGT, PID, and PS in their gVCFs).; Successful Integration Test Run [here](https://job-manager.dsde-prod.broadinstitute.org/jobs/9ab365ff-743b-4d97-9c2a-6a09cf8728f4) - But note that the Exome Integration test failed for slight (and expected) difference in table sizes. I have updated the truth in `gs://gvs-internal-quickstart/integration/2023-07-25-quicker/exome_weighted/table_sizes_expected.csv`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8531
https://github.com/broadinstitute/gatk/pull/8531:356,Deployability,Integrat,Integration,356,"Updates the code to pull the PS field out of gVCFs and stores it in the VET_* table(s).; Note schema change on the VET table; Successful BulkIngest [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Exome%20Beta%20Test%20ggrant/job_history/5d1f17a9-eeb2-4db3-a681-32d36d9e567e) (run on Exomes as they have PGT, PID, and PS in their gVCFs).; Successful Integration Test Run [here](https://job-manager.dsde-prod.broadinstitute.org/jobs/9ab365ff-743b-4d97-9c2a-6a09cf8728f4) - But note that the Exome Integration test failed for slight (and expected) difference in table sizes. I have updated the truth in `gs://gvs-internal-quickstart/integration/2023-07-25-quicker/exome_weighted/table_sizes_expected.csv`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8531
https://github.com/broadinstitute/gatk/pull/8531:502,Deployability,Integrat,Integration,502,"Updates the code to pull the PS field out of gVCFs and stores it in the VET_* table(s).; Note schema change on the VET table; Successful BulkIngest [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Exome%20Beta%20Test%20ggrant/job_history/5d1f17a9-eeb2-4db3-a681-32d36d9e567e) (run on Exomes as they have PGT, PID, and PS in their gVCFs).; Successful Integration Test Run [here](https://job-manager.dsde-prod.broadinstitute.org/jobs/9ab365ff-743b-4d97-9c2a-6a09cf8728f4) - But note that the Exome Integration test failed for slight (and expected) difference in table sizes. I have updated the truth in `gs://gvs-internal-quickstart/integration/2023-07-25-quicker/exome_weighted/table_sizes_expected.csv`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8531
https://github.com/broadinstitute/gatk/pull/8531:586,Deployability,update,updated,586,"Updates the code to pull the PS field out of gVCFs and stores it in the VET_* table(s).; Note schema change on the VET table; Successful BulkIngest [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Exome%20Beta%20Test%20ggrant/job_history/5d1f17a9-eeb2-4db3-a681-32d36d9e567e) (run on Exomes as they have PGT, PID, and PS in their gVCFs).; Successful Integration Test Run [here](https://job-manager.dsde-prod.broadinstitute.org/jobs/9ab365ff-743b-4d97-9c2a-6a09cf8728f4) - But note that the Exome Integration test failed for slight (and expected) difference in table sizes. I have updated the truth in `gs://gvs-internal-quickstart/integration/2023-07-25-quicker/exome_weighted/table_sizes_expected.csv`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8531
https://github.com/broadinstitute/gatk/pull/8531:637,Deployability,integrat,integration,637,"Updates the code to pull the PS field out of gVCFs and stores it in the VET_* table(s).; Note schema change on the VET table; Successful BulkIngest [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Exome%20Beta%20Test%20ggrant/job_history/5d1f17a9-eeb2-4db3-a681-32d36d9e567e) (run on Exomes as they have PGT, PID, and PS in their gVCFs).; Successful Integration Test Run [here](https://job-manager.dsde-prod.broadinstitute.org/jobs/9ab365ff-743b-4d97-9c2a-6a09cf8728f4) - But note that the Exome Integration test failed for slight (and expected) difference in table sizes. I have updated the truth in `gs://gvs-internal-quickstart/integration/2023-07-25-quicker/exome_weighted/table_sizes_expected.csv`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8531
https://github.com/broadinstitute/gatk/pull/8531:356,Integrability,Integrat,Integration,356,"Updates the code to pull the PS field out of gVCFs and stores it in the VET_* table(s).; Note schema change on the VET table; Successful BulkIngest [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Exome%20Beta%20Test%20ggrant/job_history/5d1f17a9-eeb2-4db3-a681-32d36d9e567e) (run on Exomes as they have PGT, PID, and PS in their gVCFs).; Successful Integration Test Run [here](https://job-manager.dsde-prod.broadinstitute.org/jobs/9ab365ff-743b-4d97-9c2a-6a09cf8728f4) - But note that the Exome Integration test failed for slight (and expected) difference in table sizes. I have updated the truth in `gs://gvs-internal-quickstart/integration/2023-07-25-quicker/exome_weighted/table_sizes_expected.csv`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8531
https://github.com/broadinstitute/gatk/pull/8531:502,Integrability,Integrat,Integration,502,"Updates the code to pull the PS field out of gVCFs and stores it in the VET_* table(s).; Note schema change on the VET table; Successful BulkIngest [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Exome%20Beta%20Test%20ggrant/job_history/5d1f17a9-eeb2-4db3-a681-32d36d9e567e) (run on Exomes as they have PGT, PID, and PS in their gVCFs).; Successful Integration Test Run [here](https://job-manager.dsde-prod.broadinstitute.org/jobs/9ab365ff-743b-4d97-9c2a-6a09cf8728f4) - But note that the Exome Integration test failed for slight (and expected) difference in table sizes. I have updated the truth in `gs://gvs-internal-quickstart/integration/2023-07-25-quicker/exome_weighted/table_sizes_expected.csv`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8531
https://github.com/broadinstitute/gatk/pull/8531:637,Integrability,integrat,integration,637,"Updates the code to pull the PS field out of gVCFs and stores it in the VET_* table(s).; Note schema change on the VET table; Successful BulkIngest [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Exome%20Beta%20Test%20ggrant/job_history/5d1f17a9-eeb2-4db3-a681-32d36d9e567e) (run on Exomes as they have PGT, PID, and PS in their gVCFs).; Successful Integration Test Run [here](https://job-manager.dsde-prod.broadinstitute.org/jobs/9ab365ff-743b-4d97-9c2a-6a09cf8728f4) - But note that the Exome Integration test failed for slight (and expected) difference in table sizes. I have updated the truth in `gs://gvs-internal-quickstart/integration/2023-07-25-quicker/exome_weighted/table_sizes_expected.csv`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8531
https://github.com/broadinstitute/gatk/pull/8531:368,Testability,Test,Test,368,"Updates the code to pull the PS field out of gVCFs and stores it in the VET_* table(s).; Note schema change on the VET table; Successful BulkIngest [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Exome%20Beta%20Test%20ggrant/job_history/5d1f17a9-eeb2-4db3-a681-32d36d9e567e) (run on Exomes as they have PGT, PID, and PS in their gVCFs).; Successful Integration Test Run [here](https://job-manager.dsde-prod.broadinstitute.org/jobs/9ab365ff-743b-4d97-9c2a-6a09cf8728f4) - But note that the Exome Integration test failed for slight (and expected) difference in table sizes. I have updated the truth in `gs://gvs-internal-quickstart/integration/2023-07-25-quicker/exome_weighted/table_sizes_expected.csv`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8531
https://github.com/broadinstitute/gatk/pull/8531:514,Testability,test,test,514,"Updates the code to pull the PS field out of gVCFs and stores it in the VET_* table(s).; Note schema change on the VET table; Successful BulkIngest [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Exome%20Beta%20Test%20ggrant/job_history/5d1f17a9-eeb2-4db3-a681-32d36d9e567e) (run on Exomes as they have PGT, PID, and PS in their gVCFs).; Successful Integration Test Run [here](https://job-manager.dsde-prod.broadinstitute.org/jobs/9ab365ff-743b-4d97-9c2a-6a09cf8728f4) - But note that the Exome Integration test failed for slight (and expected) difference in table sizes. I have updated the truth in `gs://gvs-internal-quickstart/integration/2023-07-25-quicker/exome_weighted/table_sizes_expected.csv`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8531
https://github.com/broadinstitute/gatk/pull/8533:214,Deployability,Integrat,Integration,214,Simple copy/paste bug. Closing the header line creator fixes the hanging issues as seen in [this run](https://job-manager.dsde-prod.broadinstitute.org/jobs/21c1ec08-444e-4acd-8490-cc9640d9ea03) (requires PMI ops). Integration run [in progress](https://job-manager.dsde-prod.broadinstitute.org/jobs/3b5129bb-b7fe-47db-abc4-dda5d7f5006a) (regular auth).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8533
https://github.com/broadinstitute/gatk/pull/8533:214,Integrability,Integrat,Integration,214,Simple copy/paste bug. Closing the header line creator fixes the hanging issues as seen in [this run](https://job-manager.dsde-prod.broadinstitute.org/jobs/21c1ec08-444e-4acd-8490-cc9640d9ea03) (requires PMI ops). Integration run [in progress](https://job-manager.dsde-prod.broadinstitute.org/jobs/3b5129bb-b7fe-47db-abc4-dda5d7f5006a) (regular auth).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8533
https://github.com/broadinstitute/gatk/pull/8533:0,Usability,Simpl,Simple,0,Simple copy/paste bug. Closing the header line creator fixes the hanging issues as seen in [this run](https://job-manager.dsde-prod.broadinstitute.org/jobs/21c1ec08-444e-4acd-8490-cc9640d9ea03) (requires PMI ops). Integration run [in progress](https://job-manager.dsde-prod.broadinstitute.org/jobs/3b5129bb-b7fe-47db-abc4-dda5d7f5006a) (regular auth).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8533
https://github.com/broadinstitute/gatk/pull/8534:13,Availability,down,down,13,~okokok calm down it's just a draft!~. It's alive!,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8534
https://github.com/broadinstitute/gatk/pull/8534:44,Availability,alive,alive,44,~okokok calm down it's just a draft!~. It's alive!,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8534
https://github.com/broadinstitute/gatk/issues/8535:353,Testability,log,log,353,"Please find attached samtools stats file. sample1.markdup.sorted.stats is before BQSR and ApplyBQSR; sample1.stats is after BQSR and ApplyBQSR. As you can see, ; before ApplyBQSR (NA12878-P.markdup.sorted.stats.txt), the raw total sequences is 119998832; After ApplyBQSR (NA12878-P.stats.txt), the raw total sequences is 120169477. gatk_applyBQSR_error.log is the log file of the applyBQSR step. Do you have any ideas?. [gatk_applyBQSR_error.log](https://github.com/broadinstitute/gatk/files/12745229/gatk_applyBQSR_error.log); [NA12878-P.markdup.sorted.stats.txt](https://github.com/broadinstitute/gatk/files/12745230/NA12878-P.markdup.sorted.stats.txt); [NA12878-P.stats.txt](https://github.com/broadinstitute/gatk/files/12745236/NA12878-P.stats.txt)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8535
https://github.com/broadinstitute/gatk/issues/8535:364,Testability,log,log,364,"Please find attached samtools stats file. sample1.markdup.sorted.stats is before BQSR and ApplyBQSR; sample1.stats is after BQSR and ApplyBQSR. As you can see, ; before ApplyBQSR (NA12878-P.markdup.sorted.stats.txt), the raw total sequences is 119998832; After ApplyBQSR (NA12878-P.stats.txt), the raw total sequences is 120169477. gatk_applyBQSR_error.log is the log file of the applyBQSR step. Do you have any ideas?. [gatk_applyBQSR_error.log](https://github.com/broadinstitute/gatk/files/12745229/gatk_applyBQSR_error.log); [NA12878-P.markdup.sorted.stats.txt](https://github.com/broadinstitute/gatk/files/12745230/NA12878-P.markdup.sorted.stats.txt); [NA12878-P.stats.txt](https://github.com/broadinstitute/gatk/files/12745236/NA12878-P.stats.txt)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8535
https://github.com/broadinstitute/gatk/issues/8535:442,Testability,log,log,442,"Please find attached samtools stats file. sample1.markdup.sorted.stats is before BQSR and ApplyBQSR; sample1.stats is after BQSR and ApplyBQSR. As you can see, ; before ApplyBQSR (NA12878-P.markdup.sorted.stats.txt), the raw total sequences is 119998832; After ApplyBQSR (NA12878-P.stats.txt), the raw total sequences is 120169477. gatk_applyBQSR_error.log is the log file of the applyBQSR step. Do you have any ideas?. [gatk_applyBQSR_error.log](https://github.com/broadinstitute/gatk/files/12745229/gatk_applyBQSR_error.log); [NA12878-P.markdup.sorted.stats.txt](https://github.com/broadinstitute/gatk/files/12745230/NA12878-P.markdup.sorted.stats.txt); [NA12878-P.stats.txt](https://github.com/broadinstitute/gatk/files/12745236/NA12878-P.stats.txt)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8535
https://github.com/broadinstitute/gatk/issues/8535:522,Testability,log,log,522,"Please find attached samtools stats file. sample1.markdup.sorted.stats is before BQSR and ApplyBQSR; sample1.stats is after BQSR and ApplyBQSR. As you can see, ; before ApplyBQSR (NA12878-P.markdup.sorted.stats.txt), the raw total sequences is 119998832; After ApplyBQSR (NA12878-P.stats.txt), the raw total sequences is 120169477. gatk_applyBQSR_error.log is the log file of the applyBQSR step. Do you have any ideas?. [gatk_applyBQSR_error.log](https://github.com/broadinstitute/gatk/files/12745229/gatk_applyBQSR_error.log); [NA12878-P.markdup.sorted.stats.txt](https://github.com/broadinstitute/gatk/files/12745230/NA12878-P.markdup.sorted.stats.txt); [NA12878-P.stats.txt](https://github.com/broadinstitute/gatk/files/12745236/NA12878-P.stats.txt)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8535
https://github.com/broadinstitute/gatk/pull/8536:0,Deployability,Integrat,Integration,0,"Integration test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/4de33a53-be6f-43b2-96db-7e8a0fb398f8); Example run of GvsExtractAvroFilesForHail using an Exome data set that has PGT, PID, and PS defined is [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Exome%20Beta%20Test%20ggrant/job_history/43a87886-60be-4f60-a2d3-3f4a97ceea5b).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8536
https://github.com/broadinstitute/gatk/pull/8536:0,Integrability,Integrat,Integration,0,"Integration test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/4de33a53-be6f-43b2-96db-7e8a0fb398f8); Example run of GvsExtractAvroFilesForHail using an Exome data set that has PGT, PID, and PS defined is [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Exome%20Beta%20Test%20ggrant/job_history/43a87886-60be-4f60-a2d3-3f4a97ceea5b).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8536
https://github.com/broadinstitute/gatk/pull/8536:12,Testability,test,test,12,"Integration test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/4de33a53-be6f-43b2-96db-7e8a0fb398f8); Example run of GvsExtractAvroFilesForHail using an Exome data set that has PGT, PID, and PS defined is [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Exome%20Beta%20Test%20ggrant/job_history/43a87886-60be-4f60-a2d3-3f4a97ceea5b).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8536
https://github.com/broadinstitute/gatk/pull/8537:458,Availability,avail,available,458,"Successful integration run [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/ef747737-4d19-4770-83b7-47715eff8237). tl;dr the only commit really worth looking at is 9ac0befbcc39b9c5a7eb0938dd79a7d5cbd5f297, everything else is a simple merge from master. This is just minor tweaks around recent changes in the JointVariantCalling WDL. I'll need to merge and push this locally to preserve history from master as that option is not available within the GATK GitHub repo.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8537
https://github.com/broadinstitute/gatk/pull/8537:11,Deployability,integrat,integration,11,"Successful integration run [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/ef747737-4d19-4770-83b7-47715eff8237). tl;dr the only commit really worth looking at is 9ac0befbcc39b9c5a7eb0938dd79a7d5cbd5f297, everything else is a simple merge from master. This is just minor tweaks around recent changes in the JointVariantCalling WDL. I'll need to merge and push this locally to preserve history from master as that option is not available within the GATK GitHub repo.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8537
https://github.com/broadinstitute/gatk/pull/8537:11,Integrability,integrat,integration,11,"Successful integration run [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/ef747737-4d19-4770-83b7-47715eff8237). tl;dr the only commit really worth looking at is 9ac0befbcc39b9c5a7eb0938dd79a7d5cbd5f297, everything else is a simple merge from master. This is just minor tweaks around recent changes in the JointVariantCalling WDL. I'll need to merge and push this locally to preserve history from master as that option is not available within the GATK GitHub repo.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8537
https://github.com/broadinstitute/gatk/pull/8537:257,Usability,simpl,simple,257,"Successful integration run [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/ef747737-4d19-4770-83b7-47715eff8237). tl;dr the only commit really worth looking at is 9ac0befbcc39b9c5a7eb0938dd79a7d5cbd5f297, everything else is a simple merge from master. This is just minor tweaks around recent changes in the JointVariantCalling WDL. I'll need to merge and push this locally to preserve history from master as that option is not available within the GATK GitHub repo.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8537
https://github.com/broadinstitute/gatk/pull/8538:8,Deployability,Integrat,Integration,8,"Passing Integration Test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/ad05b4d1-7aed-4482-8b5c-ced7b87d2d37).; Verified that GQ0 dropped in 'hail_lite' run and not in 'hail_vcf' run; (queries of count by state from ref ranges table):. **Hail Lite (Hail path, drop state 0):; state count**; 2 2495387; 3 4773472  ; 4 5959290. **Lite VCF (VCF path, drop_state 40):; state count**; 0 2764630; 2 2495387; 3 4773472. Spun up a notebook and ran the vds_validation.py script on the VDS generated by 'hail_lite'. And it passed:. > 2023-10-04 19:08:01.278 Hail: WARN: cols(): Resulting column table is sorted by 'col_key'.; > To preserve matrix table column order, first unkey columns with 'key_cols_by()'; > checking that:; > * no reference blocks have GQ=0; > * all ref blocks have END after start; > * all ref blocks are max 1000 bases long; > running densify on 200kb region (0 + 1) / 1]; > took 10.9s to densify 0 rows after interval query; > Hail VDS validation successful======================================(1 + 0) / 1]",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8538
https://github.com/broadinstitute/gatk/pull/8538:8,Integrability,Integrat,Integration,8,"Passing Integration Test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/ad05b4d1-7aed-4482-8b5c-ced7b87d2d37).; Verified that GQ0 dropped in 'hail_lite' run and not in 'hail_vcf' run; (queries of count by state from ref ranges table):. **Hail Lite (Hail path, drop state 0):; state count**; 2 2495387; 3 4773472  ; 4 5959290. **Lite VCF (VCF path, drop_state 40):; state count**; 0 2764630; 2 2495387; 3 4773472. Spun up a notebook and ran the vds_validation.py script on the VDS generated by 'hail_lite'. And it passed:. > 2023-10-04 19:08:01.278 Hail: WARN: cols(): Resulting column table is sorted by 'col_key'.; > To preserve matrix table column order, first unkey columns with 'key_cols_by()'; > checking that:; > * no reference blocks have GQ=0; > * all ref blocks have END after start; > * all ref blocks are max 1000 bases long; > running densify on 200kb region (0 + 1) / 1]; > took 10.9s to densify 0 rows after interval query; > Hail VDS validation successful======================================(1 + 0) / 1]",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8538
https://github.com/broadinstitute/gatk/pull/8538:981,Security,validat,validation,981,"Passing Integration Test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/ad05b4d1-7aed-4482-8b5c-ced7b87d2d37).; Verified that GQ0 dropped in 'hail_lite' run and not in 'hail_vcf' run; (queries of count by state from ref ranges table):. **Hail Lite (Hail path, drop state 0):; state count**; 2 2495387; 3 4773472  ; 4 5959290. **Lite VCF (VCF path, drop_state 40):; state count**; 0 2764630; 2 2495387; 3 4773472. Spun up a notebook and ran the vds_validation.py script on the VDS generated by 'hail_lite'. And it passed:. > 2023-10-04 19:08:01.278 Hail: WARN: cols(): Resulting column table is sorted by 'col_key'.; > To preserve matrix table column order, first unkey columns with 'key_cols_by()'; > checking that:; > * no reference blocks have GQ=0; > * all ref blocks have END after start; > * all ref blocks are max 1000 bases long; > running densify on 200kb region (0 + 1) / 1]; > took 10.9s to densify 0 rows after interval query; > Hail VDS validation successful======================================(1 + 0) / 1]",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8538
https://github.com/broadinstitute/gatk/pull/8538:20,Testability,Test,Test,20,"Passing Integration Test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/ad05b4d1-7aed-4482-8b5c-ced7b87d2d37).; Verified that GQ0 dropped in 'hail_lite' run and not in 'hail_vcf' run; (queries of count by state from ref ranges table):. **Hail Lite (Hail path, drop state 0):; state count**; 2 2495387; 3 4773472  ; 4 5959290. **Lite VCF (VCF path, drop_state 40):; state count**; 0 2764630; 2 2495387; 3 4773472. Spun up a notebook and ran the vds_validation.py script on the VDS generated by 'hail_lite'. And it passed:. > 2023-10-04 19:08:01.278 Hail: WARN: cols(): Resulting column table is sorted by 'col_key'.; > To preserve matrix table column order, first unkey columns with 'key_cols_by()'; > checking that:; > * no reference blocks have GQ=0; > * all ref blocks have END after start; > * all ref blocks are max 1000 bases long; > running densify on 200kb region (0 + 1) / 1]; > took 10.9s to densify 0 rows after interval query; > Hail VDS validation successful======================================(1 + 0) / 1]",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8538
https://github.com/broadinstitute/gatk/pull/8540:170,Deployability,Integrat,Integration,170,* `JointVariantCalling` [does set](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/f3f98f0e-2a7f-460b-886f-3442551140a8) `tighter_gcp_quotas`.; * Integration tests [do not set](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/51213d40-7583-49f1-a101-1842180a6470) `tighter_gcp_quotas`.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8540
https://github.com/broadinstitute/gatk/pull/8540:170,Integrability,Integrat,Integration,170,* `JointVariantCalling` [does set](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/f3f98f0e-2a7f-460b-886f-3442551140a8) `tighter_gcp_quotas`.; * Integration tests [do not set](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/51213d40-7583-49f1-a101-1842180a6470) `tighter_gcp_quotas`.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8540
https://github.com/broadinstitute/gatk/pull/8540:182,Testability,test,tests,182,* `JointVariantCalling` [does set](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/f3f98f0e-2a7f-460b-886f-3442551140a8) `tighter_gcp_quotas`.; * Integration tests [do not set](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/51213d40-7583-49f1-a101-1842180a6470) `tighter_gcp_quotas`.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8540
https://github.com/broadinstitute/gatk/pull/8543:1590,Availability,failure,failure,1590,"Adding in the option for using a compressed representation of our reference data. The flag use_compressed_references needs to be passed into ingest and also export, and doing so causes us to use a different internal schema for the ref_ranges table that can save ~40% on space. On the VCF path, the packed data is expanded while we populate the prepare tables before extract. On the VDS path, the packed data is expanded while we extract from the tables to create the Avro files. I updated integration tests to take use_compressed_refs as an option, and saw what I needed. Integrations tests for the VCF and VDS paths go to completion and fail, but in the expected way. AssertIdenticalOutputs, the important part, passes. AssertTableSizesAreExpected fails because the ref_ranges table is so much smaller (255884464 vs 431805033 expected) and AssertCostIsTrackedAndExpected fails because the PrepareRangesCallsetTask.GvsPrepareRanges.BigQuery Query Scanned is similarly smaller (340787200 vs 515899392). Given the nature of the change and the fact that it is an optional flag that defaults to zero, updating the integration tests to cover this path did not seem sound at this time. But using them to verify that the _content_ of the final vcfs wasn't affected by the change makes more sense. VCF integration run: https://job-manager.dsde-prod.broadinstitute.org/jobs/e9896786-9d3f-48b9-ab6f-9a76d9fafafe; Hail integration run: https://job-manager.dsde-prod.broadinstitute.org/jobs/dbdd0934-50c3-4477-a191-3282341eacd3. another post-merge integration run with same results: identical outputs, failure due to lower table sizes and query scan costs; https://job-manager.dsde-prod.broadinstitute.org/jobs/edabdac3-1856-4e2a-aadc-3bcd4c353956",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8543
https://github.com/broadinstitute/gatk/pull/8543:481,Deployability,update,updated,481,"Adding in the option for using a compressed representation of our reference data. The flag use_compressed_references needs to be passed into ingest and also export, and doing so causes us to use a different internal schema for the ref_ranges table that can save ~40% on space. On the VCF path, the packed data is expanded while we populate the prepare tables before extract. On the VDS path, the packed data is expanded while we extract from the tables to create the Avro files. I updated integration tests to take use_compressed_refs as an option, and saw what I needed. Integrations tests for the VCF and VDS paths go to completion and fail, but in the expected way. AssertIdenticalOutputs, the important part, passes. AssertTableSizesAreExpected fails because the ref_ranges table is so much smaller (255884464 vs 431805033 expected) and AssertCostIsTrackedAndExpected fails because the PrepareRangesCallsetTask.GvsPrepareRanges.BigQuery Query Scanned is similarly smaller (340787200 vs 515899392). Given the nature of the change and the fact that it is an optional flag that defaults to zero, updating the integration tests to cover this path did not seem sound at this time. But using them to verify that the _content_ of the final vcfs wasn't affected by the change makes more sense. VCF integration run: https://job-manager.dsde-prod.broadinstitute.org/jobs/e9896786-9d3f-48b9-ab6f-9a76d9fafafe; Hail integration run: https://job-manager.dsde-prod.broadinstitute.org/jobs/dbdd0934-50c3-4477-a191-3282341eacd3. another post-merge integration run with same results: identical outputs, failure due to lower table sizes and query scan costs; https://job-manager.dsde-prod.broadinstitute.org/jobs/edabdac3-1856-4e2a-aadc-3bcd4c353956",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8543
https://github.com/broadinstitute/gatk/pull/8543:489,Deployability,integrat,integration,489,"Adding in the option for using a compressed representation of our reference data. The flag use_compressed_references needs to be passed into ingest and also export, and doing so causes us to use a different internal schema for the ref_ranges table that can save ~40% on space. On the VCF path, the packed data is expanded while we populate the prepare tables before extract. On the VDS path, the packed data is expanded while we extract from the tables to create the Avro files. I updated integration tests to take use_compressed_refs as an option, and saw what I needed. Integrations tests for the VCF and VDS paths go to completion and fail, but in the expected way. AssertIdenticalOutputs, the important part, passes. AssertTableSizesAreExpected fails because the ref_ranges table is so much smaller (255884464 vs 431805033 expected) and AssertCostIsTrackedAndExpected fails because the PrepareRangesCallsetTask.GvsPrepareRanges.BigQuery Query Scanned is similarly smaller (340787200 vs 515899392). Given the nature of the change and the fact that it is an optional flag that defaults to zero, updating the integration tests to cover this path did not seem sound at this time. But using them to verify that the _content_ of the final vcfs wasn't affected by the change makes more sense. VCF integration run: https://job-manager.dsde-prod.broadinstitute.org/jobs/e9896786-9d3f-48b9-ab6f-9a76d9fafafe; Hail integration run: https://job-manager.dsde-prod.broadinstitute.org/jobs/dbdd0934-50c3-4477-a191-3282341eacd3. another post-merge integration run with same results: identical outputs, failure due to lower table sizes and query scan costs; https://job-manager.dsde-prod.broadinstitute.org/jobs/edabdac3-1856-4e2a-aadc-3bcd4c353956",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8543
https://github.com/broadinstitute/gatk/pull/8543:572,Deployability,Integrat,Integrations,572,"Adding in the option for using a compressed representation of our reference data. The flag use_compressed_references needs to be passed into ingest and also export, and doing so causes us to use a different internal schema for the ref_ranges table that can save ~40% on space. On the VCF path, the packed data is expanded while we populate the prepare tables before extract. On the VDS path, the packed data is expanded while we extract from the tables to create the Avro files. I updated integration tests to take use_compressed_refs as an option, and saw what I needed. Integrations tests for the VCF and VDS paths go to completion and fail, but in the expected way. AssertIdenticalOutputs, the important part, passes. AssertTableSizesAreExpected fails because the ref_ranges table is so much smaller (255884464 vs 431805033 expected) and AssertCostIsTrackedAndExpected fails because the PrepareRangesCallsetTask.GvsPrepareRanges.BigQuery Query Scanned is similarly smaller (340787200 vs 515899392). Given the nature of the change and the fact that it is an optional flag that defaults to zero, updating the integration tests to cover this path did not seem sound at this time. But using them to verify that the _content_ of the final vcfs wasn't affected by the change makes more sense. VCF integration run: https://job-manager.dsde-prod.broadinstitute.org/jobs/e9896786-9d3f-48b9-ab6f-9a76d9fafafe; Hail integration run: https://job-manager.dsde-prod.broadinstitute.org/jobs/dbdd0934-50c3-4477-a191-3282341eacd3. another post-merge integration run with same results: identical outputs, failure due to lower table sizes and query scan costs; https://job-manager.dsde-prod.broadinstitute.org/jobs/edabdac3-1856-4e2a-aadc-3bcd4c353956",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8543
https://github.com/broadinstitute/gatk/pull/8543:1110,Deployability,integrat,integration,1110,"Adding in the option for using a compressed representation of our reference data. The flag use_compressed_references needs to be passed into ingest and also export, and doing so causes us to use a different internal schema for the ref_ranges table that can save ~40% on space. On the VCF path, the packed data is expanded while we populate the prepare tables before extract. On the VDS path, the packed data is expanded while we extract from the tables to create the Avro files. I updated integration tests to take use_compressed_refs as an option, and saw what I needed. Integrations tests for the VCF and VDS paths go to completion and fail, but in the expected way. AssertIdenticalOutputs, the important part, passes. AssertTableSizesAreExpected fails because the ref_ranges table is so much smaller (255884464 vs 431805033 expected) and AssertCostIsTrackedAndExpected fails because the PrepareRangesCallsetTask.GvsPrepareRanges.BigQuery Query Scanned is similarly smaller (340787200 vs 515899392). Given the nature of the change and the fact that it is an optional flag that defaults to zero, updating the integration tests to cover this path did not seem sound at this time. But using them to verify that the _content_ of the final vcfs wasn't affected by the change makes more sense. VCF integration run: https://job-manager.dsde-prod.broadinstitute.org/jobs/e9896786-9d3f-48b9-ab6f-9a76d9fafafe; Hail integration run: https://job-manager.dsde-prod.broadinstitute.org/jobs/dbdd0934-50c3-4477-a191-3282341eacd3. another post-merge integration run with same results: identical outputs, failure due to lower table sizes and query scan costs; https://job-manager.dsde-prod.broadinstitute.org/jobs/edabdac3-1856-4e2a-aadc-3bcd4c353956",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8543
https://github.com/broadinstitute/gatk/pull/8543:1294,Deployability,integrat,integration,1294,"Adding in the option for using a compressed representation of our reference data. The flag use_compressed_references needs to be passed into ingest and also export, and doing so causes us to use a different internal schema for the ref_ranges table that can save ~40% on space. On the VCF path, the packed data is expanded while we populate the prepare tables before extract. On the VDS path, the packed data is expanded while we extract from the tables to create the Avro files. I updated integration tests to take use_compressed_refs as an option, and saw what I needed. Integrations tests for the VCF and VDS paths go to completion and fail, but in the expected way. AssertIdenticalOutputs, the important part, passes. AssertTableSizesAreExpected fails because the ref_ranges table is so much smaller (255884464 vs 431805033 expected) and AssertCostIsTrackedAndExpected fails because the PrepareRangesCallsetTask.GvsPrepareRanges.BigQuery Query Scanned is similarly smaller (340787200 vs 515899392). Given the nature of the change and the fact that it is an optional flag that defaults to zero, updating the integration tests to cover this path did not seem sound at this time. But using them to verify that the _content_ of the final vcfs wasn't affected by the change makes more sense. VCF integration run: https://job-manager.dsde-prod.broadinstitute.org/jobs/e9896786-9d3f-48b9-ab6f-9a76d9fafafe; Hail integration run: https://job-manager.dsde-prod.broadinstitute.org/jobs/dbdd0934-50c3-4477-a191-3282341eacd3. another post-merge integration run with same results: identical outputs, failure due to lower table sizes and query scan costs; https://job-manager.dsde-prod.broadinstitute.org/jobs/edabdac3-1856-4e2a-aadc-3bcd4c353956",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8543
https://github.com/broadinstitute/gatk/pull/8543:1408,Deployability,integrat,integration,1408,"Adding in the option for using a compressed representation of our reference data. The flag use_compressed_references needs to be passed into ingest and also export, and doing so causes us to use a different internal schema for the ref_ranges table that can save ~40% on space. On the VCF path, the packed data is expanded while we populate the prepare tables before extract. On the VDS path, the packed data is expanded while we extract from the tables to create the Avro files. I updated integration tests to take use_compressed_refs as an option, and saw what I needed. Integrations tests for the VCF and VDS paths go to completion and fail, but in the expected way. AssertIdenticalOutputs, the important part, passes. AssertTableSizesAreExpected fails because the ref_ranges table is so much smaller (255884464 vs 431805033 expected) and AssertCostIsTrackedAndExpected fails because the PrepareRangesCallsetTask.GvsPrepareRanges.BigQuery Query Scanned is similarly smaller (340787200 vs 515899392). Given the nature of the change and the fact that it is an optional flag that defaults to zero, updating the integration tests to cover this path did not seem sound at this time. But using them to verify that the _content_ of the final vcfs wasn't affected by the change makes more sense. VCF integration run: https://job-manager.dsde-prod.broadinstitute.org/jobs/e9896786-9d3f-48b9-ab6f-9a76d9fafafe; Hail integration run: https://job-manager.dsde-prod.broadinstitute.org/jobs/dbdd0934-50c3-4477-a191-3282341eacd3. another post-merge integration run with same results: identical outputs, failure due to lower table sizes and query scan costs; https://job-manager.dsde-prod.broadinstitute.org/jobs/edabdac3-1856-4e2a-aadc-3bcd4c353956",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8543
https://github.com/broadinstitute/gatk/pull/8543:1536,Deployability,integrat,integration,1536,"Adding in the option for using a compressed representation of our reference data. The flag use_compressed_references needs to be passed into ingest and also export, and doing so causes us to use a different internal schema for the ref_ranges table that can save ~40% on space. On the VCF path, the packed data is expanded while we populate the prepare tables before extract. On the VDS path, the packed data is expanded while we extract from the tables to create the Avro files. I updated integration tests to take use_compressed_refs as an option, and saw what I needed. Integrations tests for the VCF and VDS paths go to completion and fail, but in the expected way. AssertIdenticalOutputs, the important part, passes. AssertTableSizesAreExpected fails because the ref_ranges table is so much smaller (255884464 vs 431805033 expected) and AssertCostIsTrackedAndExpected fails because the PrepareRangesCallsetTask.GvsPrepareRanges.BigQuery Query Scanned is similarly smaller (340787200 vs 515899392). Given the nature of the change and the fact that it is an optional flag that defaults to zero, updating the integration tests to cover this path did not seem sound at this time. But using them to verify that the _content_ of the final vcfs wasn't affected by the change makes more sense. VCF integration run: https://job-manager.dsde-prod.broadinstitute.org/jobs/e9896786-9d3f-48b9-ab6f-9a76d9fafafe; Hail integration run: https://job-manager.dsde-prod.broadinstitute.org/jobs/dbdd0934-50c3-4477-a191-3282341eacd3. another post-merge integration run with same results: identical outputs, failure due to lower table sizes and query scan costs; https://job-manager.dsde-prod.broadinstitute.org/jobs/edabdac3-1856-4e2a-aadc-3bcd4c353956",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8543
https://github.com/broadinstitute/gatk/pull/8543:489,Integrability,integrat,integration,489,"Adding in the option for using a compressed representation of our reference data. The flag use_compressed_references needs to be passed into ingest and also export, and doing so causes us to use a different internal schema for the ref_ranges table that can save ~40% on space. On the VCF path, the packed data is expanded while we populate the prepare tables before extract. On the VDS path, the packed data is expanded while we extract from the tables to create the Avro files. I updated integration tests to take use_compressed_refs as an option, and saw what I needed. Integrations tests for the VCF and VDS paths go to completion and fail, but in the expected way. AssertIdenticalOutputs, the important part, passes. AssertTableSizesAreExpected fails because the ref_ranges table is so much smaller (255884464 vs 431805033 expected) and AssertCostIsTrackedAndExpected fails because the PrepareRangesCallsetTask.GvsPrepareRanges.BigQuery Query Scanned is similarly smaller (340787200 vs 515899392). Given the nature of the change and the fact that it is an optional flag that defaults to zero, updating the integration tests to cover this path did not seem sound at this time. But using them to verify that the _content_ of the final vcfs wasn't affected by the change makes more sense. VCF integration run: https://job-manager.dsde-prod.broadinstitute.org/jobs/e9896786-9d3f-48b9-ab6f-9a76d9fafafe; Hail integration run: https://job-manager.dsde-prod.broadinstitute.org/jobs/dbdd0934-50c3-4477-a191-3282341eacd3. another post-merge integration run with same results: identical outputs, failure due to lower table sizes and query scan costs; https://job-manager.dsde-prod.broadinstitute.org/jobs/edabdac3-1856-4e2a-aadc-3bcd4c353956",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8543
https://github.com/broadinstitute/gatk/pull/8543:572,Integrability,Integrat,Integrations,572,"Adding in the option for using a compressed representation of our reference data. The flag use_compressed_references needs to be passed into ingest and also export, and doing so causes us to use a different internal schema for the ref_ranges table that can save ~40% on space. On the VCF path, the packed data is expanded while we populate the prepare tables before extract. On the VDS path, the packed data is expanded while we extract from the tables to create the Avro files. I updated integration tests to take use_compressed_refs as an option, and saw what I needed. Integrations tests for the VCF and VDS paths go to completion and fail, but in the expected way. AssertIdenticalOutputs, the important part, passes. AssertTableSizesAreExpected fails because the ref_ranges table is so much smaller (255884464 vs 431805033 expected) and AssertCostIsTrackedAndExpected fails because the PrepareRangesCallsetTask.GvsPrepareRanges.BigQuery Query Scanned is similarly smaller (340787200 vs 515899392). Given the nature of the change and the fact that it is an optional flag that defaults to zero, updating the integration tests to cover this path did not seem sound at this time. But using them to verify that the _content_ of the final vcfs wasn't affected by the change makes more sense. VCF integration run: https://job-manager.dsde-prod.broadinstitute.org/jobs/e9896786-9d3f-48b9-ab6f-9a76d9fafafe; Hail integration run: https://job-manager.dsde-prod.broadinstitute.org/jobs/dbdd0934-50c3-4477-a191-3282341eacd3. another post-merge integration run with same results: identical outputs, failure due to lower table sizes and query scan costs; https://job-manager.dsde-prod.broadinstitute.org/jobs/edabdac3-1856-4e2a-aadc-3bcd4c353956",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8543
https://github.com/broadinstitute/gatk/pull/8543:1110,Integrability,integrat,integration,1110,"Adding in the option for using a compressed representation of our reference data. The flag use_compressed_references needs to be passed into ingest and also export, and doing so causes us to use a different internal schema for the ref_ranges table that can save ~40% on space. On the VCF path, the packed data is expanded while we populate the prepare tables before extract. On the VDS path, the packed data is expanded while we extract from the tables to create the Avro files. I updated integration tests to take use_compressed_refs as an option, and saw what I needed. Integrations tests for the VCF and VDS paths go to completion and fail, but in the expected way. AssertIdenticalOutputs, the important part, passes. AssertTableSizesAreExpected fails because the ref_ranges table is so much smaller (255884464 vs 431805033 expected) and AssertCostIsTrackedAndExpected fails because the PrepareRangesCallsetTask.GvsPrepareRanges.BigQuery Query Scanned is similarly smaller (340787200 vs 515899392). Given the nature of the change and the fact that it is an optional flag that defaults to zero, updating the integration tests to cover this path did not seem sound at this time. But using them to verify that the _content_ of the final vcfs wasn't affected by the change makes more sense. VCF integration run: https://job-manager.dsde-prod.broadinstitute.org/jobs/e9896786-9d3f-48b9-ab6f-9a76d9fafafe; Hail integration run: https://job-manager.dsde-prod.broadinstitute.org/jobs/dbdd0934-50c3-4477-a191-3282341eacd3. another post-merge integration run with same results: identical outputs, failure due to lower table sizes and query scan costs; https://job-manager.dsde-prod.broadinstitute.org/jobs/edabdac3-1856-4e2a-aadc-3bcd4c353956",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8543
https://github.com/broadinstitute/gatk/pull/8543:1294,Integrability,integrat,integration,1294,"Adding in the option for using a compressed representation of our reference data. The flag use_compressed_references needs to be passed into ingest and also export, and doing so causes us to use a different internal schema for the ref_ranges table that can save ~40% on space. On the VCF path, the packed data is expanded while we populate the prepare tables before extract. On the VDS path, the packed data is expanded while we extract from the tables to create the Avro files. I updated integration tests to take use_compressed_refs as an option, and saw what I needed. Integrations tests for the VCF and VDS paths go to completion and fail, but in the expected way. AssertIdenticalOutputs, the important part, passes. AssertTableSizesAreExpected fails because the ref_ranges table is so much smaller (255884464 vs 431805033 expected) and AssertCostIsTrackedAndExpected fails because the PrepareRangesCallsetTask.GvsPrepareRanges.BigQuery Query Scanned is similarly smaller (340787200 vs 515899392). Given the nature of the change and the fact that it is an optional flag that defaults to zero, updating the integration tests to cover this path did not seem sound at this time. But using them to verify that the _content_ of the final vcfs wasn't affected by the change makes more sense. VCF integration run: https://job-manager.dsde-prod.broadinstitute.org/jobs/e9896786-9d3f-48b9-ab6f-9a76d9fafafe; Hail integration run: https://job-manager.dsde-prod.broadinstitute.org/jobs/dbdd0934-50c3-4477-a191-3282341eacd3. another post-merge integration run with same results: identical outputs, failure due to lower table sizes and query scan costs; https://job-manager.dsde-prod.broadinstitute.org/jobs/edabdac3-1856-4e2a-aadc-3bcd4c353956",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8543
https://github.com/broadinstitute/gatk/pull/8543:1408,Integrability,integrat,integration,1408,"Adding in the option for using a compressed representation of our reference data. The flag use_compressed_references needs to be passed into ingest and also export, and doing so causes us to use a different internal schema for the ref_ranges table that can save ~40% on space. On the VCF path, the packed data is expanded while we populate the prepare tables before extract. On the VDS path, the packed data is expanded while we extract from the tables to create the Avro files. I updated integration tests to take use_compressed_refs as an option, and saw what I needed. Integrations tests for the VCF and VDS paths go to completion and fail, but in the expected way. AssertIdenticalOutputs, the important part, passes. AssertTableSizesAreExpected fails because the ref_ranges table is so much smaller (255884464 vs 431805033 expected) and AssertCostIsTrackedAndExpected fails because the PrepareRangesCallsetTask.GvsPrepareRanges.BigQuery Query Scanned is similarly smaller (340787200 vs 515899392). Given the nature of the change and the fact that it is an optional flag that defaults to zero, updating the integration tests to cover this path did not seem sound at this time. But using them to verify that the _content_ of the final vcfs wasn't affected by the change makes more sense. VCF integration run: https://job-manager.dsde-prod.broadinstitute.org/jobs/e9896786-9d3f-48b9-ab6f-9a76d9fafafe; Hail integration run: https://job-manager.dsde-prod.broadinstitute.org/jobs/dbdd0934-50c3-4477-a191-3282341eacd3. another post-merge integration run with same results: identical outputs, failure due to lower table sizes and query scan costs; https://job-manager.dsde-prod.broadinstitute.org/jobs/edabdac3-1856-4e2a-aadc-3bcd4c353956",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8543
https://github.com/broadinstitute/gatk/pull/8543:1536,Integrability,integrat,integration,1536,"Adding in the option for using a compressed representation of our reference data. The flag use_compressed_references needs to be passed into ingest and also export, and doing so causes us to use a different internal schema for the ref_ranges table that can save ~40% on space. On the VCF path, the packed data is expanded while we populate the prepare tables before extract. On the VDS path, the packed data is expanded while we extract from the tables to create the Avro files. I updated integration tests to take use_compressed_refs as an option, and saw what I needed. Integrations tests for the VCF and VDS paths go to completion and fail, but in the expected way. AssertIdenticalOutputs, the important part, passes. AssertTableSizesAreExpected fails because the ref_ranges table is so much smaller (255884464 vs 431805033 expected) and AssertCostIsTrackedAndExpected fails because the PrepareRangesCallsetTask.GvsPrepareRanges.BigQuery Query Scanned is similarly smaller (340787200 vs 515899392). Given the nature of the change and the fact that it is an optional flag that defaults to zero, updating the integration tests to cover this path did not seem sound at this time. But using them to verify that the _content_ of the final vcfs wasn't affected by the change makes more sense. VCF integration run: https://job-manager.dsde-prod.broadinstitute.org/jobs/e9896786-9d3f-48b9-ab6f-9a76d9fafafe; Hail integration run: https://job-manager.dsde-prod.broadinstitute.org/jobs/dbdd0934-50c3-4477-a191-3282341eacd3. another post-merge integration run with same results: identical outputs, failure due to lower table sizes and query scan costs; https://job-manager.dsde-prod.broadinstitute.org/jobs/edabdac3-1856-4e2a-aadc-3bcd4c353956",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8543
https://github.com/broadinstitute/gatk/pull/8543:501,Testability,test,tests,501,"Adding in the option for using a compressed representation of our reference data. The flag use_compressed_references needs to be passed into ingest and also export, and doing so causes us to use a different internal schema for the ref_ranges table that can save ~40% on space. On the VCF path, the packed data is expanded while we populate the prepare tables before extract. On the VDS path, the packed data is expanded while we extract from the tables to create the Avro files. I updated integration tests to take use_compressed_refs as an option, and saw what I needed. Integrations tests for the VCF and VDS paths go to completion and fail, but in the expected way. AssertIdenticalOutputs, the important part, passes. AssertTableSizesAreExpected fails because the ref_ranges table is so much smaller (255884464 vs 431805033 expected) and AssertCostIsTrackedAndExpected fails because the PrepareRangesCallsetTask.GvsPrepareRanges.BigQuery Query Scanned is similarly smaller (340787200 vs 515899392). Given the nature of the change and the fact that it is an optional flag that defaults to zero, updating the integration tests to cover this path did not seem sound at this time. But using them to verify that the _content_ of the final vcfs wasn't affected by the change makes more sense. VCF integration run: https://job-manager.dsde-prod.broadinstitute.org/jobs/e9896786-9d3f-48b9-ab6f-9a76d9fafafe; Hail integration run: https://job-manager.dsde-prod.broadinstitute.org/jobs/dbdd0934-50c3-4477-a191-3282341eacd3. another post-merge integration run with same results: identical outputs, failure due to lower table sizes and query scan costs; https://job-manager.dsde-prod.broadinstitute.org/jobs/edabdac3-1856-4e2a-aadc-3bcd4c353956",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8543
https://github.com/broadinstitute/gatk/pull/8543:585,Testability,test,tests,585,"Adding in the option for using a compressed representation of our reference data. The flag use_compressed_references needs to be passed into ingest and also export, and doing so causes us to use a different internal schema for the ref_ranges table that can save ~40% on space. On the VCF path, the packed data is expanded while we populate the prepare tables before extract. On the VDS path, the packed data is expanded while we extract from the tables to create the Avro files. I updated integration tests to take use_compressed_refs as an option, and saw what I needed. Integrations tests for the VCF and VDS paths go to completion and fail, but in the expected way. AssertIdenticalOutputs, the important part, passes. AssertTableSizesAreExpected fails because the ref_ranges table is so much smaller (255884464 vs 431805033 expected) and AssertCostIsTrackedAndExpected fails because the PrepareRangesCallsetTask.GvsPrepareRanges.BigQuery Query Scanned is similarly smaller (340787200 vs 515899392). Given the nature of the change and the fact that it is an optional flag that defaults to zero, updating the integration tests to cover this path did not seem sound at this time. But using them to verify that the _content_ of the final vcfs wasn't affected by the change makes more sense. VCF integration run: https://job-manager.dsde-prod.broadinstitute.org/jobs/e9896786-9d3f-48b9-ab6f-9a76d9fafafe; Hail integration run: https://job-manager.dsde-prod.broadinstitute.org/jobs/dbdd0934-50c3-4477-a191-3282341eacd3. another post-merge integration run with same results: identical outputs, failure due to lower table sizes and query scan costs; https://job-manager.dsde-prod.broadinstitute.org/jobs/edabdac3-1856-4e2a-aadc-3bcd4c353956",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8543
https://github.com/broadinstitute/gatk/pull/8543:669,Testability,Assert,AssertIdenticalOutputs,669,"Adding in the option for using a compressed representation of our reference data. The flag use_compressed_references needs to be passed into ingest and also export, and doing so causes us to use a different internal schema for the ref_ranges table that can save ~40% on space. On the VCF path, the packed data is expanded while we populate the prepare tables before extract. On the VDS path, the packed data is expanded while we extract from the tables to create the Avro files. I updated integration tests to take use_compressed_refs as an option, and saw what I needed. Integrations tests for the VCF and VDS paths go to completion and fail, but in the expected way. AssertIdenticalOutputs, the important part, passes. AssertTableSizesAreExpected fails because the ref_ranges table is so much smaller (255884464 vs 431805033 expected) and AssertCostIsTrackedAndExpected fails because the PrepareRangesCallsetTask.GvsPrepareRanges.BigQuery Query Scanned is similarly smaller (340787200 vs 515899392). Given the nature of the change and the fact that it is an optional flag that defaults to zero, updating the integration tests to cover this path did not seem sound at this time. But using them to verify that the _content_ of the final vcfs wasn't affected by the change makes more sense. VCF integration run: https://job-manager.dsde-prod.broadinstitute.org/jobs/e9896786-9d3f-48b9-ab6f-9a76d9fafafe; Hail integration run: https://job-manager.dsde-prod.broadinstitute.org/jobs/dbdd0934-50c3-4477-a191-3282341eacd3. another post-merge integration run with same results: identical outputs, failure due to lower table sizes and query scan costs; https://job-manager.dsde-prod.broadinstitute.org/jobs/edabdac3-1856-4e2a-aadc-3bcd4c353956",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8543
https://github.com/broadinstitute/gatk/pull/8543:721,Testability,Assert,AssertTableSizesAreExpected,721,"Adding in the option for using a compressed representation of our reference data. The flag use_compressed_references needs to be passed into ingest and also export, and doing so causes us to use a different internal schema for the ref_ranges table that can save ~40% on space. On the VCF path, the packed data is expanded while we populate the prepare tables before extract. On the VDS path, the packed data is expanded while we extract from the tables to create the Avro files. I updated integration tests to take use_compressed_refs as an option, and saw what I needed. Integrations tests for the VCF and VDS paths go to completion and fail, but in the expected way. AssertIdenticalOutputs, the important part, passes. AssertTableSizesAreExpected fails because the ref_ranges table is so much smaller (255884464 vs 431805033 expected) and AssertCostIsTrackedAndExpected fails because the PrepareRangesCallsetTask.GvsPrepareRanges.BigQuery Query Scanned is similarly smaller (340787200 vs 515899392). Given the nature of the change and the fact that it is an optional flag that defaults to zero, updating the integration tests to cover this path did not seem sound at this time. But using them to verify that the _content_ of the final vcfs wasn't affected by the change makes more sense. VCF integration run: https://job-manager.dsde-prod.broadinstitute.org/jobs/e9896786-9d3f-48b9-ab6f-9a76d9fafafe; Hail integration run: https://job-manager.dsde-prod.broadinstitute.org/jobs/dbdd0934-50c3-4477-a191-3282341eacd3. another post-merge integration run with same results: identical outputs, failure due to lower table sizes and query scan costs; https://job-manager.dsde-prod.broadinstitute.org/jobs/edabdac3-1856-4e2a-aadc-3bcd4c353956",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8543
https://github.com/broadinstitute/gatk/pull/8543:841,Testability,Assert,AssertCostIsTrackedAndExpected,841,"Adding in the option for using a compressed representation of our reference data. The flag use_compressed_references needs to be passed into ingest and also export, and doing so causes us to use a different internal schema for the ref_ranges table that can save ~40% on space. On the VCF path, the packed data is expanded while we populate the prepare tables before extract. On the VDS path, the packed data is expanded while we extract from the tables to create the Avro files. I updated integration tests to take use_compressed_refs as an option, and saw what I needed. Integrations tests for the VCF and VDS paths go to completion and fail, but in the expected way. AssertIdenticalOutputs, the important part, passes. AssertTableSizesAreExpected fails because the ref_ranges table is so much smaller (255884464 vs 431805033 expected) and AssertCostIsTrackedAndExpected fails because the PrepareRangesCallsetTask.GvsPrepareRanges.BigQuery Query Scanned is similarly smaller (340787200 vs 515899392). Given the nature of the change and the fact that it is an optional flag that defaults to zero, updating the integration tests to cover this path did not seem sound at this time. But using them to verify that the _content_ of the final vcfs wasn't affected by the change makes more sense. VCF integration run: https://job-manager.dsde-prod.broadinstitute.org/jobs/e9896786-9d3f-48b9-ab6f-9a76d9fafafe; Hail integration run: https://job-manager.dsde-prod.broadinstitute.org/jobs/dbdd0934-50c3-4477-a191-3282341eacd3. another post-merge integration run with same results: identical outputs, failure due to lower table sizes and query scan costs; https://job-manager.dsde-prod.broadinstitute.org/jobs/edabdac3-1856-4e2a-aadc-3bcd4c353956",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8543
https://github.com/broadinstitute/gatk/pull/8543:1122,Testability,test,tests,1122,"Adding in the option for using a compressed representation of our reference data. The flag use_compressed_references needs to be passed into ingest and also export, and doing so causes us to use a different internal schema for the ref_ranges table that can save ~40% on space. On the VCF path, the packed data is expanded while we populate the prepare tables before extract. On the VDS path, the packed data is expanded while we extract from the tables to create the Avro files. I updated integration tests to take use_compressed_refs as an option, and saw what I needed. Integrations tests for the VCF and VDS paths go to completion and fail, but in the expected way. AssertIdenticalOutputs, the important part, passes. AssertTableSizesAreExpected fails because the ref_ranges table is so much smaller (255884464 vs 431805033 expected) and AssertCostIsTrackedAndExpected fails because the PrepareRangesCallsetTask.GvsPrepareRanges.BigQuery Query Scanned is similarly smaller (340787200 vs 515899392). Given the nature of the change and the fact that it is an optional flag that defaults to zero, updating the integration tests to cover this path did not seem sound at this time. But using them to verify that the _content_ of the final vcfs wasn't affected by the change makes more sense. VCF integration run: https://job-manager.dsde-prod.broadinstitute.org/jobs/e9896786-9d3f-48b9-ab6f-9a76d9fafafe; Hail integration run: https://job-manager.dsde-prod.broadinstitute.org/jobs/dbdd0934-50c3-4477-a191-3282341eacd3. another post-merge integration run with same results: identical outputs, failure due to lower table sizes and query scan costs; https://job-manager.dsde-prod.broadinstitute.org/jobs/edabdac3-1856-4e2a-aadc-3bcd4c353956",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8543
https://github.com/broadinstitute/gatk/issues/8546:1654,Deployability,release,release,1654," addressed on the GATK forum: http://gatkforums.broadinstitute.org/gatk Post there if you have questions about expected tool behavior, output format, unexpected results, or generally any question that does not fit into the categories above; - Use a **concise** yet **descriptive** title; - Choose the corresponding template block below and fill it in, replacing or deleting text in italics (surrounded by `_`) as appropriate; - Delete the other template blocks and this header. ----. ## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_; GenotypeGVCFs, /public2/home/gaoshibin/software/gatk-4.4.0.0/gatk --java-options ""-Xmx160g -Djava.io.tmpdir=./tmp_fat"" GenotypeGVCFs -R /public2/home/gaoshibin/B73_REF/Zea_mays.AGPv4.dna.toplevel.fa -V gendb://./CHR9_gvcf_database -G StandardAnnotation -L 9:1-5000000 -O ./test.vcf.gz --genomicsdb-shared-posixfs-optimizations true; ### Affected version(s); - [ 4.1.9.0-4.4.0.0] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._; The ParaStor file system suffers from low CPU operating efficiency and extremely slow read and write speeds. If I test it on my own mobile hard drive, it's normal. The file format of my mobile hard disk is EXT4; #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_; /public2/home/gaoshibin/software/gatk-4.4.0.0/gatk --java-options ""-Xmx160g -Djava.io.tmpdir=./tmp_fat"" GenotypeGVCFs -R /public2/home/gaoshibin/B73_REF/Zea_mays.AGPv4.dna.toplevel.fa -V gendb://./CHR9_gvcf_database -G StandardAnnotation -L 9:1-5000000 -O ./test.vcf.gz --genomicsdb-shared-posixfs-optimizations true; #### Expected behavior; _Tell us what should happen_; The ParaStor file system suffers from",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8546
https://github.com/broadinstitute/gatk/issues/8546:1574,Performance,optimiz,optimizations,1574,"r tool/class javadoc only -- for forum docs please post there); - Consider if your ""issue"" is better addressed on the GATK forum: http://gatkforums.broadinstitute.org/gatk Post there if you have questions about expected tool behavior, output format, unexpected results, or generally any question that does not fit into the categories above; - Use a **concise** yet **descriptive** title; - Choose the corresponding template block below and fill it in, replacing or deleting text in italics (surrounded by `_`) as appropriate; - Delete the other template blocks and this header. ----. ## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_; GenotypeGVCFs, /public2/home/gaoshibin/software/gatk-4.4.0.0/gatk --java-options ""-Xmx160g -Djava.io.tmpdir=./tmp_fat"" GenotypeGVCFs -R /public2/home/gaoshibin/B73_REF/Zea_mays.AGPv4.dna.toplevel.fa -V gendb://./CHR9_gvcf_database -G StandardAnnotation -L 9:1-5000000 -O ./test.vcf.gz --genomicsdb-shared-posixfs-optimizations true; ### Affected version(s); - [ 4.1.9.0-4.4.0.0] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._; The ParaStor file system suffers from low CPU operating efficiency and extremely slow read and write speeds. If I test it on my own mobile hard drive, it's normal. The file format of my mobile hard disk is EXT4; #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_; /public2/home/gaoshibin/software/gatk-4.4.0.0/gatk --java-options ""-Xmx160g -Djava.io.tmpdir=./tmp_fat"" GenotypeGVCFs -R /public2/home/gaoshibin/B73_REF/Zea_mays.AGPv4.dna.toplevel.fa -V gendb://./CHR9_gvcf_database -G StandardAnnotation -L 9:1-5000000 -O ./test.vcf.gz --genomicsdb-shared-posixfs-optimizatio",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8546
https://github.com/broadinstitute/gatk/issues/8546:2573,Performance,optimiz,optimizations,2573,"r=./tmp_fat"" GenotypeGVCFs -R /public2/home/gaoshibin/B73_REF/Zea_mays.AGPv4.dna.toplevel.fa -V gendb://./CHR9_gvcf_database -G StandardAnnotation -L 9:1-5000000 -O ./test.vcf.gz --genomicsdb-shared-posixfs-optimizations true; ### Affected version(s); - [ 4.1.9.0-4.4.0.0] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._; The ParaStor file system suffers from low CPU operating efficiency and extremely slow read and write speeds. If I test it on my own mobile hard drive, it's normal. The file format of my mobile hard disk is EXT4; #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_; /public2/home/gaoshibin/software/gatk-4.4.0.0/gatk --java-options ""-Xmx160g -Djava.io.tmpdir=./tmp_fat"" GenotypeGVCFs -R /public2/home/gaoshibin/B73_REF/Zea_mays.AGPv4.dna.toplevel.fa -V gendb://./CHR9_gvcf_database -G StandardAnnotation -L 9:1-5000000 -O ./test.vcf.gz --genomicsdb-shared-posixfs-optimizations true; #### Expected behavior; _Tell us what should happen_; The ParaStor file system suffers from low CPU operating efficiency and extremely slow read and write speeds. If I test it on my own mobile hard drive, it's normal. The file format of my mobile hard disk is EXT4; #### Actual behavior; _Tell us what happens instead_. ----. ## Feature request. ### Tool(s) or class(es) involved; _Tool/class name(s), special parameters?_. ### Description; _Specify whether you want a modification of an existing behavior or addition of a new capability._; _Provide **examples**, **screenshots**, where appropriate._. ----. ## Documentation request. ### Tool(s) or class(es) involved; _Tool/class name(s), parameters?_. ### Description ; _Describe what needs to be added or modified._. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8546
https://github.com/broadinstitute/gatk/issues/8546:1534,Testability,test,test,1534,"re request**, or a **documentation request** (for tool/class javadoc only -- for forum docs please post there); - Consider if your ""issue"" is better addressed on the GATK forum: http://gatkforums.broadinstitute.org/gatk Post there if you have questions about expected tool behavior, output format, unexpected results, or generally any question that does not fit into the categories above; - Use a **concise** yet **descriptive** title; - Choose the corresponding template block below and fill it in, replacing or deleting text in italics (surrounded by `_`) as appropriate; - Delete the other template blocks and this header. ----. ## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_; GenotypeGVCFs, /public2/home/gaoshibin/software/gatk-4.4.0.0/gatk --java-options ""-Xmx160g -Djava.io.tmpdir=./tmp_fat"" GenotypeGVCFs -R /public2/home/gaoshibin/B73_REF/Zea_mays.AGPv4.dna.toplevel.fa -V gendb://./CHR9_gvcf_database -G StandardAnnotation -L 9:1-5000000 -O ./test.vcf.gz --genomicsdb-shared-posixfs-optimizations true; ### Affected version(s); - [ 4.1.9.0-4.4.0.0] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._; The ParaStor file system suffers from low CPU operating efficiency and extremely slow read and write speeds. If I test it on my own mobile hard drive, it's normal. The file format of my mobile hard disk is EXT4; #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_; /public2/home/gaoshibin/software/gatk-4.4.0.0/gatk --java-options ""-Xmx160g -Djava.io.tmpdir=./tmp_fat"" GenotypeGVCFs -R /public2/home/gaoshibin/B73_REF/Zea_mays.AGPv4.dna.toplevel.fa -V gendb://./CHR9_gvcf_database -G StandardAnnotation -L 9:1-5000000 -O ./tes",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8546
https://github.com/broadinstitute/gatk/issues/8546:1724,Testability,test,test,1724," addressed on the GATK forum: http://gatkforums.broadinstitute.org/gatk Post there if you have questions about expected tool behavior, output format, unexpected results, or generally any question that does not fit into the categories above; - Use a **concise** yet **descriptive** title; - Choose the corresponding template block below and fill it in, replacing or deleting text in italics (surrounded by `_`) as appropriate; - Delete the other template blocks and this header. ----. ## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_; GenotypeGVCFs, /public2/home/gaoshibin/software/gatk-4.4.0.0/gatk --java-options ""-Xmx160g -Djava.io.tmpdir=./tmp_fat"" GenotypeGVCFs -R /public2/home/gaoshibin/B73_REF/Zea_mays.AGPv4.dna.toplevel.fa -V gendb://./CHR9_gvcf_database -G StandardAnnotation -L 9:1-5000000 -O ./test.vcf.gz --genomicsdb-shared-posixfs-optimizations true; ### Affected version(s); - [ 4.1.9.0-4.4.0.0] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._; The ParaStor file system suffers from low CPU operating efficiency and extremely slow read and write speeds. If I test it on my own mobile hard drive, it's normal. The file format of my mobile hard disk is EXT4; #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_; /public2/home/gaoshibin/software/gatk-4.4.0.0/gatk --java-options ""-Xmx160g -Djava.io.tmpdir=./tmp_fat"" GenotypeGVCFs -R /public2/home/gaoshibin/B73_REF/Zea_mays.AGPv4.dna.toplevel.fa -V gendb://./CHR9_gvcf_database -G StandardAnnotation -L 9:1-5000000 -O ./test.vcf.gz --genomicsdb-shared-posixfs-optimizations true; #### Expected behavior; _Tell us what should happen_; The ParaStor file system suffers from",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8546
https://github.com/broadinstitute/gatk/issues/8546:1824,Testability,log,logs,1824,"vior, output format, unexpected results, or generally any question that does not fit into the categories above; - Use a **concise** yet **descriptive** title; - Choose the corresponding template block below and fill it in, replacing or deleting text in italics (surrounded by `_`) as appropriate; - Delete the other template blocks and this header. ----. ## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_; GenotypeGVCFs, /public2/home/gaoshibin/software/gatk-4.4.0.0/gatk --java-options ""-Xmx160g -Djava.io.tmpdir=./tmp_fat"" GenotypeGVCFs -R /public2/home/gaoshibin/B73_REF/Zea_mays.AGPv4.dna.toplevel.fa -V gendb://./CHR9_gvcf_database -G StandardAnnotation -L 9:1-5000000 -O ./test.vcf.gz --genomicsdb-shared-posixfs-optimizations true; ### Affected version(s); - [ 4.1.9.0-4.4.0.0] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._; The ParaStor file system suffers from low CPU operating efficiency and extremely slow read and write speeds. If I test it on my own mobile hard drive, it's normal. The file format of my mobile hard disk is EXT4; #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_; /public2/home/gaoshibin/software/gatk-4.4.0.0/gatk --java-options ""-Xmx160g -Djava.io.tmpdir=./tmp_fat"" GenotypeGVCFs -R /public2/home/gaoshibin/B73_REF/Zea_mays.AGPv4.dna.toplevel.fa -V gendb://./CHR9_gvcf_database -G StandardAnnotation -L 9:1-5000000 -O ./test.vcf.gz --genomicsdb-shared-posixfs-optimizations true; #### Expected behavior; _Tell us what should happen_; The ParaStor file system suffers from low CPU operating efficiency and extremely slow read and write speeds. If I test it on my own mobile hard drive, it's normal. The",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8546
https://github.com/broadinstitute/gatk/issues/8546:1966,Testability,test,test,1966,"rresponding template block below and fill it in, replacing or deleting text in italics (surrounded by `_`) as appropriate; - Delete the other template blocks and this header. ----. ## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_; GenotypeGVCFs, /public2/home/gaoshibin/software/gatk-4.4.0.0/gatk --java-options ""-Xmx160g -Djava.io.tmpdir=./tmp_fat"" GenotypeGVCFs -R /public2/home/gaoshibin/B73_REF/Zea_mays.AGPv4.dna.toplevel.fa -V gendb://./CHR9_gvcf_database -G StandardAnnotation -L 9:1-5000000 -O ./test.vcf.gz --genomicsdb-shared-posixfs-optimizations true; ### Affected version(s); - [ 4.1.9.0-4.4.0.0] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._; The ParaStor file system suffers from low CPU operating efficiency and extremely slow read and write speeds. If I test it on my own mobile hard drive, it's normal. The file format of my mobile hard disk is EXT4; #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_; /public2/home/gaoshibin/software/gatk-4.4.0.0/gatk --java-options ""-Xmx160g -Djava.io.tmpdir=./tmp_fat"" GenotypeGVCFs -R /public2/home/gaoshibin/B73_REF/Zea_mays.AGPv4.dna.toplevel.fa -V gendb://./CHR9_gvcf_database -G StandardAnnotation -L 9:1-5000000 -O ./test.vcf.gz --genomicsdb-shared-posixfs-optimizations true; #### Expected behavior; _Tell us what should happen_; The ParaStor file system suffers from low CPU operating efficiency and extremely slow read and write speeds. If I test it on my own mobile hard drive, it's normal. The file format of my mobile hard disk is EXT4; #### Actual behavior; _Tell us what happens instead_. ----. ## Feature request. ### Tool(s) or class(es) involved; _Tool/class na",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8546
https://github.com/broadinstitute/gatk/issues/8546:2533,Testability,test,test,2533,"r=./tmp_fat"" GenotypeGVCFs -R /public2/home/gaoshibin/B73_REF/Zea_mays.AGPv4.dna.toplevel.fa -V gendb://./CHR9_gvcf_database -G StandardAnnotation -L 9:1-5000000 -O ./test.vcf.gz --genomicsdb-shared-posixfs-optimizations true; ### Affected version(s); - [ 4.1.9.0-4.4.0.0] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._; The ParaStor file system suffers from low CPU operating efficiency and extremely slow read and write speeds. If I test it on my own mobile hard drive, it's normal. The file format of my mobile hard disk is EXT4; #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_; /public2/home/gaoshibin/software/gatk-4.4.0.0/gatk --java-options ""-Xmx160g -Djava.io.tmpdir=./tmp_fat"" GenotypeGVCFs -R /public2/home/gaoshibin/B73_REF/Zea_mays.AGPv4.dna.toplevel.fa -V gendb://./CHR9_gvcf_database -G StandardAnnotation -L 9:1-5000000 -O ./test.vcf.gz --genomicsdb-shared-posixfs-optimizations true; #### Expected behavior; _Tell us what should happen_; The ParaStor file system suffers from low CPU operating efficiency and extremely slow read and write speeds. If I test it on my own mobile hard drive, it's normal. The file format of my mobile hard disk is EXT4; #### Actual behavior; _Tell us what happens instead_. ----. ## Feature request. ### Tool(s) or class(es) involved; _Tool/class name(s), special parameters?_. ### Description; _Specify whether you want a modification of an existing behavior or addition of a new capability._; _Provide **examples**, **screenshots**, where appropriate._. ----. ## Documentation request. ### Tool(s) or class(es) involved; _Tool/class name(s), parameters?_. ### Description ; _Describe what needs to be added or modified._. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8546
https://github.com/broadinstitute/gatk/issues/8546:2761,Testability,test,test,2761,"r=./tmp_fat"" GenotypeGVCFs -R /public2/home/gaoshibin/B73_REF/Zea_mays.AGPv4.dna.toplevel.fa -V gendb://./CHR9_gvcf_database -G StandardAnnotation -L 9:1-5000000 -O ./test.vcf.gz --genomicsdb-shared-posixfs-optimizations true; ### Affected version(s); - [ 4.1.9.0-4.4.0.0] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._; The ParaStor file system suffers from low CPU operating efficiency and extremely slow read and write speeds. If I test it on my own mobile hard drive, it's normal. The file format of my mobile hard disk is EXT4; #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_; /public2/home/gaoshibin/software/gatk-4.4.0.0/gatk --java-options ""-Xmx160g -Djava.io.tmpdir=./tmp_fat"" GenotypeGVCFs -R /public2/home/gaoshibin/B73_REF/Zea_mays.AGPv4.dna.toplevel.fa -V gendb://./CHR9_gvcf_database -G StandardAnnotation -L 9:1-5000000 -O ./test.vcf.gz --genomicsdb-shared-posixfs-optimizations true; #### Expected behavior; _Tell us what should happen_; The ParaStor file system suffers from low CPU operating efficiency and extremely slow read and write speeds. If I test it on my own mobile hard drive, it's normal. The file format of my mobile hard disk is EXT4; #### Actual behavior; _Tell us what happens instead_. ----. ## Feature request. ### Tool(s) or class(es) involved; _Tool/class name(s), special parameters?_. ### Description; _Specify whether you want a modification of an existing behavior or addition of a new capability._; _Provide **examples**, **screenshots**, where appropriate._. ----. ## Documentation request. ### Tool(s) or class(es) involved; _Tool/class name(s), parameters?_. ### Description ; _Describe what needs to be added or modified._. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8546
https://github.com/broadinstitute/gatk/pull/8550:99,Deployability,Update,Updated,99,VS-857.; Change default behavior of GvsBulkIngestGenomes.wdl BACK to not dropping GQ0 ref blocks.; Updated AoU documentation to say we need to do it there (as an input). Passing Integration Test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/c4f921d8-52e7-4a44-9e0b-9f876eac71f3),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8550
https://github.com/broadinstitute/gatk/pull/8550:178,Deployability,Integrat,Integration,178,VS-857.; Change default behavior of GvsBulkIngestGenomes.wdl BACK to not dropping GQ0 ref blocks.; Updated AoU documentation to say we need to do it there (as an input). Passing Integration Test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/c4f921d8-52e7-4a44-9e0b-9f876eac71f3),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8550
https://github.com/broadinstitute/gatk/pull/8550:178,Integrability,Integrat,Integration,178,VS-857.; Change default behavior of GvsBulkIngestGenomes.wdl BACK to not dropping GQ0 ref blocks.; Updated AoU documentation to say we need to do it there (as an input). Passing Integration Test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/c4f921d8-52e7-4a44-9e0b-9f876eac71f3),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8550
https://github.com/broadinstitute/gatk/pull/8550:190,Testability,Test,Test,190,VS-857.; Change default behavior of GvsBulkIngestGenomes.wdl BACK to not dropping GQ0 ref blocks.; Updated AoU documentation to say we need to do it there (as an input). Passing Integration Test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/c4f921d8-52e7-4a44-9e0b-9f876eac71f3),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8550
https://github.com/broadinstitute/gatk/pull/8552:63,Deployability,integrat,integration,63,"Normally one provides passing workflow runs with a PR. For the integration run [that is here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/ab86fb6d-c5d6-48b6-8322-923af691751c). There's also a ""real"" run taking place using this branch [here](https://job-manager.dsde-prod.broadinstitute.org/jobs/db59d5b8-e2ac-4619-9563-aa5631bf053c). However for testing correctness of these changes with respect to the requester pays flag, my pet ""does not have serviceusage.services.use access to the Google Cloud project"". I therefore present instead a [run with my changes](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/9e712055-f466-4929-b6eb-5306f3cde1a0) that fails in exactly the same way as a [run without my changes](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/185506f5-9dc1-4c02-997d-6fe3f5695259).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8552
https://github.com/broadinstitute/gatk/pull/8552:63,Integrability,integrat,integration,63,"Normally one provides passing workflow runs with a PR. For the integration run [that is here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/ab86fb6d-c5d6-48b6-8322-923af691751c). There's also a ""real"" run taking place using this branch [here](https://job-manager.dsde-prod.broadinstitute.org/jobs/db59d5b8-e2ac-4619-9563-aa5631bf053c). However for testing correctness of these changes with respect to the requester pays flag, my pet ""does not have serviceusage.services.use access to the Google Cloud project"". I therefore present instead a [run with my changes](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/9e712055-f466-4929-b6eb-5306f3cde1a0) that fails in exactly the same way as a [run without my changes](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/185506f5-9dc1-4c02-997d-6fe3f5695259).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8552
https://github.com/broadinstitute/gatk/pull/8552:500,Security,access,access,500,"Normally one provides passing workflow runs with a PR. For the integration run [that is here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/ab86fb6d-c5d6-48b6-8322-923af691751c). There's also a ""real"" run taking place using this branch [here](https://job-manager.dsde-prod.broadinstitute.org/jobs/db59d5b8-e2ac-4619-9563-aa5631bf053c). However for testing correctness of these changes with respect to the requester pays flag, my pet ""does not have serviceusage.services.use access to the Google Cloud project"". I therefore present instead a [run with my changes](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/9e712055-f466-4929-b6eb-5306f3cde1a0) that fails in exactly the same way as a [run without my changes](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/185506f5-9dc1-4c02-997d-6fe3f5695259).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8552
https://github.com/broadinstitute/gatk/pull/8552:374,Testability,test,testing,374,"Normally one provides passing workflow runs with a PR. For the integration run [that is here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/ab86fb6d-c5d6-48b6-8322-923af691751c). There's also a ""real"" run taking place using this branch [here](https://job-manager.dsde-prod.broadinstitute.org/jobs/db59d5b8-e2ac-4619-9563-aa5631bf053c). However for testing correctness of these changes with respect to the requester pays flag, my pet ""does not have serviceusage.services.use access to the Google Cloud project"". I therefore present instead a [run with my changes](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/9e712055-f466-4929-b6eb-5306f3cde1a0) that fails in exactly the same way as a [run without my changes](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/185506f5-9dc1-4c02-997d-6fe3f5695259).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8552
https://github.com/broadinstitute/gatk/pull/8554:37,Availability,Echo,EchoCallset,37,Does this need to be merged into the EchoCallset branch eventually instead?. [Full integration run with the VDS Creation WDL instead of creating a VDS locally in a VM (yes yes also in a WDL but not scaleable)](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/be8cbcbf-1a92-4ec9-97cf-6530fe44ea2c). [VDS Creation run with VQSR Classic](https://job-manager.dsde-prod.broadinstitute.org/jobs/ec917eaf-b1c4-49d5-a51b-b82ddecf47ae). [VDS Creation run with VETS / vqsr LITE](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20cremer/job_history/112d710d-650e-4ac8-9b54-dc05e194c681),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8554
https://github.com/broadinstitute/gatk/pull/8554:83,Deployability,integrat,integration,83,Does this need to be merged into the EchoCallset branch eventually instead?. [Full integration run with the VDS Creation WDL instead of creating a VDS locally in a VM (yes yes also in a WDL but not scaleable)](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/be8cbcbf-1a92-4ec9-97cf-6530fe44ea2c). [VDS Creation run with VQSR Classic](https://job-manager.dsde-prod.broadinstitute.org/jobs/ec917eaf-b1c4-49d5-a51b-b82ddecf47ae). [VDS Creation run with VETS / vqsr LITE](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20cremer/job_history/112d710d-650e-4ac8-9b54-dc05e194c681),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8554
https://github.com/broadinstitute/gatk/pull/8554:83,Integrability,integrat,integration,83,Does this need to be merged into the EchoCallset branch eventually instead?. [Full integration run with the VDS Creation WDL instead of creating a VDS locally in a VM (yes yes also in a WDL but not scaleable)](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/be8cbcbf-1a92-4ec9-97cf-6530fe44ea2c). [VDS Creation run with VQSR Classic](https://job-manager.dsde-prod.broadinstitute.org/jobs/ec917eaf-b1c4-49d5-a51b-b82ddecf47ae). [VDS Creation run with VETS / vqsr LITE](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20cremer/job_history/112d710d-650e-4ac8-9b54-dc05e194c681),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8554
https://github.com/broadinstitute/gatk/issues/8555:609,Availability,failure,failures,609,"## Bug Report. ### Affected tool(s) or class(es); MarkDuplicatesSpark . ### Affected version(s); - Latest public release version [4.4.0.0]. ### Description . I am working on 40X human WGS data, running MarkDuplicatesSpark on the computation node of a cluster with 40 cores and 192GB RAM. MarkDuplicatesSpark usually hangs and never finish (even after few days) with log as below:. ```; 11:26:29.511 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:29.511 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:26:29.512 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:29.512 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:26:30.738 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:30.738 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:26:30.738 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:30.738 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:26:45.830 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:45.830 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:26:45.831 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:45.831 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:26:55.475 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:55.475 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folde",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8555
https://github.com/broadinstitute/gatk/issues/8555:849,Availability,failure,failures,849,"## Bug Report. ### Affected tool(s) or class(es); MarkDuplicatesSpark . ### Affected version(s); - Latest public release version [4.4.0.0]. ### Description . I am working on 40X human WGS data, running MarkDuplicatesSpark on the computation node of a cluster with 40 cores and 192GB RAM. MarkDuplicatesSpark usually hangs and never finish (even after few days) with log as below:. ```; 11:26:29.511 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:29.511 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:26:29.512 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:29.512 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:26:30.738 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:30.738 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:26:30.738 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:30.738 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:26:45.830 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:45.830 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:26:45.831 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:45.831 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:26:55.475 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:55.475 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folde",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8555
https://github.com/broadinstitute/gatk/issues/8555:1089,Availability,failure,failures,1089," class(es); MarkDuplicatesSpark . ### Affected version(s); - Latest public release version [4.4.0.0]. ### Description . I am working on 40X human WGS data, running MarkDuplicatesSpark on the computation node of a cluster with 40 cores and 192GB RAM. MarkDuplicatesSpark usually hangs and never finish (even after few days) with log as below:. ```; 11:26:29.511 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:29.511 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:26:29.512 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:29.512 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:26:30.738 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:30.738 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:26:30.738 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:30.738 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:26:45.830 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:45.830 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:26:45.831 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:45.831 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:26:55.475 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:55.475 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, igno",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8555
https://github.com/broadinstitute/gatk/issues/8555:1329,Availability,failure,failures,1329,"92GB RAM. MarkDuplicatesSpark usually hangs and never finish (even after few days) with log as below:. ```; 11:26:29.511 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:29.511 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:26:29.512 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:29.512 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:26:30.738 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:30.738 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:26:30.738 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:30.738 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:26:45.830 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:45.830 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:26:45.831 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:45.831 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:26:55.475 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:55.475 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:26:55.475 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:55.475 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, igno",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8555
https://github.com/broadinstitute/gatk/issues/8555:1569,Availability,failure,failures,1569,"tputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:26:29.512 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:29.512 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:26:30.738 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:30.738 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:26:30.738 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:30.738 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:26:45.830 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:45.830 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:26:45.831 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:45.831 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:26:55.475 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:55.475 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:26:55.475 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:55.475 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:38:18.971 INFO BlockManagerInfo - Removed broadcast_10_piece0 on hhnode-ib-16:42186 in memory (size: 1561.7 KiB, free: 17.8 GiB); ```. I have checked the node status and found MarkDuplicatesSpark suddenly cons",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8555
https://github.com/broadinstitute/gatk/issues/8555:1809,Availability,failure,failures,1809,"tputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:26:30.738 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:30.738 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:26:30.738 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:30.738 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:26:45.830 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:45.830 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:26:45.831 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:45.831 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:26:55.475 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:55.475 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:26:55.475 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:55.475 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:38:18.971 INFO BlockManagerInfo - Removed broadcast_10_piece0 on hhnode-ib-16:42186 in memory (size: 1561.7 KiB, free: 17.8 GiB); ```. I have checked the node status and found MarkDuplicatesSpark suddenly consumed huge amounts of memory. In the below image, there was no memory at ~11:26, and MarkDuplicatesSpark also hangs at that time. ![image](https://github.com/broadinstitute/gatk/assets/34618938/cc9ac23c-2f84-47c3-bbde-335efb325791). Below is",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8555
https://github.com/broadinstitute/gatk/issues/8555:2049,Availability,failure,failures,2049,"tputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:26:30.738 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:30.738 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:26:45.830 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:45.830 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:26:45.831 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:45.831 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:26:55.475 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:55.475 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:26:55.475 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:55.475 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:38:18.971 INFO BlockManagerInfo - Removed broadcast_10_piece0 on hhnode-ib-16:42186 in memory (size: 1561.7 KiB, free: 17.8 GiB); ```. I have checked the node status and found MarkDuplicatesSpark suddenly consumed huge amounts of memory. In the below image, there was no memory at ~11:26, and MarkDuplicatesSpark also hangs at that time. ![image](https://github.com/broadinstitute/gatk/assets/34618938/cc9ac23c-2f84-47c3-bbde-335efb325791). Below is the head of the log file showing my command and tool version. ```; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -j",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8555
https://github.com/broadinstitute/gatk/issues/8555:2289,Availability,failure,failures,2289,"tputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:26:45.830 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:45.830 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:26:45.831 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:45.831 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:26:55.475 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:55.475 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:26:55.475 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:55.475 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:38:18.971 INFO BlockManagerInfo - Removed broadcast_10_piece0 on hhnode-ib-16:42186 in memory (size: 1561.7 KiB, free: 17.8 GiB); ```. I have checked the node status and found MarkDuplicatesSpark suddenly consumed huge amounts of memory. In the below image, there was no memory at ~11:26, and MarkDuplicatesSpark also hangs at that time. ![image](https://github.com/broadinstitute/gatk/assets/34618938/cc9ac23c-2f84-47c3-bbde-335efb325791). Below is the head of the log file showing my command and tool version. ```; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/hcaoad/miniconda2/envs/gatk4/share/gatk4-4.4.0.0-0/gatk-package-4.4.0.0-local.jar MarkDuplicatesSpark -I U23_FDSW210237516-1r_H52MYDSX2_L4.namesort.bam -O U23.markdup.sort.bam; 10:38:16.187 INFO NativeLibraryLoader - Loading libgkl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8555
https://github.com/broadinstitute/gatk/issues/8555:5491,Availability,error,error,5491,"0:38:16.247 INFO MarkDuplicatesSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:38:16.247 INFO MarkDuplicatesSpark - Executing as hcaoad@hhnode-ib-16 on Linux v3.10.0-1062.el7.x86_64 amd64; 10:38:16.247 INFO MarkDuplicatesSpark - Java runtime: OpenJDK 64-Bit Server VM v17.0.8-internal+0-adhoc..src; 10:38:16.247 INFO MarkDuplicatesSpark - Start Date/Time: October 18, 2023 at 10:38:16 AM HKT; 10:38:16.247 INFO MarkDuplicatesSpark - ------------------------------------------------------------; 10:38:16.247 INFO MarkDuplicatesSpark - ------------------------------------------------------------; 10:38:16.248 INFO MarkDuplicatesSpark - HTSJDK Version: 3.0.5; 10:38:16.248 INFO MarkDuplicatesSpark - Picard Version: 3.0.0; 10:38:16.248 INFO MarkDuplicatesSpark - Built for Spark Version: 3.3.1; 10:38:16.249 INFO MarkDuplicatesSpark - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 10:38:16.249 INFO MarkDuplicatesSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 10:38:16.249 INFO MarkDuplicatesSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 10:38:16.249 INFO MarkDuplicatesSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 10:38:16.249 INFO MarkDuplicatesSpark - Deflater: IntelDeflater; 10:38:16.249 INFO MarkDuplicatesSpark - Inflater: IntelInflater; 10:38:16.250 INFO MarkDuplicatesSpark - GCS max retries/reopens: 20; 10:38:16.250 INFO MarkDuplicatesSpark - Requester pays: disabled; 10:38:16.250 INFO MarkDuplicatesSpark - Initializing engine; 10:38:16.250 INFO MarkDuplicatesSpark - Done initializing engine; 10:38:17.179 INFO SparkContext - Running Spark version 3.3.0; ```. #### Steps to reproduce; The most wired thing is that this issue is very hard to reproduce. When running the command for hundreds of samples, this always happens to a few samples. However, if I re-submit my job for the failed sample, this error may just disappear. As the whole log file is too big, I can upload it later if need. Thanks.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8555
https://github.com/broadinstitute/gatk/issues/8555:113,Deployability,release,release,113,"## Bug Report. ### Affected tool(s) or class(es); MarkDuplicatesSpark . ### Affected version(s); - Latest public release version [4.4.0.0]. ### Description . I am working on 40X human WGS data, running MarkDuplicatesSpark on the computation node of a cluster with 40 cores and 192GB RAM. MarkDuplicatesSpark usually hangs and never finish (even after few days) with log as below:. ```; 11:26:29.511 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:29.511 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:26:29.512 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:29.512 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:26:30.738 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:30.738 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:26:30.738 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:30.738 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:26:45.830 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:45.830 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:26:45.831 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:45.831 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:26:55.475 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:55.475 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folde",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8555
https://github.com/broadinstitute/gatk/issues/8555:3224,Performance,Load,Loading,3224,"up _temporary folders under output directory:false, ignore cleanup failures: false; 11:38:18.971 INFO BlockManagerInfo - Removed broadcast_10_piece0 on hhnode-ib-16:42186 in memory (size: 1561.7 KiB, free: 17.8 GiB); ```. I have checked the node status and found MarkDuplicatesSpark suddenly consumed huge amounts of memory. In the below image, there was no memory at ~11:26, and MarkDuplicatesSpark also hangs at that time. ![image](https://github.com/broadinstitute/gatk/assets/34618938/cc9ac23c-2f84-47c3-bbde-335efb325791). Below is the head of the log file showing my command and tool version. ```; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/hcaoad/miniconda2/envs/gatk4/share/gatk4-4.4.0.0-0/gatk-package-4.4.0.0-local.jar MarkDuplicatesSpark -I U23_FDSW210237516-1r_H52MYDSX2_L4.namesort.bam -O U23.markdup.sort.bam; 10:38:16.187 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/hcaoad/miniconda2/envs/gatk4/share/gatk4-4.4.0.0-0/gatk-package-4.4.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 10:38:16.244 INFO MarkDuplicatesSpark - ------------------------------------------------------------; 10:38:16.247 INFO MarkDuplicatesSpark - The Genome Analysis Toolkit (GATK) v4.4.0.0; 10:38:16.247 INFO MarkDuplicatesSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:38:16.247 INFO MarkDuplicatesSpark - Executing as hcaoad@hhnode-ib-16 on Linux v3.10.0-1062.el7.x86_64 amd64; 10:38:16.247 INFO MarkDuplicatesSpark - Java runtime: OpenJDK 64-Bit Server VM v17.0.8-internal+0-adhoc..src; 10:38:16.247 INFO MarkDuplicatesSpark - Start Date/Time: October 18, 2023 at 10:38:16 AM HKT; 10:38:16.247 INFO MarkDuplicatesSpark - ------------------------------------------------------------; 10:38:16.247 INFO MarkDuplicatesSpark - --------------------------------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8555
https://github.com/broadinstitute/gatk/issues/8555:366,Testability,log,log,366,"## Bug Report. ### Affected tool(s) or class(es); MarkDuplicatesSpark . ### Affected version(s); - Latest public release version [4.4.0.0]. ### Description . I am working on 40X human WGS data, running MarkDuplicatesSpark on the computation node of a cluster with 40 cores and 192GB RAM. MarkDuplicatesSpark usually hangs and never finish (even after few days) with log as below:. ```; 11:26:29.511 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:29.511 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:26:29.512 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:29.512 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:26:30.738 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:30.738 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:26:30.738 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:30.738 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:26:45.830 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:45.830 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:26:45.831 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:45.831 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:26:55.475 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:55.475 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folde",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8555
https://github.com/broadinstitute/gatk/issues/8555:2775,Testability,log,log,2775,"ry:false, ignore cleanup failures: false; 11:26:55.475 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:55.475 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:26:55.475 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:55.475 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:38:18.971 INFO BlockManagerInfo - Removed broadcast_10_piece0 on hhnode-ib-16:42186 in memory (size: 1561.7 KiB, free: 17.8 GiB); ```. I have checked the node status and found MarkDuplicatesSpark suddenly consumed huge amounts of memory. In the below image, there was no memory at ~11:26, and MarkDuplicatesSpark also hangs at that time. ![image](https://github.com/broadinstitute/gatk/assets/34618938/cc9ac23c-2f84-47c3-bbde-335efb325791). Below is the head of the log file showing my command and tool version. ```; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/hcaoad/miniconda2/envs/gatk4/share/gatk4-4.4.0.0-0/gatk-package-4.4.0.0-local.jar MarkDuplicatesSpark -I U23_FDSW210237516-1r_H52MYDSX2_L4.namesort.bam -O U23.markdup.sort.bam; 10:38:16.187 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/hcaoad/miniconda2/envs/gatk4/share/gatk4-4.4.0.0-0/gatk-package-4.4.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 10:38:16.244 INFO MarkDuplicatesSpark - ------------------------------------------------------------; 10:38:16.247 INFO MarkDuplicatesSpark - The Genome Analysis Toolkit (GATK) v4.4.0.0; 10:38:16.247 INFO MarkDuplicatesSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:38:16.247 INFO MarkDuplicatesSpark - Executing as hcaoad@hhnode-ib-16 on Li",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8555
https://github.com/broadinstitute/gatk/issues/8555:5530,Testability,log,log,5530,"0:38:16.247 INFO MarkDuplicatesSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:38:16.247 INFO MarkDuplicatesSpark - Executing as hcaoad@hhnode-ib-16 on Linux v3.10.0-1062.el7.x86_64 amd64; 10:38:16.247 INFO MarkDuplicatesSpark - Java runtime: OpenJDK 64-Bit Server VM v17.0.8-internal+0-adhoc..src; 10:38:16.247 INFO MarkDuplicatesSpark - Start Date/Time: October 18, 2023 at 10:38:16 AM HKT; 10:38:16.247 INFO MarkDuplicatesSpark - ------------------------------------------------------------; 10:38:16.247 INFO MarkDuplicatesSpark - ------------------------------------------------------------; 10:38:16.248 INFO MarkDuplicatesSpark - HTSJDK Version: 3.0.5; 10:38:16.248 INFO MarkDuplicatesSpark - Picard Version: 3.0.0; 10:38:16.248 INFO MarkDuplicatesSpark - Built for Spark Version: 3.3.1; 10:38:16.249 INFO MarkDuplicatesSpark - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 10:38:16.249 INFO MarkDuplicatesSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 10:38:16.249 INFO MarkDuplicatesSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 10:38:16.249 INFO MarkDuplicatesSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 10:38:16.249 INFO MarkDuplicatesSpark - Deflater: IntelDeflater; 10:38:16.249 INFO MarkDuplicatesSpark - Inflater: IntelInflater; 10:38:16.250 INFO MarkDuplicatesSpark - GCS max retries/reopens: 20; 10:38:16.250 INFO MarkDuplicatesSpark - Requester pays: disabled; 10:38:16.250 INFO MarkDuplicatesSpark - Initializing engine; 10:38:16.250 INFO MarkDuplicatesSpark - Done initializing engine; 10:38:17.179 INFO SparkContext - Running Spark version 3.3.0; ```. #### Steps to reproduce; The most wired thing is that this issue is very hard to reproduce. When running the command for hundreds of samples, this always happens to a few samples. However, if I re-submit my job for the failed sample, this error may just disappear. As the whole log file is too big, I can upload it later if need. Thanks.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8555
https://github.com/broadinstitute/gatk/pull/8556:51,Deployability,integrat,integration,51,there was a code path that didn't get exercised in integration tests or quickstart data (writeMissingIntervals) that wasn't made aware of the storeCompressedReferences flag. Updated to operate correctly in its presence,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8556
https://github.com/broadinstitute/gatk/pull/8556:174,Deployability,Update,Updated,174,there was a code path that didn't get exercised in integration tests or quickstart data (writeMissingIntervals) that wasn't made aware of the storeCompressedReferences flag. Updated to operate correctly in its presence,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8556
https://github.com/broadinstitute/gatk/pull/8556:51,Integrability,integrat,integration,51,there was a code path that didn't get exercised in integration tests or quickstart data (writeMissingIntervals) that wasn't made aware of the storeCompressedReferences flag. Updated to operate correctly in its presence,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8556
https://github.com/broadinstitute/gatk/pull/8556:63,Testability,test,tests,63,there was a code path that didn't get exercised in integration tests or quickstart data (writeMissingIntervals) that wasn't made aware of the storeCompressedReferences flag. Updated to operate correctly in its presence,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8556
https://github.com/broadinstitute/gatk/issues/8558:40,Availability,Down,DownsampleSam,40,"### Affected tool(s) or class(es); gatk DownsampleSam. ### Affected version(s); GATK v4.3.0.0. ### Description ; Input cram file (gs://broad-public-datasets/CHM1_CHM13_WGS2/CHM1_CHM13_WGS2.cram) ; has NM tags, but the downsampled output file no longer has them. My command-line is ; ```; gatk DownsampleSam REFERENCE_SEQUENCE=/hg38.fa I=CHM1_CHM13_WGS2.cram P=0.5 CREATE_INDEX=true O=CHM1_CHM13_WGS2.downsampled.bam ; ```. Some downstream tools require NM tags, so I have to run . `samtools calmd CHM1_CHM13_WGS2.downsampled.bam /hg38.fa`. to re-add it.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8558
https://github.com/broadinstitute/gatk/issues/8558:218,Availability,down,downsampled,218,"### Affected tool(s) or class(es); gatk DownsampleSam. ### Affected version(s); GATK v4.3.0.0. ### Description ; Input cram file (gs://broad-public-datasets/CHM1_CHM13_WGS2/CHM1_CHM13_WGS2.cram) ; has NM tags, but the downsampled output file no longer has them. My command-line is ; ```; gatk DownsampleSam REFERENCE_SEQUENCE=/hg38.fa I=CHM1_CHM13_WGS2.cram P=0.5 CREATE_INDEX=true O=CHM1_CHM13_WGS2.downsampled.bam ; ```. Some downstream tools require NM tags, so I have to run . `samtools calmd CHM1_CHM13_WGS2.downsampled.bam /hg38.fa`. to re-add it.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8558
https://github.com/broadinstitute/gatk/issues/8558:293,Availability,Down,DownsampleSam,293,"### Affected tool(s) or class(es); gatk DownsampleSam. ### Affected version(s); GATK v4.3.0.0. ### Description ; Input cram file (gs://broad-public-datasets/CHM1_CHM13_WGS2/CHM1_CHM13_WGS2.cram) ; has NM tags, but the downsampled output file no longer has them. My command-line is ; ```; gatk DownsampleSam REFERENCE_SEQUENCE=/hg38.fa I=CHM1_CHM13_WGS2.cram P=0.5 CREATE_INDEX=true O=CHM1_CHM13_WGS2.downsampled.bam ; ```. Some downstream tools require NM tags, so I have to run . `samtools calmd CHM1_CHM13_WGS2.downsampled.bam /hg38.fa`. to re-add it.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8558
https://github.com/broadinstitute/gatk/issues/8558:400,Availability,down,downsampled,400,"### Affected tool(s) or class(es); gatk DownsampleSam. ### Affected version(s); GATK v4.3.0.0. ### Description ; Input cram file (gs://broad-public-datasets/CHM1_CHM13_WGS2/CHM1_CHM13_WGS2.cram) ; has NM tags, but the downsampled output file no longer has them. My command-line is ; ```; gatk DownsampleSam REFERENCE_SEQUENCE=/hg38.fa I=CHM1_CHM13_WGS2.cram P=0.5 CREATE_INDEX=true O=CHM1_CHM13_WGS2.downsampled.bam ; ```. Some downstream tools require NM tags, so I have to run . `samtools calmd CHM1_CHM13_WGS2.downsampled.bam /hg38.fa`. to re-add it.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8558
https://github.com/broadinstitute/gatk/issues/8558:428,Availability,down,downstream,428,"### Affected tool(s) or class(es); gatk DownsampleSam. ### Affected version(s); GATK v4.3.0.0. ### Description ; Input cram file (gs://broad-public-datasets/CHM1_CHM13_WGS2/CHM1_CHM13_WGS2.cram) ; has NM tags, but the downsampled output file no longer has them. My command-line is ; ```; gatk DownsampleSam REFERENCE_SEQUENCE=/hg38.fa I=CHM1_CHM13_WGS2.cram P=0.5 CREATE_INDEX=true O=CHM1_CHM13_WGS2.downsampled.bam ; ```. Some downstream tools require NM tags, so I have to run . `samtools calmd CHM1_CHM13_WGS2.downsampled.bam /hg38.fa`. to re-add it.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8558
https://github.com/broadinstitute/gatk/issues/8558:513,Availability,down,downsampled,513,"### Affected tool(s) or class(es); gatk DownsampleSam. ### Affected version(s); GATK v4.3.0.0. ### Description ; Input cram file (gs://broad-public-datasets/CHM1_CHM13_WGS2/CHM1_CHM13_WGS2.cram) ; has NM tags, but the downsampled output file no longer has them. My command-line is ; ```; gatk DownsampleSam REFERENCE_SEQUENCE=/hg38.fa I=CHM1_CHM13_WGS2.cram P=0.5 CREATE_INDEX=true O=CHM1_CHM13_WGS2.downsampled.bam ; ```. Some downstream tools require NM tags, so I have to run . `samtools calmd CHM1_CHM13_WGS2.downsampled.bam /hg38.fa`. to re-add it.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8558
https://github.com/broadinstitute/gatk/pull/8560:152,Deployability,Integrat,Integration,152,"Modify CreateVariantIngestFiles to write missing ref intervals with the ZERO state, unless we are dropping that (ZERO) state and none other. Successful Integration test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/0509cd35-50bc-431b-88c2-590e15cd3cc9)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8560
https://github.com/broadinstitute/gatk/pull/8560:152,Integrability,Integrat,Integration,152,"Modify CreateVariantIngestFiles to write missing ref intervals with the ZERO state, unless we are dropping that (ZERO) state and none other. Successful Integration test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/0509cd35-50bc-431b-88c2-590e15cd3cc9)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8560
https://github.com/broadinstitute/gatk/pull/8560:164,Testability,test,test,164,"Modify CreateVariantIngestFiles to write missing ref intervals with the ZERO state, unless we are dropping that (ZERO) state and none other. Successful Integration test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/0509cd35-50bc-431b-88c2-590e15cd3cc9)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8560
https://github.com/broadinstitute/gatk/pull/8561:272,Deployability,update,update,272,"Copying over some discussion from Slack, with some slight modifications:. > I took a quick stab at updating the environment for gCNV. Even taking out TensorFlow (assuming that the CNN will not be supported by this environment), it's a difficult task:; > 1. The goal is to update Python from 3.6 to 3.10+, since Terra now requires the latter for officially supported images.; > 2. However, gCNV relies on the PyMC3 package. PyMC3 3.1 is currently used in GATK master. 3.1 was released in 2017, not long before our release of gCNV in 2018, but it's very old now.; > 3. The latest version of Python that is supported by PyMC3 3.1 in conda is Python 3.6.; > 4. @asmirnov239 has a draft PR (#8094) that updates PyMC3 to 3.5 and Python to 3.7, which clearly still falls short of Python 3.10+. This PR also updated some gCNV code to make it compatible with PyMC3 3.5. (It also removed TensorFlow and added PyTorch.); > 5. @asmirnov239 also merged a PR that added tests for numerical reproducibility of GermlineCNVCaller in cohort mode in #7889.; > 6. The earliest version of PyMC that supports Python 3.10+ is PyMC 4, released in 2022.; > 7. However, PyMC 4 introduces API changes, which will also require additional gCNV code changes and numerical testing.; > 8. These API changes are because the underlying computational backend for PyMC was updated from Theano (think of this as an old alternative to TensorFlow) to Aesara.; > 9. Since then, PyMC 5.9 has been released and the underlying backend has been updated again, from Aesara to PyTensor.; > 10. So if we are going to update the environment to support Python 3.10+, it probably makes sense to go all the way to PyMC 5.9. I've made some strides in this PR; as of [6b08f3a](https://github.com/broadinstitute/gatk/pull/8561/commits/6b08f3af205cb9af1f5c63a0786f9a5a52cd78c1), I've made enough updates to accommodate API changes so that cohort-mode inference for both GermlineCNVCaller and DetermineGermlineContigPloidy runs successfully under Python 3.1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8561
https://github.com/broadinstitute/gatk/pull/8561:475,Deployability,release,released,475,"Copying over some discussion from Slack, with some slight modifications:. > I took a quick stab at updating the environment for gCNV. Even taking out TensorFlow (assuming that the CNN will not be supported by this environment), it's a difficult task:; > 1. The goal is to update Python from 3.6 to 3.10+, since Terra now requires the latter for officially supported images.; > 2. However, gCNV relies on the PyMC3 package. PyMC3 3.1 is currently used in GATK master. 3.1 was released in 2017, not long before our release of gCNV in 2018, but it's very old now.; > 3. The latest version of Python that is supported by PyMC3 3.1 in conda is Python 3.6.; > 4. @asmirnov239 has a draft PR (#8094) that updates PyMC3 to 3.5 and Python to 3.7, which clearly still falls short of Python 3.10+. This PR also updated some gCNV code to make it compatible with PyMC3 3.5. (It also removed TensorFlow and added PyTorch.); > 5. @asmirnov239 also merged a PR that added tests for numerical reproducibility of GermlineCNVCaller in cohort mode in #7889.; > 6. The earliest version of PyMC that supports Python 3.10+ is PyMC 4, released in 2022.; > 7. However, PyMC 4 introduces API changes, which will also require additional gCNV code changes and numerical testing.; > 8. These API changes are because the underlying computational backend for PyMC was updated from Theano (think of this as an old alternative to TensorFlow) to Aesara.; > 9. Since then, PyMC 5.9 has been released and the underlying backend has been updated again, from Aesara to PyTensor.; > 10. So if we are going to update the environment to support Python 3.10+, it probably makes sense to go all the way to PyMC 5.9. I've made some strides in this PR; as of [6b08f3a](https://github.com/broadinstitute/gatk/pull/8561/commits/6b08f3af205cb9af1f5c63a0786f9a5a52cd78c1), I've made enough updates to accommodate API changes so that cohort-mode inference for both GermlineCNVCaller and DetermineGermlineContigPloidy runs successfully under Python 3.1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8561
https://github.com/broadinstitute/gatk/pull/8561:513,Deployability,release,release,513,"Copying over some discussion from Slack, with some slight modifications:. > I took a quick stab at updating the environment for gCNV. Even taking out TensorFlow (assuming that the CNN will not be supported by this environment), it's a difficult task:; > 1. The goal is to update Python from 3.6 to 3.10+, since Terra now requires the latter for officially supported images.; > 2. However, gCNV relies on the PyMC3 package. PyMC3 3.1 is currently used in GATK master. 3.1 was released in 2017, not long before our release of gCNV in 2018, but it's very old now.; > 3. The latest version of Python that is supported by PyMC3 3.1 in conda is Python 3.6.; > 4. @asmirnov239 has a draft PR (#8094) that updates PyMC3 to 3.5 and Python to 3.7, which clearly still falls short of Python 3.10+. This PR also updated some gCNV code to make it compatible with PyMC3 3.5. (It also removed TensorFlow and added PyTorch.); > 5. @asmirnov239 also merged a PR that added tests for numerical reproducibility of GermlineCNVCaller in cohort mode in #7889.; > 6. The earliest version of PyMC that supports Python 3.10+ is PyMC 4, released in 2022.; > 7. However, PyMC 4 introduces API changes, which will also require additional gCNV code changes and numerical testing.; > 8. These API changes are because the underlying computational backend for PyMC was updated from Theano (think of this as an old alternative to TensorFlow) to Aesara.; > 9. Since then, PyMC 5.9 has been released and the underlying backend has been updated again, from Aesara to PyTensor.; > 10. So if we are going to update the environment to support Python 3.10+, it probably makes sense to go all the way to PyMC 5.9. I've made some strides in this PR; as of [6b08f3a](https://github.com/broadinstitute/gatk/pull/8561/commits/6b08f3af205cb9af1f5c63a0786f9a5a52cd78c1), I've made enough updates to accommodate API changes so that cohort-mode inference for both GermlineCNVCaller and DetermineGermlineContigPloidy runs successfully under Python 3.1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8561
https://github.com/broadinstitute/gatk/pull/8561:698,Deployability,update,updates,698,"Copying over some discussion from Slack, with some slight modifications:. > I took a quick stab at updating the environment for gCNV. Even taking out TensorFlow (assuming that the CNN will not be supported by this environment), it's a difficult task:; > 1. The goal is to update Python from 3.6 to 3.10+, since Terra now requires the latter for officially supported images.; > 2. However, gCNV relies on the PyMC3 package. PyMC3 3.1 is currently used in GATK master. 3.1 was released in 2017, not long before our release of gCNV in 2018, but it's very old now.; > 3. The latest version of Python that is supported by PyMC3 3.1 in conda is Python 3.6.; > 4. @asmirnov239 has a draft PR (#8094) that updates PyMC3 to 3.5 and Python to 3.7, which clearly still falls short of Python 3.10+. This PR also updated some gCNV code to make it compatible with PyMC3 3.5. (It also removed TensorFlow and added PyTorch.); > 5. @asmirnov239 also merged a PR that added tests for numerical reproducibility of GermlineCNVCaller in cohort mode in #7889.; > 6. The earliest version of PyMC that supports Python 3.10+ is PyMC 4, released in 2022.; > 7. However, PyMC 4 introduces API changes, which will also require additional gCNV code changes and numerical testing.; > 8. These API changes are because the underlying computational backend for PyMC was updated from Theano (think of this as an old alternative to TensorFlow) to Aesara.; > 9. Since then, PyMC 5.9 has been released and the underlying backend has been updated again, from Aesara to PyTensor.; > 10. So if we are going to update the environment to support Python 3.10+, it probably makes sense to go all the way to PyMC 5.9. I've made some strides in this PR; as of [6b08f3a](https://github.com/broadinstitute/gatk/pull/8561/commits/6b08f3af205cb9af1f5c63a0786f9a5a52cd78c1), I've made enough updates to accommodate API changes so that cohort-mode inference for both GermlineCNVCaller and DetermineGermlineContigPloidy runs successfully under Python 3.1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8561
https://github.com/broadinstitute/gatk/pull/8561:800,Deployability,update,updated,800,"Copying over some discussion from Slack, with some slight modifications:. > I took a quick stab at updating the environment for gCNV. Even taking out TensorFlow (assuming that the CNN will not be supported by this environment), it's a difficult task:; > 1. The goal is to update Python from 3.6 to 3.10+, since Terra now requires the latter for officially supported images.; > 2. However, gCNV relies on the PyMC3 package. PyMC3 3.1 is currently used in GATK master. 3.1 was released in 2017, not long before our release of gCNV in 2018, but it's very old now.; > 3. The latest version of Python that is supported by PyMC3 3.1 in conda is Python 3.6.; > 4. @asmirnov239 has a draft PR (#8094) that updates PyMC3 to 3.5 and Python to 3.7, which clearly still falls short of Python 3.10+. This PR also updated some gCNV code to make it compatible with PyMC3 3.5. (It also removed TensorFlow and added PyTorch.); > 5. @asmirnov239 also merged a PR that added tests for numerical reproducibility of GermlineCNVCaller in cohort mode in #7889.; > 6. The earliest version of PyMC that supports Python 3.10+ is PyMC 4, released in 2022.; > 7. However, PyMC 4 introduces API changes, which will also require additional gCNV code changes and numerical testing.; > 8. These API changes are because the underlying computational backend for PyMC was updated from Theano (think of this as an old alternative to TensorFlow) to Aesara.; > 9. Since then, PyMC 5.9 has been released and the underlying backend has been updated again, from Aesara to PyTensor.; > 10. So if we are going to update the environment to support Python 3.10+, it probably makes sense to go all the way to PyMC 5.9. I've made some strides in this PR; as of [6b08f3a](https://github.com/broadinstitute/gatk/pull/8561/commits/6b08f3af205cb9af1f5c63a0786f9a5a52cd78c1), I've made enough updates to accommodate API changes so that cohort-mode inference for both GermlineCNVCaller and DetermineGermlineContigPloidy runs successfully under Python 3.1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8561
https://github.com/broadinstitute/gatk/pull/8561:1111,Deployability,release,released,1111,"environment for gCNV. Even taking out TensorFlow (assuming that the CNN will not be supported by this environment), it's a difficult task:; > 1. The goal is to update Python from 3.6 to 3.10+, since Terra now requires the latter for officially supported images.; > 2. However, gCNV relies on the PyMC3 package. PyMC3 3.1 is currently used in GATK master. 3.1 was released in 2017, not long before our release of gCNV in 2018, but it's very old now.; > 3. The latest version of Python that is supported by PyMC3 3.1 in conda is Python 3.6.; > 4. @asmirnov239 has a draft PR (#8094) that updates PyMC3 to 3.5 and Python to 3.7, which clearly still falls short of Python 3.10+. This PR also updated some gCNV code to make it compatible with PyMC3 3.5. (It also removed TensorFlow and added PyTorch.); > 5. @asmirnov239 also merged a PR that added tests for numerical reproducibility of GermlineCNVCaller in cohort mode in #7889.; > 6. The earliest version of PyMC that supports Python 3.10+ is PyMC 4, released in 2022.; > 7. However, PyMC 4 introduces API changes, which will also require additional gCNV code changes and numerical testing.; > 8. These API changes are because the underlying computational backend for PyMC was updated from Theano (think of this as an old alternative to TensorFlow) to Aesara.; > 9. Since then, PyMC 5.9 has been released and the underlying backend has been updated again, from Aesara to PyTensor.; > 10. So if we are going to update the environment to support Python 3.10+, it probably makes sense to go all the way to PyMC 5.9. I've made some strides in this PR; as of [6b08f3a](https://github.com/broadinstitute/gatk/pull/8561/commits/6b08f3af205cb9af1f5c63a0786f9a5a52cd78c1), I've made enough updates to accommodate API changes so that cohort-mode inference for both GermlineCNVCaller and DetermineGermlineContigPloidy runs successfully under Python 3.10 and PyMC 5.9.0---although note that 5.9.1 has been released in the interim!. However, our work has just begun",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8561
https://github.com/broadinstitute/gatk/pull/8561:1337,Deployability,update,updated,1337,"ter for officially supported images.; > 2. However, gCNV relies on the PyMC3 package. PyMC3 3.1 is currently used in GATK master. 3.1 was released in 2017, not long before our release of gCNV in 2018, but it's very old now.; > 3. The latest version of Python that is supported by PyMC3 3.1 in conda is Python 3.6.; > 4. @asmirnov239 has a draft PR (#8094) that updates PyMC3 to 3.5 and Python to 3.7, which clearly still falls short of Python 3.10+. This PR also updated some gCNV code to make it compatible with PyMC3 3.5. (It also removed TensorFlow and added PyTorch.); > 5. @asmirnov239 also merged a PR that added tests for numerical reproducibility of GermlineCNVCaller in cohort mode in #7889.; > 6. The earliest version of PyMC that supports Python 3.10+ is PyMC 4, released in 2022.; > 7. However, PyMC 4 introduces API changes, which will also require additional gCNV code changes and numerical testing.; > 8. These API changes are because the underlying computational backend for PyMC was updated from Theano (think of this as an old alternative to TensorFlow) to Aesara.; > 9. Since then, PyMC 5.9 has been released and the underlying backend has been updated again, from Aesara to PyTensor.; > 10. So if we are going to update the environment to support Python 3.10+, it probably makes sense to go all the way to PyMC 5.9. I've made some strides in this PR; as of [6b08f3a](https://github.com/broadinstitute/gatk/pull/8561/commits/6b08f3af205cb9af1f5c63a0786f9a5a52cd78c1), I've made enough updates to accommodate API changes so that cohort-mode inference for both GermlineCNVCaller and DetermineGermlineContigPloidy runs successfully under Python 3.10 and PyMC 5.9.0---although note that 5.9.1 has been released in the interim!. However, our work has just begun. Results now produced in the numerical tests mentioned above are quite far off from the original expected results. It remains to be seen whether this is due to the randomness of inference, some slight changes to the model pri",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8561
https://github.com/broadinstitute/gatk/pull/8561:1456,Deployability,release,released,1456," not long before our release of gCNV in 2018, but it's very old now.; > 3. The latest version of Python that is supported by PyMC3 3.1 in conda is Python 3.6.; > 4. @asmirnov239 has a draft PR (#8094) that updates PyMC3 to 3.5 and Python to 3.7, which clearly still falls short of Python 3.10+. This PR also updated some gCNV code to make it compatible with PyMC3 3.5. (It also removed TensorFlow and added PyTorch.); > 5. @asmirnov239 also merged a PR that added tests for numerical reproducibility of GermlineCNVCaller in cohort mode in #7889.; > 6. The earliest version of PyMC that supports Python 3.10+ is PyMC 4, released in 2022.; > 7. However, PyMC 4 introduces API changes, which will also require additional gCNV code changes and numerical testing.; > 8. These API changes are because the underlying computational backend for PyMC was updated from Theano (think of this as an old alternative to TensorFlow) to Aesara.; > 9. Since then, PyMC 5.9 has been released and the underlying backend has been updated again, from Aesara to PyTensor.; > 10. So if we are going to update the environment to support Python 3.10+, it probably makes sense to go all the way to PyMC 5.9. I've made some strides in this PR; as of [6b08f3a](https://github.com/broadinstitute/gatk/pull/8561/commits/6b08f3af205cb9af1f5c63a0786f9a5a52cd78c1), I've made enough updates to accommodate API changes so that cohort-mode inference for both GermlineCNVCaller and DetermineGermlineContigPloidy runs successfully under Python 3.10 and PyMC 5.9.0---although note that 5.9.1 has been released in the interim!. However, our work has just begun. Results now produced in the numerical tests mentioned above are quite far off from the original expected results. It remains to be seen whether this is due to the randomness of inference, some slight changes to the model prior that were necessitated by the API changes, or some bugs introduced in other code updates. (Also note that I believe Andrey's PR in item 4 already broke ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8561
https://github.com/broadinstitute/gatk/pull/8561:1501,Deployability,update,updated,1501," not long before our release of gCNV in 2018, but it's very old now.; > 3. The latest version of Python that is supported by PyMC3 3.1 in conda is Python 3.6.; > 4. @asmirnov239 has a draft PR (#8094) that updates PyMC3 to 3.5 and Python to 3.7, which clearly still falls short of Python 3.10+. This PR also updated some gCNV code to make it compatible with PyMC3 3.5. (It also removed TensorFlow and added PyTorch.); > 5. @asmirnov239 also merged a PR that added tests for numerical reproducibility of GermlineCNVCaller in cohort mode in #7889.; > 6. The earliest version of PyMC that supports Python 3.10+ is PyMC 4, released in 2022.; > 7. However, PyMC 4 introduces API changes, which will also require additional gCNV code changes and numerical testing.; > 8. These API changes are because the underlying computational backend for PyMC was updated from Theano (think of this as an old alternative to TensorFlow) to Aesara.; > 9. Since then, PyMC 5.9 has been released and the underlying backend has been updated again, from Aesara to PyTensor.; > 10. So if we are going to update the environment to support Python 3.10+, it probably makes sense to go all the way to PyMC 5.9. I've made some strides in this PR; as of [6b08f3a](https://github.com/broadinstitute/gatk/pull/8561/commits/6b08f3af205cb9af1f5c63a0786f9a5a52cd78c1), I've made enough updates to accommodate API changes so that cohort-mode inference for both GermlineCNVCaller and DetermineGermlineContigPloidy runs successfully under Python 3.10 and PyMC 5.9.0---although note that 5.9.1 has been released in the interim!. However, our work has just begun. Results now produced in the numerical tests mentioned above are quite far off from the original expected results. It remains to be seen whether this is due to the randomness of inference, some slight changes to the model prior that were necessitated by the API changes, or some bugs introduced in other code updates. (Also note that I believe Andrey's PR in item 4 already broke ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8561
https://github.com/broadinstitute/gatk/pull/8561:1570,Deployability,update,update,1570,"rsion of Python that is supported by PyMC3 3.1 in conda is Python 3.6.; > 4. @asmirnov239 has a draft PR (#8094) that updates PyMC3 to 3.5 and Python to 3.7, which clearly still falls short of Python 3.10+. This PR also updated some gCNV code to make it compatible with PyMC3 3.5. (It also removed TensorFlow and added PyTorch.); > 5. @asmirnov239 also merged a PR that added tests for numerical reproducibility of GermlineCNVCaller in cohort mode in #7889.; > 6. The earliest version of PyMC that supports Python 3.10+ is PyMC 4, released in 2022.; > 7. However, PyMC 4 introduces API changes, which will also require additional gCNV code changes and numerical testing.; > 8. These API changes are because the underlying computational backend for PyMC was updated from Theano (think of this as an old alternative to TensorFlow) to Aesara.; > 9. Since then, PyMC 5.9 has been released and the underlying backend has been updated again, from Aesara to PyTensor.; > 10. So if we are going to update the environment to support Python 3.10+, it probably makes sense to go all the way to PyMC 5.9. I've made some strides in this PR; as of [6b08f3a](https://github.com/broadinstitute/gatk/pull/8561/commits/6b08f3af205cb9af1f5c63a0786f9a5a52cd78c1), I've made enough updates to accommodate API changes so that cohort-mode inference for both GermlineCNVCaller and DetermineGermlineContigPloidy runs successfully under Python 3.10 and PyMC 5.9.0---although note that 5.9.1 has been released in the interim!. However, our work has just begun. Results now produced in the numerical tests mentioned above are quite far off from the original expected results. It remains to be seen whether this is due to the randomness of inference, some slight changes to the model prior that were necessitated by the API changes, or some bugs introduced in other code updates. (Also note that I believe Andrey's PR in item 4 already broke these tests, although the numerical differences were much smaller and more reasonable--",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8561
https://github.com/broadinstitute/gatk/pull/8561:1841,Deployability,update,updates,1841," removed TensorFlow and added PyTorch.); > 5. @asmirnov239 also merged a PR that added tests for numerical reproducibility of GermlineCNVCaller in cohort mode in #7889.; > 6. The earliest version of PyMC that supports Python 3.10+ is PyMC 4, released in 2022.; > 7. However, PyMC 4 introduces API changes, which will also require additional gCNV code changes and numerical testing.; > 8. These API changes are because the underlying computational backend for PyMC was updated from Theano (think of this as an old alternative to TensorFlow) to Aesara.; > 9. Since then, PyMC 5.9 has been released and the underlying backend has been updated again, from Aesara to PyTensor.; > 10. So if we are going to update the environment to support Python 3.10+, it probably makes sense to go all the way to PyMC 5.9. I've made some strides in this PR; as of [6b08f3a](https://github.com/broadinstitute/gatk/pull/8561/commits/6b08f3af205cb9af1f5c63a0786f9a5a52cd78c1), I've made enough updates to accommodate API changes so that cohort-mode inference for both GermlineCNVCaller and DetermineGermlineContigPloidy runs successfully under Python 3.10 and PyMC 5.9.0---although note that 5.9.1 has been released in the interim!. However, our work has just begun. Results now produced in the numerical tests mentioned above are quite far off from the original expected results. It remains to be seen whether this is due to the randomness of inference, some slight changes to the model prior that were necessitated by the API changes, or some bugs introduced in other code updates. (Also note that I believe Andrey's PR in item 4 already broke these tests, although the numerical differences were much smaller and more reasonable---but perhaps he can confirm. Also noting here that I think determinism is still currently broken as of this commit---there have been some changes to PyTensor/PyMC seeding so that our previous theano/PyMC3 hack no longer applies.). So I think the next step is to just go to scientific-level ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8561
https://github.com/broadinstitute/gatk/pull/8561:2054,Deployability,release,released,2054,"ion of PyMC that supports Python 3.10+ is PyMC 4, released in 2022.; > 7. However, PyMC 4 introduces API changes, which will also require additional gCNV code changes and numerical testing.; > 8. These API changes are because the underlying computational backend for PyMC was updated from Theano (think of this as an old alternative to TensorFlow) to Aesara.; > 9. Since then, PyMC 5.9 has been released and the underlying backend has been updated again, from Aesara to PyTensor.; > 10. So if we are going to update the environment to support Python 3.10+, it probably makes sense to go all the way to PyMC 5.9. I've made some strides in this PR; as of [6b08f3a](https://github.com/broadinstitute/gatk/pull/8561/commits/6b08f3af205cb9af1f5c63a0786f9a5a52cd78c1), I've made enough updates to accommodate API changes so that cohort-mode inference for both GermlineCNVCaller and DetermineGermlineContigPloidy runs successfully under Python 3.10 and PyMC 5.9.0---although note that 5.9.1 has been released in the interim!. However, our work has just begun. Results now produced in the numerical tests mentioned above are quite far off from the original expected results. It remains to be seen whether this is due to the randomness of inference, some slight changes to the model prior that were necessitated by the API changes, or some bugs introduced in other code updates. (Also note that I believe Andrey's PR in item 4 already broke these tests, although the numerical differences were much smaller and more reasonable---but perhaps he can confirm. Also noting here that I think determinism is still currently broken as of this commit---there have been some changes to PyTensor/PyMC seeding so that our previous theano/PyMC3 hack no longer applies.). So I think the next step is to just go to scientific-level testing and see what the fallout is. Ideally, we'd still get good performance (or perhaps better! at least on the runtime side, hopefully...) and we can just update the numerical tests. But i",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8561
https://github.com/broadinstitute/gatk/pull/8561:2422,Deployability,update,updates,2422,"PyMC was updated from Theano (think of this as an old alternative to TensorFlow) to Aesara.; > 9. Since then, PyMC 5.9 has been released and the underlying backend has been updated again, from Aesara to PyTensor.; > 10. So if we are going to update the environment to support Python 3.10+, it probably makes sense to go all the way to PyMC 5.9. I've made some strides in this PR; as of [6b08f3a](https://github.com/broadinstitute/gatk/pull/8561/commits/6b08f3af205cb9af1f5c63a0786f9a5a52cd78c1), I've made enough updates to accommodate API changes so that cohort-mode inference for both GermlineCNVCaller and DetermineGermlineContigPloidy runs successfully under Python 3.10 and PyMC 5.9.0---although note that 5.9.1 has been released in the interim!. However, our work has just begun. Results now produced in the numerical tests mentioned above are quite far off from the original expected results. It remains to be seen whether this is due to the randomness of inference, some slight changes to the model prior that were necessitated by the API changes, or some bugs introduced in other code updates. (Also note that I believe Andrey's PR in item 4 already broke these tests, although the numerical differences were much smaller and more reasonable---but perhaps he can confirm. Also noting here that I think determinism is still currently broken as of this commit---there have been some changes to PyTensor/PyMC seeding so that our previous theano/PyMC3 hack no longer applies.). So I think the next step is to just go to scientific-level testing and see what the fallout is. Ideally, we'd still get good performance (or perhaps better! at least on the runtime side, hopefully...) and we can just update the numerical tests. But if performance tanks, then we might need to see whether I've introduced any bugs. @mwalker174 @asmirnov239 perhaps you can comment on what might be the appropriate test suite here----1kGP?. I'll also highlight again that this PR will remove TensorFlow and might require",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8561
https://github.com/broadinstitute/gatk/pull/8561:3028,Deployability,update,update,3028,"ed and the underlying backend has been updated again, from Aesara to PyTensor.; > 10. So if we are going to update the environment to support Python 3.10+, it probably makes sense to go all the way to PyMC 5.9. I've made some strides in this PR; as of [6b08f3a](https://github.com/broadinstitute/gatk/pull/8561/commits/6b08f3af205cb9af1f5c63a0786f9a5a52cd78c1), I've made enough updates to accommodate API changes so that cohort-mode inference for both GermlineCNVCaller and DetermineGermlineContigPloidy runs successfully under Python 3.10 and PyMC 5.9.0---although note that 5.9.1 has been released in the interim!. However, our work has just begun. Results now produced in the numerical tests mentioned above are quite far off from the original expected results. It remains to be seen whether this is due to the randomness of inference, some slight changes to the model prior that were necessitated by the API changes, or some bugs introduced in other code updates. (Also note that I believe Andrey's PR in item 4 already broke these tests, although the numerical differences were much smaller and more reasonable---but perhaps he can confirm. Also noting here that I think determinism is still currently broken as of this commit---there have been some changes to PyTensor/PyMC seeding so that our previous theano/PyMC3 hack no longer applies.). So I think the next step is to just go to scientific-level testing and see what the fallout is. Ideally, we'd still get good performance (or perhaps better! at least on the runtime side, hopefully...) and we can just update the numerical tests. But if performance tanks, then we might need to see whether I've introduced any bugs. @mwalker174 @asmirnov239 perhaps you can comment on what might be the appropriate test suite here----1kGP?. I'll also highlight again that this PR will remove TensorFlow and might require that the corresponding CNN implementations be supported by an alternate strategy, at least until the PyTorch implementation goes in.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8561
https://github.com/broadinstitute/gatk/pull/8561:2936,Performance,perform,performance,2936,"ed and the underlying backend has been updated again, from Aesara to PyTensor.; > 10. So if we are going to update the environment to support Python 3.10+, it probably makes sense to go all the way to PyMC 5.9. I've made some strides in this PR; as of [6b08f3a](https://github.com/broadinstitute/gatk/pull/8561/commits/6b08f3af205cb9af1f5c63a0786f9a5a52cd78c1), I've made enough updates to accommodate API changes so that cohort-mode inference for both GermlineCNVCaller and DetermineGermlineContigPloidy runs successfully under Python 3.10 and PyMC 5.9.0---although note that 5.9.1 has been released in the interim!. However, our work has just begun. Results now produced in the numerical tests mentioned above are quite far off from the original expected results. It remains to be seen whether this is due to the randomness of inference, some slight changes to the model prior that were necessitated by the API changes, or some bugs introduced in other code updates. (Also note that I believe Andrey's PR in item 4 already broke these tests, although the numerical differences were much smaller and more reasonable---but perhaps he can confirm. Also noting here that I think determinism is still currently broken as of this commit---there have been some changes to PyTensor/PyMC seeding so that our previous theano/PyMC3 hack no longer applies.). So I think the next step is to just go to scientific-level testing and see what the fallout is. Ideally, we'd still get good performance (or perhaps better! at least on the runtime side, hopefully...) and we can just update the numerical tests. But if performance tanks, then we might need to see whether I've introduced any bugs. @mwalker174 @asmirnov239 perhaps you can comment on what might be the appropriate test suite here----1kGP?. I'll also highlight again that this PR will remove TensorFlow and might require that the corresponding CNN implementations be supported by an alternate strategy, at least until the PyTorch implementation goes in.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8561
https://github.com/broadinstitute/gatk/pull/8561:3063,Performance,perform,performance,3063,"ed and the underlying backend has been updated again, from Aesara to PyTensor.; > 10. So if we are going to update the environment to support Python 3.10+, it probably makes sense to go all the way to PyMC 5.9. I've made some strides in this PR; as of [6b08f3a](https://github.com/broadinstitute/gatk/pull/8561/commits/6b08f3af205cb9af1f5c63a0786f9a5a52cd78c1), I've made enough updates to accommodate API changes so that cohort-mode inference for both GermlineCNVCaller and DetermineGermlineContigPloidy runs successfully under Python 3.10 and PyMC 5.9.0---although note that 5.9.1 has been released in the interim!. However, our work has just begun. Results now produced in the numerical tests mentioned above are quite far off from the original expected results. It remains to be seen whether this is due to the randomness of inference, some slight changes to the model prior that were necessitated by the API changes, or some bugs introduced in other code updates. (Also note that I believe Andrey's PR in item 4 already broke these tests, although the numerical differences were much smaller and more reasonable---but perhaps he can confirm. Also noting here that I think determinism is still currently broken as of this commit---there have been some changes to PyTensor/PyMC seeding so that our previous theano/PyMC3 hack no longer applies.). So I think the next step is to just go to scientific-level testing and see what the fallout is. Ideally, we'd still get good performance (or perhaps better! at least on the runtime side, hopefully...) and we can just update the numerical tests. But if performance tanks, then we might need to see whether I've introduced any bugs. @mwalker174 @asmirnov239 perhaps you can comment on what might be the appropriate test suite here----1kGP?. I'll also highlight again that this PR will remove TensorFlow and might require that the corresponding CNN implementations be supported by an alternate strategy, at least until the PyTorch implementation goes in.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8561
https://github.com/broadinstitute/gatk/pull/8561:956,Testability,test,tests,956,"Copying over some discussion from Slack, with some slight modifications:. > I took a quick stab at updating the environment for gCNV. Even taking out TensorFlow (assuming that the CNN will not be supported by this environment), it's a difficult task:; > 1. The goal is to update Python from 3.6 to 3.10+, since Terra now requires the latter for officially supported images.; > 2. However, gCNV relies on the PyMC3 package. PyMC3 3.1 is currently used in GATK master. 3.1 was released in 2017, not long before our release of gCNV in 2018, but it's very old now.; > 3. The latest version of Python that is supported by PyMC3 3.1 in conda is Python 3.6.; > 4. @asmirnov239 has a draft PR (#8094) that updates PyMC3 to 3.5 and Python to 3.7, which clearly still falls short of Python 3.10+. This PR also updated some gCNV code to make it compatible with PyMC3 3.5. (It also removed TensorFlow and added PyTorch.); > 5. @asmirnov239 also merged a PR that added tests for numerical reproducibility of GermlineCNVCaller in cohort mode in #7889.; > 6. The earliest version of PyMC that supports Python 3.10+ is PyMC 4, released in 2022.; > 7. However, PyMC 4 introduces API changes, which will also require additional gCNV code changes and numerical testing.; > 8. These API changes are because the underlying computational backend for PyMC was updated from Theano (think of this as an old alternative to TensorFlow) to Aesara.; > 9. Since then, PyMC 5.9 has been released and the underlying backend has been updated again, from Aesara to PyTensor.; > 10. So if we are going to update the environment to support Python 3.10+, it probably makes sense to go all the way to PyMC 5.9. I've made some strides in this PR; as of [6b08f3a](https://github.com/broadinstitute/gatk/pull/8561/commits/6b08f3af205cb9af1f5c63a0786f9a5a52cd78c1), I've made enough updates to accommodate API changes so that cohort-mode inference for both GermlineCNVCaller and DetermineGermlineContigPloidy runs successfully under Python 3.1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8561
https://github.com/broadinstitute/gatk/pull/8561:1242,Testability,test,testing,1242," be supported by this environment), it's a difficult task:; > 1. The goal is to update Python from 3.6 to 3.10+, since Terra now requires the latter for officially supported images.; > 2. However, gCNV relies on the PyMC3 package. PyMC3 3.1 is currently used in GATK master. 3.1 was released in 2017, not long before our release of gCNV in 2018, but it's very old now.; > 3. The latest version of Python that is supported by PyMC3 3.1 in conda is Python 3.6.; > 4. @asmirnov239 has a draft PR (#8094) that updates PyMC3 to 3.5 and Python to 3.7, which clearly still falls short of Python 3.10+. This PR also updated some gCNV code to make it compatible with PyMC3 3.5. (It also removed TensorFlow and added PyTorch.); > 5. @asmirnov239 also merged a PR that added tests for numerical reproducibility of GermlineCNVCaller in cohort mode in #7889.; > 6. The earliest version of PyMC that supports Python 3.10+ is PyMC 4, released in 2022.; > 7. However, PyMC 4 introduces API changes, which will also require additional gCNV code changes and numerical testing.; > 8. These API changes are because the underlying computational backend for PyMC was updated from Theano (think of this as an old alternative to TensorFlow) to Aesara.; > 9. Since then, PyMC 5.9 has been released and the underlying backend has been updated again, from Aesara to PyTensor.; > 10. So if we are going to update the environment to support Python 3.10+, it probably makes sense to go all the way to PyMC 5.9. I've made some strides in this PR; as of [6b08f3a](https://github.com/broadinstitute/gatk/pull/8561/commits/6b08f3af205cb9af1f5c63a0786f9a5a52cd78c1), I've made enough updates to accommodate API changes so that cohort-mode inference for both GermlineCNVCaller and DetermineGermlineContigPloidy runs successfully under Python 3.10 and PyMC 5.9.0---although note that 5.9.1 has been released in the interim!. However, our work has just begun. Results now produced in the numerical tests mentioned above are quite far off ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8561
https://github.com/broadinstitute/gatk/pull/8561:2152,Testability,test,tests,2152,"ges, which will also require additional gCNV code changes and numerical testing.; > 8. These API changes are because the underlying computational backend for PyMC was updated from Theano (think of this as an old alternative to TensorFlow) to Aesara.; > 9. Since then, PyMC 5.9 has been released and the underlying backend has been updated again, from Aesara to PyTensor.; > 10. So if we are going to update the environment to support Python 3.10+, it probably makes sense to go all the way to PyMC 5.9. I've made some strides in this PR; as of [6b08f3a](https://github.com/broadinstitute/gatk/pull/8561/commits/6b08f3af205cb9af1f5c63a0786f9a5a52cd78c1), I've made enough updates to accommodate API changes so that cohort-mode inference for both GermlineCNVCaller and DetermineGermlineContigPloidy runs successfully under Python 3.10 and PyMC 5.9.0---although note that 5.9.1 has been released in the interim!. However, our work has just begun. Results now produced in the numerical tests mentioned above are quite far off from the original expected results. It remains to be seen whether this is due to the randomness of inference, some slight changes to the model prior that were necessitated by the API changes, or some bugs introduced in other code updates. (Also note that I believe Andrey's PR in item 4 already broke these tests, although the numerical differences were much smaller and more reasonable---but perhaps he can confirm. Also noting here that I think determinism is still currently broken as of this commit---there have been some changes to PyTensor/PyMC seeding so that our previous theano/PyMC3 hack no longer applies.). So I think the next step is to just go to scientific-level testing and see what the fallout is. Ideally, we'd still get good performance (or perhaps better! at least on the runtime side, hopefully...) and we can just update the numerical tests. But if performance tanks, then we might need to see whether I've introduced any bugs. @mwalker174 @asmirnov239 per",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8561
https://github.com/broadinstitute/gatk/pull/8561:2499,Testability,test,tests,2499,"ed and the underlying backend has been updated again, from Aesara to PyTensor.; > 10. So if we are going to update the environment to support Python 3.10+, it probably makes sense to go all the way to PyMC 5.9. I've made some strides in this PR; as of [6b08f3a](https://github.com/broadinstitute/gatk/pull/8561/commits/6b08f3af205cb9af1f5c63a0786f9a5a52cd78c1), I've made enough updates to accommodate API changes so that cohort-mode inference for both GermlineCNVCaller and DetermineGermlineContigPloidy runs successfully under Python 3.10 and PyMC 5.9.0---although note that 5.9.1 has been released in the interim!. However, our work has just begun. Results now produced in the numerical tests mentioned above are quite far off from the original expected results. It remains to be seen whether this is due to the randomness of inference, some slight changes to the model prior that were necessitated by the API changes, or some bugs introduced in other code updates. (Also note that I believe Andrey's PR in item 4 already broke these tests, although the numerical differences were much smaller and more reasonable---but perhaps he can confirm. Also noting here that I think determinism is still currently broken as of this commit---there have been some changes to PyTensor/PyMC seeding so that our previous theano/PyMC3 hack no longer applies.). So I think the next step is to just go to scientific-level testing and see what the fallout is. Ideally, we'd still get good performance (or perhaps better! at least on the runtime side, hopefully...) and we can just update the numerical tests. But if performance tanks, then we might need to see whether I've introduced any bugs. @mwalker174 @asmirnov239 perhaps you can comment on what might be the appropriate test suite here----1kGP?. I'll also highlight again that this PR will remove TensorFlow and might require that the corresponding CNN implementations be supported by an alternate strategy, at least until the PyTorch implementation goes in.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8561
https://github.com/broadinstitute/gatk/pull/8561:2870,Testability,test,testing,2870,"ed and the underlying backend has been updated again, from Aesara to PyTensor.; > 10. So if we are going to update the environment to support Python 3.10+, it probably makes sense to go all the way to PyMC 5.9. I've made some strides in this PR; as of [6b08f3a](https://github.com/broadinstitute/gatk/pull/8561/commits/6b08f3af205cb9af1f5c63a0786f9a5a52cd78c1), I've made enough updates to accommodate API changes so that cohort-mode inference for both GermlineCNVCaller and DetermineGermlineContigPloidy runs successfully under Python 3.10 and PyMC 5.9.0---although note that 5.9.1 has been released in the interim!. However, our work has just begun. Results now produced in the numerical tests mentioned above are quite far off from the original expected results. It remains to be seen whether this is due to the randomness of inference, some slight changes to the model prior that were necessitated by the API changes, or some bugs introduced in other code updates. (Also note that I believe Andrey's PR in item 4 already broke these tests, although the numerical differences were much smaller and more reasonable---but perhaps he can confirm. Also noting here that I think determinism is still currently broken as of this commit---there have been some changes to PyTensor/PyMC seeding so that our previous theano/PyMC3 hack no longer applies.). So I think the next step is to just go to scientific-level testing and see what the fallout is. Ideally, we'd still get good performance (or perhaps better! at least on the runtime side, hopefully...) and we can just update the numerical tests. But if performance tanks, then we might need to see whether I've introduced any bugs. @mwalker174 @asmirnov239 perhaps you can comment on what might be the appropriate test suite here----1kGP?. I'll also highlight again that this PR will remove TensorFlow and might require that the corresponding CNN implementations be supported by an alternate strategy, at least until the PyTorch implementation goes in.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8561
https://github.com/broadinstitute/gatk/pull/8561:3049,Testability,test,tests,3049,"ed and the underlying backend has been updated again, from Aesara to PyTensor.; > 10. So if we are going to update the environment to support Python 3.10+, it probably makes sense to go all the way to PyMC 5.9. I've made some strides in this PR; as of [6b08f3a](https://github.com/broadinstitute/gatk/pull/8561/commits/6b08f3af205cb9af1f5c63a0786f9a5a52cd78c1), I've made enough updates to accommodate API changes so that cohort-mode inference for both GermlineCNVCaller and DetermineGermlineContigPloidy runs successfully under Python 3.10 and PyMC 5.9.0---although note that 5.9.1 has been released in the interim!. However, our work has just begun. Results now produced in the numerical tests mentioned above are quite far off from the original expected results. It remains to be seen whether this is due to the randomness of inference, some slight changes to the model prior that were necessitated by the API changes, or some bugs introduced in other code updates. (Also note that I believe Andrey's PR in item 4 already broke these tests, although the numerical differences were much smaller and more reasonable---but perhaps he can confirm. Also noting here that I think determinism is still currently broken as of this commit---there have been some changes to PyTensor/PyMC seeding so that our previous theano/PyMC3 hack no longer applies.). So I think the next step is to just go to scientific-level testing and see what the fallout is. Ideally, we'd still get good performance (or perhaps better! at least on the runtime side, hopefully...) and we can just update the numerical tests. But if performance tanks, then we might need to see whether I've introduced any bugs. @mwalker174 @asmirnov239 perhaps you can comment on what might be the appropriate test suite here----1kGP?. I'll also highlight again that this PR will remove TensorFlow and might require that the corresponding CNN implementations be supported by an alternate strategy, at least until the PyTorch implementation goes in.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8561
https://github.com/broadinstitute/gatk/pull/8561:3224,Testability,test,test,3224,"ed and the underlying backend has been updated again, from Aesara to PyTensor.; > 10. So if we are going to update the environment to support Python 3.10+, it probably makes sense to go all the way to PyMC 5.9. I've made some strides in this PR; as of [6b08f3a](https://github.com/broadinstitute/gatk/pull/8561/commits/6b08f3af205cb9af1f5c63a0786f9a5a52cd78c1), I've made enough updates to accommodate API changes so that cohort-mode inference for both GermlineCNVCaller and DetermineGermlineContigPloidy runs successfully under Python 3.10 and PyMC 5.9.0---although note that 5.9.1 has been released in the interim!. However, our work has just begun. Results now produced in the numerical tests mentioned above are quite far off from the original expected results. It remains to be seen whether this is due to the randomness of inference, some slight changes to the model prior that were necessitated by the API changes, or some bugs introduced in other code updates. (Also note that I believe Andrey's PR in item 4 already broke these tests, although the numerical differences were much smaller and more reasonable---but perhaps he can confirm. Also noting here that I think determinism is still currently broken as of this commit---there have been some changes to PyTensor/PyMC seeding so that our previous theano/PyMC3 hack no longer applies.). So I think the next step is to just go to scientific-level testing and see what the fallout is. Ideally, we'd still get good performance (or perhaps better! at least on the runtime side, hopefully...) and we can just update the numerical tests. But if performance tanks, then we might need to see whether I've introduced any bugs. @mwalker174 @asmirnov239 perhaps you can comment on what might be the appropriate test suite here----1kGP?. I'll also highlight again that this PR will remove TensorFlow and might require that the corresponding CNN implementations be supported by an alternate strategy, at least until the PyTorch implementation goes in.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8561
https://github.com/broadinstitute/gatk/pull/8561:744,Usability,clear,clearly,744,"Copying over some discussion from Slack, with some slight modifications:. > I took a quick stab at updating the environment for gCNV. Even taking out TensorFlow (assuming that the CNN will not be supported by this environment), it's a difficult task:; > 1. The goal is to update Python from 3.6 to 3.10+, since Terra now requires the latter for officially supported images.; > 2. However, gCNV relies on the PyMC3 package. PyMC3 3.1 is currently used in GATK master. 3.1 was released in 2017, not long before our release of gCNV in 2018, but it's very old now.; > 3. The latest version of Python that is supported by PyMC3 3.1 in conda is Python 3.6.; > 4. @asmirnov239 has a draft PR (#8094) that updates PyMC3 to 3.5 and Python to 3.7, which clearly still falls short of Python 3.10+. This PR also updated some gCNV code to make it compatible with PyMC3 3.5. (It also removed TensorFlow and added PyTorch.); > 5. @asmirnov239 also merged a PR that added tests for numerical reproducibility of GermlineCNVCaller in cohort mode in #7889.; > 6. The earliest version of PyMC that supports Python 3.10+ is PyMC 4, released in 2022.; > 7. However, PyMC 4 introduces API changes, which will also require additional gCNV code changes and numerical testing.; > 8. These API changes are because the underlying computational backend for PyMC was updated from Theano (think of this as an old alternative to TensorFlow) to Aesara.; > 9. Since then, PyMC 5.9 has been released and the underlying backend has been updated again, from Aesara to PyTensor.; > 10. So if we are going to update the environment to support Python 3.10+, it probably makes sense to go all the way to PyMC 5.9. I've made some strides in this PR; as of [6b08f3a](https://github.com/broadinstitute/gatk/pull/8561/commits/6b08f3af205cb9af1f5c63a0786f9a5a52cd78c1), I've made enough updates to accommodate API changes so that cohort-mode inference for both GermlineCNVCaller and DetermineGermlineContigPloidy runs successfully under Python 3.1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8561
https://github.com/broadinstitute/gatk/pull/8563:8,Deployability,integrat,integration,8,"Creates integration testing datasets so the tables within them auto-delete at 2 weeks. See the tables in `gvs-internal.quickit_2023_10_24_vs_1049_tables_are_not_forever_265f343_beta` for an example, integration run [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/72e3c830-b4ba-4e52-a264-07acb81a9b5b).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8563
https://github.com/broadinstitute/gatk/pull/8563:199,Deployability,integrat,integration,199,"Creates integration testing datasets so the tables within them auto-delete at 2 weeks. See the tables in `gvs-internal.quickit_2023_10_24_vs_1049_tables_are_not_forever_265f343_beta` for an example, integration run [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/72e3c830-b4ba-4e52-a264-07acb81a9b5b).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8563
https://github.com/broadinstitute/gatk/pull/8563:8,Integrability,integrat,integration,8,"Creates integration testing datasets so the tables within them auto-delete at 2 weeks. See the tables in `gvs-internal.quickit_2023_10_24_vs_1049_tables_are_not_forever_265f343_beta` for an example, integration run [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/72e3c830-b4ba-4e52-a264-07acb81a9b5b).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8563
https://github.com/broadinstitute/gatk/pull/8563:199,Integrability,integrat,integration,199,"Creates integration testing datasets so the tables within them auto-delete at 2 weeks. See the tables in `gvs-internal.quickit_2023_10_24_vs_1049_tables_are_not_forever_265f343_beta` for an example, integration run [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/72e3c830-b4ba-4e52-a264-07acb81a9b5b).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8563
https://github.com/broadinstitute/gatk/pull/8563:20,Testability,test,testing,20,"Creates integration testing datasets so the tables within them auto-delete at 2 weeks. See the tables in `gvs-internal.quickit_2023_10_24_vs_1049_tables_are_not_forever_265f343_beta` for an example, integration run [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/72e3c830-b4ba-4e52-a264-07acb81a9b5b).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8563
https://github.com/broadinstitute/gatk/pull/8566:8,Deployability,Integrat,Integration,8,Passing Integration test (using built-in dockers) [here](https://job-manager.dsde-prod.broadinstitute.org/jobs/d2063e26-a22e-412e-ad91-aa2b14fbb7ec).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8566
https://github.com/broadinstitute/gatk/pull/8566:8,Integrability,Integrat,Integration,8,Passing Integration test (using built-in dockers) [here](https://job-manager.dsde-prod.broadinstitute.org/jobs/d2063e26-a22e-412e-ad91-aa2b14fbb7ec).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8566
https://github.com/broadinstitute/gatk/pull/8566:20,Testability,test,test,20,Passing Integration test (using built-in dockers) [here](https://job-manager.dsde-prod.broadinstitute.org/jobs/d2063e26-a22e-412e-ad91-aa2b14fbb7ec).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8566
https://github.com/broadinstitute/gatk/pull/8567:82,Availability,error,error,82,A non-minimally represented allele in dbsnp was causing GenotypeGVCFs to throw an error. This PR fixes this and removes a TODO.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8567
https://github.com/broadinstitute/gatk/issues/8568:647,Availability,down,down,647,"## Feature request. ### Tool(s) or class(es) involved; `SelectVariants`. ### Description; In order to run SelectVariants with VCF inputs that are in separate locations from their index files or to stream SelectVariants using https from Azure blob storage, we need a way to provide the index file in a separate argument from the `-V` input. @jamesemery started thinking this through (copying this from slack):. > In `featureDataSource.getTribbleFeatureReader()` we currently initialize the datasources in `getFeatureReader()` which gets called by `VariantWalker.initializeDrivingVariants()` . You could stick an override into that where you thread down the path for the index source through that path and optionally (only if the index is explicitly supplied by the user) push it down into the `getTribbleFeatureReader()` calls at the bottom of the stack there. @droazen any thoughts on this? @VJalili Would adding this feature to `SelectVariants` be useful for your pipelines at all?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8568
https://github.com/broadinstitute/gatk/issues/8568:778,Availability,down,down,778,"## Feature request. ### Tool(s) or class(es) involved; `SelectVariants`. ### Description; In order to run SelectVariants with VCF inputs that are in separate locations from their index files or to stream SelectVariants using https from Azure blob storage, we need a way to provide the index file in a separate argument from the `-V` input. @jamesemery started thinking this through (copying this from slack):. > In `featureDataSource.getTribbleFeatureReader()` we currently initialize the datasources in `getFeatureReader()` which gets called by `VariantWalker.initializeDrivingVariants()` . You could stick an override into that where you thread down the path for the index source through that path and optionally (only if the index is explicitly supplied by the user) push it down into the `getTribbleFeatureReader()` calls at the bottom of the stack there. @droazen any thoughts on this? @VJalili Would adding this feature to `SelectVariants` be useful for your pipelines at all?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8568
https://github.com/broadinstitute/gatk/issues/8568:965,Deployability,pipeline,pipelines,965,"## Feature request. ### Tool(s) or class(es) involved; `SelectVariants`. ### Description; In order to run SelectVariants with VCF inputs that are in separate locations from their index files or to stream SelectVariants using https from Azure blob storage, we need a way to provide the index file in a separate argument from the `-V` input. @jamesemery started thinking this through (copying this from slack):. > In `featureDataSource.getTribbleFeatureReader()` we currently initialize the datasources in `getFeatureReader()` which gets called by `VariantWalker.initializeDrivingVariants()` . You could stick an override into that where you thread down the path for the index source through that path and optionally (only if the index is explicitly supplied by the user) push it down into the `getTribbleFeatureReader()` calls at the bottom of the stack there. @droazen any thoughts on this? @VJalili Would adding this feature to `SelectVariants` be useful for your pipelines at all?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8568
https://github.com/broadinstitute/gatk/issues/8569:3904,Availability,down,down,3904,"lizing engine; 14:10:28.532 INFO FeatureManager - Using codec VCFCodec to read file file:///omics/groups/OE0540/internal/users/gleixner/cropseq_uli/rep_ex/test3/output.g.vcf; 14:10:28.566 INFO GenotypeGVCFs - Done initializing engine; 14:10:28.620 INFO ProgressMeter - Starting traversal; 14:10:28.622 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 14:10:29.048 WARN ReferenceConfidenceVariantContextMerger - Detected invalid annotations: When trying to merge variant contexts at location chr19:55910646 the annotation MLEAC=[1, 0] was not a numerical value and was ignored; 14:10:29.118 WARN InbreedingCoeff - InbreedingCoeff will not be calculated at position chr19:55910646 and possibly subsequent; at least 10 samples must have called genotypes; 14:10:29.175 INFO ProgressMeter - unmapped 0.0 37 4021.7; 14:10:29.175 INFO ProgressMeter - Traversal complete. Processed 37 total variants in 0.0 minutes.; 14:10:29.236 INFO GenotypeGVCFs - Shutting down engine; [October 29, 2023 at 2:10:29 PM CET] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 0.02 minutes.; Runtime.totalMemory()=285212672; ```; results in a vcf file that still has called genotypes:. ```; $ bcftools view output.vcf -c1 | bcftools annotate -x INFO,FORMAT/SB | tail; ##source=HaplotypeCaller; ##bcftools_viewVersion=1.16+htslib-1.16; ##bcftools_viewCommand=view -c1 output.vcf; Date=Sun Oct 29 14:09:42 2023; ##bcftools_annotateVersion=1.16+htslib-1.16; ##bcftools_annotateCommand=annotate -x INFO,FORMAT/SB; Date=Sun Oct 29 14:09:42 2023; #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT CGAAGAGGTAGGTGCGAG-1; chr19 55910646 . AC A 352.6 . . GT:AD:DP:GQ:PGT:PID:PL:PS 0|1:6,9:15:99:0|1:55910646_AC_A:360,0,225:55910646; chr19 55910648 . AAATCCCCC A 352.6 . . GT:AD:DP:GQ:PGT:PID:PL:PS 0|1:6,9:15:99:0|1:55910646_AC_A:360,0,225:55910646; chr19 55910653 . CCCCAT *,C 227.84 . . GT:AD:DP:GQ:PGT:PID:PL:PS 1|2:0,9,6:15:99:1|0:55910646_AC_A:630,252,225,378,0,360:55",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8569
https://github.com/broadinstitute/gatk/issues/8569:1166,Performance,Load,Loading,1166,"74aa3480b67b321dc66426b1c600a/src/main/java/org/broadinstitute/hellbender/tools/walkers/GenotypeGVCFsEngine.java#L377. and the `--genotype-assignment-method` thus has no effect:; for example (same data as in [#5727#issuecomment-1781017195](https://github.com/broadinstitute/gatk/issues/5727#issuecomment-1781017195)) :; ```; $ gatk GenotypeGVCFs -R chr19.fa -V output.g.vcf -O output.vcf --genotype-assignment-method SET_TO_NO_CALL; Using GATK jar /omics/groups/OE0540/internal/software/jvm/gatk/4.4.0.0/gatk-package-4.4.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /omics/groups/OE0540/internal/software/jvm/gatk/4.4.0.0/gatk-package-4.4.0.0-local.jar GenotypeGVCFs -R chr19.fa -V output.g.vcf -O output.vcf --genotype-assignment-method SET_TO_NO_CALL; Picked up JAVA_TOOL_OPTIONS: -Djava.net.useSystemProxies=true; 14:10:28.241 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/omics/groups/OE0540/internal/software/jvm/gatk/4.4.0.0/gatk-package-4.4.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 14:10:28.300 INFO GenotypeGVCFs - ------------------------------------------------------------; 14:10:28.305 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.4.0.0; 14:10:28.305 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:10:28.305 INFO GenotypeGVCFs - Executing as gleixner@odcf-worker02 on Linux v3.10.0-1160.76.1.el7.x86_64 amd64; 14:10:28.305 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v17+35-2724; 14:10:28.306 INFO GenotypeGVCFs - Start Date/Time: October 29, 2023 at 2:10:28 PM CET; 14:10:28.306 INFO GenotypeGVCFs - ------------------------------------------------------------; 14:10:28.306 INFO GenotypeGVCFs - ------------------------------------------------------------; 14:10:28.307 INFO GenotypeGVCFs - HTSJDK Version: 3.0.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8569
https://github.com/broadinstitute/gatk/issues/8569:3363,Safety,Detect,Detected,3363,"_READ_FOR_SAMTOOLS : false; 14:10:28.309 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 14:10:28.309 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 14:10:28.310 INFO GenotypeGVCFs - Deflater: IntelDeflater; 14:10:28.310 INFO GenotypeGVCFs - Inflater: IntelInflater; 14:10:28.310 INFO GenotypeGVCFs - GCS max retries/reopens: 20; 14:10:28.310 INFO GenotypeGVCFs - Requester pays: disabled; 14:10:28.311 INFO GenotypeGVCFs - Initializing engine; 14:10:28.532 INFO FeatureManager - Using codec VCFCodec to read file file:///omics/groups/OE0540/internal/users/gleixner/cropseq_uli/rep_ex/test3/output.g.vcf; 14:10:28.566 INFO GenotypeGVCFs - Done initializing engine; 14:10:28.620 INFO ProgressMeter - Starting traversal; 14:10:28.622 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 14:10:29.048 WARN ReferenceConfidenceVariantContextMerger - Detected invalid annotations: When trying to merge variant contexts at location chr19:55910646 the annotation MLEAC=[1, 0] was not a numerical value and was ignored; 14:10:29.118 WARN InbreedingCoeff - InbreedingCoeff will not be calculated at position chr19:55910646 and possibly subsequent; at least 10 samples must have called genotypes; 14:10:29.175 INFO ProgressMeter - unmapped 0.0 37 4021.7; 14:10:29.175 INFO ProgressMeter - Traversal complete. Processed 37 total variants in 0.0 minutes.; 14:10:29.236 INFO GenotypeGVCFs - Shutting down engine; [October 29, 2023 at 2:10:29 PM CET] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 0.02 minutes.; Runtime.totalMemory()=285212672; ```; results in a vcf file that still has called genotypes:. ```; $ bcftools view output.vcf -c1 | bcftools annotate -x INFO,FORMAT/SB | tail; ##source=HaplotypeCaller; ##bcftools_viewVersion=1.16+htslib-1.16; ##bcftools_viewCommand=view -c1 output.vcf; Date=Sun Oct 29 14:09:42 2023; ##bcftools_annotateVersion=1.16+htslib-1.16; ##bcftools_anno",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8569
https://github.com/broadinstitute/gatk/pull/8571:0,Deployability,Update,Update,0,Update AoU Documentation for GvsImportGenomes to reflect new behavior,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8571
https://github.com/broadinstitute/gatk/pull/8573:186,Availability,failure,failure,186,"VS-1092 - Fix for GvsCreateFilterSet OOD; Integration test ran [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/f2dc8eca-1fc2-4ab3-ac38-ab430fb1d60a).; One failure - in the Exome test on AssertCostIsTrackedAndExpected. Doesn't seem related to this code change, so am going to allow it.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8573
https://github.com/broadinstitute/gatk/pull/8573:42,Deployability,Integrat,Integration,42,"VS-1092 - Fix for GvsCreateFilterSet OOD; Integration test ran [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/f2dc8eca-1fc2-4ab3-ac38-ab430fb1d60a).; One failure - in the Exome test on AssertCostIsTrackedAndExpected. Doesn't seem related to this code change, so am going to allow it.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8573
https://github.com/broadinstitute/gatk/pull/8573:42,Integrability,Integrat,Integration,42,"VS-1092 - Fix for GvsCreateFilterSet OOD; Integration test ran [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/f2dc8eca-1fc2-4ab3-ac38-ab430fb1d60a).; One failure - in the Exome test on AssertCostIsTrackedAndExpected. Doesn't seem related to this code change, so am going to allow it.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8573
https://github.com/broadinstitute/gatk/pull/8573:54,Testability,test,test,54,"VS-1092 - Fix for GvsCreateFilterSet OOD; Integration test ran [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/f2dc8eca-1fc2-4ab3-ac38-ab430fb1d60a).; One failure - in the Exome test on AssertCostIsTrackedAndExpected. Doesn't seem related to this code change, so am going to allow it.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8573
https://github.com/broadinstitute/gatk/pull/8573:209,Testability,test,test,209,"VS-1092 - Fix for GvsCreateFilterSet OOD; Integration test ran [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/f2dc8eca-1fc2-4ab3-ac38-ab430fb1d60a).; One failure - in the Exome test on AssertCostIsTrackedAndExpected. Doesn't seem related to this code change, so am going to allow it.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8573
https://github.com/broadinstitute/gatk/pull/8573:217,Testability,Assert,AssertCostIsTrackedAndExpected,217,"VS-1092 - Fix for GvsCreateFilterSet OOD; Integration test ran [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/f2dc8eca-1fc2-4ab3-ac38-ab430fb1d60a).; One failure - in the Exome test on AssertCostIsTrackedAndExpected. Doesn't seem related to this code change, so am going to allow it.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8573
https://github.com/broadinstitute/gatk/issues/8574:85,Availability,error,error,85,"### Instructions. gatk version 4.4.0.0. When I run gatk GenotypeGVCFs, it shows this error：; ; A USER ERROR has occurred: Bad input: Presence of '-RAW_MQ' annotation is detected. This GATK version expects key RAW_MQandDP with a tuple of sum of squared MQ values and total reads over variant genotypes as the value. This could indicate that the provided input was produced with an older version of GATK. Use the argument '--allow-old-rms-mapping-quality-annotation-data' to override and attempt the deprecated MQ calculation. There may be differences in how newer GATK versions calculate DP and MQ that may result in worse MQ results. Use at your own risk. ----. ##; I use gatk 4.2 and gatk 4.4 to run gatk HaplotypeCaller,respectively. And then use gatk CombineGVCFs (4.4) to combine all ""gvcf.gz"" files. . Please tell me how to solve the above problem.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8574
https://github.com/broadinstitute/gatk/issues/8574:102,Availability,ERROR,ERROR,102,"### Instructions. gatk version 4.4.0.0. When I run gatk GenotypeGVCFs, it shows this error：; ; A USER ERROR has occurred: Bad input: Presence of '-RAW_MQ' annotation is detected. This GATK version expects key RAW_MQandDP with a tuple of sum of squared MQ values and total reads over variant genotypes as the value. This could indicate that the provided input was produced with an older version of GATK. Use the argument '--allow-old-rms-mapping-quality-annotation-data' to override and attempt the deprecated MQ calculation. There may be differences in how newer GATK versions calculate DP and MQ that may result in worse MQ results. Use at your own risk. ----. ##; I use gatk 4.2 and gatk 4.4 to run gatk HaplotypeCaller,respectively. And then use gatk CombineGVCFs (4.4) to combine all ""gvcf.gz"" files. . Please tell me how to solve the above problem.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8574
https://github.com/broadinstitute/gatk/issues/8574:169,Safety,detect,detected,169,"### Instructions. gatk version 4.4.0.0. When I run gatk GenotypeGVCFs, it shows this error：; ; A USER ERROR has occurred: Bad input: Presence of '-RAW_MQ' annotation is detected. This GATK version expects key RAW_MQandDP with a tuple of sum of squared MQ values and total reads over variant genotypes as the value. This could indicate that the provided input was produced with an older version of GATK. Use the argument '--allow-old-rms-mapping-quality-annotation-data' to override and attempt the deprecated MQ calculation. There may be differences in how newer GATK versions calculate DP and MQ that may result in worse MQ results. Use at your own risk. ----. ##; I use gatk 4.2 and gatk 4.4 to run gatk HaplotypeCaller,respectively. And then use gatk CombineGVCFs (4.4) to combine all ""gvcf.gz"" files. . Please tell me how to solve the above problem.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8574
https://github.com/broadinstitute/gatk/issues/8574:650,Safety,risk,risk,650,"### Instructions. gatk version 4.4.0.0. When I run gatk GenotypeGVCFs, it shows this error：; ; A USER ERROR has occurred: Bad input: Presence of '-RAW_MQ' annotation is detected. This GATK version expects key RAW_MQandDP with a tuple of sum of squared MQ values and total reads over variant genotypes as the value. This could indicate that the provided input was produced with an older version of GATK. Use the argument '--allow-old-rms-mapping-quality-annotation-data' to override and attempt the deprecated MQ calculation. There may be differences in how newer GATK versions calculate DP and MQ that may result in worse MQ results. Use at your own risk. ----. ##; I use gatk 4.2 and gatk 4.4 to run gatk HaplotypeCaller,respectively. And then use gatk CombineGVCFs (4.4) to combine all ""gvcf.gz"" files. . Please tell me how to solve the above problem.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8574
https://github.com/broadinstitute/gatk/pull/8575:11,Availability,error,error,11,Heap Space error in PopulateFilterSetInfo. Passing Integration Test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/50d867c5-3f6f-405b-8e7b-a714ab7e806f).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8575
https://github.com/broadinstitute/gatk/pull/8575:51,Deployability,Integrat,Integration,51,Heap Space error in PopulateFilterSetInfo. Passing Integration Test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/50d867c5-3f6f-405b-8e7b-a714ab7e806f).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8575
https://github.com/broadinstitute/gatk/pull/8575:51,Integrability,Integrat,Integration,51,Heap Space error in PopulateFilterSetInfo. Passing Integration Test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/50d867c5-3f6f-405b-8e7b-a714ab7e806f).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8575
https://github.com/broadinstitute/gatk/pull/8575:63,Testability,Test,Test,63,Heap Space error in PopulateFilterSetInfo. Passing Integration Test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/50d867c5-3f6f-405b-8e7b-a714ab7e806f).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8575
https://github.com/broadinstitute/gatk/pull/8576:169,Energy Efficiency,allocate,allocate,169,"@ilyasoifer I noticed this method wasn't doing anything, it looks like it should have a return here. . Also, I made the logger static because it's probably expensive to allocate one for every read and you don't need to.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8576
https://github.com/broadinstitute/gatk/pull/8576:120,Testability,log,logger,120,"@ilyasoifer I noticed this method wasn't doing anything, it looks like it should have a return here. . Also, I made the logger static because it's probably expensive to allocate one for every read and you don't need to.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8576
https://github.com/broadinstitute/gatk/pull/8579:24,Deployability,Update,Update,24,New: GroundTruthScorer; Update: FlowFeatureMapper,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8579
https://github.com/broadinstitute/gatk/pull/8581:159,Deployability,Integrat,Integration,159,"<img width=""745"" alt=""Screenshot 2023-11-14 at 12 44 24 AM"" src=""https://github.com/broadinstitute/gatk/assets/6863459/8390c9a8-f343-4c9b-9636-15b4dfc5aefa"">. Integration run:; https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/3aa1545f-7f11-4919-9ecf-8b3a5900441a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8581
https://github.com/broadinstitute/gatk/pull/8581:159,Integrability,Integrat,Integration,159,"<img width=""745"" alt=""Screenshot 2023-11-14 at 12 44 24 AM"" src=""https://github.com/broadinstitute/gatk/assets/6863459/8390c9a8-f343-4c9b-9636-15b4dfc5aefa"">. Integration run:; https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/3aa1545f-7f11-4919-9ecf-8b3a5900441a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8581
https://github.com/broadinstitute/gatk/pull/8586:0,Deployability,Integrat,Integration,0,"Integration run in progress: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/b0ba98ae-a81f-4be2-bd72-c374359b644c. I actually don't expect this to pass due to VS-1141, but hopefully once this lands VS-1141 will be the *only* thing preventing integration from passing. Stowaway bug fixes:. - Hail version defaulted properly; - VDS tie out now waits for VDS creation to finish",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8586
https://github.com/broadinstitute/gatk/pull/8586:266,Deployability,integrat,integration,266,"Integration run in progress: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/b0ba98ae-a81f-4be2-bd72-c374359b644c. I actually don't expect this to pass due to VS-1141, but hopefully once this lands VS-1141 will be the *only* thing preventing integration from passing. Stowaway bug fixes:. - Hail version defaulted properly; - VDS tie out now waits for VDS creation to finish",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8586
https://github.com/broadinstitute/gatk/pull/8586:0,Integrability,Integrat,Integration,0,"Integration run in progress: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/b0ba98ae-a81f-4be2-bd72-c374359b644c. I actually don't expect this to pass due to VS-1141, but hopefully once this lands VS-1141 will be the *only* thing preventing integration from passing. Stowaway bug fixes:. - Hail version defaulted properly; - VDS tie out now waits for VDS creation to finish",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8586
https://github.com/broadinstitute/gatk/pull/8586:266,Integrability,integrat,integration,266,"Integration run in progress: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/b0ba98ae-a81f-4be2-bd72-c374359b644c. I actually don't expect this to pass due to VS-1141, but hopefully once this lands VS-1141 will be the *only* thing preventing integration from passing. Stowaway bug fixes:. - Hail version defaulted properly; - VDS tie out now waits for VDS creation to finish",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8586
https://github.com/broadinstitute/gatk/issues/8587:1891,Availability,ERROR,ERROR,1891,"1209008209559916471805773_0002_m_000000_3: Committed. Elapsed time: 3 ms.; 23/11/16 12:09:10 INFO Executor: Finished task 0.0 in stage 2.0 (TID 3). 4103 bytes result sent to driver; 23/11/16 12:09:10 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 3) in 10327 ms on SRINIVASiNDRARAVI (executor driver) (2/2); 23/11/16 12:09:10 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool ; 23/11/16 12:09:10 INFO DAGScheduler: ResultStage 2 (parquet at StudentAws.scala:36) finished in 10.361 s; 23/11/16 12:09:10 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job; 23/11/16 12:09:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished; 23/11/16 12:09:10 INFO DAGScheduler: Job 2 finished: parquet at StudentAws.scala:36, took 10.369237 s; 23/11/16 12:09:10 INFO FileFormatWriter: Start to commit write Job b17a4b92-9ee1-46cc-858a-08ed0b22fb8b.; 23/11/16 12:09:10 ERROR FileFormatWriter: Aborting job b17a4b92-9ee1-46cc-858a-08ed0b22fb8b.; java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z; 	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method); 	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793); 	at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249); 	at org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1454); 	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601); 	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972); 	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014); 	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761); 	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972); 	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014); 	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:33",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8587
https://github.com/broadinstitute/gatk/issues/8587:7349,Energy Efficiency,adapt,adapted,7349,"on.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81); 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79); 	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133); 	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856); 	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387); 	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360); 	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239); 	at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:789); 	at StudentAws$.delayedEndpoint$StudentAws$1(StudentAws.scala:36); 	at StudentAws$delayedInit$body.apply(StudentAws.scala:8); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at StudentAws$.main(StudentAws.scala:8); 	at StudentAws.main(StudentAws.scala); Exception in thread ""main"" java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z; 	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method); 	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793); 	at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249); 	at org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1454); 	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601); 	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972); 	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014); 	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761); 	at org.apache.hadoop.fs.FileSystem.listS",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8587
https://github.com/broadinstitute/gatk/issues/8587:12992,Energy Efficiency,adapt,adapted,12992,on.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81); 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79); 	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133); 	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856); 	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387); 	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360); 	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239); 	at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:789); 	at StudentAws$.delayedEndpoint$StudentAws$1(StudentAws.scala:36); 	at StudentAws$delayedInit$body.apply(StudentAws.scala:8); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at StudentAws$.main(StudentAws.scala:8); 	at StudentAws.main(StudentAws.scala); 23/11/16 12:09:10 INFO SparkContext: Invoking stop() from shutdown hook; 23/11/16 12:09:10 INFO SparkContext: SparkContext is stopping with exitCode 0.; 23/11/16 12:09:10 INFO SparkUI: Stopped Spark web UI at http://SRINIVASiNDRARAVI:4040; 23/11/16 12:09:10 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 23/11/16 12:09:10 INFO MemoryStore: MemoryStore cleared; 23/11/16 12:09:10 INFO BlockManager: BlockManager stopped; 23/11/16 12:09:10 INFO BlockManagerMaster: BlockManagerMaster stopped; 23/11/16 12:09:10 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 23/11/16 12:09:10 INFO SparkContext: Successfully stopped SparkContext; 23/11/16 12:09:10 INFO ShutdownHookManager: Shutdown ho,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8587
https://github.com/broadinstitute/gatk/issues/8587:7349,Modifiability,adapt,adapted,7349,"on.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81); 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79); 	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133); 	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856); 	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387); 	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360); 	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239); 	at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:789); 	at StudentAws$.delayedEndpoint$StudentAws$1(StudentAws.scala:36); 	at StudentAws$delayedInit$body.apply(StudentAws.scala:8); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at StudentAws$.main(StudentAws.scala:8); 	at StudentAws.main(StudentAws.scala); Exception in thread ""main"" java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z; 	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method); 	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793); 	at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249); 	at org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1454); 	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601); 	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972); 	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014); 	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761); 	at org.apache.hadoop.fs.FileSystem.listS",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8587
https://github.com/broadinstitute/gatk/issues/8587:12992,Modifiability,adapt,adapted,12992,on.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81); 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79); 	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133); 	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856); 	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387); 	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360); 	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239); 	at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:789); 	at StudentAws$.delayedEndpoint$StudentAws$1(StudentAws.scala:36); 	at StudentAws$delayedInit$body.apply(StudentAws.scala:8); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at StudentAws$.main(StudentAws.scala:8); 	at StudentAws.main(StudentAws.scala); 23/11/16 12:09:10 INFO SparkContext: Invoking stop() from shutdown hook; 23/11/16 12:09:10 INFO SparkContext: SparkContext is stopping with exitCode 0.; 23/11/16 12:09:10 INFO SparkUI: Stopped Spark web UI at http://SRINIVASiNDRARAVI:4040; 23/11/16 12:09:10 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 23/11/16 12:09:10 INFO MemoryStore: MemoryStore cleared; 23/11/16 12:09:10 INFO BlockManager: BlockManager stopped; 23/11/16 12:09:10 INFO BlockManagerMaster: BlockManagerMaster stopped; 23/11/16 12:09:10 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 23/11/16 12:09:10 INFO SparkContext: Successfully stopped SparkContext; 23/11/16 12:09:10 INFO ShutdownHookManager: Shutdown ho,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8587
https://github.com/broadinstitute/gatk/issues/8587:1915,Safety,Abort,Aborting,1915,"1209008209559916471805773_0002_m_000000_3: Committed. Elapsed time: 3 ms.; 23/11/16 12:09:10 INFO Executor: Finished task 0.0 in stage 2.0 (TID 3). 4103 bytes result sent to driver; 23/11/16 12:09:10 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 3) in 10327 ms on SRINIVASiNDRARAVI (executor driver) (2/2); 23/11/16 12:09:10 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool ; 23/11/16 12:09:10 INFO DAGScheduler: ResultStage 2 (parquet at StudentAws.scala:36) finished in 10.361 s; 23/11/16 12:09:10 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job; 23/11/16 12:09:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished; 23/11/16 12:09:10 INFO DAGScheduler: Job 2 finished: parquet at StudentAws.scala:36, took 10.369237 s; 23/11/16 12:09:10 INFO FileFormatWriter: Start to commit write Job b17a4b92-9ee1-46cc-858a-08ed0b22fb8b.; 23/11/16 12:09:10 ERROR FileFormatWriter: Aborting job b17a4b92-9ee1-46cc-858a-08ed0b22fb8b.; java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z; 	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method); 	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793); 	at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249); 	at org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1454); 	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601); 	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972); 	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014); 	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761); 	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972); 	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014); 	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:33",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8587
https://github.com/broadinstitute/gatk/issues/8587:2203,Security,access,access,2203,"(executor driver) (2/2); 23/11/16 12:09:10 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool ; 23/11/16 12:09:10 INFO DAGScheduler: ResultStage 2 (parquet at StudentAws.scala:36) finished in 10.361 s; 23/11/16 12:09:10 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job; 23/11/16 12:09:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished; 23/11/16 12:09:10 INFO DAGScheduler: Job 2 finished: parquet at StudentAws.scala:36, took 10.369237 s; 23/11/16 12:09:10 INFO FileFormatWriter: Start to commit write Job b17a4b92-9ee1-46cc-858a-08ed0b22fb8b.; 23/11/16 12:09:10 ERROR FileFormatWriter: Aborting job b17a4b92-9ee1-46cc-858a-08ed0b22fb8b.; java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z; 	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method); 	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793); 	at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249); 	at org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1454); 	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601); 	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972); 	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014); 	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761); 	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972); 	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014); 	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334); 	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404); 	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377); 	at org.apache.parquet.hadoop.ParquetOutputCommitter.commitJob(ParquetOut",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8587
https://github.com/broadinstitute/gatk/issues/8587:2603,Security,Checksum,ChecksumFileSystem,2603,"lling all running tasks in stage 2: Stage finished; 23/11/16 12:09:10 INFO DAGScheduler: Job 2 finished: parquet at StudentAws.scala:36, took 10.369237 s; 23/11/16 12:09:10 INFO FileFormatWriter: Start to commit write Job b17a4b92-9ee1-46cc-858a-08ed0b22fb8b.; 23/11/16 12:09:10 ERROR FileFormatWriter: Aborting job b17a4b92-9ee1-46cc-858a-08ed0b22fb8b.; java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z; 	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method); 	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793); 	at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249); 	at org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1454); 	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601); 	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972); 	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014); 	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761); 	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972); 	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014); 	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334); 	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404); 	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377); 	at org.apache.parquet.hadoop.ParquetOutputCommitter.commitJob(ParquetOutputCommitter.java:48); 	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192); 	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640); 	at o",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8587
https://github.com/broadinstitute/gatk/issues/8587:2633,Security,Checksum,ChecksumFileSystem,2633,"n stage 2: Stage finished; 23/11/16 12:09:10 INFO DAGScheduler: Job 2 finished: parquet at StudentAws.scala:36, took 10.369237 s; 23/11/16 12:09:10 INFO FileFormatWriter: Start to commit write Job b17a4b92-9ee1-46cc-858a-08ed0b22fb8b.; 23/11/16 12:09:10 ERROR FileFormatWriter: Aborting job b17a4b92-9ee1-46cc-858a-08ed0b22fb8b.; java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z; 	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method); 	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793); 	at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249); 	at org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1454); 	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601); 	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972); 	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014); 	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761); 	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972); 	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014); 	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334); 	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404); 	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377); 	at org.apache.parquet.hadoop.ParquetOutputCommitter.commitJob(ParquetOutputCommitter.java:48); 	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192); 	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640); 	at org.apache.spark.sql.exec",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8587
https://github.com/broadinstitute/gatk/issues/8587:7846,Security,access,access,7846," 	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239); 	at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:789); 	at StudentAws$.delayedEndpoint$StudentAws$1(StudentAws.scala:36); 	at StudentAws$delayedInit$body.apply(StudentAws.scala:8); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at StudentAws$.main(StudentAws.scala:8); 	at StudentAws.main(StudentAws.scala); Exception in thread ""main"" java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z; 	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method); 	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793); 	at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249); 	at org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1454); 	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601); 	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972); 	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014); 	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761); 	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972); 	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014); 	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334); 	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404); 	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377); 	at org.apache.parquet.hadoop.ParquetOutputCommitter.commitJob(ParquetOut",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8587
https://github.com/broadinstitute/gatk/issues/8587:8246,Security,Checksum,ChecksumFileSystem,8246,"ime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at StudentAws$.main(StudentAws.scala:8); 	at StudentAws.main(StudentAws.scala); Exception in thread ""main"" java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z; 	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method); 	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793); 	at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249); 	at org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1454); 	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601); 	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972); 	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014); 	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761); 	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972); 	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014); 	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334); 	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404); 	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377); 	at org.apache.parquet.hadoop.ParquetOutputCommitter.commitJob(ParquetOutputCommitter.java:48); 	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192); 	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640); 	at o",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8587
https://github.com/broadinstitute/gatk/issues/8587:8276,Security,Checksum,ChecksumFileSystem,8276,"ly$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at StudentAws$.main(StudentAws.scala:8); 	at StudentAws.main(StudentAws.scala); Exception in thread ""main"" java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z; 	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method); 	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793); 	at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249); 	at org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1454); 	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601); 	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972); 	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014); 	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761); 	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972); 	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014); 	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334); 	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404); 	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377); 	at org.apache.parquet.hadoop.ParquetOutputCommitter.commitJob(ParquetOutputCommitter.java:48); 	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192); 	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640); 	at org.apache.spark.sql.exec",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8587
https://github.com/broadinstitute/gatk/issues/8587:5551,Testability,log,logical,5551,n$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118); 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195); 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103); 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827); 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65); 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98); 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94); 	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512); 	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104); 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512); 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31); 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267); 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263); 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31); 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31); 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488); 	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94); 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81); 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79); 	at org.apache.spark.sql.execution.QueryExecu,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8587
https://github.com/broadinstitute/gatk/issues/8587:5559,Testability,Log,LogicalPlan,5559,n$withNewExecutionId$6(SQLExecution.scala:118); 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195); 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103); 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827); 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65); 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98); 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94); 	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512); 	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104); 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512); 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31); 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267); 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263); 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31); 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31); 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488); 	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94); 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81); 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79); 	at org.apache.spark.sql.execution.QueryExecution.asser,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8587
https://github.com/broadinstitute/gatk/issues/8587:5607,Testability,log,logical,5607,pache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195); 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103); 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827); 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65); 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98); 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94); 	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512); 	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104); 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512); 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31); 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267); 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263); 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31); 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31); 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488); 	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94); 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81); 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79); 	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133); 	at org.apache,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8587
https://github.com/broadinstitute/gatk/issues/8587:5662,Testability,Log,LogicalPlan,5662,pache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195); 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103); 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827); 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65); 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98); 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94); 	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512); 	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104); 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512); 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31); 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267); 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263); 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31); 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31); 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488); 	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94); 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81); 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79); 	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133); 	at org.apache,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8587
https://github.com/broadinstitute/gatk/issues/8587:5725,Testability,log,logical,5725,.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103); 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827); 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65); 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98); 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94); 	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512); 	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104); 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512); 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31); 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267); 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263); 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31); 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31); 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488); 	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94); 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81); 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79); 	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133); 	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856); 	at org.apache.spark.sql.DataFrameWriter,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8587
https://github.com/broadinstitute/gatk/issues/8587:5840,Testability,log,logical,5840,ession.withActive(SparkSession.scala:827); 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65); 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98); 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94); 	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512); 	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104); 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512); 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31); 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267); 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263); 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31); 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31); 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488); 	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94); 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81); 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79); 	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133); 	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856); 	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387); 	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.s,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8587
https://github.com/broadinstitute/gatk/issues/8587:5956,Testability,log,logical,5956,ecution.scala:65); 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98); 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94); 	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512); 	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104); 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512); 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31); 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267); 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263); 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31); 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31); 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488); 	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94); 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81); 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79); 	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133); 	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856); 	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387); 	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360); 	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239); 	at org.apache.spark.sql.DataFr,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8587
https://github.com/broadinstitute/gatk/issues/8587:5964,Testability,Log,LogicalPlan,5964,ala:65); 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98); 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94); 	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512); 	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104); 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512); 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31); 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267); 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263); 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31); 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31); 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488); 	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94); 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81); 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79); 	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133); 	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856); 	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387); 	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360); 	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239); 	at org.apache.spark.sql.DataFrameWriter.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8587
https://github.com/broadinstitute/gatk/issues/8587:6001,Testability,Log,LogicalPlan,6001,spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98); 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94); 	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512); 	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104); 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512); 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31); 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267); 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263); 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31); 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31); 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488); 	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94); 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81); 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79); 	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133); 	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856); 	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387); 	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360); 	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239); 	at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.s,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8587
https://github.com/broadinstitute/gatk/issues/8587:6064,Testability,log,logical,6064,OrElse(QueryExecution.scala:98); 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94); 	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512); 	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104); 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512); 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31); 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267); 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263); 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31); 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31); 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488); 	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94); 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81); 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79); 	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133); 	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856); 	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387); 	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360); 	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239); 	at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:789); 	at StudentAws$.delayedEndpoint$StudentAws$1(StudentAws.scala:,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8587
https://github.com/broadinstitute/gatk/issues/8587:6072,Testability,Log,LogicalPlan,6072,ryExecution.scala:98); 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94); 	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512); 	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104); 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512); 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31); 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267); 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263); 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31); 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31); 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488); 	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94); 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81); 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79); 	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133); 	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856); 	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387); 	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360); 	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239); 	at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:789); 	at StudentAws$.delayedEndpoint$StudentAws$1(StudentAws.scala:36); 	at S,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8587
https://github.com/broadinstitute/gatk/issues/8587:6109,Testability,Log,LogicalPlan,6109,at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94); 	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512); 	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104); 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512); 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31); 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267); 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263); 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31); 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31); 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488); 	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94); 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81); 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79); 	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133); 	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856); 	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387); 	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360); 	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239); 	at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:789); 	at StudentAws$.delayedEndpoint$StudentAws$1(StudentAws.scala:36); 	at StudentAws$delayedInit$bod,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8587
https://github.com/broadinstitute/gatk/issues/8587:6560,Testability,assert,assertCommandExecuted,6560,che$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31); 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267); 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263); 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31); 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31); 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488); 	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94); 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81); 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79); 	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133); 	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856); 	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387); 	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360); 	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239); 	at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:789); 	at StudentAws$.delayedEndpoint$StudentAws$1(StudentAws.scala:36); 	at StudentAws$delayedInit$body.apply(StudentAws.scala:8); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at StudentAws$.main(StudentAws.scala:8); 	at StudentAws.main(StudentAws.scal,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8587
https://github.com/broadinstitute/gatk/issues/8587:11194,Testability,log,logical,11194,n$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118); 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195); 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103); 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827); 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65); 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98); 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94); 	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512); 	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104); 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512); 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31); 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267); 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263); 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31); 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31); 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488); 	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94); 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81); 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79); 	at org.apache.spark.sql.execution.QueryExecu,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8587
https://github.com/broadinstitute/gatk/issues/8587:11202,Testability,Log,LogicalPlan,11202,n$withNewExecutionId$6(SQLExecution.scala:118); 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195); 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103); 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827); 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65); 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98); 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94); 	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512); 	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104); 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512); 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31); 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267); 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263); 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31); 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31); 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488); 	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94); 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81); 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79); 	at org.apache.spark.sql.execution.QueryExecution.asser,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8587
https://github.com/broadinstitute/gatk/issues/8587:11250,Testability,log,logical,11250,pache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195); 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103); 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827); 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65); 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98); 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94); 	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512); 	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104); 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512); 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31); 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267); 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263); 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31); 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31); 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488); 	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94); 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81); 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79); 	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133); 	at org.apache,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8587
https://github.com/broadinstitute/gatk/issues/8587:11305,Testability,Log,LogicalPlan,11305,pache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195); 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103); 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827); 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65); 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98); 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94); 	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512); 	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104); 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512); 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31); 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267); 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263); 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31); 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31); 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488); 	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94); 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81); 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79); 	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133); 	at org.apache,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8587
https://github.com/broadinstitute/gatk/issues/8587:11368,Testability,log,logical,11368,.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103); 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827); 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65); 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98); 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94); 	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512); 	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104); 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512); 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31); 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267); 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263); 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31); 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31); 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488); 	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94); 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81); 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79); 	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133); 	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856); 	at org.apache.spark.sql.DataFrameWriter,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8587
https://github.com/broadinstitute/gatk/issues/8587:11483,Testability,log,logical,11483,ession.withActive(SparkSession.scala:827); 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65); 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98); 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94); 	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512); 	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104); 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512); 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31); 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267); 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263); 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31); 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31); 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488); 	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94); 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81); 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79); 	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133); 	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856); 	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387); 	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.s,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8587
https://github.com/broadinstitute/gatk/issues/8587:11599,Testability,log,logical,11599,ecution.scala:65); 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98); 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94); 	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512); 	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104); 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512); 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31); 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267); 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263); 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31); 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31); 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488); 	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94); 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81); 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79); 	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133); 	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856); 	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387); 	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360); 	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239); 	at org.apache.spark.sql.DataFr,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8587
https://github.com/broadinstitute/gatk/issues/8587:11607,Testability,Log,LogicalPlan,11607,ala:65); 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98); 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94); 	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512); 	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104); 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512); 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31); 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267); 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263); 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31); 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31); 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488); 	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94); 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81); 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79); 	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133); 	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856); 	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387); 	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360); 	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239); 	at org.apache.spark.sql.DataFrameWriter.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8587
https://github.com/broadinstitute/gatk/issues/8587:11644,Testability,Log,LogicalPlan,11644,spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98); 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94); 	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512); 	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104); 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512); 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31); 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267); 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263); 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31); 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31); 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488); 	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94); 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81); 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79); 	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133); 	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856); 	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387); 	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360); 	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239); 	at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.s,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8587
https://github.com/broadinstitute/gatk/issues/8587:11707,Testability,log,logical,11707,OrElse(QueryExecution.scala:98); 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94); 	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512); 	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104); 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512); 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31); 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267); 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263); 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31); 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31); 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488); 	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94); 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81); 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79); 	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133); 	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856); 	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387); 	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360); 	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239); 	at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:789); 	at StudentAws$.delayedEndpoint$StudentAws$1(StudentAws.scala:,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8587
https://github.com/broadinstitute/gatk/issues/8587:11715,Testability,Log,LogicalPlan,11715,ryExecution.scala:98); 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94); 	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512); 	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104); 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512); 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31); 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267); 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263); 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31); 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31); 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488); 	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94); 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81); 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79); 	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133); 	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856); 	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387); 	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360); 	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239); 	at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:789); 	at StudentAws$.delayedEndpoint$StudentAws$1(StudentAws.scala:36); 	at S,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8587
https://github.com/broadinstitute/gatk/issues/8587:11752,Testability,Log,LogicalPlan,11752,at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94); 	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512); 	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104); 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512); 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31); 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267); 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263); 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31); 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31); 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488); 	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94); 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81); 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79); 	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133); 	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856); 	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387); 	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360); 	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239); 	at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:789); 	at StudentAws$.delayedEndpoint$StudentAws$1(StudentAws.scala:36); 	at StudentAws$delayedInit$bod,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8587
https://github.com/broadinstitute/gatk/issues/8587:12203,Testability,assert,assertCommandExecuted,12203,che$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31); 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267); 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263); 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31); 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31); 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488); 	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94); 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81); 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79); 	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133); 	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856); 	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387); 	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360); 	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239); 	at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:789); 	at StudentAws$.delayedEndpoint$StudentAws$1(StudentAws.scala:36); 	at StudentAws$delayedInit$body.apply(StudentAws.scala:8); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at StudentAws$.main(StudentAws.scala:8); 	at StudentAws.main(StudentAws.scal,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8587
https://github.com/broadinstitute/gatk/issues/8587:13610,Usability,clear,cleared,13610,n.scala:79); 	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133); 	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856); 	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387); 	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360); 	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239); 	at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:789); 	at StudentAws$.delayedEndpoint$StudentAws$1(StudentAws.scala:36); 	at StudentAws$delayedInit$body.apply(StudentAws.scala:8); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at StudentAws$.main(StudentAws.scala:8); 	at StudentAws.main(StudentAws.scala); 23/11/16 12:09:10 INFO SparkContext: Invoking stop() from shutdown hook; 23/11/16 12:09:10 INFO SparkContext: SparkContext is stopping with exitCode 0.; 23/11/16 12:09:10 INFO SparkUI: Stopped Spark web UI at http://SRINIVASiNDRARAVI:4040; 23/11/16 12:09:10 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 23/11/16 12:09:10 INFO MemoryStore: MemoryStore cleared; 23/11/16 12:09:10 INFO BlockManager: BlockManager stopped; 23/11/16 12:09:10 INFO BlockManagerMaster: BlockManagerMaster stopped; 23/11/16 12:09:10 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 23/11/16 12:09:10 INFO SparkContext: Successfully stopped SparkContext; 23/11/16 12:09:10 INFO ShutdownHookManager: Shutdown hook called; 23/11/16 12:09:10 INFO ShutdownHookManager: Deleting directory C:\Users\SRINI\AppData\Local\Temp\spark-a5d9bd91-5f37-4677-8a41-0bdf406d0929,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8587
https://github.com/broadinstitute/gatk/pull/8588:954,Deployability,integrat,integration,954,"Adds a new ReadWalker tool, `RealignSoftClippedReads`, that realigns soft-clipped fragments using BWA. This tool is motivated by a specific artifact produced by Illumina DRAGEN v3.7.8 in which reads containing small indels are erroneously soft-clipped, often within mobile element contexts (LINE, SINE, ALU, SVA, etc). This is particularly problematic for mobile element insertion callers such as [Scramble](https://github.com/GeneDx/scramble) that rely on soft-clips for identifying potential insertion sites but do not perform a local assembly. In some cases, these soft-clipped reads are aligned to the incorrect region (confirmed by BLAT query and comparison to BWA alignments). An example of a false positive site produced by Scramble is shown below. <img width=""1008"" alt=""Screenshot 2023-11-16 at 2 09 45 PM"" src=""https://github.com/broadinstitute/gatk/assets/5686877/9d2c1dfd-9673-49f0-9372-c4c9cf6ffd9f"">. This PR includes the new tool and unit/integration tests and some minor refactoring to expose non-Spark BWA read mapping. This tool should be considered experimental until thorough benchmarking and analysis can be performed.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8588
https://github.com/broadinstitute/gatk/pull/8588:954,Integrability,integrat,integration,954,"Adds a new ReadWalker tool, `RealignSoftClippedReads`, that realigns soft-clipped fragments using BWA. This tool is motivated by a specific artifact produced by Illumina DRAGEN v3.7.8 in which reads containing small indels are erroneously soft-clipped, often within mobile element contexts (LINE, SINE, ALU, SVA, etc). This is particularly problematic for mobile element insertion callers such as [Scramble](https://github.com/GeneDx/scramble) that rely on soft-clips for identifying potential insertion sites but do not perform a local assembly. In some cases, these soft-clipped reads are aligned to the incorrect region (confirmed by BLAT query and comparison to BWA alignments). An example of a false positive site produced by Scramble is shown below. <img width=""1008"" alt=""Screenshot 2023-11-16 at 2 09 45 PM"" src=""https://github.com/broadinstitute/gatk/assets/5686877/9d2c1dfd-9673-49f0-9372-c4c9cf6ffd9f"">. This PR includes the new tool and unit/integration tests and some minor refactoring to expose non-Spark BWA read mapping. This tool should be considered experimental until thorough benchmarking and analysis can be performed.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8588
https://github.com/broadinstitute/gatk/pull/8588:987,Modifiability,refactor,refactoring,987,"Adds a new ReadWalker tool, `RealignSoftClippedReads`, that realigns soft-clipped fragments using BWA. This tool is motivated by a specific artifact produced by Illumina DRAGEN v3.7.8 in which reads containing small indels are erroneously soft-clipped, often within mobile element contexts (LINE, SINE, ALU, SVA, etc). This is particularly problematic for mobile element insertion callers such as [Scramble](https://github.com/GeneDx/scramble) that rely on soft-clips for identifying potential insertion sites but do not perform a local assembly. In some cases, these soft-clipped reads are aligned to the incorrect region (confirmed by BLAT query and comparison to BWA alignments). An example of a false positive site produced by Scramble is shown below. <img width=""1008"" alt=""Screenshot 2023-11-16 at 2 09 45 PM"" src=""https://github.com/broadinstitute/gatk/assets/5686877/9d2c1dfd-9673-49f0-9372-c4c9cf6ffd9f"">. This PR includes the new tool and unit/integration tests and some minor refactoring to expose non-Spark BWA read mapping. This tool should be considered experimental until thorough benchmarking and analysis can be performed.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8588
https://github.com/broadinstitute/gatk/pull/8588:521,Performance,perform,perform,521,"Adds a new ReadWalker tool, `RealignSoftClippedReads`, that realigns soft-clipped fragments using BWA. This tool is motivated by a specific artifact produced by Illumina DRAGEN v3.7.8 in which reads containing small indels are erroneously soft-clipped, often within mobile element contexts (LINE, SINE, ALU, SVA, etc). This is particularly problematic for mobile element insertion callers such as [Scramble](https://github.com/GeneDx/scramble) that rely on soft-clips for identifying potential insertion sites but do not perform a local assembly. In some cases, these soft-clipped reads are aligned to the incorrect region (confirmed by BLAT query and comparison to BWA alignments). An example of a false positive site produced by Scramble is shown below. <img width=""1008"" alt=""Screenshot 2023-11-16 at 2 09 45 PM"" src=""https://github.com/broadinstitute/gatk/assets/5686877/9d2c1dfd-9673-49f0-9372-c4c9cf6ffd9f"">. This PR includes the new tool and unit/integration tests and some minor refactoring to expose non-Spark BWA read mapping. This tool should be considered experimental until thorough benchmarking and analysis can be performed.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8588
https://github.com/broadinstitute/gatk/pull/8588:1129,Performance,perform,performed,1129,"Adds a new ReadWalker tool, `RealignSoftClippedReads`, that realigns soft-clipped fragments using BWA. This tool is motivated by a specific artifact produced by Illumina DRAGEN v3.7.8 in which reads containing small indels are erroneously soft-clipped, often within mobile element contexts (LINE, SINE, ALU, SVA, etc). This is particularly problematic for mobile element insertion callers such as [Scramble](https://github.com/GeneDx/scramble) that rely on soft-clips for identifying potential insertion sites but do not perform a local assembly. In some cases, these soft-clipped reads are aligned to the incorrect region (confirmed by BLAT query and comparison to BWA alignments). An example of a false positive site produced by Scramble is shown below. <img width=""1008"" alt=""Screenshot 2023-11-16 at 2 09 45 PM"" src=""https://github.com/broadinstitute/gatk/assets/5686877/9d2c1dfd-9673-49f0-9372-c4c9cf6ffd9f"">. This PR includes the new tool and unit/integration tests and some minor refactoring to expose non-Spark BWA read mapping. This tool should be considered experimental until thorough benchmarking and analysis can be performed.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8588
https://github.com/broadinstitute/gatk/pull/8588:1002,Security,expose,expose,1002,"Adds a new ReadWalker tool, `RealignSoftClippedReads`, that realigns soft-clipped fragments using BWA. This tool is motivated by a specific artifact produced by Illumina DRAGEN v3.7.8 in which reads containing small indels are erroneously soft-clipped, often within mobile element contexts (LINE, SINE, ALU, SVA, etc). This is particularly problematic for mobile element insertion callers such as [Scramble](https://github.com/GeneDx/scramble) that rely on soft-clips for identifying potential insertion sites but do not perform a local assembly. In some cases, these soft-clipped reads are aligned to the incorrect region (confirmed by BLAT query and comparison to BWA alignments). An example of a false positive site produced by Scramble is shown below. <img width=""1008"" alt=""Screenshot 2023-11-16 at 2 09 45 PM"" src=""https://github.com/broadinstitute/gatk/assets/5686877/9d2c1dfd-9673-49f0-9372-c4c9cf6ffd9f"">. This PR includes the new tool and unit/integration tests and some minor refactoring to expose non-Spark BWA read mapping. This tool should be considered experimental until thorough benchmarking and analysis can be performed.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8588
https://github.com/broadinstitute/gatk/pull/8588:966,Testability,test,tests,966,"Adds a new ReadWalker tool, `RealignSoftClippedReads`, that realigns soft-clipped fragments using BWA. This tool is motivated by a specific artifact produced by Illumina DRAGEN v3.7.8 in which reads containing small indels are erroneously soft-clipped, often within mobile element contexts (LINE, SINE, ALU, SVA, etc). This is particularly problematic for mobile element insertion callers such as [Scramble](https://github.com/GeneDx/scramble) that rely on soft-clips for identifying potential insertion sites but do not perform a local assembly. In some cases, these soft-clipped reads are aligned to the incorrect region (confirmed by BLAT query and comparison to BWA alignments). An example of a false positive site produced by Scramble is shown below. <img width=""1008"" alt=""Screenshot 2023-11-16 at 2 09 45 PM"" src=""https://github.com/broadinstitute/gatk/assets/5686877/9d2c1dfd-9673-49f0-9372-c4c9cf6ffd9f"">. This PR includes the new tool and unit/integration tests and some minor refactoring to expose non-Spark BWA read mapping. This tool should be considered experimental until thorough benchmarking and analysis can be performed.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8588
https://github.com/broadinstitute/gatk/pull/8588:1096,Testability,benchmark,benchmarking,1096,"Adds a new ReadWalker tool, `RealignSoftClippedReads`, that realigns soft-clipped fragments using BWA. This tool is motivated by a specific artifact produced by Illumina DRAGEN v3.7.8 in which reads containing small indels are erroneously soft-clipped, often within mobile element contexts (LINE, SINE, ALU, SVA, etc). This is particularly problematic for mobile element insertion callers such as [Scramble](https://github.com/GeneDx/scramble) that rely on soft-clips for identifying potential insertion sites but do not perform a local assembly. In some cases, these soft-clipped reads are aligned to the incorrect region (confirmed by BLAT query and comparison to BWA alignments). An example of a false positive site produced by Scramble is shown below. <img width=""1008"" alt=""Screenshot 2023-11-16 at 2 09 45 PM"" src=""https://github.com/broadinstitute/gatk/assets/5686877/9d2c1dfd-9673-49f0-9372-c4c9cf6ffd9f"">. This PR includes the new tool and unit/integration tests and some minor refactoring to expose non-Spark BWA read mapping. This tool should be considered experimental until thorough benchmarking and analysis can be performed.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8588
https://github.com/broadinstitute/gatk/pull/8589:2,Deployability,update,update,2,- update to gradle 8.4 and build with java 21; - fixing or supressing various warnings; - in progress; - allow warnings and update actions to 21 so we can run tetsts,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8589
https://github.com/broadinstitute/gatk/pull/8589:124,Deployability,update,update,124,- update to gradle 8.4 and build with java 21; - fixing or supressing various warnings; - in progress; - allow warnings and update actions to 21 so we can run tetsts,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8589
https://github.com/broadinstitute/gatk/pull/8590:72,Deployability,integrat,integration,72,Revert some phasing changes that were unnecessary for AoU and broke our integration tests. [Integration run](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/ba93baa2-9971-4c90-8ce3-635702a81eb6),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8590
https://github.com/broadinstitute/gatk/pull/8590:92,Deployability,Integrat,Integration,92,Revert some phasing changes that were unnecessary for AoU and broke our integration tests. [Integration run](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/ba93baa2-9971-4c90-8ce3-635702a81eb6),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8590
https://github.com/broadinstitute/gatk/pull/8590:72,Integrability,integrat,integration,72,Revert some phasing changes that were unnecessary for AoU and broke our integration tests. [Integration run](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/ba93baa2-9971-4c90-8ce3-635702a81eb6),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8590
https://github.com/broadinstitute/gatk/pull/8590:92,Integrability,Integrat,Integration,92,Revert some phasing changes that were unnecessary for AoU and broke our integration tests. [Integration run](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/ba93baa2-9971-4c90-8ce3-635702a81eb6),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8590
https://github.com/broadinstitute/gatk/pull/8590:84,Testability,test,tests,84,Revert some phasing changes that were unnecessary for AoU and broke our integration tests. [Integration run](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/ba93baa2-9971-4c90-8ce3-635702a81eb6),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8590
https://github.com/broadinstitute/gatk/issues/8593:256,Availability,error,error,256,"I got 348 samples to analyse their variants. I have read several turorials about how to use gatk to get a population vcf. At the beginning , I tried to use CombineGVCFs to get the Gvcf and use SelectVariants to pick the snps out. . CombineGVCFs truns to a error ""Exception in thread ""main"" java.lang.OutOfMemoryError"" .; then I chose to use GenomicsDBImport to do this job. It still doesn't work. First error is ""read_one_line_fully && ""Buffer did not have space to hold a line fully - increase buffer size""; I add ""--genomicsdb-vcf-buffer-size 16384000"" , it causes different error ""Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space"". This is my command and work log.; My java version is ; openjdk version ""1.8.0_152-release""; OpenJDK Runtime Environment (build 1.8.0_152-release-1056-b12). GATK is very helpful in my research, and I really need some help to get it work. gatk --java-options ""-Xmx48g -Xms48G"" GenomicsDBImport -V C1_sentieon_gvcf.gz .......... -V SCAU-106.gvcf.gz -V SCAU-107.gvcf.gz -V SCAU-108.gvcf.gz -V SCAU-128.gvcf.gz --genomicsdb-workspace-path my_database.chr01 -R IRGSP-1.0_genome.fasta --genomicsdb-vcf-buffer-size 16384000 --intervals chr01. 11:48:08.245 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/ayu/anaconda3/share/gatk4-4.0.5.1-0/gatk-package-4.0.5.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 11:48:09.327 INFO GenomicsDBImport - ------------------------------------------------------------; 11:48:09.327 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.0.5.1; 11:48:09.327 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:48:09.327 INFO GenomicsDBImport - Executing as ayu@ayu on Linux v5.15.90.1-microsoft-standard-WSL2 amd64; 11:48:09.327 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_152-release-1056-b12; 11:48:09.327 INFO GenomicsDBImport - Start Date/Time: November 26, 2023 11:48:08 AM CST; 11:48:0",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8593
https://github.com/broadinstitute/gatk/issues/8593:403,Availability,error,error,403,"I got 348 samples to analyse their variants. I have read several turorials about how to use gatk to get a population vcf. At the beginning , I tried to use CombineGVCFs to get the Gvcf and use SelectVariants to pick the snps out. . CombineGVCFs truns to a error ""Exception in thread ""main"" java.lang.OutOfMemoryError"" .; then I chose to use GenomicsDBImport to do this job. It still doesn't work. First error is ""read_one_line_fully && ""Buffer did not have space to hold a line fully - increase buffer size""; I add ""--genomicsdb-vcf-buffer-size 16384000"" , it causes different error ""Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space"". This is my command and work log.; My java version is ; openjdk version ""1.8.0_152-release""; OpenJDK Runtime Environment (build 1.8.0_152-release-1056-b12). GATK is very helpful in my research, and I really need some help to get it work. gatk --java-options ""-Xmx48g -Xms48G"" GenomicsDBImport -V C1_sentieon_gvcf.gz .......... -V SCAU-106.gvcf.gz -V SCAU-107.gvcf.gz -V SCAU-108.gvcf.gz -V SCAU-128.gvcf.gz --genomicsdb-workspace-path my_database.chr01 -R IRGSP-1.0_genome.fasta --genomicsdb-vcf-buffer-size 16384000 --intervals chr01. 11:48:08.245 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/ayu/anaconda3/share/gatk4-4.0.5.1-0/gatk-package-4.0.5.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 11:48:09.327 INFO GenomicsDBImport - ------------------------------------------------------------; 11:48:09.327 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.0.5.1; 11:48:09.327 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:48:09.327 INFO GenomicsDBImport - Executing as ayu@ayu on Linux v5.15.90.1-microsoft-standard-WSL2 amd64; 11:48:09.327 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_152-release-1056-b12; 11:48:09.327 INFO GenomicsDBImport - Start Date/Time: November 26, 2023 11:48:08 AM CST; 11:48:0",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8593
https://github.com/broadinstitute/gatk/issues/8593:577,Availability,error,error,577,"I got 348 samples to analyse their variants. I have read several turorials about how to use gatk to get a population vcf. At the beginning , I tried to use CombineGVCFs to get the Gvcf and use SelectVariants to pick the snps out. . CombineGVCFs truns to a error ""Exception in thread ""main"" java.lang.OutOfMemoryError"" .; then I chose to use GenomicsDBImport to do this job. It still doesn't work. First error is ""read_one_line_fully && ""Buffer did not have space to hold a line fully - increase buffer size""; I add ""--genomicsdb-vcf-buffer-size 16384000"" , it causes different error ""Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space"". This is my command and work log.; My java version is ; openjdk version ""1.8.0_152-release""; OpenJDK Runtime Environment (build 1.8.0_152-release-1056-b12). GATK is very helpful in my research, and I really need some help to get it work. gatk --java-options ""-Xmx48g -Xms48G"" GenomicsDBImport -V C1_sentieon_gvcf.gz .......... -V SCAU-106.gvcf.gz -V SCAU-107.gvcf.gz -V SCAU-108.gvcf.gz -V SCAU-128.gvcf.gz --genomicsdb-workspace-path my_database.chr01 -R IRGSP-1.0_genome.fasta --genomicsdb-vcf-buffer-size 16384000 --intervals chr01. 11:48:08.245 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/ayu/anaconda3/share/gatk4-4.0.5.1-0/gatk-package-4.0.5.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 11:48:09.327 INFO GenomicsDBImport - ------------------------------------------------------------; 11:48:09.327 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.0.5.1; 11:48:09.327 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:48:09.327 INFO GenomicsDBImport - Executing as ayu@ayu on Linux v5.15.90.1-microsoft-standard-WSL2 amd64; 11:48:09.327 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_152-release-1056-b12; 11:48:09.327 INFO GenomicsDBImport - Start Date/Time: November 26, 2023 11:48:08 AM CST; 11:48:0",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8593
https://github.com/broadinstitute/gatk/issues/8593:4020,Availability,down,down,4020,"_nio_fixes; 11:48:09.328 INFO GenomicsDBImport - Initializing engine; 11:48:14.819 INFO IntervalArgumentCollection - Processing 43270923 bp from intervals; 11:48:14.846 INFO GenomicsDBImport - Done initializing engine; Created workspace /mnt/g/ubuntushare/sequence/C271_sentieon_gvcf/my_database.chr01; 11:48:14.919 INFO GenomicsDBImport - Vid Map JSON file will be written to my_database.chr01/vidmap.json; 11:48:14.919 INFO GenomicsDBImport - Callset Map JSON file will be written to my_database.chr01/callset.json; 11:48:14.919 INFO GenomicsDBImport - Complete VCF Header will be written to my_database.chr01/vcfheader.vcf; 11:48:14.919 INFO GenomicsDBImport - Importing to array - my_database.chr01/genomicsdb_array; 11:48:14.924 INFO ProgressMeter - Starting traversal; 11:48:14.924 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 11:48:19.709 INFO GenomicsDBImport - Importing batch 1 with 348 samples; 11:48:24.549 INFO GenomicsDBImport - Shutting down engine; [November 26, 2023 11:48:24 AM CST] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 0.27 minutes.; Runtime.totalMemory()=12916359168; Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space; at com.intel.genomicsdb.SilentByteBufferStream.<init>(SilentByteBufferStream.java:55); at com.intel.genomicsdb.GenomicsDBImporterStreamWrapper.<init>(GenomicsDBImporterStreamWrapper.java:74); at com.intel.genomicsdb.GenomicsDBImporter.addBufferStream(GenomicsDBImporter.java:1289); at com.intel.genomicsdb.GenomicsDBImporter.addSortedVariantContextIterator(GenomicsDBImporter.java:1212); at com.intel.genomicsdb.GenomicsDBImporter.<init>(GenomicsDBImporter.java:597); at com.intel.genomicsdb.GenomicsDBImporter.<init>(GenomicsDBImporter.java:512); at com.intel.genomicsdb.GenomicsDBImporter.<init>(GenomicsDBImporter.java:472); at com.intel.genomicsdb.GenomicsDBImporter.<init>(GenomicsDBImporter.java:358); at org.broadinstitute.hellbender.tools.genomi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8593
https://github.com/broadinstitute/gatk/issues/8593:739,Deployability,release,release,739,"I got 348 samples to analyse their variants. I have read several turorials about how to use gatk to get a population vcf. At the beginning , I tried to use CombineGVCFs to get the Gvcf and use SelectVariants to pick the snps out. . CombineGVCFs truns to a error ""Exception in thread ""main"" java.lang.OutOfMemoryError"" .; then I chose to use GenomicsDBImport to do this job. It still doesn't work. First error is ""read_one_line_fully && ""Buffer did not have space to hold a line fully - increase buffer size""; I add ""--genomicsdb-vcf-buffer-size 16384000"" , it causes different error ""Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space"". This is my command and work log.; My java version is ; openjdk version ""1.8.0_152-release""; OpenJDK Runtime Environment (build 1.8.0_152-release-1056-b12). GATK is very helpful in my research, and I really need some help to get it work. gatk --java-options ""-Xmx48g -Xms48G"" GenomicsDBImport -V C1_sentieon_gvcf.gz .......... -V SCAU-106.gvcf.gz -V SCAU-107.gvcf.gz -V SCAU-108.gvcf.gz -V SCAU-128.gvcf.gz --genomicsdb-workspace-path my_database.chr01 -R IRGSP-1.0_genome.fasta --genomicsdb-vcf-buffer-size 16384000 --intervals chr01. 11:48:08.245 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/ayu/anaconda3/share/gatk4-4.0.5.1-0/gatk-package-4.0.5.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 11:48:09.327 INFO GenomicsDBImport - ------------------------------------------------------------; 11:48:09.327 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.0.5.1; 11:48:09.327 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:48:09.327 INFO GenomicsDBImport - Executing as ayu@ayu on Linux v5.15.90.1-microsoft-standard-WSL2 amd64; 11:48:09.327 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_152-release-1056-b12; 11:48:09.327 INFO GenomicsDBImport - Start Date/Time: November 26, 2023 11:48:08 AM CST; 11:48:0",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8593
https://github.com/broadinstitute/gatk/issues/8593:794,Deployability,release,release-,794,"I got 348 samples to analyse their variants. I have read several turorials about how to use gatk to get a population vcf. At the beginning , I tried to use CombineGVCFs to get the Gvcf and use SelectVariants to pick the snps out. . CombineGVCFs truns to a error ""Exception in thread ""main"" java.lang.OutOfMemoryError"" .; then I chose to use GenomicsDBImport to do this job. It still doesn't work. First error is ""read_one_line_fully && ""Buffer did not have space to hold a line fully - increase buffer size""; I add ""--genomicsdb-vcf-buffer-size 16384000"" , it causes different error ""Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space"". This is my command and work log.; My java version is ; openjdk version ""1.8.0_152-release""; OpenJDK Runtime Environment (build 1.8.0_152-release-1056-b12). GATK is very helpful in my research, and I really need some help to get it work. gatk --java-options ""-Xmx48g -Xms48G"" GenomicsDBImport -V C1_sentieon_gvcf.gz .......... -V SCAU-106.gvcf.gz -V SCAU-107.gvcf.gz -V SCAU-108.gvcf.gz -V SCAU-128.gvcf.gz --genomicsdb-workspace-path my_database.chr01 -R IRGSP-1.0_genome.fasta --genomicsdb-vcf-buffer-size 16384000 --intervals chr01. 11:48:08.245 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/ayu/anaconda3/share/gatk4-4.0.5.1-0/gatk-package-4.0.5.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 11:48:09.327 INFO GenomicsDBImport - ------------------------------------------------------------; 11:48:09.327 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.0.5.1; 11:48:09.327 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:48:09.327 INFO GenomicsDBImport - Executing as ayu@ayu on Linux v5.15.90.1-microsoft-standard-WSL2 amd64; 11:48:09.327 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_152-release-1056-b12; 11:48:09.327 INFO GenomicsDBImport - Start Date/Time: November 26, 2023 11:48:08 AM CST; 11:48:0",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8593
https://github.com/broadinstitute/gatk/issues/8593:1887,Deployability,release,release-,1887,"k --java-options ""-Xmx48g -Xms48G"" GenomicsDBImport -V C1_sentieon_gvcf.gz .......... -V SCAU-106.gvcf.gz -V SCAU-107.gvcf.gz -V SCAU-108.gvcf.gz -V SCAU-128.gvcf.gz --genomicsdb-workspace-path my_database.chr01 -R IRGSP-1.0_genome.fasta --genomicsdb-vcf-buffer-size 16384000 --intervals chr01. 11:48:08.245 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/ayu/anaconda3/share/gatk4-4.0.5.1-0/gatk-package-4.0.5.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 11:48:09.327 INFO GenomicsDBImport - ------------------------------------------------------------; 11:48:09.327 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.0.5.1; 11:48:09.327 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:48:09.327 INFO GenomicsDBImport - Executing as ayu@ayu on Linux v5.15.90.1-microsoft-standard-WSL2 amd64; 11:48:09.327 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_152-release-1056-b12; 11:48:09.327 INFO GenomicsDBImport - Start Date/Time: November 26, 2023 11:48:08 AM CST; 11:48:09.327 INFO GenomicsDBImport - ------------------------------------------------------------; 11:48:09.327 INFO GenomicsDBImport - ------------------------------------------------------------; 11:48:09.327 INFO GenomicsDBImport - HTSJDK Version: 2.15.1; 11:48:09.327 INFO GenomicsDBImport - Picard Version: 2.18.2; 11:48:09.327 INFO GenomicsDBImport - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 11:48:09.327 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 11:48:09.327 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 11:48:09.327 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 11:48:09.327 INFO GenomicsDBImport - Deflater: IntelDeflater; 11:48:09.327 INFO GenomicsDBImport - Inflater: IntelInflater; 11:48:09.327 INFO GenomicsDBImport - GCS max retries/reopens: 20; 11:48:09.327 INFO GenomicsDBImport - Us",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8593
https://github.com/broadinstitute/gatk/issues/8593:2920,Deployability,patch,patch,2920,"micsDBImport - Start Date/Time: November 26, 2023 11:48:08 AM CST; 11:48:09.327 INFO GenomicsDBImport - ------------------------------------------------------------; 11:48:09.327 INFO GenomicsDBImport - ------------------------------------------------------------; 11:48:09.327 INFO GenomicsDBImport - HTSJDK Version: 2.15.1; 11:48:09.327 INFO GenomicsDBImport - Picard Version: 2.18.2; 11:48:09.327 INFO GenomicsDBImport - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 11:48:09.327 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 11:48:09.327 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 11:48:09.327 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 11:48:09.327 INFO GenomicsDBImport - Deflater: IntelDeflater; 11:48:09.327 INFO GenomicsDBImport - Inflater: IntelInflater; 11:48:09.327 INFO GenomicsDBImport - GCS max retries/reopens: 20; 11:48:09.327 INFO GenomicsDBImport - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 11:48:09.328 INFO GenomicsDBImport - Initializing engine; 11:48:14.819 INFO IntervalArgumentCollection - Processing 43270923 bp from intervals; 11:48:14.846 INFO GenomicsDBImport - Done initializing engine; Created workspace /mnt/g/ubuntushare/sequence/C271_sentieon_gvcf/my_database.chr01; 11:48:14.919 INFO GenomicsDBImport - Vid Map JSON file will be written to my_database.chr01/vidmap.json; 11:48:14.919 INFO GenomicsDBImport - Callset Map JSON file will be written to my_database.chr01/callset.json; 11:48:14.919 INFO GenomicsDBImport - Complete VCF Header will be written to my_database.chr01/vcfheader.vcf; 11:48:14.919 INFO GenomicsDBImport - Importing to array - my_database.chr01/genomicsdb_array; 11:48:14.924 INFO ProgressMeter - Starting traversal; 11:48:14.924 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 11:48:19.709 INFO GenomicsD",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8593
https://github.com/broadinstitute/gatk/issues/8593:1232,Performance,Load,Loading,1232,". CombineGVCFs truns to a error ""Exception in thread ""main"" java.lang.OutOfMemoryError"" .; then I chose to use GenomicsDBImport to do this job. It still doesn't work. First error is ""read_one_line_fully && ""Buffer did not have space to hold a line fully - increase buffer size""; I add ""--genomicsdb-vcf-buffer-size 16384000"" , it causes different error ""Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space"". This is my command and work log.; My java version is ; openjdk version ""1.8.0_152-release""; OpenJDK Runtime Environment (build 1.8.0_152-release-1056-b12). GATK is very helpful in my research, and I really need some help to get it work. gatk --java-options ""-Xmx48g -Xms48G"" GenomicsDBImport -V C1_sentieon_gvcf.gz .......... -V SCAU-106.gvcf.gz -V SCAU-107.gvcf.gz -V SCAU-108.gvcf.gz -V SCAU-128.gvcf.gz --genomicsdb-workspace-path my_database.chr01 -R IRGSP-1.0_genome.fasta --genomicsdb-vcf-buffer-size 16384000 --intervals chr01. 11:48:08.245 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/ayu/anaconda3/share/gatk4-4.0.5.1-0/gatk-package-4.0.5.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 11:48:09.327 INFO GenomicsDBImport - ------------------------------------------------------------; 11:48:09.327 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.0.5.1; 11:48:09.327 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:48:09.327 INFO GenomicsDBImport - Executing as ayu@ayu on Linux v5.15.90.1-microsoft-standard-WSL2 amd64; 11:48:09.327 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_152-release-1056-b12; 11:48:09.327 INFO GenomicsDBImport - Start Date/Time: November 26, 2023 11:48:08 AM CST; 11:48:09.327 INFO GenomicsDBImport - ------------------------------------------------------------; 11:48:09.327 INFO GenomicsDBImport - ------------------------------------------------------------; 11:48:09.327 INFO GenomicsDBImport - H",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8593
https://github.com/broadinstitute/gatk/issues/8593:685,Testability,log,log,685,"I got 348 samples to analyse their variants. I have read several turorials about how to use gatk to get a population vcf. At the beginning , I tried to use CombineGVCFs to get the Gvcf and use SelectVariants to pick the snps out. . CombineGVCFs truns to a error ""Exception in thread ""main"" java.lang.OutOfMemoryError"" .; then I chose to use GenomicsDBImport to do this job. It still doesn't work. First error is ""read_one_line_fully && ""Buffer did not have space to hold a line fully - increase buffer size""; I add ""--genomicsdb-vcf-buffer-size 16384000"" , it causes different error ""Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space"". This is my command and work log.; My java version is ; openjdk version ""1.8.0_152-release""; OpenJDK Runtime Environment (build 1.8.0_152-release-1056-b12). GATK is very helpful in my research, and I really need some help to get it work. gatk --java-options ""-Xmx48g -Xms48G"" GenomicsDBImport -V C1_sentieon_gvcf.gz .......... -V SCAU-106.gvcf.gz -V SCAU-107.gvcf.gz -V SCAU-108.gvcf.gz -V SCAU-128.gvcf.gz --genomicsdb-workspace-path my_database.chr01 -R IRGSP-1.0_genome.fasta --genomicsdb-vcf-buffer-size 16384000 --intervals chr01. 11:48:08.245 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/ayu/anaconda3/share/gatk4-4.0.5.1-0/gatk-package-4.0.5.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 11:48:09.327 INFO GenomicsDBImport - ------------------------------------------------------------; 11:48:09.327 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.0.5.1; 11:48:09.327 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:48:09.327 INFO GenomicsDBImport - Executing as ayu@ayu on Linux v5.15.90.1-microsoft-standard-WSL2 amd64; 11:48:09.327 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_152-release-1056-b12; 11:48:09.327 INFO GenomicsDBImport - Start Date/Time: November 26, 2023 11:48:08 AM CST; 11:48:0",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8593
https://github.com/broadinstitute/gatk/issues/8596:386,Availability,Error,Error,386,"Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx25G -Xms25G -Djava.io.tmpdir=/Data/data/raja/cfdna/gatk-4.4.0.0/tmp -jar /Data/data/raja/cfdna/nextflow1/bin/gatk-4.4.0.0/gatk-package-4.4.0.0-local.jar MarkDuplicatesSpark -I sout_hd.bam -O sout_hd.ctrl.bam.gz; Error: LinkageError occurred while loading main class org.broadinstitute.hellbender.Main; java.lang.UnsupportedClassVersionError: org/broadinstitute/hellbender/Main has been compiled by a more recent version of the Java Runtime (class file version 61.0), this version of the Java Runtime only recognizes class file versions up to 55.0; ![Screenshot_2](https://github.com/broadinstitute/gatk/assets/75623749/cdaca619-fc57-4f6b-b350-0402036c099c)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8596
https://github.com/broadinstitute/gatk/issues/8596:421,Performance,load,loading,421,"Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx25G -Xms25G -Djava.io.tmpdir=/Data/data/raja/cfdna/gatk-4.4.0.0/tmp -jar /Data/data/raja/cfdna/nextflow1/bin/gatk-4.4.0.0/gatk-package-4.4.0.0-local.jar MarkDuplicatesSpark -I sout_hd.bam -O sout_hd.ctrl.bam.gz; Error: LinkageError occurred while loading main class org.broadinstitute.hellbender.Main; java.lang.UnsupportedClassVersionError: org/broadinstitute/hellbender/Main has been compiled by a more recent version of the Java Runtime (class file version 61.0), this version of the Java Runtime only recognizes class file versions up to 55.0; ![Screenshot_2](https://github.com/broadinstitute/gatk/assets/75623749/cdaca619-fc57-4f6b-b350-0402036c099c)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8596
https://github.com/broadinstitute/gatk/pull/8598:39,Modifiability,refactor,refactored,39,I removed an unused type parameter and refactored some functionality into a class MultiPloidyGenotyper which handles dealing with multiple ploidies in a somewhat unified way. It could probably be renamed and expanded slightly.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8598
https://github.com/broadinstitute/gatk/pull/8599:8,Deployability,update,updates,8,"This PR updates GvsWithdrawSamples to:; 1) Use a ""true"" temporary table (uniquely named, goes away after 24 hours); 2) Check if there are any samples in the uploaded list of samples to withdraw that are NOT in the existing sample_info table. Fail and print out the list of samples if so.; 3) Added a boolean flag to allow the user to pass the workflow if condition 2 is true. A) Example [Run](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/26482e64-cc22-4191-b88f-f7765b173450) with 0 samples withdrawn and 0 new samples (samples in the withdrawn file that aren't in sample_info); B) Example [Run](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/00f17100-18c4-4522-8dee-24630ef291b1) with 1 sample withdrawn and 0 new samples (samples in the withdrawn file that aren't in sample_info); C) Example [Run](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/c7dfc547-305c-4b31-aec8-685494b92221) with 1 sample withdrawn and 1 new sample (sample in the withdrawn file that wasn't in sample_info). This run was run with the override flag allowing it to pass; D) Example [Run](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/c2d2c897-ec1b-47ad-9927-70c6d7eb7e9b) with 1 sample withdrawn and 1 new sample (sample in the withdrawn file that wasn't in sample_info). This run failed (as intended)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8599
https://github.com/broadinstitute/gatk/pull/8602:39,Availability,Echo,Echo,39,Lessons learned in VDS creation during Echo Scale Testing. Successful integration run [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/9e6aa362-e25b-49d0-83cd-d64e926c6386).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8602
https://github.com/broadinstitute/gatk/pull/8602:70,Deployability,integrat,integration,70,Lessons learned in VDS creation during Echo Scale Testing. Successful integration run [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/9e6aa362-e25b-49d0-83cd-d64e926c6386).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8602
https://github.com/broadinstitute/gatk/pull/8602:70,Integrability,integrat,integration,70,Lessons learned in VDS creation during Echo Scale Testing. Successful integration run [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/9e6aa362-e25b-49d0-83cd-d64e926c6386).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8602
https://github.com/broadinstitute/gatk/pull/8602:50,Testability,Test,Testing,50,Lessons learned in VDS creation during Echo Scale Testing. Successful integration run [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/9e6aa362-e25b-49d0-83cd-d64e926c6386).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8602
https://github.com/broadinstitute/gatk/pull/8602:8,Usability,learn,learned,8,Lessons learned in VDS creation during Echo Scale Testing. Successful integration run [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/9e6aa362-e25b-49d0-83cd-d64e926c6386).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8602
https://github.com/broadinstitute/gatk/pull/8603:31,Availability,error,error,31,This should work around a rare error in `_calculate_new_intervals` that could generate invalid partitioners in a way that; `calculate_new_intervals` cannot.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8603
https://github.com/broadinstitute/gatk/pull/8605:28,Testability,test,tests,28,I should have looked at the tests... @rickymagner,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8605
https://github.com/broadinstitute/gatk/pull/8607:0,Integrability,Depend,Depends,0,Depends on https://github.com/broadinstitute/gatk/pull/8606,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8607
https://github.com/broadinstitute/gatk/issues/8608:240,Deployability,pipeline,pipeline,240,"`ValidateVariants` requires a large amount of memory (>16Gb) to validate a GVCF when another GVCF is used as the interval list. This is not the case if a regular interval list is used instead. This comes up in the production `ReblockGVCFs` pipeline since we validate the reblocked GVCF using the input (unreblocked) GVCF as the interval list to validate over (with `-L`). For now we can just use larger memory machines to run this tool, but it is confusing to me why using a ~4Gb GVCF as an interval list would cause such a large increase in memory requirement.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8608
https://github.com/broadinstitute/gatk/issues/8608:1,Security,Validat,ValidateVariants,1,"`ValidateVariants` requires a large amount of memory (>16Gb) to validate a GVCF when another GVCF is used as the interval list. This is not the case if a regular interval list is used instead. This comes up in the production `ReblockGVCFs` pipeline since we validate the reblocked GVCF using the input (unreblocked) GVCF as the interval list to validate over (with `-L`). For now we can just use larger memory machines to run this tool, but it is confusing to me why using a ~4Gb GVCF as an interval list would cause such a large increase in memory requirement.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8608
https://github.com/broadinstitute/gatk/issues/8608:64,Security,validat,validate,64,"`ValidateVariants` requires a large amount of memory (>16Gb) to validate a GVCF when another GVCF is used as the interval list. This is not the case if a regular interval list is used instead. This comes up in the production `ReblockGVCFs` pipeline since we validate the reblocked GVCF using the input (unreblocked) GVCF as the interval list to validate over (with `-L`). For now we can just use larger memory machines to run this tool, but it is confusing to me why using a ~4Gb GVCF as an interval list would cause such a large increase in memory requirement.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8608
https://github.com/broadinstitute/gatk/issues/8608:258,Security,validat,validate,258,"`ValidateVariants` requires a large amount of memory (>16Gb) to validate a GVCF when another GVCF is used as the interval list. This is not the case if a regular interval list is used instead. This comes up in the production `ReblockGVCFs` pipeline since we validate the reblocked GVCF using the input (unreblocked) GVCF as the interval list to validate over (with `-L`). For now we can just use larger memory machines to run this tool, but it is confusing to me why using a ~4Gb GVCF as an interval list would cause such a large increase in memory requirement.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8608
https://github.com/broadinstitute/gatk/issues/8608:345,Security,validat,validate,345,"`ValidateVariants` requires a large amount of memory (>16Gb) to validate a GVCF when another GVCF is used as the interval list. This is not the case if a regular interval list is used instead. This comes up in the production `ReblockGVCFs` pipeline since we validate the reblocked GVCF using the input (unreblocked) GVCF as the interval list to validate over (with `-L`). For now we can just use larger memory machines to run this tool, but it is confusing to me why using a ~4Gb GVCF as an interval list would cause such a large increase in memory requirement.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8608
https://github.com/broadinstitute/gatk/issues/8612:46,Testability,test,tests,46,We need a way to read files from azure in our tests. This means one of: ; 1. adding a service account (or whatever the azure equivalent is) and the ability to generate SAS tokens; 2. finding a way to make a SAS token that has a very long expiration time,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8612
https://github.com/broadinstitute/gatk/pull/8615:630,Deployability,integrat,integration,630,[10 sample run](https://app.terra.bio/#workspaces/gvs-dev/GVS_190k_Exomes/job_history/10d083c7-2c20-4339-aa2b-70945056de44); [5k sample run](https://app.terra.bio/#workspaces/gvs-dev/GVS_190k_Exomes/job_history/60325b52-7040-492b-82e1-23a58850fb59); [20k sample run](https://app.terra.bio/#workspaces/gvs-dev/GVS_190k_Exomes/job_history/7fee1590-d00c-4038-a575-fd06a9edba1a); [50k sample run](https://app.terra.bio/#workspaces/gvs-dev/GVS_190k_Exomes/job_history/2efb51a2-105a-4f62-a1fe-26b7350706ed); [100k sample run](https://app.terra.bio/#workspaces/gvs-dev/GVS_190k_Exomes/job_history/dba1e793-51e4-47cc-9016-c478628e7252); [integration run](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/19c865ab-69bc-45a3-a897-ff4e5f3d2a53),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8615
https://github.com/broadinstitute/gatk/pull/8615:630,Integrability,integrat,integration,630,[10 sample run](https://app.terra.bio/#workspaces/gvs-dev/GVS_190k_Exomes/job_history/10d083c7-2c20-4339-aa2b-70945056de44); [5k sample run](https://app.terra.bio/#workspaces/gvs-dev/GVS_190k_Exomes/job_history/60325b52-7040-492b-82e1-23a58850fb59); [20k sample run](https://app.terra.bio/#workspaces/gvs-dev/GVS_190k_Exomes/job_history/7fee1590-d00c-4038-a575-fd06a9edba1a); [50k sample run](https://app.terra.bio/#workspaces/gvs-dev/GVS_190k_Exomes/job_history/2efb51a2-105a-4f62-a1fe-26b7350706ed); [100k sample run](https://app.terra.bio/#workspaces/gvs-dev/GVS_190k_Exomes/job_history/dba1e793-51e4-47cc-9016-c478628e7252); [integration run](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/19c865ab-69bc-45a3-a897-ff4e5f3d2a53),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8615
https://github.com/broadinstitute/gatk/pull/8616:2049,Availability,fault,faulty,2049,"ype?; - The above is especially thorny for haplotypes that exhibit multiple variants.; - The FRD prior is only defined for individual events, not haplotypes.; - The BQD and FRD models use reads that overlap a variant site, but it is not clear how to use reads that only partially intersect a haplotype.; - BQD and FRD likelihoods are only defined for homozygous haplotypes, but heterozygous combinations of _haplotypes_ contribute to homozygous genotypes all loci where the distinct haplotypes agree. Clearly, generalizing BQD and FRD to entire haplotypes is not straightforward. Nor does it suffice to produce ""raw"" genotype likelihoods using the joint detection approach and then apply BQD and FRD on variant loci afterwards. Some difficulties with this include:. - BQD and FRD require the read-allele likelihoods matrix. Where are these likelihoods supposed to come from? The pre-joint-detection unrigorous ""marginalization"" where to each allele we assign the maximum likelihood over all haplotypes supporting that allele? Some read-allele likelihoods matrix derived from the read-haplotype likelihoods matrix?; - The drawbacks of the faulty ""marginalization"" actually become more severe with joint detection since genotyping multiple alleles together in a single determined span produces more haplotypes, which in turn increases the risk of the read-allele likelihoods cherry-picking from too many different haplotypes for different reads.; - The BQD and FRD models produce likelihoods on an absolute scale that is only meaningful relative to genotyping likelihoods from the pre-joint-detection approach. They do not inherently ""play nicely"" with the posterior probabilities produced by joint detection.; - BQD and FRD as currently implemented in the GATK modify likelihoods _before_ applying a prior, whereas joint detection yields posterior probabilities. Are we supposed to somehow un-apply the prior to joint detection likelihoods, apply BQD and FRD, then re-apply the prior? It is not clear.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8616
https://github.com/broadinstitute/gatk/pull/8616:39,Safety,detect,detection,39,"Initial implementation of DRAGEN joint detection. Functional equivalence with respect to DRAGEN joint detection is actually worse than before due to many outstanding questions. . We currently have no idea how joint detection is supposed to interaction with BQD and FRD. I have tried a few guesses and none have worked (see below for functional equivalence results of the particular guess used in this PR). The interplay of joint detection with BQD and FRD is complicated for several reasons. Naively one would simply define the BDQ and FRD likelihoods on entire haplotypes rather than alleles at one locus. Unresolved difficulties with this include:. - BQD and FRD are defined with respect to one particular variant position. How would we define them for a haplotype that has no particular locus?; - BQD involves the base qualities at one particular variant locus, how would this be defined for an entire haplotype?; - The above is especially thorny for haplotypes that exhibit multiple variants.; - The FRD prior is only defined for individual events, not haplotypes.; - The BQD and FRD models use reads that overlap a variant site, but it is not clear how to use reads that only partially intersect a haplotype.; - BQD and FRD likelihoods are only defined for homozygous haplotypes, but heterozygous combinations of _haplotypes_ contribute to homozygous genotypes all loci where the distinct haplotypes agree. Clearly, generalizing BQD and FRD to entire haplotypes is not straightforward. Nor does it suffice to produce ""raw"" genotype likelihoods using the joint detection approach and then apply BQD and FRD on variant loci afterwards. Some difficulties with this include:. - BQD and FRD require the read-allele likelihoods matrix. Where are these likelihoods supposed to come from? The pre-joint-detection unrigorous ""marginalization"" where to each allele we assign the maximum likelihood over all haplotypes supporting that allele? Some read-allele likelihoods matrix derived from the read-haplot",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8616
https://github.com/broadinstitute/gatk/pull/8616:102,Safety,detect,detection,102,"Initial implementation of DRAGEN joint detection. Functional equivalence with respect to DRAGEN joint detection is actually worse than before due to many outstanding questions. . We currently have no idea how joint detection is supposed to interaction with BQD and FRD. I have tried a few guesses and none have worked (see below for functional equivalence results of the particular guess used in this PR). The interplay of joint detection with BQD and FRD is complicated for several reasons. Naively one would simply define the BDQ and FRD likelihoods on entire haplotypes rather than alleles at one locus. Unresolved difficulties with this include:. - BQD and FRD are defined with respect to one particular variant position. How would we define them for a haplotype that has no particular locus?; - BQD involves the base qualities at one particular variant locus, how would this be defined for an entire haplotype?; - The above is especially thorny for haplotypes that exhibit multiple variants.; - The FRD prior is only defined for individual events, not haplotypes.; - The BQD and FRD models use reads that overlap a variant site, but it is not clear how to use reads that only partially intersect a haplotype.; - BQD and FRD likelihoods are only defined for homozygous haplotypes, but heterozygous combinations of _haplotypes_ contribute to homozygous genotypes all loci where the distinct haplotypes agree. Clearly, generalizing BQD and FRD to entire haplotypes is not straightforward. Nor does it suffice to produce ""raw"" genotype likelihoods using the joint detection approach and then apply BQD and FRD on variant loci afterwards. Some difficulties with this include:. - BQD and FRD require the read-allele likelihoods matrix. Where are these likelihoods supposed to come from? The pre-joint-detection unrigorous ""marginalization"" where to each allele we assign the maximum likelihood over all haplotypes supporting that allele? Some read-allele likelihoods matrix derived from the read-haplot",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8616
https://github.com/broadinstitute/gatk/pull/8616:215,Safety,detect,detection,215,"Initial implementation of DRAGEN joint detection. Functional equivalence with respect to DRAGEN joint detection is actually worse than before due to many outstanding questions. . We currently have no idea how joint detection is supposed to interaction with BQD and FRD. I have tried a few guesses and none have worked (see below for functional equivalence results of the particular guess used in this PR). The interplay of joint detection with BQD and FRD is complicated for several reasons. Naively one would simply define the BDQ and FRD likelihoods on entire haplotypes rather than alleles at one locus. Unresolved difficulties with this include:. - BQD and FRD are defined with respect to one particular variant position. How would we define them for a haplotype that has no particular locus?; - BQD involves the base qualities at one particular variant locus, how would this be defined for an entire haplotype?; - The above is especially thorny for haplotypes that exhibit multiple variants.; - The FRD prior is only defined for individual events, not haplotypes.; - The BQD and FRD models use reads that overlap a variant site, but it is not clear how to use reads that only partially intersect a haplotype.; - BQD and FRD likelihoods are only defined for homozygous haplotypes, but heterozygous combinations of _haplotypes_ contribute to homozygous genotypes all loci where the distinct haplotypes agree. Clearly, generalizing BQD and FRD to entire haplotypes is not straightforward. Nor does it suffice to produce ""raw"" genotype likelihoods using the joint detection approach and then apply BQD and FRD on variant loci afterwards. Some difficulties with this include:. - BQD and FRD require the read-allele likelihoods matrix. Where are these likelihoods supposed to come from? The pre-joint-detection unrigorous ""marginalization"" where to each allele we assign the maximum likelihood over all haplotypes supporting that allele? Some read-allele likelihoods matrix derived from the read-haplot",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8616
https://github.com/broadinstitute/gatk/pull/8616:429,Safety,detect,detection,429,"Initial implementation of DRAGEN joint detection. Functional equivalence with respect to DRAGEN joint detection is actually worse than before due to many outstanding questions. . We currently have no idea how joint detection is supposed to interaction with BQD and FRD. I have tried a few guesses and none have worked (see below for functional equivalence results of the particular guess used in this PR). The interplay of joint detection with BQD and FRD is complicated for several reasons. Naively one would simply define the BDQ and FRD likelihoods on entire haplotypes rather than alleles at one locus. Unresolved difficulties with this include:. - BQD and FRD are defined with respect to one particular variant position. How would we define them for a haplotype that has no particular locus?; - BQD involves the base qualities at one particular variant locus, how would this be defined for an entire haplotype?; - The above is especially thorny for haplotypes that exhibit multiple variants.; - The FRD prior is only defined for individual events, not haplotypes.; - The BQD and FRD models use reads that overlap a variant site, but it is not clear how to use reads that only partially intersect a haplotype.; - BQD and FRD likelihoods are only defined for homozygous haplotypes, but heterozygous combinations of _haplotypes_ contribute to homozygous genotypes all loci where the distinct haplotypes agree. Clearly, generalizing BQD and FRD to entire haplotypes is not straightforward. Nor does it suffice to produce ""raw"" genotype likelihoods using the joint detection approach and then apply BQD and FRD on variant loci afterwards. Some difficulties with this include:. - BQD and FRD require the read-allele likelihoods matrix. Where are these likelihoods supposed to come from? The pre-joint-detection unrigorous ""marginalization"" where to each allele we assign the maximum likelihood over all haplotypes supporting that allele? Some read-allele likelihoods matrix derived from the read-haplot",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8616
https://github.com/broadinstitute/gatk/pull/8616:1565,Safety,detect,detection,1565,"plotypes rather than alleles at one locus. Unresolved difficulties with this include:. - BQD and FRD are defined with respect to one particular variant position. How would we define them for a haplotype that has no particular locus?; - BQD involves the base qualities at one particular variant locus, how would this be defined for an entire haplotype?; - The above is especially thorny for haplotypes that exhibit multiple variants.; - The FRD prior is only defined for individual events, not haplotypes.; - The BQD and FRD models use reads that overlap a variant site, but it is not clear how to use reads that only partially intersect a haplotype.; - BQD and FRD likelihoods are only defined for homozygous haplotypes, but heterozygous combinations of _haplotypes_ contribute to homozygous genotypes all loci where the distinct haplotypes agree. Clearly, generalizing BQD and FRD to entire haplotypes is not straightforward. Nor does it suffice to produce ""raw"" genotype likelihoods using the joint detection approach and then apply BQD and FRD on variant loci afterwards. Some difficulties with this include:. - BQD and FRD require the read-allele likelihoods matrix. Where are these likelihoods supposed to come from? The pre-joint-detection unrigorous ""marginalization"" where to each allele we assign the maximum likelihood over all haplotypes supporting that allele? Some read-allele likelihoods matrix derived from the read-haplotype likelihoods matrix?; - The drawbacks of the faulty ""marginalization"" actually become more severe with joint detection since genotyping multiple alleles together in a single determined span produces more haplotypes, which in turn increases the risk of the read-allele likelihoods cherry-picking from too many different haplotypes for different reads.; - The BQD and FRD models produce likelihoods on an absolute scale that is only meaningful relative to genotyping likelihoods from the pre-joint-detection approach. They do not inherently ""play nicely"" with th",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8616
https://github.com/broadinstitute/gatk/pull/8616:1800,Safety,detect,detection,1800,"ype?; - The above is especially thorny for haplotypes that exhibit multiple variants.; - The FRD prior is only defined for individual events, not haplotypes.; - The BQD and FRD models use reads that overlap a variant site, but it is not clear how to use reads that only partially intersect a haplotype.; - BQD and FRD likelihoods are only defined for homozygous haplotypes, but heterozygous combinations of _haplotypes_ contribute to homozygous genotypes all loci where the distinct haplotypes agree. Clearly, generalizing BQD and FRD to entire haplotypes is not straightforward. Nor does it suffice to produce ""raw"" genotype likelihoods using the joint detection approach and then apply BQD and FRD on variant loci afterwards. Some difficulties with this include:. - BQD and FRD require the read-allele likelihoods matrix. Where are these likelihoods supposed to come from? The pre-joint-detection unrigorous ""marginalization"" where to each allele we assign the maximum likelihood over all haplotypes supporting that allele? Some read-allele likelihoods matrix derived from the read-haplotype likelihoods matrix?; - The drawbacks of the faulty ""marginalization"" actually become more severe with joint detection since genotyping multiple alleles together in a single determined span produces more haplotypes, which in turn increases the risk of the read-allele likelihoods cherry-picking from too many different haplotypes for different reads.; - The BQD and FRD models produce likelihoods on an absolute scale that is only meaningful relative to genotyping likelihoods from the pre-joint-detection approach. They do not inherently ""play nicely"" with the posterior probabilities produced by joint detection.; - BQD and FRD as currently implemented in the GATK modify likelihoods _before_ applying a prior, whereas joint detection yields posterior probabilities. Are we supposed to somehow un-apply the prior to joint detection likelihoods, apply BQD and FRD, then re-apply the prior? It is not clear.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8616
https://github.com/broadinstitute/gatk/pull/8616:2113,Safety,detect,detection,2113,"ype?; - The above is especially thorny for haplotypes that exhibit multiple variants.; - The FRD prior is only defined for individual events, not haplotypes.; - The BQD and FRD models use reads that overlap a variant site, but it is not clear how to use reads that only partially intersect a haplotype.; - BQD and FRD likelihoods are only defined for homozygous haplotypes, but heterozygous combinations of _haplotypes_ contribute to homozygous genotypes all loci where the distinct haplotypes agree. Clearly, generalizing BQD and FRD to entire haplotypes is not straightforward. Nor does it suffice to produce ""raw"" genotype likelihoods using the joint detection approach and then apply BQD and FRD on variant loci afterwards. Some difficulties with this include:. - BQD and FRD require the read-allele likelihoods matrix. Where are these likelihoods supposed to come from? The pre-joint-detection unrigorous ""marginalization"" where to each allele we assign the maximum likelihood over all haplotypes supporting that allele? Some read-allele likelihoods matrix derived from the read-haplotype likelihoods matrix?; - The drawbacks of the faulty ""marginalization"" actually become more severe with joint detection since genotyping multiple alleles together in a single determined span produces more haplotypes, which in turn increases the risk of the read-allele likelihoods cherry-picking from too many different haplotypes for different reads.; - The BQD and FRD models produce likelihoods on an absolute scale that is only meaningful relative to genotyping likelihoods from the pre-joint-detection approach. They do not inherently ""play nicely"" with the posterior probabilities produced by joint detection.; - BQD and FRD as currently implemented in the GATK modify likelihoods _before_ applying a prior, whereas joint detection yields posterior probabilities. Are we supposed to somehow un-apply the prior to joint detection likelihoods, apply BQD and FRD, then re-apply the prior? It is not clear.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8616
https://github.com/broadinstitute/gatk/pull/8616:2248,Safety,risk,risk,2248,"ype?; - The above is especially thorny for haplotypes that exhibit multiple variants.; - The FRD prior is only defined for individual events, not haplotypes.; - The BQD and FRD models use reads that overlap a variant site, but it is not clear how to use reads that only partially intersect a haplotype.; - BQD and FRD likelihoods are only defined for homozygous haplotypes, but heterozygous combinations of _haplotypes_ contribute to homozygous genotypes all loci where the distinct haplotypes agree. Clearly, generalizing BQD and FRD to entire haplotypes is not straightforward. Nor does it suffice to produce ""raw"" genotype likelihoods using the joint detection approach and then apply BQD and FRD on variant loci afterwards. Some difficulties with this include:. - BQD and FRD require the read-allele likelihoods matrix. Where are these likelihoods supposed to come from? The pre-joint-detection unrigorous ""marginalization"" where to each allele we assign the maximum likelihood over all haplotypes supporting that allele? Some read-allele likelihoods matrix derived from the read-haplotype likelihoods matrix?; - The drawbacks of the faulty ""marginalization"" actually become more severe with joint detection since genotyping multiple alleles together in a single determined span produces more haplotypes, which in turn increases the risk of the read-allele likelihoods cherry-picking from too many different haplotypes for different reads.; - The BQD and FRD models produce likelihoods on an absolute scale that is only meaningful relative to genotyping likelihoods from the pre-joint-detection approach. They do not inherently ""play nicely"" with the posterior probabilities produced by joint detection.; - BQD and FRD as currently implemented in the GATK modify likelihoods _before_ applying a prior, whereas joint detection yields posterior probabilities. Are we supposed to somehow un-apply the prior to joint detection likelihoods, apply BQD and FRD, then re-apply the prior? It is not clear.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8616
https://github.com/broadinstitute/gatk/pull/8616:2500,Safety,detect,detection,2500,"ype?; - The above is especially thorny for haplotypes that exhibit multiple variants.; - The FRD prior is only defined for individual events, not haplotypes.; - The BQD and FRD models use reads that overlap a variant site, but it is not clear how to use reads that only partially intersect a haplotype.; - BQD and FRD likelihoods are only defined for homozygous haplotypes, but heterozygous combinations of _haplotypes_ contribute to homozygous genotypes all loci where the distinct haplotypes agree. Clearly, generalizing BQD and FRD to entire haplotypes is not straightforward. Nor does it suffice to produce ""raw"" genotype likelihoods using the joint detection approach and then apply BQD and FRD on variant loci afterwards. Some difficulties with this include:. - BQD and FRD require the read-allele likelihoods matrix. Where are these likelihoods supposed to come from? The pre-joint-detection unrigorous ""marginalization"" where to each allele we assign the maximum likelihood over all haplotypes supporting that allele? Some read-allele likelihoods matrix derived from the read-haplotype likelihoods matrix?; - The drawbacks of the faulty ""marginalization"" actually become more severe with joint detection since genotyping multiple alleles together in a single determined span produces more haplotypes, which in turn increases the risk of the read-allele likelihoods cherry-picking from too many different haplotypes for different reads.; - The BQD and FRD models produce likelihoods on an absolute scale that is only meaningful relative to genotyping likelihoods from the pre-joint-detection approach. They do not inherently ""play nicely"" with the posterior probabilities produced by joint detection.; - BQD and FRD as currently implemented in the GATK modify likelihoods _before_ applying a prior, whereas joint detection yields posterior probabilities. Are we supposed to somehow un-apply the prior to joint detection likelihoods, apply BQD and FRD, then re-apply the prior? It is not clear.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8616
https://github.com/broadinstitute/gatk/pull/8616:2608,Safety,detect,detection,2608,"ype?; - The above is especially thorny for haplotypes that exhibit multiple variants.; - The FRD prior is only defined for individual events, not haplotypes.; - The BQD and FRD models use reads that overlap a variant site, but it is not clear how to use reads that only partially intersect a haplotype.; - BQD and FRD likelihoods are only defined for homozygous haplotypes, but heterozygous combinations of _haplotypes_ contribute to homozygous genotypes all loci where the distinct haplotypes agree. Clearly, generalizing BQD and FRD to entire haplotypes is not straightforward. Nor does it suffice to produce ""raw"" genotype likelihoods using the joint detection approach and then apply BQD and FRD on variant loci afterwards. Some difficulties with this include:. - BQD and FRD require the read-allele likelihoods matrix. Where are these likelihoods supposed to come from? The pre-joint-detection unrigorous ""marginalization"" where to each allele we assign the maximum likelihood over all haplotypes supporting that allele? Some read-allele likelihoods matrix derived from the read-haplotype likelihoods matrix?; - The drawbacks of the faulty ""marginalization"" actually become more severe with joint detection since genotyping multiple alleles together in a single determined span produces more haplotypes, which in turn increases the risk of the read-allele likelihoods cherry-picking from too many different haplotypes for different reads.; - The BQD and FRD models produce likelihoods on an absolute scale that is only meaningful relative to genotyping likelihoods from the pre-joint-detection approach. They do not inherently ""play nicely"" with the posterior probabilities produced by joint detection.; - BQD and FRD as currently implemented in the GATK modify likelihoods _before_ applying a prior, whereas joint detection yields posterior probabilities. Are we supposed to somehow un-apply the prior to joint detection likelihoods, apply BQD and FRD, then re-apply the prior? It is not clear.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8616
https://github.com/broadinstitute/gatk/pull/8616:2731,Safety,detect,detection,2731,"ype?; - The above is especially thorny for haplotypes that exhibit multiple variants.; - The FRD prior is only defined for individual events, not haplotypes.; - The BQD and FRD models use reads that overlap a variant site, but it is not clear how to use reads that only partially intersect a haplotype.; - BQD and FRD likelihoods are only defined for homozygous haplotypes, but heterozygous combinations of _haplotypes_ contribute to homozygous genotypes all loci where the distinct haplotypes agree. Clearly, generalizing BQD and FRD to entire haplotypes is not straightforward. Nor does it suffice to produce ""raw"" genotype likelihoods using the joint detection approach and then apply BQD and FRD on variant loci afterwards. Some difficulties with this include:. - BQD and FRD require the read-allele likelihoods matrix. Where are these likelihoods supposed to come from? The pre-joint-detection unrigorous ""marginalization"" where to each allele we assign the maximum likelihood over all haplotypes supporting that allele? Some read-allele likelihoods matrix derived from the read-haplotype likelihoods matrix?; - The drawbacks of the faulty ""marginalization"" actually become more severe with joint detection since genotyping multiple alleles together in a single determined span produces more haplotypes, which in turn increases the risk of the read-allele likelihoods cherry-picking from too many different haplotypes for different reads.; - The BQD and FRD models produce likelihoods on an absolute scale that is only meaningful relative to genotyping likelihoods from the pre-joint-detection approach. They do not inherently ""play nicely"" with the posterior probabilities produced by joint detection.; - BQD and FRD as currently implemented in the GATK modify likelihoods _before_ applying a prior, whereas joint detection yields posterior probabilities. Are we supposed to somehow un-apply the prior to joint detection likelihoods, apply BQD and FRD, then re-apply the prior? It is not clear.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8616
https://github.com/broadinstitute/gatk/pull/8616:2828,Safety,detect,detection,2828,"ype?; - The above is especially thorny for haplotypes that exhibit multiple variants.; - The FRD prior is only defined for individual events, not haplotypes.; - The BQD and FRD models use reads that overlap a variant site, but it is not clear how to use reads that only partially intersect a haplotype.; - BQD and FRD likelihoods are only defined for homozygous haplotypes, but heterozygous combinations of _haplotypes_ contribute to homozygous genotypes all loci where the distinct haplotypes agree. Clearly, generalizing BQD and FRD to entire haplotypes is not straightforward. Nor does it suffice to produce ""raw"" genotype likelihoods using the joint detection approach and then apply BQD and FRD on variant loci afterwards. Some difficulties with this include:. - BQD and FRD require the read-allele likelihoods matrix. Where are these likelihoods supposed to come from? The pre-joint-detection unrigorous ""marginalization"" where to each allele we assign the maximum likelihood over all haplotypes supporting that allele? Some read-allele likelihoods matrix derived from the read-haplotype likelihoods matrix?; - The drawbacks of the faulty ""marginalization"" actually become more severe with joint detection since genotyping multiple alleles together in a single determined span produces more haplotypes, which in turn increases the risk of the read-allele likelihoods cherry-picking from too many different haplotypes for different reads.; - The BQD and FRD models produce likelihoods on an absolute scale that is only meaningful relative to genotyping likelihoods from the pre-joint-detection approach. They do not inherently ""play nicely"" with the posterior probabilities produced by joint detection.; - BQD and FRD as currently implemented in the GATK modify likelihoods _before_ applying a prior, whereas joint detection yields posterior probabilities. Are we supposed to somehow un-apply the prior to joint detection likelihoods, apply BQD and FRD, then re-apply the prior? It is not clear.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8616
https://github.com/broadinstitute/gatk/pull/8616:510,Usability,simpl,simply,510,"Initial implementation of DRAGEN joint detection. Functional equivalence with respect to DRAGEN joint detection is actually worse than before due to many outstanding questions. . We currently have no idea how joint detection is supposed to interaction with BQD and FRD. I have tried a few guesses and none have worked (see below for functional equivalence results of the particular guess used in this PR). The interplay of joint detection with BQD and FRD is complicated for several reasons. Naively one would simply define the BDQ and FRD likelihoods on entire haplotypes rather than alleles at one locus. Unresolved difficulties with this include:. - BQD and FRD are defined with respect to one particular variant position. How would we define them for a haplotype that has no particular locus?; - BQD involves the base qualities at one particular variant locus, how would this be defined for an entire haplotype?; - The above is especially thorny for haplotypes that exhibit multiple variants.; - The FRD prior is only defined for individual events, not haplotypes.; - The BQD and FRD models use reads that overlap a variant site, but it is not clear how to use reads that only partially intersect a haplotype.; - BQD and FRD likelihoods are only defined for homozygous haplotypes, but heterozygous combinations of _haplotypes_ contribute to homozygous genotypes all loci where the distinct haplotypes agree. Clearly, generalizing BQD and FRD to entire haplotypes is not straightforward. Nor does it suffice to produce ""raw"" genotype likelihoods using the joint detection approach and then apply BQD and FRD on variant loci afterwards. Some difficulties with this include:. - BQD and FRD require the read-allele likelihoods matrix. Where are these likelihoods supposed to come from? The pre-joint-detection unrigorous ""marginalization"" where to each allele we assign the maximum likelihood over all haplotypes supporting that allele? Some read-allele likelihoods matrix derived from the read-haplot",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8616
https://github.com/broadinstitute/gatk/pull/8616:1148,Usability,clear,clear,1148,"e due to many outstanding questions. . We currently have no idea how joint detection is supposed to interaction with BQD and FRD. I have tried a few guesses and none have worked (see below for functional equivalence results of the particular guess used in this PR). The interplay of joint detection with BQD and FRD is complicated for several reasons. Naively one would simply define the BDQ and FRD likelihoods on entire haplotypes rather than alleles at one locus. Unresolved difficulties with this include:. - BQD and FRD are defined with respect to one particular variant position. How would we define them for a haplotype that has no particular locus?; - BQD involves the base qualities at one particular variant locus, how would this be defined for an entire haplotype?; - The above is especially thorny for haplotypes that exhibit multiple variants.; - The FRD prior is only defined for individual events, not haplotypes.; - The BQD and FRD models use reads that overlap a variant site, but it is not clear how to use reads that only partially intersect a haplotype.; - BQD and FRD likelihoods are only defined for homozygous haplotypes, but heterozygous combinations of _haplotypes_ contribute to homozygous genotypes all loci where the distinct haplotypes agree. Clearly, generalizing BQD and FRD to entire haplotypes is not straightforward. Nor does it suffice to produce ""raw"" genotype likelihoods using the joint detection approach and then apply BQD and FRD on variant loci afterwards. Some difficulties with this include:. - BQD and FRD require the read-allele likelihoods matrix. Where are these likelihoods supposed to come from? The pre-joint-detection unrigorous ""marginalization"" where to each allele we assign the maximum likelihood over all haplotypes supporting that allele? Some read-allele likelihoods matrix derived from the read-haplotype likelihoods matrix?; - The drawbacks of the faulty ""marginalization"" actually become more severe with joint detection since genotyping m",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8616
https://github.com/broadinstitute/gatk/pull/8616:1412,Usability,Clear,Clearly,1412,"d FRD is complicated for several reasons. Naively one would simply define the BDQ and FRD likelihoods on entire haplotypes rather than alleles at one locus. Unresolved difficulties with this include:. - BQD and FRD are defined with respect to one particular variant position. How would we define them for a haplotype that has no particular locus?; - BQD involves the base qualities at one particular variant locus, how would this be defined for an entire haplotype?; - The above is especially thorny for haplotypes that exhibit multiple variants.; - The FRD prior is only defined for individual events, not haplotypes.; - The BQD and FRD models use reads that overlap a variant site, but it is not clear how to use reads that only partially intersect a haplotype.; - BQD and FRD likelihoods are only defined for homozygous haplotypes, but heterozygous combinations of _haplotypes_ contribute to homozygous genotypes all loci where the distinct haplotypes agree. Clearly, generalizing BQD and FRD to entire haplotypes is not straightforward. Nor does it suffice to produce ""raw"" genotype likelihoods using the joint detection approach and then apply BQD and FRD on variant loci afterwards. Some difficulties with this include:. - BQD and FRD require the read-allele likelihoods matrix. Where are these likelihoods supposed to come from? The pre-joint-detection unrigorous ""marginalization"" where to each allele we assign the maximum likelihood over all haplotypes supporting that allele? Some read-allele likelihoods matrix derived from the read-haplotype likelihoods matrix?; - The drawbacks of the faulty ""marginalization"" actually become more severe with joint detection since genotyping multiple alleles together in a single determined span produces more haplotypes, which in turn increases the risk of the read-allele likelihoods cherry-picking from too many different haplotypes for different reads.; - The BQD and FRD models produce likelihoods on an absolute scale that is only meaningful relat",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8616
https://github.com/broadinstitute/gatk/pull/8616:2905,Usability,clear,clear,2905,"ype?; - The above is especially thorny for haplotypes that exhibit multiple variants.; - The FRD prior is only defined for individual events, not haplotypes.; - The BQD and FRD models use reads that overlap a variant site, but it is not clear how to use reads that only partially intersect a haplotype.; - BQD and FRD likelihoods are only defined for homozygous haplotypes, but heterozygous combinations of _haplotypes_ contribute to homozygous genotypes all loci where the distinct haplotypes agree. Clearly, generalizing BQD and FRD to entire haplotypes is not straightforward. Nor does it suffice to produce ""raw"" genotype likelihoods using the joint detection approach and then apply BQD and FRD on variant loci afterwards. Some difficulties with this include:. - BQD and FRD require the read-allele likelihoods matrix. Where are these likelihoods supposed to come from? The pre-joint-detection unrigorous ""marginalization"" where to each allele we assign the maximum likelihood over all haplotypes supporting that allele? Some read-allele likelihoods matrix derived from the read-haplotype likelihoods matrix?; - The drawbacks of the faulty ""marginalization"" actually become more severe with joint detection since genotyping multiple alleles together in a single determined span produces more haplotypes, which in turn increases the risk of the read-allele likelihoods cherry-picking from too many different haplotypes for different reads.; - The BQD and FRD models produce likelihoods on an absolute scale that is only meaningful relative to genotyping likelihoods from the pre-joint-detection approach. They do not inherently ""play nicely"" with the posterior probabilities produced by joint detection.; - BQD and FRD as currently implemented in the GATK modify likelihoods _before_ applying a prior, whereas joint detection yields posterior probabilities. Are we supposed to somehow un-apply the prior to joint detection likelihoods, apply BQD and FRD, then re-apply the prior? It is not clear.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8616
https://github.com/broadinstitute/gatk/issues/8618:2300,Availability,error,error,2300,"d in directory: /tmp/pip-ephem-wheel-cache-ecx6e_m0/wheels/06/f7/e1/87cb7da6f705baa602256a58c9514b47dc313aade8809a01da; Successfully built gatkpythonpackages; Installing collected packages: gatkpythonpackages; Successfully installed gatkpythonpackages-0.1. done; #; # To activate this environment, use; #; # $ conda activate gatk; #; # To deactivate an active environment, use; #; # $ conda deactivate. ```. #### Actual behavior; ```sh; root@d12ac7710afc:/soft/gatk-4.4.0.0# conda --version; conda 23.10.0; root@d12ac7710afc:/soft/gatk-4.4.0.0# ""$CONDA"" env create -n gatk -f ""$SOFT/gatk-${GATK_VERSION}/gatkcondaenv.yml""; ...; Preparing transaction: done; Verifying transaction: done; Executing transaction: done; Installing pip dependencies: | Ran pip subprocess with arguments:; ['/opt/miniconda/envs/gatk/bin/python', '-m', 'pip', 'install', '-U', '-r', '/soft/gatk-4.4.0.0/condaenv.i9brvcrk.requirements.txt', '--exists-action=b']; Pip subprocess output:. Pip subprocess error:; /opt/miniconda/envs/gatk/bin/python: No module named pip. failed. CondaEnvException: Pip failed. ```; ---; It can be fixed with setting classic colver:; ```; root@d12ac7710afc:/soft/gatk-4.4.0.0# conda --version; conda 23.10.0; root@d12ac7710afc:/soft/gatk-4.4.0.0# conda config --set solver classic; root@d12ac7710afc:/soft/gatk-4.4.0.0# ""$CONDA"" env create -n gatk -f ""$SOFT/gatk-${GATK_VERSION}/gatkcondaenv.yml""; ...; Preparing transaction: done; Verifying transaction: done; Executing transaction: done; Installing pip dependencies: \ Ran pip subprocess with arguments:; ['/opt/miniconda/envs/gatk/bin/python', '-m', 'pip', 'install', '-U', '-r', '/soft/gatk-4.4.0.0/condaenv.rtsyg5rl.requirements.txt', '--exists-action=b']; Pip subprocess output:; Processing ./gatkPythonPackageArchive.zip; Building wheels for collected packages: gatkpythonpackages; Building wheel for gatkpythonpackages (setup.py): started; Building wheel for gatkpythonpackages (setup.py): finished with status 'done'; Created wheel for gat",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8618
https://github.com/broadinstitute/gatk/issues/8618:55,Deployability,install,installation,55,"## Bug Report. ### Affected tool(s) or class(es); GATK installation. ### Affected version(s); - [x] Latest public release version 4.4.0.0; - [x] Latest master branch as of [11.12.2023]. ### Description ; Latest `conda` versions cannot install pip packages. #### Steps to reproduce; Create `gatk` `env` with `conda` version equal or newer than `23.10`. #### Expected behavior; ```sh; root@d12ac7710afc:/soft/gatk-4.4.0.0# conda --version; conda 23.9.0; root@d12ac7710afc:/soft/gatk-4.4.0.0# ""$CONDA"" env create -n gatk -f ""$SOFT/gatk-${GATK_VERSION}/gatkcondaenv.yml""; ...; Preparing transaction: done; Verifying transaction: done; Executing transaction: done; Installing pip dependencies: - Ran pip subprocess with arguments:; ['/opt/miniconda/envs/gatk/bin/python', '-m', 'pip', 'install', '-U', '-r', '/soft/gatk-4.4.0.0/condaenv.vn0sukco.requirements.txt', '--exists-action=b']; Pip subprocess output:; Processing ./gatkPythonPackageArchive.zip; Building wheels for collected packages: gatkpythonpackages; Building wheel for gatkpythonpackages (setup.py): started; Building wheel for gatkpythonpackages (setup.py): finished with status 'done'; Created wheel for gatkpythonpackages: filename=gatkpythonpackages-0.1-py3-none-any.whl size=117686 sha256=8095375e139fa0729c7a41c8f5e8a43281fc1b6859b6d3951d3bfba7296ee349; Stored in directory: /tmp/pip-ephem-wheel-cache-ecx6e_m0/wheels/06/f7/e1/87cb7da6f705baa602256a58c9514b47dc313aade8809a01da; Successfully built gatkpythonpackages; Installing collected packages: gatkpythonpackages; Successfully installed gatkpythonpackages-0.1. done; #; # To activate this environment, use; #; # $ conda activate gatk; #; # To deactivate an active environment, use; #; # $ conda deactivate. ```. #### Actual behavior; ```sh; root@d12ac7710afc:/soft/gatk-4.4.0.0# conda --version; conda 23.10.0; root@d12ac7710afc:/soft/gatk-4.4.0.0# ""$CONDA"" env create -n gatk -f ""$SOFT/gatk-${GATK_VERSION}/gatkcondaenv.yml""; ...; Preparing transaction: done; Verifying transactio",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8618
https://github.com/broadinstitute/gatk/issues/8618:114,Deployability,release,release,114,"## Bug Report. ### Affected tool(s) or class(es); GATK installation. ### Affected version(s); - [x] Latest public release version 4.4.0.0; - [x] Latest master branch as of [11.12.2023]. ### Description ; Latest `conda` versions cannot install pip packages. #### Steps to reproduce; Create `gatk` `env` with `conda` version equal or newer than `23.10`. #### Expected behavior; ```sh; root@d12ac7710afc:/soft/gatk-4.4.0.0# conda --version; conda 23.9.0; root@d12ac7710afc:/soft/gatk-4.4.0.0# ""$CONDA"" env create -n gatk -f ""$SOFT/gatk-${GATK_VERSION}/gatkcondaenv.yml""; ...; Preparing transaction: done; Verifying transaction: done; Executing transaction: done; Installing pip dependencies: - Ran pip subprocess with arguments:; ['/opt/miniconda/envs/gatk/bin/python', '-m', 'pip', 'install', '-U', '-r', '/soft/gatk-4.4.0.0/condaenv.vn0sukco.requirements.txt', '--exists-action=b']; Pip subprocess output:; Processing ./gatkPythonPackageArchive.zip; Building wheels for collected packages: gatkpythonpackages; Building wheel for gatkpythonpackages (setup.py): started; Building wheel for gatkpythonpackages (setup.py): finished with status 'done'; Created wheel for gatkpythonpackages: filename=gatkpythonpackages-0.1-py3-none-any.whl size=117686 sha256=8095375e139fa0729c7a41c8f5e8a43281fc1b6859b6d3951d3bfba7296ee349; Stored in directory: /tmp/pip-ephem-wheel-cache-ecx6e_m0/wheels/06/f7/e1/87cb7da6f705baa602256a58c9514b47dc313aade8809a01da; Successfully built gatkpythonpackages; Installing collected packages: gatkpythonpackages; Successfully installed gatkpythonpackages-0.1. done; #; # To activate this environment, use; #; # $ conda activate gatk; #; # To deactivate an active environment, use; #; # $ conda deactivate. ```. #### Actual behavior; ```sh; root@d12ac7710afc:/soft/gatk-4.4.0.0# conda --version; conda 23.10.0; root@d12ac7710afc:/soft/gatk-4.4.0.0# ""$CONDA"" env create -n gatk -f ""$SOFT/gatk-${GATK_VERSION}/gatkcondaenv.yml""; ...; Preparing transaction: done; Verifying transactio",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8618
https://github.com/broadinstitute/gatk/issues/8618:235,Deployability,install,install,235,"## Bug Report. ### Affected tool(s) or class(es); GATK installation. ### Affected version(s); - [x] Latest public release version 4.4.0.0; - [x] Latest master branch as of [11.12.2023]. ### Description ; Latest `conda` versions cannot install pip packages. #### Steps to reproduce; Create `gatk` `env` with `conda` version equal or newer than `23.10`. #### Expected behavior; ```sh; root@d12ac7710afc:/soft/gatk-4.4.0.0# conda --version; conda 23.9.0; root@d12ac7710afc:/soft/gatk-4.4.0.0# ""$CONDA"" env create -n gatk -f ""$SOFT/gatk-${GATK_VERSION}/gatkcondaenv.yml""; ...; Preparing transaction: done; Verifying transaction: done; Executing transaction: done; Installing pip dependencies: - Ran pip subprocess with arguments:; ['/opt/miniconda/envs/gatk/bin/python', '-m', 'pip', 'install', '-U', '-r', '/soft/gatk-4.4.0.0/condaenv.vn0sukco.requirements.txt', '--exists-action=b']; Pip subprocess output:; Processing ./gatkPythonPackageArchive.zip; Building wheels for collected packages: gatkpythonpackages; Building wheel for gatkpythonpackages (setup.py): started; Building wheel for gatkpythonpackages (setup.py): finished with status 'done'; Created wheel for gatkpythonpackages: filename=gatkpythonpackages-0.1-py3-none-any.whl size=117686 sha256=8095375e139fa0729c7a41c8f5e8a43281fc1b6859b6d3951d3bfba7296ee349; Stored in directory: /tmp/pip-ephem-wheel-cache-ecx6e_m0/wheels/06/f7/e1/87cb7da6f705baa602256a58c9514b47dc313aade8809a01da; Successfully built gatkpythonpackages; Installing collected packages: gatkpythonpackages; Successfully installed gatkpythonpackages-0.1. done; #; # To activate this environment, use; #; # $ conda activate gatk; #; # To deactivate an active environment, use; #; # $ conda deactivate. ```. #### Actual behavior; ```sh; root@d12ac7710afc:/soft/gatk-4.4.0.0# conda --version; conda 23.10.0; root@d12ac7710afc:/soft/gatk-4.4.0.0# ""$CONDA"" env create -n gatk -f ""$SOFT/gatk-${GATK_VERSION}/gatkcondaenv.yml""; ...; Preparing transaction: done; Verifying transactio",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8618
https://github.com/broadinstitute/gatk/issues/8618:660,Deployability,Install,Installing,660,"## Bug Report. ### Affected tool(s) or class(es); GATK installation. ### Affected version(s); - [x] Latest public release version 4.4.0.0; - [x] Latest master branch as of [11.12.2023]. ### Description ; Latest `conda` versions cannot install pip packages. #### Steps to reproduce; Create `gatk` `env` with `conda` version equal or newer than `23.10`. #### Expected behavior; ```sh; root@d12ac7710afc:/soft/gatk-4.4.0.0# conda --version; conda 23.9.0; root@d12ac7710afc:/soft/gatk-4.4.0.0# ""$CONDA"" env create -n gatk -f ""$SOFT/gatk-${GATK_VERSION}/gatkcondaenv.yml""; ...; Preparing transaction: done; Verifying transaction: done; Executing transaction: done; Installing pip dependencies: - Ran pip subprocess with arguments:; ['/opt/miniconda/envs/gatk/bin/python', '-m', 'pip', 'install', '-U', '-r', '/soft/gatk-4.4.0.0/condaenv.vn0sukco.requirements.txt', '--exists-action=b']; Pip subprocess output:; Processing ./gatkPythonPackageArchive.zip; Building wheels for collected packages: gatkpythonpackages; Building wheel for gatkpythonpackages (setup.py): started; Building wheel for gatkpythonpackages (setup.py): finished with status 'done'; Created wheel for gatkpythonpackages: filename=gatkpythonpackages-0.1-py3-none-any.whl size=117686 sha256=8095375e139fa0729c7a41c8f5e8a43281fc1b6859b6d3951d3bfba7296ee349; Stored in directory: /tmp/pip-ephem-wheel-cache-ecx6e_m0/wheels/06/f7/e1/87cb7da6f705baa602256a58c9514b47dc313aade8809a01da; Successfully built gatkpythonpackages; Installing collected packages: gatkpythonpackages; Successfully installed gatkpythonpackages-0.1. done; #; # To activate this environment, use; #; # $ conda activate gatk; #; # To deactivate an active environment, use; #; # $ conda deactivate. ```. #### Actual behavior; ```sh; root@d12ac7710afc:/soft/gatk-4.4.0.0# conda --version; conda 23.10.0; root@d12ac7710afc:/soft/gatk-4.4.0.0# ""$CONDA"" env create -n gatk -f ""$SOFT/gatk-${GATK_VERSION}/gatkcondaenv.yml""; ...; Preparing transaction: done; Verifying transactio",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8618
https://github.com/broadinstitute/gatk/issues/8618:781,Deployability,install,install,781,"## Bug Report. ### Affected tool(s) or class(es); GATK installation. ### Affected version(s); - [x] Latest public release version 4.4.0.0; - [x] Latest master branch as of [11.12.2023]. ### Description ; Latest `conda` versions cannot install pip packages. #### Steps to reproduce; Create `gatk` `env` with `conda` version equal or newer than `23.10`. #### Expected behavior; ```sh; root@d12ac7710afc:/soft/gatk-4.4.0.0# conda --version; conda 23.9.0; root@d12ac7710afc:/soft/gatk-4.4.0.0# ""$CONDA"" env create -n gatk -f ""$SOFT/gatk-${GATK_VERSION}/gatkcondaenv.yml""; ...; Preparing transaction: done; Verifying transaction: done; Executing transaction: done; Installing pip dependencies: - Ran pip subprocess with arguments:; ['/opt/miniconda/envs/gatk/bin/python', '-m', 'pip', 'install', '-U', '-r', '/soft/gatk-4.4.0.0/condaenv.vn0sukco.requirements.txt', '--exists-action=b']; Pip subprocess output:; Processing ./gatkPythonPackageArchive.zip; Building wheels for collected packages: gatkpythonpackages; Building wheel for gatkpythonpackages (setup.py): started; Building wheel for gatkpythonpackages (setup.py): finished with status 'done'; Created wheel for gatkpythonpackages: filename=gatkpythonpackages-0.1-py3-none-any.whl size=117686 sha256=8095375e139fa0729c7a41c8f5e8a43281fc1b6859b6d3951d3bfba7296ee349; Stored in directory: /tmp/pip-ephem-wheel-cache-ecx6e_m0/wheels/06/f7/e1/87cb7da6f705baa602256a58c9514b47dc313aade8809a01da; Successfully built gatkpythonpackages; Installing collected packages: gatkpythonpackages; Successfully installed gatkpythonpackages-0.1. done; #; # To activate this environment, use; #; # $ conda activate gatk; #; # To deactivate an active environment, use; #; # $ conda deactivate. ```. #### Actual behavior; ```sh; root@d12ac7710afc:/soft/gatk-4.4.0.0# conda --version; conda 23.10.0; root@d12ac7710afc:/soft/gatk-4.4.0.0# ""$CONDA"" env create -n gatk -f ""$SOFT/gatk-${GATK_VERSION}/gatkcondaenv.yml""; ...; Preparing transaction: done; Verifying transactio",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8618
https://github.com/broadinstitute/gatk/issues/8618:1483,Deployability,Install,Installing,1483,"ft/gatk-4.4.0.0# conda --version; conda 23.9.0; root@d12ac7710afc:/soft/gatk-4.4.0.0# ""$CONDA"" env create -n gatk -f ""$SOFT/gatk-${GATK_VERSION}/gatkcondaenv.yml""; ...; Preparing transaction: done; Verifying transaction: done; Executing transaction: done; Installing pip dependencies: - Ran pip subprocess with arguments:; ['/opt/miniconda/envs/gatk/bin/python', '-m', 'pip', 'install', '-U', '-r', '/soft/gatk-4.4.0.0/condaenv.vn0sukco.requirements.txt', '--exists-action=b']; Pip subprocess output:; Processing ./gatkPythonPackageArchive.zip; Building wheels for collected packages: gatkpythonpackages; Building wheel for gatkpythonpackages (setup.py): started; Building wheel for gatkpythonpackages (setup.py): finished with status 'done'; Created wheel for gatkpythonpackages: filename=gatkpythonpackages-0.1-py3-none-any.whl size=117686 sha256=8095375e139fa0729c7a41c8f5e8a43281fc1b6859b6d3951d3bfba7296ee349; Stored in directory: /tmp/pip-ephem-wheel-cache-ecx6e_m0/wheels/06/f7/e1/87cb7da6f705baa602256a58c9514b47dc313aade8809a01da; Successfully built gatkpythonpackages; Installing collected packages: gatkpythonpackages; Successfully installed gatkpythonpackages-0.1. done; #; # To activate this environment, use; #; # $ conda activate gatk; #; # To deactivate an active environment, use; #; # $ conda deactivate. ```. #### Actual behavior; ```sh; root@d12ac7710afc:/soft/gatk-4.4.0.0# conda --version; conda 23.10.0; root@d12ac7710afc:/soft/gatk-4.4.0.0# ""$CONDA"" env create -n gatk -f ""$SOFT/gatk-${GATK_VERSION}/gatkcondaenv.yml""; ...; Preparing transaction: done; Verifying transaction: done; Executing transaction: done; Installing pip dependencies: | Ran pip subprocess with arguments:; ['/opt/miniconda/envs/gatk/bin/python', '-m', 'pip', 'install', '-U', '-r', '/soft/gatk-4.4.0.0/condaenv.i9brvcrk.requirements.txt', '--exists-action=b']; Pip subprocess output:. Pip subprocess error:; /opt/miniconda/envs/gatk/bin/python: No module named pip. failed. CondaEnvException: Pip failed.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8618
https://github.com/broadinstitute/gatk/issues/8618:1547,Deployability,install,installed,1547,"ft/gatk-4.4.0.0# conda --version; conda 23.9.0; root@d12ac7710afc:/soft/gatk-4.4.0.0# ""$CONDA"" env create -n gatk -f ""$SOFT/gatk-${GATK_VERSION}/gatkcondaenv.yml""; ...; Preparing transaction: done; Verifying transaction: done; Executing transaction: done; Installing pip dependencies: - Ran pip subprocess with arguments:; ['/opt/miniconda/envs/gatk/bin/python', '-m', 'pip', 'install', '-U', '-r', '/soft/gatk-4.4.0.0/condaenv.vn0sukco.requirements.txt', '--exists-action=b']; Pip subprocess output:; Processing ./gatkPythonPackageArchive.zip; Building wheels for collected packages: gatkpythonpackages; Building wheel for gatkpythonpackages (setup.py): started; Building wheel for gatkpythonpackages (setup.py): finished with status 'done'; Created wheel for gatkpythonpackages: filename=gatkpythonpackages-0.1-py3-none-any.whl size=117686 sha256=8095375e139fa0729c7a41c8f5e8a43281fc1b6859b6d3951d3bfba7296ee349; Stored in directory: /tmp/pip-ephem-wheel-cache-ecx6e_m0/wheels/06/f7/e1/87cb7da6f705baa602256a58c9514b47dc313aade8809a01da; Successfully built gatkpythonpackages; Installing collected packages: gatkpythonpackages; Successfully installed gatkpythonpackages-0.1. done; #; # To activate this environment, use; #; # $ conda activate gatk; #; # To deactivate an active environment, use; #; # $ conda deactivate. ```. #### Actual behavior; ```sh; root@d12ac7710afc:/soft/gatk-4.4.0.0# conda --version; conda 23.10.0; root@d12ac7710afc:/soft/gatk-4.4.0.0# ""$CONDA"" env create -n gatk -f ""$SOFT/gatk-${GATK_VERSION}/gatkcondaenv.yml""; ...; Preparing transaction: done; Verifying transaction: done; Executing transaction: done; Installing pip dependencies: | Ran pip subprocess with arguments:; ['/opt/miniconda/envs/gatk/bin/python', '-m', 'pip', 'install', '-U', '-r', '/soft/gatk-4.4.0.0/condaenv.i9brvcrk.requirements.txt', '--exists-action=b']; Pip subprocess output:. Pip subprocess error:; /opt/miniconda/envs/gatk/bin/python: No module named pip. failed. CondaEnvException: Pip failed.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8618
https://github.com/broadinstitute/gatk/issues/8618:2039,Deployability,Install,Installing,2039,"ing wheel for gatkpythonpackages (setup.py): finished with status 'done'; Created wheel for gatkpythonpackages: filename=gatkpythonpackages-0.1-py3-none-any.whl size=117686 sha256=8095375e139fa0729c7a41c8f5e8a43281fc1b6859b6d3951d3bfba7296ee349; Stored in directory: /tmp/pip-ephem-wheel-cache-ecx6e_m0/wheels/06/f7/e1/87cb7da6f705baa602256a58c9514b47dc313aade8809a01da; Successfully built gatkpythonpackages; Installing collected packages: gatkpythonpackages; Successfully installed gatkpythonpackages-0.1. done; #; # To activate this environment, use; #; # $ conda activate gatk; #; # To deactivate an active environment, use; #; # $ conda deactivate. ```. #### Actual behavior; ```sh; root@d12ac7710afc:/soft/gatk-4.4.0.0# conda --version; conda 23.10.0; root@d12ac7710afc:/soft/gatk-4.4.0.0# ""$CONDA"" env create -n gatk -f ""$SOFT/gatk-${GATK_VERSION}/gatkcondaenv.yml""; ...; Preparing transaction: done; Verifying transaction: done; Executing transaction: done; Installing pip dependencies: | Ran pip subprocess with arguments:; ['/opt/miniconda/envs/gatk/bin/python', '-m', 'pip', 'install', '-U', '-r', '/soft/gatk-4.4.0.0/condaenv.i9brvcrk.requirements.txt', '--exists-action=b']; Pip subprocess output:. Pip subprocess error:; /opt/miniconda/envs/gatk/bin/python: No module named pip. failed. CondaEnvException: Pip failed. ```; ---; It can be fixed with setting classic colver:; ```; root@d12ac7710afc:/soft/gatk-4.4.0.0# conda --version; conda 23.10.0; root@d12ac7710afc:/soft/gatk-4.4.0.0# conda config --set solver classic; root@d12ac7710afc:/soft/gatk-4.4.0.0# ""$CONDA"" env create -n gatk -f ""$SOFT/gatk-${GATK_VERSION}/gatkcondaenv.yml""; ...; Preparing transaction: done; Verifying transaction: done; Executing transaction: done; Installing pip dependencies: \ Ran pip subprocess with arguments:; ['/opt/miniconda/envs/gatk/bin/python', '-m', 'pip', 'install', '-U', '-r', '/soft/gatk-4.4.0.0/condaenv.rtsyg5rl.requirements.txt', '--exists-action=b']; Pip subprocess output:; Processing",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8618
https://github.com/broadinstitute/gatk/issues/8618:2160,Deployability,install,install,2160,"ing wheel for gatkpythonpackages (setup.py): finished with status 'done'; Created wheel for gatkpythonpackages: filename=gatkpythonpackages-0.1-py3-none-any.whl size=117686 sha256=8095375e139fa0729c7a41c8f5e8a43281fc1b6859b6d3951d3bfba7296ee349; Stored in directory: /tmp/pip-ephem-wheel-cache-ecx6e_m0/wheels/06/f7/e1/87cb7da6f705baa602256a58c9514b47dc313aade8809a01da; Successfully built gatkpythonpackages; Installing collected packages: gatkpythonpackages; Successfully installed gatkpythonpackages-0.1. done; #; # To activate this environment, use; #; # $ conda activate gatk; #; # To deactivate an active environment, use; #; # $ conda deactivate. ```. #### Actual behavior; ```sh; root@d12ac7710afc:/soft/gatk-4.4.0.0# conda --version; conda 23.10.0; root@d12ac7710afc:/soft/gatk-4.4.0.0# ""$CONDA"" env create -n gatk -f ""$SOFT/gatk-${GATK_VERSION}/gatkcondaenv.yml""; ...; Preparing transaction: done; Verifying transaction: done; Executing transaction: done; Installing pip dependencies: | Ran pip subprocess with arguments:; ['/opt/miniconda/envs/gatk/bin/python', '-m', 'pip', 'install', '-U', '-r', '/soft/gatk-4.4.0.0/condaenv.i9brvcrk.requirements.txt', '--exists-action=b']; Pip subprocess output:. Pip subprocess error:; /opt/miniconda/envs/gatk/bin/python: No module named pip. failed. CondaEnvException: Pip failed. ```; ---; It can be fixed with setting classic colver:; ```; root@d12ac7710afc:/soft/gatk-4.4.0.0# conda --version; conda 23.10.0; root@d12ac7710afc:/soft/gatk-4.4.0.0# conda config --set solver classic; root@d12ac7710afc:/soft/gatk-4.4.0.0# ""$CONDA"" env create -n gatk -f ""$SOFT/gatk-${GATK_VERSION}/gatkcondaenv.yml""; ...; Preparing transaction: done; Verifying transaction: done; Executing transaction: done; Installing pip dependencies: \ Ran pip subprocess with arguments:; ['/opt/miniconda/envs/gatk/bin/python', '-m', 'pip', 'install', '-U', '-r', '/soft/gatk-4.4.0.0/condaenv.rtsyg5rl.requirements.txt', '--exists-action=b']; Pip subprocess output:; Processing",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8618
https://github.com/broadinstitute/gatk/issues/8618:2817,Deployability,Install,Installing,2817,"oft/gatk-4.4.0.0# ""$CONDA"" env create -n gatk -f ""$SOFT/gatk-${GATK_VERSION}/gatkcondaenv.yml""; ...; Preparing transaction: done; Verifying transaction: done; Executing transaction: done; Installing pip dependencies: | Ran pip subprocess with arguments:; ['/opt/miniconda/envs/gatk/bin/python', '-m', 'pip', 'install', '-U', '-r', '/soft/gatk-4.4.0.0/condaenv.i9brvcrk.requirements.txt', '--exists-action=b']; Pip subprocess output:. Pip subprocess error:; /opt/miniconda/envs/gatk/bin/python: No module named pip. failed. CondaEnvException: Pip failed. ```; ---; It can be fixed with setting classic colver:; ```; root@d12ac7710afc:/soft/gatk-4.4.0.0# conda --version; conda 23.10.0; root@d12ac7710afc:/soft/gatk-4.4.0.0# conda config --set solver classic; root@d12ac7710afc:/soft/gatk-4.4.0.0# ""$CONDA"" env create -n gatk -f ""$SOFT/gatk-${GATK_VERSION}/gatkcondaenv.yml""; ...; Preparing transaction: done; Verifying transaction: done; Executing transaction: done; Installing pip dependencies: \ Ran pip subprocess with arguments:; ['/opt/miniconda/envs/gatk/bin/python', '-m', 'pip', 'install', '-U', '-r', '/soft/gatk-4.4.0.0/condaenv.rtsyg5rl.requirements.txt', '--exists-action=b']; Pip subprocess output:; Processing ./gatkPythonPackageArchive.zip; Building wheels for collected packages: gatkpythonpackages; Building wheel for gatkpythonpackages (setup.py): started; Building wheel for gatkpythonpackages (setup.py): finished with status 'done'; Created wheel for gatkpythonpackages: filename=gatkpythonpackages-0.1-py3-none-any.whl size=117686 sha256=f2165b43e412c95ff9a788022d355279e5434032fb8c9cf82fbd71779acd1a76; Stored in directory: /tmp/pip-ephem-wheel-cache-5a9zdytx/wheels/06/f7/e1/87cb7da6f705baa602256a58c9514b47dc313aade8809a01da; Successfully built gatkpythonpackages; Installing collected packages: gatkpythonpackages; Successfully installed gatkpythonpackages-0.1. done; #; # To activate this environment, use; #; # $ conda activate gatk; #; # To deactivate an active environmen",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8618
https://github.com/broadinstitute/gatk/issues/8618:2938,Deployability,install,install,2938,"oft/gatk-4.4.0.0# ""$CONDA"" env create -n gatk -f ""$SOFT/gatk-${GATK_VERSION}/gatkcondaenv.yml""; ...; Preparing transaction: done; Verifying transaction: done; Executing transaction: done; Installing pip dependencies: | Ran pip subprocess with arguments:; ['/opt/miniconda/envs/gatk/bin/python', '-m', 'pip', 'install', '-U', '-r', '/soft/gatk-4.4.0.0/condaenv.i9brvcrk.requirements.txt', '--exists-action=b']; Pip subprocess output:. Pip subprocess error:; /opt/miniconda/envs/gatk/bin/python: No module named pip. failed. CondaEnvException: Pip failed. ```; ---; It can be fixed with setting classic colver:; ```; root@d12ac7710afc:/soft/gatk-4.4.0.0# conda --version; conda 23.10.0; root@d12ac7710afc:/soft/gatk-4.4.0.0# conda config --set solver classic; root@d12ac7710afc:/soft/gatk-4.4.0.0# ""$CONDA"" env create -n gatk -f ""$SOFT/gatk-${GATK_VERSION}/gatkcondaenv.yml""; ...; Preparing transaction: done; Verifying transaction: done; Executing transaction: done; Installing pip dependencies: \ Ran pip subprocess with arguments:; ['/opt/miniconda/envs/gatk/bin/python', '-m', 'pip', 'install', '-U', '-r', '/soft/gatk-4.4.0.0/condaenv.rtsyg5rl.requirements.txt', '--exists-action=b']; Pip subprocess output:; Processing ./gatkPythonPackageArchive.zip; Building wheels for collected packages: gatkpythonpackages; Building wheel for gatkpythonpackages (setup.py): started; Building wheel for gatkpythonpackages (setup.py): finished with status 'done'; Created wheel for gatkpythonpackages: filename=gatkpythonpackages-0.1-py3-none-any.whl size=117686 sha256=f2165b43e412c95ff9a788022d355279e5434032fb8c9cf82fbd71779acd1a76; Stored in directory: /tmp/pip-ephem-wheel-cache-5a9zdytx/wheels/06/f7/e1/87cb7da6f705baa602256a58c9514b47dc313aade8809a01da; Successfully built gatkpythonpackages; Installing collected packages: gatkpythonpackages; Successfully installed gatkpythonpackages-0.1. done; #; # To activate this environment, use; #; # $ conda activate gatk; #; # To deactivate an active environmen",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8618
https://github.com/broadinstitute/gatk/issues/8618:3640,Deployability,Install,Installing,3640,"guments:; ['/opt/miniconda/envs/gatk/bin/python', '-m', 'pip', 'install', '-U', '-r', '/soft/gatk-4.4.0.0/condaenv.i9brvcrk.requirements.txt', '--exists-action=b']; Pip subprocess output:. Pip subprocess error:; /opt/miniconda/envs/gatk/bin/python: No module named pip. failed. CondaEnvException: Pip failed. ```; ---; It can be fixed with setting classic colver:; ```; root@d12ac7710afc:/soft/gatk-4.4.0.0# conda --version; conda 23.10.0; root@d12ac7710afc:/soft/gatk-4.4.0.0# conda config --set solver classic; root@d12ac7710afc:/soft/gatk-4.4.0.0# ""$CONDA"" env create -n gatk -f ""$SOFT/gatk-${GATK_VERSION}/gatkcondaenv.yml""; ...; Preparing transaction: done; Verifying transaction: done; Executing transaction: done; Installing pip dependencies: \ Ran pip subprocess with arguments:; ['/opt/miniconda/envs/gatk/bin/python', '-m', 'pip', 'install', '-U', '-r', '/soft/gatk-4.4.0.0/condaenv.rtsyg5rl.requirements.txt', '--exists-action=b']; Pip subprocess output:; Processing ./gatkPythonPackageArchive.zip; Building wheels for collected packages: gatkpythonpackages; Building wheel for gatkpythonpackages (setup.py): started; Building wheel for gatkpythonpackages (setup.py): finished with status 'done'; Created wheel for gatkpythonpackages: filename=gatkpythonpackages-0.1-py3-none-any.whl size=117686 sha256=f2165b43e412c95ff9a788022d355279e5434032fb8c9cf82fbd71779acd1a76; Stored in directory: /tmp/pip-ephem-wheel-cache-5a9zdytx/wheels/06/f7/e1/87cb7da6f705baa602256a58c9514b47dc313aade8809a01da; Successfully built gatkpythonpackages; Installing collected packages: gatkpythonpackages; Successfully installed gatkpythonpackages-0.1. done; #; # To activate this environment, use; #; # $ conda activate gatk; #; # To deactivate an active environment, use; #; # $ conda deactivate. ```. I see some changed in master (probably fixing this issue too), but no description: https://github.com/broadinstitute/gatk/pull/8610/files#diff-5c3c54d49d09fd7ab0957b7c3185e22c6161e225b9e3ed65e72716fa2a635a96",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8618
https://github.com/broadinstitute/gatk/issues/8618:3704,Deployability,install,installed,3704,"guments:; ['/opt/miniconda/envs/gatk/bin/python', '-m', 'pip', 'install', '-U', '-r', '/soft/gatk-4.4.0.0/condaenv.i9brvcrk.requirements.txt', '--exists-action=b']; Pip subprocess output:. Pip subprocess error:; /opt/miniconda/envs/gatk/bin/python: No module named pip. failed. CondaEnvException: Pip failed. ```; ---; It can be fixed with setting classic colver:; ```; root@d12ac7710afc:/soft/gatk-4.4.0.0# conda --version; conda 23.10.0; root@d12ac7710afc:/soft/gatk-4.4.0.0# conda config --set solver classic; root@d12ac7710afc:/soft/gatk-4.4.0.0# ""$CONDA"" env create -n gatk -f ""$SOFT/gatk-${GATK_VERSION}/gatkcondaenv.yml""; ...; Preparing transaction: done; Verifying transaction: done; Executing transaction: done; Installing pip dependencies: \ Ran pip subprocess with arguments:; ['/opt/miniconda/envs/gatk/bin/python', '-m', 'pip', 'install', '-U', '-r', '/soft/gatk-4.4.0.0/condaenv.rtsyg5rl.requirements.txt', '--exists-action=b']; Pip subprocess output:; Processing ./gatkPythonPackageArchive.zip; Building wheels for collected packages: gatkpythonpackages; Building wheel for gatkpythonpackages (setup.py): started; Building wheel for gatkpythonpackages (setup.py): finished with status 'done'; Created wheel for gatkpythonpackages: filename=gatkpythonpackages-0.1-py3-none-any.whl size=117686 sha256=f2165b43e412c95ff9a788022d355279e5434032fb8c9cf82fbd71779acd1a76; Stored in directory: /tmp/pip-ephem-wheel-cache-5a9zdytx/wheels/06/f7/e1/87cb7da6f705baa602256a58c9514b47dc313aade8809a01da; Successfully built gatkpythonpackages; Installing collected packages: gatkpythonpackages; Successfully installed gatkpythonpackages-0.1. done; #; # To activate this environment, use; #; # $ conda activate gatk; #; # To deactivate an active environment, use; #; # $ conda deactivate. ```. I see some changed in master (probably fixing this issue too), but no description: https://github.com/broadinstitute/gatk/pull/8610/files#diff-5c3c54d49d09fd7ab0957b7c3185e22c6161e225b9e3ed65e72716fa2a635a96",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8618
https://github.com/broadinstitute/gatk/issues/8618:675,Integrability,depend,dependencies,675,"## Bug Report. ### Affected tool(s) or class(es); GATK installation. ### Affected version(s); - [x] Latest public release version 4.4.0.0; - [x] Latest master branch as of [11.12.2023]. ### Description ; Latest `conda` versions cannot install pip packages. #### Steps to reproduce; Create `gatk` `env` with `conda` version equal or newer than `23.10`. #### Expected behavior; ```sh; root@d12ac7710afc:/soft/gatk-4.4.0.0# conda --version; conda 23.9.0; root@d12ac7710afc:/soft/gatk-4.4.0.0# ""$CONDA"" env create -n gatk -f ""$SOFT/gatk-${GATK_VERSION}/gatkcondaenv.yml""; ...; Preparing transaction: done; Verifying transaction: done; Executing transaction: done; Installing pip dependencies: - Ran pip subprocess with arguments:; ['/opt/miniconda/envs/gatk/bin/python', '-m', 'pip', 'install', '-U', '-r', '/soft/gatk-4.4.0.0/condaenv.vn0sukco.requirements.txt', '--exists-action=b']; Pip subprocess output:; Processing ./gatkPythonPackageArchive.zip; Building wheels for collected packages: gatkpythonpackages; Building wheel for gatkpythonpackages (setup.py): started; Building wheel for gatkpythonpackages (setup.py): finished with status 'done'; Created wheel for gatkpythonpackages: filename=gatkpythonpackages-0.1-py3-none-any.whl size=117686 sha256=8095375e139fa0729c7a41c8f5e8a43281fc1b6859b6d3951d3bfba7296ee349; Stored in directory: /tmp/pip-ephem-wheel-cache-ecx6e_m0/wheels/06/f7/e1/87cb7da6f705baa602256a58c9514b47dc313aade8809a01da; Successfully built gatkpythonpackages; Installing collected packages: gatkpythonpackages; Successfully installed gatkpythonpackages-0.1. done; #; # To activate this environment, use; #; # $ conda activate gatk; #; # To deactivate an active environment, use; #; # $ conda deactivate. ```. #### Actual behavior; ```sh; root@d12ac7710afc:/soft/gatk-4.4.0.0# conda --version; conda 23.10.0; root@d12ac7710afc:/soft/gatk-4.4.0.0# ""$CONDA"" env create -n gatk -f ""$SOFT/gatk-${GATK_VERSION}/gatkcondaenv.yml""; ...; Preparing transaction: done; Verifying transactio",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8618
https://github.com/broadinstitute/gatk/issues/8618:2054,Integrability,depend,dependencies,2054,"ing wheel for gatkpythonpackages (setup.py): finished with status 'done'; Created wheel for gatkpythonpackages: filename=gatkpythonpackages-0.1-py3-none-any.whl size=117686 sha256=8095375e139fa0729c7a41c8f5e8a43281fc1b6859b6d3951d3bfba7296ee349; Stored in directory: /tmp/pip-ephem-wheel-cache-ecx6e_m0/wheels/06/f7/e1/87cb7da6f705baa602256a58c9514b47dc313aade8809a01da; Successfully built gatkpythonpackages; Installing collected packages: gatkpythonpackages; Successfully installed gatkpythonpackages-0.1. done; #; # To activate this environment, use; #; # $ conda activate gatk; #; # To deactivate an active environment, use; #; # $ conda deactivate. ```. #### Actual behavior; ```sh; root@d12ac7710afc:/soft/gatk-4.4.0.0# conda --version; conda 23.10.0; root@d12ac7710afc:/soft/gatk-4.4.0.0# ""$CONDA"" env create -n gatk -f ""$SOFT/gatk-${GATK_VERSION}/gatkcondaenv.yml""; ...; Preparing transaction: done; Verifying transaction: done; Executing transaction: done; Installing pip dependencies: | Ran pip subprocess with arguments:; ['/opt/miniconda/envs/gatk/bin/python', '-m', 'pip', 'install', '-U', '-r', '/soft/gatk-4.4.0.0/condaenv.i9brvcrk.requirements.txt', '--exists-action=b']; Pip subprocess output:. Pip subprocess error:; /opt/miniconda/envs/gatk/bin/python: No module named pip. failed. CondaEnvException: Pip failed. ```; ---; It can be fixed with setting classic colver:; ```; root@d12ac7710afc:/soft/gatk-4.4.0.0# conda --version; conda 23.10.0; root@d12ac7710afc:/soft/gatk-4.4.0.0# conda config --set solver classic; root@d12ac7710afc:/soft/gatk-4.4.0.0# ""$CONDA"" env create -n gatk -f ""$SOFT/gatk-${GATK_VERSION}/gatkcondaenv.yml""; ...; Preparing transaction: done; Verifying transaction: done; Executing transaction: done; Installing pip dependencies: \ Ran pip subprocess with arguments:; ['/opt/miniconda/envs/gatk/bin/python', '-m', 'pip', 'install', '-U', '-r', '/soft/gatk-4.4.0.0/condaenv.rtsyg5rl.requirements.txt', '--exists-action=b']; Pip subprocess output:; Processing",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8618
https://github.com/broadinstitute/gatk/issues/8618:2832,Integrability,depend,dependencies,2832,"oft/gatk-4.4.0.0# ""$CONDA"" env create -n gatk -f ""$SOFT/gatk-${GATK_VERSION}/gatkcondaenv.yml""; ...; Preparing transaction: done; Verifying transaction: done; Executing transaction: done; Installing pip dependencies: | Ran pip subprocess with arguments:; ['/opt/miniconda/envs/gatk/bin/python', '-m', 'pip', 'install', '-U', '-r', '/soft/gatk-4.4.0.0/condaenv.i9brvcrk.requirements.txt', '--exists-action=b']; Pip subprocess output:. Pip subprocess error:; /opt/miniconda/envs/gatk/bin/python: No module named pip. failed. CondaEnvException: Pip failed. ```; ---; It can be fixed with setting classic colver:; ```; root@d12ac7710afc:/soft/gatk-4.4.0.0# conda --version; conda 23.10.0; root@d12ac7710afc:/soft/gatk-4.4.0.0# conda config --set solver classic; root@d12ac7710afc:/soft/gatk-4.4.0.0# ""$CONDA"" env create -n gatk -f ""$SOFT/gatk-${GATK_VERSION}/gatkcondaenv.yml""; ...; Preparing transaction: done; Verifying transaction: done; Executing transaction: done; Installing pip dependencies: \ Ran pip subprocess with arguments:; ['/opt/miniconda/envs/gatk/bin/python', '-m', 'pip', 'install', '-U', '-r', '/soft/gatk-4.4.0.0/condaenv.rtsyg5rl.requirements.txt', '--exists-action=b']; Pip subprocess output:; Processing ./gatkPythonPackageArchive.zip; Building wheels for collected packages: gatkpythonpackages; Building wheel for gatkpythonpackages (setup.py): started; Building wheel for gatkpythonpackages (setup.py): finished with status 'done'; Created wheel for gatkpythonpackages: filename=gatkpythonpackages-0.1-py3-none-any.whl size=117686 sha256=f2165b43e412c95ff9a788022d355279e5434032fb8c9cf82fbd71779acd1a76; Stored in directory: /tmp/pip-ephem-wheel-cache-5a9zdytx/wheels/06/f7/e1/87cb7da6f705baa602256a58c9514b47dc313aade8809a01da; Successfully built gatkpythonpackages; Installing collected packages: gatkpythonpackages; Successfully installed gatkpythonpackages-0.1. done; #; # To activate this environment, use; #; # $ conda activate gatk; #; # To deactivate an active environmen",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8618
https://github.com/broadinstitute/gatk/issues/8618:2580,Modifiability,config,config,2580,"his environment, use; #; # $ conda activate gatk; #; # To deactivate an active environment, use; #; # $ conda deactivate. ```. #### Actual behavior; ```sh; root@d12ac7710afc:/soft/gatk-4.4.0.0# conda --version; conda 23.10.0; root@d12ac7710afc:/soft/gatk-4.4.0.0# ""$CONDA"" env create -n gatk -f ""$SOFT/gatk-${GATK_VERSION}/gatkcondaenv.yml""; ...; Preparing transaction: done; Verifying transaction: done; Executing transaction: done; Installing pip dependencies: | Ran pip subprocess with arguments:; ['/opt/miniconda/envs/gatk/bin/python', '-m', 'pip', 'install', '-U', '-r', '/soft/gatk-4.4.0.0/condaenv.i9brvcrk.requirements.txt', '--exists-action=b']; Pip subprocess output:. Pip subprocess error:; /opt/miniconda/envs/gatk/bin/python: No module named pip. failed. CondaEnvException: Pip failed. ```; ---; It can be fixed with setting classic colver:; ```; root@d12ac7710afc:/soft/gatk-4.4.0.0# conda --version; conda 23.10.0; root@d12ac7710afc:/soft/gatk-4.4.0.0# conda config --set solver classic; root@d12ac7710afc:/soft/gatk-4.4.0.0# ""$CONDA"" env create -n gatk -f ""$SOFT/gatk-${GATK_VERSION}/gatkcondaenv.yml""; ...; Preparing transaction: done; Verifying transaction: done; Executing transaction: done; Installing pip dependencies: \ Ran pip subprocess with arguments:; ['/opt/miniconda/envs/gatk/bin/python', '-m', 'pip', 'install', '-U', '-r', '/soft/gatk-4.4.0.0/condaenv.rtsyg5rl.requirements.txt', '--exists-action=b']; Pip subprocess output:; Processing ./gatkPythonPackageArchive.zip; Building wheels for collected packages: gatkpythonpackages; Building wheel for gatkpythonpackages (setup.py): started; Building wheel for gatkpythonpackages (setup.py): finished with status 'done'; Created wheel for gatkpythonpackages: filename=gatkpythonpackages-0.1-py3-none-any.whl size=117686 sha256=f2165b43e412c95ff9a788022d355279e5434032fb8c9cf82fbd71779acd1a76; Stored in directory: /tmp/pip-ephem-wheel-cache-5a9zdytx/wheels/06/f7/e1/87cb7da6f705baa602256a58c9514b47dc313aade8809a01da; Succe",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8618
https://github.com/broadinstitute/gatk/issues/8618:1361,Performance,cache,cache-,1361,"ft/gatk-4.4.0.0# conda --version; conda 23.9.0; root@d12ac7710afc:/soft/gatk-4.4.0.0# ""$CONDA"" env create -n gatk -f ""$SOFT/gatk-${GATK_VERSION}/gatkcondaenv.yml""; ...; Preparing transaction: done; Verifying transaction: done; Executing transaction: done; Installing pip dependencies: - Ran pip subprocess with arguments:; ['/opt/miniconda/envs/gatk/bin/python', '-m', 'pip', 'install', '-U', '-r', '/soft/gatk-4.4.0.0/condaenv.vn0sukco.requirements.txt', '--exists-action=b']; Pip subprocess output:; Processing ./gatkPythonPackageArchive.zip; Building wheels for collected packages: gatkpythonpackages; Building wheel for gatkpythonpackages (setup.py): started; Building wheel for gatkpythonpackages (setup.py): finished with status 'done'; Created wheel for gatkpythonpackages: filename=gatkpythonpackages-0.1-py3-none-any.whl size=117686 sha256=8095375e139fa0729c7a41c8f5e8a43281fc1b6859b6d3951d3bfba7296ee349; Stored in directory: /tmp/pip-ephem-wheel-cache-ecx6e_m0/wheels/06/f7/e1/87cb7da6f705baa602256a58c9514b47dc313aade8809a01da; Successfully built gatkpythonpackages; Installing collected packages: gatkpythonpackages; Successfully installed gatkpythonpackages-0.1. done; #; # To activate this environment, use; #; # $ conda activate gatk; #; # To deactivate an active environment, use; #; # $ conda deactivate. ```. #### Actual behavior; ```sh; root@d12ac7710afc:/soft/gatk-4.4.0.0# conda --version; conda 23.10.0; root@d12ac7710afc:/soft/gatk-4.4.0.0# ""$CONDA"" env create -n gatk -f ""$SOFT/gatk-${GATK_VERSION}/gatkcondaenv.yml""; ...; Preparing transaction: done; Verifying transaction: done; Executing transaction: done; Installing pip dependencies: | Ran pip subprocess with arguments:; ['/opt/miniconda/envs/gatk/bin/python', '-m', 'pip', 'install', '-U', '-r', '/soft/gatk-4.4.0.0/condaenv.i9brvcrk.requirements.txt', '--exists-action=b']; Pip subprocess output:. Pip subprocess error:; /opt/miniconda/envs/gatk/bin/python: No module named pip. failed. CondaEnvException: Pip failed.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8618
https://github.com/broadinstitute/gatk/issues/8618:3518,Performance,cache,cache-,3518,"guments:; ['/opt/miniconda/envs/gatk/bin/python', '-m', 'pip', 'install', '-U', '-r', '/soft/gatk-4.4.0.0/condaenv.i9brvcrk.requirements.txt', '--exists-action=b']; Pip subprocess output:. Pip subprocess error:; /opt/miniconda/envs/gatk/bin/python: No module named pip. failed. CondaEnvException: Pip failed. ```; ---; It can be fixed with setting classic colver:; ```; root@d12ac7710afc:/soft/gatk-4.4.0.0# conda --version; conda 23.10.0; root@d12ac7710afc:/soft/gatk-4.4.0.0# conda config --set solver classic; root@d12ac7710afc:/soft/gatk-4.4.0.0# ""$CONDA"" env create -n gatk -f ""$SOFT/gatk-${GATK_VERSION}/gatkcondaenv.yml""; ...; Preparing transaction: done; Verifying transaction: done; Executing transaction: done; Installing pip dependencies: \ Ran pip subprocess with arguments:; ['/opt/miniconda/envs/gatk/bin/python', '-m', 'pip', 'install', '-U', '-r', '/soft/gatk-4.4.0.0/condaenv.rtsyg5rl.requirements.txt', '--exists-action=b']; Pip subprocess output:; Processing ./gatkPythonPackageArchive.zip; Building wheels for collected packages: gatkpythonpackages; Building wheel for gatkpythonpackages (setup.py): started; Building wheel for gatkpythonpackages (setup.py): finished with status 'done'; Created wheel for gatkpythonpackages: filename=gatkpythonpackages-0.1-py3-none-any.whl size=117686 sha256=f2165b43e412c95ff9a788022d355279e5434032fb8c9cf82fbd71779acd1a76; Stored in directory: /tmp/pip-ephem-wheel-cache-5a9zdytx/wheels/06/f7/e1/87cb7da6f705baa602256a58c9514b47dc313aade8809a01da; Successfully built gatkpythonpackages; Installing collected packages: gatkpythonpackages; Successfully installed gatkpythonpackages-0.1. done; #; # To activate this environment, use; #; # $ conda activate gatk; #; # To deactivate an active environment, use; #; # $ conda deactivate. ```. I see some changed in master (probably fixing this issue too), but no description: https://github.com/broadinstitute/gatk/pull/8610/files#diff-5c3c54d49d09fd7ab0957b7c3185e22c6161e225b9e3ed65e72716fa2a635a96",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8618
https://github.com/broadinstitute/gatk/issues/8619:2714,Availability,down,down,2714,"NVCaller --run-mode COHORT -L /outputs/gatk_intervals.interval_list --interval-merging-rule OVERLAPPING_ONLY --annotated-intervals /outputs/gatk_intervals.interval_list.annotated.tsv --contig-ploidy-calls /outputs/COHORT_runDir/COHORT-calls --input /outputs/E07002_normal_alignment.bam.counts.hdf5 --input /outputs/E07002_tumor_alignment.bam.counts.hdf5 --output /outputs/COHORT_runDir --output-prefix COHORT; ```. We used data from `PRJNA399748` project to test. #### Expected behavior. - `test_gatkgermlinecnvcaller_genotyped-intervals-cohort_0.woTimestamp.vcf` (`##contig` cut from header and only first 5 `chr22` CNVs present). ```; ##fileformat=VCFv4.2; ##FORMAT=<ID=CN,Number=1,Type=Integer,Description=""Copy number maximum a posteriori value"">; ##FORMAT=<ID=CNLP,Number=.,Type=Integer,Description=""Copy number log posterior (in Phred-scale) rounded down"">; ##FORMAT=<ID=CNQ,Number=1,Type=Integer,Description=""Genotype call quality as the difference between the best and second best phred-scaled log posterior scores"">; ##FORMAT=<ID=GT,Number=1,Type=Integer,Description=""Genotype"">; ##INFO=<ID=END,Number=1,Type=Integer,Description=""End coordinate of the variant"">; ##contig=<ID=chr1,length=248956422,assembly=GRCh38.d1.vd1>; ...; ##contig=<ID=HPV-mSD2,length=7300,assembly=GRCh38.d1.vd1>; ##source=PostprocessGermlineCNVCalls; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	E07002_normal; chr1	10000	CNV_chr1_10000_10999	N	<DEL>,<DUP>	.	.	END=10999	GT:CN:CNLP:CNQ	1:0:0,80,90,100,108,116:80; chr1	11000	CNV_chr1_11000_11999	N	<DEL>,<DUP>	.	.	END=11999	GT:CN:CNLP:CNQ	1:0:0,81,86,89,92,95:81; chr1	12000	CNV_chr1_12000_12999	N	<DEL>,<DUP>	.	.	END=12999	GT:CN:CNLP:CNQ	1:0:0,93,107,119,129,137:93; chr1	13000	CNV_chr1_13000_13999	N	<DEL>,<DUP>	.	.	END=13999	GT:CN:CNLP:CNQ	1:0:0,89,95,99,102,104:89; chr1	14000	CNV_chr1_14000_14999	N	<DEL>,<DUP>	.	.	END=14999	GT:CN:CNLP:CNQ	1:0:0,86,91,93,96,97:86; chr1	15000	CNV_chr1_15000_15999	N	<DEL>,<DUP>	.	.	END=15999	GT:CN:CNLP:CNQ	1:0:0,82,88,92,97,101:",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8619
https://github.com/broadinstitute/gatk/issues/8619:19664,Availability,down,down,19664,"1:644:1:1; chr22	19895477	CNV_chr22_19895477_19901476	N	<DUP>	2.30	.	END=19901476	GT:CN:NP:QA:QS:QSE:QSS	./.:3:6:1:2:1:0; chr22	19901477	CNV_chr22_19901477_19946476	N	<DEL>	123.62	.	END=19946476	GT:CN:NP:QA:QS:QSE:QSS	1/1:0:45:0:124:1:1; chr22	19946477	CNV_chr22_19946477_19971476	N	<DUP>	6.86	.	END=19971476	GT:CN:NP:QA:QS:QSE:QSS	./.:3:25:0:7:2:1; chr22	19971477	CNV_chr22_19971477_20003000	N	<DEL>	198.68	.	END=20003000	GT:CN:NP:QA:QS:QSE:QSS	1/1:0:32:0:199:2:2. ```. #### Actual behavior. - `gatkgermlinecnvcaller_genotyped-intervals-COHORT_0.woTimestamp.vcf` (`##contig` cut from header and only first 5 `chr22` CNVs present). ```; ##fileformat=VCFv4.2; ##FORMAT=<ID=CN,Number=1,Type=Integer,Description=""Copy number maximum a posteriori value"">; ##FORMAT=<ID=CNLP,Number=.,Type=Integer,Description=""Copy number log posterior (in Phred-scale) rounded down"">; ##FORMAT=<ID=CNQ,Number=1,Type=Integer,Description=""Genotype call quality as the difference between the best and second best phred-scaled log posterior scores"">; ##FORMAT=<ID=GT,Number=1,Type=Integer,Description=""Genotype"">; ##INFO=<ID=END,Number=1,Type=Integer,Description=""End coordinate of the variant"">; ##contig=<ID=chr1,length=248956422,assembly=GRCh38.d1.vd1>; ...; ##contig=<ID=HPV-mSD2,length=7300,assembly=GRCh38.d1.vd1>; ##source=PostprocessGermlineCNVCalls; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	E07002_normal; chr1	10000	CNV_chr1_10000_10999	N	<DEL>,<DUP>	.	.	END=10999	GT:CN:CNLP:CNQ	1:0:0,78,88,96,104,111:78; chr1	11000	CNV_chr1_11000_11999	N	<DEL>,<DUP>	.	.	END=11999	GT:CN:CNLP:CNQ	1:0:0,80,85,88,90,93:80; chr1	12000	CNV_chr1_12000_12999	N	<DEL>,<DUP>	.	.	END=12999	GT:CN:CNLP:CNQ	1:0:0,89,101,110,119,126:89; chr1	13000	CNV_chr1_13000_13999	N	<DEL>,<DUP>	.	.	END=13999	GT:CN:CNLP:CNQ	1:0:0,89,96,101,105,108:89; chr1	14000	CNV_chr1_14000_14999	N	<DEL>,<DUP>	.	.	END=14999	GT:CN:CNLP:CNQ	1:0:0,86,91,94,96,98:86; chr1	15000	CNV_chr1_15000_15999	N	<DEL>,<DUP>	.	.	END=15999	GT:CN:CNLP:CNQ	1:0:0,83,89,94,99,103:",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8619
https://github.com/broadinstitute/gatk/issues/8619:116,Deployability,release,release,116,## Bug Report. ### Affected tool(s) or class(es); `GermlineCNVCaller`. ### Affected version(s); - [x] Latest public release version [`4.3.0.0` and `4.4.0.0`]. ### Description ; `GermlineCNVCaller` pipeline provide different results with same GATK version (`4.3.0.0`) on different base Ubuntu images (`18.04` and `22.04`). Test results of GATK version `4.3.0.0` and `4.4.0.0` are same on Ubuntu 22.04 - I assume there are no changes in `GermlineCNVCaller` between `4.3.0.0` and `4.4.0.0`. #### Steps to reproduce; Command list:; ```sh; /soft/gatk-4.3.0.0/gatk PreprocessIntervals -R /ref/GRCh38.d1.vd1/GRCh38.d1.vd1.fa --padding 0 -L chr1:10000-35000 -L chr22:198477-20003000 -imr OVERLAPPING_ONLY -O /outputs/gatk_intervals.interval_list. /soft/gatk-4.3.0.0/gatk AnnotateIntervals -L /outputs/gatk_intervals.interval_list -R /ref/GRCh38.d1.vd1/GRCh38.d1.vd1.fa -imr OVERLAPPING_ONLY -O /outputs/gatk_intervals.interval_list.annotated.tsv. /soft/gatk-4.3.0.0/gatk CollectReadCounts -I /inputs/E07002_normal_alignment.bam -R /ref/GRCh38.d1.vd1/GRCh38.d1.vd1.fa -L /outputs/gatk_intervals.interval_list --interval-merging-rule OVERLAPPING_ONLY -O /outputs/E07002_normal_alignment.bam.counts.hdf5; /soft/gatk-4.3.0.0/gatk CollectReadCounts -I /inputs/E07002_tumor_alignment.bam -R /ref/GRCh38.d1.vd1/GRCh38.d1.vd1.fa -L /outputs/gatk_intervals.interval_list --interval-merging-rule OVERLAPPING_ONLY -O /outputs/E07002_tumor_alignment.bam.counts.hdf5. /soft/gatk-4.3.0.0/gatk DetermineGermlineContigPloidy -L /outputs/gatk_intervals.interval_list --interval-merging-rule OVERLAPPING_ONLY --contig-ploidy-priors /outputs/a_valid_ploidy_priors_table.tsv.copy.tsv --output /outputs/COHORT_runDir --output-prefix COHORT --input /outputs/E07002_normal_alignment.bam.counts.hdf5 --input /outputs/E07002_tumor_alignment.bam.counts.hdf5. /soft/gatk-4.3.0.0/gatk GermlineCNVCaller --run-mode COHORT -L /outputs/gatk_intervals.interval_list --interval-merging-rule OVERLAPPING_ONLY --annotated-intervals /outputs/gat,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8619
https://github.com/broadinstitute/gatk/issues/8619:197,Deployability,pipeline,pipeline,197,## Bug Report. ### Affected tool(s) or class(es); `GermlineCNVCaller`. ### Affected version(s); - [x] Latest public release version [`4.3.0.0` and `4.4.0.0`]. ### Description ; `GermlineCNVCaller` pipeline provide different results with same GATK version (`4.3.0.0`) on different base Ubuntu images (`18.04` and `22.04`). Test results of GATK version `4.3.0.0` and `4.4.0.0` are same on Ubuntu 22.04 - I assume there are no changes in `GermlineCNVCaller` between `4.3.0.0` and `4.4.0.0`. #### Steps to reproduce; Command list:; ```sh; /soft/gatk-4.3.0.0/gatk PreprocessIntervals -R /ref/GRCh38.d1.vd1/GRCh38.d1.vd1.fa --padding 0 -L chr1:10000-35000 -L chr22:198477-20003000 -imr OVERLAPPING_ONLY -O /outputs/gatk_intervals.interval_list. /soft/gatk-4.3.0.0/gatk AnnotateIntervals -L /outputs/gatk_intervals.interval_list -R /ref/GRCh38.d1.vd1/GRCh38.d1.vd1.fa -imr OVERLAPPING_ONLY -O /outputs/gatk_intervals.interval_list.annotated.tsv. /soft/gatk-4.3.0.0/gatk CollectReadCounts -I /inputs/E07002_normal_alignment.bam -R /ref/GRCh38.d1.vd1/GRCh38.d1.vd1.fa -L /outputs/gatk_intervals.interval_list --interval-merging-rule OVERLAPPING_ONLY -O /outputs/E07002_normal_alignment.bam.counts.hdf5; /soft/gatk-4.3.0.0/gatk CollectReadCounts -I /inputs/E07002_tumor_alignment.bam -R /ref/GRCh38.d1.vd1/GRCh38.d1.vd1.fa -L /outputs/gatk_intervals.interval_list --interval-merging-rule OVERLAPPING_ONLY -O /outputs/E07002_tumor_alignment.bam.counts.hdf5. /soft/gatk-4.3.0.0/gatk DetermineGermlineContigPloidy -L /outputs/gatk_intervals.interval_list --interval-merging-rule OVERLAPPING_ONLY --contig-ploidy-priors /outputs/a_valid_ploidy_priors_table.tsv.copy.tsv --output /outputs/COHORT_runDir --output-prefix COHORT --input /outputs/E07002_normal_alignment.bam.counts.hdf5 --input /outputs/E07002_tumor_alignment.bam.counts.hdf5. /soft/gatk-4.3.0.0/gatk GermlineCNVCaller --run-mode COHORT -L /outputs/gatk_intervals.interval_list --interval-merging-rule OVERLAPPING_ONLY --annotated-intervals /outputs/gat,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8619
https://github.com/broadinstitute/gatk/issues/8619:322,Testability,Test,Test,322,## Bug Report. ### Affected tool(s) or class(es); `GermlineCNVCaller`. ### Affected version(s); - [x] Latest public release version [`4.3.0.0` and `4.4.0.0`]. ### Description ; `GermlineCNVCaller` pipeline provide different results with same GATK version (`4.3.0.0`) on different base Ubuntu images (`18.04` and `22.04`). Test results of GATK version `4.3.0.0` and `4.4.0.0` are same on Ubuntu 22.04 - I assume there are no changes in `GermlineCNVCaller` between `4.3.0.0` and `4.4.0.0`. #### Steps to reproduce; Command list:; ```sh; /soft/gatk-4.3.0.0/gatk PreprocessIntervals -R /ref/GRCh38.d1.vd1/GRCh38.d1.vd1.fa --padding 0 -L chr1:10000-35000 -L chr22:198477-20003000 -imr OVERLAPPING_ONLY -O /outputs/gatk_intervals.interval_list. /soft/gatk-4.3.0.0/gatk AnnotateIntervals -L /outputs/gatk_intervals.interval_list -R /ref/GRCh38.d1.vd1/GRCh38.d1.vd1.fa -imr OVERLAPPING_ONLY -O /outputs/gatk_intervals.interval_list.annotated.tsv. /soft/gatk-4.3.0.0/gatk CollectReadCounts -I /inputs/E07002_normal_alignment.bam -R /ref/GRCh38.d1.vd1/GRCh38.d1.vd1.fa -L /outputs/gatk_intervals.interval_list --interval-merging-rule OVERLAPPING_ONLY -O /outputs/E07002_normal_alignment.bam.counts.hdf5; /soft/gatk-4.3.0.0/gatk CollectReadCounts -I /inputs/E07002_tumor_alignment.bam -R /ref/GRCh38.d1.vd1/GRCh38.d1.vd1.fa -L /outputs/gatk_intervals.interval_list --interval-merging-rule OVERLAPPING_ONLY -O /outputs/E07002_tumor_alignment.bam.counts.hdf5. /soft/gatk-4.3.0.0/gatk DetermineGermlineContigPloidy -L /outputs/gatk_intervals.interval_list --interval-merging-rule OVERLAPPING_ONLY --contig-ploidy-priors /outputs/a_valid_ploidy_priors_table.tsv.copy.tsv --output /outputs/COHORT_runDir --output-prefix COHORT --input /outputs/E07002_normal_alignment.bam.counts.hdf5 --input /outputs/E07002_tumor_alignment.bam.counts.hdf5. /soft/gatk-4.3.0.0/gatk GermlineCNVCaller --run-mode COHORT -L /outputs/gatk_intervals.interval_list --interval-merging-rule OVERLAPPING_ONLY --annotated-intervals /outputs/gat,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8619
https://github.com/broadinstitute/gatk/issues/8619:2316,Testability,test,test,2316,"GRCh38.d1.vd1.fa -L /outputs/gatk_intervals.interval_list --interval-merging-rule OVERLAPPING_ONLY -O /outputs/E07002_tumor_alignment.bam.counts.hdf5. /soft/gatk-4.3.0.0/gatk DetermineGermlineContigPloidy -L /outputs/gatk_intervals.interval_list --interval-merging-rule OVERLAPPING_ONLY --contig-ploidy-priors /outputs/a_valid_ploidy_priors_table.tsv.copy.tsv --output /outputs/COHORT_runDir --output-prefix COHORT --input /outputs/E07002_normal_alignment.bam.counts.hdf5 --input /outputs/E07002_tumor_alignment.bam.counts.hdf5. /soft/gatk-4.3.0.0/gatk GermlineCNVCaller --run-mode COHORT -L /outputs/gatk_intervals.interval_list --interval-merging-rule OVERLAPPING_ONLY --annotated-intervals /outputs/gatk_intervals.interval_list.annotated.tsv --contig-ploidy-calls /outputs/COHORT_runDir/COHORT-calls --input /outputs/E07002_normal_alignment.bam.counts.hdf5 --input /outputs/E07002_tumor_alignment.bam.counts.hdf5 --output /outputs/COHORT_runDir --output-prefix COHORT; ```. We used data from `PRJNA399748` project to test. #### Expected behavior. - `test_gatkgermlinecnvcaller_genotyped-intervals-cohort_0.woTimestamp.vcf` (`##contig` cut from header and only first 5 `chr22` CNVs present). ```; ##fileformat=VCFv4.2; ##FORMAT=<ID=CN,Number=1,Type=Integer,Description=""Copy number maximum a posteriori value"">; ##FORMAT=<ID=CNLP,Number=.,Type=Integer,Description=""Copy number log posterior (in Phred-scale) rounded down"">; ##FORMAT=<ID=CNQ,Number=1,Type=Integer,Description=""Genotype call quality as the difference between the best and second best phred-scaled log posterior scores"">; ##FORMAT=<ID=GT,Number=1,Type=Integer,Description=""Genotype"">; ##INFO=<ID=END,Number=1,Type=Integer,Description=""End coordinate of the variant"">; ##contig=<ID=chr1,length=248956422,assembly=GRCh38.d1.vd1>; ...; ##contig=<ID=HPV-mSD2,length=7300,assembly=GRCh38.d1.vd1>; ##source=PostprocessGermlineCNVCalls; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	E07002_normal; chr1	10000	CNV_chr1_10000_10999	N	<DEL>,<DUP",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8619
https://github.com/broadinstitute/gatk/issues/8619:2675,Testability,log,log,2675,"NVCaller --run-mode COHORT -L /outputs/gatk_intervals.interval_list --interval-merging-rule OVERLAPPING_ONLY --annotated-intervals /outputs/gatk_intervals.interval_list.annotated.tsv --contig-ploidy-calls /outputs/COHORT_runDir/COHORT-calls --input /outputs/E07002_normal_alignment.bam.counts.hdf5 --input /outputs/E07002_tumor_alignment.bam.counts.hdf5 --output /outputs/COHORT_runDir --output-prefix COHORT; ```. We used data from `PRJNA399748` project to test. #### Expected behavior. - `test_gatkgermlinecnvcaller_genotyped-intervals-cohort_0.woTimestamp.vcf` (`##contig` cut from header and only first 5 `chr22` CNVs present). ```; ##fileformat=VCFv4.2; ##FORMAT=<ID=CN,Number=1,Type=Integer,Description=""Copy number maximum a posteriori value"">; ##FORMAT=<ID=CNLP,Number=.,Type=Integer,Description=""Copy number log posterior (in Phred-scale) rounded down"">; ##FORMAT=<ID=CNQ,Number=1,Type=Integer,Description=""Genotype call quality as the difference between the best and second best phred-scaled log posterior scores"">; ##FORMAT=<ID=GT,Number=1,Type=Integer,Description=""Genotype"">; ##INFO=<ID=END,Number=1,Type=Integer,Description=""End coordinate of the variant"">; ##contig=<ID=chr1,length=248956422,assembly=GRCh38.d1.vd1>; ...; ##contig=<ID=HPV-mSD2,length=7300,assembly=GRCh38.d1.vd1>; ##source=PostprocessGermlineCNVCalls; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	E07002_normal; chr1	10000	CNV_chr1_10000_10999	N	<DEL>,<DUP>	.	.	END=10999	GT:CN:CNLP:CNQ	1:0:0,80,90,100,108,116:80; chr1	11000	CNV_chr1_11000_11999	N	<DEL>,<DUP>	.	.	END=11999	GT:CN:CNLP:CNQ	1:0:0,81,86,89,92,95:81; chr1	12000	CNV_chr1_12000_12999	N	<DEL>,<DUP>	.	.	END=12999	GT:CN:CNLP:CNQ	1:0:0,93,107,119,129,137:93; chr1	13000	CNV_chr1_13000_13999	N	<DEL>,<DUP>	.	.	END=13999	GT:CN:CNLP:CNQ	1:0:0,89,95,99,102,104:89; chr1	14000	CNV_chr1_14000_14999	N	<DEL>,<DUP>	.	.	END=14999	GT:CN:CNLP:CNQ	1:0:0,86,91,93,96,97:86; chr1	15000	CNV_chr1_15000_15999	N	<DEL>,<DUP>	.	.	END=15999	GT:CN:CNLP:CNQ	1:0:0,82,88,92,97,101:",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8619
https://github.com/broadinstitute/gatk/issues/8619:2860,Testability,log,log,2860,"NVCaller --run-mode COHORT -L /outputs/gatk_intervals.interval_list --interval-merging-rule OVERLAPPING_ONLY --annotated-intervals /outputs/gatk_intervals.interval_list.annotated.tsv --contig-ploidy-calls /outputs/COHORT_runDir/COHORT-calls --input /outputs/E07002_normal_alignment.bam.counts.hdf5 --input /outputs/E07002_tumor_alignment.bam.counts.hdf5 --output /outputs/COHORT_runDir --output-prefix COHORT; ```. We used data from `PRJNA399748` project to test. #### Expected behavior. - `test_gatkgermlinecnvcaller_genotyped-intervals-cohort_0.woTimestamp.vcf` (`##contig` cut from header and only first 5 `chr22` CNVs present). ```; ##fileformat=VCFv4.2; ##FORMAT=<ID=CN,Number=1,Type=Integer,Description=""Copy number maximum a posteriori value"">; ##FORMAT=<ID=CNLP,Number=.,Type=Integer,Description=""Copy number log posterior (in Phred-scale) rounded down"">; ##FORMAT=<ID=CNQ,Number=1,Type=Integer,Description=""Genotype call quality as the difference between the best and second best phred-scaled log posterior scores"">; ##FORMAT=<ID=GT,Number=1,Type=Integer,Description=""Genotype"">; ##INFO=<ID=END,Number=1,Type=Integer,Description=""End coordinate of the variant"">; ##contig=<ID=chr1,length=248956422,assembly=GRCh38.d1.vd1>; ...; ##contig=<ID=HPV-mSD2,length=7300,assembly=GRCh38.d1.vd1>; ##source=PostprocessGermlineCNVCalls; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	E07002_normal; chr1	10000	CNV_chr1_10000_10999	N	<DEL>,<DUP>	.	.	END=10999	GT:CN:CNLP:CNQ	1:0:0,80,90,100,108,116:80; chr1	11000	CNV_chr1_11000_11999	N	<DEL>,<DUP>	.	.	END=11999	GT:CN:CNLP:CNQ	1:0:0,81,86,89,92,95:81; chr1	12000	CNV_chr1_12000_12999	N	<DEL>,<DUP>	.	.	END=12999	GT:CN:CNLP:CNQ	1:0:0,93,107,119,129,137:93; chr1	13000	CNV_chr1_13000_13999	N	<DEL>,<DUP>	.	.	END=13999	GT:CN:CNLP:CNQ	1:0:0,89,95,99,102,104:89; chr1	14000	CNV_chr1_14000_14999	N	<DEL>,<DUP>	.	.	END=14999	GT:CN:CNLP:CNQ	1:0:0,86,91,93,96,97:86; chr1	15000	CNV_chr1_15000_15999	N	<DEL>,<DUP>	.	.	END=15999	GT:CN:CNLP:CNQ	1:0:0,82,88,92,97,101:",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8619
https://github.com/broadinstitute/gatk/issues/8619:19625,Testability,log,log,19625,"1:644:1:1; chr22	19895477	CNV_chr22_19895477_19901476	N	<DUP>	2.30	.	END=19901476	GT:CN:NP:QA:QS:QSE:QSS	./.:3:6:1:2:1:0; chr22	19901477	CNV_chr22_19901477_19946476	N	<DEL>	123.62	.	END=19946476	GT:CN:NP:QA:QS:QSE:QSS	1/1:0:45:0:124:1:1; chr22	19946477	CNV_chr22_19946477_19971476	N	<DUP>	6.86	.	END=19971476	GT:CN:NP:QA:QS:QSE:QSS	./.:3:25:0:7:2:1; chr22	19971477	CNV_chr22_19971477_20003000	N	<DEL>	198.68	.	END=20003000	GT:CN:NP:QA:QS:QSE:QSS	1/1:0:32:0:199:2:2. ```. #### Actual behavior. - `gatkgermlinecnvcaller_genotyped-intervals-COHORT_0.woTimestamp.vcf` (`##contig` cut from header and only first 5 `chr22` CNVs present). ```; ##fileformat=VCFv4.2; ##FORMAT=<ID=CN,Number=1,Type=Integer,Description=""Copy number maximum a posteriori value"">; ##FORMAT=<ID=CNLP,Number=.,Type=Integer,Description=""Copy number log posterior (in Phred-scale) rounded down"">; ##FORMAT=<ID=CNQ,Number=1,Type=Integer,Description=""Genotype call quality as the difference between the best and second best phred-scaled log posterior scores"">; ##FORMAT=<ID=GT,Number=1,Type=Integer,Description=""Genotype"">; ##INFO=<ID=END,Number=1,Type=Integer,Description=""End coordinate of the variant"">; ##contig=<ID=chr1,length=248956422,assembly=GRCh38.d1.vd1>; ...; ##contig=<ID=HPV-mSD2,length=7300,assembly=GRCh38.d1.vd1>; ##source=PostprocessGermlineCNVCalls; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	E07002_normal; chr1	10000	CNV_chr1_10000_10999	N	<DEL>,<DUP>	.	.	END=10999	GT:CN:CNLP:CNQ	1:0:0,78,88,96,104,111:78; chr1	11000	CNV_chr1_11000_11999	N	<DEL>,<DUP>	.	.	END=11999	GT:CN:CNLP:CNQ	1:0:0,80,85,88,90,93:80; chr1	12000	CNV_chr1_12000_12999	N	<DEL>,<DUP>	.	.	END=12999	GT:CN:CNLP:CNQ	1:0:0,89,101,110,119,126:89; chr1	13000	CNV_chr1_13000_13999	N	<DEL>,<DUP>	.	.	END=13999	GT:CN:CNLP:CNQ	1:0:0,89,96,101,105,108:89; chr1	14000	CNV_chr1_14000_14999	N	<DEL>,<DUP>	.	.	END=14999	GT:CN:CNLP:CNQ	1:0:0,86,91,94,96,98:86; chr1	15000	CNV_chr1_15000_15999	N	<DEL>,<DUP>	.	.	END=15999	GT:CN:CNLP:CNQ	1:0:0,83,89,94,99,103:",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8619
https://github.com/broadinstitute/gatk/issues/8619:19810,Testability,log,log,19810,"1:644:1:1; chr22	19895477	CNV_chr22_19895477_19901476	N	<DUP>	2.30	.	END=19901476	GT:CN:NP:QA:QS:QSE:QSS	./.:3:6:1:2:1:0; chr22	19901477	CNV_chr22_19901477_19946476	N	<DEL>	123.62	.	END=19946476	GT:CN:NP:QA:QS:QSE:QSS	1/1:0:45:0:124:1:1; chr22	19946477	CNV_chr22_19946477_19971476	N	<DUP>	6.86	.	END=19971476	GT:CN:NP:QA:QS:QSE:QSS	./.:3:25:0:7:2:1; chr22	19971477	CNV_chr22_19971477_20003000	N	<DEL>	198.68	.	END=20003000	GT:CN:NP:QA:QS:QSE:QSS	1/1:0:32:0:199:2:2. ```. #### Actual behavior. - `gatkgermlinecnvcaller_genotyped-intervals-COHORT_0.woTimestamp.vcf` (`##contig` cut from header and only first 5 `chr22` CNVs present). ```; ##fileformat=VCFv4.2; ##FORMAT=<ID=CN,Number=1,Type=Integer,Description=""Copy number maximum a posteriori value"">; ##FORMAT=<ID=CNLP,Number=.,Type=Integer,Description=""Copy number log posterior (in Phred-scale) rounded down"">; ##FORMAT=<ID=CNQ,Number=1,Type=Integer,Description=""Genotype call quality as the difference between the best and second best phred-scaled log posterior scores"">; ##FORMAT=<ID=GT,Number=1,Type=Integer,Description=""Genotype"">; ##INFO=<ID=END,Number=1,Type=Integer,Description=""End coordinate of the variant"">; ##contig=<ID=chr1,length=248956422,assembly=GRCh38.d1.vd1>; ...; ##contig=<ID=HPV-mSD2,length=7300,assembly=GRCh38.d1.vd1>; ##source=PostprocessGermlineCNVCalls; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	E07002_normal; chr1	10000	CNV_chr1_10000_10999	N	<DEL>,<DUP>	.	.	END=10999	GT:CN:CNLP:CNQ	1:0:0,78,88,96,104,111:78; chr1	11000	CNV_chr1_11000_11999	N	<DEL>,<DUP>	.	.	END=11999	GT:CN:CNLP:CNQ	1:0:0,80,85,88,90,93:80; chr1	12000	CNV_chr1_12000_12999	N	<DEL>,<DUP>	.	.	END=12999	GT:CN:CNLP:CNQ	1:0:0,89,101,110,119,126:89; chr1	13000	CNV_chr1_13000_13999	N	<DEL>,<DUP>	.	.	END=13999	GT:CN:CNLP:CNQ	1:0:0,89,96,101,105,108:89; chr1	14000	CNV_chr1_14000_14999	N	<DEL>,<DUP>	.	.	END=14999	GT:CN:CNLP:CNQ	1:0:0,86,91,94,96,98:86; chr1	15000	CNV_chr1_15000_15999	N	<DEL>,<DUP>	.	.	END=15999	GT:CN:CNLP:CNQ	1:0:0,83,89,94,99,103:",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8619
https://github.com/broadinstitute/gatk/pull/8623:970,Deployability,integrat,integration,970,"The SVConcordance tool is currently too inefficient in terms of memory usage, requiring several 100's of GB of heap space on ~100K samples. This PR aims to reduce memory usage in two ways:. 1. Truth VCF records are stripped of all genotype fields except `GT` and `CN`, which are necessary and sufficient for concordance computations.; 2. A new option `--do-not-sort` is introduced to skip output record sorting. A major source of heap usage is the output buffer in the `ClosestSVFinder` class, which ensures records are emitted in coordinate-sorted order. This buffer quickly fills, however, when there is at least one record being actively clustered that spans a large interval because the buffer cannot be flushed until a variant beyond the maximal clusterable coordinate of that large variant is encountered. This option will allow users to substantially reduce max heap usage on larger call sets (a single SVRecord can consume ~100MB with 100K samples). Includes an integration test to cover the `--do-not-sort` functionality.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8623
https://github.com/broadinstitute/gatk/pull/8623:156,Energy Efficiency,reduce,reduce,156,"The SVConcordance tool is currently too inefficient in terms of memory usage, requiring several 100's of GB of heap space on ~100K samples. This PR aims to reduce memory usage in two ways:. 1. Truth VCF records are stripped of all genotype fields except `GT` and `CN`, which are necessary and sufficient for concordance computations.; 2. A new option `--do-not-sort` is introduced to skip output record sorting. A major source of heap usage is the output buffer in the `ClosestSVFinder` class, which ensures records are emitted in coordinate-sorted order. This buffer quickly fills, however, when there is at least one record being actively clustered that spans a large interval because the buffer cannot be flushed until a variant beyond the maximal clusterable coordinate of that large variant is encountered. This option will allow users to substantially reduce max heap usage on larger call sets (a single SVRecord can consume ~100MB with 100K samples). Includes an integration test to cover the `--do-not-sort` functionality.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8623
https://github.com/broadinstitute/gatk/pull/8623:858,Energy Efficiency,reduce,reduce,858,"The SVConcordance tool is currently too inefficient in terms of memory usage, requiring several 100's of GB of heap space on ~100K samples. This PR aims to reduce memory usage in two ways:. 1. Truth VCF records are stripped of all genotype fields except `GT` and `CN`, which are necessary and sufficient for concordance computations.; 2. A new option `--do-not-sort` is introduced to skip output record sorting. A major source of heap usage is the output buffer in the `ClosestSVFinder` class, which ensures records are emitted in coordinate-sorted order. This buffer quickly fills, however, when there is at least one record being actively clustered that spans a large interval because the buffer cannot be flushed until a variant beyond the maximal clusterable coordinate of that large variant is encountered. This option will allow users to substantially reduce max heap usage on larger call sets (a single SVRecord can consume ~100MB with 100K samples). Includes an integration test to cover the `--do-not-sort` functionality.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8623
https://github.com/broadinstitute/gatk/pull/8623:970,Integrability,integrat,integration,970,"The SVConcordance tool is currently too inefficient in terms of memory usage, requiring several 100's of GB of heap space on ~100K samples. This PR aims to reduce memory usage in two ways:. 1. Truth VCF records are stripped of all genotype fields except `GT` and `CN`, which are necessary and sufficient for concordance computations.; 2. A new option `--do-not-sort` is introduced to skip output record sorting. A major source of heap usage is the output buffer in the `ClosestSVFinder` class, which ensures records are emitted in coordinate-sorted order. This buffer quickly fills, however, when there is at least one record being actively clustered that spans a large interval because the buffer cannot be flushed until a variant beyond the maximal clusterable coordinate of that large variant is encountered. This option will allow users to substantially reduce max heap usage on larger call sets (a single SVRecord can consume ~100MB with 100K samples). Includes an integration test to cover the `--do-not-sort` functionality.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8623
https://github.com/broadinstitute/gatk/pull/8623:982,Testability,test,test,982,"The SVConcordance tool is currently too inefficient in terms of memory usage, requiring several 100's of GB of heap space on ~100K samples. This PR aims to reduce memory usage in two ways:. 1. Truth VCF records are stripped of all genotype fields except `GT` and `CN`, which are necessary and sufficient for concordance computations.; 2. A new option `--do-not-sort` is introduced to skip output record sorting. A major source of heap usage is the output buffer in the `ClosestSVFinder` class, which ensures records are emitted in coordinate-sorted order. This buffer quickly fills, however, when there is at least one record being actively clustered that spans a large interval because the buffer cannot be flushed until a variant beyond the maximal clusterable coordinate of that large variant is encountered. This option will allow users to substantially reduce max heap usage on larger call sets (a single SVRecord can consume ~100MB with 100K samples). Includes an integration test to cover the `--do-not-sort` functionality.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8623
https://github.com/broadinstitute/gatk/pull/8626:17,Security,access,access,17,This should http access more seem less in a lot of places. . The way this handles query parameters is not ideal for signed url cases so we'll need to revisit that.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8626
https://github.com/broadinstitute/gatk/issues/8628:2584,Availability,down,down,2584,"am.counts.hdf5. /soft/gatk-4.5.0.0/gatk GermlineCNVCaller --run-mode COHORT -L /outputs/gatk_intervals.interval_list --interval-merging-rule OVERLAPPING_ONLY --annotated-intervals /outputs/gatk_intervals.interval_list.annotated.tsv --contig-ploidy-calls /outputs/COHORT_runDir/COHORT-calls --input /outputs/E07002_normal_alignment.bam.counts.hdf5 --input /outputs/E07002_tumor_alignment.bam.counts.hdf5 --output /outputs/COHORT_runDir --output-prefix COHORT; ```; #### Expected behavior; - `test_gatkgermlinecnvcaller_genotyped-intervals-cohort_0.woTimestamp.vcf` (`##contig` cut from header and only first 5 `chr22` CNVs present). ```; ##fileformat=VCFv4.2; ##FORMAT=<ID=CN,Number=1,Type=Integer,Description=""Copy number maximum a posteriori value"">; ##FORMAT=<ID=CNLP,Number=.,Type=Integer,Description=""Copy number log posterior (in Phred-scale) rounded down"">; ##FORMAT=<ID=CNQ,Number=1,Type=Integer,Description=""Genotype call quality as the difference between the best and second best phred-scaled log posterior scores"">; ##FORMAT=<ID=GT,Number=1,Type=Integer,Description=""Genotype"">; ##INFO=<ID=END,Number=1,Type=Integer,Description=""End coordinate of the variant"">; ##contig=<ID=chr1,length=248956422,assembly=GRCh38.d1.vd1>; ...; ##contig=<ID=HPV-mSD2,length=7300,assembly=GRCh38.d1.vd1>; ##source=PostprocessGermlineCNVCalls; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	E07002_normal; chr1	10000	CNV_chr1_10000_10999	N	<DEL>,<DUP>	.	.	END=10999	GT:CN:CNLP:CNQ	1:0:0,78,88,96,104,111:78; chr1	11000	CNV_chr1_11000_11999	N	<DEL>,<DUP>	.	.	END=11999	GT:CN:CNLP:CNQ	1:0:0,80,85,88,90,93:80; chr1	12000	CNV_chr1_12000_12999	N	<DEL>,<DUP>	.	.	END=12999	GT:CN:CNLP:CNQ	1:0:0,89,101,110,119,126:89; chr1	13000	CNV_chr1_13000_13999	N	<DEL>,<DUP>	.	.	END=13999	GT:CN:CNLP:CNQ	1:0:0,89,96,101,105,108:89; chr1	14000	CNV_chr1_14000_14999	N	<DEL>,<DUP>	.	.	END=14999	GT:CN:CNLP:CNQ	1:0:0,86,91,94,96,98:86; chr1	15000	CNV_chr1_15000_15999	N	<DEL>,<DUP>	.	.	END=15999	GT:CN:CNLP:CNQ	1:0:0,83,89,94,99,103:",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8628
https://github.com/broadinstitute/gatk/issues/8628:19661,Availability,down,down,19661,":44:1:689:1:1; chr22	19895477	CNV_chr22_19895477_19901476	N	<DUP>	2.39	.	END=19901476	GT:CN:NP:QA:QS:QSE:QSS	./.:3:6:1:2:1:0; chr22	19901477	CNV_chr22_19901477_19946476	N	<DEL>	114.12	.	END=19946476	GT:CN:NP:QA:QS:QSE:QSS	1/1:0:45:0:114:1:1; chr22	19946477	CNV_chr22_19946477_19971476	N	<DUP>	4.69	.	END=19971476	GT:CN:NP:QA:QS:QSE:QSS	./.:3:25:0:5:2:0; chr22	19971477	CNV_chr22_19971477_20003000	N	<DEL>	96.55	.	END=20003000	GT:CN:NP:QA:QS:QSE:QSS	1/1:0:32:0:97:2:2. ```. #### Actual behavior. - `gatkgermlinecnvcaller_genotyped-intervals-COHORT_0.woTimestamp.vcf` (`##contig` cut from header and only first 5 chr22 CNVs present). ```; ##fileformat=VCFv4.2; ##FORMAT=<ID=CN,Number=1,Type=Integer,Description=""Copy number maximum a posteriori value"">; ##FORMAT=<ID=CNLP,Number=.,Type=Integer,Description=""Copy number log posterior (in Phred-scale) rounded down"">; ##FORMAT=<ID=CNQ,Number=1,Type=Integer,Description=""Genotype call quality as the difference between the best and second best phred-scaled log posterior scores"">; ##FORMAT=<ID=GT,Number=1,Type=Integer,Description=""Genotype"">; ##INFO=<ID=END,Number=1,Type=Integer,Description=""End coordinate of the variant"">; ##contig=<ID=chr1,length=248956422,assembly=GRCh38.d1.vd1>; ...; ##contig=<ID=HPV-mSD2,length=7300,assembly=GRCh38.d1.vd1>; ##source=PostprocessGermlineCNVCalls; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	E07002_normal; chr1	10000	CNV_chr1_10000_10999	N	<DEL>,<DUP>	.	.	END=10999	GT:CN:CNLP:CNQ	0:2:30,32,0,33,33,33:30; chr1	11000	CNV_chr1_11000_11999	N	<DEL>,<DUP>	.	.	END=11999	GT:CN:CNLP:CNQ	0:2:30,32,0,33,33,33:30; chr1	12000	CNV_chr1_12000_12999	N	<DEL>,<DUP>	.	.	END=12999	GT:CN:CNLP:CNQ	0:2:30,32,0,33,33,33:30; chr1	13000	CNV_chr1_13000_13999	N	<DEL>,<DUP>	.	.	END=13999	GT:CN:CNLP:CNQ	0:2:30,32,0,33,33,33:30; chr1	14000	CNV_chr1_14000_14999	N	<DEL>,<DUP>	.	.	END=14999	GT:CN:CNLP:CNQ	0:2:29,32,0,33,33,33:29; chr1	15000	CNV_chr1_15000_15999	N	<DEL>,<DUP>	.	.	END=15999	GT:CN:CNLP:CNQ	0:2:29,32,0,33,33,33:29; chr1	1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8628
https://github.com/broadinstitute/gatk/issues/8628:114,Deployability,release,release,114,"## Bug Report. ### Affected tool(s) or class(es); GermlineCNVCaller. ### Affected version(s); - [x] Latest public release version [4.5.0.0]; - [x] Latest master branch as of [14.12.2023]. ### Description ; Very different results after update from 4.4.0.0 to 4.5.0.0. We updated test results after https://github.com/broadinstitute/gatk/issues/8619, but now we see big changes (especially in `segments` file). #### Steps to reproduce; Command list:; ```; /soft/gatk-4.5.0.0/gatk PreprocessIntervals -R /ref/GRCh38.d1.vd1/GRCh38.d1.vd1.fa --padding 0 -L chr1:10000-35000 -L chr22:198477-20003000 -imr OVERLAPPING_ONLY -O /outputs/gatk_intervals.interval_list. /soft/gatk-4.5.0.0/gatk AnnotateIntervals -L /outputs/gatk_intervals.interval_list -R /ref/GRCh38.d1.vd1/GRCh38.d1.vd1.fa -imr OVERLAPPING_ONLY -O /outputs/gatk_intervals.interval_list.annotated.tsv. /soft/gatk-4.5.0.0/gatk CollectReadCounts -I /inputs/E07002_normal_alignment.bam -R /ref/GRCh38.d1.vd1/GRCh38.d1.vd1.fa -L /outputs/gatk_intervals.interval_list --interval-merging-rule OVERLAPPING_ONLY -O /outputs/E07002_normal_alignment.bam.counts.hdf5; /soft/gatk-4.5.0.0/gatk CollectReadCounts -I /inputs/E07002_tumor_alignment.bam -R /ref/GRCh38.d1.vd1/GRCh38.d1.vd1.fa -L /outputs/gatk_intervals.interval_list --interval-merging-rule OVERLAPPING_ONLY -O /outputs/E07002_tumor_alignment.bam.counts.hdf5. /soft/gatk-4.5.0.0/gatk DetermineGermlineContigPloidy -L /outputs/gatk_intervals.interval_list --interval-merging-rule OVERLAPPING_ONLY --contig-ploidy-priors /outputs/a_valid_ploidy_priors_table.tsv.copy.tsv --output /outputs/COHORT_runDir --output-prefix COHORT --input /outputs/E07002_normal_alignment.bam.counts.hdf5 --input /outputs/E07002_tumor_alignment.bam.counts.hdf5. /soft/gatk-4.5.0.0/gatk GermlineCNVCaller --run-mode COHORT -L /outputs/gatk_intervals.interval_list --interval-merging-rule OVERLAPPING_ONLY --annotated-intervals /outputs/gatk_intervals.interval_list.annotated.tsv --contig-ploidy-calls /outputs/COHORT_run",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8628
https://github.com/broadinstitute/gatk/issues/8628:235,Deployability,update,update,235,"## Bug Report. ### Affected tool(s) or class(es); GermlineCNVCaller. ### Affected version(s); - [x] Latest public release version [4.5.0.0]; - [x] Latest master branch as of [14.12.2023]. ### Description ; Very different results after update from 4.4.0.0 to 4.5.0.0. We updated test results after https://github.com/broadinstitute/gatk/issues/8619, but now we see big changes (especially in `segments` file). #### Steps to reproduce; Command list:; ```; /soft/gatk-4.5.0.0/gatk PreprocessIntervals -R /ref/GRCh38.d1.vd1/GRCh38.d1.vd1.fa --padding 0 -L chr1:10000-35000 -L chr22:198477-20003000 -imr OVERLAPPING_ONLY -O /outputs/gatk_intervals.interval_list. /soft/gatk-4.5.0.0/gatk AnnotateIntervals -L /outputs/gatk_intervals.interval_list -R /ref/GRCh38.d1.vd1/GRCh38.d1.vd1.fa -imr OVERLAPPING_ONLY -O /outputs/gatk_intervals.interval_list.annotated.tsv. /soft/gatk-4.5.0.0/gatk CollectReadCounts -I /inputs/E07002_normal_alignment.bam -R /ref/GRCh38.d1.vd1/GRCh38.d1.vd1.fa -L /outputs/gatk_intervals.interval_list --interval-merging-rule OVERLAPPING_ONLY -O /outputs/E07002_normal_alignment.bam.counts.hdf5; /soft/gatk-4.5.0.0/gatk CollectReadCounts -I /inputs/E07002_tumor_alignment.bam -R /ref/GRCh38.d1.vd1/GRCh38.d1.vd1.fa -L /outputs/gatk_intervals.interval_list --interval-merging-rule OVERLAPPING_ONLY -O /outputs/E07002_tumor_alignment.bam.counts.hdf5. /soft/gatk-4.5.0.0/gatk DetermineGermlineContigPloidy -L /outputs/gatk_intervals.interval_list --interval-merging-rule OVERLAPPING_ONLY --contig-ploidy-priors /outputs/a_valid_ploidy_priors_table.tsv.copy.tsv --output /outputs/COHORT_runDir --output-prefix COHORT --input /outputs/E07002_normal_alignment.bam.counts.hdf5 --input /outputs/E07002_tumor_alignment.bam.counts.hdf5. /soft/gatk-4.5.0.0/gatk GermlineCNVCaller --run-mode COHORT -L /outputs/gatk_intervals.interval_list --interval-merging-rule OVERLAPPING_ONLY --annotated-intervals /outputs/gatk_intervals.interval_list.annotated.tsv --contig-ploidy-calls /outputs/COHORT_run",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8628
https://github.com/broadinstitute/gatk/issues/8628:270,Deployability,update,updated,270,"## Bug Report. ### Affected tool(s) or class(es); GermlineCNVCaller. ### Affected version(s); - [x] Latest public release version [4.5.0.0]; - [x] Latest master branch as of [14.12.2023]. ### Description ; Very different results after update from 4.4.0.0 to 4.5.0.0. We updated test results after https://github.com/broadinstitute/gatk/issues/8619, but now we see big changes (especially in `segments` file). #### Steps to reproduce; Command list:; ```; /soft/gatk-4.5.0.0/gatk PreprocessIntervals -R /ref/GRCh38.d1.vd1/GRCh38.d1.vd1.fa --padding 0 -L chr1:10000-35000 -L chr22:198477-20003000 -imr OVERLAPPING_ONLY -O /outputs/gatk_intervals.interval_list. /soft/gatk-4.5.0.0/gatk AnnotateIntervals -L /outputs/gatk_intervals.interval_list -R /ref/GRCh38.d1.vd1/GRCh38.d1.vd1.fa -imr OVERLAPPING_ONLY -O /outputs/gatk_intervals.interval_list.annotated.tsv. /soft/gatk-4.5.0.0/gatk CollectReadCounts -I /inputs/E07002_normal_alignment.bam -R /ref/GRCh38.d1.vd1/GRCh38.d1.vd1.fa -L /outputs/gatk_intervals.interval_list --interval-merging-rule OVERLAPPING_ONLY -O /outputs/E07002_normal_alignment.bam.counts.hdf5; /soft/gatk-4.5.0.0/gatk CollectReadCounts -I /inputs/E07002_tumor_alignment.bam -R /ref/GRCh38.d1.vd1/GRCh38.d1.vd1.fa -L /outputs/gatk_intervals.interval_list --interval-merging-rule OVERLAPPING_ONLY -O /outputs/E07002_tumor_alignment.bam.counts.hdf5. /soft/gatk-4.5.0.0/gatk DetermineGermlineContigPloidy -L /outputs/gatk_intervals.interval_list --interval-merging-rule OVERLAPPING_ONLY --contig-ploidy-priors /outputs/a_valid_ploidy_priors_table.tsv.copy.tsv --output /outputs/COHORT_runDir --output-prefix COHORT --input /outputs/E07002_normal_alignment.bam.counts.hdf5 --input /outputs/E07002_tumor_alignment.bam.counts.hdf5. /soft/gatk-4.5.0.0/gatk GermlineCNVCaller --run-mode COHORT -L /outputs/gatk_intervals.interval_list --interval-merging-rule OVERLAPPING_ONLY --annotated-intervals /outputs/gatk_intervals.interval_list.annotated.tsv --contig-ploidy-calls /outputs/COHORT_run",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8628
https://github.com/broadinstitute/gatk/issues/8628:278,Testability,test,test,278,"## Bug Report. ### Affected tool(s) or class(es); GermlineCNVCaller. ### Affected version(s); - [x] Latest public release version [4.5.0.0]; - [x] Latest master branch as of [14.12.2023]. ### Description ; Very different results after update from 4.4.0.0 to 4.5.0.0. We updated test results after https://github.com/broadinstitute/gatk/issues/8619, but now we see big changes (especially in `segments` file). #### Steps to reproduce; Command list:; ```; /soft/gatk-4.5.0.0/gatk PreprocessIntervals -R /ref/GRCh38.d1.vd1/GRCh38.d1.vd1.fa --padding 0 -L chr1:10000-35000 -L chr22:198477-20003000 -imr OVERLAPPING_ONLY -O /outputs/gatk_intervals.interval_list. /soft/gatk-4.5.0.0/gatk AnnotateIntervals -L /outputs/gatk_intervals.interval_list -R /ref/GRCh38.d1.vd1/GRCh38.d1.vd1.fa -imr OVERLAPPING_ONLY -O /outputs/gatk_intervals.interval_list.annotated.tsv. /soft/gatk-4.5.0.0/gatk CollectReadCounts -I /inputs/E07002_normal_alignment.bam -R /ref/GRCh38.d1.vd1/GRCh38.d1.vd1.fa -L /outputs/gatk_intervals.interval_list --interval-merging-rule OVERLAPPING_ONLY -O /outputs/E07002_normal_alignment.bam.counts.hdf5; /soft/gatk-4.5.0.0/gatk CollectReadCounts -I /inputs/E07002_tumor_alignment.bam -R /ref/GRCh38.d1.vd1/GRCh38.d1.vd1.fa -L /outputs/gatk_intervals.interval_list --interval-merging-rule OVERLAPPING_ONLY -O /outputs/E07002_tumor_alignment.bam.counts.hdf5. /soft/gatk-4.5.0.0/gatk DetermineGermlineContigPloidy -L /outputs/gatk_intervals.interval_list --interval-merging-rule OVERLAPPING_ONLY --contig-ploidy-priors /outputs/a_valid_ploidy_priors_table.tsv.copy.tsv --output /outputs/COHORT_runDir --output-prefix COHORT --input /outputs/E07002_normal_alignment.bam.counts.hdf5 --input /outputs/E07002_tumor_alignment.bam.counts.hdf5. /soft/gatk-4.5.0.0/gatk GermlineCNVCaller --run-mode COHORT -L /outputs/gatk_intervals.interval_list --interval-merging-rule OVERLAPPING_ONLY --annotated-intervals /outputs/gatk_intervals.interval_list.annotated.tsv --contig-ploidy-calls /outputs/COHORT_run",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8628
https://github.com/broadinstitute/gatk/issues/8628:2545,Testability,log,log,2545,"am.counts.hdf5. /soft/gatk-4.5.0.0/gatk GermlineCNVCaller --run-mode COHORT -L /outputs/gatk_intervals.interval_list --interval-merging-rule OVERLAPPING_ONLY --annotated-intervals /outputs/gatk_intervals.interval_list.annotated.tsv --contig-ploidy-calls /outputs/COHORT_runDir/COHORT-calls --input /outputs/E07002_normal_alignment.bam.counts.hdf5 --input /outputs/E07002_tumor_alignment.bam.counts.hdf5 --output /outputs/COHORT_runDir --output-prefix COHORT; ```; #### Expected behavior; - `test_gatkgermlinecnvcaller_genotyped-intervals-cohort_0.woTimestamp.vcf` (`##contig` cut from header and only first 5 `chr22` CNVs present). ```; ##fileformat=VCFv4.2; ##FORMAT=<ID=CN,Number=1,Type=Integer,Description=""Copy number maximum a posteriori value"">; ##FORMAT=<ID=CNLP,Number=.,Type=Integer,Description=""Copy number log posterior (in Phred-scale) rounded down"">; ##FORMAT=<ID=CNQ,Number=1,Type=Integer,Description=""Genotype call quality as the difference between the best and second best phred-scaled log posterior scores"">; ##FORMAT=<ID=GT,Number=1,Type=Integer,Description=""Genotype"">; ##INFO=<ID=END,Number=1,Type=Integer,Description=""End coordinate of the variant"">; ##contig=<ID=chr1,length=248956422,assembly=GRCh38.d1.vd1>; ...; ##contig=<ID=HPV-mSD2,length=7300,assembly=GRCh38.d1.vd1>; ##source=PostprocessGermlineCNVCalls; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	E07002_normal; chr1	10000	CNV_chr1_10000_10999	N	<DEL>,<DUP>	.	.	END=10999	GT:CN:CNLP:CNQ	1:0:0,78,88,96,104,111:78; chr1	11000	CNV_chr1_11000_11999	N	<DEL>,<DUP>	.	.	END=11999	GT:CN:CNLP:CNQ	1:0:0,80,85,88,90,93:80; chr1	12000	CNV_chr1_12000_12999	N	<DEL>,<DUP>	.	.	END=12999	GT:CN:CNLP:CNQ	1:0:0,89,101,110,119,126:89; chr1	13000	CNV_chr1_13000_13999	N	<DEL>,<DUP>	.	.	END=13999	GT:CN:CNLP:CNQ	1:0:0,89,96,101,105,108:89; chr1	14000	CNV_chr1_14000_14999	N	<DEL>,<DUP>	.	.	END=14999	GT:CN:CNLP:CNQ	1:0:0,86,91,94,96,98:86; chr1	15000	CNV_chr1_15000_15999	N	<DEL>,<DUP>	.	.	END=15999	GT:CN:CNLP:CNQ	1:0:0,83,89,94,99,103:",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8628
https://github.com/broadinstitute/gatk/issues/8628:2730,Testability,log,log,2730,"am.counts.hdf5. /soft/gatk-4.5.0.0/gatk GermlineCNVCaller --run-mode COHORT -L /outputs/gatk_intervals.interval_list --interval-merging-rule OVERLAPPING_ONLY --annotated-intervals /outputs/gatk_intervals.interval_list.annotated.tsv --contig-ploidy-calls /outputs/COHORT_runDir/COHORT-calls --input /outputs/E07002_normal_alignment.bam.counts.hdf5 --input /outputs/E07002_tumor_alignment.bam.counts.hdf5 --output /outputs/COHORT_runDir --output-prefix COHORT; ```; #### Expected behavior; - `test_gatkgermlinecnvcaller_genotyped-intervals-cohort_0.woTimestamp.vcf` (`##contig` cut from header and only first 5 `chr22` CNVs present). ```; ##fileformat=VCFv4.2; ##FORMAT=<ID=CN,Number=1,Type=Integer,Description=""Copy number maximum a posteriori value"">; ##FORMAT=<ID=CNLP,Number=.,Type=Integer,Description=""Copy number log posterior (in Phred-scale) rounded down"">; ##FORMAT=<ID=CNQ,Number=1,Type=Integer,Description=""Genotype call quality as the difference between the best and second best phred-scaled log posterior scores"">; ##FORMAT=<ID=GT,Number=1,Type=Integer,Description=""Genotype"">; ##INFO=<ID=END,Number=1,Type=Integer,Description=""End coordinate of the variant"">; ##contig=<ID=chr1,length=248956422,assembly=GRCh38.d1.vd1>; ...; ##contig=<ID=HPV-mSD2,length=7300,assembly=GRCh38.d1.vd1>; ##source=PostprocessGermlineCNVCalls; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	E07002_normal; chr1	10000	CNV_chr1_10000_10999	N	<DEL>,<DUP>	.	.	END=10999	GT:CN:CNLP:CNQ	1:0:0,78,88,96,104,111:78; chr1	11000	CNV_chr1_11000_11999	N	<DEL>,<DUP>	.	.	END=11999	GT:CN:CNLP:CNQ	1:0:0,80,85,88,90,93:80; chr1	12000	CNV_chr1_12000_12999	N	<DEL>,<DUP>	.	.	END=12999	GT:CN:CNLP:CNQ	1:0:0,89,101,110,119,126:89; chr1	13000	CNV_chr1_13000_13999	N	<DEL>,<DUP>	.	.	END=13999	GT:CN:CNLP:CNQ	1:0:0,89,96,101,105,108:89; chr1	14000	CNV_chr1_14000_14999	N	<DEL>,<DUP>	.	.	END=14999	GT:CN:CNLP:CNQ	1:0:0,86,91,94,96,98:86; chr1	15000	CNV_chr1_15000_15999	N	<DEL>,<DUP>	.	.	END=15999	GT:CN:CNLP:CNQ	1:0:0,83,89,94,99,103:",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8628
https://github.com/broadinstitute/gatk/issues/8628:19622,Testability,log,log,19622,":44:1:689:1:1; chr22	19895477	CNV_chr22_19895477_19901476	N	<DUP>	2.39	.	END=19901476	GT:CN:NP:QA:QS:QSE:QSS	./.:3:6:1:2:1:0; chr22	19901477	CNV_chr22_19901477_19946476	N	<DEL>	114.12	.	END=19946476	GT:CN:NP:QA:QS:QSE:QSS	1/1:0:45:0:114:1:1; chr22	19946477	CNV_chr22_19946477_19971476	N	<DUP>	4.69	.	END=19971476	GT:CN:NP:QA:QS:QSE:QSS	./.:3:25:0:5:2:0; chr22	19971477	CNV_chr22_19971477_20003000	N	<DEL>	96.55	.	END=20003000	GT:CN:NP:QA:QS:QSE:QSS	1/1:0:32:0:97:2:2. ```. #### Actual behavior. - `gatkgermlinecnvcaller_genotyped-intervals-COHORT_0.woTimestamp.vcf` (`##contig` cut from header and only first 5 chr22 CNVs present). ```; ##fileformat=VCFv4.2; ##FORMAT=<ID=CN,Number=1,Type=Integer,Description=""Copy number maximum a posteriori value"">; ##FORMAT=<ID=CNLP,Number=.,Type=Integer,Description=""Copy number log posterior (in Phred-scale) rounded down"">; ##FORMAT=<ID=CNQ,Number=1,Type=Integer,Description=""Genotype call quality as the difference between the best and second best phred-scaled log posterior scores"">; ##FORMAT=<ID=GT,Number=1,Type=Integer,Description=""Genotype"">; ##INFO=<ID=END,Number=1,Type=Integer,Description=""End coordinate of the variant"">; ##contig=<ID=chr1,length=248956422,assembly=GRCh38.d1.vd1>; ...; ##contig=<ID=HPV-mSD2,length=7300,assembly=GRCh38.d1.vd1>; ##source=PostprocessGermlineCNVCalls; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	E07002_normal; chr1	10000	CNV_chr1_10000_10999	N	<DEL>,<DUP>	.	.	END=10999	GT:CN:CNLP:CNQ	0:2:30,32,0,33,33,33:30; chr1	11000	CNV_chr1_11000_11999	N	<DEL>,<DUP>	.	.	END=11999	GT:CN:CNLP:CNQ	0:2:30,32,0,33,33,33:30; chr1	12000	CNV_chr1_12000_12999	N	<DEL>,<DUP>	.	.	END=12999	GT:CN:CNLP:CNQ	0:2:30,32,0,33,33,33:30; chr1	13000	CNV_chr1_13000_13999	N	<DEL>,<DUP>	.	.	END=13999	GT:CN:CNLP:CNQ	0:2:30,32,0,33,33,33:30; chr1	14000	CNV_chr1_14000_14999	N	<DEL>,<DUP>	.	.	END=14999	GT:CN:CNLP:CNQ	0:2:29,32,0,33,33,33:29; chr1	15000	CNV_chr1_15000_15999	N	<DEL>,<DUP>	.	.	END=15999	GT:CN:CNLP:CNQ	0:2:29,32,0,33,33,33:29; chr1	1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8628
https://github.com/broadinstitute/gatk/issues/8628:19807,Testability,log,log,19807,":44:1:689:1:1; chr22	19895477	CNV_chr22_19895477_19901476	N	<DUP>	2.39	.	END=19901476	GT:CN:NP:QA:QS:QSE:QSS	./.:3:6:1:2:1:0; chr22	19901477	CNV_chr22_19901477_19946476	N	<DEL>	114.12	.	END=19946476	GT:CN:NP:QA:QS:QSE:QSS	1/1:0:45:0:114:1:1; chr22	19946477	CNV_chr22_19946477_19971476	N	<DUP>	4.69	.	END=19971476	GT:CN:NP:QA:QS:QSE:QSS	./.:3:25:0:5:2:0; chr22	19971477	CNV_chr22_19971477_20003000	N	<DEL>	96.55	.	END=20003000	GT:CN:NP:QA:QS:QSE:QSS	1/1:0:32:0:97:2:2. ```. #### Actual behavior. - `gatkgermlinecnvcaller_genotyped-intervals-COHORT_0.woTimestamp.vcf` (`##contig` cut from header and only first 5 chr22 CNVs present). ```; ##fileformat=VCFv4.2; ##FORMAT=<ID=CN,Number=1,Type=Integer,Description=""Copy number maximum a posteriori value"">; ##FORMAT=<ID=CNLP,Number=.,Type=Integer,Description=""Copy number log posterior (in Phred-scale) rounded down"">; ##FORMAT=<ID=CNQ,Number=1,Type=Integer,Description=""Genotype call quality as the difference between the best and second best phred-scaled log posterior scores"">; ##FORMAT=<ID=GT,Number=1,Type=Integer,Description=""Genotype"">; ##INFO=<ID=END,Number=1,Type=Integer,Description=""End coordinate of the variant"">; ##contig=<ID=chr1,length=248956422,assembly=GRCh38.d1.vd1>; ...; ##contig=<ID=HPV-mSD2,length=7300,assembly=GRCh38.d1.vd1>; ##source=PostprocessGermlineCNVCalls; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	E07002_normal; chr1	10000	CNV_chr1_10000_10999	N	<DEL>,<DUP>	.	.	END=10999	GT:CN:CNLP:CNQ	0:2:30,32,0,33,33,33:30; chr1	11000	CNV_chr1_11000_11999	N	<DEL>,<DUP>	.	.	END=11999	GT:CN:CNLP:CNQ	0:2:30,32,0,33,33,33:30; chr1	12000	CNV_chr1_12000_12999	N	<DEL>,<DUP>	.	.	END=12999	GT:CN:CNLP:CNQ	0:2:30,32,0,33,33,33:30; chr1	13000	CNV_chr1_13000_13999	N	<DEL>,<DUP>	.	.	END=13999	GT:CN:CNLP:CNQ	0:2:30,32,0,33,33,33:30; chr1	14000	CNV_chr1_14000_14999	N	<DEL>,<DUP>	.	.	END=14999	GT:CN:CNLP:CNQ	0:2:29,32,0,33,33,33:29; chr1	15000	CNV_chr1_15000_15999	N	<DEL>,<DUP>	.	.	END=15999	GT:CN:CNLP:CNQ	0:2:29,32,0,33,33,33:29; chr1	1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8628
https://github.com/broadinstitute/gatk/issues/8629:1346,Availability,down,downstream,1346,"I am using GATK 4.4.0.0 via the official docker release to reheader output from SVABA with an appropriate sequence dictionary. I am using `UpdateVCFSequenceDictionary` for this purpose with the following command: . ```; singularity exec -B ""$PWD"" broadinstitute-gatk-4.4.0.0.img gatk UpdateVCFSequenceDictionary --source-dictionary Mus_musculus.GRCm39.dna.primary_assembly.dict -V svaba.somatic.indel.vcf --replace true -O svaba.somatic.indel.vcf.reheaded.vcf; ```. I have encountered a curious behavior, where by the tool is not simply adjusting the sequence dictionary, but is also modifying a FORMAT field. . Original VCF header: . ```; ##FORMAT=<ID=GQ,Number=1,Type=String,Description=""Genotype quality (currently not supported. Always 0)"">; ```. Updated VCF header: . ```; ##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=""Genotype Quality"">; ```. From what I can see, the updated text is used frequently in your GATK VCF files, but I can't dig out the specific code where it is being set via `UpdateVCFSequenceDictionary`. I am wondering if there is a collision where `UpdateVCFSequenceDictionary` detects GQ and prints a stock header field to match expectation, rather than leaving it alone. I would expect the tool to simply replace the dictionary portion of the VCF without modifying the FORMAT/INFO fields. This is causing issues with downstream analysis because SVABA QC values are float/string not integer.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8629
https://github.com/broadinstitute/gatk/issues/8629:48,Deployability,release,release,48,"I am using GATK 4.4.0.0 via the official docker release to reheader output from SVABA with an appropriate sequence dictionary. I am using `UpdateVCFSequenceDictionary` for this purpose with the following command: . ```; singularity exec -B ""$PWD"" broadinstitute-gatk-4.4.0.0.img gatk UpdateVCFSequenceDictionary --source-dictionary Mus_musculus.GRCm39.dna.primary_assembly.dict -V svaba.somatic.indel.vcf --replace true -O svaba.somatic.indel.vcf.reheaded.vcf; ```. I have encountered a curious behavior, where by the tool is not simply adjusting the sequence dictionary, but is also modifying a FORMAT field. . Original VCF header: . ```; ##FORMAT=<ID=GQ,Number=1,Type=String,Description=""Genotype quality (currently not supported. Always 0)"">; ```. Updated VCF header: . ```; ##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=""Genotype Quality"">; ```. From what I can see, the updated text is used frequently in your GATK VCF files, but I can't dig out the specific code where it is being set via `UpdateVCFSequenceDictionary`. I am wondering if there is a collision where `UpdateVCFSequenceDictionary` detects GQ and prints a stock header field to match expectation, rather than leaving it alone. I would expect the tool to simply replace the dictionary portion of the VCF without modifying the FORMAT/INFO fields. This is causing issues with downstream analysis because SVABA QC values are float/string not integer.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8629
https://github.com/broadinstitute/gatk/issues/8629:139,Deployability,Update,UpdateVCFSequenceDictionary,139,"I am using GATK 4.4.0.0 via the official docker release to reheader output from SVABA with an appropriate sequence dictionary. I am using `UpdateVCFSequenceDictionary` for this purpose with the following command: . ```; singularity exec -B ""$PWD"" broadinstitute-gatk-4.4.0.0.img gatk UpdateVCFSequenceDictionary --source-dictionary Mus_musculus.GRCm39.dna.primary_assembly.dict -V svaba.somatic.indel.vcf --replace true -O svaba.somatic.indel.vcf.reheaded.vcf; ```. I have encountered a curious behavior, where by the tool is not simply adjusting the sequence dictionary, but is also modifying a FORMAT field. . Original VCF header: . ```; ##FORMAT=<ID=GQ,Number=1,Type=String,Description=""Genotype quality (currently not supported. Always 0)"">; ```. Updated VCF header: . ```; ##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=""Genotype Quality"">; ```. From what I can see, the updated text is used frequently in your GATK VCF files, but I can't dig out the specific code where it is being set via `UpdateVCFSequenceDictionary`. I am wondering if there is a collision where `UpdateVCFSequenceDictionary` detects GQ and prints a stock header field to match expectation, rather than leaving it alone. I would expect the tool to simply replace the dictionary portion of the VCF without modifying the FORMAT/INFO fields. This is causing issues with downstream analysis because SVABA QC values are float/string not integer.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8629
https://github.com/broadinstitute/gatk/issues/8629:284,Deployability,Update,UpdateVCFSequenceDictionary,284,"I am using GATK 4.4.0.0 via the official docker release to reheader output from SVABA with an appropriate sequence dictionary. I am using `UpdateVCFSequenceDictionary` for this purpose with the following command: . ```; singularity exec -B ""$PWD"" broadinstitute-gatk-4.4.0.0.img gatk UpdateVCFSequenceDictionary --source-dictionary Mus_musculus.GRCm39.dna.primary_assembly.dict -V svaba.somatic.indel.vcf --replace true -O svaba.somatic.indel.vcf.reheaded.vcf; ```. I have encountered a curious behavior, where by the tool is not simply adjusting the sequence dictionary, but is also modifying a FORMAT field. . Original VCF header: . ```; ##FORMAT=<ID=GQ,Number=1,Type=String,Description=""Genotype quality (currently not supported. Always 0)"">; ```. Updated VCF header: . ```; ##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=""Genotype Quality"">; ```. From what I can see, the updated text is used frequently in your GATK VCF files, but I can't dig out the specific code where it is being set via `UpdateVCFSequenceDictionary`. I am wondering if there is a collision where `UpdateVCFSequenceDictionary` detects GQ and prints a stock header field to match expectation, rather than leaving it alone. I would expect the tool to simply replace the dictionary portion of the VCF without modifying the FORMAT/INFO fields. This is causing issues with downstream analysis because SVABA QC values are float/string not integer.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8629
https://github.com/broadinstitute/gatk/issues/8629:751,Deployability,Update,Updated,751,"I am using GATK 4.4.0.0 via the official docker release to reheader output from SVABA with an appropriate sequence dictionary. I am using `UpdateVCFSequenceDictionary` for this purpose with the following command: . ```; singularity exec -B ""$PWD"" broadinstitute-gatk-4.4.0.0.img gatk UpdateVCFSequenceDictionary --source-dictionary Mus_musculus.GRCm39.dna.primary_assembly.dict -V svaba.somatic.indel.vcf --replace true -O svaba.somatic.indel.vcf.reheaded.vcf; ```. I have encountered a curious behavior, where by the tool is not simply adjusting the sequence dictionary, but is also modifying a FORMAT field. . Original VCF header: . ```; ##FORMAT=<ID=GQ,Number=1,Type=String,Description=""Genotype quality (currently not supported. Always 0)"">; ```. Updated VCF header: . ```; ##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=""Genotype Quality"">; ```. From what I can see, the updated text is used frequently in your GATK VCF files, but I can't dig out the specific code where it is being set via `UpdateVCFSequenceDictionary`. I am wondering if there is a collision where `UpdateVCFSequenceDictionary` detects GQ and prints a stock header field to match expectation, rather than leaving it alone. I would expect the tool to simply replace the dictionary portion of the VCF without modifying the FORMAT/INFO fields. This is causing issues with downstream analysis because SVABA QC values are float/string not integer.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8629
https://github.com/broadinstitute/gatk/issues/8629:879,Deployability,update,updated,879,"I am using GATK 4.4.0.0 via the official docker release to reheader output from SVABA with an appropriate sequence dictionary. I am using `UpdateVCFSequenceDictionary` for this purpose with the following command: . ```; singularity exec -B ""$PWD"" broadinstitute-gatk-4.4.0.0.img gatk UpdateVCFSequenceDictionary --source-dictionary Mus_musculus.GRCm39.dna.primary_assembly.dict -V svaba.somatic.indel.vcf --replace true -O svaba.somatic.indel.vcf.reheaded.vcf; ```. I have encountered a curious behavior, where by the tool is not simply adjusting the sequence dictionary, but is also modifying a FORMAT field. . Original VCF header: . ```; ##FORMAT=<ID=GQ,Number=1,Type=String,Description=""Genotype quality (currently not supported. Always 0)"">; ```. Updated VCF header: . ```; ##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=""Genotype Quality"">; ```. From what I can see, the updated text is used frequently in your GATK VCF files, but I can't dig out the specific code where it is being set via `UpdateVCFSequenceDictionary`. I am wondering if there is a collision where `UpdateVCFSequenceDictionary` detects GQ and prints a stock header field to match expectation, rather than leaving it alone. I would expect the tool to simply replace the dictionary portion of the VCF without modifying the FORMAT/INFO fields. This is causing issues with downstream analysis because SVABA QC values are float/string not integer.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8629
https://github.com/broadinstitute/gatk/issues/8629:1000,Deployability,Update,UpdateVCFSequenceDictionary,1000,"I am using GATK 4.4.0.0 via the official docker release to reheader output from SVABA with an appropriate sequence dictionary. I am using `UpdateVCFSequenceDictionary` for this purpose with the following command: . ```; singularity exec -B ""$PWD"" broadinstitute-gatk-4.4.0.0.img gatk UpdateVCFSequenceDictionary --source-dictionary Mus_musculus.GRCm39.dna.primary_assembly.dict -V svaba.somatic.indel.vcf --replace true -O svaba.somatic.indel.vcf.reheaded.vcf; ```. I have encountered a curious behavior, where by the tool is not simply adjusting the sequence dictionary, but is also modifying a FORMAT field. . Original VCF header: . ```; ##FORMAT=<ID=GQ,Number=1,Type=String,Description=""Genotype quality (currently not supported. Always 0)"">; ```. Updated VCF header: . ```; ##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=""Genotype Quality"">; ```. From what I can see, the updated text is used frequently in your GATK VCF files, but I can't dig out the specific code where it is being set via `UpdateVCFSequenceDictionary`. I am wondering if there is a collision where `UpdateVCFSequenceDictionary` detects GQ and prints a stock header field to match expectation, rather than leaving it alone. I would expect the tool to simply replace the dictionary portion of the VCF without modifying the FORMAT/INFO fields. This is causing issues with downstream analysis because SVABA QC values are float/string not integer.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8629
https://github.com/broadinstitute/gatk/issues/8629:1076,Deployability,Update,UpdateVCFSequenceDictionary,1076,"I am using GATK 4.4.0.0 via the official docker release to reheader output from SVABA with an appropriate sequence dictionary. I am using `UpdateVCFSequenceDictionary` for this purpose with the following command: . ```; singularity exec -B ""$PWD"" broadinstitute-gatk-4.4.0.0.img gatk UpdateVCFSequenceDictionary --source-dictionary Mus_musculus.GRCm39.dna.primary_assembly.dict -V svaba.somatic.indel.vcf --replace true -O svaba.somatic.indel.vcf.reheaded.vcf; ```. I have encountered a curious behavior, where by the tool is not simply adjusting the sequence dictionary, but is also modifying a FORMAT field. . Original VCF header: . ```; ##FORMAT=<ID=GQ,Number=1,Type=String,Description=""Genotype quality (currently not supported. Always 0)"">; ```. Updated VCF header: . ```; ##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=""Genotype Quality"">; ```. From what I can see, the updated text is used frequently in your GATK VCF files, but I can't dig out the specific code where it is being set via `UpdateVCFSequenceDictionary`. I am wondering if there is a collision where `UpdateVCFSequenceDictionary` detects GQ and prints a stock header field to match expectation, rather than leaving it alone. I would expect the tool to simply replace the dictionary portion of the VCF without modifying the FORMAT/INFO fields. This is causing issues with downstream analysis because SVABA QC values are float/string not integer.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8629
https://github.com/broadinstitute/gatk/issues/8629:1105,Safety,detect,detects,1105,"I am using GATK 4.4.0.0 via the official docker release to reheader output from SVABA with an appropriate sequence dictionary. I am using `UpdateVCFSequenceDictionary` for this purpose with the following command: . ```; singularity exec -B ""$PWD"" broadinstitute-gatk-4.4.0.0.img gatk UpdateVCFSequenceDictionary --source-dictionary Mus_musculus.GRCm39.dna.primary_assembly.dict -V svaba.somatic.indel.vcf --replace true -O svaba.somatic.indel.vcf.reheaded.vcf; ```. I have encountered a curious behavior, where by the tool is not simply adjusting the sequence dictionary, but is also modifying a FORMAT field. . Original VCF header: . ```; ##FORMAT=<ID=GQ,Number=1,Type=String,Description=""Genotype quality (currently not supported. Always 0)"">; ```. Updated VCF header: . ```; ##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=""Genotype Quality"">; ```. From what I can see, the updated text is used frequently in your GATK VCF files, but I can't dig out the specific code where it is being set via `UpdateVCFSequenceDictionary`. I am wondering if there is a collision where `UpdateVCFSequenceDictionary` detects GQ and prints a stock header field to match expectation, rather than leaving it alone. I would expect the tool to simply replace the dictionary portion of the VCF without modifying the FORMAT/INFO fields. This is causing issues with downstream analysis because SVABA QC values are float/string not integer.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8629
https://github.com/broadinstitute/gatk/issues/8629:530,Usability,simpl,simply,530,"I am using GATK 4.4.0.0 via the official docker release to reheader output from SVABA with an appropriate sequence dictionary. I am using `UpdateVCFSequenceDictionary` for this purpose with the following command: . ```; singularity exec -B ""$PWD"" broadinstitute-gatk-4.4.0.0.img gatk UpdateVCFSequenceDictionary --source-dictionary Mus_musculus.GRCm39.dna.primary_assembly.dict -V svaba.somatic.indel.vcf --replace true -O svaba.somatic.indel.vcf.reheaded.vcf; ```. I have encountered a curious behavior, where by the tool is not simply adjusting the sequence dictionary, but is also modifying a FORMAT field. . Original VCF header: . ```; ##FORMAT=<ID=GQ,Number=1,Type=String,Description=""Genotype quality (currently not supported. Always 0)"">; ```. Updated VCF header: . ```; ##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=""Genotype Quality"">; ```. From what I can see, the updated text is used frequently in your GATK VCF files, but I can't dig out the specific code where it is being set via `UpdateVCFSequenceDictionary`. I am wondering if there is a collision where `UpdateVCFSequenceDictionary` detects GQ and prints a stock header field to match expectation, rather than leaving it alone. I would expect the tool to simply replace the dictionary portion of the VCF without modifying the FORMAT/INFO fields. This is causing issues with downstream analysis because SVABA QC values are float/string not integer.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8629
https://github.com/broadinstitute/gatk/issues/8629:1227,Usability,simpl,simply,1227,"I am using GATK 4.4.0.0 via the official docker release to reheader output from SVABA with an appropriate sequence dictionary. I am using `UpdateVCFSequenceDictionary` for this purpose with the following command: . ```; singularity exec -B ""$PWD"" broadinstitute-gatk-4.4.0.0.img gatk UpdateVCFSequenceDictionary --source-dictionary Mus_musculus.GRCm39.dna.primary_assembly.dict -V svaba.somatic.indel.vcf --replace true -O svaba.somatic.indel.vcf.reheaded.vcf; ```. I have encountered a curious behavior, where by the tool is not simply adjusting the sequence dictionary, but is also modifying a FORMAT field. . Original VCF header: . ```; ##FORMAT=<ID=GQ,Number=1,Type=String,Description=""Genotype quality (currently not supported. Always 0)"">; ```. Updated VCF header: . ```; ##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=""Genotype Quality"">; ```. From what I can see, the updated text is used frequently in your GATK VCF files, but I can't dig out the specific code where it is being set via `UpdateVCFSequenceDictionary`. I am wondering if there is a collision where `UpdateVCFSequenceDictionary` detects GQ and prints a stock header field to match expectation, rather than leaving it alone. I would expect the tool to simply replace the dictionary portion of the VCF without modifying the FORMAT/INFO fields. This is causing issues with downstream analysis because SVABA QC values are float/string not integer.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8629
https://github.com/broadinstitute/gatk/issues/8631:651,Performance,perform,perform,651,"a) GATK version used: 4.2.0.0; b) Exact command used:; ```; ""gatk --java-options '{java_opts}' HaplotypeCaller""; ""-L {interval_list} ""; ""-R {ref} ""; ""-I {bam} ""; ""-bamout {output_bam} ""; ""-O {vcf} {dbsnp} {log}""; ```; . Hi GATK community,. I am experiencing an issue while working with GATK, and I'd appreciate your assistance in resolving it. In my BED file, I have the following line:. `chrX 31182732 31182909 DMD . .`. There is a significant variant at the position chrX-31182732 T>G. However, when I run the command with the provided BED file, I cannot see the variant at the ""chrX 31182732"" position in the output VCF file. Interestingly, when I perform a test by expanding the BED file by one base on each side (changing it to ""`chrX 31182731 31182910 DMD . .`""), I can observe the variant at the ""chrX 31182732"" position in the VCF file. Is it expected behavior for GATK not to consider variants at the boundaries of interval files? Your insights and guidance on resolving this issue would be greatly appreciated. ![Screenshot from 2023-12-18 13-50-05](https://github.com/broadinstitute/gatk/assets/57506727/62386a08-2d9d-4a03-bcad-3c6b34f35357). Thank you in advance for your help. Best regards,; Meryem",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8631
https://github.com/broadinstitute/gatk/issues/8631:206,Testability,log,log,206,"a) GATK version used: 4.2.0.0; b) Exact command used:; ```; ""gatk --java-options '{java_opts}' HaplotypeCaller""; ""-L {interval_list} ""; ""-R {ref} ""; ""-I {bam} ""; ""-bamout {output_bam} ""; ""-O {vcf} {dbsnp} {log}""; ```; . Hi GATK community,. I am experiencing an issue while working with GATK, and I'd appreciate your assistance in resolving it. In my BED file, I have the following line:. `chrX 31182732 31182909 DMD . .`. There is a significant variant at the position chrX-31182732 T>G. However, when I run the command with the provided BED file, I cannot see the variant at the ""chrX 31182732"" position in the output VCF file. Interestingly, when I perform a test by expanding the BED file by one base on each side (changing it to ""`chrX 31182731 31182910 DMD . .`""), I can observe the variant at the ""chrX 31182732"" position in the VCF file. Is it expected behavior for GATK not to consider variants at the boundaries of interval files? Your insights and guidance on resolving this issue would be greatly appreciated. ![Screenshot from 2023-12-18 13-50-05](https://github.com/broadinstitute/gatk/assets/57506727/62386a08-2d9d-4a03-bcad-3c6b34f35357). Thank you in advance for your help. Best regards,; Meryem",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8631
https://github.com/broadinstitute/gatk/issues/8631:661,Testability,test,test,661,"a) GATK version used: 4.2.0.0; b) Exact command used:; ```; ""gatk --java-options '{java_opts}' HaplotypeCaller""; ""-L {interval_list} ""; ""-R {ref} ""; ""-I {bam} ""; ""-bamout {output_bam} ""; ""-O {vcf} {dbsnp} {log}""; ```; . Hi GATK community,. I am experiencing an issue while working with GATK, and I'd appreciate your assistance in resolving it. In my BED file, I have the following line:. `chrX 31182732 31182909 DMD . .`. There is a significant variant at the position chrX-31182732 T>G. However, when I run the command with the provided BED file, I cannot see the variant at the ""chrX 31182732"" position in the output VCF file. Interestingly, when I perform a test by expanding the BED file by one base on each side (changing it to ""`chrX 31182731 31182910 DMD . .`""), I can observe the variant at the ""chrX 31182732"" position in the VCF file. Is it expected behavior for GATK not to consider variants at the boundaries of interval files? Your insights and guidance on resolving this issue would be greatly appreciated. ![Screenshot from 2023-12-18 13-50-05](https://github.com/broadinstitute/gatk/assets/57506727/62386a08-2d9d-4a03-bcad-3c6b34f35357). Thank you in advance for your help. Best regards,; Meryem",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8631
https://github.com/broadinstitute/gatk/issues/8631:958,Usability,guid,guidance,958,"a) GATK version used: 4.2.0.0; b) Exact command used:; ```; ""gatk --java-options '{java_opts}' HaplotypeCaller""; ""-L {interval_list} ""; ""-R {ref} ""; ""-I {bam} ""; ""-bamout {output_bam} ""; ""-O {vcf} {dbsnp} {log}""; ```; . Hi GATK community,. I am experiencing an issue while working with GATK, and I'd appreciate your assistance in resolving it. In my BED file, I have the following line:. `chrX 31182732 31182909 DMD . .`. There is a significant variant at the position chrX-31182732 T>G. However, when I run the command with the provided BED file, I cannot see the variant at the ""chrX 31182732"" position in the output VCF file. Interestingly, when I perform a test by expanding the BED file by one base on each side (changing it to ""`chrX 31182731 31182910 DMD . .`""), I can observe the variant at the ""chrX 31182732"" position in the VCF file. Is it expected behavior for GATK not to consider variants at the boundaries of interval files? Your insights and guidance on resolving this issue would be greatly appreciated. ![Screenshot from 2023-12-18 13-50-05](https://github.com/broadinstitute/gatk/assets/57506727/62386a08-2d9d-4a03-bcad-3c6b34f35357). Thank you in advance for your help. Best regards,; Meryem",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8631
https://github.com/broadinstitute/gatk/issues/8632:655,Availability,error,error,655,"@lbergelson, just want to discuss some issues here-. 1. We currently have to use `--avoid-nio` with `--sample-name-map` and `--bypass-feature-reader` to get `GenomicsDBImport` to work with azure URIs. Why don't we just merge the `--avoid-nio` functionality with `--bypass-feauture-reader`, that is allow GenomicsDB to process the URIs by default?; 2. Noticed that the only way to use azure URIs for vcf names is by using `--sample-name-map`. Directly specifying vcfs with the `-V` option is not possible because `--avoid-nio` cannot be used in conjunction. Should this be supported?; 3. @lbergelson, w.r.t malformed Azure URIs, GenomicsDB does put out an error -; ```; 11:10:12.658 error NativeGenomicsDB - pid=30608 tid=2980282 htslib_plugin could not open file az://genomicsdb@oda/vcfs/t0.vcf.gz [TileDB::StorageManagerConfig] Error: Azure Storage Blob initialization failed for home=az://genomicsdb@container/vcfs/sample.vcf.gz; ; Azure Blob URI does not seem to have either an account or a container: Protocol error; [E::hts_open_format] Failed to open file ""az://genomicsdb@container/vcfs/sample.vcf.gz"" : Input/output error; ```; Is this not sufficient? These are the acceptable azure URIs currently; ```; az://<container_name>@<account_name>.blob/<folder>/<file> # for default endpoints; az://<container_name>@<account_name>.blob.core.windows.net/<folder>/<file> # if the endpoint is blob.core.windows.net; azb://<container_name>/<folder>/<file> # following java.nio for azure URIs; azb://<container_name>/<folder>/<file>?account=<account_name>&endpoint=<endpoint>; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8632
https://github.com/broadinstitute/gatk/issues/8632:682,Availability,error,error,682,"@lbergelson, just want to discuss some issues here-. 1. We currently have to use `--avoid-nio` with `--sample-name-map` and `--bypass-feature-reader` to get `GenomicsDBImport` to work with azure URIs. Why don't we just merge the `--avoid-nio` functionality with `--bypass-feauture-reader`, that is allow GenomicsDB to process the URIs by default?; 2. Noticed that the only way to use azure URIs for vcf names is by using `--sample-name-map`. Directly specifying vcfs with the `-V` option is not possible because `--avoid-nio` cannot be used in conjunction. Should this be supported?; 3. @lbergelson, w.r.t malformed Azure URIs, GenomicsDB does put out an error -; ```; 11:10:12.658 error NativeGenomicsDB - pid=30608 tid=2980282 htslib_plugin could not open file az://genomicsdb@oda/vcfs/t0.vcf.gz [TileDB::StorageManagerConfig] Error: Azure Storage Blob initialization failed for home=az://genomicsdb@container/vcfs/sample.vcf.gz; ; Azure Blob URI does not seem to have either an account or a container: Protocol error; [E::hts_open_format] Failed to open file ""az://genomicsdb@container/vcfs/sample.vcf.gz"" : Input/output error; ```; Is this not sufficient? These are the acceptable azure URIs currently; ```; az://<container_name>@<account_name>.blob/<folder>/<file> # for default endpoints; az://<container_name>@<account_name>.blob.core.windows.net/<folder>/<file> # if the endpoint is blob.core.windows.net; azb://<container_name>/<folder>/<file> # following java.nio for azure URIs; azb://<container_name>/<folder>/<file>?account=<account_name>&endpoint=<endpoint>; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8632
https://github.com/broadinstitute/gatk/issues/8632:829,Availability,Error,Error,829,"@lbergelson, just want to discuss some issues here-. 1. We currently have to use `--avoid-nio` with `--sample-name-map` and `--bypass-feature-reader` to get `GenomicsDBImport` to work with azure URIs. Why don't we just merge the `--avoid-nio` functionality with `--bypass-feauture-reader`, that is allow GenomicsDB to process the URIs by default?; 2. Noticed that the only way to use azure URIs for vcf names is by using `--sample-name-map`. Directly specifying vcfs with the `-V` option is not possible because `--avoid-nio` cannot be used in conjunction. Should this be supported?; 3. @lbergelson, w.r.t malformed Azure URIs, GenomicsDB does put out an error -; ```; 11:10:12.658 error NativeGenomicsDB - pid=30608 tid=2980282 htslib_plugin could not open file az://genomicsdb@oda/vcfs/t0.vcf.gz [TileDB::StorageManagerConfig] Error: Azure Storage Blob initialization failed for home=az://genomicsdb@container/vcfs/sample.vcf.gz; ; Azure Blob URI does not seem to have either an account or a container: Protocol error; [E::hts_open_format] Failed to open file ""az://genomicsdb@container/vcfs/sample.vcf.gz"" : Input/output error; ```; Is this not sufficient? These are the acceptable azure URIs currently; ```; az://<container_name>@<account_name>.blob/<folder>/<file> # for default endpoints; az://<container_name>@<account_name>.blob.core.windows.net/<folder>/<file> # if the endpoint is blob.core.windows.net; azb://<container_name>/<folder>/<file> # following java.nio for azure URIs; azb://<container_name>/<folder>/<file>?account=<account_name>&endpoint=<endpoint>; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8632
https://github.com/broadinstitute/gatk/issues/8632:1014,Availability,error,error,1014,"@lbergelson, just want to discuss some issues here-. 1. We currently have to use `--avoid-nio` with `--sample-name-map` and `--bypass-feature-reader` to get `GenomicsDBImport` to work with azure URIs. Why don't we just merge the `--avoid-nio` functionality with `--bypass-feauture-reader`, that is allow GenomicsDB to process the URIs by default?; 2. Noticed that the only way to use azure URIs for vcf names is by using `--sample-name-map`. Directly specifying vcfs with the `-V` option is not possible because `--avoid-nio` cannot be used in conjunction. Should this be supported?; 3. @lbergelson, w.r.t malformed Azure URIs, GenomicsDB does put out an error -; ```; 11:10:12.658 error NativeGenomicsDB - pid=30608 tid=2980282 htslib_plugin could not open file az://genomicsdb@oda/vcfs/t0.vcf.gz [TileDB::StorageManagerConfig] Error: Azure Storage Blob initialization failed for home=az://genomicsdb@container/vcfs/sample.vcf.gz; ; Azure Blob URI does not seem to have either an account or a container: Protocol error; [E::hts_open_format] Failed to open file ""az://genomicsdb@container/vcfs/sample.vcf.gz"" : Input/output error; ```; Is this not sufficient? These are the acceptable azure URIs currently; ```; az://<container_name>@<account_name>.blob/<folder>/<file> # for default endpoints; az://<container_name>@<account_name>.blob.core.windows.net/<folder>/<file> # if the endpoint is blob.core.windows.net; azb://<container_name>/<folder>/<file> # following java.nio for azure URIs; azb://<container_name>/<folder>/<file>?account=<account_name>&endpoint=<endpoint>; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8632
https://github.com/broadinstitute/gatk/issues/8632:1124,Availability,error,error,1124,"@lbergelson, just want to discuss some issues here-. 1. We currently have to use `--avoid-nio` with `--sample-name-map` and `--bypass-feature-reader` to get `GenomicsDBImport` to work with azure URIs. Why don't we just merge the `--avoid-nio` functionality with `--bypass-feauture-reader`, that is allow GenomicsDB to process the URIs by default?; 2. Noticed that the only way to use azure URIs for vcf names is by using `--sample-name-map`. Directly specifying vcfs with the `-V` option is not possible because `--avoid-nio` cannot be used in conjunction. Should this be supported?; 3. @lbergelson, w.r.t malformed Azure URIs, GenomicsDB does put out an error -; ```; 11:10:12.658 error NativeGenomicsDB - pid=30608 tid=2980282 htslib_plugin could not open file az://genomicsdb@oda/vcfs/t0.vcf.gz [TileDB::StorageManagerConfig] Error: Azure Storage Blob initialization failed for home=az://genomicsdb@container/vcfs/sample.vcf.gz; ; Azure Blob URI does not seem to have either an account or a container: Protocol error; [E::hts_open_format] Failed to open file ""az://genomicsdb@container/vcfs/sample.vcf.gz"" : Input/output error; ```; Is this not sufficient? These are the acceptable azure URIs currently; ```; az://<container_name>@<account_name>.blob/<folder>/<file> # for default endpoints; az://<container_name>@<account_name>.blob.core.windows.net/<folder>/<file> # if the endpoint is blob.core.windows.net; azb://<container_name>/<folder>/<file> # following java.nio for azure URIs; azb://<container_name>/<folder>/<file>?account=<account_name>&endpoint=<endpoint>; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8632
https://github.com/broadinstitute/gatk/issues/8632:1005,Integrability,Protocol,Protocol,1005,"@lbergelson, just want to discuss some issues here-. 1. We currently have to use `--avoid-nio` with `--sample-name-map` and `--bypass-feature-reader` to get `GenomicsDBImport` to work with azure URIs. Why don't we just merge the `--avoid-nio` functionality with `--bypass-feauture-reader`, that is allow GenomicsDB to process the URIs by default?; 2. Noticed that the only way to use azure URIs for vcf names is by using `--sample-name-map`. Directly specifying vcfs with the `-V` option is not possible because `--avoid-nio` cannot be used in conjunction. Should this be supported?; 3. @lbergelson, w.r.t malformed Azure URIs, GenomicsDB does put out an error -; ```; 11:10:12.658 error NativeGenomicsDB - pid=30608 tid=2980282 htslib_plugin could not open file az://genomicsdb@oda/vcfs/t0.vcf.gz [TileDB::StorageManagerConfig] Error: Azure Storage Blob initialization failed for home=az://genomicsdb@container/vcfs/sample.vcf.gz; ; Azure Blob URI does not seem to have either an account or a container: Protocol error; [E::hts_open_format] Failed to open file ""az://genomicsdb@container/vcfs/sample.vcf.gz"" : Input/output error; ```; Is this not sufficient? These are the acceptable azure URIs currently; ```; az://<container_name>@<account_name>.blob/<folder>/<file> # for default endpoints; az://<container_name>@<account_name>.blob.core.windows.net/<folder>/<file> # if the endpoint is blob.core.windows.net; azb://<container_name>/<folder>/<file> # following java.nio for azure URIs; azb://<container_name>/<folder>/<file>?account=<account_name>&endpoint=<endpoint>; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8632
https://github.com/broadinstitute/gatk/issues/8632:84,Safety,avoid,avoid-nio,84,"@lbergelson, just want to discuss some issues here-. 1. We currently have to use `--avoid-nio` with `--sample-name-map` and `--bypass-feature-reader` to get `GenomicsDBImport` to work with azure URIs. Why don't we just merge the `--avoid-nio` functionality with `--bypass-feauture-reader`, that is allow GenomicsDB to process the URIs by default?; 2. Noticed that the only way to use azure URIs for vcf names is by using `--sample-name-map`. Directly specifying vcfs with the `-V` option is not possible because `--avoid-nio` cannot be used in conjunction. Should this be supported?; 3. @lbergelson, w.r.t malformed Azure URIs, GenomicsDB does put out an error -; ```; 11:10:12.658 error NativeGenomicsDB - pid=30608 tid=2980282 htslib_plugin could not open file az://genomicsdb@oda/vcfs/t0.vcf.gz [TileDB::StorageManagerConfig] Error: Azure Storage Blob initialization failed for home=az://genomicsdb@container/vcfs/sample.vcf.gz; ; Azure Blob URI does not seem to have either an account or a container: Protocol error; [E::hts_open_format] Failed to open file ""az://genomicsdb@container/vcfs/sample.vcf.gz"" : Input/output error; ```; Is this not sufficient? These are the acceptable azure URIs currently; ```; az://<container_name>@<account_name>.blob/<folder>/<file> # for default endpoints; az://<container_name>@<account_name>.blob.core.windows.net/<folder>/<file> # if the endpoint is blob.core.windows.net; azb://<container_name>/<folder>/<file> # following java.nio for azure URIs; azb://<container_name>/<folder>/<file>?account=<account_name>&endpoint=<endpoint>; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8632
https://github.com/broadinstitute/gatk/issues/8632:232,Safety,avoid,avoid-nio,232,"@lbergelson, just want to discuss some issues here-. 1. We currently have to use `--avoid-nio` with `--sample-name-map` and `--bypass-feature-reader` to get `GenomicsDBImport` to work with azure URIs. Why don't we just merge the `--avoid-nio` functionality with `--bypass-feauture-reader`, that is allow GenomicsDB to process the URIs by default?; 2. Noticed that the only way to use azure URIs for vcf names is by using `--sample-name-map`. Directly specifying vcfs with the `-V` option is not possible because `--avoid-nio` cannot be used in conjunction. Should this be supported?; 3. @lbergelson, w.r.t malformed Azure URIs, GenomicsDB does put out an error -; ```; 11:10:12.658 error NativeGenomicsDB - pid=30608 tid=2980282 htslib_plugin could not open file az://genomicsdb@oda/vcfs/t0.vcf.gz [TileDB::StorageManagerConfig] Error: Azure Storage Blob initialization failed for home=az://genomicsdb@container/vcfs/sample.vcf.gz; ; Azure Blob URI does not seem to have either an account or a container: Protocol error; [E::hts_open_format] Failed to open file ""az://genomicsdb@container/vcfs/sample.vcf.gz"" : Input/output error; ```; Is this not sufficient? These are the acceptable azure URIs currently; ```; az://<container_name>@<account_name>.blob/<folder>/<file> # for default endpoints; az://<container_name>@<account_name>.blob.core.windows.net/<folder>/<file> # if the endpoint is blob.core.windows.net; azb://<container_name>/<folder>/<file> # following java.nio for azure URIs; azb://<container_name>/<folder>/<file>?account=<account_name>&endpoint=<endpoint>; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8632
https://github.com/broadinstitute/gatk/issues/8632:515,Safety,avoid,avoid-nio,515,"@lbergelson, just want to discuss some issues here-. 1. We currently have to use `--avoid-nio` with `--sample-name-map` and `--bypass-feature-reader` to get `GenomicsDBImport` to work with azure URIs. Why don't we just merge the `--avoid-nio` functionality with `--bypass-feauture-reader`, that is allow GenomicsDB to process the URIs by default?; 2. Noticed that the only way to use azure URIs for vcf names is by using `--sample-name-map`. Directly specifying vcfs with the `-V` option is not possible because `--avoid-nio` cannot be used in conjunction. Should this be supported?; 3. @lbergelson, w.r.t malformed Azure URIs, GenomicsDB does put out an error -; ```; 11:10:12.658 error NativeGenomicsDB - pid=30608 tid=2980282 htslib_plugin could not open file az://genomicsdb@oda/vcfs/t0.vcf.gz [TileDB::StorageManagerConfig] Error: Azure Storage Blob initialization failed for home=az://genomicsdb@container/vcfs/sample.vcf.gz; ; Azure Blob URI does not seem to have either an account or a container: Protocol error; [E::hts_open_format] Failed to open file ""az://genomicsdb@container/vcfs/sample.vcf.gz"" : Input/output error; ```; Is this not sufficient? These are the acceptable azure URIs currently; ```; az://<container_name>@<account_name>.blob/<folder>/<file> # for default endpoints; az://<container_name>@<account_name>.blob.core.windows.net/<folder>/<file> # if the endpoint is blob.core.windows.net; azb://<container_name>/<folder>/<file> # following java.nio for azure URIs; azb://<container_name>/<folder>/<file>?account=<account_name>&endpoint=<endpoint>; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8632
https://github.com/broadinstitute/gatk/pull/8633:182,Security,secur,secure-,182,Example Run (AoU) [here](https://app.terra.bio/#workspaces/allofus-drc-wgs-dev/GVS%20AoU%20Echo%20RD/job_history/f4e85f7d-d8bf-45e1-a564-0dd2ae9f3761); New output file here: gs://fc-secure-4c3976f3-d84d-4243-876f-baa9f9a4256f/submissions/f4e85f7d-d8bf-45e1-a564-0dd2ae9f3761/GvsCalculatePrecisionAndSensitivity/85938f82-4731-437e-96a1-140a8ba7fcb7/call-CollateReports/stdout,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8633
https://github.com/broadinstitute/gatk/issues/8635:289,Performance,perform,perform,289,"Hi all,. I am using `gatk VariantFiltration` to filter my GBS data. I need to set a threshold for the missing rate, such as 0.7. I know I can use `vcftools -max-missing 0.7`, and then I can further use other parameters to filter: `--maf; --max-maf; --min-alleles; --max-alleles`. I try to perform the same code via gatk code, and I went through the official doc on the gatk website, which does not list all information, or it does, and I have a hard time understanding the arguments. I found some potential parameters, but I don't really know which one is the correct parameter.; --max-filtered-genotypes: Maximum number of samples filtered at the genotype level;; --max-fraction-filtered-genotypes: Maximum fraction of samples filtered at the genotype level;; --max-nocall-fraction: Maximum fraction of samples with no-call genotypes;. Thanks!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8635
https://github.com/broadinstitute/gatk/issues/8637:227,Testability,test,test,227,I have imported into GenomicDB and am using 3200 intervals to paralelise across hg38 but most intervals don't finish within four hours. It worked decently for ten to fifteen samples but not now that I have 108 samples. Can you test it out on such data to reproduce?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8637
https://github.com/broadinstitute/gatk/issues/8638:548,Availability,error,error,548,"## Bug Report. ### Affected tool(s) or class(es); In the tutorial ""[(How to part I) Sensitively detect copy ratio alterations and allelic segments](https://gatk.broadinstitute.org/hc/en-us/articles/360035531092)"", users are asked to install R components using [install_R_packages.R](https://github.com/broadinstitute/gatk/blob/4.0.1.1/scripts/docker/gatkbase/install_R_packages.R). . ### Affected version(s); Latest public release version [4.5.0.0]. ### Description ; Running the script with `Rscript install_R_packages.R` results in the following error:. `Error in download.file(p, destfile, method, mode = ""wb"", ...) : ; cannot open URL 'http://cran.r-project.org/src/contrib/HMM_1.0.tar.gz'; In addition: Warning message:; In download.file(p, destfile, method, mode = ""wb"", ...) :; cannot open URL 'http://cran.r-project.org/src/contrib/HMM_1.0.tar.gz': HTTP status was '404 Not Found'`. This can be fixed by changing line [35 of install_R_packages.R](https://github.com/broadinstitute/gatk/blob/4.0.1.1/scripts/docker/gatkbase/install_R_packages.R#L35) from `hmmUrl = ""http://cran.r-project.org/src/contrib/HMM_1.0.tar.gz""` to `hmmUrl = ""http://cran.r-project.org/src/contrib/HMM_1.0.1.tar.gz""`. . The script runs as expected once this change is made. #### Steps to reproduce; Run `Rscript install_R_packages.R`. #### Expected behavior; Successfully installs all necessary R packages with the correct versions. #### Actual behavior; Fails to install the 'HMM' package.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8638
https://github.com/broadinstitute/gatk/issues/8638:557,Availability,Error,Error,557,"## Bug Report. ### Affected tool(s) or class(es); In the tutorial ""[(How to part I) Sensitively detect copy ratio alterations and allelic segments](https://gatk.broadinstitute.org/hc/en-us/articles/360035531092)"", users are asked to install R components using [install_R_packages.R](https://github.com/broadinstitute/gatk/blob/4.0.1.1/scripts/docker/gatkbase/install_R_packages.R). . ### Affected version(s); Latest public release version [4.5.0.0]. ### Description ; Running the script with `Rscript install_R_packages.R` results in the following error:. `Error in download.file(p, destfile, method, mode = ""wb"", ...) : ; cannot open URL 'http://cran.r-project.org/src/contrib/HMM_1.0.tar.gz'; In addition: Warning message:; In download.file(p, destfile, method, mode = ""wb"", ...) :; cannot open URL 'http://cran.r-project.org/src/contrib/HMM_1.0.tar.gz': HTTP status was '404 Not Found'`. This can be fixed by changing line [35 of install_R_packages.R](https://github.com/broadinstitute/gatk/blob/4.0.1.1/scripts/docker/gatkbase/install_R_packages.R#L35) from `hmmUrl = ""http://cran.r-project.org/src/contrib/HMM_1.0.tar.gz""` to `hmmUrl = ""http://cran.r-project.org/src/contrib/HMM_1.0.1.tar.gz""`. . The script runs as expected once this change is made. #### Steps to reproduce; Run `Rscript install_R_packages.R`. #### Expected behavior; Successfully installs all necessary R packages with the correct versions. #### Actual behavior; Fails to install the 'HMM' package.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8638
https://github.com/broadinstitute/gatk/issues/8638:566,Availability,down,download,566,"## Bug Report. ### Affected tool(s) or class(es); In the tutorial ""[(How to part I) Sensitively detect copy ratio alterations and allelic segments](https://gatk.broadinstitute.org/hc/en-us/articles/360035531092)"", users are asked to install R components using [install_R_packages.R](https://github.com/broadinstitute/gatk/blob/4.0.1.1/scripts/docker/gatkbase/install_R_packages.R). . ### Affected version(s); Latest public release version [4.5.0.0]. ### Description ; Running the script with `Rscript install_R_packages.R` results in the following error:. `Error in download.file(p, destfile, method, mode = ""wb"", ...) : ; cannot open URL 'http://cran.r-project.org/src/contrib/HMM_1.0.tar.gz'; In addition: Warning message:; In download.file(p, destfile, method, mode = ""wb"", ...) :; cannot open URL 'http://cran.r-project.org/src/contrib/HMM_1.0.tar.gz': HTTP status was '404 Not Found'`. This can be fixed by changing line [35 of install_R_packages.R](https://github.com/broadinstitute/gatk/blob/4.0.1.1/scripts/docker/gatkbase/install_R_packages.R#L35) from `hmmUrl = ""http://cran.r-project.org/src/contrib/HMM_1.0.tar.gz""` to `hmmUrl = ""http://cran.r-project.org/src/contrib/HMM_1.0.1.tar.gz""`. . The script runs as expected once this change is made. #### Steps to reproduce; Run `Rscript install_R_packages.R`. #### Expected behavior; Successfully installs all necessary R packages with the correct versions. #### Actual behavior; Fails to install the 'HMM' package.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8638
https://github.com/broadinstitute/gatk/issues/8638:729,Availability,down,download,729,"## Bug Report. ### Affected tool(s) or class(es); In the tutorial ""[(How to part I) Sensitively detect copy ratio alterations and allelic segments](https://gatk.broadinstitute.org/hc/en-us/articles/360035531092)"", users are asked to install R components using [install_R_packages.R](https://github.com/broadinstitute/gatk/blob/4.0.1.1/scripts/docker/gatkbase/install_R_packages.R). . ### Affected version(s); Latest public release version [4.5.0.0]. ### Description ; Running the script with `Rscript install_R_packages.R` results in the following error:. `Error in download.file(p, destfile, method, mode = ""wb"", ...) : ; cannot open URL 'http://cran.r-project.org/src/contrib/HMM_1.0.tar.gz'; In addition: Warning message:; In download.file(p, destfile, method, mode = ""wb"", ...) :; cannot open URL 'http://cran.r-project.org/src/contrib/HMM_1.0.tar.gz': HTTP status was '404 Not Found'`. This can be fixed by changing line [35 of install_R_packages.R](https://github.com/broadinstitute/gatk/blob/4.0.1.1/scripts/docker/gatkbase/install_R_packages.R#L35) from `hmmUrl = ""http://cran.r-project.org/src/contrib/HMM_1.0.tar.gz""` to `hmmUrl = ""http://cran.r-project.org/src/contrib/HMM_1.0.1.tar.gz""`. . The script runs as expected once this change is made. #### Steps to reproduce; Run `Rscript install_R_packages.R`. #### Expected behavior; Successfully installs all necessary R packages with the correct versions. #### Actual behavior; Fails to install the 'HMM' package.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8638
https://github.com/broadinstitute/gatk/issues/8638:233,Deployability,install,install,233,"## Bug Report. ### Affected tool(s) or class(es); In the tutorial ""[(How to part I) Sensitively detect copy ratio alterations and allelic segments](https://gatk.broadinstitute.org/hc/en-us/articles/360035531092)"", users are asked to install R components using [install_R_packages.R](https://github.com/broadinstitute/gatk/blob/4.0.1.1/scripts/docker/gatkbase/install_R_packages.R). . ### Affected version(s); Latest public release version [4.5.0.0]. ### Description ; Running the script with `Rscript install_R_packages.R` results in the following error:. `Error in download.file(p, destfile, method, mode = ""wb"", ...) : ; cannot open URL 'http://cran.r-project.org/src/contrib/HMM_1.0.tar.gz'; In addition: Warning message:; In download.file(p, destfile, method, mode = ""wb"", ...) :; cannot open URL 'http://cran.r-project.org/src/contrib/HMM_1.0.tar.gz': HTTP status was '404 Not Found'`. This can be fixed by changing line [35 of install_R_packages.R](https://github.com/broadinstitute/gatk/blob/4.0.1.1/scripts/docker/gatkbase/install_R_packages.R#L35) from `hmmUrl = ""http://cran.r-project.org/src/contrib/HMM_1.0.tar.gz""` to `hmmUrl = ""http://cran.r-project.org/src/contrib/HMM_1.0.1.tar.gz""`. . The script runs as expected once this change is made. #### Steps to reproduce; Run `Rscript install_R_packages.R`. #### Expected behavior; Successfully installs all necessary R packages with the correct versions. #### Actual behavior; Fails to install the 'HMM' package.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8638
https://github.com/broadinstitute/gatk/issues/8638:423,Deployability,release,release,423,"## Bug Report. ### Affected tool(s) or class(es); In the tutorial ""[(How to part I) Sensitively detect copy ratio alterations and allelic segments](https://gatk.broadinstitute.org/hc/en-us/articles/360035531092)"", users are asked to install R components using [install_R_packages.R](https://github.com/broadinstitute/gatk/blob/4.0.1.1/scripts/docker/gatkbase/install_R_packages.R). . ### Affected version(s); Latest public release version [4.5.0.0]. ### Description ; Running the script with `Rscript install_R_packages.R` results in the following error:. `Error in download.file(p, destfile, method, mode = ""wb"", ...) : ; cannot open URL 'http://cran.r-project.org/src/contrib/HMM_1.0.tar.gz'; In addition: Warning message:; In download.file(p, destfile, method, mode = ""wb"", ...) :; cannot open URL 'http://cran.r-project.org/src/contrib/HMM_1.0.tar.gz': HTTP status was '404 Not Found'`. This can be fixed by changing line [35 of install_R_packages.R](https://github.com/broadinstitute/gatk/blob/4.0.1.1/scripts/docker/gatkbase/install_R_packages.R#L35) from `hmmUrl = ""http://cran.r-project.org/src/contrib/HMM_1.0.tar.gz""` to `hmmUrl = ""http://cran.r-project.org/src/contrib/HMM_1.0.1.tar.gz""`. . The script runs as expected once this change is made. #### Steps to reproduce; Run `Rscript install_R_packages.R`. #### Expected behavior; Successfully installs all necessary R packages with the correct versions. #### Actual behavior; Fails to install the 'HMM' package.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8638
https://github.com/broadinstitute/gatk/issues/8638:1354,Deployability,install,installs,1354,"## Bug Report. ### Affected tool(s) or class(es); In the tutorial ""[(How to part I) Sensitively detect copy ratio alterations and allelic segments](https://gatk.broadinstitute.org/hc/en-us/articles/360035531092)"", users are asked to install R components using [install_R_packages.R](https://github.com/broadinstitute/gatk/blob/4.0.1.1/scripts/docker/gatkbase/install_R_packages.R). . ### Affected version(s); Latest public release version [4.5.0.0]. ### Description ; Running the script with `Rscript install_R_packages.R` results in the following error:. `Error in download.file(p, destfile, method, mode = ""wb"", ...) : ; cannot open URL 'http://cran.r-project.org/src/contrib/HMM_1.0.tar.gz'; In addition: Warning message:; In download.file(p, destfile, method, mode = ""wb"", ...) :; cannot open URL 'http://cran.r-project.org/src/contrib/HMM_1.0.tar.gz': HTTP status was '404 Not Found'`. This can be fixed by changing line [35 of install_R_packages.R](https://github.com/broadinstitute/gatk/blob/4.0.1.1/scripts/docker/gatkbase/install_R_packages.R#L35) from `hmmUrl = ""http://cran.r-project.org/src/contrib/HMM_1.0.tar.gz""` to `hmmUrl = ""http://cran.r-project.org/src/contrib/HMM_1.0.1.tar.gz""`. . The script runs as expected once this change is made. #### Steps to reproduce; Run `Rscript install_R_packages.R`. #### Expected behavior; Successfully installs all necessary R packages with the correct versions. #### Actual behavior; Fails to install the 'HMM' package.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8638
https://github.com/broadinstitute/gatk/issues/8638:1446,Deployability,install,install,1446,"## Bug Report. ### Affected tool(s) or class(es); In the tutorial ""[(How to part I) Sensitively detect copy ratio alterations and allelic segments](https://gatk.broadinstitute.org/hc/en-us/articles/360035531092)"", users are asked to install R components using [install_R_packages.R](https://github.com/broadinstitute/gatk/blob/4.0.1.1/scripts/docker/gatkbase/install_R_packages.R). . ### Affected version(s); Latest public release version [4.5.0.0]. ### Description ; Running the script with `Rscript install_R_packages.R` results in the following error:. `Error in download.file(p, destfile, method, mode = ""wb"", ...) : ; cannot open URL 'http://cran.r-project.org/src/contrib/HMM_1.0.tar.gz'; In addition: Warning message:; In download.file(p, destfile, method, mode = ""wb"", ...) :; cannot open URL 'http://cran.r-project.org/src/contrib/HMM_1.0.tar.gz': HTTP status was '404 Not Found'`. This can be fixed by changing line [35 of install_R_packages.R](https://github.com/broadinstitute/gatk/blob/4.0.1.1/scripts/docker/gatkbase/install_R_packages.R#L35) from `hmmUrl = ""http://cran.r-project.org/src/contrib/HMM_1.0.tar.gz""` to `hmmUrl = ""http://cran.r-project.org/src/contrib/HMM_1.0.1.tar.gz""`. . The script runs as expected once this change is made. #### Steps to reproduce; Run `Rscript install_R_packages.R`. #### Expected behavior; Successfully installs all necessary R packages with the correct versions. #### Actual behavior; Fails to install the 'HMM' package.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8638
https://github.com/broadinstitute/gatk/issues/8638:716,Integrability,message,message,716,"## Bug Report. ### Affected tool(s) or class(es); In the tutorial ""[(How to part I) Sensitively detect copy ratio alterations and allelic segments](https://gatk.broadinstitute.org/hc/en-us/articles/360035531092)"", users are asked to install R components using [install_R_packages.R](https://github.com/broadinstitute/gatk/blob/4.0.1.1/scripts/docker/gatkbase/install_R_packages.R). . ### Affected version(s); Latest public release version [4.5.0.0]. ### Description ; Running the script with `Rscript install_R_packages.R` results in the following error:. `Error in download.file(p, destfile, method, mode = ""wb"", ...) : ; cannot open URL 'http://cran.r-project.org/src/contrib/HMM_1.0.tar.gz'; In addition: Warning message:; In download.file(p, destfile, method, mode = ""wb"", ...) :; cannot open URL 'http://cran.r-project.org/src/contrib/HMM_1.0.tar.gz': HTTP status was '404 Not Found'`. This can be fixed by changing line [35 of install_R_packages.R](https://github.com/broadinstitute/gatk/blob/4.0.1.1/scripts/docker/gatkbase/install_R_packages.R#L35) from `hmmUrl = ""http://cran.r-project.org/src/contrib/HMM_1.0.tar.gz""` to `hmmUrl = ""http://cran.r-project.org/src/contrib/HMM_1.0.1.tar.gz""`. . The script runs as expected once this change is made. #### Steps to reproduce; Run `Rscript install_R_packages.R`. #### Expected behavior; Successfully installs all necessary R packages with the correct versions. #### Actual behavior; Fails to install the 'HMM' package.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8638
https://github.com/broadinstitute/gatk/issues/8638:96,Safety,detect,detect,96,"## Bug Report. ### Affected tool(s) or class(es); In the tutorial ""[(How to part I) Sensitively detect copy ratio alterations and allelic segments](https://gatk.broadinstitute.org/hc/en-us/articles/360035531092)"", users are asked to install R components using [install_R_packages.R](https://github.com/broadinstitute/gatk/blob/4.0.1.1/scripts/docker/gatkbase/install_R_packages.R). . ### Affected version(s); Latest public release version [4.5.0.0]. ### Description ; Running the script with `Rscript install_R_packages.R` results in the following error:. `Error in download.file(p, destfile, method, mode = ""wb"", ...) : ; cannot open URL 'http://cran.r-project.org/src/contrib/HMM_1.0.tar.gz'; In addition: Warning message:; In download.file(p, destfile, method, mode = ""wb"", ...) :; cannot open URL 'http://cran.r-project.org/src/contrib/HMM_1.0.tar.gz': HTTP status was '404 Not Found'`. This can be fixed by changing line [35 of install_R_packages.R](https://github.com/broadinstitute/gatk/blob/4.0.1.1/scripts/docker/gatkbase/install_R_packages.R#L35) from `hmmUrl = ""http://cran.r-project.org/src/contrib/HMM_1.0.tar.gz""` to `hmmUrl = ""http://cran.r-project.org/src/contrib/HMM_1.0.1.tar.gz""`. . The script runs as expected once this change is made. #### Steps to reproduce; Run `Rscript install_R_packages.R`. #### Expected behavior; Successfully installs all necessary R packages with the correct versions. #### Actual behavior; Fails to install the 'HMM' package.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8638
https://github.com/broadinstitute/gatk/pull/8640:0,Testability,Test,Test,0,Test run here: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/81030f0d-af0e-4f75-8350-0dfeabb5e4a3,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8640
https://github.com/broadinstitute/gatk/issues/8643:496,Safety,avoid,avoid,496,"## Bug Report. ### Affected tool(s) or class(es); gatk HaplotypeCaller _--do-not-run-physical-phasing false_. ### Affected version(s); gatk-4.4.0.0. ### Description ; I used **gatk HaplotypeCaller** with parameter _--do-not-run-physical-phasing false_, some missing genotypes were phased to "".|."". What I was expecting was this ""./. "", and it will turn to haploid ""."" after vcftools(0.1.17) _recode_ processing.; So, I have to use gatk HaplotypeCaller _--do-not-run-physical-phasing **true**_ to avoid phasing processing.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8643
https://github.com/broadinstitute/gatk/pull/8644:225,Deployability,Integrat,Integration,225,* Successful run in PMI land that finds nothing to clean up [here](https://app.terra.bio/#workspaces/allofus-drc-wgs-dev/GVS%20AoU%20Echo%20First%20Look/job_history/ba861882-96ee-4635-b522-2fe9489b0076).; * Successful run in Integration land that finds loads to clean up [here](https://job-manager.dsde-prod.broadinstitute.org/jobs/01667ae7-fd85-4a12-abcb-69e892500fa3).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8644
https://github.com/broadinstitute/gatk/pull/8644:225,Integrability,Integrat,Integration,225,* Successful run in PMI land that finds nothing to clean up [here](https://app.terra.bio/#workspaces/allofus-drc-wgs-dev/GVS%20AoU%20Echo%20First%20Look/job_history/ba861882-96ee-4635-b522-2fe9489b0076).; * Successful run in Integration land that finds loads to clean up [here](https://job-manager.dsde-prod.broadinstitute.org/jobs/01667ae7-fd85-4a12-abcb-69e892500fa3).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8644
https://github.com/broadinstitute/gatk/pull/8644:253,Performance,load,loads,253,* Successful run in PMI land that finds nothing to clean up [here](https://app.terra.bio/#workspaces/allofus-drc-wgs-dev/GVS%20AoU%20Echo%20First%20Look/job_history/ba861882-96ee-4635-b522-2fe9489b0076).; * Successful run in Integration land that finds loads to clean up [here](https://job-manager.dsde-prod.broadinstitute.org/jobs/01667ae7-fd85-4a12-abcb-69e892500fa3).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8644
https://github.com/broadinstitute/gatk/issues/8647:808,Availability,error,error,808,"Hi,. When I try to run Funcotator, using the below command:. gatk Funcotator \; --variant /rsrch5/home/tdccct/ppshah/shared/CAS_MOSAIC/mutect/mrn_2507919/WES/KShaw-ROPR0004-DNA-229761-WX01-T_HMCKJDSX2-4-ATTGGCTC/KShaw-ROPR0004-DNA-229761-WX01-T_HMCKJDSX2-4-ATTGGCTC_filtered.vcf.gz \; --reference /rsrch5/home/tdccct/ppshah/shared/gencode/Homo_sapiens/GATK/GRCh38/Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta \; --ref-version hg38 \; --data-sources-path /rsrch5/home/tdccct/ppshah/shared/pipelines/mutect/funcotator_dataSources.v1.7.20200521s \; --output /rsrch5/home/tdccct/ppshah/shared/CAS_MOSAIC/mutect/mrn_2507919/WES/KShaw-ROPR0004-DNA-229761-WX01-T_HMCKJDSX2-4-ATTGGCTC/KShaw-ROPR0004-DNA-229761-WX01-T_HMCKJDSX2-4-ATTGGCTC_funcotated.vcf \; --output-file-format VCF; ; I get the following error:; ; Using GATK jar /gatk/gatk-package-4.4.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.4.0.0-local.jar Funcotator --variant /home/ppshah/shared/CAS_MOSAIC/mutect/mrn_2507919/WES/KShaw-ROPR0004-DNA-229761-WX01-T_HMCKJDSX2-4-ATTGGCTC/KShaw-ROPR0004-DNA-229761-WX01-T_HMCKJDSX2-4-ATTGGCTC_filtered.vcf.gz --reference /home/ppshah/shared/gencode/Homo_sapiens/GATK/GRCh38/Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta --ref-version hg38 --data-sources-path /home/ppshah/shared/pipelines/mutect/funcotator_dataSources.v1.7.20200521s --output /home/ppshah/shared/CAS_MOSAIC/mutect/mrn_2507919/WES/KShaw-ROPR0004-DNA-229761-WX01-T_HMCKJDSX2-4-ATTGGCTC/KShaw-ROPR0004-DNA-229761-WX01-T_HMCKJDSX2-4-ATTGGCTC_funcotated.vcf --output-file-format VCF; 16:36:22.352 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.4.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 16:36:22.392 INFO Funcotator - ------------------------------------------------------------; 16:36:22.396 ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8647
https://github.com/broadinstitute/gatk/issues/8647:4547,Availability,down,down,4547,"nflater; 16:36:22.399 INFO Funcotator - GCS max retries/reopens: 20; 16:36:22.399 INFO Funcotator - Requester pays: disabled; 16:36:22.399 INFO Funcotator - Initializing engine; 16:36:22.624 INFO FeatureManager - Using codec VCFCodec to read file file:///home/ppshah/shared/CAS_MOSAIC/mutect/mrn_2507919/WES/KShaw-ROPR0004-DNA-229761-WX01-T_HMCKJDSX2-4-ATTGGCTC/KShaw-ROPR0004-DNA-229761-WX01-T_HMCKJDSX2-4-ATTGGCTC_filtered.vcf.gz; 16:36:22.842 INFO Funcotator - Done initializing engine; 16:36:22.842 INFO Funcotator - Validating sequence dictionaries...; 16:36:22.856 INFO Funcotator - Processing user transcripts/defaults/overrides...; 16:36:22.857 INFO Funcotator - Initializing data sources...; 16:36:22.859 INFO DataSourceUtils - Initializing data sources from directory: /home/ppshah/shared/pipelines/mutect/funcotator_dataSources.v1.7.20200521s; 16:36:22.871 INFO DataSourceUtils - Data sources version: 1.7.2020429s; 16:36:22.871 INFO DataSourceUtils - Data sources source: ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/funcotator/funcotator_dataSources.v1.7.20200429s.tar.gz; 16:36:22.871 INFO DataSourceUtils - Data sources alternate source: gs://broad-public-datasets/funcotator/funcotator_dataSources.v1.7.20200429s.tar.gz; 16:36:22.891 INFO Funcotator - Shutting down engine; [January 10, 2024 at 4:36:22 PM GMT] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=285212672; ***********************************************************************. A USER ERROR has occurred: ERROR: Directory contains more than one config file: file:///home/ppshah/shared/pipelines/mutect/funcotator_dataSources.v1.7.20200521s/gencode_xrefseq/hg38/. ***********************************************************************; Set the system property GATK_STACKTRACE_ON_USER_EXCEPTION (--java-options '-DGATK_STACKTRACE_ON_USER_EXCEPTION=true') to print the stack trace. Any guidance to resolve the issue is appreciated.; Thank you!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8647
https://github.com/broadinstitute/gatk/issues/8647:4803,Availability,ERROR,ERROR,4803,"nflater; 16:36:22.399 INFO Funcotator - GCS max retries/reopens: 20; 16:36:22.399 INFO Funcotator - Requester pays: disabled; 16:36:22.399 INFO Funcotator - Initializing engine; 16:36:22.624 INFO FeatureManager - Using codec VCFCodec to read file file:///home/ppshah/shared/CAS_MOSAIC/mutect/mrn_2507919/WES/KShaw-ROPR0004-DNA-229761-WX01-T_HMCKJDSX2-4-ATTGGCTC/KShaw-ROPR0004-DNA-229761-WX01-T_HMCKJDSX2-4-ATTGGCTC_filtered.vcf.gz; 16:36:22.842 INFO Funcotator - Done initializing engine; 16:36:22.842 INFO Funcotator - Validating sequence dictionaries...; 16:36:22.856 INFO Funcotator - Processing user transcripts/defaults/overrides...; 16:36:22.857 INFO Funcotator - Initializing data sources...; 16:36:22.859 INFO DataSourceUtils - Initializing data sources from directory: /home/ppshah/shared/pipelines/mutect/funcotator_dataSources.v1.7.20200521s; 16:36:22.871 INFO DataSourceUtils - Data sources version: 1.7.2020429s; 16:36:22.871 INFO DataSourceUtils - Data sources source: ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/funcotator/funcotator_dataSources.v1.7.20200429s.tar.gz; 16:36:22.871 INFO DataSourceUtils - Data sources alternate source: gs://broad-public-datasets/funcotator/funcotator_dataSources.v1.7.20200429s.tar.gz; 16:36:22.891 INFO Funcotator - Shutting down engine; [January 10, 2024 at 4:36:22 PM GMT] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=285212672; ***********************************************************************. A USER ERROR has occurred: ERROR: Directory contains more than one config file: file:///home/ppshah/shared/pipelines/mutect/funcotator_dataSources.v1.7.20200521s/gencode_xrefseq/hg38/. ***********************************************************************; Set the system property GATK_STACKTRACE_ON_USER_EXCEPTION (--java-options '-DGATK_STACKTRACE_ON_USER_EXCEPTION=true') to print the stack trace. Any guidance to resolve the issue is appreciated.; Thank you!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8647
https://github.com/broadinstitute/gatk/issues/8647:4823,Availability,ERROR,ERROR,4823,"nflater; 16:36:22.399 INFO Funcotator - GCS max retries/reopens: 20; 16:36:22.399 INFO Funcotator - Requester pays: disabled; 16:36:22.399 INFO Funcotator - Initializing engine; 16:36:22.624 INFO FeatureManager - Using codec VCFCodec to read file file:///home/ppshah/shared/CAS_MOSAIC/mutect/mrn_2507919/WES/KShaw-ROPR0004-DNA-229761-WX01-T_HMCKJDSX2-4-ATTGGCTC/KShaw-ROPR0004-DNA-229761-WX01-T_HMCKJDSX2-4-ATTGGCTC_filtered.vcf.gz; 16:36:22.842 INFO Funcotator - Done initializing engine; 16:36:22.842 INFO Funcotator - Validating sequence dictionaries...; 16:36:22.856 INFO Funcotator - Processing user transcripts/defaults/overrides...; 16:36:22.857 INFO Funcotator - Initializing data sources...; 16:36:22.859 INFO DataSourceUtils - Initializing data sources from directory: /home/ppshah/shared/pipelines/mutect/funcotator_dataSources.v1.7.20200521s; 16:36:22.871 INFO DataSourceUtils - Data sources version: 1.7.2020429s; 16:36:22.871 INFO DataSourceUtils - Data sources source: ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/funcotator/funcotator_dataSources.v1.7.20200429s.tar.gz; 16:36:22.871 INFO DataSourceUtils - Data sources alternate source: gs://broad-public-datasets/funcotator/funcotator_dataSources.v1.7.20200429s.tar.gz; 16:36:22.891 INFO Funcotator - Shutting down engine; [January 10, 2024 at 4:36:22 PM GMT] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=285212672; ***********************************************************************. A USER ERROR has occurred: ERROR: Directory contains more than one config file: file:///home/ppshah/shared/pipelines/mutect/funcotator_dataSources.v1.7.20200521s/gencode_xrefseq/hg38/. ***********************************************************************; Set the system property GATK_STACKTRACE_ON_USER_EXCEPTION (--java-options '-DGATK_STACKTRACE_ON_USER_EXCEPTION=true') to print the stack trace. Any guidance to resolve the issue is appreciated.; Thank you!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8647
https://github.com/broadinstitute/gatk/issues/8647:499,Deployability,pipeline,pipelines,499,"Hi,. When I try to run Funcotator, using the below command:. gatk Funcotator \; --variant /rsrch5/home/tdccct/ppshah/shared/CAS_MOSAIC/mutect/mrn_2507919/WES/KShaw-ROPR0004-DNA-229761-WX01-T_HMCKJDSX2-4-ATTGGCTC/KShaw-ROPR0004-DNA-229761-WX01-T_HMCKJDSX2-4-ATTGGCTC_filtered.vcf.gz \; --reference /rsrch5/home/tdccct/ppshah/shared/gencode/Homo_sapiens/GATK/GRCh38/Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta \; --ref-version hg38 \; --data-sources-path /rsrch5/home/tdccct/ppshah/shared/pipelines/mutect/funcotator_dataSources.v1.7.20200521s \; --output /rsrch5/home/tdccct/ppshah/shared/CAS_MOSAIC/mutect/mrn_2507919/WES/KShaw-ROPR0004-DNA-229761-WX01-T_HMCKJDSX2-4-ATTGGCTC/KShaw-ROPR0004-DNA-229761-WX01-T_HMCKJDSX2-4-ATTGGCTC_funcotated.vcf \; --output-file-format VCF; ; I get the following error:; ; Using GATK jar /gatk/gatk-package-4.4.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.4.0.0-local.jar Funcotator --variant /home/ppshah/shared/CAS_MOSAIC/mutect/mrn_2507919/WES/KShaw-ROPR0004-DNA-229761-WX01-T_HMCKJDSX2-4-ATTGGCTC/KShaw-ROPR0004-DNA-229761-WX01-T_HMCKJDSX2-4-ATTGGCTC_filtered.vcf.gz --reference /home/ppshah/shared/gencode/Homo_sapiens/GATK/GRCh38/Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta --ref-version hg38 --data-sources-path /home/ppshah/shared/pipelines/mutect/funcotator_dataSources.v1.7.20200521s --output /home/ppshah/shared/CAS_MOSAIC/mutect/mrn_2507919/WES/KShaw-ROPR0004-DNA-229761-WX01-T_HMCKJDSX2-4-ATTGGCTC/KShaw-ROPR0004-DNA-229761-WX01-T_HMCKJDSX2-4-ATTGGCTC_funcotated.vcf --output-file-format VCF; 16:36:22.352 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.4.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 16:36:22.392 INFO Funcotator - ------------------------------------------------------------; 16:36:22.396 ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8647
https://github.com/broadinstitute/gatk/issues/8647:1462,Deployability,pipeline,pipelines,1462,"ta-sources-path /rsrch5/home/tdccct/ppshah/shared/pipelines/mutect/funcotator_dataSources.v1.7.20200521s \; --output /rsrch5/home/tdccct/ppshah/shared/CAS_MOSAIC/mutect/mrn_2507919/WES/KShaw-ROPR0004-DNA-229761-WX01-T_HMCKJDSX2-4-ATTGGCTC/KShaw-ROPR0004-DNA-229761-WX01-T_HMCKJDSX2-4-ATTGGCTC_funcotated.vcf \; --output-file-format VCF; ; I get the following error:; ; Using GATK jar /gatk/gatk-package-4.4.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.4.0.0-local.jar Funcotator --variant /home/ppshah/shared/CAS_MOSAIC/mutect/mrn_2507919/WES/KShaw-ROPR0004-DNA-229761-WX01-T_HMCKJDSX2-4-ATTGGCTC/KShaw-ROPR0004-DNA-229761-WX01-T_HMCKJDSX2-4-ATTGGCTC_filtered.vcf.gz --reference /home/ppshah/shared/gencode/Homo_sapiens/GATK/GRCh38/Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta --ref-version hg38 --data-sources-path /home/ppshah/shared/pipelines/mutect/funcotator_dataSources.v1.7.20200521s --output /home/ppshah/shared/CAS_MOSAIC/mutect/mrn_2507919/WES/KShaw-ROPR0004-DNA-229761-WX01-T_HMCKJDSX2-4-ATTGGCTC/KShaw-ROPR0004-DNA-229761-WX01-T_HMCKJDSX2-4-ATTGGCTC_funcotated.vcf --output-file-format VCF; 16:36:22.352 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.4.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 16:36:22.392 INFO Funcotator - ------------------------------------------------------------; 16:36:22.396 INFO Funcotator - The Genome Analysis Toolkit (GATK) v4.4.0.0; 16:36:22.396 INFO Funcotator - For support and documentation go to https://software.broadinstitute.org/gatk/; 16:36:22.396 INFO Funcotator - Executing as ppshah@ldragon1 on Linux v3.10.0-1160.15.2.el7.x86_64 amd64; 16:36:22.396 INFO Funcotator - Java runtime: OpenJDK 64-Bit Server VM v17.0.6+10-Ubuntu-0ubuntu118.04.1; 16:36:22.396 INFO Funcotator - Start Date/Time: January 10, 2024 a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8647
https://github.com/broadinstitute/gatk/issues/8647:4058,Deployability,pipeline,pipelines,4058,"ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 16:36:22.398 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:36:22.398 INFO Funcotator - Deflater: IntelDeflater; 16:36:22.398 INFO Funcotator - Inflater: IntelInflater; 16:36:22.399 INFO Funcotator - GCS max retries/reopens: 20; 16:36:22.399 INFO Funcotator - Requester pays: disabled; 16:36:22.399 INFO Funcotator - Initializing engine; 16:36:22.624 INFO FeatureManager - Using codec VCFCodec to read file file:///home/ppshah/shared/CAS_MOSAIC/mutect/mrn_2507919/WES/KShaw-ROPR0004-DNA-229761-WX01-T_HMCKJDSX2-4-ATTGGCTC/KShaw-ROPR0004-DNA-229761-WX01-T_HMCKJDSX2-4-ATTGGCTC_filtered.vcf.gz; 16:36:22.842 INFO Funcotator - Done initializing engine; 16:36:22.842 INFO Funcotator - Validating sequence dictionaries...; 16:36:22.856 INFO Funcotator - Processing user transcripts/defaults/overrides...; 16:36:22.857 INFO Funcotator - Initializing data sources...; 16:36:22.859 INFO DataSourceUtils - Initializing data sources from directory: /home/ppshah/shared/pipelines/mutect/funcotator_dataSources.v1.7.20200521s; 16:36:22.871 INFO DataSourceUtils - Data sources version: 1.7.2020429s; 16:36:22.871 INFO DataSourceUtils - Data sources source: ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/funcotator/funcotator_dataSources.v1.7.20200429s.tar.gz; 16:36:22.871 INFO DataSourceUtils - Data sources alternate source: gs://broad-public-datasets/funcotator/funcotator_dataSources.v1.7.20200429s.tar.gz; 16:36:22.891 INFO Funcotator - Shutting down engine; [January 10, 2024 at 4:36:22 PM GMT] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=285212672; ***********************************************************************. A USER ERROR has occurred: ERROR: Directory contains more than one config file: file:///home/ppshah/shared/pipelines/mutect/funcotator_dataSources.v1.7.20200521s/gencode_xrefseq/hg38/. *****************************************************",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8647
https://github.com/broadinstitute/gatk/issues/8647:4903,Deployability,pipeline,pipelines,4903,"nflater; 16:36:22.399 INFO Funcotator - GCS max retries/reopens: 20; 16:36:22.399 INFO Funcotator - Requester pays: disabled; 16:36:22.399 INFO Funcotator - Initializing engine; 16:36:22.624 INFO FeatureManager - Using codec VCFCodec to read file file:///home/ppshah/shared/CAS_MOSAIC/mutect/mrn_2507919/WES/KShaw-ROPR0004-DNA-229761-WX01-T_HMCKJDSX2-4-ATTGGCTC/KShaw-ROPR0004-DNA-229761-WX01-T_HMCKJDSX2-4-ATTGGCTC_filtered.vcf.gz; 16:36:22.842 INFO Funcotator - Done initializing engine; 16:36:22.842 INFO Funcotator - Validating sequence dictionaries...; 16:36:22.856 INFO Funcotator - Processing user transcripts/defaults/overrides...; 16:36:22.857 INFO Funcotator - Initializing data sources...; 16:36:22.859 INFO DataSourceUtils - Initializing data sources from directory: /home/ppshah/shared/pipelines/mutect/funcotator_dataSources.v1.7.20200521s; 16:36:22.871 INFO DataSourceUtils - Data sources version: 1.7.2020429s; 16:36:22.871 INFO DataSourceUtils - Data sources source: ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/funcotator/funcotator_dataSources.v1.7.20200429s.tar.gz; 16:36:22.871 INFO DataSourceUtils - Data sources alternate source: gs://broad-public-datasets/funcotator/funcotator_dataSources.v1.7.20200429s.tar.gz; 16:36:22.891 INFO Funcotator - Shutting down engine; [January 10, 2024 at 4:36:22 PM GMT] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=285212672; ***********************************************************************. A USER ERROR has occurred: ERROR: Directory contains more than one config file: file:///home/ppshah/shared/pipelines/mutect/funcotator_dataSources.v1.7.20200521s/gencode_xrefseq/hg38/. ***********************************************************************; Set the system property GATK_STACKTRACE_ON_USER_EXCEPTION (--java-options '-DGATK_STACKTRACE_ON_USER_EXCEPTION=true') to print the stack trace. Any guidance to resolve the issue is appreciated.; Thank you!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8647
https://github.com/broadinstitute/gatk/issues/8647:4863,Modifiability,config,config,4863,"nflater; 16:36:22.399 INFO Funcotator - GCS max retries/reopens: 20; 16:36:22.399 INFO Funcotator - Requester pays: disabled; 16:36:22.399 INFO Funcotator - Initializing engine; 16:36:22.624 INFO FeatureManager - Using codec VCFCodec to read file file:///home/ppshah/shared/CAS_MOSAIC/mutect/mrn_2507919/WES/KShaw-ROPR0004-DNA-229761-WX01-T_HMCKJDSX2-4-ATTGGCTC/KShaw-ROPR0004-DNA-229761-WX01-T_HMCKJDSX2-4-ATTGGCTC_filtered.vcf.gz; 16:36:22.842 INFO Funcotator - Done initializing engine; 16:36:22.842 INFO Funcotator - Validating sequence dictionaries...; 16:36:22.856 INFO Funcotator - Processing user transcripts/defaults/overrides...; 16:36:22.857 INFO Funcotator - Initializing data sources...; 16:36:22.859 INFO DataSourceUtils - Initializing data sources from directory: /home/ppshah/shared/pipelines/mutect/funcotator_dataSources.v1.7.20200521s; 16:36:22.871 INFO DataSourceUtils - Data sources version: 1.7.2020429s; 16:36:22.871 INFO DataSourceUtils - Data sources source: ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/funcotator/funcotator_dataSources.v1.7.20200429s.tar.gz; 16:36:22.871 INFO DataSourceUtils - Data sources alternate source: gs://broad-public-datasets/funcotator/funcotator_dataSources.v1.7.20200429s.tar.gz; 16:36:22.891 INFO Funcotator - Shutting down engine; [January 10, 2024 at 4:36:22 PM GMT] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=285212672; ***********************************************************************. A USER ERROR has occurred: ERROR: Directory contains more than one config file: file:///home/ppshah/shared/pipelines/mutect/funcotator_dataSources.v1.7.20200521s/gencode_xrefseq/hg38/. ***********************************************************************; Set the system property GATK_STACKTRACE_ON_USER_EXCEPTION (--java-options '-DGATK_STACKTRACE_ON_USER_EXCEPTION=true') to print the stack trace. Any guidance to resolve the issue is appreciated.; Thank you!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8647
https://github.com/broadinstitute/gatk/issues/8647:1769,Performance,Load,Loading,1769,"t-file-format VCF; ; I get the following error:; ; Using GATK jar /gatk/gatk-package-4.4.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.4.0.0-local.jar Funcotator --variant /home/ppshah/shared/CAS_MOSAIC/mutect/mrn_2507919/WES/KShaw-ROPR0004-DNA-229761-WX01-T_HMCKJDSX2-4-ATTGGCTC/KShaw-ROPR0004-DNA-229761-WX01-T_HMCKJDSX2-4-ATTGGCTC_filtered.vcf.gz --reference /home/ppshah/shared/gencode/Homo_sapiens/GATK/GRCh38/Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta --ref-version hg38 --data-sources-path /home/ppshah/shared/pipelines/mutect/funcotator_dataSources.v1.7.20200521s --output /home/ppshah/shared/CAS_MOSAIC/mutect/mrn_2507919/WES/KShaw-ROPR0004-DNA-229761-WX01-T_HMCKJDSX2-4-ATTGGCTC/KShaw-ROPR0004-DNA-229761-WX01-T_HMCKJDSX2-4-ATTGGCTC_funcotated.vcf --output-file-format VCF; 16:36:22.352 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.4.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 16:36:22.392 INFO Funcotator - ------------------------------------------------------------; 16:36:22.396 INFO Funcotator - The Genome Analysis Toolkit (GATK) v4.4.0.0; 16:36:22.396 INFO Funcotator - For support and documentation go to https://software.broadinstitute.org/gatk/; 16:36:22.396 INFO Funcotator - Executing as ppshah@ldragon1 on Linux v3.10.0-1160.15.2.el7.x86_64 amd64; 16:36:22.396 INFO Funcotator - Java runtime: OpenJDK 64-Bit Server VM v17.0.6+10-Ubuntu-0ubuntu118.04.1; 16:36:22.396 INFO Funcotator - Start Date/Time: January 10, 2024 at 4:36:22 PM GMT; 16:36:22.396 INFO Funcotator - ------------------------------------------------------------; 16:36:22.396 INFO Funcotator - ------------------------------------------------------------; 16:36:22.397 INFO Funcotator - HTSJDK Version: 3.0.5; 16:36:22.397 INFO Funcotator - Picard Version: 3.0.0; 16:36",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8647
https://github.com/broadinstitute/gatk/issues/8647:3780,Security,Validat,Validating,3780,"tator - Built for Spark Version: 3.3.1; 16:36:22.397 INFO Funcotator - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 16:36:22.397 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 16:36:22.398 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 16:36:22.398 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:36:22.398 INFO Funcotator - Deflater: IntelDeflater; 16:36:22.398 INFO Funcotator - Inflater: IntelInflater; 16:36:22.399 INFO Funcotator - GCS max retries/reopens: 20; 16:36:22.399 INFO Funcotator - Requester pays: disabled; 16:36:22.399 INFO Funcotator - Initializing engine; 16:36:22.624 INFO FeatureManager - Using codec VCFCodec to read file file:///home/ppshah/shared/CAS_MOSAIC/mutect/mrn_2507919/WES/KShaw-ROPR0004-DNA-229761-WX01-T_HMCKJDSX2-4-ATTGGCTC/KShaw-ROPR0004-DNA-229761-WX01-T_HMCKJDSX2-4-ATTGGCTC_filtered.vcf.gz; 16:36:22.842 INFO Funcotator - Done initializing engine; 16:36:22.842 INFO Funcotator - Validating sequence dictionaries...; 16:36:22.856 INFO Funcotator - Processing user transcripts/defaults/overrides...; 16:36:22.857 INFO Funcotator - Initializing data sources...; 16:36:22.859 INFO DataSourceUtils - Initializing data sources from directory: /home/ppshah/shared/pipelines/mutect/funcotator_dataSources.v1.7.20200521s; 16:36:22.871 INFO DataSourceUtils - Data sources version: 1.7.2020429s; 16:36:22.871 INFO DataSourceUtils - Data sources source: ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/funcotator/funcotator_dataSources.v1.7.20200429s.tar.gz; 16:36:22.871 INFO DataSourceUtils - Data sources alternate source: gs://broad-public-datasets/funcotator/funcotator_dataSources.v1.7.20200429s.tar.gz; 16:36:22.891 INFO Funcotator - Shutting down engine; [January 10, 2024 at 4:36:22 PM GMT] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=285212672; ***************************************************************",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8647
https://github.com/broadinstitute/gatk/issues/8647:5202,Usability,guid,guidance,5202,"nflater; 16:36:22.399 INFO Funcotator - GCS max retries/reopens: 20; 16:36:22.399 INFO Funcotator - Requester pays: disabled; 16:36:22.399 INFO Funcotator - Initializing engine; 16:36:22.624 INFO FeatureManager - Using codec VCFCodec to read file file:///home/ppshah/shared/CAS_MOSAIC/mutect/mrn_2507919/WES/KShaw-ROPR0004-DNA-229761-WX01-T_HMCKJDSX2-4-ATTGGCTC/KShaw-ROPR0004-DNA-229761-WX01-T_HMCKJDSX2-4-ATTGGCTC_filtered.vcf.gz; 16:36:22.842 INFO Funcotator - Done initializing engine; 16:36:22.842 INFO Funcotator - Validating sequence dictionaries...; 16:36:22.856 INFO Funcotator - Processing user transcripts/defaults/overrides...; 16:36:22.857 INFO Funcotator - Initializing data sources...; 16:36:22.859 INFO DataSourceUtils - Initializing data sources from directory: /home/ppshah/shared/pipelines/mutect/funcotator_dataSources.v1.7.20200521s; 16:36:22.871 INFO DataSourceUtils - Data sources version: 1.7.2020429s; 16:36:22.871 INFO DataSourceUtils - Data sources source: ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/funcotator/funcotator_dataSources.v1.7.20200429s.tar.gz; 16:36:22.871 INFO DataSourceUtils - Data sources alternate source: gs://broad-public-datasets/funcotator/funcotator_dataSources.v1.7.20200429s.tar.gz; 16:36:22.891 INFO Funcotator - Shutting down engine; [January 10, 2024 at 4:36:22 PM GMT] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=285212672; ***********************************************************************. A USER ERROR has occurred: ERROR: Directory contains more than one config file: file:///home/ppshah/shared/pipelines/mutect/funcotator_dataSources.v1.7.20200521s/gencode_xrefseq/hg38/. ***********************************************************************; Set the system property GATK_STACKTRACE_ON_USER_EXCEPTION (--java-options '-DGATK_STACKTRACE_ON_USER_EXCEPTION=true') to print the stack trace. Any guidance to resolve the issue is appreciated.; Thank you!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8647
https://github.com/broadinstitute/gatk/pull/8649:28,Availability,Echo,Echo,28,"As John loads data into the Echo callset, he will certainly run into issues in the documentation, that should be addressed",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8649
https://github.com/broadinstitute/gatk/pull/8649:8,Performance,load,loads,8,"As John loads data into the Echo callset, he will certainly run into issues in the documentation, that should be addressed",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8649
https://github.com/broadinstitute/gatk/pull/8650:343,Availability,error,error,343,Reduce the logging a bit.; Probably should make a PR directly into gatk master so that when we next merge gatk master changes we'll get this goodness?. Integration run [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/f8c38f97-7945-414f-9432-13b2f12138bb) (note failed one of the subtests for a random docker pull error); Example CreateFilterSet run [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/b2e7eb86-e494-4891-885b-5a96cb1056b3),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8650
https://github.com/broadinstitute/gatk/pull/8650:152,Deployability,Integrat,Integration,152,Reduce the logging a bit.; Probably should make a PR directly into gatk master so that when we next merge gatk master changes we'll get this goodness?. Integration run [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/f8c38f97-7945-414f-9432-13b2f12138bb) (note failed one of the subtests for a random docker pull error); Example CreateFilterSet run [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/b2e7eb86-e494-4891-885b-5a96cb1056b3),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8650
https://github.com/broadinstitute/gatk/pull/8650:0,Energy Efficiency,Reduce,Reduce,0,Reduce the logging a bit.; Probably should make a PR directly into gatk master so that when we next merge gatk master changes we'll get this goodness?. Integration run [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/f8c38f97-7945-414f-9432-13b2f12138bb) (note failed one of the subtests for a random docker pull error); Example CreateFilterSet run [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/b2e7eb86-e494-4891-885b-5a96cb1056b3),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8650
https://github.com/broadinstitute/gatk/pull/8650:152,Integrability,Integrat,Integration,152,Reduce the logging a bit.; Probably should make a PR directly into gatk master so that when we next merge gatk master changes we'll get this goodness?. Integration run [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/f8c38f97-7945-414f-9432-13b2f12138bb) (note failed one of the subtests for a random docker pull error); Example CreateFilterSet run [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/b2e7eb86-e494-4891-885b-5a96cb1056b3),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8650
https://github.com/broadinstitute/gatk/pull/8650:11,Testability,log,logging,11,Reduce the logging a bit.; Probably should make a PR directly into gatk master so that when we next merge gatk master changes we'll get this goodness?. Integration run [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/f8c38f97-7945-414f-9432-13b2f12138bb) (note failed one of the subtests for a random docker pull error); Example CreateFilterSet run [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/b2e7eb86-e494-4891-885b-5a96cb1056b3),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8650
https://github.com/broadinstitute/gatk/pull/8651:2,Deployability,update,update,2,* update setup-gcloud@v0 -> v2 since v0 is deprecated,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8651
https://github.com/broadinstitute/gatk/issues/8654:792,Availability,error,error,792,"CPU usage was high. ```; %CPU WallTime Time Lim RSS mem memlim cpus; hugemem-ex; 105958574 R ds6924 hm82 getpileu 99 07:23:01 16:00:00 1120GB 1120GB 1400GB 37; ```. However, it never proceeds past the first interval. ```; 08:21:42.921 INFO GetPileupSummaries - The Genome Analysis Toolkit (GATK) v4.4.0.0; 08:21:42.921 INFO GetPileupSummaries - Start Date/Time: January 12, 2024 at 8:21:42 AM GMT+10:00; 08:21:42.927 INFO GetPileupSummaries - Initializing engine; 08:55:35.361 INFO IntervalArgumentCollection - Processing 326649654 bp from intervals; 08:57:45.036 INFO GetPileupSummaries - Done initializing engine; 08:57:45.101 INFO ProgressMeter - Starting traversal; 08:57:45.106 INFO ProgressMeter - Current Locus Elapsed Minutes Loci Processed Loci/Minute; (END); ```. There is a memory error in some log files but only after many hours and no intervals processed. ```; 08:34:26.243 INFO ProgressMeter - Starting traversal; 08:34:26.244 INFO ProgressMeter - Current Locus Elapsed Minutes Loci Processed Loci/Minute; 15:35:01.977 INFO GetPileupSummaries - Shutting down engine; [January 12, 2024 at 3:35:02 PM GMT+10:00] org.broadinstitute.hellbender.tools.walkers.contamination.GetPileupSummaries done. Elapsed time: 433.32 minutes.; Runtime.totalMemory()=31136546816; Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8654
https://github.com/broadinstitute/gatk/issues/8654:1069,Availability,down,down,1069,"CPU usage was high. ```; %CPU WallTime Time Lim RSS mem memlim cpus; hugemem-ex; 105958574 R ds6924 hm82 getpileu 99 07:23:01 16:00:00 1120GB 1120GB 1400GB 37; ```. However, it never proceeds past the first interval. ```; 08:21:42.921 INFO GetPileupSummaries - The Genome Analysis Toolkit (GATK) v4.4.0.0; 08:21:42.921 INFO GetPileupSummaries - Start Date/Time: January 12, 2024 at 8:21:42 AM GMT+10:00; 08:21:42.927 INFO GetPileupSummaries - Initializing engine; 08:55:35.361 INFO IntervalArgumentCollection - Processing 326649654 bp from intervals; 08:57:45.036 INFO GetPileupSummaries - Done initializing engine; 08:57:45.101 INFO ProgressMeter - Starting traversal; 08:57:45.106 INFO ProgressMeter - Current Locus Elapsed Minutes Loci Processed Loci/Minute; (END); ```. There is a memory error in some log files but only after many hours and no intervals processed. ```; 08:34:26.243 INFO ProgressMeter - Starting traversal; 08:34:26.244 INFO ProgressMeter - Current Locus Elapsed Minutes Loci Processed Loci/Minute; 15:35:01.977 INFO GetPileupSummaries - Shutting down engine; [January 12, 2024 at 3:35:02 PM GMT+10:00] org.broadinstitute.hellbender.tools.walkers.contamination.GetPileupSummaries done. Elapsed time: 433.32 minutes.; Runtime.totalMemory()=31136546816; Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8654
https://github.com/broadinstitute/gatk/issues/8654:806,Testability,log,log,806,"CPU usage was high. ```; %CPU WallTime Time Lim RSS mem memlim cpus; hugemem-ex; 105958574 R ds6924 hm82 getpileu 99 07:23:01 16:00:00 1120GB 1120GB 1400GB 37; ```. However, it never proceeds past the first interval. ```; 08:21:42.921 INFO GetPileupSummaries - The Genome Analysis Toolkit (GATK) v4.4.0.0; 08:21:42.921 INFO GetPileupSummaries - Start Date/Time: January 12, 2024 at 8:21:42 AM GMT+10:00; 08:21:42.927 INFO GetPileupSummaries - Initializing engine; 08:55:35.361 INFO IntervalArgumentCollection - Processing 326649654 bp from intervals; 08:57:45.036 INFO GetPileupSummaries - Done initializing engine; 08:57:45.101 INFO ProgressMeter - Starting traversal; 08:57:45.106 INFO ProgressMeter - Current Locus Elapsed Minutes Loci Processed Loci/Minute; (END); ```. There is a memory error in some log files but only after many hours and no intervals processed. ```; 08:34:26.243 INFO ProgressMeter - Starting traversal; 08:34:26.244 INFO ProgressMeter - Current Locus Elapsed Minutes Loci Processed Loci/Minute; 15:35:01.977 INFO GetPileupSummaries - Shutting down engine; [January 12, 2024 at 3:35:02 PM GMT+10:00] org.broadinstitute.hellbender.tools.walkers.contamination.GetPileupSummaries done. Elapsed time: 433.32 minutes.; Runtime.totalMemory()=31136546816; Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8654
https://github.com/broadinstitute/gatk/pull/8655:130,Deployability,Integrat,Integration,130,[Here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/5eb30da2-4afe-4f8e-ad72-e96ac647c588) is a passing Integration test (note - updated truth).; [Here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/7ab3d8de-4325-4ca1-b507-1546f9b89986) is a run of GvsJointCalling using this code.; [Here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/494b9661-b5d2-4d4e-8da7-8df84d52e162) is a run of GvsExtractCalset extracting from tables created by this code (with phasing fields in the prepare tables) ; [Here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/dddfefce-f4c7-49bf-aabb-d9b92720b809) is a run of GvsExtractCallset extracing from (OLD) tables created before this code (without phasing fields in the prepare tables).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8655
https://github.com/broadinstitute/gatk/pull/8655:155,Deployability,update,updated,155,[Here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/5eb30da2-4afe-4f8e-ad72-e96ac647c588) is a passing Integration test (note - updated truth).; [Here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/7ab3d8de-4325-4ca1-b507-1546f9b89986) is a run of GvsJointCalling using this code.; [Here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/494b9661-b5d2-4d4e-8da7-8df84d52e162) is a run of GvsExtractCalset extracting from tables created by this code (with phasing fields in the prepare tables) ; [Here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/dddfefce-f4c7-49bf-aabb-d9b92720b809) is a run of GvsExtractCallset extracing from (OLD) tables created before this code (without phasing fields in the prepare tables).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8655
https://github.com/broadinstitute/gatk/pull/8655:130,Integrability,Integrat,Integration,130,[Here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/5eb30da2-4afe-4f8e-ad72-e96ac647c588) is a passing Integration test (note - updated truth).; [Here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/7ab3d8de-4325-4ca1-b507-1546f9b89986) is a run of GvsJointCalling using this code.; [Here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/494b9661-b5d2-4d4e-8da7-8df84d52e162) is a run of GvsExtractCalset extracting from tables created by this code (with phasing fields in the prepare tables) ; [Here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/dddfefce-f4c7-49bf-aabb-d9b92720b809) is a run of GvsExtractCallset extracing from (OLD) tables created before this code (without phasing fields in the prepare tables).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8655
https://github.com/broadinstitute/gatk/pull/8655:142,Testability,test,test,142,[Here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/5eb30da2-4afe-4f8e-ad72-e96ac647c588) is a passing Integration test (note - updated truth).; [Here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/7ab3d8de-4325-4ca1-b507-1546f9b89986) is a run of GvsJointCalling using this code.; [Here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/494b9661-b5d2-4d4e-8da7-8df84d52e162) is a run of GvsExtractCalset extracting from tables created by this code (with phasing fields in the prepare tables) ; [Here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/dddfefce-f4c7-49bf-aabb-d9b92720b809) is a run of GvsExtractCallset extracing from (OLD) tables created before this code (without phasing fields in the prepare tables).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8655
https://github.com/broadinstitute/gatk/pull/8656:730,Testability,test,tested,730,"In some cases it may be useful to know what reads are giving rise to which specific variants. I have run into several cases while debugging some strange results where this would be useful to know, and also there is a QC workflow we would like to implement where this would be essential information. This is unlikely to be generally useful, however. This PR adds a flag, `--write-qnames`, which will, for each variant, write the list of qnames in the bam that give rise to that variant as a comma separated list in the final column. This PR also makes synonymous variants (with no protein-level consequence) write an empty value rather than nothing, in order to keep column order. This seems to work with SE reads, but hasn't been tested much with PE reads. This should also probably not parse read names by default, but only if write-qnames is set.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8656
https://github.com/broadinstitute/gatk/pull/8662:54,Availability,avail,available,54,"Make the logging frequency used by the ProgressLogger available as an input. If not used, sets the default value. Variants team is using a branch of gatk and have made this change there, so pulling this change into master to simplify future merges / branch updates.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8662
https://github.com/broadinstitute/gatk/pull/8662:257,Deployability,update,updates,257,"Make the logging frequency used by the ProgressLogger available as an input. If not used, sets the default value. Variants team is using a branch of gatk and have made this change there, so pulling this change into master to simplify future merges / branch updates.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8662
https://github.com/broadinstitute/gatk/pull/8662:9,Testability,log,logging,9,"Make the logging frequency used by the ProgressLogger available as an input. If not used, sets the default value. Variants team is using a branch of gatk and have made this change there, so pulling this change into master to simplify future merges / branch updates.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8662
https://github.com/broadinstitute/gatk/pull/8662:225,Usability,simpl,simplify,225,"Make the logging frequency used by the ProgressLogger available as an input. If not used, sets the default value. Variants team is using a branch of gatk and have made this change there, so pulling this change into master to simplify future merges / branch updates.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8662
https://github.com/broadinstitute/gatk/issues/8664:320,Availability,error,error,320,"## Bug Report. ### Affected tool(s) or class(es); VariantRecalibrator. ### Affected version(s); - [X] Latest public release version [4.5.0.0]; - [ ] Latest master branch as of [date of test?]. ### Description ; As of v1.3.0 the `scales` R package turns the use of deprecated values for the `space` parameter into a hard error, resulting in the VariantRecalibrator R-script terminating with the following message:. > The `space` argument of `pal_gradient_n()` only supports be ""Lab"" as of scales 0.3.0. This parameter is used repeatedly in the generated R-script via. ```R; scale_fill_gradient(high=""green"", low=""red"", space=""rgb""); ```. #### Steps to reproduce. ```shell; $ R --version; R version 4.1.2 (2021-11-01) -- ""Bird Hippie""; $ rm -rf ~/R; $ R; > install.packages(""ggplot2"", repos=""https://cloud.r-project.org/""); > packageVersion(""scales""); [1] ‘1.3.0’; > quit(); $ gatk --version; The Genome Analysis Toolkit (GATK) v4.5.0.0; HTSJDK Version: 4.1.0; Picard Version: 3.1.1; $ gatk VariantRecalibrator [arguments omitted for brevity]; org.broadinstitute.hellbender.utils.R.RScriptExecutorException: ; Rscript exited with 1; Command Line: Rscript -e tempLibDir = '/tmp/Rlib.9339186078473502558';source('/path/to/rscript.r');; Stdout: ; Stderr: Error:; ! The `space` argument of `pal_gradient_n()` only supports be ""Lab"" as; of scales 0.3.0.; Backtrace:; ▆; 1. ├─base::source(""/path/to/rscript.r""); 2. │ ├─base::withVisible(eval(ei, envir)); 3. │ └─base::eval(ei, envir); 4. │ └─base::eval(ei, envir); 5. └─ggplot2::scale_fill_gradient(high = ""green"", low = ""red"", space = ""rgb""); 6. ├─ggplot2::continuous_scale(...); 7. │ └─ggplot2::ggproto(...); 8. │ └─rlang::list2(...); 9. └─scales::seq_gradient_pal(low, high, space); 10. └─scales::pal_gradient_n(c(low, high), space = space); 11. └─lifecycle::deprecate_stop(""0.3.0"", ""pal_gradient_n(space = 'only supports be \""Lab\""')""); 12. └─lifecycle:::deprecate_stop0(msg); 13. └─rlang::cnd_signal(...); Execution halted; $ R; > install.packages(""remot",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8664
https://github.com/broadinstitute/gatk/issues/8664:1250,Availability,Error,Error,1250,"r the `space` parameter into a hard error, resulting in the VariantRecalibrator R-script terminating with the following message:. > The `space` argument of `pal_gradient_n()` only supports be ""Lab"" as of scales 0.3.0. This parameter is used repeatedly in the generated R-script via. ```R; scale_fill_gradient(high=""green"", low=""red"", space=""rgb""); ```. #### Steps to reproduce. ```shell; $ R --version; R version 4.1.2 (2021-11-01) -- ""Bird Hippie""; $ rm -rf ~/R; $ R; > install.packages(""ggplot2"", repos=""https://cloud.r-project.org/""); > packageVersion(""scales""); [1] ‘1.3.0’; > quit(); $ gatk --version; The Genome Analysis Toolkit (GATK) v4.5.0.0; HTSJDK Version: 4.1.0; Picard Version: 3.1.1; $ gatk VariantRecalibrator [arguments omitted for brevity]; org.broadinstitute.hellbender.utils.R.RScriptExecutorException: ; Rscript exited with 1; Command Line: Rscript -e tempLibDir = '/tmp/Rlib.9339186078473502558';source('/path/to/rscript.r');; Stdout: ; Stderr: Error:; ! The `space` argument of `pal_gradient_n()` only supports be ""Lab"" as; of scales 0.3.0.; Backtrace:; ▆; 1. ├─base::source(""/path/to/rscript.r""); 2. │ ├─base::withVisible(eval(ei, envir)); 3. │ └─base::eval(ei, envir); 4. │ └─base::eval(ei, envir); 5. └─ggplot2::scale_fill_gradient(high = ""green"", low = ""red"", space = ""rgb""); 6. ├─ggplot2::continuous_scale(...); 7. │ └─ggplot2::ggproto(...); 8. │ └─rlang::list2(...); 9. └─scales::seq_gradient_pal(low, high, space); 10. └─scales::pal_gradient_n(c(low, high), space = space); 11. └─lifecycle::deprecate_stop(""0.3.0"", ""pal_gradient_n(space = 'only supports be \""Lab\""')""); 12. └─lifecycle:::deprecate_stop0(msg); 13. └─rlang::cnd_signal(...); Execution halted; $ R; > install.packages(""remotes"", repos=""https://cloud.r-project.org/""); > library(remotes); > install_version(""scales"", version=""1.2.1"", repos=""https://cloud.r-project.org/""); > packageVersion(""scales""); [1] ‘1.2.1’; > quit(); $ gatk VariantRecalibrator [arguments omitted for brevity]; $; ```. #### Expected be",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8664
https://github.com/broadinstitute/gatk/issues/8664:116,Deployability,release,release,116,"## Bug Report. ### Affected tool(s) or class(es); VariantRecalibrator. ### Affected version(s); - [X] Latest public release version [4.5.0.0]; - [ ] Latest master branch as of [date of test?]. ### Description ; As of v1.3.0 the `scales` R package turns the use of deprecated values for the `space` parameter into a hard error, resulting in the VariantRecalibrator R-script terminating with the following message:. > The `space` argument of `pal_gradient_n()` only supports be ""Lab"" as of scales 0.3.0. This parameter is used repeatedly in the generated R-script via. ```R; scale_fill_gradient(high=""green"", low=""red"", space=""rgb""); ```. #### Steps to reproduce. ```shell; $ R --version; R version 4.1.2 (2021-11-01) -- ""Bird Hippie""; $ rm -rf ~/R; $ R; > install.packages(""ggplot2"", repos=""https://cloud.r-project.org/""); > packageVersion(""scales""); [1] ‘1.3.0’; > quit(); $ gatk --version; The Genome Analysis Toolkit (GATK) v4.5.0.0; HTSJDK Version: 4.1.0; Picard Version: 3.1.1; $ gatk VariantRecalibrator [arguments omitted for brevity]; org.broadinstitute.hellbender.utils.R.RScriptExecutorException: ; Rscript exited with 1; Command Line: Rscript -e tempLibDir = '/tmp/Rlib.9339186078473502558';source('/path/to/rscript.r');; Stdout: ; Stderr: Error:; ! The `space` argument of `pal_gradient_n()` only supports be ""Lab"" as; of scales 0.3.0.; Backtrace:; ▆; 1. ├─base::source(""/path/to/rscript.r""); 2. │ ├─base::withVisible(eval(ei, envir)); 3. │ └─base::eval(ei, envir); 4. │ └─base::eval(ei, envir); 5. └─ggplot2::scale_fill_gradient(high = ""green"", low = ""red"", space = ""rgb""); 6. ├─ggplot2::continuous_scale(...); 7. │ └─ggplot2::ggproto(...); 8. │ └─rlang::list2(...); 9. └─scales::seq_gradient_pal(low, high, space); 10. └─scales::pal_gradient_n(c(low, high), space = space); 11. └─lifecycle::deprecate_stop(""0.3.0"", ""pal_gradient_n(space = 'only supports be \""Lab\""')""); 12. └─lifecycle:::deprecate_stop0(msg); 13. └─rlang::cnd_signal(...); Execution halted; $ R; > install.packages(""remot",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8664
https://github.com/broadinstitute/gatk/issues/8664:755,Deployability,install,install,755,"## Bug Report. ### Affected tool(s) or class(es); VariantRecalibrator. ### Affected version(s); - [X] Latest public release version [4.5.0.0]; - [ ] Latest master branch as of [date of test?]. ### Description ; As of v1.3.0 the `scales` R package turns the use of deprecated values for the `space` parameter into a hard error, resulting in the VariantRecalibrator R-script terminating with the following message:. > The `space` argument of `pal_gradient_n()` only supports be ""Lab"" as of scales 0.3.0. This parameter is used repeatedly in the generated R-script via. ```R; scale_fill_gradient(high=""green"", low=""red"", space=""rgb""); ```. #### Steps to reproduce. ```shell; $ R --version; R version 4.1.2 (2021-11-01) -- ""Bird Hippie""; $ rm -rf ~/R; $ R; > install.packages(""ggplot2"", repos=""https://cloud.r-project.org/""); > packageVersion(""scales""); [1] ‘1.3.0’; > quit(); $ gatk --version; The Genome Analysis Toolkit (GATK) v4.5.0.0; HTSJDK Version: 4.1.0; Picard Version: 3.1.1; $ gatk VariantRecalibrator [arguments omitted for brevity]; org.broadinstitute.hellbender.utils.R.RScriptExecutorException: ; Rscript exited with 1; Command Line: Rscript -e tempLibDir = '/tmp/Rlib.9339186078473502558';source('/path/to/rscript.r');; Stdout: ; Stderr: Error:; ! The `space` argument of `pal_gradient_n()` only supports be ""Lab"" as; of scales 0.3.0.; Backtrace:; ▆; 1. ├─base::source(""/path/to/rscript.r""); 2. │ ├─base::withVisible(eval(ei, envir)); 3. │ └─base::eval(ei, envir); 4. │ └─base::eval(ei, envir); 5. └─ggplot2::scale_fill_gradient(high = ""green"", low = ""red"", space = ""rgb""); 6. ├─ggplot2::continuous_scale(...); 7. │ └─ggplot2::ggproto(...); 8. │ └─rlang::list2(...); 9. └─scales::seq_gradient_pal(low, high, space); 10. └─scales::pal_gradient_n(c(low, high), space = space); 11. └─lifecycle::deprecate_stop(""0.3.0"", ""pal_gradient_n(space = 'only supports be \""Lab\""')""); 12. └─lifecycle:::deprecate_stop0(msg); 13. └─rlang::cnd_signal(...); Execution halted; $ R; > install.packages(""remot",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8664
https://github.com/broadinstitute/gatk/issues/8664:1978,Deployability,install,install,1978,"ab"" as of scales 0.3.0. This parameter is used repeatedly in the generated R-script via. ```R; scale_fill_gradient(high=""green"", low=""red"", space=""rgb""); ```. #### Steps to reproduce. ```shell; $ R --version; R version 4.1.2 (2021-11-01) -- ""Bird Hippie""; $ rm -rf ~/R; $ R; > install.packages(""ggplot2"", repos=""https://cloud.r-project.org/""); > packageVersion(""scales""); [1] ‘1.3.0’; > quit(); $ gatk --version; The Genome Analysis Toolkit (GATK) v4.5.0.0; HTSJDK Version: 4.1.0; Picard Version: 3.1.1; $ gatk VariantRecalibrator [arguments omitted for brevity]; org.broadinstitute.hellbender.utils.R.RScriptExecutorException: ; Rscript exited with 1; Command Line: Rscript -e tempLibDir = '/tmp/Rlib.9339186078473502558';source('/path/to/rscript.r');; Stdout: ; Stderr: Error:; ! The `space` argument of `pal_gradient_n()` only supports be ""Lab"" as; of scales 0.3.0.; Backtrace:; ▆; 1. ├─base::source(""/path/to/rscript.r""); 2. │ ├─base::withVisible(eval(ei, envir)); 3. │ └─base::eval(ei, envir); 4. │ └─base::eval(ei, envir); 5. └─ggplot2::scale_fill_gradient(high = ""green"", low = ""red"", space = ""rgb""); 6. ├─ggplot2::continuous_scale(...); 7. │ └─ggplot2::ggproto(...); 8. │ └─rlang::list2(...); 9. └─scales::seq_gradient_pal(low, high, space); 10. └─scales::pal_gradient_n(c(low, high), space = space); 11. └─lifecycle::deprecate_stop(""0.3.0"", ""pal_gradient_n(space = 'only supports be \""Lab\""')""); 12. └─lifecycle:::deprecate_stop0(msg); 13. └─rlang::cnd_signal(...); Execution halted; $ R; > install.packages(""remotes"", repos=""https://cloud.r-project.org/""); > library(remotes); > install_version(""scales"", version=""1.2.1"", repos=""https://cloud.r-project.org/""); > packageVersion(""scales""); [1] ‘1.2.1’; > quit(); $ gatk VariantRecalibrator [arguments omitted for brevity]; $; ```. #### Expected behavior; The output rscript file is used to generate a PDF. #### Actual behavior; Generation of the PDF fails due to an deprecation in the `scales` library causing the `Rscript` command to abort.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8664
https://github.com/broadinstitute/gatk/issues/8664:599,Energy Efficiency,green,green,599,"## Bug Report. ### Affected tool(s) or class(es); VariantRecalibrator. ### Affected version(s); - [X] Latest public release version [4.5.0.0]; - [ ] Latest master branch as of [date of test?]. ### Description ; As of v1.3.0 the `scales` R package turns the use of deprecated values for the `space` parameter into a hard error, resulting in the VariantRecalibrator R-script terminating with the following message:. > The `space` argument of `pal_gradient_n()` only supports be ""Lab"" as of scales 0.3.0. This parameter is used repeatedly in the generated R-script via. ```R; scale_fill_gradient(high=""green"", low=""red"", space=""rgb""); ```. #### Steps to reproduce. ```shell; $ R --version; R version 4.1.2 (2021-11-01) -- ""Bird Hippie""; $ rm -rf ~/R; $ R; > install.packages(""ggplot2"", repos=""https://cloud.r-project.org/""); > packageVersion(""scales""); [1] ‘1.3.0’; > quit(); $ gatk --version; The Genome Analysis Toolkit (GATK) v4.5.0.0; HTSJDK Version: 4.1.0; Picard Version: 3.1.1; $ gatk VariantRecalibrator [arguments omitted for brevity]; org.broadinstitute.hellbender.utils.R.RScriptExecutorException: ; Rscript exited with 1; Command Line: Rscript -e tempLibDir = '/tmp/Rlib.9339186078473502558';source('/path/to/rscript.r');; Stdout: ; Stderr: Error:; ! The `space` argument of `pal_gradient_n()` only supports be ""Lab"" as; of scales 0.3.0.; Backtrace:; ▆; 1. ├─base::source(""/path/to/rscript.r""); 2. │ ├─base::withVisible(eval(ei, envir)); 3. │ └─base::eval(ei, envir); 4. │ └─base::eval(ei, envir); 5. └─ggplot2::scale_fill_gradient(high = ""green"", low = ""red"", space = ""rgb""); 6. ├─ggplot2::continuous_scale(...); 7. │ └─ggplot2::ggproto(...); 8. │ └─rlang::list2(...); 9. └─scales::seq_gradient_pal(low, high, space); 10. └─scales::pal_gradient_n(c(low, high), space = space); 11. └─lifecycle::deprecate_stop(""0.3.0"", ""pal_gradient_n(space = 'only supports be \""Lab\""')""); 12. └─lifecycle:::deprecate_stop0(msg); 13. └─rlang::cnd_signal(...); Execution halted; $ R; > install.packages(""remot",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8664
https://github.com/broadinstitute/gatk/issues/8664:1549,Energy Efficiency,green,green,1549,"ab"" as of scales 0.3.0. This parameter is used repeatedly in the generated R-script via. ```R; scale_fill_gradient(high=""green"", low=""red"", space=""rgb""); ```. #### Steps to reproduce. ```shell; $ R --version; R version 4.1.2 (2021-11-01) -- ""Bird Hippie""; $ rm -rf ~/R; $ R; > install.packages(""ggplot2"", repos=""https://cloud.r-project.org/""); > packageVersion(""scales""); [1] ‘1.3.0’; > quit(); $ gatk --version; The Genome Analysis Toolkit (GATK) v4.5.0.0; HTSJDK Version: 4.1.0; Picard Version: 3.1.1; $ gatk VariantRecalibrator [arguments omitted for brevity]; org.broadinstitute.hellbender.utils.R.RScriptExecutorException: ; Rscript exited with 1; Command Line: Rscript -e tempLibDir = '/tmp/Rlib.9339186078473502558';source('/path/to/rscript.r');; Stdout: ; Stderr: Error:; ! The `space` argument of `pal_gradient_n()` only supports be ""Lab"" as; of scales 0.3.0.; Backtrace:; ▆; 1. ├─base::source(""/path/to/rscript.r""); 2. │ ├─base::withVisible(eval(ei, envir)); 3. │ └─base::eval(ei, envir); 4. │ └─base::eval(ei, envir); 5. └─ggplot2::scale_fill_gradient(high = ""green"", low = ""red"", space = ""rgb""); 6. ├─ggplot2::continuous_scale(...); 7. │ └─ggplot2::ggproto(...); 8. │ └─rlang::list2(...); 9. └─scales::seq_gradient_pal(low, high, space); 10. └─scales::pal_gradient_n(c(low, high), space = space); 11. └─lifecycle::deprecate_stop(""0.3.0"", ""pal_gradient_n(space = 'only supports be \""Lab\""')""); 12. └─lifecycle:::deprecate_stop0(msg); 13. └─rlang::cnd_signal(...); Execution halted; $ R; > install.packages(""remotes"", repos=""https://cloud.r-project.org/""); > library(remotes); > install_version(""scales"", version=""1.2.1"", repos=""https://cloud.r-project.org/""); > packageVersion(""scales""); [1] ‘1.2.1’; > quit(); $ gatk VariantRecalibrator [arguments omitted for brevity]; $; ```. #### Expected behavior; The output rscript file is used to generate a PDF. #### Actual behavior; Generation of the PDF fails due to an deprecation in the `scales` library causing the `Rscript` command to abort.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8664
https://github.com/broadinstitute/gatk/issues/8664:404,Integrability,message,message,404,"## Bug Report. ### Affected tool(s) or class(es); VariantRecalibrator. ### Affected version(s); - [X] Latest public release version [4.5.0.0]; - [ ] Latest master branch as of [date of test?]. ### Description ; As of v1.3.0 the `scales` R package turns the use of deprecated values for the `space` parameter into a hard error, resulting in the VariantRecalibrator R-script terminating with the following message:. > The `space` argument of `pal_gradient_n()` only supports be ""Lab"" as of scales 0.3.0. This parameter is used repeatedly in the generated R-script via. ```R; scale_fill_gradient(high=""green"", low=""red"", space=""rgb""); ```. #### Steps to reproduce. ```shell; $ R --version; R version 4.1.2 (2021-11-01) -- ""Bird Hippie""; $ rm -rf ~/R; $ R; > install.packages(""ggplot2"", repos=""https://cloud.r-project.org/""); > packageVersion(""scales""); [1] ‘1.3.0’; > quit(); $ gatk --version; The Genome Analysis Toolkit (GATK) v4.5.0.0; HTSJDK Version: 4.1.0; Picard Version: 3.1.1; $ gatk VariantRecalibrator [arguments omitted for brevity]; org.broadinstitute.hellbender.utils.R.RScriptExecutorException: ; Rscript exited with 1; Command Line: Rscript -e tempLibDir = '/tmp/Rlib.9339186078473502558';source('/path/to/rscript.r');; Stdout: ; Stderr: Error:; ! The `space` argument of `pal_gradient_n()` only supports be ""Lab"" as; of scales 0.3.0.; Backtrace:; ▆; 1. ├─base::source(""/path/to/rscript.r""); 2. │ ├─base::withVisible(eval(ei, envir)); 3. │ └─base::eval(ei, envir); 4. │ └─base::eval(ei, envir); 5. └─ggplot2::scale_fill_gradient(high = ""green"", low = ""red"", space = ""rgb""); 6. ├─ggplot2::continuous_scale(...); 7. │ └─ggplot2::ggproto(...); 8. │ └─rlang::list2(...); 9. └─scales::seq_gradient_pal(low, high, space); 10. └─scales::pal_gradient_n(c(low, high), space = space); 11. └─lifecycle::deprecate_stop(""0.3.0"", ""pal_gradient_n(space = 'only supports be \""Lab\""')""); 12. └─lifecycle:::deprecate_stop0(msg); 13. └─rlang::cnd_signal(...); Execution halted; $ R; > install.packages(""remot",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8664
https://github.com/broadinstitute/gatk/issues/8664:2472,Safety,abort,abort,2472,"ab"" as of scales 0.3.0. This parameter is used repeatedly in the generated R-script via. ```R; scale_fill_gradient(high=""green"", low=""red"", space=""rgb""); ```. #### Steps to reproduce. ```shell; $ R --version; R version 4.1.2 (2021-11-01) -- ""Bird Hippie""; $ rm -rf ~/R; $ R; > install.packages(""ggplot2"", repos=""https://cloud.r-project.org/""); > packageVersion(""scales""); [1] ‘1.3.0’; > quit(); $ gatk --version; The Genome Analysis Toolkit (GATK) v4.5.0.0; HTSJDK Version: 4.1.0; Picard Version: 3.1.1; $ gatk VariantRecalibrator [arguments omitted for brevity]; org.broadinstitute.hellbender.utils.R.RScriptExecutorException: ; Rscript exited with 1; Command Line: Rscript -e tempLibDir = '/tmp/Rlib.9339186078473502558';source('/path/to/rscript.r');; Stdout: ; Stderr: Error:; ! The `space` argument of `pal_gradient_n()` only supports be ""Lab"" as; of scales 0.3.0.; Backtrace:; ▆; 1. ├─base::source(""/path/to/rscript.r""); 2. │ ├─base::withVisible(eval(ei, envir)); 3. │ └─base::eval(ei, envir); 4. │ └─base::eval(ei, envir); 5. └─ggplot2::scale_fill_gradient(high = ""green"", low = ""red"", space = ""rgb""); 6. ├─ggplot2::continuous_scale(...); 7. │ └─ggplot2::ggproto(...); 8. │ └─rlang::list2(...); 9. └─scales::seq_gradient_pal(low, high, space); 10. └─scales::pal_gradient_n(c(low, high), space = space); 11. └─lifecycle::deprecate_stop(""0.3.0"", ""pal_gradient_n(space = 'only supports be \""Lab\""')""); 12. └─lifecycle:::deprecate_stop0(msg); 13. └─rlang::cnd_signal(...); Execution halted; $ R; > install.packages(""remotes"", repos=""https://cloud.r-project.org/""); > library(remotes); > install_version(""scales"", version=""1.2.1"", repos=""https://cloud.r-project.org/""); > packageVersion(""scales""); [1] ‘1.2.1’; > quit(); $ gatk VariantRecalibrator [arguments omitted for brevity]; $; ```. #### Expected behavior; The output rscript file is used to generate a PDF. #### Actual behavior; Generation of the PDF fails due to an deprecation in the `scales` library causing the `Rscript` command to abort.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8664
https://github.com/broadinstitute/gatk/issues/8664:185,Testability,test,test,185,"## Bug Report. ### Affected tool(s) or class(es); VariantRecalibrator. ### Affected version(s); - [X] Latest public release version [4.5.0.0]; - [ ] Latest master branch as of [date of test?]. ### Description ; As of v1.3.0 the `scales` R package turns the use of deprecated values for the `space` parameter into a hard error, resulting in the VariantRecalibrator R-script terminating with the following message:. > The `space` argument of `pal_gradient_n()` only supports be ""Lab"" as of scales 0.3.0. This parameter is used repeatedly in the generated R-script via. ```R; scale_fill_gradient(high=""green"", low=""red"", space=""rgb""); ```. #### Steps to reproduce. ```shell; $ R --version; R version 4.1.2 (2021-11-01) -- ""Bird Hippie""; $ rm -rf ~/R; $ R; > install.packages(""ggplot2"", repos=""https://cloud.r-project.org/""); > packageVersion(""scales""); [1] ‘1.3.0’; > quit(); $ gatk --version; The Genome Analysis Toolkit (GATK) v4.5.0.0; HTSJDK Version: 4.1.0; Picard Version: 3.1.1; $ gatk VariantRecalibrator [arguments omitted for brevity]; org.broadinstitute.hellbender.utils.R.RScriptExecutorException: ; Rscript exited with 1; Command Line: Rscript -e tempLibDir = '/tmp/Rlib.9339186078473502558';source('/path/to/rscript.r');; Stdout: ; Stderr: Error:; ! The `space` argument of `pal_gradient_n()` only supports be ""Lab"" as; of scales 0.3.0.; Backtrace:; ▆; 1. ├─base::source(""/path/to/rscript.r""); 2. │ ├─base::withVisible(eval(ei, envir)); 3. │ └─base::eval(ei, envir); 4. │ └─base::eval(ei, envir); 5. └─ggplot2::scale_fill_gradient(high = ""green"", low = ""red"", space = ""rgb""); 6. ├─ggplot2::continuous_scale(...); 7. │ └─ggplot2::ggproto(...); 8. │ └─rlang::list2(...); 9. └─scales::seq_gradient_pal(low, high, space); 10. └─scales::pal_gradient_n(c(low, high), space = space); 11. └─lifecycle::deprecate_stop(""0.3.0"", ""pal_gradient_n(space = 'only supports be \""Lab\""')""); 12. └─lifecycle:::deprecate_stop0(msg); 13. └─rlang::cnd_signal(...); Execution halted; $ R; > install.packages(""remot",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8664
https://github.com/broadinstitute/gatk/pull/8665:322,Availability,failure,failures,322,Created VAT here: https://app.terra.bio/#workspaces/gvs-dev/RSA%20-%20GVS%20Quickstart%20V2%20/job_history/b5c03ce6-cc74-462a-b81d-8ea102be314e; Validated VAT here: https://app.terra.bio/#workspaces/gvs-dev/RSA%20-%20GVS%20Quickstart%20V2%20/job_history/e6511960-8340-4e74-8f06-6c1a69d848bd (confirming with Rori that the failures are not surprising given it's only 10 samples),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8665
https://github.com/broadinstitute/gatk/pull/8665:145,Security,Validat,Validated,145,Created VAT here: https://app.terra.bio/#workspaces/gvs-dev/RSA%20-%20GVS%20Quickstart%20V2%20/job_history/b5c03ce6-cc74-462a-b81d-8ea102be314e; Validated VAT here: https://app.terra.bio/#workspaces/gvs-dev/RSA%20-%20GVS%20Quickstart%20V2%20/job_history/e6511960-8340-4e74-8f06-6c1a69d848bd (confirming with Rori that the failures are not surprising given it's only 10 samples),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8665
https://github.com/broadinstitute/gatk/pull/8667:541,Availability,failure,failure,541,"In running and re-running GvsPrepareCallset.wdl, one past run did not use compressed references, so that is always used with call caching is turned on (which it is by default), even though the dataset has reingested compressed references since then. This is the exact scenario that GetBQTableLastModifiedDatetime was created for — database-based tasks that we want to be able to call cache accurately. Integration run here: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/ea2ecb01-f35f-441a-ba08-1e7938da2ebe (single failure is for ExtractFilterTask.GvsCreateFilterSet.BigQuery Query Scanned ""The relative difference between these is 0.0507051, which is greater than the allowed tolerance (0.05)"")",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8667
https://github.com/broadinstitute/gatk/pull/8667:703,Availability,toler,tolerance,703,"In running and re-running GvsPrepareCallset.wdl, one past run did not use compressed references, so that is always used with call caching is turned on (which it is by default), even though the dataset has reingested compressed references since then. This is the exact scenario that GetBQTableLastModifiedDatetime was created for — database-based tasks that we want to be able to call cache accurately. Integration run here: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/ea2ecb01-f35f-441a-ba08-1e7938da2ebe (single failure is for ExtractFilterTask.GvsCreateFilterSet.BigQuery Query Scanned ""The relative difference between these is 0.0507051, which is greater than the allowed tolerance (0.05)"")",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8667
https://github.com/broadinstitute/gatk/pull/8667:402,Deployability,Integrat,Integration,402,"In running and re-running GvsPrepareCallset.wdl, one past run did not use compressed references, so that is always used with call caching is turned on (which it is by default), even though the dataset has reingested compressed references since then. This is the exact scenario that GetBQTableLastModifiedDatetime was created for — database-based tasks that we want to be able to call cache accurately. Integration run here: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/ea2ecb01-f35f-441a-ba08-1e7938da2ebe (single failure is for ExtractFilterTask.GvsCreateFilterSet.BigQuery Query Scanned ""The relative difference between these is 0.0507051, which is greater than the allowed tolerance (0.05)"")",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8667
https://github.com/broadinstitute/gatk/pull/8667:402,Integrability,Integrat,Integration,402,"In running and re-running GvsPrepareCallset.wdl, one past run did not use compressed references, so that is always used with call caching is turned on (which it is by default), even though the dataset has reingested compressed references since then. This is the exact scenario that GetBQTableLastModifiedDatetime was created for — database-based tasks that we want to be able to call cache accurately. Integration run here: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/ea2ecb01-f35f-441a-ba08-1e7938da2ebe (single failure is for ExtractFilterTask.GvsCreateFilterSet.BigQuery Query Scanned ""The relative difference between these is 0.0507051, which is greater than the allowed tolerance (0.05)"")",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8667
https://github.com/broadinstitute/gatk/pull/8667:384,Performance,cache,cache,384,"In running and re-running GvsPrepareCallset.wdl, one past run did not use compressed references, so that is always used with call caching is turned on (which it is by default), even though the dataset has reingested compressed references since then. This is the exact scenario that GetBQTableLastModifiedDatetime was created for — database-based tasks that we want to be able to call cache accurately. Integration run here: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/ea2ecb01-f35f-441a-ba08-1e7938da2ebe (single failure is for ExtractFilterTask.GvsCreateFilterSet.BigQuery Query Scanned ""The relative difference between these is 0.0507051, which is greater than the allowed tolerance (0.05)"")",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8667
https://github.com/broadinstitute/gatk/pull/8668:24,Availability,error,error,24,This fixes a small math error in Permutect that was harming precision in tumor-normal mode.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8668
https://github.com/broadinstitute/gatk/pull/8669:0,Testability,Test,Test,0,"Test run: https://app.terra.bio/#workspaces/gvs-dev/RSA%20-%20GVS%20Quickstart%20V2%20/job_history/60e18b74-9d4d-45d1-b62e-34d2afce2d3b (it is expected — I believe — that ClinvarSignificance, SpotCheckForAAChangeAndExonNumberConsistency, and CheckForNullColumns would fail on a 10-sample VAT).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8669
https://github.com/broadinstitute/gatk/pull/8670:168,Security,validat,validation,168,- Successful [VAT creation](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/9503ad5e-676f-4e48-90ac-44022bc4d608); - Reasonably successful [VAT validation](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/9b4dc893-cad5-4204-b40d-3d3f0a541db3)? Could use a check on this,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8670
https://github.com/broadinstitute/gatk/issues/8671:712,Deployability,integrat,integration,712,"## Bug Report. ### Affected tool(s) or class(es); `SAMRecord` from `GATKRead`. ### Affected version(s); - [x] Latest master branch as of January 30, 2024. ### Description ; When I run a tool with a bam file as input, the following code will give me a null:; ```java; @Override; public void apply(GATKRead read, ReferenceContext referenceContext, FeatureContext featureContext) {. // Build sets of read IDs for each file.; final SAMRecord samRecord = read.convertToSAMRecord(getHeaderForReads());; final SAMFileSource fileSource = samRecord.getFileSource();; System.out.println(fileSource);; ```. Output:; (a long list of `null`). #### Steps to reproduce; Create a ReadWalker that takes in a bam file. Here is an integration test that will replicate the issue:. ```java; public class ReadConcordanceIntegrationTest extends CommandLineProgramTest {. @Test; public void testTwoCrams() throws IOException {; final File output = createTempFile(""testReadConcordanceOutputFile"", "".txt"");; final File input = new File(GATKBaseTest.largeFileTestDir, ""expected.K-562.splitNCigarReads.chr20.bam"");. final ArgumentsBuilder args = new ArgumentsBuilder();. args.addInput(input);; this.runCommandLine(args.getArgsArray());; }; }; ```. #### Expected behavior; Output should be the file used in the read data source (bam file) for each read. #### Actual behavior; I get nulls instead",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8671
https://github.com/broadinstitute/gatk/issues/8671:712,Integrability,integrat,integration,712,"## Bug Report. ### Affected tool(s) or class(es); `SAMRecord` from `GATKRead`. ### Affected version(s); - [x] Latest master branch as of January 30, 2024. ### Description ; When I run a tool with a bam file as input, the following code will give me a null:; ```java; @Override; public void apply(GATKRead read, ReferenceContext referenceContext, FeatureContext featureContext) {. // Build sets of read IDs for each file.; final SAMRecord samRecord = read.convertToSAMRecord(getHeaderForReads());; final SAMFileSource fileSource = samRecord.getFileSource();; System.out.println(fileSource);; ```. Output:; (a long list of `null`). #### Steps to reproduce; Create a ReadWalker that takes in a bam file. Here is an integration test that will replicate the issue:. ```java; public class ReadConcordanceIntegrationTest extends CommandLineProgramTest {. @Test; public void testTwoCrams() throws IOException {; final File output = createTempFile(""testReadConcordanceOutputFile"", "".txt"");; final File input = new File(GATKBaseTest.largeFileTestDir, ""expected.K-562.splitNCigarReads.chr20.bam"");. final ArgumentsBuilder args = new ArgumentsBuilder();. args.addInput(input);; this.runCommandLine(args.getArgsArray());; }; }; ```. #### Expected behavior; Output should be the file used in the read data source (bam file) for each read. #### Actual behavior; I get nulls instead",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8671
https://github.com/broadinstitute/gatk/issues/8671:814,Modifiability,extend,extends,814,"## Bug Report. ### Affected tool(s) or class(es); `SAMRecord` from `GATKRead`. ### Affected version(s); - [x] Latest master branch as of January 30, 2024. ### Description ; When I run a tool with a bam file as input, the following code will give me a null:; ```java; @Override; public void apply(GATKRead read, ReferenceContext referenceContext, FeatureContext featureContext) {. // Build sets of read IDs for each file.; final SAMRecord samRecord = read.convertToSAMRecord(getHeaderForReads());; final SAMFileSource fileSource = samRecord.getFileSource();; System.out.println(fileSource);; ```. Output:; (a long list of `null`). #### Steps to reproduce; Create a ReadWalker that takes in a bam file. Here is an integration test that will replicate the issue:. ```java; public class ReadConcordanceIntegrationTest extends CommandLineProgramTest {. @Test; public void testTwoCrams() throws IOException {; final File output = createTempFile(""testReadConcordanceOutputFile"", "".txt"");; final File input = new File(GATKBaseTest.largeFileTestDir, ""expected.K-562.splitNCigarReads.chr20.bam"");. final ArgumentsBuilder args = new ArgumentsBuilder();. args.addInput(input);; this.runCommandLine(args.getArgsArray());; }; }; ```. #### Expected behavior; Output should be the file used in the read data source (bam file) for each read. #### Actual behavior; I get nulls instead",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8671
https://github.com/broadinstitute/gatk/issues/8671:724,Testability,test,test,724,"## Bug Report. ### Affected tool(s) or class(es); `SAMRecord` from `GATKRead`. ### Affected version(s); - [x] Latest master branch as of January 30, 2024. ### Description ; When I run a tool with a bam file as input, the following code will give me a null:; ```java; @Override; public void apply(GATKRead read, ReferenceContext referenceContext, FeatureContext featureContext) {. // Build sets of read IDs for each file.; final SAMRecord samRecord = read.convertToSAMRecord(getHeaderForReads());; final SAMFileSource fileSource = samRecord.getFileSource();; System.out.println(fileSource);; ```. Output:; (a long list of `null`). #### Steps to reproduce; Create a ReadWalker that takes in a bam file. Here is an integration test that will replicate the issue:. ```java; public class ReadConcordanceIntegrationTest extends CommandLineProgramTest {. @Test; public void testTwoCrams() throws IOException {; final File output = createTempFile(""testReadConcordanceOutputFile"", "".txt"");; final File input = new File(GATKBaseTest.largeFileTestDir, ""expected.K-562.splitNCigarReads.chr20.bam"");. final ArgumentsBuilder args = new ArgumentsBuilder();. args.addInput(input);; this.runCommandLine(args.getArgsArray());; }; }; ```. #### Expected behavior; Output should be the file used in the read data source (bam file) for each read. #### Actual behavior; I get nulls instead",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8671
https://github.com/broadinstitute/gatk/issues/8671:849,Testability,Test,Test,849,"## Bug Report. ### Affected tool(s) or class(es); `SAMRecord` from `GATKRead`. ### Affected version(s); - [x] Latest master branch as of January 30, 2024. ### Description ; When I run a tool with a bam file as input, the following code will give me a null:; ```java; @Override; public void apply(GATKRead read, ReferenceContext referenceContext, FeatureContext featureContext) {. // Build sets of read IDs for each file.; final SAMRecord samRecord = read.convertToSAMRecord(getHeaderForReads());; final SAMFileSource fileSource = samRecord.getFileSource();; System.out.println(fileSource);; ```. Output:; (a long list of `null`). #### Steps to reproduce; Create a ReadWalker that takes in a bam file. Here is an integration test that will replicate the issue:. ```java; public class ReadConcordanceIntegrationTest extends CommandLineProgramTest {. @Test; public void testTwoCrams() throws IOException {; final File output = createTempFile(""testReadConcordanceOutputFile"", "".txt"");; final File input = new File(GATKBaseTest.largeFileTestDir, ""expected.K-562.splitNCigarReads.chr20.bam"");. final ArgumentsBuilder args = new ArgumentsBuilder();. args.addInput(input);; this.runCommandLine(args.getArgsArray());; }; }; ```. #### Expected behavior; Output should be the file used in the read data source (bam file) for each read. #### Actual behavior; I get nulls instead",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8671
https://github.com/broadinstitute/gatk/issues/8671:867,Testability,test,testTwoCrams,867,"## Bug Report. ### Affected tool(s) or class(es); `SAMRecord` from `GATKRead`. ### Affected version(s); - [x] Latest master branch as of January 30, 2024. ### Description ; When I run a tool with a bam file as input, the following code will give me a null:; ```java; @Override; public void apply(GATKRead read, ReferenceContext referenceContext, FeatureContext featureContext) {. // Build sets of read IDs for each file.; final SAMRecord samRecord = read.convertToSAMRecord(getHeaderForReads());; final SAMFileSource fileSource = samRecord.getFileSource();; System.out.println(fileSource);; ```. Output:; (a long list of `null`). #### Steps to reproduce; Create a ReadWalker that takes in a bam file. Here is an integration test that will replicate the issue:. ```java; public class ReadConcordanceIntegrationTest extends CommandLineProgramTest {. @Test; public void testTwoCrams() throws IOException {; final File output = createTempFile(""testReadConcordanceOutputFile"", "".txt"");; final File input = new File(GATKBaseTest.largeFileTestDir, ""expected.K-562.splitNCigarReads.chr20.bam"");. final ArgumentsBuilder args = new ArgumentsBuilder();. args.addInput(input);; this.runCommandLine(args.getArgsArray());; }; }; ```. #### Expected behavior; Output should be the file used in the read data source (bam file) for each read. #### Actual behavior; I get nulls instead",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8671
https://github.com/broadinstitute/gatk/issues/8671:940,Testability,test,testReadConcordanceOutputFile,940,"## Bug Report. ### Affected tool(s) or class(es); `SAMRecord` from `GATKRead`. ### Affected version(s); - [x] Latest master branch as of January 30, 2024. ### Description ; When I run a tool with a bam file as input, the following code will give me a null:; ```java; @Override; public void apply(GATKRead read, ReferenceContext referenceContext, FeatureContext featureContext) {. // Build sets of read IDs for each file.; final SAMRecord samRecord = read.convertToSAMRecord(getHeaderForReads());; final SAMFileSource fileSource = samRecord.getFileSource();; System.out.println(fileSource);; ```. Output:; (a long list of `null`). #### Steps to reproduce; Create a ReadWalker that takes in a bam file. Here is an integration test that will replicate the issue:. ```java; public class ReadConcordanceIntegrationTest extends CommandLineProgramTest {. @Test; public void testTwoCrams() throws IOException {; final File output = createTempFile(""testReadConcordanceOutputFile"", "".txt"");; final File input = new File(GATKBaseTest.largeFileTestDir, ""expected.K-562.splitNCigarReads.chr20.bam"");. final ArgumentsBuilder args = new ArgumentsBuilder();. args.addInput(input);; this.runCommandLine(args.getArgsArray());; }; }; ```. #### Expected behavior; Output should be the file used in the read data source (bam file) for each read. #### Actual behavior; I get nulls instead",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8671
https://github.com/broadinstitute/gatk/pull/8672:1747,Availability,error,error,1747,"in conjunction with a locally modified version of gatk, to communicate with aws. Since we had the code that allows for communication with aws anyway, we decided to share it and maybe it can be part of the gatk toolkit in the future. # How does it work?; The user is able to provide an additional parameter '--s3', adding the nio-spi-for-s3-2.0.0-dev-all.jar file to the java classpath. File locations starting with 's3://' are then able to be provided, resulting of reading/writing of these files to aws. When using this option, however, the aws credentials have to be set correctly, for which you can find more information [here](https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/credentials.html). Currently, I haven't implemented it for --spark due to a lack of need/inexperience with spark. # Current Issues; We found some issues for which we do not know any solution. If this tool was to be implemented in GATK in the future, these have to be resolved eventually. ## Doesn't work for picard-based tools; First, 'aws-java-nio-spi-for-s3' doesn't seem work for (most) picard tools, since most of them utilise the java.io.File package, which is limited to local filesystem files, as opposed to java.nio.Path (we think).; ## Issues reading genome reference files from AWS; Secondy, most tools that require a reference genome (i.e. BaseRecalibrator, HaplotypeCaller..) do not seem function when provided with a reference genome file stored on AWS. The error we receive can be found underneath and is much less clear. We believe that the issue lies in the interaction between the caching of the indexed reference file and 'aws-java-nio-spi-for-s3', since we tested in a custom java script that the package 'htsjdk' works like intended when the reference genome is read from AWS.; Notably, some tools do not have this issue, such as the the vqsr tools (VariantRecalibrator and applyVQSR).; ![image](https://github.com/broadinstitute/gatk/assets/149685151/24d16941-b40c-4a5a-b8e6-0dc7415c6b1b)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8672
https://github.com/broadinstitute/gatk/pull/8672:1952,Testability,test,tested,1952,"in conjunction with a locally modified version of gatk, to communicate with aws. Since we had the code that allows for communication with aws anyway, we decided to share it and maybe it can be part of the gatk toolkit in the future. # How does it work?; The user is able to provide an additional parameter '--s3', adding the nio-spi-for-s3-2.0.0-dev-all.jar file to the java classpath. File locations starting with 's3://' are then able to be provided, resulting of reading/writing of these files to aws. When using this option, however, the aws credentials have to be set correctly, for which you can find more information [here](https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/credentials.html). Currently, I haven't implemented it for --spark due to a lack of need/inexperience with spark. # Current Issues; We found some issues for which we do not know any solution. If this tool was to be implemented in GATK in the future, these have to be resolved eventually. ## Doesn't work for picard-based tools; First, 'aws-java-nio-spi-for-s3' doesn't seem work for (most) picard tools, since most of them utilise the java.io.File package, which is limited to local filesystem files, as opposed to java.nio.Path (we think).; ## Issues reading genome reference files from AWS; Secondy, most tools that require a reference genome (i.e. BaseRecalibrator, HaplotypeCaller..) do not seem function when provided with a reference genome file stored on AWS. The error we receive can be found underneath and is much less clear. We believe that the issue lies in the interaction between the caching of the indexed reference file and 'aws-java-nio-spi-for-s3', since we tested in a custom java script that the package 'htsjdk' works like intended when the reference genome is read from AWS.; Notably, some tools do not have this issue, such as the the vqsr tools (VariantRecalibrator and applyVQSR).; ![image](https://github.com/broadinstitute/gatk/assets/149685151/24d16941-b40c-4a5a-b8e6-0dc7415c6b1b)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8672
https://github.com/broadinstitute/gatk/pull/8672:970,Usability,guid,guide,970,"# Introduction; Recently, Amazon has created the tool [aws-java-nio-spi-for-s3](https://github.com/awslabs/aws-java-nio-spi-for-s3) that allows java-based applications to read and/or write to aws without the need for recompilation during runtime. Since then, we've utilised this tool, in conjunction with a locally modified version of gatk, to communicate with aws. Since we had the code that allows for communication with aws anyway, we decided to share it and maybe it can be part of the gatk toolkit in the future. # How does it work?; The user is able to provide an additional parameter '--s3', adding the nio-spi-for-s3-2.0.0-dev-all.jar file to the java classpath. File locations starting with 's3://' are then able to be provided, resulting of reading/writing of these files to aws. When using this option, however, the aws credentials have to be set correctly, for which you can find more information [here](https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/credentials.html). Currently, I haven't implemented it for --spark due to a lack of need/inexperience with spark. # Current Issues; We found some issues for which we do not know any solution. If this tool was to be implemented in GATK in the future, these have to be resolved eventually. ## Doesn't work for picard-based tools; First, 'aws-java-nio-spi-for-s3' doesn't seem work for (most) picard tools, since most of them utilise the java.io.File package, which is limited to local filesystem files, as opposed to java.nio.Path (we think).; ## Issues reading genome reference files from AWS; Secondy, most tools that require a reference genome (i.e. BaseRecalibrator, HaplotypeCaller..) do not seem function when provided with a reference genome file stored on AWS. The error we receive can be found underneath and is much less clear. We believe that the issue lies in the interaction between the caching of the indexed reference file and 'aws-java-nio-spi-for-s3', since we tested in a custom java script that the package '",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8672
https://github.com/broadinstitute/gatk/pull/8672:1805,Usability,clear,clear,1805,"in conjunction with a locally modified version of gatk, to communicate with aws. Since we had the code that allows for communication with aws anyway, we decided to share it and maybe it can be part of the gatk toolkit in the future. # How does it work?; The user is able to provide an additional parameter '--s3', adding the nio-spi-for-s3-2.0.0-dev-all.jar file to the java classpath. File locations starting with 's3://' are then able to be provided, resulting of reading/writing of these files to aws. When using this option, however, the aws credentials have to be set correctly, for which you can find more information [here](https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/credentials.html). Currently, I haven't implemented it for --spark due to a lack of need/inexperience with spark. # Current Issues; We found some issues for which we do not know any solution. If this tool was to be implemented in GATK in the future, these have to be resolved eventually. ## Doesn't work for picard-based tools; First, 'aws-java-nio-spi-for-s3' doesn't seem work for (most) picard tools, since most of them utilise the java.io.File package, which is limited to local filesystem files, as opposed to java.nio.Path (we think).; ## Issues reading genome reference files from AWS; Secondy, most tools that require a reference genome (i.e. BaseRecalibrator, HaplotypeCaller..) do not seem function when provided with a reference genome file stored on AWS. The error we receive can be found underneath and is much less clear. We believe that the issue lies in the interaction between the caching of the indexed reference file and 'aws-java-nio-spi-for-s3', since we tested in a custom java script that the package 'htsjdk' works like intended when the reference genome is read from AWS.; Notably, some tools do not have this issue, such as the the vqsr tools (VariantRecalibrator and applyVQSR).; ![image](https://github.com/broadinstitute/gatk/assets/149685151/24d16941-b40c-4a5a-b8e6-0dc7415c6b1b)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8672
https://github.com/broadinstitute/gatk/pull/8673:126,Testability,Test,Test,126,- separates sample Avro output from other non-partitioned (filter tables) so that they can be in groups of 4000 samples each. Test run here: https://app.terra.bio/#workspaces/gvs-dev/RSA%20-%20GVS%20Quickstart%20V2%20/job_history/30321bb4-614b-4678-a124-12385deee37a,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8673
https://github.com/broadinstitute/gatk/pull/8674:0,Deployability,Integrat,Integration,0,"Integration run [in progress](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/544fb86b-ffb7-447c-b380-fdefce10be99). Does away with the `STARTED` and `FINISHED` sample load statuses to more explicitly record what work has actually been done for a sample: `REFERENCES_LOADED`, `VARIANTS_LOADED` or `HEADERS_LOADED`. Legacy `FINISHED` and `STARTED` states are recognized and handled appropriately (short circuiting data load and being ignored, respectively).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8674
https://github.com/broadinstitute/gatk/pull/8674:0,Integrability,Integrat,Integration,0,"Integration run [in progress](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/544fb86b-ffb7-447c-b380-fdefce10be99). Does away with the `STARTED` and `FINISHED` sample load statuses to more explicitly record what work has actually been done for a sample: `REFERENCES_LOADED`, `VARIANTS_LOADED` or `HEADERS_LOADED`. Legacy `FINISHED` and `STARTED` states are recognized and handled appropriately (short circuiting data load and being ignored, respectively).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8674
https://github.com/broadinstitute/gatk/pull/8674:192,Performance,load,load,192,"Integration run [in progress](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/544fb86b-ffb7-447c-b380-fdefce10be99). Does away with the `STARTED` and `FINISHED` sample load statuses to more explicitly record what work has actually been done for a sample: `REFERENCES_LOADED`, `VARIANTS_LOADED` or `HEADERS_LOADED`. Legacy `FINISHED` and `STARTED` states are recognized and handled appropriately (short circuiting data load and being ignored, respectively).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8674
https://github.com/broadinstitute/gatk/pull/8674:442,Performance,load,load,442,"Integration run [in progress](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/544fb86b-ffb7-447c-b380-fdefce10be99). Does away with the `STARTED` and `FINISHED` sample load statuses to more explicitly record what work has actually been done for a sample: `REFERENCES_LOADED`, `VARIANTS_LOADED` or `HEADERS_LOADED`. Legacy `FINISHED` and `STARTED` states are recognized and handled appropriately (short circuiting data load and being ignored, respectively).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8674
https://github.com/broadinstitute/gatk/pull/8676:1149,Energy Efficiency,reduce,reduces,1149,"A matrix table is a dense matrix. GVS's refs and vets tables are sparse matrices, specifically using the [coordinate-list; representation](https://en.wikipedia.org/wiki/Sparse_matrix#Coordinate_list_(COO)). For example, the following coordinate-list representation:. ```; row, col, value; 0, 2, 42.0; 0, 3, 48.0; 0, 5, 43.0; 1, 0, 41.0; 1, 1, 45.0; 1, 3, 43.0; ```. corresponds to this dense representation; ```; NA NA 42.0 48.0 NA 43.0; 41.0 45.0 NA 43.0 NA NA; ```. I must read the entire coordinate-list representation to conclude this matrix has five columns. Moreover, in vets and refs, instead of column indices, we have unique column identifiers [1], so we even lack the index of each column. In a single-threaded, single-machine world, we could assign identifiers to indices as we scanned the refs/vets from top to bottom. However, when processing the refs and vets in parallel, we must know the identifier-to-index mapping before execution. The current implementation reads the entire refs Avro to discover the present sample identifiers. The new implementation uses a new `sample_info` Avro file which contains one record per sample. This reduces the complexity of assigning samples to indices from $O(N_{SAMPLES} N_{REFS})$ to $O(N_{SAMPLES})$. In practice, this is a substantial reduction because the genomic axis is much larger than 4000, the typical group size. [1] In practice, currently, the column identifiers are a dense interval of the global column; indices. Concretely: sample group one contains only the samples 0 through 3999. Sample group two; contains only the samples 4000 through 7999. We do not use this fact in import_gvs.py.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8676
https://github.com/broadinstitute/gatk/pull/8677:177,Performance,perform,performs,177,"The GVS VQSR filters, site filters, refs, and vets tables are sorted by locus by design. The current implementation of `import_gvs.py` does not use this assumption. Instead, it performs a distributed sort of all four (sorted) tables. This PR uses the `_key_by_assert_sorted` escape hatch to replace a distributed sort with run-time verification of within- and between- partition sortedness.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8677
https://github.com/broadinstitute/gatk/pull/8678:313,Security,validat,validate,313,- run to create the VDS: https://app.terra.bio/#workspaces/gvs-dev/RSA%20-%20GVS%20Quickstart%20V2%20/job_history/6af5e497-7ee2-48de-9c35-79a586d7d0eb; - run to create the VAT: https://app.terra.bio/#workspaces/gvs-dev/RSA%20-%20GVS%20Quickstart%20V2%20/job_history/0c11d926-a0cc-40f7-83fd-ee37c69b67f5; - run to validate the VAT: https://app.terra.bio/#workspaces/gvs-dev/RSA%20-%20GVS%20Quickstart%20V2%20/job_history/926767be-fd29-4696-b341-bf9592d9e6e1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8678
https://github.com/broadinstitute/gatk/pull/8679:13,Deployability,update,update,13,"A very quick update to our AoU docs to mention the ticket for deletion. Note: yes, this branch is incorrectly named because it's VS-1206 and not VS-1207. It wasn't important enough to rename the branch for its short lifetime",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8679
https://github.com/broadinstitute/gatk/issues/8681:110,Performance,perform,performing,110,"Hi GATK, I have done the steps, i.e., 1)haplotypecaller 2)combineGVCF 3) GenotypeGVCF. Now I am interested in performing the allele matching between two samples, like whether the alleles are the same or different at a particular chromosomal location. Does GATK offer to perform this test? if yes, then which function in GATK will be preferable?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8681
https://github.com/broadinstitute/gatk/issues/8681:270,Performance,perform,perform,270,"Hi GATK, I have done the steps, i.e., 1)haplotypecaller 2)combineGVCF 3) GenotypeGVCF. Now I am interested in performing the allele matching between two samples, like whether the alleles are the same or different at a particular chromosomal location. Does GATK offer to perform this test? if yes, then which function in GATK will be preferable?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8681
https://github.com/broadinstitute/gatk/issues/8681:283,Testability,test,test,283,"Hi GATK, I have done the steps, i.e., 1)haplotypecaller 2)combineGVCF 3) GenotypeGVCF. Now I am interested in performing the allele matching between two samples, like whether the alleles are the same or different at a particular chromosomal location. Does GATK offer to perform this test? if yes, then which function in GATK will be preferable?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8681
https://github.com/broadinstitute/gatk/issues/8683:737,Availability,error,error,737,"----. ## Bug Report. ### Affected tool(s) or class(es); - Tool/class name(s), special parameters: GenomicsDBImport. ### Affected version(s); - Version: gatk4-4.4.0.0-0. ### Description ; Hello,. I have been having an issue come up when utilizing `GenomicsDBImport`. This issue has happened when using a range of samples and shard counts (8 - 1000 samples, shard count of up to 2000). My current example is an attempt to joint call 1000 samples together. I will submit the jobs and 1-2 of the shards (of the ~100 concurrently running) will throw a `malloc(): unaligned tcache chunk detected`. When I resubmit that shard, it will usually rerun without a problem. Or if I kill all jobs and resubmit, a different shard will throw the malloc error. . I have run approximately 20 tests and I seem to get this failure 2/3 times. However, it only arises on the initial submission and not when additional jobs are submitted as previous shards complete. Please note that the 1000 samples have successfully been imported into the GenomicsDB but this error seems to persist somewhat randomly across multiple machines. . Thank you for your assistance! . #### Steps to reproduce. - Command used (omitting paths to 1000 samples for brevity) for one of the failed shards. ```; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx8g -jar /gpfs/gpfs_de6000/home/dalegre/miniconda3/envs/GOASTv4.0/share/gatk4-4.4.0.0-0/gatk-package-4.4.0.0-local.jar GenomicsDBImport -V [samples 1-1002] --genomicsdb-workspace-path results/jointcalling/genomicsDB/temp_0882_of_2000_DB --merge-input-intervals false --bypass-feature-reader --tmp-dir temp --max-num-intervals-to-import-in-parallel 10 --batch-size 50 --intervals results/germline/interval/temp_0882_of_2000/scattered.interval_list --genomicsdb-shared-posixfs-optimizations true; ```. #### Expected behavior; All shards are imported into the GenomicsDB successfu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8683
https://github.com/broadinstitute/gatk/issues/8683:803,Availability,failure,failure,803,"----. ## Bug Report. ### Affected tool(s) or class(es); - Tool/class name(s), special parameters: GenomicsDBImport. ### Affected version(s); - Version: gatk4-4.4.0.0-0. ### Description ; Hello,. I have been having an issue come up when utilizing `GenomicsDBImport`. This issue has happened when using a range of samples and shard counts (8 - 1000 samples, shard count of up to 2000). My current example is an attempt to joint call 1000 samples together. I will submit the jobs and 1-2 of the shards (of the ~100 concurrently running) will throw a `malloc(): unaligned tcache chunk detected`. When I resubmit that shard, it will usually rerun without a problem. Or if I kill all jobs and resubmit, a different shard will throw the malloc error. . I have run approximately 20 tests and I seem to get this failure 2/3 times. However, it only arises on the initial submission and not when additional jobs are submitted as previous shards complete. Please note that the 1000 samples have successfully been imported into the GenomicsDB but this error seems to persist somewhat randomly across multiple machines. . Thank you for your assistance! . #### Steps to reproduce. - Command used (omitting paths to 1000 samples for brevity) for one of the failed shards. ```; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx8g -jar /gpfs/gpfs_de6000/home/dalegre/miniconda3/envs/GOASTv4.0/share/gatk4-4.4.0.0-0/gatk-package-4.4.0.0-local.jar GenomicsDBImport -V [samples 1-1002] --genomicsdb-workspace-path results/jointcalling/genomicsDB/temp_0882_of_2000_DB --merge-input-intervals false --bypass-feature-reader --tmp-dir temp --max-num-intervals-to-import-in-parallel 10 --batch-size 50 --intervals results/germline/interval/temp_0882_of_2000/scattered.interval_list --genomicsdb-shared-posixfs-optimizations true; ```. #### Expected behavior; All shards are imported into the GenomicsDB successfu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8683
https://github.com/broadinstitute/gatk/issues/8683:1039,Availability,error,error,1039," Affected tool(s) or class(es); - Tool/class name(s), special parameters: GenomicsDBImport. ### Affected version(s); - Version: gatk4-4.4.0.0-0. ### Description ; Hello,. I have been having an issue come up when utilizing `GenomicsDBImport`. This issue has happened when using a range of samples and shard counts (8 - 1000 samples, shard count of up to 2000). My current example is an attempt to joint call 1000 samples together. I will submit the jobs and 1-2 of the shards (of the ~100 concurrently running) will throw a `malloc(): unaligned tcache chunk detected`. When I resubmit that shard, it will usually rerun without a problem. Or if I kill all jobs and resubmit, a different shard will throw the malloc error. . I have run approximately 20 tests and I seem to get this failure 2/3 times. However, it only arises on the initial submission and not when additional jobs are submitted as previous shards complete. Please note that the 1000 samples have successfully been imported into the GenomicsDB but this error seems to persist somewhat randomly across multiple machines. . Thank you for your assistance! . #### Steps to reproduce. - Command used (omitting paths to 1000 samples for brevity) for one of the failed shards. ```; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx8g -jar /gpfs/gpfs_de6000/home/dalegre/miniconda3/envs/GOASTv4.0/share/gatk4-4.4.0.0-0/gatk-package-4.4.0.0-local.jar GenomicsDBImport -V [samples 1-1002] --genomicsdb-workspace-path results/jointcalling/genomicsDB/temp_0882_of_2000_DB --merge-input-intervals false --bypass-feature-reader --tmp-dir temp --max-num-intervals-to-import-in-parallel 10 --batch-size 50 --intervals results/germline/interval/temp_0882_of_2000/scattered.interval_list --genomicsdb-shared-posixfs-optimizations true; ```. #### Expected behavior; All shards are imported into the GenomicsDB successfully. . #### Actual beha",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8683
https://github.com/broadinstitute/gatk/issues/8683:2081,Availability,error,error,2081,"domly across multiple machines. . Thank you for your assistance! . #### Steps to reproduce. - Command used (omitting paths to 1000 samples for brevity) for one of the failed shards. ```; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx8g -jar /gpfs/gpfs_de6000/home/dalegre/miniconda3/envs/GOASTv4.0/share/gatk4-4.4.0.0-0/gatk-package-4.4.0.0-local.jar GenomicsDBImport -V [samples 1-1002] --genomicsdb-workspace-path results/jointcalling/genomicsDB/temp_0882_of_2000_DB --merge-input-intervals false --bypass-feature-reader --tmp-dir temp --max-num-intervals-to-import-in-parallel 10 --batch-size 50 --intervals results/germline/interval/temp_0882_of_2000/scattered.interval_list --genomicsdb-shared-posixfs-optimizations true; ```. #### Expected behavior; All shards are imported into the GenomicsDB successfully. . #### Actual behavior; _Tell us what happens instead_. job dies with this error:. `malloc(): unaligned tcache chunk detected`. ```; 23:45:26.793 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gpfs/gpfs_de6000/home/dalegre/miniconda3/e; nvs/GOASTv4.0/share/gatk4-4.4.0.0-0/gatk-package-4.4.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 23:45:26.822 INFO GenomicsDBImport - ------------------------------------------------------------; 23:45:26.824 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.4.0.0; 23:45:26.824 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 23:45:26.824 INFO GenomicsDBImport - Executing as dalegre@amd4103.hpc.eu.lenovo.com on Linux v5.14.0-284.11.1.el9_2.x86_64 amd6; 4; 23:45:26.824 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v17.0.3-internal+0-adhoc..src; 23:45:26.824 INFO GenomicsDBImport - Start Date/Time: February 6, 2024 at 11:45:26 PM CET; 23:45:26.824 INFO GenomicsDBImport - -------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8683
https://github.com/broadinstitute/gatk/issues/8683:512,Performance,concurren,concurrently,512,"----. ## Bug Report. ### Affected tool(s) or class(es); - Tool/class name(s), special parameters: GenomicsDBImport. ### Affected version(s); - Version: gatk4-4.4.0.0-0. ### Description ; Hello,. I have been having an issue come up when utilizing `GenomicsDBImport`. This issue has happened when using a range of samples and shard counts (8 - 1000 samples, shard count of up to 2000). My current example is an attempt to joint call 1000 samples together. I will submit the jobs and 1-2 of the shards (of the ~100 concurrently running) will throw a `malloc(): unaligned tcache chunk detected`. When I resubmit that shard, it will usually rerun without a problem. Or if I kill all jobs and resubmit, a different shard will throw the malloc error. . I have run approximately 20 tests and I seem to get this failure 2/3 times. However, it only arises on the initial submission and not when additional jobs are submitted as previous shards complete. Please note that the 1000 samples have successfully been imported into the GenomicsDB but this error seems to persist somewhat randomly across multiple machines. . Thank you for your assistance! . #### Steps to reproduce. - Command used (omitting paths to 1000 samples for brevity) for one of the failed shards. ```; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx8g -jar /gpfs/gpfs_de6000/home/dalegre/miniconda3/envs/GOASTv4.0/share/gatk4-4.4.0.0-0/gatk-package-4.4.0.0-local.jar GenomicsDBImport -V [samples 1-1002] --genomicsdb-workspace-path results/jointcalling/genomicsDB/temp_0882_of_2000_DB --merge-input-intervals false --bypass-feature-reader --tmp-dir temp --max-num-intervals-to-import-in-parallel 10 --batch-size 50 --intervals results/germline/interval/temp_0882_of_2000/scattered.interval_list --genomicsdb-shared-posixfs-optimizations true; ```. #### Expected behavior; All shards are imported into the GenomicsDB successfu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8683
https://github.com/broadinstitute/gatk/issues/8683:1899,Performance,optimiz,optimizations,1899,ional jobs are submitted as previous shards complete. Please note that the 1000 samples have successfully been imported into the GenomicsDB but this error seems to persist somewhat randomly across multiple machines. . Thank you for your assistance! . #### Steps to reproduce. - Command used (omitting paths to 1000 samples for brevity) for one of the failed shards. ```; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx8g -jar /gpfs/gpfs_de6000/home/dalegre/miniconda3/envs/GOASTv4.0/share/gatk4-4.4.0.0-0/gatk-package-4.4.0.0-local.jar GenomicsDBImport -V [samples 1-1002] --genomicsdb-workspace-path results/jointcalling/genomicsDB/temp_0882_of_2000_DB --merge-input-intervals false --bypass-feature-reader --tmp-dir temp --max-num-intervals-to-import-in-parallel 10 --batch-size 50 --intervals results/germline/interval/temp_0882_of_2000/scattered.interval_list --genomicsdb-shared-posixfs-optimizations true; ```. #### Expected behavior; All shards are imported into the GenomicsDB successfully. . #### Actual behavior; _Tell us what happens instead_. job dies with this error:. `malloc(): unaligned tcache chunk detected`. ```; 23:45:26.793 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gpfs/gpfs_de6000/home/dalegre/miniconda3/e; nvs/GOASTv4.0/share/gatk4-4.4.0.0-0/gatk-package-4.4.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 23:45:26.822 INFO GenomicsDBImport - ------------------------------------------------------------; 23:45:26.824 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.4.0.0; 23:45:26.824 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 23:45:26.824 INFO GenomicsDBImport - Executing as dalegre@amd4103.hpc.eu.lenovo.com on Linux v5.14.0-284.11.1.el9_2.x86_64 amd6; 4; 23:45:26.824 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v17.0,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8683
https://github.com/broadinstitute/gatk/issues/8683:2179,Performance,Load,Loading,2179,"sed (omitting paths to 1000 samples for brevity) for one of the failed shards. ```; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx8g -jar /gpfs/gpfs_de6000/home/dalegre/miniconda3/envs/GOASTv4.0/share/gatk4-4.4.0.0-0/gatk-package-4.4.0.0-local.jar GenomicsDBImport -V [samples 1-1002] --genomicsdb-workspace-path results/jointcalling/genomicsDB/temp_0882_of_2000_DB --merge-input-intervals false --bypass-feature-reader --tmp-dir temp --max-num-intervals-to-import-in-parallel 10 --batch-size 50 --intervals results/germline/interval/temp_0882_of_2000/scattered.interval_list --genomicsdb-shared-posixfs-optimizations true; ```. #### Expected behavior; All shards are imported into the GenomicsDB successfully. . #### Actual behavior; _Tell us what happens instead_. job dies with this error:. `malloc(): unaligned tcache chunk detected`. ```; 23:45:26.793 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gpfs/gpfs_de6000/home/dalegre/miniconda3/e; nvs/GOASTv4.0/share/gatk4-4.4.0.0-0/gatk-package-4.4.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 23:45:26.822 INFO GenomicsDBImport - ------------------------------------------------------------; 23:45:26.824 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.4.0.0; 23:45:26.824 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 23:45:26.824 INFO GenomicsDBImport - Executing as dalegre@amd4103.hpc.eu.lenovo.com on Linux v5.14.0-284.11.1.el9_2.x86_64 amd6; 4; 23:45:26.824 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v17.0.3-internal+0-adhoc..src; 23:45:26.824 INFO GenomicsDBImport - Start Date/Time: February 6, 2024 at 11:45:26 PM CET; 23:45:26.824 INFO GenomicsDBImport - ------------------------------------------------------------; 23:45:26.824 INFO GenomicsDBImport - ----------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8683
https://github.com/broadinstitute/gatk/issues/8683:581,Safety,detect,detected,581,"----. ## Bug Report. ### Affected tool(s) or class(es); - Tool/class name(s), special parameters: GenomicsDBImport. ### Affected version(s); - Version: gatk4-4.4.0.0-0. ### Description ; Hello,. I have been having an issue come up when utilizing `GenomicsDBImport`. This issue has happened when using a range of samples and shard counts (8 - 1000 samples, shard count of up to 2000). My current example is an attempt to joint call 1000 samples together. I will submit the jobs and 1-2 of the shards (of the ~100 concurrently running) will throw a `malloc(): unaligned tcache chunk detected`. When I resubmit that shard, it will usually rerun without a problem. Or if I kill all jobs and resubmit, a different shard will throw the malloc error. . I have run approximately 20 tests and I seem to get this failure 2/3 times. However, it only arises on the initial submission and not when additional jobs are submitted as previous shards complete. Please note that the 1000 samples have successfully been imported into the GenomicsDB but this error seems to persist somewhat randomly across multiple machines. . Thank you for your assistance! . #### Steps to reproduce. - Command used (omitting paths to 1000 samples for brevity) for one of the failed shards. ```; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx8g -jar /gpfs/gpfs_de6000/home/dalegre/miniconda3/envs/GOASTv4.0/share/gatk4-4.4.0.0-0/gatk-package-4.4.0.0-local.jar GenomicsDBImport -V [samples 1-1002] --genomicsdb-workspace-path results/jointcalling/genomicsDB/temp_0882_of_2000_DB --merge-input-intervals false --bypass-feature-reader --tmp-dir temp --max-num-intervals-to-import-in-parallel 10 --batch-size 50 --intervals results/germline/interval/temp_0882_of_2000/scattered.interval_list --genomicsdb-shared-posixfs-optimizations true; ```. #### Expected behavior; All shards are imported into the GenomicsDB successfu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8683
https://github.com/broadinstitute/gatk/issues/8683:2123,Safety,detect,detected,2123,"ank you for your assistance! . #### Steps to reproduce. - Command used (omitting paths to 1000 samples for brevity) for one of the failed shards. ```; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx8g -jar /gpfs/gpfs_de6000/home/dalegre/miniconda3/envs/GOASTv4.0/share/gatk4-4.4.0.0-0/gatk-package-4.4.0.0-local.jar GenomicsDBImport -V [samples 1-1002] --genomicsdb-workspace-path results/jointcalling/genomicsDB/temp_0882_of_2000_DB --merge-input-intervals false --bypass-feature-reader --tmp-dir temp --max-num-intervals-to-import-in-parallel 10 --batch-size 50 --intervals results/germline/interval/temp_0882_of_2000/scattered.interval_list --genomicsdb-shared-posixfs-optimizations true; ```. #### Expected behavior; All shards are imported into the GenomicsDB successfully. . #### Actual behavior; _Tell us what happens instead_. job dies with this error:. `malloc(): unaligned tcache chunk detected`. ```; 23:45:26.793 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gpfs/gpfs_de6000/home/dalegre/miniconda3/e; nvs/GOASTv4.0/share/gatk4-4.4.0.0-0/gatk-package-4.4.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 23:45:26.822 INFO GenomicsDBImport - ------------------------------------------------------------; 23:45:26.824 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.4.0.0; 23:45:26.824 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 23:45:26.824 INFO GenomicsDBImport - Executing as dalegre@amd4103.hpc.eu.lenovo.com on Linux v5.14.0-284.11.1.el9_2.x86_64 amd6; 4; 23:45:26.824 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v17.0.3-internal+0-adhoc..src; 23:45:26.824 INFO GenomicsDBImport - Start Date/Time: February 6, 2024 at 11:45:26 PM CET; 23:45:26.824 INFO GenomicsDBImport - ------------------------------------------------------------; 23:45",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8683
https://github.com/broadinstitute/gatk/issues/8683:5641,Safety,detect,detected,5641,; 23:45:26.826 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 23:45:26.826 INFO GenomicsDBImport - Deflater: IntelDeflater; 23:45:26.827 INFO GenomicsDBImport - Inflater: IntelInflater; 23:45:26.827 INFO GenomicsDBImport - GCS max retries/reopens: 20; 23:45:26.827 INFO GenomicsDBImport - Requester pays: disabled; 23:45:26.827 INFO GenomicsDBImport - Initializing engine; 23:45:46.550 INFO FeatureManager - Using codec IntervalListCodec to read file file:///gpfs/gpfs_de6000/home/dalegre/projects/1000-Genomes/jointcalling-test/goast_workflows/JointCalling/test_samples-1000.1.4/results/germline/interval/temp_0882_of_2000/scattered.interval_list; 23:45:46.584 INFO IntervalArgumentCollection - Processing 1086188 bp from intervals; 23:45:46.586 INFO GenomicsDBImport - Done initializing engine; 23:45:47.489 INFO GenomicsDBLibLoader - GenomicsDB native library version : 1.4.4-ce4e1b9; 23:45:47.491 INFO GenomicsDBImport - Vid Map JSON file will be written to /gpfs/gpfs_de6000/home/dalegre/projects/1000-Genomes/jointcalling-test/goast_workflows/JointCalling/test_samples-1000.1.4/results/jointcalling/genomicsDB/temp_0882_of_2000_DB/vidmap.json; 23:45:47.491 INFO GenomicsDBImport - Callset Map JSON file will be written to /gpfs/gpfs_de6000/home/dalegre/projects/1000-Genomes/jointcalling-test/goast_workflows/JointCalling/test_samples-1000.1.4/results/jointcalling/genomicsDB/temp_0882_of_2000_DB/callset.json; 23:45:47.491 INFO GenomicsDBImport - Complete VCF Header will be written to /gpfs/gpfs_de6000/home/dalegre/projects/1000-Genomes/jointcalling-test/goast_workflows/JointCalling/test_samples-1000.1.4/results/jointcalling/genomicsDB/temp_0882_of_2000_DB/vcfheader.vcf; 23:45:47.491 INFO GenomicsDBImport - Importing to workspace - /gpfs/gpfs_de6000/home/dalegre/projects/1000-Genomes/jointcalling-test/goast_workflows/JointCalling/test_samples-1000.1.4/results/jointcalling/genomicsDB/temp_0882_of_2000_DB; malloc(): unaligned tcache chunk detected; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8683
https://github.com/broadinstitute/gatk/issues/8683:774,Testability,test,tests,774,"----. ## Bug Report. ### Affected tool(s) or class(es); - Tool/class name(s), special parameters: GenomicsDBImport. ### Affected version(s); - Version: gatk4-4.4.0.0-0. ### Description ; Hello,. I have been having an issue come up when utilizing `GenomicsDBImport`. This issue has happened when using a range of samples and shard counts (8 - 1000 samples, shard count of up to 2000). My current example is an attempt to joint call 1000 samples together. I will submit the jobs and 1-2 of the shards (of the ~100 concurrently running) will throw a `malloc(): unaligned tcache chunk detected`. When I resubmit that shard, it will usually rerun without a problem. Or if I kill all jobs and resubmit, a different shard will throw the malloc error. . I have run approximately 20 tests and I seem to get this failure 2/3 times. However, it only arises on the initial submission and not when additional jobs are submitted as previous shards complete. Please note that the 1000 samples have successfully been imported into the GenomicsDB but this error seems to persist somewhat randomly across multiple machines. . Thank you for your assistance! . #### Steps to reproduce. - Command used (omitting paths to 1000 samples for brevity) for one of the failed shards. ```; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx8g -jar /gpfs/gpfs_de6000/home/dalegre/miniconda3/envs/GOASTv4.0/share/gatk4-4.4.0.0-0/gatk-package-4.4.0.0-local.jar GenomicsDBImport -V [samples 1-1002] --genomicsdb-workspace-path results/jointcalling/genomicsDB/temp_0882_of_2000_DB --merge-input-intervals false --bypass-feature-reader --tmp-dir temp --max-num-intervals-to-import-in-parallel 10 --batch-size 50 --intervals results/germline/interval/temp_0882_of_2000/scattered.interval_list --genomicsdb-shared-posixfs-optimizations true; ```. #### Expected behavior; All shards are imported into the GenomicsDB successfu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8683
https://github.com/broadinstitute/gatk/issues/8683:4211,Testability,test,test,4211,-------------------------------------; 23:45:26.825 INFO GenomicsDBImport - HTSJDK Version: 3.0.5; 23:45:26.825 INFO GenomicsDBImport - Picard Version: 3.0.0; 23:45:26.825 INFO GenomicsDBImport - Built for Spark Version: 3.3.1; 23:45:26.826 INFO GenomicsDBImport - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 23:45:26.826 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 23:45:26.826 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 23:45:26.826 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 23:45:26.826 INFO GenomicsDBImport - Deflater: IntelDeflater; 23:45:26.827 INFO GenomicsDBImport - Inflater: IntelInflater; 23:45:26.827 INFO GenomicsDBImport - GCS max retries/reopens: 20; 23:45:26.827 INFO GenomicsDBImport - Requester pays: disabled; 23:45:26.827 INFO GenomicsDBImport - Initializing engine; 23:45:46.550 INFO FeatureManager - Using codec IntervalListCodec to read file file:///gpfs/gpfs_de6000/home/dalegre/projects/1000-Genomes/jointcalling-test/goast_workflows/JointCalling/test_samples-1000.1.4/results/germline/interval/temp_0882_of_2000/scattered.interval_list; 23:45:46.584 INFO IntervalArgumentCollection - Processing 1086188 bp from intervals; 23:45:46.586 INFO GenomicsDBImport - Done initializing engine; 23:45:47.489 INFO GenomicsDBLibLoader - GenomicsDB native library version : 1.4.4-ce4e1b9; 23:45:47.491 INFO GenomicsDBImport - Vid Map JSON file will be written to /gpfs/gpfs_de6000/home/dalegre/projects/1000-Genomes/jointcalling-test/goast_workflows/JointCalling/test_samples-1000.1.4/results/jointcalling/genomicsDB/temp_0882_of_2000_DB/vidmap.json; 23:45:47.491 INFO GenomicsDBImport - Callset Map JSON file will be written to /gpfs/gpfs_de6000/home/dalegre/projects/1000-Genomes/jointcalling-test/goast_workflows/JointCalling/test_samples-1000.1.4/results/jointcalling/genomicsDB/temp_0882_of_2000_DB/callset.json; 23:45:47.491 INFO GenomicsDBImport - Complete VCF Header will ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8683
https://github.com/broadinstitute/gatk/issues/8683:4715,Testability,test,test,4715,; 23:45:26.826 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 23:45:26.826 INFO GenomicsDBImport - Deflater: IntelDeflater; 23:45:26.827 INFO GenomicsDBImport - Inflater: IntelInflater; 23:45:26.827 INFO GenomicsDBImport - GCS max retries/reopens: 20; 23:45:26.827 INFO GenomicsDBImport - Requester pays: disabled; 23:45:26.827 INFO GenomicsDBImport - Initializing engine; 23:45:46.550 INFO FeatureManager - Using codec IntervalListCodec to read file file:///gpfs/gpfs_de6000/home/dalegre/projects/1000-Genomes/jointcalling-test/goast_workflows/JointCalling/test_samples-1000.1.4/results/germline/interval/temp_0882_of_2000/scattered.interval_list; 23:45:46.584 INFO IntervalArgumentCollection - Processing 1086188 bp from intervals; 23:45:46.586 INFO GenomicsDBImport - Done initializing engine; 23:45:47.489 INFO GenomicsDBLibLoader - GenomicsDB native library version : 1.4.4-ce4e1b9; 23:45:47.491 INFO GenomicsDBImport - Vid Map JSON file will be written to /gpfs/gpfs_de6000/home/dalegre/projects/1000-Genomes/jointcalling-test/goast_workflows/JointCalling/test_samples-1000.1.4/results/jointcalling/genomicsDB/temp_0882_of_2000_DB/vidmap.json; 23:45:47.491 INFO GenomicsDBImport - Callset Map JSON file will be written to /gpfs/gpfs_de6000/home/dalegre/projects/1000-Genomes/jointcalling-test/goast_workflows/JointCalling/test_samples-1000.1.4/results/jointcalling/genomicsDB/temp_0882_of_2000_DB/callset.json; 23:45:47.491 INFO GenomicsDBImport - Complete VCF Header will be written to /gpfs/gpfs_de6000/home/dalegre/projects/1000-Genomes/jointcalling-test/goast_workflows/JointCalling/test_samples-1000.1.4/results/jointcalling/genomicsDB/temp_0882_of_2000_DB/vcfheader.vcf; 23:45:47.491 INFO GenomicsDBImport - Importing to workspace - /gpfs/gpfs_de6000/home/dalegre/projects/1000-Genomes/jointcalling-test/goast_workflows/JointCalling/test_samples-1000.1.4/results/jointcalling/genomicsDB/temp_0882_of_2000_DB; malloc(): unaligned tcache chunk detected; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8683
https://github.com/broadinstitute/gatk/issues/8683:4981,Testability,test,test,4981,; 23:45:26.826 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 23:45:26.826 INFO GenomicsDBImport - Deflater: IntelDeflater; 23:45:26.827 INFO GenomicsDBImport - Inflater: IntelInflater; 23:45:26.827 INFO GenomicsDBImport - GCS max retries/reopens: 20; 23:45:26.827 INFO GenomicsDBImport - Requester pays: disabled; 23:45:26.827 INFO GenomicsDBImport - Initializing engine; 23:45:46.550 INFO FeatureManager - Using codec IntervalListCodec to read file file:///gpfs/gpfs_de6000/home/dalegre/projects/1000-Genomes/jointcalling-test/goast_workflows/JointCalling/test_samples-1000.1.4/results/germline/interval/temp_0882_of_2000/scattered.interval_list; 23:45:46.584 INFO IntervalArgumentCollection - Processing 1086188 bp from intervals; 23:45:46.586 INFO GenomicsDBImport - Done initializing engine; 23:45:47.489 INFO GenomicsDBLibLoader - GenomicsDB native library version : 1.4.4-ce4e1b9; 23:45:47.491 INFO GenomicsDBImport - Vid Map JSON file will be written to /gpfs/gpfs_de6000/home/dalegre/projects/1000-Genomes/jointcalling-test/goast_workflows/JointCalling/test_samples-1000.1.4/results/jointcalling/genomicsDB/temp_0882_of_2000_DB/vidmap.json; 23:45:47.491 INFO GenomicsDBImport - Callset Map JSON file will be written to /gpfs/gpfs_de6000/home/dalegre/projects/1000-Genomes/jointcalling-test/goast_workflows/JointCalling/test_samples-1000.1.4/results/jointcalling/genomicsDB/temp_0882_of_2000_DB/callset.json; 23:45:47.491 INFO GenomicsDBImport - Complete VCF Header will be written to /gpfs/gpfs_de6000/home/dalegre/projects/1000-Genomes/jointcalling-test/goast_workflows/JointCalling/test_samples-1000.1.4/results/jointcalling/genomicsDB/temp_0882_of_2000_DB/vcfheader.vcf; 23:45:47.491 INFO GenomicsDBImport - Importing to workspace - /gpfs/gpfs_de6000/home/dalegre/projects/1000-Genomes/jointcalling-test/goast_workflows/JointCalling/test_samples-1000.1.4/results/jointcalling/genomicsDB/temp_0882_of_2000_DB; malloc(): unaligned tcache chunk detected; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8683
https://github.com/broadinstitute/gatk/issues/8683:5246,Testability,test,test,5246,; 23:45:26.826 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 23:45:26.826 INFO GenomicsDBImport - Deflater: IntelDeflater; 23:45:26.827 INFO GenomicsDBImport - Inflater: IntelInflater; 23:45:26.827 INFO GenomicsDBImport - GCS max retries/reopens: 20; 23:45:26.827 INFO GenomicsDBImport - Requester pays: disabled; 23:45:26.827 INFO GenomicsDBImport - Initializing engine; 23:45:46.550 INFO FeatureManager - Using codec IntervalListCodec to read file file:///gpfs/gpfs_de6000/home/dalegre/projects/1000-Genomes/jointcalling-test/goast_workflows/JointCalling/test_samples-1000.1.4/results/germline/interval/temp_0882_of_2000/scattered.interval_list; 23:45:46.584 INFO IntervalArgumentCollection - Processing 1086188 bp from intervals; 23:45:46.586 INFO GenomicsDBImport - Done initializing engine; 23:45:47.489 INFO GenomicsDBLibLoader - GenomicsDB native library version : 1.4.4-ce4e1b9; 23:45:47.491 INFO GenomicsDBImport - Vid Map JSON file will be written to /gpfs/gpfs_de6000/home/dalegre/projects/1000-Genomes/jointcalling-test/goast_workflows/JointCalling/test_samples-1000.1.4/results/jointcalling/genomicsDB/temp_0882_of_2000_DB/vidmap.json; 23:45:47.491 INFO GenomicsDBImport - Callset Map JSON file will be written to /gpfs/gpfs_de6000/home/dalegre/projects/1000-Genomes/jointcalling-test/goast_workflows/JointCalling/test_samples-1000.1.4/results/jointcalling/genomicsDB/temp_0882_of_2000_DB/callset.json; 23:45:47.491 INFO GenomicsDBImport - Complete VCF Header will be written to /gpfs/gpfs_de6000/home/dalegre/projects/1000-Genomes/jointcalling-test/goast_workflows/JointCalling/test_samples-1000.1.4/results/jointcalling/genomicsDB/temp_0882_of_2000_DB/vcfheader.vcf; 23:45:47.491 INFO GenomicsDBImport - Importing to workspace - /gpfs/gpfs_de6000/home/dalegre/projects/1000-Genomes/jointcalling-test/goast_workflows/JointCalling/test_samples-1000.1.4/results/jointcalling/genomicsDB/temp_0882_of_2000_DB; malloc(): unaligned tcache chunk detected; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8683
https://github.com/broadinstitute/gatk/issues/8683:5498,Testability,test,test,5498,; 23:45:26.826 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 23:45:26.826 INFO GenomicsDBImport - Deflater: IntelDeflater; 23:45:26.827 INFO GenomicsDBImport - Inflater: IntelInflater; 23:45:26.827 INFO GenomicsDBImport - GCS max retries/reopens: 20; 23:45:26.827 INFO GenomicsDBImport - Requester pays: disabled; 23:45:26.827 INFO GenomicsDBImport - Initializing engine; 23:45:46.550 INFO FeatureManager - Using codec IntervalListCodec to read file file:///gpfs/gpfs_de6000/home/dalegre/projects/1000-Genomes/jointcalling-test/goast_workflows/JointCalling/test_samples-1000.1.4/results/germline/interval/temp_0882_of_2000/scattered.interval_list; 23:45:46.584 INFO IntervalArgumentCollection - Processing 1086188 bp from intervals; 23:45:46.586 INFO GenomicsDBImport - Done initializing engine; 23:45:47.489 INFO GenomicsDBLibLoader - GenomicsDB native library version : 1.4.4-ce4e1b9; 23:45:47.491 INFO GenomicsDBImport - Vid Map JSON file will be written to /gpfs/gpfs_de6000/home/dalegre/projects/1000-Genomes/jointcalling-test/goast_workflows/JointCalling/test_samples-1000.1.4/results/jointcalling/genomicsDB/temp_0882_of_2000_DB/vidmap.json; 23:45:47.491 INFO GenomicsDBImport - Callset Map JSON file will be written to /gpfs/gpfs_de6000/home/dalegre/projects/1000-Genomes/jointcalling-test/goast_workflows/JointCalling/test_samples-1000.1.4/results/jointcalling/genomicsDB/temp_0882_of_2000_DB/callset.json; 23:45:47.491 INFO GenomicsDBImport - Complete VCF Header will be written to /gpfs/gpfs_de6000/home/dalegre/projects/1000-Genomes/jointcalling-test/goast_workflows/JointCalling/test_samples-1000.1.4/results/jointcalling/genomicsDB/temp_0882_of_2000_DB/vcfheader.vcf; 23:45:47.491 INFO GenomicsDBImport - Importing to workspace - /gpfs/gpfs_de6000/home/dalegre/projects/1000-Genomes/jointcalling-test/goast_workflows/JointCalling/test_samples-1000.1.4/results/jointcalling/genomicsDB/temp_0882_of_2000_DB; malloc(): unaligned tcache chunk detected; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8683
https://github.com/broadinstitute/gatk/issues/8684:1594,Deployability,update,update,1594,"## Bug Report. ### Affected tool(s) or class(es); The docker image: `broadinstitute/gatk`. ### Affected version(s); `latest`. ### Description ; - The current GATK image has 44 layers; - In [the Azure Container Registry standard service tier](https://learn.microsoft.com/en-us/azure/container-registry/container-registry-skus#registry-throughput-and-throttling), ""ReadOps per minute"" is limited to 3000; - ""A `docker pull` translates to multiple read operations based on the number of layers in the image, plus the manifest retrieval.""; - 3000 / 45 = 66. That means that the image can only be pulled 66 times per minute. This is problematic for running many concurrent workflows that also have many shards. Once that limit is exceeded, the task can fail, which can cause the entire workflow to fail. ; - Layers can be viewed here: `docker history --no-trunc broadinstitute/gatk > gatk-image-layers.txt`; [gatk-image-layers.txt](https://github.com/broadinstitute/gatk/files/14212774/gatk-image-layers.txt). #### Steps to reproduce; `docker history --no-trunc broadinstitute/gatk > gatk-image-layers.txt`. #### Expected behavior; `--squash` shall be added to `build_docker_base_cloud.sh`, like has been added to `build_docker_base_locally.sh` already: https://github.com/broadinstitute/gatk/blob/a353e49f218e675f331abf629f0bb46df1d5151d/scripts/docker/gatkbase/build_docker_base_locally.sh#L24. #### Workaround; Users can pull the existing image, and [use `docker-squash` to squash the image to a single layer](https://github.com/goldmann/docker-squash), then push it into their private ACR, then update their WDLs to reference the new image.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8684
https://github.com/broadinstitute/gatk/issues/8684:176,Modifiability,layers,layers,176,"## Bug Report. ### Affected tool(s) or class(es); The docker image: `broadinstitute/gatk`. ### Affected version(s); `latest`. ### Description ; - The current GATK image has 44 layers; - In [the Azure Container Registry standard service tier](https://learn.microsoft.com/en-us/azure/container-registry/container-registry-skus#registry-throughput-and-throttling), ""ReadOps per minute"" is limited to 3000; - ""A `docker pull` translates to multiple read operations based on the number of layers in the image, plus the manifest retrieval.""; - 3000 / 45 = 66. That means that the image can only be pulled 66 times per minute. This is problematic for running many concurrent workflows that also have many shards. Once that limit is exceeded, the task can fail, which can cause the entire workflow to fail. ; - Layers can be viewed here: `docker history --no-trunc broadinstitute/gatk > gatk-image-layers.txt`; [gatk-image-layers.txt](https://github.com/broadinstitute/gatk/files/14212774/gatk-image-layers.txt). #### Steps to reproduce; `docker history --no-trunc broadinstitute/gatk > gatk-image-layers.txt`. #### Expected behavior; `--squash` shall be added to `build_docker_base_cloud.sh`, like has been added to `build_docker_base_locally.sh` already: https://github.com/broadinstitute/gatk/blob/a353e49f218e675f331abf629f0bb46df1d5151d/scripts/docker/gatkbase/build_docker_base_locally.sh#L24. #### Workaround; Users can pull the existing image, and [use `docker-squash` to squash the image to a single layer](https://github.com/goldmann/docker-squash), then push it into their private ACR, then update their WDLs to reference the new image.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8684
https://github.com/broadinstitute/gatk/issues/8684:484,Modifiability,layers,layers,484,"## Bug Report. ### Affected tool(s) or class(es); The docker image: `broadinstitute/gatk`. ### Affected version(s); `latest`. ### Description ; - The current GATK image has 44 layers; - In [the Azure Container Registry standard service tier](https://learn.microsoft.com/en-us/azure/container-registry/container-registry-skus#registry-throughput-and-throttling), ""ReadOps per minute"" is limited to 3000; - ""A `docker pull` translates to multiple read operations based on the number of layers in the image, plus the manifest retrieval.""; - 3000 / 45 = 66. That means that the image can only be pulled 66 times per minute. This is problematic for running many concurrent workflows that also have many shards. Once that limit is exceeded, the task can fail, which can cause the entire workflow to fail. ; - Layers can be viewed here: `docker history --no-trunc broadinstitute/gatk > gatk-image-layers.txt`; [gatk-image-layers.txt](https://github.com/broadinstitute/gatk/files/14212774/gatk-image-layers.txt). #### Steps to reproduce; `docker history --no-trunc broadinstitute/gatk > gatk-image-layers.txt`. #### Expected behavior; `--squash` shall be added to `build_docker_base_cloud.sh`, like has been added to `build_docker_base_locally.sh` already: https://github.com/broadinstitute/gatk/blob/a353e49f218e675f331abf629f0bb46df1d5151d/scripts/docker/gatkbase/build_docker_base_locally.sh#L24. #### Workaround; Users can pull the existing image, and [use `docker-squash` to squash the image to a single layer](https://github.com/goldmann/docker-squash), then push it into their private ACR, then update their WDLs to reference the new image.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8684
https://github.com/broadinstitute/gatk/issues/8684:803,Modifiability,Layers,Layers,803,"## Bug Report. ### Affected tool(s) or class(es); The docker image: `broadinstitute/gatk`. ### Affected version(s); `latest`. ### Description ; - The current GATK image has 44 layers; - In [the Azure Container Registry standard service tier](https://learn.microsoft.com/en-us/azure/container-registry/container-registry-skus#registry-throughput-and-throttling), ""ReadOps per minute"" is limited to 3000; - ""A `docker pull` translates to multiple read operations based on the number of layers in the image, plus the manifest retrieval.""; - 3000 / 45 = 66. That means that the image can only be pulled 66 times per minute. This is problematic for running many concurrent workflows that also have many shards. Once that limit is exceeded, the task can fail, which can cause the entire workflow to fail. ; - Layers can be viewed here: `docker history --no-trunc broadinstitute/gatk > gatk-image-layers.txt`; [gatk-image-layers.txt](https://github.com/broadinstitute/gatk/files/14212774/gatk-image-layers.txt). #### Steps to reproduce; `docker history --no-trunc broadinstitute/gatk > gatk-image-layers.txt`. #### Expected behavior; `--squash` shall be added to `build_docker_base_cloud.sh`, like has been added to `build_docker_base_locally.sh` already: https://github.com/broadinstitute/gatk/blob/a353e49f218e675f331abf629f0bb46df1d5151d/scripts/docker/gatkbase/build_docker_base_locally.sh#L24. #### Workaround; Users can pull the existing image, and [use `docker-squash` to squash the image to a single layer](https://github.com/goldmann/docker-squash), then push it into their private ACR, then update their WDLs to reference the new image.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8684
https://github.com/broadinstitute/gatk/issues/8684:890,Modifiability,layers,layers,890,"## Bug Report. ### Affected tool(s) or class(es); The docker image: `broadinstitute/gatk`. ### Affected version(s); `latest`. ### Description ; - The current GATK image has 44 layers; - In [the Azure Container Registry standard service tier](https://learn.microsoft.com/en-us/azure/container-registry/container-registry-skus#registry-throughput-and-throttling), ""ReadOps per minute"" is limited to 3000; - ""A `docker pull` translates to multiple read operations based on the number of layers in the image, plus the manifest retrieval.""; - 3000 / 45 = 66. That means that the image can only be pulled 66 times per minute. This is problematic for running many concurrent workflows that also have many shards. Once that limit is exceeded, the task can fail, which can cause the entire workflow to fail. ; - Layers can be viewed here: `docker history --no-trunc broadinstitute/gatk > gatk-image-layers.txt`; [gatk-image-layers.txt](https://github.com/broadinstitute/gatk/files/14212774/gatk-image-layers.txt). #### Steps to reproduce; `docker history --no-trunc broadinstitute/gatk > gatk-image-layers.txt`. #### Expected behavior; `--squash` shall be added to `build_docker_base_cloud.sh`, like has been added to `build_docker_base_locally.sh` already: https://github.com/broadinstitute/gatk/blob/a353e49f218e675f331abf629f0bb46df1d5151d/scripts/docker/gatkbase/build_docker_base_locally.sh#L24. #### Workaround; Users can pull the existing image, and [use `docker-squash` to squash the image to a single layer](https://github.com/goldmann/docker-squash), then push it into their private ACR, then update their WDLs to reference the new image.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8684
https://github.com/broadinstitute/gatk/issues/8684:915,Modifiability,layers,layers,915,"## Bug Report. ### Affected tool(s) or class(es); The docker image: `broadinstitute/gatk`. ### Affected version(s); `latest`. ### Description ; - The current GATK image has 44 layers; - In [the Azure Container Registry standard service tier](https://learn.microsoft.com/en-us/azure/container-registry/container-registry-skus#registry-throughput-and-throttling), ""ReadOps per minute"" is limited to 3000; - ""A `docker pull` translates to multiple read operations based on the number of layers in the image, plus the manifest retrieval.""; - 3000 / 45 = 66. That means that the image can only be pulled 66 times per minute. This is problematic for running many concurrent workflows that also have many shards. Once that limit is exceeded, the task can fail, which can cause the entire workflow to fail. ; - Layers can be viewed here: `docker history --no-trunc broadinstitute/gatk > gatk-image-layers.txt`; [gatk-image-layers.txt](https://github.com/broadinstitute/gatk/files/14212774/gatk-image-layers.txt). #### Steps to reproduce; `docker history --no-trunc broadinstitute/gatk > gatk-image-layers.txt`. #### Expected behavior; `--squash` shall be added to `build_docker_base_cloud.sh`, like has been added to `build_docker_base_locally.sh` already: https://github.com/broadinstitute/gatk/blob/a353e49f218e675f331abf629f0bb46df1d5151d/scripts/docker/gatkbase/build_docker_base_locally.sh#L24. #### Workaround; Users can pull the existing image, and [use `docker-squash` to squash the image to a single layer](https://github.com/goldmann/docker-squash), then push it into their private ACR, then update their WDLs to reference the new image.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8684
https://github.com/broadinstitute/gatk/issues/8684:992,Modifiability,layers,layers,992,"## Bug Report. ### Affected tool(s) or class(es); The docker image: `broadinstitute/gatk`. ### Affected version(s); `latest`. ### Description ; - The current GATK image has 44 layers; - In [the Azure Container Registry standard service tier](https://learn.microsoft.com/en-us/azure/container-registry/container-registry-skus#registry-throughput-and-throttling), ""ReadOps per minute"" is limited to 3000; - ""A `docker pull` translates to multiple read operations based on the number of layers in the image, plus the manifest retrieval.""; - 3000 / 45 = 66. That means that the image can only be pulled 66 times per minute. This is problematic for running many concurrent workflows that also have many shards. Once that limit is exceeded, the task can fail, which can cause the entire workflow to fail. ; - Layers can be viewed here: `docker history --no-trunc broadinstitute/gatk > gatk-image-layers.txt`; [gatk-image-layers.txt](https://github.com/broadinstitute/gatk/files/14212774/gatk-image-layers.txt). #### Steps to reproduce; `docker history --no-trunc broadinstitute/gatk > gatk-image-layers.txt`. #### Expected behavior; `--squash` shall be added to `build_docker_base_cloud.sh`, like has been added to `build_docker_base_locally.sh` already: https://github.com/broadinstitute/gatk/blob/a353e49f218e675f331abf629f0bb46df1d5151d/scripts/docker/gatkbase/build_docker_base_locally.sh#L24. #### Workaround; Users can pull the existing image, and [use `docker-squash` to squash the image to a single layer](https://github.com/goldmann/docker-squash), then push it into their private ACR, then update their WDLs to reference the new image.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8684
https://github.com/broadinstitute/gatk/issues/8684:1090,Modifiability,layers,layers,1090,"## Bug Report. ### Affected tool(s) or class(es); The docker image: `broadinstitute/gatk`. ### Affected version(s); `latest`. ### Description ; - The current GATK image has 44 layers; - In [the Azure Container Registry standard service tier](https://learn.microsoft.com/en-us/azure/container-registry/container-registry-skus#registry-throughput-and-throttling), ""ReadOps per minute"" is limited to 3000; - ""A `docker pull` translates to multiple read operations based on the number of layers in the image, plus the manifest retrieval.""; - 3000 / 45 = 66. That means that the image can only be pulled 66 times per minute. This is problematic for running many concurrent workflows that also have many shards. Once that limit is exceeded, the task can fail, which can cause the entire workflow to fail. ; - Layers can be viewed here: `docker history --no-trunc broadinstitute/gatk > gatk-image-layers.txt`; [gatk-image-layers.txt](https://github.com/broadinstitute/gatk/files/14212774/gatk-image-layers.txt). #### Steps to reproduce; `docker history --no-trunc broadinstitute/gatk > gatk-image-layers.txt`. #### Expected behavior; `--squash` shall be added to `build_docker_base_cloud.sh`, like has been added to `build_docker_base_locally.sh` already: https://github.com/broadinstitute/gatk/blob/a353e49f218e675f331abf629f0bb46df1d5151d/scripts/docker/gatkbase/build_docker_base_locally.sh#L24. #### Workaround; Users can pull the existing image, and [use `docker-squash` to squash the image to a single layer](https://github.com/goldmann/docker-squash), then push it into their private ACR, then update their WDLs to reference the new image.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8684
https://github.com/broadinstitute/gatk/issues/8684:334,Performance,throughput,throughput-and-throttling,334,"## Bug Report. ### Affected tool(s) or class(es); The docker image: `broadinstitute/gatk`. ### Affected version(s); `latest`. ### Description ; - The current GATK image has 44 layers; - In [the Azure Container Registry standard service tier](https://learn.microsoft.com/en-us/azure/container-registry/container-registry-skus#registry-throughput-and-throttling), ""ReadOps per minute"" is limited to 3000; - ""A `docker pull` translates to multiple read operations based on the number of layers in the image, plus the manifest retrieval.""; - 3000 / 45 = 66. That means that the image can only be pulled 66 times per minute. This is problematic for running many concurrent workflows that also have many shards. Once that limit is exceeded, the task can fail, which can cause the entire workflow to fail. ; - Layers can be viewed here: `docker history --no-trunc broadinstitute/gatk > gatk-image-layers.txt`; [gatk-image-layers.txt](https://github.com/broadinstitute/gatk/files/14212774/gatk-image-layers.txt). #### Steps to reproduce; `docker history --no-trunc broadinstitute/gatk > gatk-image-layers.txt`. #### Expected behavior; `--squash` shall be added to `build_docker_base_cloud.sh`, like has been added to `build_docker_base_locally.sh` already: https://github.com/broadinstitute/gatk/blob/a353e49f218e675f331abf629f0bb46df1d5151d/scripts/docker/gatkbase/build_docker_base_locally.sh#L24. #### Workaround; Users can pull the existing image, and [use `docker-squash` to squash the image to a single layer](https://github.com/goldmann/docker-squash), then push it into their private ACR, then update their WDLs to reference the new image.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8684
https://github.com/broadinstitute/gatk/issues/8684:657,Performance,concurren,concurrent,657,"## Bug Report. ### Affected tool(s) or class(es); The docker image: `broadinstitute/gatk`. ### Affected version(s); `latest`. ### Description ; - The current GATK image has 44 layers; - In [the Azure Container Registry standard service tier](https://learn.microsoft.com/en-us/azure/container-registry/container-registry-skus#registry-throughput-and-throttling), ""ReadOps per minute"" is limited to 3000; - ""A `docker pull` translates to multiple read operations based on the number of layers in the image, plus the manifest retrieval.""; - 3000 / 45 = 66. That means that the image can only be pulled 66 times per minute. This is problematic for running many concurrent workflows that also have many shards. Once that limit is exceeded, the task can fail, which can cause the entire workflow to fail. ; - Layers can be viewed here: `docker history --no-trunc broadinstitute/gatk > gatk-image-layers.txt`; [gatk-image-layers.txt](https://github.com/broadinstitute/gatk/files/14212774/gatk-image-layers.txt). #### Steps to reproduce; `docker history --no-trunc broadinstitute/gatk > gatk-image-layers.txt`. #### Expected behavior; `--squash` shall be added to `build_docker_base_cloud.sh`, like has been added to `build_docker_base_locally.sh` already: https://github.com/broadinstitute/gatk/blob/a353e49f218e675f331abf629f0bb46df1d5151d/scripts/docker/gatkbase/build_docker_base_locally.sh#L24. #### Workaround; Users can pull the existing image, and [use `docker-squash` to squash the image to a single layer](https://github.com/goldmann/docker-squash), then push it into their private ACR, then update their WDLs to reference the new image.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8684
https://github.com/broadinstitute/gatk/issues/8684:250,Usability,learn,learn,250,"## Bug Report. ### Affected tool(s) or class(es); The docker image: `broadinstitute/gatk`. ### Affected version(s); `latest`. ### Description ; - The current GATK image has 44 layers; - In [the Azure Container Registry standard service tier](https://learn.microsoft.com/en-us/azure/container-registry/container-registry-skus#registry-throughput-and-throttling), ""ReadOps per minute"" is limited to 3000; - ""A `docker pull` translates to multiple read operations based on the number of layers in the image, plus the manifest retrieval.""; - 3000 / 45 = 66. That means that the image can only be pulled 66 times per minute. This is problematic for running many concurrent workflows that also have many shards. Once that limit is exceeded, the task can fail, which can cause the entire workflow to fail. ; - Layers can be viewed here: `docker history --no-trunc broadinstitute/gatk > gatk-image-layers.txt`; [gatk-image-layers.txt](https://github.com/broadinstitute/gatk/files/14212774/gatk-image-layers.txt). #### Steps to reproduce; `docker history --no-trunc broadinstitute/gatk > gatk-image-layers.txt`. #### Expected behavior; `--squash` shall be added to `build_docker_base_cloud.sh`, like has been added to `build_docker_base_locally.sh` already: https://github.com/broadinstitute/gatk/blob/a353e49f218e675f331abf629f0bb46df1d5151d/scripts/docker/gatkbase/build_docker_base_locally.sh#L24. #### Workaround; Users can pull the existing image, and [use `docker-squash` to squash the image to a single layer](https://github.com/goldmann/docker-squash), then push it into their private ACR, then update their WDLs to reference the new image.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8684
https://github.com/broadinstitute/gatk/pull/8685:317,Testability,test,test,317,Run in quickstart: https://app.terra.bio/#workspaces/gvs-dev/RSA%20-%20GVS%20Quickstart%20V2%20/job_history/588a50be-9e03-4e34-8f12-a61a26262763; Run on delta: https://app.terra.bio/#workspaces/allofus-drc-wgs-dev/GVS%20AoU%20WGS%20Delta%20Callset%20v2/job_history/135f5cc3-4948-426b-a90c-8ced5ec9f1e6 (failed on the test that was added after Delta because the typo in the nfe column names),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8685
https://github.com/broadinstitute/gatk/pull/8686:51,Energy Efficiency,reduce,reduce,51,- Refactors both the base and root `dockerfile` to reduce the total # of layers. Addresses: https://github.com/broadinstitute/gatk/issues/8684,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8686
https://github.com/broadinstitute/gatk/pull/8686:2,Modifiability,Refactor,Refactors,2,- Refactors both the base and root `dockerfile` to reduce the total # of layers. Addresses: https://github.com/broadinstitute/gatk/issues/8684,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8686
https://github.com/broadinstitute/gatk/pull/8686:73,Modifiability,layers,layers,73,- Refactors both the base and root `dockerfile` to reduce the total # of layers. Addresses: https://github.com/broadinstitute/gatk/issues/8684,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8686
https://github.com/broadinstitute/gatk/pull/8687:0,Deployability,Integrat,Integration,0,Integration run with [all the tests here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/d6892c6d-0d1e-415b-819b-24a30ed08f0f),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8687
https://github.com/broadinstitute/gatk/pull/8687:0,Integrability,Integrat,Integration,0,Integration run with [all the tests here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/d6892c6d-0d1e-415b-819b-24a30ed08f0f),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8687
https://github.com/broadinstitute/gatk/pull/8687:30,Testability,test,tests,30,Integration run with [all the tests here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/d6892c6d-0d1e-415b-819b-24a30ed08f0f),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8687
https://github.com/broadinstitute/gatk/pull/8689:30,Availability,failure,failures,30,Initial check-in to find test failures. . Adresses #8328,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8689
https://github.com/broadinstitute/gatk/pull/8689:25,Testability,test,test,25,Initial check-in to find test failures. . Adresses #8328,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8689
https://github.com/broadinstitute/gatk/pull/8692:160,Deployability,integrat,integration,160,Spawn of VS-1214 which required the ability to run with a wheel. Hopefully we never need to use this but now we would have the ability if we ever need it. Full integration run [in progress](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/6d67fda8-1237-4cd8-bf49-fe582ae7fc13). Runs requiring PMI ops access exercising this new wheel functionality with a Delta-age 0.2.98 wheel:; - [Delta](https://app.terra.bio/#workspaces/allofus-drc-wgs-dev/GVS%20AoU%20Echo%20RD/job_history/7215bdc8-f951-4b84-b9bf-3aaa80eae0a1); - [Delcho](https://app.terra.bio/#workspaces/allofus-drc-wgs-dev/GVS%20AoU%20Echo%20RD/job_history/a336972e-d9f4-4a74-92fe-6ed94d2b5fff),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8692
https://github.com/broadinstitute/gatk/pull/8692:160,Integrability,integrat,integration,160,Spawn of VS-1214 which required the ability to run with a wheel. Hopefully we never need to use this but now we would have the ability if we ever need it. Full integration run [in progress](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/6d67fda8-1237-4cd8-bf49-fe582ae7fc13). Runs requiring PMI ops access exercising this new wheel functionality with a Delta-age 0.2.98 wheel:; - [Delta](https://app.terra.bio/#workspaces/allofus-drc-wgs-dev/GVS%20AoU%20Echo%20RD/job_history/7215bdc8-f951-4b84-b9bf-3aaa80eae0a1); - [Delcho](https://app.terra.bio/#workspaces/allofus-drc-wgs-dev/GVS%20AoU%20Echo%20RD/job_history/a336972e-d9f4-4a74-92fe-6ed94d2b5fff),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8692
https://github.com/broadinstitute/gatk/pull/8692:324,Security,access,access,324,Spawn of VS-1214 which required the ability to run with a wheel. Hopefully we never need to use this but now we would have the ability if we ever need it. Full integration run [in progress](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/6d67fda8-1237-4cd8-bf49-fe582ae7fc13). Runs requiring PMI ops access exercising this new wheel functionality with a Delta-age 0.2.98 wheel:; - [Delta](https://app.terra.bio/#workspaces/allofus-drc-wgs-dev/GVS%20AoU%20Echo%20RD/job_history/7215bdc8-f951-4b84-b9bf-3aaa80eae0a1); - [Delcho](https://app.terra.bio/#workspaces/allofus-drc-wgs-dev/GVS%20AoU%20Echo%20RD/job_history/a336972e-d9f4-4a74-92fe-6ed94d2b5fff),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8692
https://github.com/broadinstitute/gatk/pull/8693:4,Deployability,Update,Updates,4,"### Updates; A record with SVTYPE=CTX and CPX_TYPE=CTX_INV was added to a recent GATK-SV VCF after manual curation. The following changes were made to be able to properly annotate this type of event.; * CPX_TYPE will be checked for CTX records, and if it is CTX_INV, the INV interval from CPX_INTERVALS will be added to the annotation segments.; * Additionally, instead of annotating two breakpoint intervals CHROM:POS-END and CHR2:END2-END2+1 for CTX events, we will now annotate 4 individual breakpoints to cover the case where END != POS+1. Those 4 breakpoints are CHROM:POS-POS, CHROM:END-END, CHR2:END2-END2, and CHR2:END2+1-END2+1.; * In the future, to be able to represent intervals on CHR2, POS2 may be added. SVAnnotate will need to be updated accordingly at that time. ### Testing; * Unit tests for CTX_INV added; * Unit tests for other CTX updated; * A one-line VCF was created to test the real-life example CTX_INV event that was curated, and it was annotated correctly",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8693
https://github.com/broadinstitute/gatk/pull/8693:745,Deployability,update,updated,745,"### Updates; A record with SVTYPE=CTX and CPX_TYPE=CTX_INV was added to a recent GATK-SV VCF after manual curation. The following changes were made to be able to properly annotate this type of event.; * CPX_TYPE will be checked for CTX records, and if it is CTX_INV, the INV interval from CPX_INTERVALS will be added to the annotation segments.; * Additionally, instead of annotating two breakpoint intervals CHROM:POS-END and CHR2:END2-END2+1 for CTX events, we will now annotate 4 individual breakpoints to cover the case where END != POS+1. Those 4 breakpoints are CHROM:POS-POS, CHROM:END-END, CHR2:END2-END2, and CHR2:END2+1-END2+1.; * In the future, to be able to represent intervals on CHR2, POS2 may be added. SVAnnotate will need to be updated accordingly at that time. ### Testing; * Unit tests for CTX_INV added; * Unit tests for other CTX updated; * A one-line VCF was created to test the real-life example CTX_INV event that was curated, and it was annotated correctly",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8693
https://github.com/broadinstitute/gatk/pull/8693:851,Deployability,update,updated,851,"### Updates; A record with SVTYPE=CTX and CPX_TYPE=CTX_INV was added to a recent GATK-SV VCF after manual curation. The following changes were made to be able to properly annotate this type of event.; * CPX_TYPE will be checked for CTX records, and if it is CTX_INV, the INV interval from CPX_INTERVALS will be added to the annotation segments.; * Additionally, instead of annotating two breakpoint intervals CHROM:POS-END and CHR2:END2-END2+1 for CTX events, we will now annotate 4 individual breakpoints to cover the case where END != POS+1. Those 4 breakpoints are CHROM:POS-POS, CHROM:END-END, CHR2:END2-END2, and CHR2:END2+1-END2+1.; * In the future, to be able to represent intervals on CHR2, POS2 may be added. SVAnnotate will need to be updated accordingly at that time. ### Testing; * Unit tests for CTX_INV added; * Unit tests for other CTX updated; * A one-line VCF was created to test the real-life example CTX_INV event that was curated, and it was annotated correctly",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8693
https://github.com/broadinstitute/gatk/pull/8693:783,Testability,Test,Testing,783,"### Updates; A record with SVTYPE=CTX and CPX_TYPE=CTX_INV was added to a recent GATK-SV VCF after manual curation. The following changes were made to be able to properly annotate this type of event.; * CPX_TYPE will be checked for CTX records, and if it is CTX_INV, the INV interval from CPX_INTERVALS will be added to the annotation segments.; * Additionally, instead of annotating two breakpoint intervals CHROM:POS-END and CHR2:END2-END2+1 for CTX events, we will now annotate 4 individual breakpoints to cover the case where END != POS+1. Those 4 breakpoints are CHROM:POS-POS, CHROM:END-END, CHR2:END2-END2, and CHR2:END2+1-END2+1.; * In the future, to be able to represent intervals on CHR2, POS2 may be added. SVAnnotate will need to be updated accordingly at that time. ### Testing; * Unit tests for CTX_INV added; * Unit tests for other CTX updated; * A one-line VCF was created to test the real-life example CTX_INV event that was curated, and it was annotated correctly",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8693
https://github.com/broadinstitute/gatk/pull/8693:799,Testability,test,tests,799,"### Updates; A record with SVTYPE=CTX and CPX_TYPE=CTX_INV was added to a recent GATK-SV VCF after manual curation. The following changes were made to be able to properly annotate this type of event.; * CPX_TYPE will be checked for CTX records, and if it is CTX_INV, the INV interval from CPX_INTERVALS will be added to the annotation segments.; * Additionally, instead of annotating two breakpoint intervals CHROM:POS-END and CHR2:END2-END2+1 for CTX events, we will now annotate 4 individual breakpoints to cover the case where END != POS+1. Those 4 breakpoints are CHROM:POS-POS, CHROM:END-END, CHR2:END2-END2, and CHR2:END2+1-END2+1.; * In the future, to be able to represent intervals on CHR2, POS2 may be added. SVAnnotate will need to be updated accordingly at that time. ### Testing; * Unit tests for CTX_INV added; * Unit tests for other CTX updated; * A one-line VCF was created to test the real-life example CTX_INV event that was curated, and it was annotated correctly",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8693
https://github.com/broadinstitute/gatk/pull/8693:831,Testability,test,tests,831,"### Updates; A record with SVTYPE=CTX and CPX_TYPE=CTX_INV was added to a recent GATK-SV VCF after manual curation. The following changes were made to be able to properly annotate this type of event.; * CPX_TYPE will be checked for CTX records, and if it is CTX_INV, the INV interval from CPX_INTERVALS will be added to the annotation segments.; * Additionally, instead of annotating two breakpoint intervals CHROM:POS-END and CHR2:END2-END2+1 for CTX events, we will now annotate 4 individual breakpoints to cover the case where END != POS+1. Those 4 breakpoints are CHROM:POS-POS, CHROM:END-END, CHR2:END2-END2, and CHR2:END2+1-END2+1.; * In the future, to be able to represent intervals on CHR2, POS2 may be added. SVAnnotate will need to be updated accordingly at that time. ### Testing; * Unit tests for CTX_INV added; * Unit tests for other CTX updated; * A one-line VCF was created to test the real-life example CTX_INV event that was curated, and it was annotated correctly",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8693
https://github.com/broadinstitute/gatk/pull/8693:892,Testability,test,test,892,"### Updates; A record with SVTYPE=CTX and CPX_TYPE=CTX_INV was added to a recent GATK-SV VCF after manual curation. The following changes were made to be able to properly annotate this type of event.; * CPX_TYPE will be checked for CTX records, and if it is CTX_INV, the INV interval from CPX_INTERVALS will be added to the annotation segments.; * Additionally, instead of annotating two breakpoint intervals CHROM:POS-END and CHR2:END2-END2+1 for CTX events, we will now annotate 4 individual breakpoints to cover the case where END != POS+1. Those 4 breakpoints are CHROM:POS-POS, CHROM:END-END, CHR2:END2-END2, and CHR2:END2+1-END2+1.; * In the future, to be able to represent intervals on CHR2, POS2 may be added. SVAnnotate will need to be updated accordingly at that time. ### Testing; * Unit tests for CTX_INV added; * Unit tests for other CTX updated; * A one-line VCF was created to test the real-life example CTX_INV event that was curated, and it was annotated correctly",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8693
https://github.com/broadinstitute/gatk/pull/8696:11,Deployability,Integrat,Integration,11,Successful Integration Test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/d43ca844-632b-4737-962e-56369ac91e53),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8696
https://github.com/broadinstitute/gatk/pull/8696:11,Integrability,Integrat,Integration,11,Successful Integration Test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/d43ca844-632b-4737-962e-56369ac91e53),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8696
https://github.com/broadinstitute/gatk/pull/8696:23,Testability,Test,Test,23,Successful Integration Test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/d43ca844-632b-4737-962e-56369ac91e53),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8696
https://github.com/broadinstitute/gatk/pull/8697:61,Availability,robust,robust,61,"The pull request addresses two issues:. 1. Improved and more robust parsing of FlowBasedReads. Specifically, the code now determines the minimal reportable quality; 2. New tool AddFlowSNVQuality that allows users to convert the flow-based quality format when every base quality reports probability of an insertion or deletion to a conventional format that gives base qualities (total probability of mismatch and probability of each mismatch in separate tags). . We believe that this tool is going to be important for users of the Ultima Genomics data that care about calling SNVs, especially in somatic setting, so the goal was to make documentation more accessible. . Happy to receive feedback about it",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8697
https://github.com/broadinstitute/gatk/pull/8697:655,Security,access,accessible,655,"The pull request addresses two issues:. 1. Improved and more robust parsing of FlowBasedReads. Specifically, the code now determines the minimal reportable quality; 2. New tool AddFlowSNVQuality that allows users to convert the flow-based quality format when every base quality reports probability of an insertion or deletion to a conventional format that gives base qualities (total probability of mismatch and probability of each mismatch in separate tags). . We believe that this tool is going to be important for users of the Ultima Genomics data that care about calling SNVs, especially in somatic setting, so the goal was to make documentation more accessible. . Happy to receive feedback about it",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8697
https://github.com/broadinstitute/gatk/pull/8697:686,Usability,feedback,feedback,686,"The pull request addresses two issues:. 1. Improved and more robust parsing of FlowBasedReads. Specifically, the code now determines the minimal reportable quality; 2. New tool AddFlowSNVQuality that allows users to convert the flow-based quality format when every base quality reports probability of an insertion or deletion to a conventional format that gives base qualities (total probability of mismatch and probability of each mismatch in separate tags). . We believe that this tool is going to be important for users of the Ultima Genomics data that care about calling SNVs, especially in somatic setting, so the goal was to make documentation more accessible. . Happy to receive feedback about it",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8697
https://github.com/broadinstitute/gatk/issues/8699:147,Deployability,pipeline,pipeline,147,"## Bug Report. ### Affected tool(s) or class(es); Mutect2. ### Affected version(s); 4.4.0.0. ### Description ; Hello GATK team,. I am working on a pipeline that calls variants using Mutect2 and uses those mutations for somatic variant calling in other samples. To achieve this I'm using the -allele flag in Mutect2 to pass the desired positions to genotype to Mutect2. However it appears that Mutect2 will output variants that it then is unable to parse as an -allele file. I believe this is because the REF column is too long. #### Steps to reproduce; Try to variant call using an -allele VCF containing this line:. ```; 5	283041	.	AGAAGACTCGGGGAGGAGCTGAGGTTCTAGTTTGAGGGTCGTGCACCTGGAGAACTGGACAGGAGCTGATGTTCTAGATTGAGCATCGTACAGCTGAAGACTTGGGGAGGAGCTTATGTTGTTCACTTTGAGGGTCTTTCAGCTGGAGACTCAGGCAGGAGCTGATGTTCTAGTTTGAGGATCTCGTAGCTGCAGAATCAGAGAGGAGCTGATGTTCTAGATTGAGGATCTTGTAGCTACAGACCCATAGAGGAGCTGATGATCTAGATTCAGGGTCATGCAGCT	A	.	.	AS_SB_TABLE=57,54|8,7;DP=126;ECNT=1;MBQ=30,31;MFRL=358,237;MMQ=60,60;MPOS=15;POPAF=7.30;TLOD=3.22	GT:AD:AF:DP:F1R2:F2R1:FAD:SB	0/1:111,15:0.028:126:17,1:18,5:86,9:57,54,8,7; ```. #### Expected behavior; Mutect2 should either not emit an invalid variant or it should be able to parse it; #### Actual behavior. ```; java.lang.ArrayIndexOutOfBoundsException: arraycopy: source index -152 out of bounds for byte[278]; 	at java.base/java.lang.System.arraycopy(Native Method); 	at java.base/java.util.Arrays.copyOfRange(Arrays.java:3823); 	at org.broadinstitute.hellbender.tools.walkers.annotator.TandemRepeat.getNumTandemRepeatUnits(TandemRepeat.java:54); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyRegionTrimmer.trim(AssemblyRegionTrimmer.java:189); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2Engine.callRegion(Mutect2Engine.java:273); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2.apply(Mutect2.java:304); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:200",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8699
https://github.com/broadinstitute/gatk/pull/8702:442,Security,Validat,Validate,442,Generate Avro Files run: https://app.terra.bio/#workspaces/gvs-dev/RSA%20-%20GVS%20Quickstart%20V2%20/job_history/577e77f2-4174-46c0-9d8a-9fdbbd2a27ed. Generate VDS run: https://app.terra.bio/#workspaces/gvs-dev/RSA%20-%20GVS%20Quickstart%20V2%20/job_history/27299041-7779-49d7-b7d3-6c0349f5ffc3. Generate VAT run: https://app.terra.bio/#workspaces/gvs-dev/RSA%20-%20GVS%20Quickstart%20V2%20/job_history/ad1b2687-7df0-4e92-b4e2-b61fcb77cd19. Validate VAT run: https://app.terra.bio/#workspaces/gvs-dev/RSA%20-%20GVS%20Quickstart%20V2%20/job_history/dae4874f-a619-40d4-84b4-066295b76cb2,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8702
https://github.com/broadinstitute/gatk/issues/8703:445,Security,access,access,445,"## Feature request. ### Tool(s) or class(es) involved; src/main/java/org/broadinstitute/hellbender/tools/walkers/rnaseq/SplitNCigarReads.java. ### Description. Modification requested. When splitting a read based on the spliced RNA cigar (N cigar symbol), the new split read fragment that gets incorporated into the bam file has a new 'HC' identifier for the read name, and the original read name is lost. There are cases where we really need to access the original read name. Would it be possible to incorporate the original read name somewhere into the read alignment record, perhaps as a custom SAM tag or attribute, or as a prefix or suffix to the HC-identifier in the read name?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8703
https://github.com/broadinstitute/gatk/pull/8707:147,Deployability,update,updates,147,"- uses GvsExtractCohortFromSampleNames.wdl to generate VCFs for calculating P & S all in one WDL; - allows for use of an interval_list in P & S; - updates to docs; - Successful run here: https://app.terra.bio/#workspaces/gvs-dev/RSA%20-%20GVS%20Quickstart%20V2%20/job_history/2537ea7b-f635-4609-8fbb-7eaec41a6df8; - integration run, since I touched the bulk ingest and extract VCF WDLs: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/f71941ba-f02f-4472-a04c-aedff24fdd14",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8707
https://github.com/broadinstitute/gatk/pull/8707:316,Deployability,integrat,integration,316,"- uses GvsExtractCohortFromSampleNames.wdl to generate VCFs for calculating P & S all in one WDL; - allows for use of an interval_list in P & S; - updates to docs; - Successful run here: https://app.terra.bio/#workspaces/gvs-dev/RSA%20-%20GVS%20Quickstart%20V2%20/job_history/2537ea7b-f635-4609-8fbb-7eaec41a6df8; - integration run, since I touched the bulk ingest and extract VCF WDLs: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/f71941ba-f02f-4472-a04c-aedff24fdd14",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8707
https://github.com/broadinstitute/gatk/pull/8707:316,Integrability,integrat,integration,316,"- uses GvsExtractCohortFromSampleNames.wdl to generate VCFs for calculating P & S all in one WDL; - allows for use of an interval_list in P & S; - updates to docs; - Successful run here: https://app.terra.bio/#workspaces/gvs-dev/RSA%20-%20GVS%20Quickstart%20V2%20/job_history/2537ea7b-f635-4609-8fbb-7eaec41a6df8; - integration run, since I touched the bulk ingest and extract VCF WDLs: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/f71941ba-f02f-4472-a04c-aedff24fdd14",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8707
https://github.com/broadinstitute/gatk/pull/8708:4866,Availability,failure,failures,4866,"wo different write modes. When `WRITE_AND_COPY` is selected, a temporary .pgen file is created and written to during the running of the tool, and then once all records have been written, a new file is created with the index at the top and the contents of the temporary .pgen file appended to it. When `WRITE_SEPARATE_INDEX` is selected, the index is instead written to a separate .pgi file. The default is `WRITE_AND_COPY`. #### max-alt-alleles; The PGEN format can only support up to 254 alt alleles per site. This argument allows you to specify a limit. The default is the max of 254. Any sites with more alt alleles than the specified max will not be written. #### lenient-ploidy-validation; PGEN is a bit quirky in that it requires samples to be diploid but has a special case for sex chromosomes, which are allowed to be haploid. By default, any attempt to write a record with an unsupported ploidy will result in an exception being thrown. If this flag is used, then ploidy failures will instead be logged and the records will be written as missing. #### writer-log-file; The C++ code in the PGEN writer in PGEN-JNI will log sites that exceed max-alt-alleles and with unsupported ploidy (if lenient-ploidy-validation is set) to the specified log file, if this argument is set. #### allow-empty-pgen; Empty PGEN files are not technically valid PGEN files. However, for parallel processing purposes, it is sometimes helpful to allow the creation of empty files when there are no variants to be written. The GvsExtractCallsetPgenMerged workflow relies on this. If this flag is set and no variants are written, an empty .pgen, .psam, and .pvar.zst file will be written in `onShutdown()`. By default (i.e. if this flag is not set), if there are no variants written, an exception will be thrown. . ### Part 3: GvsExtractCallsetPgenMerged; GvsExtractCallsetPgenMerged is a WDL workflow that calls ExtractCohortToPgen to extract data from GVS and write it to PGEN files, and then merges those PGEN file",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8708
https://github.com/broadinstitute/gatk/pull/8708:10454,Availability,failure,failures,10454,"rs specific to PGEN extract. ### Large scale testing; All of my testing (with the exception of some very small scale stuff early on) has been done in the [GVS_AoU_PGEN_Extract_Development Terra workspace](https://app.terra.bio/#workspaces/allofus-drc-prod-auxiliary/GVS_AoU_PGEN_Extract_Development) and using data from the GVS Delta callset (aou-genomics-curation-prod.aou_wgs_fullref_v2). My test process (for the majority of tests) has been as follows:. 1. Select a list of sample names from aou_wgs_fullref_v2.sample_info (excluding control samples and sample 3224672 because of the data issue mentioned [here](https://broadinstitute.slack.com/archives/CJRLP6ZSA/p1699026273329339)).; 2. Use that list as an input to GvsPrepareRangesCallset to create a cohort of test data in a separate BigQuery dataset (aou-genomics-curation-prod.klydon_pgen_extract_test).; 3. Run GvsExtractCallsetPgen on the newly created cohort. (I would just run GvsExtractCallsetPgenMerged, but I like using Workflow Dashboard to monitor how the job is going and dig into it if there are any failures. Workflow Dashboard doesn't seem to let you dig into individual tasks for workflows with sub-workflows, so it wouldn't allow me to look at individual shards running ExtractTask if I ran GvsExtractCallsetPgenMerged. Job Manager would be an alternative for this, but it seems to be pretty much unusable for even moderately-sized jobs.); 4. Run GvsExtractCallset on the newly created cohort, making sure to use the same parameters, including scatter count. This will generate VCF files that we can use to compare to the PGEN files created during the previous step for validation.; 5. Run GvsExtractCallsetPgenMerged with the same parameters used to run GvsExtractCallsetPgen in Step 3. This will use call-caching for the extract steps and then merge the PGEN files by chromosome. (Running it this way is maybe not the ideal way to do this, but it's what I've been doing for reasons described in the parenthetical in Step 3).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8708
https://github.com/broadinstitute/gatk/pull/8708:13877,Deployability,release,release,13877,"rated by GvsExtractCallset. . It ignores basically everything except genotypes, because PGENs do not store all the other fields and annotations that the VCFs might have. It will also skip over any sites in the VCFs with >254 alleles because those will not be present in the PGEN files. Any differences are written to diff files, in the form of the differing lines in the VCFs being compared. The code for this comparison tool lives [here](https://github.com/KevinCLydon/pgen_vcf_comparator) in a repo I created under my GitHub account. (I didn't create it under the Broad org because it's sort of half-baked and bad and not actually meant to be used by anyone other than me.) I don't know if y'all want to continue using this tool, but I'm happy to discuss it more if it would actually be useful to you. ## To-dos / caveats. ### PGEN-JNI; The version of PGEN-JNI I'm referencing in the current build.gradle file is a beta version that is hosted on artifactory. Functionally, this is totally fine, but we want to get a 1.0 version of it hosted publicly. Chris Norman, who developed the tool is currently very working on this and very close to done. Once he's completed this, I want to run a sanity test or two against a small subset of the Delta callset just to make sure everything is functioning as intended. ### Merging by chromosome arm; Right now, the last step of the PGEN extract workflow merges the PGEN files by contig name, so the final result is one trio of files (.pgen, .psam, and .pvar.zst) per chromosome. There was discussion about changing this to merge instead by chromosome arm. I want to make this change, but it's not super simple, so I've prioritized getting this version of the code ready for merging before tackling that. ### The PGEN format; As I mentioned above, Plink 2.0 and the PGEN file format are still not in full release, so the format could be subject to change in the future, which will require updates to our PGEN writing code and could possibly introduce problems.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8708
https://github.com/broadinstitute/gatk/pull/8708:13961,Deployability,update,updates,13961,"rated by GvsExtractCallset. . It ignores basically everything except genotypes, because PGENs do not store all the other fields and annotations that the VCFs might have. It will also skip over any sites in the VCFs with >254 alleles because those will not be present in the PGEN files. Any differences are written to diff files, in the form of the differing lines in the VCFs being compared. The code for this comparison tool lives [here](https://github.com/KevinCLydon/pgen_vcf_comparator) in a repo I created under my GitHub account. (I didn't create it under the Broad org because it's sort of half-baked and bad and not actually meant to be used by anyone other than me.) I don't know if y'all want to continue using this tool, but I'm happy to discuss it more if it would actually be useful to you. ## To-dos / caveats. ### PGEN-JNI; The version of PGEN-JNI I'm referencing in the current build.gradle file is a beta version that is hosted on artifactory. Functionally, this is totally fine, but we want to get a 1.0 version of it hosted publicly. Chris Norman, who developed the tool is currently very working on this and very close to done. Once he's completed this, I want to run a sanity test or two against a small subset of the Delta callset just to make sure everything is functioning as intended. ### Merging by chromosome arm; Right now, the last step of the PGEN extract workflow merges the PGEN files by contig name, so the final result is one trio of files (.pgen, .psam, and .pvar.zst) per chromosome. There was discussion about changing this to merge instead by chromosome arm. I want to make this change, but it's not super simple, so I've prioritized getting this version of the code ready for merging before tackling that. ### The PGEN format; As I mentioned above, Plink 2.0 and the PGEN file format are still not in full release, so the format could be subject to change in the future, which will require updates to our PGEN writing code and could possibly introduce problems.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8708
https://github.com/broadinstitute/gatk/pull/8708:10392,Energy Efficiency,monitor,monitor,10392,"rs specific to PGEN extract. ### Large scale testing; All of my testing (with the exception of some very small scale stuff early on) has been done in the [GVS_AoU_PGEN_Extract_Development Terra workspace](https://app.terra.bio/#workspaces/allofus-drc-prod-auxiliary/GVS_AoU_PGEN_Extract_Development) and using data from the GVS Delta callset (aou-genomics-curation-prod.aou_wgs_fullref_v2). My test process (for the majority of tests) has been as follows:. 1. Select a list of sample names from aou_wgs_fullref_v2.sample_info (excluding control samples and sample 3224672 because of the data issue mentioned [here](https://broadinstitute.slack.com/archives/CJRLP6ZSA/p1699026273329339)).; 2. Use that list as an input to GvsPrepareRangesCallset to create a cohort of test data in a separate BigQuery dataset (aou-genomics-curation-prod.klydon_pgen_extract_test).; 3. Run GvsExtractCallsetPgen on the newly created cohort. (I would just run GvsExtractCallsetPgenMerged, but I like using Workflow Dashboard to monitor how the job is going and dig into it if there are any failures. Workflow Dashboard doesn't seem to let you dig into individual tasks for workflows with sub-workflows, so it wouldn't allow me to look at individual shards running ExtractTask if I ran GvsExtractCallsetPgenMerged. Job Manager would be an alternative for this, but it seems to be pretty much unusable for even moderately-sized jobs.); 4. Run GvsExtractCallset on the newly created cohort, making sure to use the same parameters, including scatter count. This will generate VCF files that we can use to compare to the PGEN files created during the previous step for validation.; 5. Run GvsExtractCallsetPgenMerged with the same parameters used to run GvsExtractCallsetPgen in Step 3. This will use call-caching for the extract steps and then merge the PGEN files by chromosome. (Running it this way is maybe not the ideal way to do this, but it's what I've been doing for reasons described in the parenthetical in Step 3).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8708
https://github.com/broadinstitute/gatk/pull/8708:2664,Integrability,depend,dependency,2664," 3 parts:; 1. The PGEN-JNI, a C++/JNI library that handles writing HTSJDK VariantContext objects to PGEN files,; 2. ExtractCohortToPgen, a GATK tool based on ExtractCohortToVcf that processes VariantContexts and passes them to PGEN-JNI for writing, and; 3. GvsExtractCallsetPgenMerged, a WDL workflow based on GvsExtractCallset that uses ExtractCohortToPgen to write a series of PGEN files and then merges them by chromosome. ### Part 1: PGEN-JNI; The PGEN-JNI library was written by Chris Norman of the GATK Engine Team and lives [here](https://github.com/broadinstitute/pgen-jni). It is written primarily in C++ for performance purposes and also compatibility with the pgenlib library (part of the [plink repo](https://github.com/chrchang/plink-ng/tree/master)). It builds on top of pgenlib to provide a writer for creating PGEN files and writing to them from HTSJDK VariantContext objects. PGEN-JNI is compatible with Linux and macOS. A build of this library is currently hosted on the Broad's artifactory repo, and that is being used as a dependency for GATK. Ownership of the PGEN-JNI library will stay with the GATK Engine Team and we will provide support for it if y'all encounter any issues with it. ### Part 2: ExtractCohortToPgen; ExtractCohortToPgen is a GATK tool that inherits from ExtractCohort and is based very closely on ExtractCohortToVcf. It produces 3-4 files:. 1. A `.pgen` file, which contains a mapping of samples and sites to variants,; 2. A `.psam` file, which contains a list of sample names,; 3. A `.pvar.zst` file, which is a zstd compressed list of sites with alleles, similar to a sites-only VCF, and; 4. Optionally (if specified by setting `write-mode` to `WRITE_SEPARATE_INDEX`), a `.pgi` file, which contains an index for the `.pgen` file. It has a few arguments that are specific to it that warrant explanation. #### pgen-chromosome-code; Plink defines a set of [chromosome codes](https://www.cog-genomics.org/plink/2.0/data#irreg_output) that correspond to differen",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8708
https://github.com/broadinstitute/gatk/pull/8708:2902,Modifiability,inherit,inherits,2902,"ed, a WDL workflow based on GvsExtractCallset that uses ExtractCohortToPgen to write a series of PGEN files and then merges them by chromosome. ### Part 1: PGEN-JNI; The PGEN-JNI library was written by Chris Norman of the GATK Engine Team and lives [here](https://github.com/broadinstitute/pgen-jni). It is written primarily in C++ for performance purposes and also compatibility with the pgenlib library (part of the [plink repo](https://github.com/chrchang/plink-ng/tree/master)). It builds on top of pgenlib to provide a writer for creating PGEN files and writing to them from HTSJDK VariantContext objects. PGEN-JNI is compatible with Linux and macOS. A build of this library is currently hosted on the Broad's artifactory repo, and that is being used as a dependency for GATK. Ownership of the PGEN-JNI library will stay with the GATK Engine Team and we will provide support for it if y'all encounter any issues with it. ### Part 2: ExtractCohortToPgen; ExtractCohortToPgen is a GATK tool that inherits from ExtractCohort and is based very closely on ExtractCohortToVcf. It produces 3-4 files:. 1. A `.pgen` file, which contains a mapping of samples and sites to variants,; 2. A `.psam` file, which contains a list of sample names,; 3. A `.pvar.zst` file, which is a zstd compressed list of sites with alleles, similar to a sites-only VCF, and; 4. Optionally (if specified by setting `write-mode` to `WRITE_SEPARATE_INDEX`), a `.pgi` file, which contains an index for the `.pgen` file. It has a few arguments that are specific to it that warrant explanation. #### pgen-chromosome-code; Plink defines a set of [chromosome codes](https://www.cog-genomics.org/plink/2.0/data#irreg_output) that correspond to different sets of contig names of chromosomes. This tool supports two of those chromosome code options: `chrM` and `MT`, which correspond to hg38 and hg19 contig naming, respectively. This argument is required. #### write-mode; The PGEN writer defined in PGEN-JNI defines two different writ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8708
https://github.com/broadinstitute/gatk/pull/8708:785,Performance,perform,performance,785,"# Extracting PGEN from GVS. ## The PGEN format; PGEN is a format written for and used by version 2 of [PLINK](https://www.cog-genomics.org/plink/2.0/). ***IT IS VERY IMPORTANT TO NOTE THAT VERSION 2 OF PLINK IS STILL IN ALPHA AND THE PGEN FORMAT IS STILL SUBJECT TO CHANGE.*** The format comprises 3 file types (or actually sometimes 4):; 1. A `.pgen` file. This is a binary file that stores a mapping of samples and sites to genotypes in a very cleverly compressed way that I can't explain super well because it's complicated.; 2. A `.pvar` file. This is essentially a sites-only VCF. It has information for each site referenced in the `.pgen` file. PLINK also provides an option to produce/use a zstd compressed version of the file (`.pvar.zst`), and we have opted to write that for performance purposes.; 3. A `.psam` file. This is a plaintext file that contains a list of samples referenced in the `.pgen` file. It also optionally includes some phenotype data.; 4. Optionally, a `.pgi` file. Typically, a `.pgen` file has an index at the top. Optionally, PLINK supports using a `.pgen` file with an index in a separate `.pgi` file. The PGEN format does not store all of the information that a VCF has. It leaves out a lot of the fields and annotations you can store in a VCF. As a result of this and the clever compression in the `.pgen` file, these files are typically much smaller than equivalent VCFs. For more information on the PGEN file format, see the official spec [here](https://github.com/chrchang/plink-ng/blob/master/pgen_spec/pgen_spec.pdf). ## The code; The code for the PGEN extract can be divided into 3 parts:; 1. The PGEN-JNI, a C++/JNI library that handles writing HTSJDK VariantContext objects to PGEN files,; 2. ExtractCohortToPgen, a GATK tool based on ExtractCohortToVcf that processes VariantContexts and passes them to PGEN-JNI for writing, and; 3. GvsExtractCallsetPgenMerged, a WDL workflow based on GvsExtractCallset that uses ExtractCohortToPgen to write a series of P",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8708
https://github.com/broadinstitute/gatk/pull/8708:2239,Performance,perform,performance,2239,"CF. As a result of this and the clever compression in the `.pgen` file, these files are typically much smaller than equivalent VCFs. For more information on the PGEN file format, see the official spec [here](https://github.com/chrchang/plink-ng/blob/master/pgen_spec/pgen_spec.pdf). ## The code; The code for the PGEN extract can be divided into 3 parts:; 1. The PGEN-JNI, a C++/JNI library that handles writing HTSJDK VariantContext objects to PGEN files,; 2. ExtractCohortToPgen, a GATK tool based on ExtractCohortToVcf that processes VariantContexts and passes them to PGEN-JNI for writing, and; 3. GvsExtractCallsetPgenMerged, a WDL workflow based on GvsExtractCallset that uses ExtractCohortToPgen to write a series of PGEN files and then merges them by chromosome. ### Part 1: PGEN-JNI; The PGEN-JNI library was written by Chris Norman of the GATK Engine Team and lives [here](https://github.com/broadinstitute/pgen-jni). It is written primarily in C++ for performance purposes and also compatibility with the pgenlib library (part of the [plink repo](https://github.com/chrchang/plink-ng/tree/master)). It builds on top of pgenlib to provide a writer for creating PGEN files and writing to them from HTSJDK VariantContext objects. PGEN-JNI is compatible with Linux and macOS. A build of this library is currently hosted on the Broad's artifactory repo, and that is being used as a dependency for GATK. Ownership of the PGEN-JNI library will stay with the GATK Engine Team and we will provide support for it if y'all encounter any issues with it. ### Part 2: ExtractCohortToPgen; ExtractCohortToPgen is a GATK tool that inherits from ExtractCohort and is based very closely on ExtractCohortToVcf. It produces 3-4 files:. 1. A `.pgen` file, which contains a mapping of samples and sites to variants,; 2. A `.psam` file, which contains a list of sample names,; 3. A `.pvar.zst` file, which is a zstd compressed list of sites with alleles, similar to a sites-only VCF, and; 4. Optionally (if speci",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8708
https://github.com/broadinstitute/gatk/pull/8708:8438,Performance,perform,performance,8438,"l list tar and a list of the filenames within the tar (ordered to match the pgen/psam/pvar files so they can be matched up to each other more easily). #### Step 2: SplitFilesByChromosome; This is a short task defined within GvsExtractCallsetPgenMerged. It loops through the interval list files in the interval list tar and extracts the contig name from each. Then, going by that contig name, it writes the names of the corresponding .pgen, .psam, and .pvar.zst files to list files named for the corresponding contigs. . The output of this task is 3 arrays of files, containing names for .pgen, .psam, and .pvar.zst files which contain sites for each contig. For example, the file `chr1.pgen_list` would contain a list of all the .pgen files that contain variants for sites on chromosome 1. #### Step 3: MergePgenHierarchical; This workflow accepts a list of .pgen, .psam, and .pvar.zst files and merges them all into one file using Plink's `--pmerge-list` functionality. For performance purposes, the merging is done in two stages. One big, monolithic merge takes too long. First, the file lists are sorted by index (the index in the filename). Plink does not like (i.e. does not support) merging files with overlapping intervals, so the files need to be sorted so that when they are merged in stages, we do not create merged files with intervals that overlap. Next, the lists are split, so we can parallelize the merge. Each of the split lists is then sent to a task that merges the files using Plink. Then, this is repeated to merge all the resulting files into one. The output is a single .pgen file, .psam file, and .pvar.zst file containing all the extracted data for the corresponding chromosome. ## Testing. ### Unit tests; Unit tests for the ExtractCohortToPgen tool exist in the ExtractCohortToPgenTest file. They are based closely on the tests in ExtractCohortToVcfTest, with a few extras to account for behaviors specific to PGEN extract. ### Large scale testing; All of my testing (with th",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8708
https://github.com/broadinstitute/gatk/pull/8708:4569,Security,validat,validation,4569,"tig names of chromosomes. This tool supports two of those chromosome code options: `chrM` and `MT`, which correspond to hg38 and hg19 contig naming, respectively. This argument is required. #### write-mode; The PGEN writer defined in PGEN-JNI defines two different write modes. When `WRITE_AND_COPY` is selected, a temporary .pgen file is created and written to during the running of the tool, and then once all records have been written, a new file is created with the index at the top and the contents of the temporary .pgen file appended to it. When `WRITE_SEPARATE_INDEX` is selected, the index is instead written to a separate .pgi file. The default is `WRITE_AND_COPY`. #### max-alt-alleles; The PGEN format can only support up to 254 alt alleles per site. This argument allows you to specify a limit. The default is the max of 254. Any sites with more alt alleles than the specified max will not be written. #### lenient-ploidy-validation; PGEN is a bit quirky in that it requires samples to be diploid but has a special case for sex chromosomes, which are allowed to be haploid. By default, any attempt to write a record with an unsupported ploidy will result in an exception being thrown. If this flag is used, then ploidy failures will instead be logged and the records will be written as missing. #### writer-log-file; The C++ code in the PGEN writer in PGEN-JNI will log sites that exceed max-alt-alleles and with unsupported ploidy (if lenient-ploidy-validation is set) to the specified log file, if this argument is set. #### allow-empty-pgen; Empty PGEN files are not technically valid PGEN files. However, for parallel processing purposes, it is sometimes helpful to allow the creation of empty files when there are no variants to be written. The GvsExtractCallsetPgenMerged workflow relies on this. If this flag is set and no variants are written, an empty .pgen, .psam, and .pvar.zst file will be written in `onShutdown()`. By default (i.e. if this flag is not set), if there are no",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8708
https://github.com/broadinstitute/gatk/pull/8708:5098,Security,validat,validation,5098,"have been written, a new file is created with the index at the top and the contents of the temporary .pgen file appended to it. When `WRITE_SEPARATE_INDEX` is selected, the index is instead written to a separate .pgi file. The default is `WRITE_AND_COPY`. #### max-alt-alleles; The PGEN format can only support up to 254 alt alleles per site. This argument allows you to specify a limit. The default is the max of 254. Any sites with more alt alleles than the specified max will not be written. #### lenient-ploidy-validation; PGEN is a bit quirky in that it requires samples to be diploid but has a special case for sex chromosomes, which are allowed to be haploid. By default, any attempt to write a record with an unsupported ploidy will result in an exception being thrown. If this flag is used, then ploidy failures will instead be logged and the records will be written as missing. #### writer-log-file; The C++ code in the PGEN writer in PGEN-JNI will log sites that exceed max-alt-alleles and with unsupported ploidy (if lenient-ploidy-validation is set) to the specified log file, if this argument is set. #### allow-empty-pgen; Empty PGEN files are not technically valid PGEN files. However, for parallel processing purposes, it is sometimes helpful to allow the creation of empty files when there are no variants to be written. The GvsExtractCallsetPgenMerged workflow relies on this. If this flag is set and no variants are written, an empty .pgen, .psam, and .pvar.zst file will be written in `onShutdown()`. By default (i.e. if this flag is not set), if there are no variants written, an exception will be thrown. . ### Part 3: GvsExtractCallsetPgenMerged; GvsExtractCallsetPgenMerged is a WDL workflow that calls ExtractCohortToPgen to extract data from GVS and write it to PGEN files, and then merges those PGEN files by chromosome. This workflow has 3 steps:. #### Step 1: GvsExtractCallsetPgen; This is a workflow based very closely on the GvsExtractCallset workflow (which is used f",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8708
https://github.com/broadinstitute/gatk/pull/8708:11028,Security,validat,validation,11028,"ssue mentioned [here](https://broadinstitute.slack.com/archives/CJRLP6ZSA/p1699026273329339)).; 2. Use that list as an input to GvsPrepareRangesCallset to create a cohort of test data in a separate BigQuery dataset (aou-genomics-curation-prod.klydon_pgen_extract_test).; 3. Run GvsExtractCallsetPgen on the newly created cohort. (I would just run GvsExtractCallsetPgenMerged, but I like using Workflow Dashboard to monitor how the job is going and dig into it if there are any failures. Workflow Dashboard doesn't seem to let you dig into individual tasks for workflows with sub-workflows, so it wouldn't allow me to look at individual shards running ExtractTask if I ran GvsExtractCallsetPgenMerged. Job Manager would be an alternative for this, but it seems to be pretty much unusable for even moderately-sized jobs.); 4. Run GvsExtractCallset on the newly created cohort, making sure to use the same parameters, including scatter count. This will generate VCF files that we can use to compare to the PGEN files created during the previous step for validation.; 5. Run GvsExtractCallsetPgenMerged with the same parameters used to run GvsExtractCallsetPgen in Step 3. This will use call-caching for the extract steps and then merge the PGEN files by chromosome. (Running it this way is maybe not the ideal way to do this, but it's what I've been doing for reasons described in the parenthetical in Step 3).; 6. Create list files, by file type, containing the gs:// URIs for the .pgen, .psam, and .pvar.zst files created in Step 3, along with the .vcf.gz files created in Step 4. Upload them to the workspace to use for validation.; 7. Run ComparePgenAndVcfScatter using the file lists as inputs. If there are any differences, it will output files that contain those differences. If there are no diff files generated, the files match. ComparePgenAndVcfScatter is a workflow I wrote that converts a list of .pgen, .psam, and .pvar.zst files generated by GvsExtractCallsetPgen into .vcf.gz files and the",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8708
https://github.com/broadinstitute/gatk/pull/8708:11597,Security,validat,validation,11597,"allow me to look at individual shards running ExtractTask if I ran GvsExtractCallsetPgenMerged. Job Manager would be an alternative for this, but it seems to be pretty much unusable for even moderately-sized jobs.); 4. Run GvsExtractCallset on the newly created cohort, making sure to use the same parameters, including scatter count. This will generate VCF files that we can use to compare to the PGEN files created during the previous step for validation.; 5. Run GvsExtractCallsetPgenMerged with the same parameters used to run GvsExtractCallsetPgen in Step 3. This will use call-caching for the extract steps and then merge the PGEN files by chromosome. (Running it this way is maybe not the ideal way to do this, but it's what I've been doing for reasons described in the parenthetical in Step 3).; 6. Create list files, by file type, containing the gs:// URIs for the .pgen, .psam, and .pvar.zst files created in Step 3, along with the .vcf.gz files created in Step 4. Upload them to the workspace to use for validation.; 7. Run ComparePgenAndVcfScatter using the file lists as inputs. If there are any differences, it will output files that contain those differences. If there are no diff files generated, the files match. ComparePgenAndVcfScatter is a workflow I wrote that converts a list of .pgen, .psam, and .pvar.zst files generated by GvsExtractCallsetPgen into .vcf.gz files and then compares those files to a list of .vcf.gz files generated by GvsExtractCallset. . It ignores basically everything except genotypes, because PGENs do not store all the other fields and annotations that the VCFs might have. It will also skip over any sites in the VCFs with >254 alleles because those will not be present in the PGEN files. Any differences are written to diff files, in the form of the differing lines in the VCFs being compared. The code for this comparison tool lives [here](https://github.com/KevinCLydon/pgen_vcf_comparator) in a repo I created under my GitHub account. (I didn't crea",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8708
https://github.com/broadinstitute/gatk/pull/8708:4891,Testability,log,logged,4891,"wo different write modes. When `WRITE_AND_COPY` is selected, a temporary .pgen file is created and written to during the running of the tool, and then once all records have been written, a new file is created with the index at the top and the contents of the temporary .pgen file appended to it. When `WRITE_SEPARATE_INDEX` is selected, the index is instead written to a separate .pgi file. The default is `WRITE_AND_COPY`. #### max-alt-alleles; The PGEN format can only support up to 254 alt alleles per site. This argument allows you to specify a limit. The default is the max of 254. Any sites with more alt alleles than the specified max will not be written. #### lenient-ploidy-validation; PGEN is a bit quirky in that it requires samples to be diploid but has a special case for sex chromosomes, which are allowed to be haploid. By default, any attempt to write a record with an unsupported ploidy will result in an exception being thrown. If this flag is used, then ploidy failures will instead be logged and the records will be written as missing. #### writer-log-file; The C++ code in the PGEN writer in PGEN-JNI will log sites that exceed max-alt-alleles and with unsupported ploidy (if lenient-ploidy-validation is set) to the specified log file, if this argument is set. #### allow-empty-pgen; Empty PGEN files are not technically valid PGEN files. However, for parallel processing purposes, it is sometimes helpful to allow the creation of empty files when there are no variants to be written. The GvsExtractCallsetPgenMerged workflow relies on this. If this flag is set and no variants are written, an empty .pgen, .psam, and .pvar.zst file will be written in `onShutdown()`. By default (i.e. if this flag is not set), if there are no variants written, an exception will be thrown. . ### Part 3: GvsExtractCallsetPgenMerged; GvsExtractCallsetPgenMerged is a WDL workflow that calls ExtractCohortToPgen to extract data from GVS and write it to PGEN files, and then merges those PGEN file",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8708
https://github.com/broadinstitute/gatk/pull/8708:4954,Testability,log,log-file,4954,"have been written, a new file is created with the index at the top and the contents of the temporary .pgen file appended to it. When `WRITE_SEPARATE_INDEX` is selected, the index is instead written to a separate .pgi file. The default is `WRITE_AND_COPY`. #### max-alt-alleles; The PGEN format can only support up to 254 alt alleles per site. This argument allows you to specify a limit. The default is the max of 254. Any sites with more alt alleles than the specified max will not be written. #### lenient-ploidy-validation; PGEN is a bit quirky in that it requires samples to be diploid but has a special case for sex chromosomes, which are allowed to be haploid. By default, any attempt to write a record with an unsupported ploidy will result in an exception being thrown. If this flag is used, then ploidy failures will instead be logged and the records will be written as missing. #### writer-log-file; The C++ code in the PGEN writer in PGEN-JNI will log sites that exceed max-alt-alleles and with unsupported ploidy (if lenient-ploidy-validation is set) to the specified log file, if this argument is set. #### allow-empty-pgen; Empty PGEN files are not technically valid PGEN files. However, for parallel processing purposes, it is sometimes helpful to allow the creation of empty files when there are no variants to be written. The GvsExtractCallsetPgenMerged workflow relies on this. If this flag is set and no variants are written, an empty .pgen, .psam, and .pvar.zst file will be written in `onShutdown()`. By default (i.e. if this flag is not set), if there are no variants written, an exception will be thrown. . ### Part 3: GvsExtractCallsetPgenMerged; GvsExtractCallsetPgenMerged is a WDL workflow that calls ExtractCohortToPgen to extract data from GVS and write it to PGEN files, and then merges those PGEN files by chromosome. This workflow has 3 steps:. #### Step 1: GvsExtractCallsetPgen; This is a workflow based very closely on the GvsExtractCallset workflow (which is used f",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8708
https://github.com/broadinstitute/gatk/pull/8708:5013,Testability,log,log,5013,"have been written, a new file is created with the index at the top and the contents of the temporary .pgen file appended to it. When `WRITE_SEPARATE_INDEX` is selected, the index is instead written to a separate .pgi file. The default is `WRITE_AND_COPY`. #### max-alt-alleles; The PGEN format can only support up to 254 alt alleles per site. This argument allows you to specify a limit. The default is the max of 254. Any sites with more alt alleles than the specified max will not be written. #### lenient-ploidy-validation; PGEN is a bit quirky in that it requires samples to be diploid but has a special case for sex chromosomes, which are allowed to be haploid. By default, any attempt to write a record with an unsupported ploidy will result in an exception being thrown. If this flag is used, then ploidy failures will instead be logged and the records will be written as missing. #### writer-log-file; The C++ code in the PGEN writer in PGEN-JNI will log sites that exceed max-alt-alleles and with unsupported ploidy (if lenient-ploidy-validation is set) to the specified log file, if this argument is set. #### allow-empty-pgen; Empty PGEN files are not technically valid PGEN files. However, for parallel processing purposes, it is sometimes helpful to allow the creation of empty files when there are no variants to be written. The GvsExtractCallsetPgenMerged workflow relies on this. If this flag is set and no variants are written, an empty .pgen, .psam, and .pvar.zst file will be written in `onShutdown()`. By default (i.e. if this flag is not set), if there are no variants written, an exception will be thrown. . ### Part 3: GvsExtractCallsetPgenMerged; GvsExtractCallsetPgenMerged is a WDL workflow that calls ExtractCohortToPgen to extract data from GVS and write it to PGEN files, and then merges those PGEN files by chromosome. This workflow has 3 steps:. #### Step 1: GvsExtractCallsetPgen; This is a workflow based very closely on the GvsExtractCallset workflow (which is used f",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8708
https://github.com/broadinstitute/gatk/pull/8708:5134,Testability,log,log,5134,"have been written, a new file is created with the index at the top and the contents of the temporary .pgen file appended to it. When `WRITE_SEPARATE_INDEX` is selected, the index is instead written to a separate .pgi file. The default is `WRITE_AND_COPY`. #### max-alt-alleles; The PGEN format can only support up to 254 alt alleles per site. This argument allows you to specify a limit. The default is the max of 254. Any sites with more alt alleles than the specified max will not be written. #### lenient-ploidy-validation; PGEN is a bit quirky in that it requires samples to be diploid but has a special case for sex chromosomes, which are allowed to be haploid. By default, any attempt to write a record with an unsupported ploidy will result in an exception being thrown. If this flag is used, then ploidy failures will instead be logged and the records will be written as missing. #### writer-log-file; The C++ code in the PGEN writer in PGEN-JNI will log sites that exceed max-alt-alleles and with unsupported ploidy (if lenient-ploidy-validation is set) to the specified log file, if this argument is set. #### allow-empty-pgen; Empty PGEN files are not technically valid PGEN files. However, for parallel processing purposes, it is sometimes helpful to allow the creation of empty files when there are no variants to be written. The GvsExtractCallsetPgenMerged workflow relies on this. If this flag is set and no variants are written, an empty .pgen, .psam, and .pvar.zst file will be written in `onShutdown()`. By default (i.e. if this flag is not set), if there are no variants written, an exception will be thrown. . ### Part 3: GvsExtractCallsetPgenMerged; GvsExtractCallsetPgenMerged is a WDL workflow that calls ExtractCohortToPgen to extract data from GVS and write it to PGEN files, and then merges those PGEN files by chromosome. This workflow has 3 steps:. #### Step 1: GvsExtractCallsetPgen; This is a workflow based very closely on the GvsExtractCallset workflow (which is used f",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8708
https://github.com/broadinstitute/gatk/pull/8708:9169,Testability,Test,Testing,9169,"in a list of all the .pgen files that contain variants for sites on chromosome 1. #### Step 3: MergePgenHierarchical; This workflow accepts a list of .pgen, .psam, and .pvar.zst files and merges them all into one file using Plink's `--pmerge-list` functionality. For performance purposes, the merging is done in two stages. One big, monolithic merge takes too long. First, the file lists are sorted by index (the index in the filename). Plink does not like (i.e. does not support) merging files with overlapping intervals, so the files need to be sorted so that when they are merged in stages, we do not create merged files with intervals that overlap. Next, the lists are split, so we can parallelize the merge. Each of the split lists is then sent to a task that merges the files using Plink. Then, this is repeated to merge all the resulting files into one. The output is a single .pgen file, .psam file, and .pvar.zst file containing all the extracted data for the corresponding chromosome. ## Testing. ### Unit tests; Unit tests for the ExtractCohortToPgen tool exist in the ExtractCohortToPgenTest file. They are based closely on the tests in ExtractCohortToVcfTest, with a few extras to account for behaviors specific to PGEN extract. ### Large scale testing; All of my testing (with the exception of some very small scale stuff early on) has been done in the [GVS_AoU_PGEN_Extract_Development Terra workspace](https://app.terra.bio/#workspaces/allofus-drc-prod-auxiliary/GVS_AoU_PGEN_Extract_Development) and using data from the GVS Delta callset (aou-genomics-curation-prod.aou_wgs_fullref_v2). My test process (for the majority of tests) has been as follows:. 1. Select a list of sample names from aou_wgs_fullref_v2.sample_info (excluding control samples and sample 3224672 because of the data issue mentioned [here](https://broadinstitute.slack.com/archives/CJRLP6ZSA/p1699026273329339)).; 2. Use that list as an input to GvsPrepareRangesCallset to create a cohort of test data in a separ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8708
https://github.com/broadinstitute/gatk/pull/8708:9187,Testability,test,tests,9187,"r sites on chromosome 1. #### Step 3: MergePgenHierarchical; This workflow accepts a list of .pgen, .psam, and .pvar.zst files and merges them all into one file using Plink's `--pmerge-list` functionality. For performance purposes, the merging is done in two stages. One big, monolithic merge takes too long. First, the file lists are sorted by index (the index in the filename). Plink does not like (i.e. does not support) merging files with overlapping intervals, so the files need to be sorted so that when they are merged in stages, we do not create merged files with intervals that overlap. Next, the lists are split, so we can parallelize the merge. Each of the split lists is then sent to a task that merges the files using Plink. Then, this is repeated to merge all the resulting files into one. The output is a single .pgen file, .psam file, and .pvar.zst file containing all the extracted data for the corresponding chromosome. ## Testing. ### Unit tests; Unit tests for the ExtractCohortToPgen tool exist in the ExtractCohortToPgenTest file. They are based closely on the tests in ExtractCohortToVcfTest, with a few extras to account for behaviors specific to PGEN extract. ### Large scale testing; All of my testing (with the exception of some very small scale stuff early on) has been done in the [GVS_AoU_PGEN_Extract_Development Terra workspace](https://app.terra.bio/#workspaces/allofus-drc-prod-auxiliary/GVS_AoU_PGEN_Extract_Development) and using data from the GVS Delta callset (aou-genomics-curation-prod.aou_wgs_fullref_v2). My test process (for the majority of tests) has been as follows:. 1. Select a list of sample names from aou_wgs_fullref_v2.sample_info (excluding control samples and sample 3224672 because of the data issue mentioned [here](https://broadinstitute.slack.com/archives/CJRLP6ZSA/p1699026273329339)).; 2. Use that list as an input to GvsPrepareRangesCallset to create a cohort of test data in a separate BigQuery dataset (aou-genomics-curation-prod.klydon_pg",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8708
https://github.com/broadinstitute/gatk/pull/8708:9199,Testability,test,tests,9199,"r sites on chromosome 1. #### Step 3: MergePgenHierarchical; This workflow accepts a list of .pgen, .psam, and .pvar.zst files and merges them all into one file using Plink's `--pmerge-list` functionality. For performance purposes, the merging is done in two stages. One big, monolithic merge takes too long. First, the file lists are sorted by index (the index in the filename). Plink does not like (i.e. does not support) merging files with overlapping intervals, so the files need to be sorted so that when they are merged in stages, we do not create merged files with intervals that overlap. Next, the lists are split, so we can parallelize the merge. Each of the split lists is then sent to a task that merges the files using Plink. Then, this is repeated to merge all the resulting files into one. The output is a single .pgen file, .psam file, and .pvar.zst file containing all the extracted data for the corresponding chromosome. ## Testing. ### Unit tests; Unit tests for the ExtractCohortToPgen tool exist in the ExtractCohortToPgenTest file. They are based closely on the tests in ExtractCohortToVcfTest, with a few extras to account for behaviors specific to PGEN extract. ### Large scale testing; All of my testing (with the exception of some very small scale stuff early on) has been done in the [GVS_AoU_PGEN_Extract_Development Terra workspace](https://app.terra.bio/#workspaces/allofus-drc-prod-auxiliary/GVS_AoU_PGEN_Extract_Development) and using data from the GVS Delta callset (aou-genomics-curation-prod.aou_wgs_fullref_v2). My test process (for the majority of tests) has been as follows:. 1. Select a list of sample names from aou_wgs_fullref_v2.sample_info (excluding control samples and sample 3224672 because of the data issue mentioned [here](https://broadinstitute.slack.com/archives/CJRLP6ZSA/p1699026273329339)).; 2. Use that list as an input to GvsPrepareRangesCallset to create a cohort of test data in a separate BigQuery dataset (aou-genomics-curation-prod.klydon_pg",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8708
https://github.com/broadinstitute/gatk/pull/8708:9311,Testability,test,tests,9311,"st files and merges them all into one file using Plink's `--pmerge-list` functionality. For performance purposes, the merging is done in two stages. One big, monolithic merge takes too long. First, the file lists are sorted by index (the index in the filename). Plink does not like (i.e. does not support) merging files with overlapping intervals, so the files need to be sorted so that when they are merged in stages, we do not create merged files with intervals that overlap. Next, the lists are split, so we can parallelize the merge. Each of the split lists is then sent to a task that merges the files using Plink. Then, this is repeated to merge all the resulting files into one. The output is a single .pgen file, .psam file, and .pvar.zst file containing all the extracted data for the corresponding chromosome. ## Testing. ### Unit tests; Unit tests for the ExtractCohortToPgen tool exist in the ExtractCohortToPgenTest file. They are based closely on the tests in ExtractCohortToVcfTest, with a few extras to account for behaviors specific to PGEN extract. ### Large scale testing; All of my testing (with the exception of some very small scale stuff early on) has been done in the [GVS_AoU_PGEN_Extract_Development Terra workspace](https://app.terra.bio/#workspaces/allofus-drc-prod-auxiliary/GVS_AoU_PGEN_Extract_Development) and using data from the GVS Delta callset (aou-genomics-curation-prod.aou_wgs_fullref_v2). My test process (for the majority of tests) has been as follows:. 1. Select a list of sample names from aou_wgs_fullref_v2.sample_info (excluding control samples and sample 3224672 because of the data issue mentioned [here](https://broadinstitute.slack.com/archives/CJRLP6ZSA/p1699026273329339)).; 2. Use that list as an input to GvsPrepareRangesCallset to create a cohort of test data in a separate BigQuery dataset (aou-genomics-curation-prod.klydon_pgen_extract_test).; 3. Run GvsExtractCallsetPgen on the newly created cohort. (I would just run GvsExtractCallsetPgenM",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8708
https://github.com/broadinstitute/gatk/pull/8708:9429,Testability,test,testing,9429,"nolithic merge takes too long. First, the file lists are sorted by index (the index in the filename). Plink does not like (i.e. does not support) merging files with overlapping intervals, so the files need to be sorted so that when they are merged in stages, we do not create merged files with intervals that overlap. Next, the lists are split, so we can parallelize the merge. Each of the split lists is then sent to a task that merges the files using Plink. Then, this is repeated to merge all the resulting files into one. The output is a single .pgen file, .psam file, and .pvar.zst file containing all the extracted data for the corresponding chromosome. ## Testing. ### Unit tests; Unit tests for the ExtractCohortToPgen tool exist in the ExtractCohortToPgenTest file. They are based closely on the tests in ExtractCohortToVcfTest, with a few extras to account for behaviors specific to PGEN extract. ### Large scale testing; All of my testing (with the exception of some very small scale stuff early on) has been done in the [GVS_AoU_PGEN_Extract_Development Terra workspace](https://app.terra.bio/#workspaces/allofus-drc-prod-auxiliary/GVS_AoU_PGEN_Extract_Development) and using data from the GVS Delta callset (aou-genomics-curation-prod.aou_wgs_fullref_v2). My test process (for the majority of tests) has been as follows:. 1. Select a list of sample names from aou_wgs_fullref_v2.sample_info (excluding control samples and sample 3224672 because of the data issue mentioned [here](https://broadinstitute.slack.com/archives/CJRLP6ZSA/p1699026273329339)).; 2. Use that list as an input to GvsPrepareRangesCallset to create a cohort of test data in a separate BigQuery dataset (aou-genomics-curation-prod.klydon_pgen_extract_test).; 3. Run GvsExtractCallsetPgen on the newly created cohort. (I would just run GvsExtractCallsetPgenMerged, but I like using Workflow Dashboard to monitor how the job is going and dig into it if there are any failures. Workflow Dashboard doesn't seem to let you ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8708
https://github.com/broadinstitute/gatk/pull/8708:9448,Testability,test,testing,9448,"nolithic merge takes too long. First, the file lists are sorted by index (the index in the filename). Plink does not like (i.e. does not support) merging files with overlapping intervals, so the files need to be sorted so that when they are merged in stages, we do not create merged files with intervals that overlap. Next, the lists are split, so we can parallelize the merge. Each of the split lists is then sent to a task that merges the files using Plink. Then, this is repeated to merge all the resulting files into one. The output is a single .pgen file, .psam file, and .pvar.zst file containing all the extracted data for the corresponding chromosome. ## Testing. ### Unit tests; Unit tests for the ExtractCohortToPgen tool exist in the ExtractCohortToPgenTest file. They are based closely on the tests in ExtractCohortToVcfTest, with a few extras to account for behaviors specific to PGEN extract. ### Large scale testing; All of my testing (with the exception of some very small scale stuff early on) has been done in the [GVS_AoU_PGEN_Extract_Development Terra workspace](https://app.terra.bio/#workspaces/allofus-drc-prod-auxiliary/GVS_AoU_PGEN_Extract_Development) and using data from the GVS Delta callset (aou-genomics-curation-prod.aou_wgs_fullref_v2). My test process (for the majority of tests) has been as follows:. 1. Select a list of sample names from aou_wgs_fullref_v2.sample_info (excluding control samples and sample 3224672 because of the data issue mentioned [here](https://broadinstitute.slack.com/archives/CJRLP6ZSA/p1699026273329339)).; 2. Use that list as an input to GvsPrepareRangesCallset to create a cohort of test data in a separate BigQuery dataset (aou-genomics-curation-prod.klydon_pgen_extract_test).; 3. Run GvsExtractCallsetPgen on the newly created cohort. (I would just run GvsExtractCallsetPgenMerged, but I like using Workflow Dashboard to monitor how the job is going and dig into it if there are any failures. Workflow Dashboard doesn't seem to let you ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8708
https://github.com/broadinstitute/gatk/pull/8708:9778,Testability,test,test,9778,"ls that overlap. Next, the lists are split, so we can parallelize the merge. Each of the split lists is then sent to a task that merges the files using Plink. Then, this is repeated to merge all the resulting files into one. The output is a single .pgen file, .psam file, and .pvar.zst file containing all the extracted data for the corresponding chromosome. ## Testing. ### Unit tests; Unit tests for the ExtractCohortToPgen tool exist in the ExtractCohortToPgenTest file. They are based closely on the tests in ExtractCohortToVcfTest, with a few extras to account for behaviors specific to PGEN extract. ### Large scale testing; All of my testing (with the exception of some very small scale stuff early on) has been done in the [GVS_AoU_PGEN_Extract_Development Terra workspace](https://app.terra.bio/#workspaces/allofus-drc-prod-auxiliary/GVS_AoU_PGEN_Extract_Development) and using data from the GVS Delta callset (aou-genomics-curation-prod.aou_wgs_fullref_v2). My test process (for the majority of tests) has been as follows:. 1. Select a list of sample names from aou_wgs_fullref_v2.sample_info (excluding control samples and sample 3224672 because of the data issue mentioned [here](https://broadinstitute.slack.com/archives/CJRLP6ZSA/p1699026273329339)).; 2. Use that list as an input to GvsPrepareRangesCallset to create a cohort of test data in a separate BigQuery dataset (aou-genomics-curation-prod.klydon_pgen_extract_test).; 3. Run GvsExtractCallsetPgen on the newly created cohort. (I would just run GvsExtractCallsetPgenMerged, but I like using Workflow Dashboard to monitor how the job is going and dig into it if there are any failures. Workflow Dashboard doesn't seem to let you dig into individual tasks for workflows with sub-workflows, so it wouldn't allow me to look at individual shards running ExtractTask if I ran GvsExtractCallsetPgenMerged. Job Manager would be an alternative for this, but it seems to be pretty much unusable for even moderately-sized jobs.); 4. Run Gv",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8708
https://github.com/broadinstitute/gatk/pull/8708:9812,Testability,test,tests,9812,"ls that overlap. Next, the lists are split, so we can parallelize the merge. Each of the split lists is then sent to a task that merges the files using Plink. Then, this is repeated to merge all the resulting files into one. The output is a single .pgen file, .psam file, and .pvar.zst file containing all the extracted data for the corresponding chromosome. ## Testing. ### Unit tests; Unit tests for the ExtractCohortToPgen tool exist in the ExtractCohortToPgenTest file. They are based closely on the tests in ExtractCohortToVcfTest, with a few extras to account for behaviors specific to PGEN extract. ### Large scale testing; All of my testing (with the exception of some very small scale stuff early on) has been done in the [GVS_AoU_PGEN_Extract_Development Terra workspace](https://app.terra.bio/#workspaces/allofus-drc-prod-auxiliary/GVS_AoU_PGEN_Extract_Development) and using data from the GVS Delta callset (aou-genomics-curation-prod.aou_wgs_fullref_v2). My test process (for the majority of tests) has been as follows:. 1. Select a list of sample names from aou_wgs_fullref_v2.sample_info (excluding control samples and sample 3224672 because of the data issue mentioned [here](https://broadinstitute.slack.com/archives/CJRLP6ZSA/p1699026273329339)).; 2. Use that list as an input to GvsPrepareRangesCallset to create a cohort of test data in a separate BigQuery dataset (aou-genomics-curation-prod.klydon_pgen_extract_test).; 3. Run GvsExtractCallsetPgen on the newly created cohort. (I would just run GvsExtractCallsetPgenMerged, but I like using Workflow Dashboard to monitor how the job is going and dig into it if there are any failures. Workflow Dashboard doesn't seem to let you dig into individual tasks for workflows with sub-workflows, so it wouldn't allow me to look at individual shards running ExtractTask if I ran GvsExtractCallsetPgenMerged. Job Manager would be an alternative for this, but it seems to be pretty much unusable for even moderately-sized jobs.); 4. Run Gv",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8708
https://github.com/broadinstitute/gatk/pull/8708:10151,Testability,test,test,10151,"onding chromosome. ## Testing. ### Unit tests; Unit tests for the ExtractCohortToPgen tool exist in the ExtractCohortToPgenTest file. They are based closely on the tests in ExtractCohortToVcfTest, with a few extras to account for behaviors specific to PGEN extract. ### Large scale testing; All of my testing (with the exception of some very small scale stuff early on) has been done in the [GVS_AoU_PGEN_Extract_Development Terra workspace](https://app.terra.bio/#workspaces/allofus-drc-prod-auxiliary/GVS_AoU_PGEN_Extract_Development) and using data from the GVS Delta callset (aou-genomics-curation-prod.aou_wgs_fullref_v2). My test process (for the majority of tests) has been as follows:. 1. Select a list of sample names from aou_wgs_fullref_v2.sample_info (excluding control samples and sample 3224672 because of the data issue mentioned [here](https://broadinstitute.slack.com/archives/CJRLP6ZSA/p1699026273329339)).; 2. Use that list as an input to GvsPrepareRangesCallset to create a cohort of test data in a separate BigQuery dataset (aou-genomics-curation-prod.klydon_pgen_extract_test).; 3. Run GvsExtractCallsetPgen on the newly created cohort. (I would just run GvsExtractCallsetPgenMerged, but I like using Workflow Dashboard to monitor how the job is going and dig into it if there are any failures. Workflow Dashboard doesn't seem to let you dig into individual tasks for workflows with sub-workflows, so it wouldn't allow me to look at individual shards running ExtractTask if I ran GvsExtractCallsetPgenMerged. Job Manager would be an alternative for this, but it seems to be pretty much unusable for even moderately-sized jobs.); 4. Run GvsExtractCallset on the newly created cohort, making sure to use the same parameters, including scatter count. This will generate VCF files that we can use to compare to the PGEN files created during the previous step for validation.; 5. Run GvsExtractCallsetPgenMerged with the same parameters used to run GvsExtractCallsetPgen in Step 3. Th",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8708
https://github.com/broadinstitute/gatk/pull/8708:13229,Testability,test,test,13229,"rated by GvsExtractCallset. . It ignores basically everything except genotypes, because PGENs do not store all the other fields and annotations that the VCFs might have. It will also skip over any sites in the VCFs with >254 alleles because those will not be present in the PGEN files. Any differences are written to diff files, in the form of the differing lines in the VCFs being compared. The code for this comparison tool lives [here](https://github.com/KevinCLydon/pgen_vcf_comparator) in a repo I created under my GitHub account. (I didn't create it under the Broad org because it's sort of half-baked and bad and not actually meant to be used by anyone other than me.) I don't know if y'all want to continue using this tool, but I'm happy to discuss it more if it would actually be useful to you. ## To-dos / caveats. ### PGEN-JNI; The version of PGEN-JNI I'm referencing in the current build.gradle file is a beta version that is hosted on artifactory. Functionally, this is totally fine, but we want to get a 1.0 version of it hosted publicly. Chris Norman, who developed the tool is currently very working on this and very close to done. Once he's completed this, I want to run a sanity test or two against a small subset of the Delta callset just to make sure everything is functioning as intended. ### Merging by chromosome arm; Right now, the last step of the PGEN extract workflow merges the PGEN files by contig name, so the final result is one trio of files (.pgen, .psam, and .pvar.zst) per chromosome. There was discussion about changing this to merge instead by chromosome arm. I want to make this change, but it's not super simple, so I've prioritized getting this version of the code ready for merging before tackling that. ### The PGEN format; As I mentioned above, Plink 2.0 and the PGEN file format are still not in full release, so the format could be subject to change in the future, which will require updates to our PGEN writing code and could possibly introduce problems.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8708
https://github.com/broadinstitute/gatk/pull/8708:13676,Usability,simpl,simple,13676,"rated by GvsExtractCallset. . It ignores basically everything except genotypes, because PGENs do not store all the other fields and annotations that the VCFs might have. It will also skip over any sites in the VCFs with >254 alleles because those will not be present in the PGEN files. Any differences are written to diff files, in the form of the differing lines in the VCFs being compared. The code for this comparison tool lives [here](https://github.com/KevinCLydon/pgen_vcf_comparator) in a repo I created under my GitHub account. (I didn't create it under the Broad org because it's sort of half-baked and bad and not actually meant to be used by anyone other than me.) I don't know if y'all want to continue using this tool, but I'm happy to discuss it more if it would actually be useful to you. ## To-dos / caveats. ### PGEN-JNI; The version of PGEN-JNI I'm referencing in the current build.gradle file is a beta version that is hosted on artifactory. Functionally, this is totally fine, but we want to get a 1.0 version of it hosted publicly. Chris Norman, who developed the tool is currently very working on this and very close to done. Once he's completed this, I want to run a sanity test or two against a small subset of the Delta callset just to make sure everything is functioning as intended. ### Merging by chromosome arm; Right now, the last step of the PGEN extract workflow merges the PGEN files by contig name, so the final result is one trio of files (.pgen, .psam, and .pvar.zst) per chromosome. There was discussion about changing this to merge instead by chromosome arm. I want to make this change, but it's not super simple, so I've prioritized getting this version of the code ready for merging before tackling that. ### The PGEN format; As I mentioned above, Plink 2.0 and the PGEN file format are still not in full release, so the format could be subject to change in the future, which will require updates to our PGEN writing code and could possibly introduce problems.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8708
https://github.com/broadinstitute/gatk/issues/8709:596,Availability,error,error,596,"Dear all,. I'm trying to run GenotypeGVCFs using the my_folder database created with GenomicsDBImport which should contain 222 samples. The database was created by adding progressively 10 samples at a time using the command --genomicsdb-update-workspace-path and the relative .sample_map file containing the path to my g.vcf.gz and g.vcf.gz.tbi files. I tried running the GenomicsDBImport followed by GenotypeGVCFs using only four sampled and it worked appropriately by generating a .vcf.gz file along with the index .vcf.gz.tbi file. However, when I run GenotypeGVCFs with 222 samples I get the error: A USER ERROR has occurred: Couldn't create GenomicsDBFeatureReader. The GATK version used is gatk-4.4.0.0 and the command used is the following:. python2.7 /home/administrator/tool/gatk-4.4.0.0/gatk --java-options ""-Xmx4g"" GenotypeGVCFs -R /mnt/nas/Stefano/Cashmere/Reference_Genome/GCF_001704415.1_ARS1_genomic.fna -V gendb://my_database -O /mnt/nas2/Stefano/Cashmere/joint_variant_calling/222_goats.vcf.gz. attaches below also the complete program log. and the content of my callset.json file. Any idea about that?. Thank you very much. Stefano. REQUIRED for all errors and issues:; a) GATK version used: gatk-4.4.0.0; b) Exact command used: python2.7 /home/administrator/tool/gatk-4.4.0.0/gatk --java-options ""-Xmx4g"" GenotypeGVCFs -R /mnt/nas/Stefano/Cashmere/Reference_Genome/GCF_001704415.1_ARS1_genomic.fna -V gendb://my_database -O /mnt/nas2/Stefano/Cashmere/joint_variant_calling/222_goats_fatte_con_GenomicsDBImport.vcf.gz. c) Entire program log:. (base) administrator@srv2-napolioni:/mnt/nas2/Stefano/Cashmere/joint_variant_calling$ python2.7 /home/administrator/tool/gatk-4.4.0.0/gatk --java-options ""-Xmx4g"" GenotypeGVCFs -R /mnt/nas/Stefano/Cashmere/Reference_Genome/GCF_001704415.1_ARS1_genomic.fna -V gendb://my_database -O /mnt/nas2/Stefano/Cashmere/joint_variant_calling/222_goats.vcf.gz; Using GATK jar /home/administrator/tool/gatk-4.4.0.0/gatk-package-4.4.0.0-local.jar; Runnin",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8709
https://github.com/broadinstitute/gatk/issues/8709:610,Availability,ERROR,ERROR,610,"Dear all,. I'm trying to run GenotypeGVCFs using the my_folder database created with GenomicsDBImport which should contain 222 samples. The database was created by adding progressively 10 samples at a time using the command --genomicsdb-update-workspace-path and the relative .sample_map file containing the path to my g.vcf.gz and g.vcf.gz.tbi files. I tried running the GenomicsDBImport followed by GenotypeGVCFs using only four sampled and it worked appropriately by generating a .vcf.gz file along with the index .vcf.gz.tbi file. However, when I run GenotypeGVCFs with 222 samples I get the error: A USER ERROR has occurred: Couldn't create GenomicsDBFeatureReader. The GATK version used is gatk-4.4.0.0 and the command used is the following:. python2.7 /home/administrator/tool/gatk-4.4.0.0/gatk --java-options ""-Xmx4g"" GenotypeGVCFs -R /mnt/nas/Stefano/Cashmere/Reference_Genome/GCF_001704415.1_ARS1_genomic.fna -V gendb://my_database -O /mnt/nas2/Stefano/Cashmere/joint_variant_calling/222_goats.vcf.gz. attaches below also the complete program log. and the content of my callset.json file. Any idea about that?. Thank you very much. Stefano. REQUIRED for all errors and issues:; a) GATK version used: gatk-4.4.0.0; b) Exact command used: python2.7 /home/administrator/tool/gatk-4.4.0.0/gatk --java-options ""-Xmx4g"" GenotypeGVCFs -R /mnt/nas/Stefano/Cashmere/Reference_Genome/GCF_001704415.1_ARS1_genomic.fna -V gendb://my_database -O /mnt/nas2/Stefano/Cashmere/joint_variant_calling/222_goats_fatte_con_GenomicsDBImport.vcf.gz. c) Entire program log:. (base) administrator@srv2-napolioni:/mnt/nas2/Stefano/Cashmere/joint_variant_calling$ python2.7 /home/administrator/tool/gatk-4.4.0.0/gatk --java-options ""-Xmx4g"" GenotypeGVCFs -R /mnt/nas/Stefano/Cashmere/Reference_Genome/GCF_001704415.1_ARS1_genomic.fna -V gendb://my_database -O /mnt/nas2/Stefano/Cashmere/joint_variant_calling/222_goats.vcf.gz; Using GATK jar /home/administrator/tool/gatk-4.4.0.0/gatk-package-4.4.0.0-local.jar; Runnin",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8709
https://github.com/broadinstitute/gatk/issues/8709:1168,Availability,error,errors,1168,"y 10 samples at a time using the command --genomicsdb-update-workspace-path and the relative .sample_map file containing the path to my g.vcf.gz and g.vcf.gz.tbi files. I tried running the GenomicsDBImport followed by GenotypeGVCFs using only four sampled and it worked appropriately by generating a .vcf.gz file along with the index .vcf.gz.tbi file. However, when I run GenotypeGVCFs with 222 samples I get the error: A USER ERROR has occurred: Couldn't create GenomicsDBFeatureReader. The GATK version used is gatk-4.4.0.0 and the command used is the following:. python2.7 /home/administrator/tool/gatk-4.4.0.0/gatk --java-options ""-Xmx4g"" GenotypeGVCFs -R /mnt/nas/Stefano/Cashmere/Reference_Genome/GCF_001704415.1_ARS1_genomic.fna -V gendb://my_database -O /mnt/nas2/Stefano/Cashmere/joint_variant_calling/222_goats.vcf.gz. attaches below also the complete program log. and the content of my callset.json file. Any idea about that?. Thank you very much. Stefano. REQUIRED for all errors and issues:; a) GATK version used: gatk-4.4.0.0; b) Exact command used: python2.7 /home/administrator/tool/gatk-4.4.0.0/gatk --java-options ""-Xmx4g"" GenotypeGVCFs -R /mnt/nas/Stefano/Cashmere/Reference_Genome/GCF_001704415.1_ARS1_genomic.fna -V gendb://my_database -O /mnt/nas2/Stefano/Cashmere/joint_variant_calling/222_goats_fatte_con_GenomicsDBImport.vcf.gz. c) Entire program log:. (base) administrator@srv2-napolioni:/mnt/nas2/Stefano/Cashmere/joint_variant_calling$ python2.7 /home/administrator/tool/gatk-4.4.0.0/gatk --java-options ""-Xmx4g"" GenotypeGVCFs -R /mnt/nas/Stefano/Cashmere/Reference_Genome/GCF_001704415.1_ARS1_genomic.fna -V gendb://my_database -O /mnt/nas2/Stefano/Cashmere/joint_variant_calling/222_goats.vcf.gz; Using GATK jar /home/administrator/tool/gatk-4.4.0.0/gatk-package-4.4.0.0-local.jar; Running:;     java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx4g -jar /ho",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8709
https://github.com/broadinstitute/gatk/issues/8709:4986,Availability,down,down,4986,"ter: IntelDeflater; 12:01:18.571 INFO  GenotypeGVCFs - Inflater: IntelInflater; 12:01:18.571 INFO  GenotypeGVCFs - GCS max retries/reopens: 20; 12:01:18.571 INFO  GenotypeGVCFs - Requester pays: disabled; 12:01:18.571 INFO  GenotypeGVCFs - Initializing engine; 12:01:19.353 INFO  GenomicsDBLibLoader - GenomicsDB native library version : 1.4.4-ce4e1b9; 12:01:33.262 INFO  NativeGenomicsDB - pid=1923139 tid=1923140 No valid combination operation found for INFO field InbreedingCoeff  - the field will NOT be part of INFO fields in the generated VCF records; 12:01:33.262 INFO  NativeGenomicsDB - pid=1923139 tid=1923140 No valid combination operation found for INFO field MLEAC  - the field will NOT be part of INFO fields in the generated VCF records; 12:01:33.262 INFO  NativeGenomicsDB - pid=1923139 tid=1923140 No valid combination operation found for INFO field MLEAF  - the field will NOT be part of INFO fields in the generated VCF records; 12:01:33.288 INFO  GenotypeGVCFs - Shutting down engine; [March 1, 2024 at 12:01:33 PM UTC] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 0.25 minutes.; Runtime.totalMemory()=1130364928; ***********************************************************************. A USER ERROR has occurred: Couldn't create GenomicsDBFeatureReader. ***********************************************************************; Set the system property GATK_STACKTRACE_ON_USER_EXCEPTION (--java-options '-DGATK_STACKTRACE_ON_USER_EXCEPTION=true') to print the stack trace.  ; content of my callset.json file:. {""callsets"": [{""sample_name"": ""ERR318225"",""row_idx"": 223,""idx_in_file"": 0,""stream_name"": ""ERR318225_stream""},{""sample_name"": ""ERR318226"",""row_idx"": 224,""idx_in_file"": 0,""stream_name"": ""ERR318226_stream""},{""sample_name"": ""ERR4133262"",""row_idx"": 225,""idx_in_file"": 0,""stream_name"": ""ERR4133262_stream""},{""sample_name"": ""ERR4133361"",""row_idx"": 226,""idx_in_file"": 0,""stream_name"": ""ERR4133361_stream""},{""sample_name"": ""ERR4133400"",""row_idx"": 2",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8709
https://github.com/broadinstitute/gatk/issues/8709:5241,Availability,ERROR,ERROR,5241," INFO  GenomicsDBLibLoader - GenomicsDB native library version : 1.4.4-ce4e1b9; 12:01:33.262 INFO  NativeGenomicsDB - pid=1923139 tid=1923140 No valid combination operation found for INFO field InbreedingCoeff  - the field will NOT be part of INFO fields in the generated VCF records; 12:01:33.262 INFO  NativeGenomicsDB - pid=1923139 tid=1923140 No valid combination operation found for INFO field MLEAC  - the field will NOT be part of INFO fields in the generated VCF records; 12:01:33.262 INFO  NativeGenomicsDB - pid=1923139 tid=1923140 No valid combination operation found for INFO field MLEAF  - the field will NOT be part of INFO fields in the generated VCF records; 12:01:33.288 INFO  GenotypeGVCFs - Shutting down engine; [March 1, 2024 at 12:01:33 PM UTC] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 0.25 minutes.; Runtime.totalMemory()=1130364928; ***********************************************************************. A USER ERROR has occurred: Couldn't create GenomicsDBFeatureReader. ***********************************************************************; Set the system property GATK_STACKTRACE_ON_USER_EXCEPTION (--java-options '-DGATK_STACKTRACE_ON_USER_EXCEPTION=true') to print the stack trace.  ; content of my callset.json file:. {""callsets"": [{""sample_name"": ""ERR318225"",""row_idx"": 223,""idx_in_file"": 0,""stream_name"": ""ERR318225_stream""},{""sample_name"": ""ERR318226"",""row_idx"": 224,""idx_in_file"": 0,""stream_name"": ""ERR318226_stream""},{""sample_name"": ""ERR4133262"",""row_idx"": 225,""idx_in_file"": 0,""stream_name"": ""ERR4133262_stream""},{""sample_name"": ""ERR4133361"",""row_idx"": 226,""idx_in_file"": 0,""stream_name"": ""ERR4133361_stream""},{""sample_name"": ""ERR4133400"",""row_idx"": 227,""idx_in_file"": 0,""stream_name"": ""ERR4133400_stream""},{""sample_name"": ""ERR4133407"",""row_idx"": 228,""idx_in_file"": 0,""stream_name"": ""ERR4133407_stream""},{""sample_name"": ""ERR4133418"",""row_idx"": 229,""idx_in_file"": 0,""stream_name"": ""ERR4133418_stream""},{""sample_name"": ""ERR41",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8709
https://github.com/broadinstitute/gatk/issues/8709:237,Deployability,update,update-workspace-path,237,"Dear all,. I'm trying to run GenotypeGVCFs using the my_folder database created with GenomicsDBImport which should contain 222 samples. The database was created by adding progressively 10 samples at a time using the command --genomicsdb-update-workspace-path and the relative .sample_map file containing the path to my g.vcf.gz and g.vcf.gz.tbi files. I tried running the GenomicsDBImport followed by GenotypeGVCFs using only four sampled and it worked appropriately by generating a .vcf.gz file along with the index .vcf.gz.tbi file. However, when I run GenotypeGVCFs with 222 samples I get the error: A USER ERROR has occurred: Couldn't create GenomicsDBFeatureReader. The GATK version used is gatk-4.4.0.0 and the command used is the following:. python2.7 /home/administrator/tool/gatk-4.4.0.0/gatk --java-options ""-Xmx4g"" GenotypeGVCFs -R /mnt/nas/Stefano/Cashmere/Reference_Genome/GCF_001704415.1_ARS1_genomic.fna -V gendb://my_database -O /mnt/nas2/Stefano/Cashmere/joint_variant_calling/222_goats.vcf.gz. attaches below also the complete program log. and the content of my callset.json file. Any idea about that?. Thank you very much. Stefano. REQUIRED for all errors and issues:; a) GATK version used: gatk-4.4.0.0; b) Exact command used: python2.7 /home/administrator/tool/gatk-4.4.0.0/gatk --java-options ""-Xmx4g"" GenotypeGVCFs -R /mnt/nas/Stefano/Cashmere/Reference_Genome/GCF_001704415.1_ARS1_genomic.fna -V gendb://my_database -O /mnt/nas2/Stefano/Cashmere/joint_variant_calling/222_goats_fatte_con_GenomicsDBImport.vcf.gz. c) Entire program log:. (base) administrator@srv2-napolioni:/mnt/nas2/Stefano/Cashmere/joint_variant_calling$ python2.7 /home/administrator/tool/gatk-4.4.0.0/gatk --java-options ""-Xmx4g"" GenotypeGVCFs -R /mnt/nas/Stefano/Cashmere/Reference_Genome/GCF_001704415.1_ARS1_genomic.fna -V gendb://my_database -O /mnt/nas2/Stefano/Cashmere/joint_variant_calling/222_goats.vcf.gz; Using GATK jar /home/administrator/tool/gatk-4.4.0.0/gatk-package-4.4.0.0-local.jar; Runnin",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8709
https://github.com/broadinstitute/gatk/issues/8709:2477,Performance,Load,Loading,2477,"t_variant_calling/222_goats_fatte_con_GenomicsDBImport.vcf.gz. c) Entire program log:. (base) administrator@srv2-napolioni:/mnt/nas2/Stefano/Cashmere/joint_variant_calling$ python2.7 /home/administrator/tool/gatk-4.4.0.0/gatk --java-options ""-Xmx4g"" GenotypeGVCFs -R /mnt/nas/Stefano/Cashmere/Reference_Genome/GCF_001704415.1_ARS1_genomic.fna -V gendb://my_database -O /mnt/nas2/Stefano/Cashmere/joint_variant_calling/222_goats.vcf.gz; Using GATK jar /home/administrator/tool/gatk-4.4.0.0/gatk-package-4.4.0.0-local.jar; Running:;     java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx4g -jar /home/administrator/tool/gatk-4.4.0.0/gatk-package-4.4.0.0-local.jar GenotypeGVCFs -R /mnt/nas/Stefano/Cashmere/Reference_Genome/GCF_001704415.1_ARS1_genomic.fna -V gendb://my_database -O /mnt/nas2/Stefano/Cashmere/joint_variant_calling/222_goats.vcf.gz; 12:01:18.521 INFO  NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/administrator/tool/gatk-4.4.0.0/gatk-package-4.4.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 12:01:18.564 INFO  GenotypeGVCFs - ------------------------------------------------------------; 12:01:18.567 INFO  GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.4.0.0; 12:01:18.567 INFO  GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:01:18.568 INFO  GenotypeGVCFs - Executing as administrator@srv2-napolioni on Linux v5.15.0-73-generic amd64; 12:01:18.568 INFO  GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v19.0.2+7-Ubuntu-0ubuntu322.04; 12:01:18.568 INFO  GenotypeGVCFs - Start Date/Time: March 1, 2024 at 12:01:18 PM UTC; 12:01:18.568 INFO  GenotypeGVCFs - ------------------------------------------------------------; 12:01:18.568 INFO  GenotypeGVCFs - ------------------------------------------------------------; 12:01:18.569 INFO  GenotypeGVCFs - HTSJDK Version: ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8709
https://github.com/broadinstitute/gatk/issues/8709:1053,Testability,log,log,1053,"ypeGVCFs using the my_folder database created with GenomicsDBImport which should contain 222 samples. The database was created by adding progressively 10 samples at a time using the command --genomicsdb-update-workspace-path and the relative .sample_map file containing the path to my g.vcf.gz and g.vcf.gz.tbi files. I tried running the GenomicsDBImport followed by GenotypeGVCFs using only four sampled and it worked appropriately by generating a .vcf.gz file along with the index .vcf.gz.tbi file. However, when I run GenotypeGVCFs with 222 samples I get the error: A USER ERROR has occurred: Couldn't create GenomicsDBFeatureReader. The GATK version used is gatk-4.4.0.0 and the command used is the following:. python2.7 /home/administrator/tool/gatk-4.4.0.0/gatk --java-options ""-Xmx4g"" GenotypeGVCFs -R /mnt/nas/Stefano/Cashmere/Reference_Genome/GCF_001704415.1_ARS1_genomic.fna -V gendb://my_database -O /mnt/nas2/Stefano/Cashmere/joint_variant_calling/222_goats.vcf.gz. attaches below also the complete program log. and the content of my callset.json file. Any idea about that?. Thank you very much. Stefano. REQUIRED for all errors and issues:; a) GATK version used: gatk-4.4.0.0; b) Exact command used: python2.7 /home/administrator/tool/gatk-4.4.0.0/gatk --java-options ""-Xmx4g"" GenotypeGVCFs -R /mnt/nas/Stefano/Cashmere/Reference_Genome/GCF_001704415.1_ARS1_genomic.fna -V gendb://my_database -O /mnt/nas2/Stefano/Cashmere/joint_variant_calling/222_goats_fatte_con_GenomicsDBImport.vcf.gz. c) Entire program log:. (base) administrator@srv2-napolioni:/mnt/nas2/Stefano/Cashmere/joint_variant_calling$ python2.7 /home/administrator/tool/gatk-4.4.0.0/gatk --java-options ""-Xmx4g"" GenotypeGVCFs -R /mnt/nas/Stefano/Cashmere/Reference_Genome/GCF_001704415.1_ARS1_genomic.fna -V gendb://my_database -O /mnt/nas2/Stefano/Cashmere/joint_variant_calling/222_goats.vcf.gz; Using GATK jar /home/administrator/tool/gatk-4.4.0.0/gatk-package-4.4.0.0-local.jar; Running:;     java -Dsamjdk.use_async_i",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8709
https://github.com/broadinstitute/gatk/issues/8709:1555,Testability,log,log,1555," I run GenotypeGVCFs with 222 samples I get the error: A USER ERROR has occurred: Couldn't create GenomicsDBFeatureReader. The GATK version used is gatk-4.4.0.0 and the command used is the following:. python2.7 /home/administrator/tool/gatk-4.4.0.0/gatk --java-options ""-Xmx4g"" GenotypeGVCFs -R /mnt/nas/Stefano/Cashmere/Reference_Genome/GCF_001704415.1_ARS1_genomic.fna -V gendb://my_database -O /mnt/nas2/Stefano/Cashmere/joint_variant_calling/222_goats.vcf.gz. attaches below also the complete program log. and the content of my callset.json file. Any idea about that?. Thank you very much. Stefano. REQUIRED for all errors and issues:; a) GATK version used: gatk-4.4.0.0; b) Exact command used: python2.7 /home/administrator/tool/gatk-4.4.0.0/gatk --java-options ""-Xmx4g"" GenotypeGVCFs -R /mnt/nas/Stefano/Cashmere/Reference_Genome/GCF_001704415.1_ARS1_genomic.fna -V gendb://my_database -O /mnt/nas2/Stefano/Cashmere/joint_variant_calling/222_goats_fatte_con_GenomicsDBImport.vcf.gz. c) Entire program log:. (base) administrator@srv2-napolioni:/mnt/nas2/Stefano/Cashmere/joint_variant_calling$ python2.7 /home/administrator/tool/gatk-4.4.0.0/gatk --java-options ""-Xmx4g"" GenotypeGVCFs -R /mnt/nas/Stefano/Cashmere/Reference_Genome/GCF_001704415.1_ARS1_genomic.fna -V gendb://my_database -O /mnt/nas2/Stefano/Cashmere/joint_variant_calling/222_goats.vcf.gz; Using GATK jar /home/administrator/tool/gatk-4.4.0.0/gatk-package-4.4.0.0-local.jar; Running:;     java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx4g -jar /home/administrator/tool/gatk-4.4.0.0/gatk-package-4.4.0.0-local.jar GenotypeGVCFs -R /mnt/nas/Stefano/Cashmere/Reference_Genome/GCF_001704415.1_ARS1_genomic.fna -V gendb://my_database -O /mnt/nas2/Stefano/Cashmere/joint_variant_calling/222_goats.vcf.gz; 12:01:18.521 INFO  NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/administrator/tool/ga",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8709
https://github.com/broadinstitute/gatk/pull/8710:45,Deployability,release,releases,45,See https://github.com/GenomicsDB/GenomicsDB/releases/tag/v1.5.2 for release notes.; Of relevance to gatk are the following ; ```; Support for MacOS universal builds; Catch JNI importer exceptions and propagate them as java IOExceptions; Turn off HDFS support by default; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8710
https://github.com/broadinstitute/gatk/pull/8710:69,Deployability,release,release,69,See https://github.com/GenomicsDB/GenomicsDB/releases/tag/v1.5.2 for release notes.; Of relevance to gatk are the following ; ```; Support for MacOS universal builds; Catch JNI importer exceptions and propagate them as java IOExceptions; Turn off HDFS support by default; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8710
https://github.com/broadinstitute/gatk/pull/8712:124,Performance,optimiz,optimized,124,"Test out the preview version of the GKL, containing the new native PDHMM. The jar was build using Java 1.8 and contains AVX optimized binaries for Linux hosts and not optimized binaries for Mac (Intel CPUs). Example code showing how to call into the PDHMM is here: ; https://github.com/Intel-HLS/GKL/blob/master/src/test/java/com/intel/gkl/pdhmm/IntelPDHMMUnitTest.java",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8712
https://github.com/broadinstitute/gatk/pull/8712:167,Performance,optimiz,optimized,167,"Test out the preview version of the GKL, containing the new native PDHMM. The jar was build using Java 1.8 and contains AVX optimized binaries for Linux hosts and not optimized binaries for Mac (Intel CPUs). Example code showing how to call into the PDHMM is here: ; https://github.com/Intel-HLS/GKL/blob/master/src/test/java/com/intel/gkl/pdhmm/IntelPDHMMUnitTest.java",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8712
https://github.com/broadinstitute/gatk/pull/8712:0,Testability,Test,Test,0,"Test out the preview version of the GKL, containing the new native PDHMM. The jar was build using Java 1.8 and contains AVX optimized binaries for Linux hosts and not optimized binaries for Mac (Intel CPUs). Example code showing how to call into the PDHMM is here: ; https://github.com/Intel-HLS/GKL/blob/master/src/test/java/com/intel/gkl/pdhmm/IntelPDHMMUnitTest.java",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8712
https://github.com/broadinstitute/gatk/pull/8712:316,Testability,test,test,316,"Test out the preview version of the GKL, containing the new native PDHMM. The jar was build using Java 1.8 and contains AVX optimized binaries for Linux hosts and not optimized binaries for Mac (Intel CPUs). Example code showing how to call into the PDHMM is here: ; https://github.com/Intel-HLS/GKL/blob/master/src/test/java/com/intel/gkl/pdhmm/IntelPDHMMUnitTest.java",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8712
https://github.com/broadinstitute/gatk/pull/8718:11,Testability,test,test,11,* Changing test results directory for the docker tests to match the test results directory from the non docker test.; * Awkward handling of this seems to have been resulting in different test output paths and causing the reporting bot to sometimes fail to report the correct file location,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8718
https://github.com/broadinstitute/gatk/pull/8718:49,Testability,test,tests,49,* Changing test results directory for the docker tests to match the test results directory from the non docker test.; * Awkward handling of this seems to have been resulting in different test output paths and causing the reporting bot to sometimes fail to report the correct file location,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8718
https://github.com/broadinstitute/gatk/pull/8718:68,Testability,test,test,68,* Changing test results directory for the docker tests to match the test results directory from the non docker test.; * Awkward handling of this seems to have been resulting in different test output paths and causing the reporting bot to sometimes fail to report the correct file location,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8718
https://github.com/broadinstitute/gatk/pull/8718:111,Testability,test,test,111,* Changing test results directory for the docker tests to match the test results directory from the non docker test.; * Awkward handling of this seems to have been resulting in different test output paths and causing the reporting bot to sometimes fail to report the correct file location,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8718
https://github.com/broadinstitute/gatk/pull/8718:187,Testability,test,test,187,* Changing test results directory for the docker tests to match the test results directory from the non docker test.; * Awkward handling of this seems to have been resulting in different test output paths and causing the reporting bot to sometimes fail to report the correct file location,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8718
https://github.com/broadinstitute/gatk/issues/8721:61,Usability,clear,clear,61,"In the documentation for the `SelectVariants` tool, it isn't clear what does `-R` do. The documentation for the `-R` is empty, it is set as an optional parameter, but both examples on the page do set it. https://gatk.broadinstitute.org/hc/en-us/articles/360036362532-SelectVariants. On the other hand, examples mentioned elsewhere ignore reference sequence when calling `SelectVariants`. https://gatk.broadinstitute.org/hc/en-us/articles/360035531112--How-to-Filter-variants-either-with-VQSR-or-by-hard-filtering",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8721
https://github.com/broadinstitute/gatk/issues/8722:390,Usability,guid,guidance,390,"`--argument_file` is a parameter shared by many tools. It allows providing arguments in a file instead on a command line, which is very convenient when e.g., merging multiple files using `CombineGVCFs`. And yet, the format of the `argument_file` is not mentioned anywhere. Compare with the `--sample-name-map` from `GenomicDBImport` which mentions tab-delimited format for this file.; Some guidance like this for the `argument_file` would be useful.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8722
https://github.com/broadinstitute/gatk/pull/8724:111,Availability,failure,failure,111,A quick and dirty implementation. The on thing that needs extra scrutiny is going to be the various conditions/failure states in the validate method. I think i caught most of the cases but there might be a gap somewhere.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8724
https://github.com/broadinstitute/gatk/pull/8724:133,Security,validat,validate,133,A quick and dirty implementation. The on thing that needs extra scrutiny is going to be the various conditions/failure states in the validate method. I think i caught most of the cases but there might be a gap somewhere.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8724
https://github.com/broadinstitute/gatk/pull/8725:56,Availability,redundant,redundant,56,Tiny improvement that was languishing in an ancient and redundant PR https://github.com/broadinstitute/gatk/pull/6736,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8725
https://github.com/broadinstitute/gatk/pull/8725:56,Safety,redund,redundant,56,Tiny improvement that was languishing in an ancient and redundant PR https://github.com/broadinstitute/gatk/pull/6736,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8725
https://github.com/broadinstitute/gatk/issues/8726:170,Availability,error,error,170,"Hi, ; I need to run BaseRecalibrator as a part of the preprocessing of my RNAseq bam files before variant calling. But I experience difficulties with memory! Here is the error I get:. ```; 22:30:25.477 INFO BaseRecalibrator - Start Date/Time: March 8, 2024 at 10:30:25 PM GMT; 22:30:25.477 INFO BaseRecalibrator - ------------------------------------------------------------; 22:30:25.477 INFO BaseRecalibrator - ------------------------------------------------------------; 22:30:25.477 INFO BaseRecalibrator - HTSJDK Version: 4.1.0; 22:30:25.478 INFO BaseRecalibrator - Picard Version: 3.1.1; 22:30:25.478 INFO BaseRecalibrator - Built for Spark Version: 3.5.0; 22:30:25.478 INFO BaseRecalibrator - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 22:30:25.478 INFO BaseRecalibrator - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 22:30:25.478 INFO BaseRecalibrator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 22:30:25.478 INFO BaseRecalibrator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 22:30:25.478 INFO BaseRecalibrator - Deflater: IntelDeflater; 22:30:25.478 INFO BaseRecalibrator - Inflater: IntelInflater; 22:30:25.478 INFO BaseRecalibrator - GCS max retries/reopens: 20; 22:30:25.479 INFO BaseRecalibrator - Requester pays: disabled; 22:30:25.479 INFO BaseRecalibrator - Initializing engine; WARNING 2024-03-08 22:30:25 SamFiles The index file /mnt/storage/users/dockworker/mpedersen/work/RNAseq_variant_call/work/d6/362957b6215ad2e8193c27c895d42d/VR0024SA.withoutERCCs.withRG.markedDup.splitNcigar.bai was found by resolving the canonical path of a symlink: VR0024SA.withoutERCCs.withRG.markedDup.splitNcigar.bam -> /mnt/storage/users/dockworker/mpedersen/work/RNAseq_variant_call/work/d6/362957b6215ad2e8193c27c895d42d/VR0024SA.withoutERCCs.withRG.markedDup.splitNcigar.bam; 22:30:25.631 INFO FeatureManager - Using codec VCFCodec to read file file://1000G_phase1.snps.high_confidence.hg38.vcf.gz; 22:30:25.754 INFO FeatureManager - Using codec VCFCodec to read",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8726
https://github.com/broadinstitute/gatk/issues/8726:2110,Availability,down,down,2110," Inflater: IntelInflater; 22:30:25.478 INFO BaseRecalibrator - GCS max retries/reopens: 20; 22:30:25.479 INFO BaseRecalibrator - Requester pays: disabled; 22:30:25.479 INFO BaseRecalibrator - Initializing engine; WARNING 2024-03-08 22:30:25 SamFiles The index file /mnt/storage/users/dockworker/mpedersen/work/RNAseq_variant_call/work/d6/362957b6215ad2e8193c27c895d42d/VR0024SA.withoutERCCs.withRG.markedDup.splitNcigar.bai was found by resolving the canonical path of a symlink: VR0024SA.withoutERCCs.withRG.markedDup.splitNcigar.bam -> /mnt/storage/users/dockworker/mpedersen/work/RNAseq_variant_call/work/d6/362957b6215ad2e8193c27c895d42d/VR0024SA.withoutERCCs.withRG.markedDup.splitNcigar.bam; 22:30:25.631 INFO FeatureManager - Using codec VCFCodec to read file file://1000G_phase1.snps.high_confidence.hg38.vcf.gz; 22:30:25.754 INFO FeatureManager - Using codec VCFCodec to read file file://Mills_and_1000G_gold_standard.indels.hg38.vcf.gz; 23:39:21.541 INFO BaseRecalibrator - Shutting down engine; [March 8, 2024 at 11:39:21 PM GMT] org.broadinstitute.hellbender.tools.walkers.bqsr.BaseRecalibrator done. Elapsed time: 68.94 minutes.; Runtime.totalMemory()=214748364800; java.lang.OutOfMemoryError: Java heap space; at htsjdk.tribble.readers.TabixReader.readInt(TabixReader.java:189); at htsjdk.tribble.readers.TabixReader.readIndex(TabixReader.java:274); at htsjdk.tribble.readers.TabixReader.readIndex(TabixReader.java:287); at htsjdk.tribble.readers.TabixReader.<init>(TabixReader.java:165); at htsjdk.tribble.readers.TabixReader.<init>(TabixReader.java:129); at htsjdk.tribble.TabixFeatureReader.<init>(TabixFeatureReader.java:80); at htsjdk.tribble.AbstractFeatureReader.getFeatureReader(AbstractFeatureReader.java:117); at org.broadinstitute.hellbender.engine.FeatureDataSource.getTribbleFeatureReader(FeatureDataSource.java:433); at org.broadinstitute.hellbender.engine.FeatureDataSource.getFeatureReader(FeatureDataSource.java:377); at org.broadinstitute.hellbender.engine.FeatureDataS",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8726
https://github.com/broadinstitute/gatk/issues/8726:5601,Availability,error,error,5601,"ne.CommandLineProgram.runTool(CommandLineProgram.java:147); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:198); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:217); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:166); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:209); at org.broadinstitute.hellbender.Main.main(Main.java:306); Using GATK jar /gatk/gatk-package-4.5.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xms200G -Xmx200G -XX:ParallelGCThreads=2 -jar /gatk/gatk-package-4.5.0.0-local.jar BaseRecalibrator -I VR0024SA.withoutERCCs.withRG.markedDup.splitNcigar.bam -O VR0024SA.withoutERCCs.withRG.markedDup.splitNcigar.baseRecal.bam -R GRCh38.primary_assembly.genome.fa --known-sites 1000G_phase1.snps.high_confidence.hg38.vcf.gz --known-sites Mills_and_1000G_gold_standard.indels.hg38.vcf.gz --tmp-dir /tmp --disable-bam-index-caching true. Work dir:; /mnt/storage/users/dockworker/mpedersen/work/RNAseq_variant_call/work/71/ac26344f0e095f7fe77cbb45a334db. Tip: view the complete command output by changing to the process work dir and entering the command `cat .command.out`. -- Check '.nextflow.log' file for details. ```. I tried to run it like this:; ```; gatk --java-options ""-Xms200G -Xmx200G -XX:ParallelGCThreads=2"" \; BaseRecalibrator \; -I $input_bam \; -O ""${file(input_bam).baseName}.baseRecal.bam"" \; -R $reference \; --known-sites $kg_snp \; --known-sites $kg_indel \; --tmp-dir /tmp \; --disable-bam-index-caching true; ```. but I still get the memory error. I have more memory to use, but it seems very inefficient if I need to go up to 1TB? Why can I not make this run? And is there any alternative when I want to do the MarkDup, SplitCigar, BaseRecal ? . Hope you can help, ; BR, ; Mette",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8726
https://github.com/broadinstitute/gatk/issues/8726:5231,Testability,log,log,5231,"ne.CommandLineProgram.runTool(CommandLineProgram.java:147); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:198); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:217); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:166); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:209); at org.broadinstitute.hellbender.Main.main(Main.java:306); Using GATK jar /gatk/gatk-package-4.5.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xms200G -Xmx200G -XX:ParallelGCThreads=2 -jar /gatk/gatk-package-4.5.0.0-local.jar BaseRecalibrator -I VR0024SA.withoutERCCs.withRG.markedDup.splitNcigar.bam -O VR0024SA.withoutERCCs.withRG.markedDup.splitNcigar.baseRecal.bam -R GRCh38.primary_assembly.genome.fa --known-sites 1000G_phase1.snps.high_confidence.hg38.vcf.gz --known-sites Mills_and_1000G_gold_standard.indels.hg38.vcf.gz --tmp-dir /tmp --disable-bam-index-caching true. Work dir:; /mnt/storage/users/dockworker/mpedersen/work/RNAseq_variant_call/work/71/ac26344f0e095f7fe77cbb45a334db. Tip: view the complete command output by changing to the process work dir and entering the command `cat .command.out`. -- Check '.nextflow.log' file for details. ```. I tried to run it like this:; ```; gatk --java-options ""-Xms200G -Xmx200G -XX:ParallelGCThreads=2"" \; BaseRecalibrator \; -I $input_bam \; -O ""${file(input_bam).baseName}.baseRecal.bam"" \; -R $reference \; --known-sites $kg_snp \; --known-sites $kg_indel \; --tmp-dir /tmp \; --disable-bam-index-caching true; ```. but I still get the memory error. I have more memory to use, but it seems very inefficient if I need to go up to 1TB? Why can I not make this run? And is there any alternative when I want to do the MarkDup, SplitCigar, BaseRecal ? . Hope you can help, ; BR, ; Mette",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8726
https://github.com/broadinstitute/gatk/pull/8728:21,Deployability,update,update,21,note I still need to update the docker image and utils,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8728
https://github.com/broadinstitute/gatk/pull/8730:29,Modifiability,refactor,refactored,29,Run the GATK tests using the refactored rans code in htsjdk.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8730
https://github.com/broadinstitute/gatk/pull/8730:13,Testability,test,tests,13,Run the GATK tests using the refactored rans code in htsjdk.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8730
https://github.com/broadinstitute/gatk/issues/8733:532,Deployability,toggle,toggle,532,"The current implementation of VETS borrowed VQSR's logic for classifying variants into SNPs and indels, for which separate models are trained. We retained this logic to make comparisons with VQSR as straightforward as possible. In this logic, which originates from htsjdk, an alternate allele with len(REF) = len(ALT) is counted as a MNP and classified as a SNP. However, we are now applying VETS to long-read genotyping of SVs, where inversions satisfy the same criterion but are then awkwardly classified as SNPs. We should add a toggle to allow classification of MNPs as indels, probably retaining the old behavior as default to not cause any changes for e.g. GVS. Tagging @koncheto-broad and @fabio-cunial for their visibility.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8733
https://github.com/broadinstitute/gatk/issues/8733:51,Testability,log,logic,51,"The current implementation of VETS borrowed VQSR's logic for classifying variants into SNPs and indels, for which separate models are trained. We retained this logic to make comparisons with VQSR as straightforward as possible. In this logic, which originates from htsjdk, an alternate allele with len(REF) = len(ALT) is counted as a MNP and classified as a SNP. However, we are now applying VETS to long-read genotyping of SVs, where inversions satisfy the same criterion but are then awkwardly classified as SNPs. We should add a toggle to allow classification of MNPs as indels, probably retaining the old behavior as default to not cause any changes for e.g. GVS. Tagging @koncheto-broad and @fabio-cunial for their visibility.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8733
https://github.com/broadinstitute/gatk/issues/8733:160,Testability,log,logic,160,"The current implementation of VETS borrowed VQSR's logic for classifying variants into SNPs and indels, for which separate models are trained. We retained this logic to make comparisons with VQSR as straightforward as possible. In this logic, which originates from htsjdk, an alternate allele with len(REF) = len(ALT) is counted as a MNP and classified as a SNP. However, we are now applying VETS to long-read genotyping of SVs, where inversions satisfy the same criterion but are then awkwardly classified as SNPs. We should add a toggle to allow classification of MNPs as indels, probably retaining the old behavior as default to not cause any changes for e.g. GVS. Tagging @koncheto-broad and @fabio-cunial for their visibility.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8733
https://github.com/broadinstitute/gatk/issues/8733:236,Testability,log,logic,236,"The current implementation of VETS borrowed VQSR's logic for classifying variants into SNPs and indels, for which separate models are trained. We retained this logic to make comparisons with VQSR as straightforward as possible. In this logic, which originates from htsjdk, an alternate allele with len(REF) = len(ALT) is counted as a MNP and classified as a SNP. However, we are now applying VETS to long-read genotyping of SVs, where inversions satisfy the same criterion but are then awkwardly classified as SNPs. We should add a toggle to allow classification of MNPs as indels, probably retaining the old behavior as default to not cause any changes for e.g. GVS. Tagging @koncheto-broad and @fabio-cunial for their visibility.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8733
https://github.com/broadinstitute/gatk/pull/8734:79,Availability,Echo,EchoCallset,79,"Exclude the field AS_YNG from VCFs extracted from GVS.; This is the merge into EchoCallset; Integration tests (run on the branch into ah_var_store) [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/ffed3910-761e-487f-98b3-c0582acc13e8); Note that I had to update the truth data (since the contents of the VCFs had changed, and that there is one failure, but it's due to a slight cost difference - which is presumably in the noise.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8734
https://github.com/broadinstitute/gatk/pull/8734:374,Availability,failure,failure,374,"Exclude the field AS_YNG from VCFs extracted from GVS.; This is the merge into EchoCallset; Integration tests (run on the branch into ah_var_store) [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/ffed3910-761e-487f-98b3-c0582acc13e8); Note that I had to update the truth data (since the contents of the VCFs had changed, and that there is one failure, but it's due to a slight cost difference - which is presumably in the noise.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8734
https://github.com/broadinstitute/gatk/pull/8734:92,Deployability,Integrat,Integration,92,"Exclude the field AS_YNG from VCFs extracted from GVS.; This is the merge into EchoCallset; Integration tests (run on the branch into ah_var_store) [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/ffed3910-761e-487f-98b3-c0582acc13e8); Note that I had to update the truth data (since the contents of the VCFs had changed, and that there is one failure, but it's due to a slight cost difference - which is presumably in the noise.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8734
https://github.com/broadinstitute/gatk/pull/8734:285,Deployability,update,update,285,"Exclude the field AS_YNG from VCFs extracted from GVS.; This is the merge into EchoCallset; Integration tests (run on the branch into ah_var_store) [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/ffed3910-761e-487f-98b3-c0582acc13e8); Note that I had to update the truth data (since the contents of the VCFs had changed, and that there is one failure, but it's due to a slight cost difference - which is presumably in the noise.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8734
https://github.com/broadinstitute/gatk/pull/8734:92,Integrability,Integrat,Integration,92,"Exclude the field AS_YNG from VCFs extracted from GVS.; This is the merge into EchoCallset; Integration tests (run on the branch into ah_var_store) [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/ffed3910-761e-487f-98b3-c0582acc13e8); Note that I had to update the truth data (since the contents of the VCFs had changed, and that there is one failure, but it's due to a slight cost difference - which is presumably in the noise.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8734
https://github.com/broadinstitute/gatk/pull/8734:104,Testability,test,tests,104,"Exclude the field AS_YNG from VCFs extracted from GVS.; This is the merge into EchoCallset; Integration tests (run on the branch into ah_var_store) [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/ffed3910-761e-487f-98b3-c0582acc13e8); Note that I had to update the truth data (since the contents of the VCFs had changed, and that there is one failure, but it's due to a slight cost difference - which is presumably in the noise.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8734
https://github.com/broadinstitute/gatk/issues/8735:657,Availability,error,error,657,"Dear developers,; At present, I am using GATK's CombineGVCFs module in AWS platform to merge gvcf of 30 samples respectively according to different chromosomes. This species has 10 chromosomes, so a total of 10 CombineGVCFs tasks are carried out in parallel (10 EC2 virtual machines are opened respectively). The input gvcf file is stored in S3 and mounted to the EC2 VM. In this process, java.io.IOException occurs after some samples are analyzed: The Transport endpoint is not connected, but the merged gvcf file and its index are still produced. I did not find any feedback about GATK relation on the Internet, so I would like to know the reason for the error and why some staining machines reported errors. Some will not report errors, in addition, will the gvcf file generated after the ""Transport endpoint is not connected"" prompt be used? At present, I have tried gatk4.5, 4.4, 4.2 and other versions, and this has happened. Paste the run log as follows:. 06:26:14.775 INFO  NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.5.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 06:26:14.867 INFO  CombineGVCFs - ------------------------------------------------------------; 06:26:14.869 INFO  CombineGVCFs - The Genome Analysis Toolkit (GATK) v4.5.0.0; 06:26:14.869 INFO  CombineGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 06:26:14.869 INFO  CombineGVCFs - Executing as root@ip-10-1-156-254.cn-northwest-1.compute.internal on Linux v4.14.334-252.552.amzn2.x86_64 amd64; 06:26:14.869 INFO  CombineGVCFs - Java runtime: OpenJDK 64-Bit Server VM v17.0.9+9-Ubuntu-122.04; 06:26:14.869 INFO  CombineGVCFs - Start Date/Time: March 13, 2024 at 6:26:14 AM GMT; 06:26:14.869 INFO  CombineGVCFs - ------------------------------------------------------------; 06:26:14.869 INFO  CombineGVCFs - ------------------------------------------------------------; 06:26:14.870 INFO  CombineGVCFs - HTSJDK Version: 4.1.0; 06:2",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8735
https://github.com/broadinstitute/gatk/issues/8735:703,Availability,error,errors,703,"Dear developers,; At present, I am using GATK's CombineGVCFs module in AWS platform to merge gvcf of 30 samples respectively according to different chromosomes. This species has 10 chromosomes, so a total of 10 CombineGVCFs tasks are carried out in parallel (10 EC2 virtual machines are opened respectively). The input gvcf file is stored in S3 and mounted to the EC2 VM. In this process, java.io.IOException occurs after some samples are analyzed: The Transport endpoint is not connected, but the merged gvcf file and its index are still produced. I did not find any feedback about GATK relation on the Internet, so I would like to know the reason for the error and why some staining machines reported errors. Some will not report errors, in addition, will the gvcf file generated after the ""Transport endpoint is not connected"" prompt be used? At present, I have tried gatk4.5, 4.4, 4.2 and other versions, and this has happened. Paste the run log as follows:. 06:26:14.775 INFO  NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.5.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 06:26:14.867 INFO  CombineGVCFs - ------------------------------------------------------------; 06:26:14.869 INFO  CombineGVCFs - The Genome Analysis Toolkit (GATK) v4.5.0.0; 06:26:14.869 INFO  CombineGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 06:26:14.869 INFO  CombineGVCFs - Executing as root@ip-10-1-156-254.cn-northwest-1.compute.internal on Linux v4.14.334-252.552.amzn2.x86_64 amd64; 06:26:14.869 INFO  CombineGVCFs - Java runtime: OpenJDK 64-Bit Server VM v17.0.9+9-Ubuntu-122.04; 06:26:14.869 INFO  CombineGVCFs - Start Date/Time: March 13, 2024 at 6:26:14 AM GMT; 06:26:14.869 INFO  CombineGVCFs - ------------------------------------------------------------; 06:26:14.869 INFO  CombineGVCFs - ------------------------------------------------------------; 06:26:14.870 INFO  CombineGVCFs - HTSJDK Version: 4.1.0; 06:2",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8735
https://github.com/broadinstitute/gatk/issues/8735:732,Availability,error,errors,732,"Dear developers,; At present, I am using GATK's CombineGVCFs module in AWS platform to merge gvcf of 30 samples respectively according to different chromosomes. This species has 10 chromosomes, so a total of 10 CombineGVCFs tasks are carried out in parallel (10 EC2 virtual machines are opened respectively). The input gvcf file is stored in S3 and mounted to the EC2 VM. In this process, java.io.IOException occurs after some samples are analyzed: The Transport endpoint is not connected, but the merged gvcf file and its index are still produced. I did not find any feedback about GATK relation on the Internet, so I would like to know the reason for the error and why some staining machines reported errors. Some will not report errors, in addition, will the gvcf file generated after the ""Transport endpoint is not connected"" prompt be used? At present, I have tried gatk4.5, 4.4, 4.2 and other versions, and this has happened. Paste the run log as follows:. 06:26:14.775 INFO  NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.5.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 06:26:14.867 INFO  CombineGVCFs - ------------------------------------------------------------; 06:26:14.869 INFO  CombineGVCFs - The Genome Analysis Toolkit (GATK) v4.5.0.0; 06:26:14.869 INFO  CombineGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 06:26:14.869 INFO  CombineGVCFs - Executing as root@ip-10-1-156-254.cn-northwest-1.compute.internal on Linux v4.14.334-252.552.amzn2.x86_64 amd64; 06:26:14.869 INFO  CombineGVCFs - Java runtime: OpenJDK 64-Bit Server VM v17.0.9+9-Ubuntu-122.04; 06:26:14.869 INFO  CombineGVCFs - Start Date/Time: March 13, 2024 at 6:26:14 AM GMT; 06:26:14.869 INFO  CombineGVCFs - ------------------------------------------------------------; 06:26:14.869 INFO  CombineGVCFs - ------------------------------------------------------------; 06:26:14.870 INFO  CombineGVCFs - HTSJDK Version: 4.1.0; 06:2",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8735
https://github.com/broadinstitute/gatk/issues/8735:21979,Availability,down,down,21979,"9.8             114664000        5778139.5; 06:46:23.072 INFO  ProgressMeter - NC_038255.2:25849821             20.0             115712000        5782366.7; 06:46:33.076 INFO  ProgressMeter - NC_038255.2:26067132             20.2             116682000        5782658.4; 06:46:43.076 INFO  ProgressMeter - NC_038255.2:26257373             20.3             117691000        5784881.3; 06:46:53.083 INFO  ProgressMeter - NC_038255.2:26457381             20.5             118693000        5786693.9; 06:47:03.088 INFO  ProgressMeter - NC_038255.2:26643575             20.7             119720000        5789695.5; 06:47:13.106 INFO  ProgressMeter - NC_038255.2:26850339             20.8             120726000        5791581.5; 06:47:23.111 INFO  ProgressMeter - NC_038255.2:27050560             21.0             121742000        5793973.2; 06:47:33.116 INFO  ProgressMeter - NC_038255.2:27256247             21.2             122736000        5795288.5; 06:47:42.432 INFO  CombineGVCFs - Shutting down engine; [March 13, 2024 at 6:47:42 AM GMT] org.broadinstitute.hellbender.tools.walkers.CombineGVCFs done. Elapsed time: 21.46 minutes.; Runtime.totalMemory()=920649728; htsjdk.samtools.util.RuntimeIOException: java.io.IOException: Transport endpoint is not connected; at htsjdk.tribble.readers.TabixIteratorLineReader.readLine(TabixIteratorLineReader.java:48); at htsjdk.tribble.TabixFeatureReader$FeatureIterator.readNextRecord(TabixFeatureReader.java:170); at htsjdk.tribble.TabixFeatureReader$FeatureIterator.next(TabixFeatureReader.java:205); at htsjdk.tribble.TabixFeatureReader$FeatureIterator.next(TabixFeatureReader.java:149); at org.broadinstitute.hellbender.engine.FeatureIntervalIterator.loadNextFeature(FeatureIntervalIterator.java:98); at org.broadinstitute.hellbender.engine.FeatureIntervalIterator.loadNextNovelFeature(FeatureIntervalIterator.java:74); at org.broadinstitute.hellbender.engine.FeatureIntervalIterator.next(FeatureIntervalIterator.java:62); at org.broadinstitute.hellbender.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8735
https://github.com/broadinstitute/gatk/issues/8735:26177,Availability,avail,available,26177,"le.java:405); at htsjdk.samtools.seekablestream.SeekableFileStream.read(SeekableFileStream.java:85); at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:244); at java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:284); at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:343); at htsjdk.samtools.seekablestream.SeekableBufferedStream.read(SeekableBufferedStream.java:133); at htsjdk.samtools.util.BlockCompressedInputStream.readBytes(BlockCompressedInputStream.java:582); at htsjdk.samtools.util.BlockCompressedInputStream.readBytes(BlockCompressedInputStream.java:571); at htsjdk.samtools.util.BlockCompressedInputStream.processNextBlock(BlockCompressedInputStream.java:536); at htsjdk.samtools.util.BlockCompressedInputStream.nextBlock(BlockCompressedInputStream.java:479); at htsjdk.samtools.util.BlockCompressedInputStream.readBlock(BlockCompressedInputStream.java:469); at htsjdk.samtools.util.BlockCompressedInputStream.available(BlockCompressedInputStream.java:207); at htsjdk.samtools.util.BlockCompressedInputStream.read(BlockCompressedInputStream.java:252); at htsjdk.tribble.readers.TabixReader.readLine(TabixReader.java:215); at htsjdk.tribble.readers.TabixReader$IteratorImpl.next(TabixReader.java:434); at htsjdk.tribble.readers.TabixIteratorLineReader.readLine(TabixIteratorLineReader.java:46); ... 29 more; Using GATK jar /gatk/gatk-package-4.5.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.5.0.0-local.jar CombineGVCFs -R ./test/test.fna -V ./gvcf_all.list -L NC_038255.2 -O ./NC_038255.2.merged.g.vcf.gz; total 2.3G; '""-rw-rw-rw- 1 root root  3.6K Dec 13 23:32 GATKConfig.EXAMPLE.properties""; drwxr-xr-x 2 root root  4.0K Mar 13 06:26 GCF_000004515.6_Glycine_max_v4.0; '""-rw-r--r-- 1 root root  1.6G Mar 13 06:47 NC_038255.2.merged.g.vcf.gz""; '""-rw-r--r-- 1 root",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8735
https://github.com/broadinstitute/gatk/issues/8735:28247,Availability,Avail,Avail,28247,"re; Using GATK jar /gatk/gatk-package-4.5.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.5.0.0-local.jar CombineGVCFs -R ./test/test.fna -V ./gvcf_all.list -L NC_038255.2 -O ./NC_038255.2.merged.g.vcf.gz; total 2.3G; '""-rw-rw-rw- 1 root root  3.6K Dec 13 23:32 GATKConfig.EXAMPLE.properties""; drwxr-xr-x 2 root root  4.0K Mar 13 06:26 GCF_000004515.6_Glycine_max_v4.0; '""-rw-r--r-- 1 root root  1.6G Mar 13 06:47 NC_038255.2.merged.g.vcf.gz""; '""-rw-r--r-- 1 root root   24K Mar 13 06:47 NC_038255.2.merged.g.vcf.gz.tbi""; '""-rw-rw-rw- 1 root root   40K Dec 13 23:32 README.md""; '""-rwxrwxrwx 1 root root   21K Dec 13 23:32 gatk""; '""-rw-rw-rw- 1 root root 1016K Dec 13 23:32 gatk-completion.sh""; '""-rw-rw-rw- 1 root root  422M Dec 13 23:32 gatk-package-4.5.0.0-local.jar""; '""-rw-rw-rw- 1 root root  320M Dec 13 23:32 gatk-package-4.5.0.0-spark.jar""; lrwxrwxrwx 1 root root    36 Dec 13 23:33 gatk-spark.jar -> /gatk/gatk-package-4.5.0.0-spark.jar; lrwxrwxrwx 1 root root    36 Dec 13 23:33 gatk.jar -> /gatk/gatk-package-4.5.0.0-local.jar; '""-rw-rw-rw- 1 root root  117K Dec 13 23:32 gatkPythonPackageArchive.zip""; '""-rw-rw-rw- 1 root root  4.2K Dec 13 23:32 gatkcondaenv.yml""; '""-rw-r--r-- 1 root root    53 Dec 13 23:37 gatkenv.rc""; '""-rw-r--r-- 1 root root  2.3K Mar 13 06:26 gvcf_all.list""; '""-rw-r--r-- 1 root root   866 Dec 13 23:33 run_unit_tests.sh""; drwxrwxrwx 5 root root  4.0K Dec 13 23:32 scripts; '""-rw-r--r-- 1 root root  1.3K Mar 13 06:26 wgs_jcalling_combine_gvcf_job.sh""; Filesystem      Size  Used Avail Use% Mounted on; overlay         100G   13G   88G  13% /; tmpfs            64M     0   64M   0% /dev; tmpfs           7.7G     0  7.7G   0% /sys/fs/cgroup; /dev/nvme0n1p1  100G   13G   88G  13% /etc/hosts; shm              64M     0   64M   0% /dev/shm; wgs-pipeline    1.0P     0  1.0P   0% /mnt. Thank you very much",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8735
https://github.com/broadinstitute/gatk/issues/8735:28511,Deployability,pipeline,pipeline,28511,"re; Using GATK jar /gatk/gatk-package-4.5.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.5.0.0-local.jar CombineGVCFs -R ./test/test.fna -V ./gvcf_all.list -L NC_038255.2 -O ./NC_038255.2.merged.g.vcf.gz; total 2.3G; '""-rw-rw-rw- 1 root root  3.6K Dec 13 23:32 GATKConfig.EXAMPLE.properties""; drwxr-xr-x 2 root root  4.0K Mar 13 06:26 GCF_000004515.6_Glycine_max_v4.0; '""-rw-r--r-- 1 root root  1.6G Mar 13 06:47 NC_038255.2.merged.g.vcf.gz""; '""-rw-r--r-- 1 root root   24K Mar 13 06:47 NC_038255.2.merged.g.vcf.gz.tbi""; '""-rw-rw-rw- 1 root root   40K Dec 13 23:32 README.md""; '""-rwxrwxrwx 1 root root   21K Dec 13 23:32 gatk""; '""-rw-rw-rw- 1 root root 1016K Dec 13 23:32 gatk-completion.sh""; '""-rw-rw-rw- 1 root root  422M Dec 13 23:32 gatk-package-4.5.0.0-local.jar""; '""-rw-rw-rw- 1 root root  320M Dec 13 23:32 gatk-package-4.5.0.0-spark.jar""; lrwxrwxrwx 1 root root    36 Dec 13 23:33 gatk-spark.jar -> /gatk/gatk-package-4.5.0.0-spark.jar; lrwxrwxrwx 1 root root    36 Dec 13 23:33 gatk.jar -> /gatk/gatk-package-4.5.0.0-local.jar; '""-rw-rw-rw- 1 root root  117K Dec 13 23:32 gatkPythonPackageArchive.zip""; '""-rw-rw-rw- 1 root root  4.2K Dec 13 23:32 gatkcondaenv.yml""; '""-rw-r--r-- 1 root root    53 Dec 13 23:37 gatkenv.rc""; '""-rw-r--r-- 1 root root  2.3K Mar 13 06:26 gvcf_all.list""; '""-rw-r--r-- 1 root root   866 Dec 13 23:33 run_unit_tests.sh""; drwxrwxrwx 5 root root  4.0K Dec 13 23:32 scripts; '""-rw-r--r-- 1 root root  1.3K Mar 13 06:26 wgs_jcalling_combine_gvcf_job.sh""; Filesystem      Size  Used Avail Use% Mounted on; overlay         100G   13G   88G  13% /; tmpfs            64M     0   64M   0% /dev; tmpfs           7.7G     0  7.7G   0% /sys/fs/cgroup; /dev/nvme0n1p1  100G   13G   88G  13% /etc/hosts; shm              64M     0   64M   0% /dev/shm; wgs-pipeline    1.0P     0  1.0P   0% /mnt. Thank you very much",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8735
https://github.com/broadinstitute/gatk/issues/8735:23785,Integrability,wrap,wrapAndCopyInto,23785,NextNovelFeature(FeatureIntervalIterator.java:74); at org.broadinstitute.hellbender.engine.FeatureIntervalIterator.next(FeatureIntervalIterator.java:62); at org.broadinstitute.hellbender.engine.FeatureIntervalIterator.next(FeatureIntervalIterator.java:24); at org.broadinstitute.hellbender.engine.MultiVariantDataSource$1.next(MultiVariantDataSource.java:408); at org.broadinstitute.hellbender.engine.MultiVariantDataSource$1.next(MultiVariantDataSource.java:393); at htsjdk.samtools.util.PeekableIterator.advance(PeekableIterator.java:71); at htsjdk.samtools.util.PeekableIterator.next(PeekableIterator.java:57); at htsjdk.samtools.util.MergingIterator.next(MergingIterator.java:101); at java.base/java.util.Iterator.forEachRemaining(Iterator.java:133); at java.base/java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1845); at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509); at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499); at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150); at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173); at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:596); at org.broadinstitute.hellbender.engine.MultiVariantWalker.traverse(MultiVariantWalker.java:136); at org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart.traverse(MultiVariantWalkerGroupedOnStart.java:165); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1098); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:149); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:198); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:21,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8735
https://github.com/broadinstitute/gatk/issues/8735:1004,Performance,Load,Loading,1004,"ear developers,; At present, I am using GATK's CombineGVCFs module in AWS platform to merge gvcf of 30 samples respectively according to different chromosomes. This species has 10 chromosomes, so a total of 10 CombineGVCFs tasks are carried out in parallel (10 EC2 virtual machines are opened respectively). The input gvcf file is stored in S3 and mounted to the EC2 VM. In this process, java.io.IOException occurs after some samples are analyzed: The Transport endpoint is not connected, but the merged gvcf file and its index are still produced. I did not find any feedback about GATK relation on the Internet, so I would like to know the reason for the error and why some staining machines reported errors. Some will not report errors, in addition, will the gvcf file generated after the ""Transport endpoint is not connected"" prompt be used? At present, I have tried gatk4.5, 4.4, 4.2 and other versions, and this has happened. Paste the run log as follows:. 06:26:14.775 INFO  NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.5.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 06:26:14.867 INFO  CombineGVCFs - ------------------------------------------------------------; 06:26:14.869 INFO  CombineGVCFs - The Genome Analysis Toolkit (GATK) v4.5.0.0; 06:26:14.869 INFO  CombineGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 06:26:14.869 INFO  CombineGVCFs - Executing as root@ip-10-1-156-254.cn-northwest-1.compute.internal on Linux v4.14.334-252.552.amzn2.x86_64 amd64; 06:26:14.869 INFO  CombineGVCFs - Java runtime: OpenJDK 64-Bit Server VM v17.0.9+9-Ubuntu-122.04; 06:26:14.869 INFO  CombineGVCFs - Start Date/Time: March 13, 2024 at 6:26:14 AM GMT; 06:26:14.869 INFO  CombineGVCFs - ------------------------------------------------------------; 06:26:14.869 INFO  CombineGVCFs - ------------------------------------------------------------; 06:26:14.870 INFO  CombineGVCFs - HTSJDK Version: 4.1.0; 06:26",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8735
https://github.com/broadinstitute/gatk/issues/8735:22683,Performance,load,loadNextFeature,22683,"581.5; 06:47:23.111 INFO  ProgressMeter - NC_038255.2:27050560             21.0             121742000        5793973.2; 06:47:33.116 INFO  ProgressMeter - NC_038255.2:27256247             21.2             122736000        5795288.5; 06:47:42.432 INFO  CombineGVCFs - Shutting down engine; [March 13, 2024 at 6:47:42 AM GMT] org.broadinstitute.hellbender.tools.walkers.CombineGVCFs done. Elapsed time: 21.46 minutes.; Runtime.totalMemory()=920649728; htsjdk.samtools.util.RuntimeIOException: java.io.IOException: Transport endpoint is not connected; at htsjdk.tribble.readers.TabixIteratorLineReader.readLine(TabixIteratorLineReader.java:48); at htsjdk.tribble.TabixFeatureReader$FeatureIterator.readNextRecord(TabixFeatureReader.java:170); at htsjdk.tribble.TabixFeatureReader$FeatureIterator.next(TabixFeatureReader.java:205); at htsjdk.tribble.TabixFeatureReader$FeatureIterator.next(TabixFeatureReader.java:149); at org.broadinstitute.hellbender.engine.FeatureIntervalIterator.loadNextFeature(FeatureIntervalIterator.java:98); at org.broadinstitute.hellbender.engine.FeatureIntervalIterator.loadNextNovelFeature(FeatureIntervalIterator.java:74); at org.broadinstitute.hellbender.engine.FeatureIntervalIterator.next(FeatureIntervalIterator.java:62); at org.broadinstitute.hellbender.engine.FeatureIntervalIterator.next(FeatureIntervalIterator.java:24); at org.broadinstitute.hellbender.engine.MultiVariantDataSource$1.next(MultiVariantDataSource.java:408); at org.broadinstitute.hellbender.engine.MultiVariantDataSource$1.next(MultiVariantDataSource.java:393); at htsjdk.samtools.util.PeekableIterator.advance(PeekableIterator.java:71); at htsjdk.samtools.util.PeekableIterator.next(PeekableIterator.java:57); at htsjdk.samtools.util.MergingIterator.next(MergingIterator.java:101); at java.base/java.util.Iterator.forEachRemaining(Iterator.java:133); at java.base/java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1845); at java.base/java.util.stream.AbstractPipeline.co",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8735
https://github.com/broadinstitute/gatk/issues/8735:22797,Performance,load,loadNextNovelFeature,22797,".2; 06:47:33.116 INFO  ProgressMeter - NC_038255.2:27256247             21.2             122736000        5795288.5; 06:47:42.432 INFO  CombineGVCFs - Shutting down engine; [March 13, 2024 at 6:47:42 AM GMT] org.broadinstitute.hellbender.tools.walkers.CombineGVCFs done. Elapsed time: 21.46 minutes.; Runtime.totalMemory()=920649728; htsjdk.samtools.util.RuntimeIOException: java.io.IOException: Transport endpoint is not connected; at htsjdk.tribble.readers.TabixIteratorLineReader.readLine(TabixIteratorLineReader.java:48); at htsjdk.tribble.TabixFeatureReader$FeatureIterator.readNextRecord(TabixFeatureReader.java:170); at htsjdk.tribble.TabixFeatureReader$FeatureIterator.next(TabixFeatureReader.java:205); at htsjdk.tribble.TabixFeatureReader$FeatureIterator.next(TabixFeatureReader.java:149); at org.broadinstitute.hellbender.engine.FeatureIntervalIterator.loadNextFeature(FeatureIntervalIterator.java:98); at org.broadinstitute.hellbender.engine.FeatureIntervalIterator.loadNextNovelFeature(FeatureIntervalIterator.java:74); at org.broadinstitute.hellbender.engine.FeatureIntervalIterator.next(FeatureIntervalIterator.java:62); at org.broadinstitute.hellbender.engine.FeatureIntervalIterator.next(FeatureIntervalIterator.java:24); at org.broadinstitute.hellbender.engine.MultiVariantDataSource$1.next(MultiVariantDataSource.java:408); at org.broadinstitute.hellbender.engine.MultiVariantDataSource$1.next(MultiVariantDataSource.java:393); at htsjdk.samtools.util.PeekableIterator.advance(PeekableIterator.java:71); at htsjdk.samtools.util.PeekableIterator.next(PeekableIterator.java:57); at htsjdk.samtools.util.MergingIterator.next(MergingIterator.java:101); at java.base/java.util.Iterator.forEachRemaining(Iterator.java:133); at java.base/java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1845); at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509); at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.ja",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8735
https://github.com/broadinstitute/gatk/issues/8735:7419,Safety,Detect,Detected,7419,"eManager - Using codec VCFCodec to read file file:///mnt/OutData/proj1/CRR108494/CRR108494.g.vcf.gz; 06:26:19.874 INFO  FeatureManager - Using codec VCFCodec to read file file:///mnt/OutData/proj1/CRR108508/CRR108508.g.vcf.gz; 06:26:20.052 INFO  FeatureManager - Using codec VCFCodec to read file file:///mnt/OutData/proj1/CRR108528/CRR108528.g.vcf.gz; 06:26:20.221 INFO  FeatureManager - Using codec VCFCodec to read file file:///mnt/OutData/proj1/CRR108547/CRR108547.g.vcf.gz; 06:26:20.417 INFO  FeatureManager - Using codec VCFCodec to read file file:///mnt/OutData/proj1/CRR108592/CRR108592.g.vcf.gz; 06:26:22.363 INFO  IntervalArgumentCollection - Processing 51272880 bp from intervals; 06:26:22.388 INFO  CombineGVCFs - Done initializing engine; 06:26:22.401 INFO  ProgressMeter - Starting traversal; 06:26:22.401 INFO  ProgressMeter -        Current Locus  Elapsed Minutes    Variants Processed  Variants/Minute; 06:26:24.884 WARN  ReferenceConfidenceVariantContextMerger - Detected invalid annotations: When trying to merge variant contexts at location NC_038255.2:36 the annotation MLEAC=[1, 0] was not a numerical value and was ignored; 06:26:32.408 INFO  ProgressMeter -   NC_038255.2:132104              0.2                702000        4209474.3; 06:26:42.411 INFO  ProgressMeter -   NC_038255.2:329020              0.3               1725000        5172413.8; 06:26:52.415 INFO  ProgressMeter -   NC_038255.2:548109              0.5               2781000        5559405.6; 06:27:02.421 INFO  ProgressMeter -   NC_038255.2:735845              0.7               3727000        5587706.1; 06:27:12.426 INFO  ProgressMeter -   NC_038255.2:934042              0.8               4757000        5705547.2; 06:27:22.431 INFO  ProgressMeter -  NC_038255.2:1124882              1.0               5747000        5744127.9; 06:27:32.437 INFO  ProgressMeter -  NC_038255.2:1314436              1.2               6761000        5792164.0; 06:27:42.439 INFO  ProgressMeter -  NC_038255.2:1509338       ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8735
https://github.com/broadinstitute/gatk/issues/8735:946,Testability,log,log,946,"Dear developers,; At present, I am using GATK's CombineGVCFs module in AWS platform to merge gvcf of 30 samples respectively according to different chromosomes. This species has 10 chromosomes, so a total of 10 CombineGVCFs tasks are carried out in parallel (10 EC2 virtual machines are opened respectively). The input gvcf file is stored in S3 and mounted to the EC2 VM. In this process, java.io.IOException occurs after some samples are analyzed: The Transport endpoint is not connected, but the merged gvcf file and its index are still produced. I did not find any feedback about GATK relation on the Internet, so I would like to know the reason for the error and why some staining machines reported errors. Some will not report errors, in addition, will the gvcf file generated after the ""Transport endpoint is not connected"" prompt be used? At present, I have tried gatk4.5, 4.4, 4.2 and other versions, and this has happened. Paste the run log as follows:. 06:26:14.775 INFO  NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.5.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 06:26:14.867 INFO  CombineGVCFs - ------------------------------------------------------------; 06:26:14.869 INFO  CombineGVCFs - The Genome Analysis Toolkit (GATK) v4.5.0.0; 06:26:14.869 INFO  CombineGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 06:26:14.869 INFO  CombineGVCFs - Executing as root@ip-10-1-156-254.cn-northwest-1.compute.internal on Linux v4.14.334-252.552.amzn2.x86_64 amd64; 06:26:14.869 INFO  CombineGVCFs - Java runtime: OpenJDK 64-Bit Server VM v17.0.9+9-Ubuntu-122.04; 06:26:14.869 INFO  CombineGVCFs - Start Date/Time: March 13, 2024 at 6:26:14 AM GMT; 06:26:14.869 INFO  CombineGVCFs - ------------------------------------------------------------; 06:26:14.869 INFO  CombineGVCFs - ------------------------------------------------------------; 06:26:14.870 INFO  CombineGVCFs - HTSJDK Version: 4.1.0; 06:2",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8735
https://github.com/broadinstitute/gatk/issues/8735:26857,Testability,test,test,26857,"InputStream.processNextBlock(BlockCompressedInputStream.java:536); at htsjdk.samtools.util.BlockCompressedInputStream.nextBlock(BlockCompressedInputStream.java:479); at htsjdk.samtools.util.BlockCompressedInputStream.readBlock(BlockCompressedInputStream.java:469); at htsjdk.samtools.util.BlockCompressedInputStream.available(BlockCompressedInputStream.java:207); at htsjdk.samtools.util.BlockCompressedInputStream.read(BlockCompressedInputStream.java:252); at htsjdk.tribble.readers.TabixReader.readLine(TabixReader.java:215); at htsjdk.tribble.readers.TabixReader$IteratorImpl.next(TabixReader.java:434); at htsjdk.tribble.readers.TabixIteratorLineReader.readLine(TabixIteratorLineReader.java:46); ... 29 more; Using GATK jar /gatk/gatk-package-4.5.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.5.0.0-local.jar CombineGVCFs -R ./test/test.fna -V ./gvcf_all.list -L NC_038255.2 -O ./NC_038255.2.merged.g.vcf.gz; total 2.3G; '""-rw-rw-rw- 1 root root  3.6K Dec 13 23:32 GATKConfig.EXAMPLE.properties""; drwxr-xr-x 2 root root  4.0K Mar 13 06:26 GCF_000004515.6_Glycine_max_v4.0; '""-rw-r--r-- 1 root root  1.6G Mar 13 06:47 NC_038255.2.merged.g.vcf.gz""; '""-rw-r--r-- 1 root root   24K Mar 13 06:47 NC_038255.2.merged.g.vcf.gz.tbi""; '""-rw-rw-rw- 1 root root   40K Dec 13 23:32 README.md""; '""-rwxrwxrwx 1 root root   21K Dec 13 23:32 gatk""; '""-rw-rw-rw- 1 root root 1016K Dec 13 23:32 gatk-completion.sh""; '""-rw-rw-rw- 1 root root  422M Dec 13 23:32 gatk-package-4.5.0.0-local.jar""; '""-rw-rw-rw- 1 root root  320M Dec 13 23:32 gatk-package-4.5.0.0-spark.jar""; lrwxrwxrwx 1 root root    36 Dec 13 23:33 gatk-spark.jar -> /gatk/gatk-package-4.5.0.0-spark.jar; lrwxrwxrwx 1 root root    36 Dec 13 23:33 gatk.jar -> /gatk/gatk-package-4.5.0.0-local.jar; '""-rw-rw-rw- 1 root root  117K Dec 13 23:32 gatkPythonPackageArchive.zip""; '""-rw-rw-rw- 1 r",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8735
https://github.com/broadinstitute/gatk/issues/8735:26862,Testability,test,test,26862,"InputStream.processNextBlock(BlockCompressedInputStream.java:536); at htsjdk.samtools.util.BlockCompressedInputStream.nextBlock(BlockCompressedInputStream.java:479); at htsjdk.samtools.util.BlockCompressedInputStream.readBlock(BlockCompressedInputStream.java:469); at htsjdk.samtools.util.BlockCompressedInputStream.available(BlockCompressedInputStream.java:207); at htsjdk.samtools.util.BlockCompressedInputStream.read(BlockCompressedInputStream.java:252); at htsjdk.tribble.readers.TabixReader.readLine(TabixReader.java:215); at htsjdk.tribble.readers.TabixReader$IteratorImpl.next(TabixReader.java:434); at htsjdk.tribble.readers.TabixIteratorLineReader.readLine(TabixIteratorLineReader.java:46); ... 29 more; Using GATK jar /gatk/gatk-package-4.5.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.5.0.0-local.jar CombineGVCFs -R ./test/test.fna -V ./gvcf_all.list -L NC_038255.2 -O ./NC_038255.2.merged.g.vcf.gz; total 2.3G; '""-rw-rw-rw- 1 root root  3.6K Dec 13 23:32 GATKConfig.EXAMPLE.properties""; drwxr-xr-x 2 root root  4.0K Mar 13 06:26 GCF_000004515.6_Glycine_max_v4.0; '""-rw-r--r-- 1 root root  1.6G Mar 13 06:47 NC_038255.2.merged.g.vcf.gz""; '""-rw-r--r-- 1 root root   24K Mar 13 06:47 NC_038255.2.merged.g.vcf.gz.tbi""; '""-rw-rw-rw- 1 root root   40K Dec 13 23:32 README.md""; '""-rwxrwxrwx 1 root root   21K Dec 13 23:32 gatk""; '""-rw-rw-rw- 1 root root 1016K Dec 13 23:32 gatk-completion.sh""; '""-rw-rw-rw- 1 root root  422M Dec 13 23:32 gatk-package-4.5.0.0-local.jar""; '""-rw-rw-rw- 1 root root  320M Dec 13 23:32 gatk-package-4.5.0.0-spark.jar""; lrwxrwxrwx 1 root root    36 Dec 13 23:33 gatk-spark.jar -> /gatk/gatk-package-4.5.0.0-spark.jar; lrwxrwxrwx 1 root root    36 Dec 13 23:33 gatk.jar -> /gatk/gatk-package-4.5.0.0-local.jar; '""-rw-rw-rw- 1 root root  117K Dec 13 23:32 gatkPythonPackageArchive.zip""; '""-rw-rw-rw- 1 r",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8735
https://github.com/broadinstitute/gatk/issues/8735:568,Usability,feedback,feedback,568,"Dear developers,; At present, I am using GATK's CombineGVCFs module in AWS platform to merge gvcf of 30 samples respectively according to different chromosomes. This species has 10 chromosomes, so a total of 10 CombineGVCFs tasks are carried out in parallel (10 EC2 virtual machines are opened respectively). The input gvcf file is stored in S3 and mounted to the EC2 VM. In this process, java.io.IOException occurs after some samples are analyzed: The Transport endpoint is not connected, but the merged gvcf file and its index are still produced. I did not find any feedback about GATK relation on the Internet, so I would like to know the reason for the error and why some staining machines reported errors. Some will not report errors, in addition, will the gvcf file generated after the ""Transport endpoint is not connected"" prompt be used? At present, I have tried gatk4.5, 4.4, 4.2 and other versions, and this has happened. Paste the run log as follows:. 06:26:14.775 INFO  NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.5.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 06:26:14.867 INFO  CombineGVCFs - ------------------------------------------------------------; 06:26:14.869 INFO  CombineGVCFs - The Genome Analysis Toolkit (GATK) v4.5.0.0; 06:26:14.869 INFO  CombineGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 06:26:14.869 INFO  CombineGVCFs - Executing as root@ip-10-1-156-254.cn-northwest-1.compute.internal on Linux v4.14.334-252.552.amzn2.x86_64 amd64; 06:26:14.869 INFO  CombineGVCFs - Java runtime: OpenJDK 64-Bit Server VM v17.0.9+9-Ubuntu-122.04; 06:26:14.869 INFO  CombineGVCFs - Start Date/Time: March 13, 2024 at 6:26:14 AM GMT; 06:26:14.869 INFO  CombineGVCFs - ------------------------------------------------------------; 06:26:14.869 INFO  CombineGVCFs - ------------------------------------------------------------; 06:26:14.870 INFO  CombineGVCFs - HTSJDK Version: 4.1.0; 06:2",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8735
https://github.com/broadinstitute/gatk/pull/8736:21,Deployability,integrat,integration,21,"This doesn't fix the integration test, but a bug in GvsBulkIngest",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8736
https://github.com/broadinstitute/gatk/pull/8736:21,Integrability,integrat,integration,21,"This doesn't fix the integration test, but a bug in GvsBulkIngest",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8736
https://github.com/broadinstitute/gatk/pull/8736:33,Testability,test,test,33,"This doesn't fix the integration test, but a bug in GvsBulkIngest",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8736
https://github.com/broadinstitute/gatk/pull/8737:66,Availability,Echo,EchoCallset,66,"This is a patch to fix the integration test that is broken in the EchoCallset.; There was refactoring done on GvsExtractAvroFilesForHail (in the EchoCallset branch) that has broken the inputs to the integration test on that branch. ; I'm not sure this is the perfect solution, but I'd like to get it merged into EchoCallset so we can unify EchoCallset and ah_var_store",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8737
https://github.com/broadinstitute/gatk/pull/8737:145,Availability,Echo,EchoCallset,145,"This is a patch to fix the integration test that is broken in the EchoCallset.; There was refactoring done on GvsExtractAvroFilesForHail (in the EchoCallset branch) that has broken the inputs to the integration test on that branch. ; I'm not sure this is the perfect solution, but I'd like to get it merged into EchoCallset so we can unify EchoCallset and ah_var_store",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8737
https://github.com/broadinstitute/gatk/pull/8737:312,Availability,Echo,EchoCallset,312,"This is a patch to fix the integration test that is broken in the EchoCallset.; There was refactoring done on GvsExtractAvroFilesForHail (in the EchoCallset branch) that has broken the inputs to the integration test on that branch. ; I'm not sure this is the perfect solution, but I'd like to get it merged into EchoCallset so we can unify EchoCallset and ah_var_store",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8737
https://github.com/broadinstitute/gatk/pull/8737:340,Availability,Echo,EchoCallset,340,"This is a patch to fix the integration test that is broken in the EchoCallset.; There was refactoring done on GvsExtractAvroFilesForHail (in the EchoCallset branch) that has broken the inputs to the integration test on that branch. ; I'm not sure this is the perfect solution, but I'd like to get it merged into EchoCallset so we can unify EchoCallset and ah_var_store",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8737
https://github.com/broadinstitute/gatk/pull/8737:10,Deployability,patch,patch,10,"This is a patch to fix the integration test that is broken in the EchoCallset.; There was refactoring done on GvsExtractAvroFilesForHail (in the EchoCallset branch) that has broken the inputs to the integration test on that branch. ; I'm not sure this is the perfect solution, but I'd like to get it merged into EchoCallset so we can unify EchoCallset and ah_var_store",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8737
https://github.com/broadinstitute/gatk/pull/8737:27,Deployability,integrat,integration,27,"This is a patch to fix the integration test that is broken in the EchoCallset.; There was refactoring done on GvsExtractAvroFilesForHail (in the EchoCallset branch) that has broken the inputs to the integration test on that branch. ; I'm not sure this is the perfect solution, but I'd like to get it merged into EchoCallset so we can unify EchoCallset and ah_var_store",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8737
https://github.com/broadinstitute/gatk/pull/8737:199,Deployability,integrat,integration,199,"This is a patch to fix the integration test that is broken in the EchoCallset.; There was refactoring done on GvsExtractAvroFilesForHail (in the EchoCallset branch) that has broken the inputs to the integration test on that branch. ; I'm not sure this is the perfect solution, but I'd like to get it merged into EchoCallset so we can unify EchoCallset and ah_var_store",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8737
https://github.com/broadinstitute/gatk/pull/8737:27,Integrability,integrat,integration,27,"This is a patch to fix the integration test that is broken in the EchoCallset.; There was refactoring done on GvsExtractAvroFilesForHail (in the EchoCallset branch) that has broken the inputs to the integration test on that branch. ; I'm not sure this is the perfect solution, but I'd like to get it merged into EchoCallset so we can unify EchoCallset and ah_var_store",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8737
https://github.com/broadinstitute/gatk/pull/8737:199,Integrability,integrat,integration,199,"This is a patch to fix the integration test that is broken in the EchoCallset.; There was refactoring done on GvsExtractAvroFilesForHail (in the EchoCallset branch) that has broken the inputs to the integration test on that branch. ; I'm not sure this is the perfect solution, but I'd like to get it merged into EchoCallset so we can unify EchoCallset and ah_var_store",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8737
https://github.com/broadinstitute/gatk/pull/8737:90,Modifiability,refactor,refactoring,90,"This is a patch to fix the integration test that is broken in the EchoCallset.; There was refactoring done on GvsExtractAvroFilesForHail (in the EchoCallset branch) that has broken the inputs to the integration test on that branch. ; I'm not sure this is the perfect solution, but I'd like to get it merged into EchoCallset so we can unify EchoCallset and ah_var_store",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8737
https://github.com/broadinstitute/gatk/pull/8737:39,Testability,test,test,39,"This is a patch to fix the integration test that is broken in the EchoCallset.; There was refactoring done on GvsExtractAvroFilesForHail (in the EchoCallset branch) that has broken the inputs to the integration test on that branch. ; I'm not sure this is the perfect solution, but I'd like to get it merged into EchoCallset so we can unify EchoCallset and ah_var_store",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8737
https://github.com/broadinstitute/gatk/pull/8737:211,Testability,test,test,211,"This is a patch to fix the integration test that is broken in the EchoCallset.; There was refactoring done on GvsExtractAvroFilesForHail (in the EchoCallset branch) that has broken the inputs to the integration test on that branch. ; I'm not sure this is the perfect solution, but I'd like to get it merged into EchoCallset so we can unify EchoCallset and ah_var_store",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8737
https://github.com/broadinstitute/gatk/pull/8738:114,Availability,Echo,EchoCallset,114,"Exclude the field AS_YNG from VCFs extracted from GVS.; This is the merge into ah_var_store to match the one into EchoCallset; Integration tests [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/5b513630-203f-4e4c-a14c-7423f300645e) - one failure for unrelated reasons.; Note that I had to update the truth data (since the contents of the VCFs had changed, and that there is one failure, but it's due to a slight cost difference - which is presumably in the noise.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8738
https://github.com/broadinstitute/gatk/pull/8738:268,Availability,failure,failure,268,"Exclude the field AS_YNG from VCFs extracted from GVS.; This is the merge into ah_var_store to match the one into EchoCallset; Integration tests [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/5b513630-203f-4e4c-a14c-7423f300645e) - one failure for unrelated reasons.; Note that I had to update the truth data (since the contents of the VCFs had changed, and that there is one failure, but it's due to a slight cost difference - which is presumably in the noise.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8738
https://github.com/broadinstitute/gatk/pull/8738:408,Availability,failure,failure,408,"Exclude the field AS_YNG from VCFs extracted from GVS.; This is the merge into ah_var_store to match the one into EchoCallset; Integration tests [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/5b513630-203f-4e4c-a14c-7423f300645e) - one failure for unrelated reasons.; Note that I had to update the truth data (since the contents of the VCFs had changed, and that there is one failure, but it's due to a slight cost difference - which is presumably in the noise.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8738
https://github.com/broadinstitute/gatk/pull/8738:127,Deployability,Integrat,Integration,127,"Exclude the field AS_YNG from VCFs extracted from GVS.; This is the merge into ah_var_store to match the one into EchoCallset; Integration tests [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/5b513630-203f-4e4c-a14c-7423f300645e) - one failure for unrelated reasons.; Note that I had to update the truth data (since the contents of the VCFs had changed, and that there is one failure, but it's due to a slight cost difference - which is presumably in the noise.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8738
https://github.com/broadinstitute/gatk/pull/8738:319,Deployability,update,update,319,"Exclude the field AS_YNG from VCFs extracted from GVS.; This is the merge into ah_var_store to match the one into EchoCallset; Integration tests [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/5b513630-203f-4e4c-a14c-7423f300645e) - one failure for unrelated reasons.; Note that I had to update the truth data (since the contents of the VCFs had changed, and that there is one failure, but it's due to a slight cost difference - which is presumably in the noise.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8738
https://github.com/broadinstitute/gatk/pull/8738:127,Integrability,Integrat,Integration,127,"Exclude the field AS_YNG from VCFs extracted from GVS.; This is the merge into ah_var_store to match the one into EchoCallset; Integration tests [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/5b513630-203f-4e4c-a14c-7423f300645e) - one failure for unrelated reasons.; Note that I had to update the truth data (since the contents of the VCFs had changed, and that there is one failure, but it's due to a slight cost difference - which is presumably in the noise.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8738
https://github.com/broadinstitute/gatk/pull/8738:139,Testability,test,tests,139,"Exclude the field AS_YNG from VCFs extracted from GVS.; This is the merge into ah_var_store to match the one into EchoCallset; Integration tests [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/5b513630-203f-4e4c-a14c-7423f300645e) - one failure for unrelated reasons.; Note that I had to update the truth data (since the contents of the VCFs had changed, and that there is one failure, but it's due to a slight cost difference - which is presumably in the noise.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8738
https://github.com/broadinstitute/gatk/pull/8739:147,Availability,Echo,EchoCallset,147,"Currently based off Kevin's branch, I plan to merge this to ah_var_store after Kevin's branch merges. We should then merge / pick this back to the EchoCallset branch and add an entry for EchoCallset in .dockstore.yml",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8739
https://github.com/broadinstitute/gatk/pull/8739:187,Availability,Echo,EchoCallset,187,"Currently based off Kevin's branch, I plan to merge this to ah_var_store after Kevin's branch merges. We should then merge / pick this back to the EchoCallset branch and add an entry for EchoCallset in .dockstore.yml",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8739
https://github.com/broadinstitute/gatk/issues/8740:263,Availability,error,error,263,"Hi all, . I am using CNV detection with GATK v4.3.0.0 for quite a while very successfully. Now we changed the enrichment kit and I had to do a new model. Everything worked well for the model phase. . As I now run one sample against this model I got the following error at the CNV detection step:. ```gatk GermlineCNVCaller --run-mode CASE -contig-ploidy-calls /media/Ergebnisse/0115-24_Masterpanel_NB501654_0623/0115-24_DGCP_noProbe-calls/ --model /media/Data/MasterV3/GCNV_noProbe-model/ --input /media/Ergebnisse/0115-24_Masterpanel_NB501654_0623/0115-24_noProbe.hdf5 --output /media/Ergebnisse/0115-24_Masterpanel_NB501654_0623/ --output-prefix 0115-24_GCNV_noProbe --tmp-dir /media/Data/tmp/; Using GATK jar /usr/BioinfSoftware/GATK/4.3.0.0/gatk-package-4.3.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /usr/BioinfSoftware/GATK/4.3.0.0/gatk-package-4.3.0.0-local.jar GermlineCNVCaller --run-mode CASE -contig-ploidy-calls /media/Ergebnisse/0115-24_Masterpanel_NB501654_0623/0115-24_DGCP_noProbe-calls/ --model /media/Data/MasterV3/GCNV_noProbe-model/ --input /media/Ergebnisse/0115-24_Masterpanel_NB501654_0623/0115-24_noProbe.hdf5 --output /media/Ergebnisse/0115-24_Masterpanel_NB501654_0623/ --output-prefix 0115-24_GCNV_noProbe --tmp-dir /media/Data/tmp/; 10:20:01.611 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/usr/BioinfSoftware/GATK/4.3.0.0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 10:20:01.717 INFO GermlineCNVCaller - ------------------------------------------------------------; 10:20:01.718 INFO GermlineCNVCaller - The Genome Analysis Toolkit (GATK) v4.3.0.0; 10:20:01.718 INFO GermlineCNVCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:20:01.718 INFO GermlineCNVCaller - Executing as die9s@k-hg-srv3 on Linux v5.3.18-24.37-default am",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8740
https://github.com/broadinstitute/gatk/issues/8740:3936,Availability,down,down,3936,"LE : false; 10:20:01.719 INFO GermlineCNVCaller - Deflater: IntelDeflater; 10:20:01.719 INFO GermlineCNVCaller - Inflater: IntelInflater; 10:20:01.719 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 10:20:01.719 INFO GermlineCNVCaller - Requester pays: disabled; 10:20:01.720 INFO GermlineCNVCaller - Initializing engine; 10:20:07.111 INFO GermlineCNVCaller - Done initializing engine; 10:20:07.207 INFO GermlineCNVCaller - Running the tool in CASE mode...; 10:20:07.207 INFO GermlineCNVCaller - Validating and aggregating data from input read-count files...; 10:20:07.231 INFO GermlineCNVCaller - Aggregating read-count file /media/Ergebnisse/0115-24_Masterpanel_NB501654_0623/0115-24_noProbe.hdf5 (1 / 1); log4j:WARN No appenders could be found for logger (org.broadinstitute.hdf5.HDF5Library).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 10:20:25.874 INFO GermlineCNVCaller - Shutting down engine; [March 14, 2024 at 10:20:25 AM CET] org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller done. Elapsed time: 0.40 minutes.; Runtime.totalMemory()=2147483648; org.broadinstitute.hellbender.utils.python.PythonScriptExecutorException:; python exited with 1; Command Line: python /media/Data/tmp/case_denoising_calling.3564509013495540802.py --ploidy_calls_path=/media/Ergebnisse/0115-24_Masterpanel_NB501654_0623/0115-24_DGCP_noProbe-calls --output_calls_path=/media/Ergebnisse/0115-24_Masterpanel_NB501654_0623/0115-24_GCNV_noProbe-calls --output_tracking_path=/media/Ergebnisse/0115-24_Masterpanel_NB501654_0623/0115-24_GCNV_noProbe-tracking --input_model_path=/media/Data/MasterV3/GCNV_noProbe-model --random_seed=1984 --read_count_tsv_files /media/Data/tmp/0115-24.rc16220482177493702615.tsv --psi_s_scale=1.000000e-04 --mapping_error_rate=1.000000e-02 --depth_correction_tau=1.000000e+04 --q_c_expectation_mode=hybrid --num_samples_copy_ratio_approx=200 --p_alt=1.000000e-06 --cnv_",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8740
https://github.com/broadinstitute/gatk/issues/8740:9679,Availability,error,error,9679,"sks.task_case_denoising_calling - Loading the model and updating the instantiated model and workspace...; 10:20:25.005 INFO gcnvkernel.io.io_commons - Reading model parameter values for ""log_mean_bias_t""... Stderr: Traceback (most recent call last):; File ""/media/Data/tmp/case_denoising_calling.3564509013495540802.py"", line 201, in <module>; shared_workspace, initial_params_supplier, args.input_model_path); File ""/usr/BioinfSoftware/Anaconda/3-2020.11/envs/gatk4.3.0.0/lib/python3.6/site-packages/gcnvkernel/tasks/task_case_denoising_calling.py"", line 128, in __init__; self.continuous_model_approx, input_model_path)(); File ""/usr/BioinfSoftware/Anaconda/3-2020.11/envs/gatk4.3.0.0/lib/python3.6/site-packages/gcnvkernel/io/io_denoising_calling.py"", line 93, in __call__; self.input_path, self.denoising_model_approx, self.denoising_model); File ""/usr/BioinfSoftware/Anaconda/3-2020.11/envs/gatk4.3.0.0/lib/python3.6/site-packages/gcnvkernel/io/io_commons.py"", line 471, in read_mean_field_global_params; ""expected: {2}"".format(var_name, var_mu.shape, vmap.shp); AssertionError: Loaded mean for ""log_mean_bias_t"" has an unexpected shape; loaded: (11903,), expected: (11901,). at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75); at org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller.doWork(GermlineCNVCaller.java:351); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289) ```. Can you give me some hint where this error comes from? ; Thanks in advanve; Stefan",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8740
https://github.com/broadinstitute/gatk/issues/8740:6513,Deployability,configurat,configuration,6513,"10 --max_calling_iters=10 --caller_update_convergence_threshold=1.000000e-03 --caller_internal_admixing_rate=7.500000e-01 --caller_external_admixing_rate=1.000000e+00 --disable_caller=false --disable_sampler=false --disable_annealing=false; Stdout: 10:20:12.111 INFO case_denoising_calling - THEANO_FLAGS environment variable has been set to: device=cpu,floatX=float64,optimizer=fast_run,compute_test_value=ignore,openmp=true,blas.ldflags=-lmkl_rt,openmp_elemwise_minsize=10; 10:20:12.273 INFO root - Loading modeling interval list from the provided model...; 10:20:12.475 INFO gcnvkernel.io.io_intervals_and_counts - The given interval list provides the following interval annotations: {'GC_CONTENT'}; 10:20:12.491 INFO root - The model contains 11901 intervals and 23 contig(s); 10:20:12.491 INFO root - Loading 1 read counts file(s)...; 10:20:12.545 INFO gcnvkernel.io.io_metadata - Loading germline contig ploidy and global read depth metadata...; 10:20:12.554 INFO root - Loading denoising model configuration from the provided model...; 10:20:12.555 INFO root - - bias factors enabled: True; 10:20:12.555 INFO root - - explicit GC bias modeling enabled: True; 10:20:12.555 INFO root - - bias factors in active classes disabled: False; 10:20:12.555 INFO root - - maximum number of bias factors: 5; 10:20:12.555 INFO root - - number of GC curve knobs: 20; 10:20:12.555 INFO root - - GC curve prior standard deviation: 1.0; 10:20:12.954 INFO gcnvkernel.tasks.task_case_denoising_calling - Instantiating the denoising model...; 10:20:15.806 INFO gcnvkernel.tasks.task_case_denoising_calling - Instantiating the sampler...; 10:20:15.807 INFO gcnvkernel.tasks.task_case_denoising_calling - Instantiating the copy number caller...; 10:20:18.549 INFO gcnvkernel.models.fancy_model - Global model variables: {'log_mean_bias_t', 'psi_t_log__', 'W_tu', 'ard_u_log__'}; 10:20:18.549 INFO gcnvkernel.models.fancy_model - Sample-specific model variables: {'read_depth_s_log__', 'psi_s_log__', 'z_sg', 'z_su'}",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8740
https://github.com/broadinstitute/gatk/issues/8740:5829,Modifiability,variab,variable,5829,"_expectation_mode=hybrid --num_samples_copy_ratio_approx=200 --p_alt=1.000000e-06 --cnv_coherence_length=1.000000e+04 --max_copy_number=5 --learning_rate=1.000000e-02 --adamax_beta1=9.000000e-01 --adamax_beta2=9.900000e-01 --log_emission_samples_per_round=50 --log_emission_sampling_rounds=10 --log_emission_sampling_median_rel_error=5.000000e-03 --max_advi_iter_first_epoch=5000 --max_advi_iter_subsequent_epochs=200 --min_training_epochs=10 --max_training_epochs=50 --initial_temperature=1.500000e+00 --num_thermal_advi_iters=2500 --convergence_snr_averaging_window=500 --convergence_snr_trigger_threshold=1.000000e-01 --convergence_snr_countdown_window=10 --max_calling_iters=10 --caller_update_convergence_threshold=1.000000e-03 --caller_internal_admixing_rate=7.500000e-01 --caller_external_admixing_rate=1.000000e+00 --disable_caller=false --disable_sampler=false --disable_annealing=false; Stdout: 10:20:12.111 INFO case_denoising_calling - THEANO_FLAGS environment variable has been set to: device=cpu,floatX=float64,optimizer=fast_run,compute_test_value=ignore,openmp=true,blas.ldflags=-lmkl_rt,openmp_elemwise_minsize=10; 10:20:12.273 INFO root - Loading modeling interval list from the provided model...; 10:20:12.475 INFO gcnvkernel.io.io_intervals_and_counts - The given interval list provides the following interval annotations: {'GC_CONTENT'}; 10:20:12.491 INFO root - The model contains 11901 intervals and 23 contig(s); 10:20:12.491 INFO root - Loading 1 read counts file(s)...; 10:20:12.545 INFO gcnvkernel.io.io_metadata - Loading germline contig ploidy and global read depth metadata...; 10:20:12.554 INFO root - Loading denoising model configuration from the provided model...; 10:20:12.555 INFO root - - bias factors enabled: True; 10:20:12.555 INFO root - - explicit GC bias modeling enabled: True; 10:20:12.555 INFO root - - bias factors in active classes disabled: False; 10:20:12.555 INFO root - - maximum number of bias factors: 5; 10:20:12.555 INFO root - - number of GC cu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8740
https://github.com/broadinstitute/gatk/issues/8740:6513,Modifiability,config,configuration,6513,"10 --max_calling_iters=10 --caller_update_convergence_threshold=1.000000e-03 --caller_internal_admixing_rate=7.500000e-01 --caller_external_admixing_rate=1.000000e+00 --disable_caller=false --disable_sampler=false --disable_annealing=false; Stdout: 10:20:12.111 INFO case_denoising_calling - THEANO_FLAGS environment variable has been set to: device=cpu,floatX=float64,optimizer=fast_run,compute_test_value=ignore,openmp=true,blas.ldflags=-lmkl_rt,openmp_elemwise_minsize=10; 10:20:12.273 INFO root - Loading modeling interval list from the provided model...; 10:20:12.475 INFO gcnvkernel.io.io_intervals_and_counts - The given interval list provides the following interval annotations: {'GC_CONTENT'}; 10:20:12.491 INFO root - The model contains 11901 intervals and 23 contig(s); 10:20:12.491 INFO root - Loading 1 read counts file(s)...; 10:20:12.545 INFO gcnvkernel.io.io_metadata - Loading germline contig ploidy and global read depth metadata...; 10:20:12.554 INFO root - Loading denoising model configuration from the provided model...; 10:20:12.555 INFO root - - bias factors enabled: True; 10:20:12.555 INFO root - - explicit GC bias modeling enabled: True; 10:20:12.555 INFO root - - bias factors in active classes disabled: False; 10:20:12.555 INFO root - - maximum number of bias factors: 5; 10:20:12.555 INFO root - - number of GC curve knobs: 20; 10:20:12.555 INFO root - - GC curve prior standard deviation: 1.0; 10:20:12.954 INFO gcnvkernel.tasks.task_case_denoising_calling - Instantiating the denoising model...; 10:20:15.806 INFO gcnvkernel.tasks.task_case_denoising_calling - Instantiating the sampler...; 10:20:15.807 INFO gcnvkernel.tasks.task_case_denoising_calling - Instantiating the copy number caller...; 10:20:18.549 INFO gcnvkernel.models.fancy_model - Global model variables: {'log_mean_bias_t', 'psi_t_log__', 'W_tu', 'ard_u_log__'}; 10:20:18.549 INFO gcnvkernel.models.fancy_model - Sample-specific model variables: {'read_depth_s_log__', 'psi_s_log__', 'z_sg', 'z_su'}",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8740
https://github.com/broadinstitute/gatk/issues/8740:7306,Modifiability,variab,variables,7306," counts file(s)...; 10:20:12.545 INFO gcnvkernel.io.io_metadata - Loading germline contig ploidy and global read depth metadata...; 10:20:12.554 INFO root - Loading denoising model configuration from the provided model...; 10:20:12.555 INFO root - - bias factors enabled: True; 10:20:12.555 INFO root - - explicit GC bias modeling enabled: True; 10:20:12.555 INFO root - - bias factors in active classes disabled: False; 10:20:12.555 INFO root - - maximum number of bias factors: 5; 10:20:12.555 INFO root - - number of GC curve knobs: 20; 10:20:12.555 INFO root - - GC curve prior standard deviation: 1.0; 10:20:12.954 INFO gcnvkernel.tasks.task_case_denoising_calling - Instantiating the denoising model...; 10:20:15.806 INFO gcnvkernel.tasks.task_case_denoising_calling - Instantiating the sampler...; 10:20:15.807 INFO gcnvkernel.tasks.task_case_denoising_calling - Instantiating the copy number caller...; 10:20:18.549 INFO gcnvkernel.models.fancy_model - Global model variables: {'log_mean_bias_t', 'psi_t_log__', 'W_tu', 'ard_u_log__'}; 10:20:18.549 INFO gcnvkernel.models.fancy_model - Sample-specific model variables: {'read_depth_s_log__', 'psi_s_log__', 'z_sg', 'z_su'}; 10:20:18.549 INFO gcnvkernel.tasks.inference_task_base - Instantiating the convergence tracker...; 10:20:18.549 INFO gcnvkernel.tasks.inference_task_base - Setting up DA-ADVI...; 10:20:24.995 INFO gcnvkernel.tasks.task_case_denoising_calling - Loading the model and updating the instantiated model and workspace...; 10:20:25.005 INFO gcnvkernel.io.io_commons - Reading model parameter values for ""log_mean_bias_t""... Stderr: Traceback (most recent call last):; File ""/media/Data/tmp/case_denoising_calling.3564509013495540802.py"", line 201, in <module>; shared_workspace, initial_params_supplier, args.input_model_path); File ""/usr/BioinfSoftware/Anaconda/3-2020.11/envs/gatk4.3.0.0/lib/python3.6/site-packages/gcnvkernel/tasks/task_case_denoising_calling.py"", line 128, in __init__; self.continuous_model_approx, inpu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8740
https://github.com/broadinstitute/gatk/issues/8740:7448,Modifiability,variab,variables,7448,"20:12.554 INFO root - Loading denoising model configuration from the provided model...; 10:20:12.555 INFO root - - bias factors enabled: True; 10:20:12.555 INFO root - - explicit GC bias modeling enabled: True; 10:20:12.555 INFO root - - bias factors in active classes disabled: False; 10:20:12.555 INFO root - - maximum number of bias factors: 5; 10:20:12.555 INFO root - - number of GC curve knobs: 20; 10:20:12.555 INFO root - - GC curve prior standard deviation: 1.0; 10:20:12.954 INFO gcnvkernel.tasks.task_case_denoising_calling - Instantiating the denoising model...; 10:20:15.806 INFO gcnvkernel.tasks.task_case_denoising_calling - Instantiating the sampler...; 10:20:15.807 INFO gcnvkernel.tasks.task_case_denoising_calling - Instantiating the copy number caller...; 10:20:18.549 INFO gcnvkernel.models.fancy_model - Global model variables: {'log_mean_bias_t', 'psi_t_log__', 'W_tu', 'ard_u_log__'}; 10:20:18.549 INFO gcnvkernel.models.fancy_model - Sample-specific model variables: {'read_depth_s_log__', 'psi_s_log__', 'z_sg', 'z_su'}; 10:20:18.549 INFO gcnvkernel.tasks.inference_task_base - Instantiating the convergence tracker...; 10:20:18.549 INFO gcnvkernel.tasks.inference_task_base - Setting up DA-ADVI...; 10:20:24.995 INFO gcnvkernel.tasks.task_case_denoising_calling - Loading the model and updating the instantiated model and workspace...; 10:20:25.005 INFO gcnvkernel.io.io_commons - Reading model parameter values for ""log_mean_bias_t""... Stderr: Traceback (most recent call last):; File ""/media/Data/tmp/case_denoising_calling.3564509013495540802.py"", line 201, in <module>; shared_workspace, initial_params_supplier, args.input_model_path); File ""/usr/BioinfSoftware/Anaconda/3-2020.11/envs/gatk4.3.0.0/lib/python3.6/site-packages/gcnvkernel/tasks/task_case_denoising_calling.py"", line 128, in __init__; self.continuous_model_approx, input_model_path)(); File ""/usr/BioinfSoftware/Anaconda/3-2020.11/envs/gatk4.3.0.0/lib/python3.6/site-packages/gcnvkernel/io/io_denoising_ca",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8740
https://github.com/broadinstitute/gatk/issues/8740:1448,Performance,Load,Loading,1448,"l /media/Data/MasterV3/GCNV_noProbe-model/ --input /media/Ergebnisse/0115-24_Masterpanel_NB501654_0623/0115-24_noProbe.hdf5 --output /media/Ergebnisse/0115-24_Masterpanel_NB501654_0623/ --output-prefix 0115-24_GCNV_noProbe --tmp-dir /media/Data/tmp/; Using GATK jar /usr/BioinfSoftware/GATK/4.3.0.0/gatk-package-4.3.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /usr/BioinfSoftware/GATK/4.3.0.0/gatk-package-4.3.0.0-local.jar GermlineCNVCaller --run-mode CASE -contig-ploidy-calls /media/Ergebnisse/0115-24_Masterpanel_NB501654_0623/0115-24_DGCP_noProbe-calls/ --model /media/Data/MasterV3/GCNV_noProbe-model/ --input /media/Ergebnisse/0115-24_Masterpanel_NB501654_0623/0115-24_noProbe.hdf5 --output /media/Ergebnisse/0115-24_Masterpanel_NB501654_0623/ --output-prefix 0115-24_GCNV_noProbe --tmp-dir /media/Data/tmp/; 10:20:01.611 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/usr/BioinfSoftware/GATK/4.3.0.0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 10:20:01.717 INFO GermlineCNVCaller - ------------------------------------------------------------; 10:20:01.718 INFO GermlineCNVCaller - The Genome Analysis Toolkit (GATK) v4.3.0.0; 10:20:01.718 INFO GermlineCNVCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:20:01.718 INFO GermlineCNVCaller - Executing as die9s@k-hg-srv3 on Linux v5.3.18-24.37-default amd64; 10:20:01.718 INFO GermlineCNVCaller - Java runtime: OpenJDK 64-Bit Server VM v11.0.11+9-suse-3.56.1-x8664; 10:20:01.718 INFO GermlineCNVCaller - Start Date/Time: March 14, 2024 at 10:20:01 AM CET; 10:20:01.718 INFO GermlineCNVCaller - ------------------------------------------------------------; 10:20:01.718 INFO GermlineCNVCaller - ------------------------------------------------------------; 10:20:01.719 INFO GermlineCNVCaller - HTSJD",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8740
https://github.com/broadinstitute/gatk/issues/8740:5881,Performance,optimiz,optimizer,5881,"_expectation_mode=hybrid --num_samples_copy_ratio_approx=200 --p_alt=1.000000e-06 --cnv_coherence_length=1.000000e+04 --max_copy_number=5 --learning_rate=1.000000e-02 --adamax_beta1=9.000000e-01 --adamax_beta2=9.900000e-01 --log_emission_samples_per_round=50 --log_emission_sampling_rounds=10 --log_emission_sampling_median_rel_error=5.000000e-03 --max_advi_iter_first_epoch=5000 --max_advi_iter_subsequent_epochs=200 --min_training_epochs=10 --max_training_epochs=50 --initial_temperature=1.500000e+00 --num_thermal_advi_iters=2500 --convergence_snr_averaging_window=500 --convergence_snr_trigger_threshold=1.000000e-01 --convergence_snr_countdown_window=10 --max_calling_iters=10 --caller_update_convergence_threshold=1.000000e-03 --caller_internal_admixing_rate=7.500000e-01 --caller_external_admixing_rate=1.000000e+00 --disable_caller=false --disable_sampler=false --disable_annealing=false; Stdout: 10:20:12.111 INFO case_denoising_calling - THEANO_FLAGS environment variable has been set to: device=cpu,floatX=float64,optimizer=fast_run,compute_test_value=ignore,openmp=true,blas.ldflags=-lmkl_rt,openmp_elemwise_minsize=10; 10:20:12.273 INFO root - Loading modeling interval list from the provided model...; 10:20:12.475 INFO gcnvkernel.io.io_intervals_and_counts - The given interval list provides the following interval annotations: {'GC_CONTENT'}; 10:20:12.491 INFO root - The model contains 11901 intervals and 23 contig(s); 10:20:12.491 INFO root - Loading 1 read counts file(s)...; 10:20:12.545 INFO gcnvkernel.io.io_metadata - Loading germline contig ploidy and global read depth metadata...; 10:20:12.554 INFO root - Loading denoising model configuration from the provided model...; 10:20:12.555 INFO root - - bias factors enabled: True; 10:20:12.555 INFO root - - explicit GC bias modeling enabled: True; 10:20:12.555 INFO root - - bias factors in active classes disabled: False; 10:20:12.555 INFO root - - maximum number of bias factors: 5; 10:20:12.555 INFO root - - number of GC cu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8740
https://github.com/broadinstitute/gatk/issues/8740:6013,Performance,Load,Loading,6013,"beta1=9.000000e-01 --adamax_beta2=9.900000e-01 --log_emission_samples_per_round=50 --log_emission_sampling_rounds=10 --log_emission_sampling_median_rel_error=5.000000e-03 --max_advi_iter_first_epoch=5000 --max_advi_iter_subsequent_epochs=200 --min_training_epochs=10 --max_training_epochs=50 --initial_temperature=1.500000e+00 --num_thermal_advi_iters=2500 --convergence_snr_averaging_window=500 --convergence_snr_trigger_threshold=1.000000e-01 --convergence_snr_countdown_window=10 --max_calling_iters=10 --caller_update_convergence_threshold=1.000000e-03 --caller_internal_admixing_rate=7.500000e-01 --caller_external_admixing_rate=1.000000e+00 --disable_caller=false --disable_sampler=false --disable_annealing=false; Stdout: 10:20:12.111 INFO case_denoising_calling - THEANO_FLAGS environment variable has been set to: device=cpu,floatX=float64,optimizer=fast_run,compute_test_value=ignore,openmp=true,blas.ldflags=-lmkl_rt,openmp_elemwise_minsize=10; 10:20:12.273 INFO root - Loading modeling interval list from the provided model...; 10:20:12.475 INFO gcnvkernel.io.io_intervals_and_counts - The given interval list provides the following interval annotations: {'GC_CONTENT'}; 10:20:12.491 INFO root - The model contains 11901 intervals and 23 contig(s); 10:20:12.491 INFO root - Loading 1 read counts file(s)...; 10:20:12.545 INFO gcnvkernel.io.io_metadata - Loading germline contig ploidy and global read depth metadata...; 10:20:12.554 INFO root - Loading denoising model configuration from the provided model...; 10:20:12.555 INFO root - - bias factors enabled: True; 10:20:12.555 INFO root - - explicit GC bias modeling enabled: True; 10:20:12.555 INFO root - - bias factors in active classes disabled: False; 10:20:12.555 INFO root - - maximum number of bias factors: 5; 10:20:12.555 INFO root - - number of GC curve knobs: 20; 10:20:12.555 INFO root - - GC curve prior standard deviation: 1.0; 10:20:12.954 INFO gcnvkernel.tasks.task_case_denoising_calling - Instantiating the denoising m",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8740
https://github.com/broadinstitute/gatk/issues/8740:6318,Performance,Load,Loading,6318,"-initial_temperature=1.500000e+00 --num_thermal_advi_iters=2500 --convergence_snr_averaging_window=500 --convergence_snr_trigger_threshold=1.000000e-01 --convergence_snr_countdown_window=10 --max_calling_iters=10 --caller_update_convergence_threshold=1.000000e-03 --caller_internal_admixing_rate=7.500000e-01 --caller_external_admixing_rate=1.000000e+00 --disable_caller=false --disable_sampler=false --disable_annealing=false; Stdout: 10:20:12.111 INFO case_denoising_calling - THEANO_FLAGS environment variable has been set to: device=cpu,floatX=float64,optimizer=fast_run,compute_test_value=ignore,openmp=true,blas.ldflags=-lmkl_rt,openmp_elemwise_minsize=10; 10:20:12.273 INFO root - Loading modeling interval list from the provided model...; 10:20:12.475 INFO gcnvkernel.io.io_intervals_and_counts - The given interval list provides the following interval annotations: {'GC_CONTENT'}; 10:20:12.491 INFO root - The model contains 11901 intervals and 23 contig(s); 10:20:12.491 INFO root - Loading 1 read counts file(s)...; 10:20:12.545 INFO gcnvkernel.io.io_metadata - Loading germline contig ploidy and global read depth metadata...; 10:20:12.554 INFO root - Loading denoising model configuration from the provided model...; 10:20:12.555 INFO root - - bias factors enabled: True; 10:20:12.555 INFO root - - explicit GC bias modeling enabled: True; 10:20:12.555 INFO root - - bias factors in active classes disabled: False; 10:20:12.555 INFO root - - maximum number of bias factors: 5; 10:20:12.555 INFO root - - number of GC curve knobs: 20; 10:20:12.555 INFO root - - GC curve prior standard deviation: 1.0; 10:20:12.954 INFO gcnvkernel.tasks.task_case_denoising_calling - Instantiating the denoising model...; 10:20:15.806 INFO gcnvkernel.tasks.task_case_denoising_calling - Instantiating the sampler...; 10:20:15.807 INFO gcnvkernel.tasks.task_case_denoising_calling - Instantiating the copy number caller...; 10:20:18.549 INFO gcnvkernel.models.fancy_model - Global model variables: {'log_me",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8740
https://github.com/broadinstitute/gatk/issues/8740:6398,Performance,Load,Loading,6398,"w=500 --convergence_snr_trigger_threshold=1.000000e-01 --convergence_snr_countdown_window=10 --max_calling_iters=10 --caller_update_convergence_threshold=1.000000e-03 --caller_internal_admixing_rate=7.500000e-01 --caller_external_admixing_rate=1.000000e+00 --disable_caller=false --disable_sampler=false --disable_annealing=false; Stdout: 10:20:12.111 INFO case_denoising_calling - THEANO_FLAGS environment variable has been set to: device=cpu,floatX=float64,optimizer=fast_run,compute_test_value=ignore,openmp=true,blas.ldflags=-lmkl_rt,openmp_elemwise_minsize=10; 10:20:12.273 INFO root - Loading modeling interval list from the provided model...; 10:20:12.475 INFO gcnvkernel.io.io_intervals_and_counts - The given interval list provides the following interval annotations: {'GC_CONTENT'}; 10:20:12.491 INFO root - The model contains 11901 intervals and 23 contig(s); 10:20:12.491 INFO root - Loading 1 read counts file(s)...; 10:20:12.545 INFO gcnvkernel.io.io_metadata - Loading germline contig ploidy and global read depth metadata...; 10:20:12.554 INFO root - Loading denoising model configuration from the provided model...; 10:20:12.555 INFO root - - bias factors enabled: True; 10:20:12.555 INFO root - - explicit GC bias modeling enabled: True; 10:20:12.555 INFO root - - bias factors in active classes disabled: False; 10:20:12.555 INFO root - - maximum number of bias factors: 5; 10:20:12.555 INFO root - - number of GC curve knobs: 20; 10:20:12.555 INFO root - - GC curve prior standard deviation: 1.0; 10:20:12.954 INFO gcnvkernel.tasks.task_case_denoising_calling - Instantiating the denoising model...; 10:20:15.806 INFO gcnvkernel.tasks.task_case_denoising_calling - Instantiating the sampler...; 10:20:15.807 INFO gcnvkernel.tasks.task_case_denoising_calling - Instantiating the copy number caller...; 10:20:18.549 INFO gcnvkernel.models.fancy_model - Global model variables: {'log_mean_bias_t', 'psi_t_log__', 'W_tu', 'ard_u_log__'}; 10:20:18.549 INFO gcnvkernel.models.fancy_mode",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8740
https://github.com/broadinstitute/gatk/issues/8740:6489,Performance,Load,Loading,6489,"10 --max_calling_iters=10 --caller_update_convergence_threshold=1.000000e-03 --caller_internal_admixing_rate=7.500000e-01 --caller_external_admixing_rate=1.000000e+00 --disable_caller=false --disable_sampler=false --disable_annealing=false; Stdout: 10:20:12.111 INFO case_denoising_calling - THEANO_FLAGS environment variable has been set to: device=cpu,floatX=float64,optimizer=fast_run,compute_test_value=ignore,openmp=true,blas.ldflags=-lmkl_rt,openmp_elemwise_minsize=10; 10:20:12.273 INFO root - Loading modeling interval list from the provided model...; 10:20:12.475 INFO gcnvkernel.io.io_intervals_and_counts - The given interval list provides the following interval annotations: {'GC_CONTENT'}; 10:20:12.491 INFO root - The model contains 11901 intervals and 23 contig(s); 10:20:12.491 INFO root - Loading 1 read counts file(s)...; 10:20:12.545 INFO gcnvkernel.io.io_metadata - Loading germline contig ploidy and global read depth metadata...; 10:20:12.554 INFO root - Loading denoising model configuration from the provided model...; 10:20:12.555 INFO root - - bias factors enabled: True; 10:20:12.555 INFO root - - explicit GC bias modeling enabled: True; 10:20:12.555 INFO root - - bias factors in active classes disabled: False; 10:20:12.555 INFO root - - maximum number of bias factors: 5; 10:20:12.555 INFO root - - number of GC curve knobs: 20; 10:20:12.555 INFO root - - GC curve prior standard deviation: 1.0; 10:20:12.954 INFO gcnvkernel.tasks.task_case_denoising_calling - Instantiating the denoising model...; 10:20:15.806 INFO gcnvkernel.tasks.task_case_denoising_calling - Instantiating the sampler...; 10:20:15.807 INFO gcnvkernel.tasks.task_case_denoising_calling - Instantiating the copy number caller...; 10:20:18.549 INFO gcnvkernel.models.fancy_model - Global model variables: {'log_mean_bias_t', 'psi_t_log__', 'W_tu', 'ard_u_log__'}; 10:20:18.549 INFO gcnvkernel.models.fancy_model - Sample-specific model variables: {'read_depth_s_log__', 'psi_s_log__', 'z_sg', 'z_su'}",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8740
https://github.com/broadinstitute/gatk/issues/8740:7758,Performance,Load,Loading,7758," - maximum number of bias factors: 5; 10:20:12.555 INFO root - - number of GC curve knobs: 20; 10:20:12.555 INFO root - - GC curve prior standard deviation: 1.0; 10:20:12.954 INFO gcnvkernel.tasks.task_case_denoising_calling - Instantiating the denoising model...; 10:20:15.806 INFO gcnvkernel.tasks.task_case_denoising_calling - Instantiating the sampler...; 10:20:15.807 INFO gcnvkernel.tasks.task_case_denoising_calling - Instantiating the copy number caller...; 10:20:18.549 INFO gcnvkernel.models.fancy_model - Global model variables: {'log_mean_bias_t', 'psi_t_log__', 'W_tu', 'ard_u_log__'}; 10:20:18.549 INFO gcnvkernel.models.fancy_model - Sample-specific model variables: {'read_depth_s_log__', 'psi_s_log__', 'z_sg', 'z_su'}; 10:20:18.549 INFO gcnvkernel.tasks.inference_task_base - Instantiating the convergence tracker...; 10:20:18.549 INFO gcnvkernel.tasks.inference_task_base - Setting up DA-ADVI...; 10:20:24.995 INFO gcnvkernel.tasks.task_case_denoising_calling - Loading the model and updating the instantiated model and workspace...; 10:20:25.005 INFO gcnvkernel.io.io_commons - Reading model parameter values for ""log_mean_bias_t""... Stderr: Traceback (most recent call last):; File ""/media/Data/tmp/case_denoising_calling.3564509013495540802.py"", line 201, in <module>; shared_workspace, initial_params_supplier, args.input_model_path); File ""/usr/BioinfSoftware/Anaconda/3-2020.11/envs/gatk4.3.0.0/lib/python3.6/site-packages/gcnvkernel/tasks/task_case_denoising_calling.py"", line 128, in __init__; self.continuous_model_approx, input_model_path)(); File ""/usr/BioinfSoftware/Anaconda/3-2020.11/envs/gatk4.3.0.0/lib/python3.6/site-packages/gcnvkernel/io/io_denoising_calling.py"", line 93, in __call__; self.input_path, self.denoising_model_approx, self.denoising_model); File ""/usr/BioinfSoftware/Anaconda/3-2020.11/envs/gatk4.3.0.0/lib/python3.6/site-packages/gcnvkernel/io/io_commons.py"", line 471, in read_mean_field_global_params; ""expected: {2}"".format(var_name, var_mu.sha",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8740
https://github.com/broadinstitute/gatk/issues/8740:8808,Performance,Load,Loaded,8808,"sks.task_case_denoising_calling - Loading the model and updating the instantiated model and workspace...; 10:20:25.005 INFO gcnvkernel.io.io_commons - Reading model parameter values for ""log_mean_bias_t""... Stderr: Traceback (most recent call last):; File ""/media/Data/tmp/case_denoising_calling.3564509013495540802.py"", line 201, in <module>; shared_workspace, initial_params_supplier, args.input_model_path); File ""/usr/BioinfSoftware/Anaconda/3-2020.11/envs/gatk4.3.0.0/lib/python3.6/site-packages/gcnvkernel/tasks/task_case_denoising_calling.py"", line 128, in __init__; self.continuous_model_approx, input_model_path)(); File ""/usr/BioinfSoftware/Anaconda/3-2020.11/envs/gatk4.3.0.0/lib/python3.6/site-packages/gcnvkernel/io/io_denoising_calling.py"", line 93, in __call__; self.input_path, self.denoising_model_approx, self.denoising_model); File ""/usr/BioinfSoftware/Anaconda/3-2020.11/envs/gatk4.3.0.0/lib/python3.6/site-packages/gcnvkernel/io/io_commons.py"", line 471, in read_mean_field_global_params; ""expected: {2}"".format(var_name, var_mu.shape, vmap.shp); AssertionError: Loaded mean for ""log_mean_bias_t"" has an unexpected shape; loaded: (11903,), expected: (11901,). at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75); at org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller.doWork(GermlineCNVCaller.java:351); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289) ```. Can you give me some hint where this error comes from? ; Thanks in advanve; Stefan",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8740
https://github.com/broadinstitute/gatk/issues/8740:8867,Performance,load,loaded,8867,"sks.task_case_denoising_calling - Loading the model and updating the instantiated model and workspace...; 10:20:25.005 INFO gcnvkernel.io.io_commons - Reading model parameter values for ""log_mean_bias_t""... Stderr: Traceback (most recent call last):; File ""/media/Data/tmp/case_denoising_calling.3564509013495540802.py"", line 201, in <module>; shared_workspace, initial_params_supplier, args.input_model_path); File ""/usr/BioinfSoftware/Anaconda/3-2020.11/envs/gatk4.3.0.0/lib/python3.6/site-packages/gcnvkernel/tasks/task_case_denoising_calling.py"", line 128, in __init__; self.continuous_model_approx, input_model_path)(); File ""/usr/BioinfSoftware/Anaconda/3-2020.11/envs/gatk4.3.0.0/lib/python3.6/site-packages/gcnvkernel/io/io_denoising_calling.py"", line 93, in __call__; self.input_path, self.denoising_model_approx, self.denoising_model); File ""/usr/BioinfSoftware/Anaconda/3-2020.11/envs/gatk4.3.0.0/lib/python3.6/site-packages/gcnvkernel/io/io_commons.py"", line 471, in read_mean_field_global_params; ""expected: {2}"".format(var_name, var_mu.shape, vmap.shp); AssertionError: Loaded mean for ""log_mean_bias_t"" has an unexpected shape; loaded: (11903,), expected: (11901,). at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75); at org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller.doWork(GermlineCNVCaller.java:351); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289) ```. Can you give me some hint where this error comes from? ; Thanks in advanve; Stefan",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8740
https://github.com/broadinstitute/gatk/issues/8740:25,Safety,detect,detection,25,"Hi all, . I am using CNV detection with GATK v4.3.0.0 for quite a while very successfully. Now we changed the enrichment kit and I had to do a new model. Everything worked well for the model phase. . As I now run one sample against this model I got the following error at the CNV detection step:. ```gatk GermlineCNVCaller --run-mode CASE -contig-ploidy-calls /media/Ergebnisse/0115-24_Masterpanel_NB501654_0623/0115-24_DGCP_noProbe-calls/ --model /media/Data/MasterV3/GCNV_noProbe-model/ --input /media/Ergebnisse/0115-24_Masterpanel_NB501654_0623/0115-24_noProbe.hdf5 --output /media/Ergebnisse/0115-24_Masterpanel_NB501654_0623/ --output-prefix 0115-24_GCNV_noProbe --tmp-dir /media/Data/tmp/; Using GATK jar /usr/BioinfSoftware/GATK/4.3.0.0/gatk-package-4.3.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /usr/BioinfSoftware/GATK/4.3.0.0/gatk-package-4.3.0.0-local.jar GermlineCNVCaller --run-mode CASE -contig-ploidy-calls /media/Ergebnisse/0115-24_Masterpanel_NB501654_0623/0115-24_DGCP_noProbe-calls/ --model /media/Data/MasterV3/GCNV_noProbe-model/ --input /media/Ergebnisse/0115-24_Masterpanel_NB501654_0623/0115-24_noProbe.hdf5 --output /media/Ergebnisse/0115-24_Masterpanel_NB501654_0623/ --output-prefix 0115-24_GCNV_noProbe --tmp-dir /media/Data/tmp/; 10:20:01.611 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/usr/BioinfSoftware/GATK/4.3.0.0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 10:20:01.717 INFO GermlineCNVCaller - ------------------------------------------------------------; 10:20:01.718 INFO GermlineCNVCaller - The Genome Analysis Toolkit (GATK) v4.3.0.0; 10:20:01.718 INFO GermlineCNVCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:20:01.718 INFO GermlineCNVCaller - Executing as die9s@k-hg-srv3 on Linux v5.3.18-24.37-default am",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8740
https://github.com/broadinstitute/gatk/issues/8740:280,Safety,detect,detection,280,"Hi all, . I am using CNV detection with GATK v4.3.0.0 for quite a while very successfully. Now we changed the enrichment kit and I had to do a new model. Everything worked well for the model phase. . As I now run one sample against this model I got the following error at the CNV detection step:. ```gatk GermlineCNVCaller --run-mode CASE -contig-ploidy-calls /media/Ergebnisse/0115-24_Masterpanel_NB501654_0623/0115-24_DGCP_noProbe-calls/ --model /media/Data/MasterV3/GCNV_noProbe-model/ --input /media/Ergebnisse/0115-24_Masterpanel_NB501654_0623/0115-24_noProbe.hdf5 --output /media/Ergebnisse/0115-24_Masterpanel_NB501654_0623/ --output-prefix 0115-24_GCNV_noProbe --tmp-dir /media/Data/tmp/; Using GATK jar /usr/BioinfSoftware/GATK/4.3.0.0/gatk-package-4.3.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /usr/BioinfSoftware/GATK/4.3.0.0/gatk-package-4.3.0.0-local.jar GermlineCNVCaller --run-mode CASE -contig-ploidy-calls /media/Ergebnisse/0115-24_Masterpanel_NB501654_0623/0115-24_DGCP_noProbe-calls/ --model /media/Data/MasterV3/GCNV_noProbe-model/ --input /media/Ergebnisse/0115-24_Masterpanel_NB501654_0623/0115-24_noProbe.hdf5 --output /media/Ergebnisse/0115-24_Masterpanel_NB501654_0623/ --output-prefix 0115-24_GCNV_noProbe --tmp-dir /media/Data/tmp/; 10:20:01.611 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/usr/BioinfSoftware/GATK/4.3.0.0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 10:20:01.717 INFO GermlineCNVCaller - ------------------------------------------------------------; 10:20:01.718 INFO GermlineCNVCaller - The Genome Analysis Toolkit (GATK) v4.3.0.0; 10:20:01.718 INFO GermlineCNVCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:20:01.718 INFO GermlineCNVCaller - Executing as die9s@k-hg-srv3 on Linux v5.3.18-24.37-default am",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8740
https://github.com/broadinstitute/gatk/issues/8740:3445,Security,Validat,Validating,3445,".1; 10:20:01.719 INFO GermlineCNVCaller - Picard Version: 2.27.5; 10:20:01.719 INFO GermlineCNVCaller - Built for Spark Version: 2.4.5; 10:20:01.719 INFO GermlineCNVCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 10:20:01.719 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 10:20:01.719 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 10:20:01.719 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 10:20:01.719 INFO GermlineCNVCaller - Deflater: IntelDeflater; 10:20:01.719 INFO GermlineCNVCaller - Inflater: IntelInflater; 10:20:01.719 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 10:20:01.719 INFO GermlineCNVCaller - Requester pays: disabled; 10:20:01.720 INFO GermlineCNVCaller - Initializing engine; 10:20:07.111 INFO GermlineCNVCaller - Done initializing engine; 10:20:07.207 INFO GermlineCNVCaller - Running the tool in CASE mode...; 10:20:07.207 INFO GermlineCNVCaller - Validating and aggregating data from input read-count files...; 10:20:07.231 INFO GermlineCNVCaller - Aggregating read-count file /media/Ergebnisse/0115-24_Masterpanel_NB501654_0623/0115-24_noProbe.hdf5 (1 / 1); log4j:WARN No appenders could be found for logger (org.broadinstitute.hdf5.HDF5Library).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 10:20:25.874 INFO GermlineCNVCaller - Shutting down engine; [March 14, 2024 at 10:20:25 AM CET] org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller done. Elapsed time: 0.40 minutes.; Runtime.totalMemory()=2147483648; org.broadinstitute.hellbender.utils.python.PythonScriptExecutorException:; python exited with 1; Command Line: python /media/Data/tmp/case_denoising_calling.3564509013495540802.py --ploidy_calls_path=/media/Ergebnisse/0115-24_Masterpanel_NB501654_0623/0115-24_DGCP_noProbe-calls --output_calls_path=/media/Ergebnisse/0115-24_Masterpanel_NB501",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8740
https://github.com/broadinstitute/gatk/issues/8740:3700,Testability,log,logger,3700,"0:01.719 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 10:20:01.719 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 10:20:01.719 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 10:20:01.719 INFO GermlineCNVCaller - Deflater: IntelDeflater; 10:20:01.719 INFO GermlineCNVCaller - Inflater: IntelInflater; 10:20:01.719 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 10:20:01.719 INFO GermlineCNVCaller - Requester pays: disabled; 10:20:01.720 INFO GermlineCNVCaller - Initializing engine; 10:20:07.111 INFO GermlineCNVCaller - Done initializing engine; 10:20:07.207 INFO GermlineCNVCaller - Running the tool in CASE mode...; 10:20:07.207 INFO GermlineCNVCaller - Validating and aggregating data from input read-count files...; 10:20:07.231 INFO GermlineCNVCaller - Aggregating read-count file /media/Ergebnisse/0115-24_Masterpanel_NB501654_0623/0115-24_noProbe.hdf5 (1 / 1); log4j:WARN No appenders could be found for logger (org.broadinstitute.hdf5.HDF5Library).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 10:20:25.874 INFO GermlineCNVCaller - Shutting down engine; [March 14, 2024 at 10:20:25 AM CET] org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller done. Elapsed time: 0.40 minutes.; Runtime.totalMemory()=2147483648; org.broadinstitute.hellbender.utils.python.PythonScriptExecutorException:; python exited with 1; Command Line: python /media/Data/tmp/case_denoising_calling.3564509013495540802.py --ploidy_calls_path=/media/Ergebnisse/0115-24_Masterpanel_NB501654_0623/0115-24_DGCP_noProbe-calls --output_calls_path=/media/Ergebnisse/0115-24_Masterpanel_NB501654_0623/0115-24_GCNV_noProbe-calls --output_tracking_path=/media/Ergebnisse/0115-24_Masterpanel_NB501654_0623/0115-24_GCNV_noProbe-tracking --input_model_path=/media/Data/MasterV3/GCNV_noProbe-model --random_seed=198",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8740
https://github.com/broadinstitute/gatk/issues/8740:3826,Testability,log,logging,3826,"ts.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 10:20:01.719 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 10:20:01.719 INFO GermlineCNVCaller - Deflater: IntelDeflater; 10:20:01.719 INFO GermlineCNVCaller - Inflater: IntelInflater; 10:20:01.719 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 10:20:01.719 INFO GermlineCNVCaller - Requester pays: disabled; 10:20:01.720 INFO GermlineCNVCaller - Initializing engine; 10:20:07.111 INFO GermlineCNVCaller - Done initializing engine; 10:20:07.207 INFO GermlineCNVCaller - Running the tool in CASE mode...; 10:20:07.207 INFO GermlineCNVCaller - Validating and aggregating data from input read-count files...; 10:20:07.231 INFO GermlineCNVCaller - Aggregating read-count file /media/Ergebnisse/0115-24_Masterpanel_NB501654_0623/0115-24_noProbe.hdf5 (1 / 1); log4j:WARN No appenders could be found for logger (org.broadinstitute.hdf5.HDF5Library).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 10:20:25.874 INFO GermlineCNVCaller - Shutting down engine; [March 14, 2024 at 10:20:25 AM CET] org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller done. Elapsed time: 0.40 minutes.; Runtime.totalMemory()=2147483648; org.broadinstitute.hellbender.utils.python.PythonScriptExecutorException:; python exited with 1; Command Line: python /media/Data/tmp/case_denoising_calling.3564509013495540802.py --ploidy_calls_path=/media/Ergebnisse/0115-24_Masterpanel_NB501654_0623/0115-24_DGCP_noProbe-calls --output_calls_path=/media/Ergebnisse/0115-24_Masterpanel_NB501654_0623/0115-24_GCNV_noProbe-calls --output_tracking_path=/media/Ergebnisse/0115-24_Masterpanel_NB501654_0623/0115-24_GCNV_noProbe-tracking --input_model_path=/media/Data/MasterV3/GCNV_noProbe-model --random_seed=1984 --read_count_tsv_files /media/Data/tmp/0115-24.rc16220482177493702615.tsv --psi_s_scale=1.000000e-04 --mapping_error_rate=1.000000e-02 --d",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8740
https://github.com/broadinstitute/gatk/issues/8740:8792,Testability,Assert,AssertionError,8792,"sks.task_case_denoising_calling - Loading the model and updating the instantiated model and workspace...; 10:20:25.005 INFO gcnvkernel.io.io_commons - Reading model parameter values for ""log_mean_bias_t""... Stderr: Traceback (most recent call last):; File ""/media/Data/tmp/case_denoising_calling.3564509013495540802.py"", line 201, in <module>; shared_workspace, initial_params_supplier, args.input_model_path); File ""/usr/BioinfSoftware/Anaconda/3-2020.11/envs/gatk4.3.0.0/lib/python3.6/site-packages/gcnvkernel/tasks/task_case_denoising_calling.py"", line 128, in __init__; self.continuous_model_approx, input_model_path)(); File ""/usr/BioinfSoftware/Anaconda/3-2020.11/envs/gatk4.3.0.0/lib/python3.6/site-packages/gcnvkernel/io/io_denoising_calling.py"", line 93, in __call__; self.input_path, self.denoising_model_approx, self.denoising_model); File ""/usr/BioinfSoftware/Anaconda/3-2020.11/envs/gatk4.3.0.0/lib/python3.6/site-packages/gcnvkernel/io/io_commons.py"", line 471, in read_mean_field_global_params; ""expected: {2}"".format(var_name, var_mu.shape, vmap.shp); AssertionError: Loaded mean for ""log_mean_bias_t"" has an unexpected shape; loaded: (11903,), expected: (11901,). at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75); at org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller.doWork(GermlineCNVCaller.java:351); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289) ```. Can you give me some hint where this error comes from? ; Thanks in advanve; Stefan",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8740
https://github.com/broadinstitute/gatk/pull/8741:263,Energy Efficiency,reduce,reduced,263,"Several GQ0 cleanup changes:; Set GGVCFs --all-sites GQ0 hom-refs to no-calls; Set regular GGVCFs GQ0 hom-refs to no-calls (any DP, PL) for better AF/AN annotations; Remove PLs in ""no data"" case where DP=0 for more accurate QUAL score. Users can expect ANs to be reduced where GQ0 hom-refs previously occurred. QUALs may be decreased where PL=[0,0,0] because those genotypes are no longer included in QUAL calculations. QD will change where QUAL changes. InbreedingCoeff and ExcessHet will change because GQ0 hom-refs don't count anymore. None of these changes significantly impacted NA12878 accuracy in exome and WGS tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8741
https://github.com/broadinstitute/gatk/pull/8741:618,Testability,test,tests,618,"Several GQ0 cleanup changes:; Set GGVCFs --all-sites GQ0 hom-refs to no-calls; Set regular GGVCFs GQ0 hom-refs to no-calls (any DP, PL) for better AF/AN annotations; Remove PLs in ""no data"" case where DP=0 for more accurate QUAL score. Users can expect ANs to be reduced where GQ0 hom-refs previously occurred. QUALs may be decreased where PL=[0,0,0] because those genotypes are no longer included in QUAL calculations. QD will change where QUAL changes. InbreedingCoeff and ExcessHet will change because GQ0 hom-refs don't count anymore. None of these changes significantly impacted NA12878 accuracy in exome and WGS tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8741
https://github.com/broadinstitute/gatk/pull/8743:0,Deployability,Update,Update,0,Update changelog for Beta release,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8743
https://github.com/broadinstitute/gatk/pull/8743:26,Deployability,release,release,26,Update changelog for Beta release,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8743
https://github.com/broadinstitute/gatk/pull/8744:30,Testability,test,test,30,"Codecov seems to be including test files in its coverage calculation, which is probably not ideal because obviously there are no tests for our tests. This change should exclude them.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8744
https://github.com/broadinstitute/gatk/pull/8744:129,Testability,test,tests,129,"Codecov seems to be including test files in its coverage calculation, which is probably not ideal because obviously there are no tests for our tests. This change should exclude them.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8744
https://github.com/broadinstitute/gatk/pull/8744:143,Testability,test,tests,143,"Codecov seems to be including test files in its coverage calculation, which is probably not ideal because obviously there are no tests for our tests. This change should exclude them.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8744
https://github.com/broadinstitute/gatk/pull/8745:48,Deployability,update,update,48,"As the title suggests, this PR includes a quick update to our README to include documentation on what tools are included in our docker images.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8745
https://github.com/broadinstitute/gatk/pull/8746:62,Availability,Echo,EchoCallset,62,This PR is against ah_var_store. Need to make another against EchoCallset. The new reference disk is installed.; [Here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/ba18dc3d-0548-48ac-a67b-cec55250de8a) is a passing run of GvsCreateVatFromVDS using quickstart against the new reference.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8746
https://github.com/broadinstitute/gatk/pull/8746:101,Deployability,install,installed,101,This PR is against ah_var_store. Need to make another against EchoCallset. The new reference disk is installed.; [Here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/ba18dc3d-0548-48ac-a67b-cec55250de8a) is a passing run of GvsCreateVatFromVDS using quickstart against the new reference.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8746
https://github.com/broadinstitute/gatk/issues/8747:423,Availability,error,error,423,"Hello,. Could you help me with this? I ran this code:; ```; prg=/home/user1/Programs/gatk-4.5.0.0; log_dir=/home/user1/Programs/logs; java -Xmx64g -XX:ParallelGCThreads=2 -DGATK_STACKTRACE_ON_USER_EXCEPTION=true \; -jar ${prg}/gatk-package-4.5.0.0-local.jar IndexFeatureFile -I ${dir}/snp_allsamples.vcf.gz \; --output snp_allsamples.vcf.tbi \; 2>${log_dir}/snp_allsamples_gvcf_index.err. ```; and I received the following error message. ```; 09:36:35.254 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/user1/Programs/gatk-4.5.0.0/gatk-package-4.5.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 09:36:35.386 INFO IndexFeatureFile - ------------------------------------------------------------; 09:36:35.389 INFO IndexFeatureFile - The Genome Analysis Toolkit (GATK) v4.5.0.0; 09:36:35.389 INFO IndexFeatureFile - For support and documentation go to https://software.broadinstitute.org/gatk/; 09:36:35.389 INFO IndexFeatureFile - Executing as user1@xxx.xx on Linux v5.4.0-150-generic amd64; 09:36:35.389 INFO IndexFeatureFile - Java runtime: OpenJDK 64-Bit Server VM v17.0.3-internal+0-adhoc..src; 09:36:35.389 INFO IndexFeatureFile - Start Date/Time: March 21, 2024 at 9:36:35 a.m. CST; 09:36:35.390 INFO IndexFeatureFile - ------------------------------------------------------------; 09:36:35.390 INFO IndexFeatureFile - ------------------------------------------------------------; 09:36:35.390 INFO IndexFeatureFile - HTSJDK Version: 4.1.0; 09:36:35.391 INFO IndexFeatureFile - Picard Version: 3.1.1; 09:36:35.391 INFO IndexFeatureFile - Built for Spark Version: 3.5.0; 09:36:35.391 INFO IndexFeatureFile - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 09:36:35.391 INFO IndexFeatureFile - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 09:36:35.392 INFO IndexFeatureFile - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 09:36:35.392 INFO IndexFeatureFile - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 09:36:35.392 INFO IndexFeature",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8747
https://github.com/broadinstitute/gatk/issues/8747:2653,Availability,down,down,2653,"SJDK Defaults.COMPRESSION_LEVEL : 2; 09:36:35.391 INFO IndexFeatureFile - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 09:36:35.392 INFO IndexFeatureFile - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 09:36:35.392 INFO IndexFeatureFile - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 09:36:35.392 INFO IndexFeatureFile - Deflater: IntelDeflater; 09:36:35.392 INFO IndexFeatureFile - Inflater: IntelInflater; 09:36:35.392 INFO IndexFeatureFile - GCS max retries/reopens: 20; 09:36:35.392 INFO IndexFeatureFile - Requester pays: disabled; 09:36:35.393 INFO IndexFeatureFile - Initializing engine; 09:36:35.393 INFO IndexFeatureFile - Done initializing engine; 09:36:35.502 INFO FeatureManager - Using codec VCFCodec to read file file:///mnt/user1/snp_allsamples.vcf.gz; 09:36:35.518 INFO ProgressMeter - Starting traversal; 09:36:35.518 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 09:36:36.979 INFO IndexFeatureFile - Shutting down engine; [March 21, 2024 at 9:36:36 a.m. CST] org.broadinstitute.hellbender.tools.IndexFeatureFile done. Elapsed time: 0.03 minutes.; Runtime.totalMemory()=1241513984; java.lang.ArrayIndexOutOfBoundsException: Index 37451 out of bounds for length 37451; at htsjdk.samtools.BinningIndexBuilder.processFeature(BinningIndexBuilder.java:102); at htsjdk.tribble.index.tabix.TabixIndexCreator.finalizeFeature(TabixIndexCreator.java:106); at htsjdk.tribble.index.tabix.TabixIndexCreator.addFeature(TabixIndexCreator.java:92); at htsjdk.tribble.index.IndexFactory.createIndex(IndexFactory.java:529); at htsjdk.tribble.index.IndexFactory.createTabixIndex(IndexFactory.java:476); at htsjdk.tribble.index.IndexFactory.createTabixIndex(IndexFactory.java:502); at htsjdk.tribble.index.IndexFactory.createIndex(IndexFactory.java:403); at org.broadinstitute.hellbender.tools.IndexFeatureFile.createAppropriateIndexInMemory(IndexFeatureFile.java:109); at org.broadinstitute.hellbender.tools.IndexFeatureFile.doWork(",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8747
https://github.com/broadinstitute/gatk/issues/8747:5521,Deployability,release,release,5521,"gatk).; - Search the existing github issues to see if your issue (or something similar) has already been reported. If the issue already exists, you may comment there to inquire about the progress.; - Determine whether your issue is a **bug report**, a **feature request**, or a **documentation request** (for tool/class javadoc only -- for forum docs please post there); - Consider if your ""issue"" is better addressed on the GATK forum: http://gatkforums.broadinstitute.org/gatk Post there if you have questions about expected tool behavior, output format, unexpected results, or generally any question that does not fit into the categories above; - Use a **concise** yet **descriptive** title; - Choose the corresponding template block below and fill it in, replacing or deleting text in italics (surrounded by `_`) as appropriate; - Delete the other template blocks and this header. ----. ## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_. ### Affected version(s); - [ ] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._. #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_. #### Expected behavior; _Tell us what should happen_. #### Actual behavior; _Tell us what happens instead_. ----. ## Feature request. ### Tool(s) or class(es) involved; _Tool/class name(s), special parameters?_. ### Description; _Specify whether you want a modification of an existing behavior or addition of a new capability._; _Provide **examples**, **screenshots**, where appropriate._. ----. ## Documentation request. ### Tool(s) or class(es) involved; _Tool/class name(s), parameters?_. ### Description ; _Describe what needs to be added or modified._. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8747
https://github.com/broadinstitute/gatk/issues/8747:429,Integrability,message,message,429,"Hello,. Could you help me with this? I ran this code:; ```; prg=/home/user1/Programs/gatk-4.5.0.0; log_dir=/home/user1/Programs/logs; java -Xmx64g -XX:ParallelGCThreads=2 -DGATK_STACKTRACE_ON_USER_EXCEPTION=true \; -jar ${prg}/gatk-package-4.5.0.0-local.jar IndexFeatureFile -I ${dir}/snp_allsamples.vcf.gz \; --output snp_allsamples.vcf.tbi \; 2>${log_dir}/snp_allsamples_gvcf_index.err. ```; and I received the following error message. ```; 09:36:35.254 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/user1/Programs/gatk-4.5.0.0/gatk-package-4.5.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 09:36:35.386 INFO IndexFeatureFile - ------------------------------------------------------------; 09:36:35.389 INFO IndexFeatureFile - The Genome Analysis Toolkit (GATK) v4.5.0.0; 09:36:35.389 INFO IndexFeatureFile - For support and documentation go to https://software.broadinstitute.org/gatk/; 09:36:35.389 INFO IndexFeatureFile - Executing as user1@xxx.xx on Linux v5.4.0-150-generic amd64; 09:36:35.389 INFO IndexFeatureFile - Java runtime: OpenJDK 64-Bit Server VM v17.0.3-internal+0-adhoc..src; 09:36:35.389 INFO IndexFeatureFile - Start Date/Time: March 21, 2024 at 9:36:35 a.m. CST; 09:36:35.390 INFO IndexFeatureFile - ------------------------------------------------------------; 09:36:35.390 INFO IndexFeatureFile - ------------------------------------------------------------; 09:36:35.390 INFO IndexFeatureFile - HTSJDK Version: 4.1.0; 09:36:35.391 INFO IndexFeatureFile - Picard Version: 3.1.1; 09:36:35.391 INFO IndexFeatureFile - Built for Spark Version: 3.5.0; 09:36:35.391 INFO IndexFeatureFile - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 09:36:35.391 INFO IndexFeatureFile - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 09:36:35.392 INFO IndexFeatureFile - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 09:36:35.392 INFO IndexFeatureFile - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 09:36:35.392 INFO IndexFeature",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8747
https://github.com/broadinstitute/gatk/issues/8747:483,Performance,Load,Loading,483,"Hello,. Could you help me with this? I ran this code:; ```; prg=/home/user1/Programs/gatk-4.5.0.0; log_dir=/home/user1/Programs/logs; java -Xmx64g -XX:ParallelGCThreads=2 -DGATK_STACKTRACE_ON_USER_EXCEPTION=true \; -jar ${prg}/gatk-package-4.5.0.0-local.jar IndexFeatureFile -I ${dir}/snp_allsamples.vcf.gz \; --output snp_allsamples.vcf.tbi \; 2>${log_dir}/snp_allsamples_gvcf_index.err. ```; and I received the following error message. ```; 09:36:35.254 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/user1/Programs/gatk-4.5.0.0/gatk-package-4.5.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 09:36:35.386 INFO IndexFeatureFile - ------------------------------------------------------------; 09:36:35.389 INFO IndexFeatureFile - The Genome Analysis Toolkit (GATK) v4.5.0.0; 09:36:35.389 INFO IndexFeatureFile - For support and documentation go to https://software.broadinstitute.org/gatk/; 09:36:35.389 INFO IndexFeatureFile - Executing as user1@xxx.xx on Linux v5.4.0-150-generic amd64; 09:36:35.389 INFO IndexFeatureFile - Java runtime: OpenJDK 64-Bit Server VM v17.0.3-internal+0-adhoc..src; 09:36:35.389 INFO IndexFeatureFile - Start Date/Time: March 21, 2024 at 9:36:35 a.m. CST; 09:36:35.390 INFO IndexFeatureFile - ------------------------------------------------------------; 09:36:35.390 INFO IndexFeatureFile - ------------------------------------------------------------; 09:36:35.390 INFO IndexFeatureFile - HTSJDK Version: 4.1.0; 09:36:35.391 INFO IndexFeatureFile - Picard Version: 3.1.1; 09:36:35.391 INFO IndexFeatureFile - Built for Spark Version: 3.5.0; 09:36:35.391 INFO IndexFeatureFile - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 09:36:35.391 INFO IndexFeatureFile - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 09:36:35.392 INFO IndexFeatureFile - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 09:36:35.392 INFO IndexFeatureFile - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 09:36:35.392 INFO IndexFeature",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8747
https://github.com/broadinstitute/gatk/issues/8747:128,Testability,log,logs,128,"Hello,. Could you help me with this? I ran this code:; ```; prg=/home/user1/Programs/gatk-4.5.0.0; log_dir=/home/user1/Programs/logs; java -Xmx64g -XX:ParallelGCThreads=2 -DGATK_STACKTRACE_ON_USER_EXCEPTION=true \; -jar ${prg}/gatk-package-4.5.0.0-local.jar IndexFeatureFile -I ${dir}/snp_allsamples.vcf.gz \; --output snp_allsamples.vcf.tbi \; 2>${log_dir}/snp_allsamples_gvcf_index.err. ```; and I received the following error message. ```; 09:36:35.254 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/user1/Programs/gatk-4.5.0.0/gatk-package-4.5.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 09:36:35.386 INFO IndexFeatureFile - ------------------------------------------------------------; 09:36:35.389 INFO IndexFeatureFile - The Genome Analysis Toolkit (GATK) v4.5.0.0; 09:36:35.389 INFO IndexFeatureFile - For support and documentation go to https://software.broadinstitute.org/gatk/; 09:36:35.389 INFO IndexFeatureFile - Executing as user1@xxx.xx on Linux v5.4.0-150-generic amd64; 09:36:35.389 INFO IndexFeatureFile - Java runtime: OpenJDK 64-Bit Server VM v17.0.3-internal+0-adhoc..src; 09:36:35.389 INFO IndexFeatureFile - Start Date/Time: March 21, 2024 at 9:36:35 a.m. CST; 09:36:35.390 INFO IndexFeatureFile - ------------------------------------------------------------; 09:36:35.390 INFO IndexFeatureFile - ------------------------------------------------------------; 09:36:35.390 INFO IndexFeatureFile - HTSJDK Version: 4.1.0; 09:36:35.391 INFO IndexFeatureFile - Picard Version: 3.1.1; 09:36:35.391 INFO IndexFeatureFile - Built for Spark Version: 3.5.0; 09:36:35.391 INFO IndexFeatureFile - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 09:36:35.391 INFO IndexFeatureFile - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 09:36:35.392 INFO IndexFeatureFile - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 09:36:35.392 INFO IndexFeatureFile - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 09:36:35.392 INFO IndexFeature",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8747
https://github.com/broadinstitute/gatk/issues/8747:5591,Testability,test,test,5591,"gatk).; - Search the existing github issues to see if your issue (or something similar) has already been reported. If the issue already exists, you may comment there to inquire about the progress.; - Determine whether your issue is a **bug report**, a **feature request**, or a **documentation request** (for tool/class javadoc only -- for forum docs please post there); - Consider if your ""issue"" is better addressed on the GATK forum: http://gatkforums.broadinstitute.org/gatk Post there if you have questions about expected tool behavior, output format, unexpected results, or generally any question that does not fit into the categories above; - Use a **concise** yet **descriptive** title; - Choose the corresponding template block below and fill it in, replacing or deleting text in italics (surrounded by `_`) as appropriate; - Delete the other template blocks and this header. ----. ## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_. ### Affected version(s); - [ ] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._. #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_. #### Expected behavior; _Tell us what should happen_. #### Actual behavior; _Tell us what happens instead_. ----. ## Feature request. ### Tool(s) or class(es) involved; _Tool/class name(s), special parameters?_. ### Description; _Specify whether you want a modification of an existing behavior or addition of a new capability._; _Provide **examples**, **screenshots**, where appropriate._. ----. ## Documentation request. ### Tool(s) or class(es) involved; _Tool/class name(s), parameters?_. ### Description ; _Describe what needs to be added or modified._. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8747
https://github.com/broadinstitute/gatk/issues/8747:5691,Testability,log,logs,5691,"gatk).; - Search the existing github issues to see if your issue (or something similar) has already been reported. If the issue already exists, you may comment there to inquire about the progress.; - Determine whether your issue is a **bug report**, a **feature request**, or a **documentation request** (for tool/class javadoc only -- for forum docs please post there); - Consider if your ""issue"" is better addressed on the GATK forum: http://gatkforums.broadinstitute.org/gatk Post there if you have questions about expected tool behavior, output format, unexpected results, or generally any question that does not fit into the categories above; - Use a **concise** yet **descriptive** title; - Choose the corresponding template block below and fill it in, replacing or deleting text in italics (surrounded by `_`) as appropriate; - Delete the other template blocks and this header. ----. ## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_. ### Affected version(s); - [ ] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._. #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_. #### Expected behavior; _Tell us what should happen_. #### Actual behavior; _Tell us what happens instead_. ----. ## Feature request. ### Tool(s) or class(es) involved; _Tool/class name(s), special parameters?_. ### Description; _Specify whether you want a modification of an existing behavior or addition of a new capability._; _Provide **examples**, **screenshots**, where appropriate._. ----. ## Documentation request. ### Tool(s) or class(es) involved; _Tool/class name(s), parameters?_. ### Description ; _Describe what needs to be added or modified._. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8747
https://github.com/broadinstitute/gatk/pull/8750:405,Deployability,update,updates,405,"In GATK3, when merging variants, the IDs of all the source VCFs were retained. This code path seems like it intended that, since the variantSources set is generated, but it doesnt get used for anything. This PR will use that set to set the source of the resulting merged VC. Note: i dont think I can kick off the test suite. It is possible this change would result in tests breaking, and those would need updates.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8750
https://github.com/broadinstitute/gatk/pull/8750:313,Testability,test,test,313,"In GATK3, when merging variants, the IDs of all the source VCFs were retained. This code path seems like it intended that, since the variantSources set is generated, but it doesnt get used for anything. This PR will use that set to set the source of the resulting merged VC. Note: i dont think I can kick off the test suite. It is possible this change would result in tests breaking, and those would need updates.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8750
https://github.com/broadinstitute/gatk/pull/8750:368,Testability,test,tests,368,"In GATK3, when merging variants, the IDs of all the source VCFs were retained. This code path seems like it intended that, since the variantSources set is generated, but it doesnt get used for anything. This PR will use that set to set the source of the resulting merged VC. Note: i dont think I can kick off the test suite. It is possible this change would result in tests breaking, and those would need updates.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8750
https://github.com/broadinstitute/gatk/issues/8751:111,Deployability,release,release,111,"## Bug Report. ### Affected tool(s) or class(es); SelectVariants. ### Affected version(s); - [x] Latest public release version [4.5.0.0]. ### Description ; When trying to stream a reference file from a public URL, there is trouble interpreting the path when the file ends with `.fa.gz`, especially in finding the index. . #### Steps to reproduce; This was discovered trying to debug another Picard issue. To reproduce, run this command:; ```; gatk SelectVariants -L chr17:22477226-22477227 -V https://jmorp.megabank.tohoku.ac.jp/datasets/tommo-54kjpn-20230626-af_snvindelall/files/tommo-54kjpn-20230626r3-GRCh38-af-autosome.vcf.gz -R https://jmorp.megabank.tohoku.ac.jp/datasets/tommo-jg2.1.0-20211208/files/jg2.1.0.fa.gz -O subset.vcf.gz; ```; Here we try to stream a small region from a VCF using a public reference file. . #### Expected behavior; The file `subset.vcf.gz` should be written with just the regions given. #### Actual behavior; You get a stacktrace:; ```; org.broadinstitute.http.nio.HttpPath$CantDealWithThisException: Attempting to resolve this against a path which is relatve but looks like it has a scheme.; This: https://jmorp.megabank.tohoku.ac.jp/datasets/tommo-jg2.1.0-20211208/files; Other: https:/jmorp.megabank.tohoku.ac.jp/jg2.1.0.fa.gz.fai; Other interpretted as URI: https:/jmorp.megabank.tohoku.ac.jp/jg2.1.0.fa.gz.fai; This is a limitatation of the current implementation of resolve.; Please use choose a less horrible file name or get in touch with the developers to complain.; 	at org.broadinstitute.http.nio.HttpPath.resolve(HttpPath.java:381); 	at org.broadinstitute.http.nio.HttpPath.resolve(HttpPath.java:53); 	at java.base/java.nio.file.Path.resolveSibling(Path.java:549); 	at org.broadinstitute.http.nio.HttpPath.resolveSibling(HttpPath.java:418); 	at htsjdk.samtools.reference.ReferenceSequenceFileFactory.getFastaIndexFileName(ReferenceSequenceFileFactory.java:262); 	at org.broadinstitute.hellbender.utils.fasta.CachingIndexedFastaSequenceFile.checkFastaPath",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8751
https://github.com/broadinstitute/gatk/pull/8752:22,Testability,test,test,22,@bbimber I've added a test to your pr https://github.com/broadinstitute/gatk/pull/8750. Should be good to merge if tests pass.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8752
https://github.com/broadinstitute/gatk/pull/8752:115,Testability,test,tests,115,@bbimber I've added a test to your pr https://github.com/broadinstitute/gatk/pull/8750. Should be good to merge if tests pass.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8752
https://github.com/broadinstitute/gatk/pull/8754:0,Energy Efficiency,Adapt,Adapt,0,"Adapt PGEN extract to work with Cromwell's ""Retry with more memory"" feature to address issues with a small percentage of ""problem"" shards OOMing. Successful run [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/dd29b0d9-73e5-4e1b-af83-e4fba53c0c65).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8754
https://github.com/broadinstitute/gatk/pull/8754:0,Modifiability,Adapt,Adapt,0,"Adapt PGEN extract to work with Cromwell's ""Retry with more memory"" feature to address issues with a small percentage of ""problem"" shards OOMing. Successful run [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/dd29b0d9-73e5-4e1b-af83-e4fba53c0c65).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8754
https://github.com/broadinstitute/gatk/pull/8755:62,Availability,Echo,EchoCallset,62,This migrates ONLY that part of the code from ah_var_store to EchoCallset branch (there are other changes in ah_var_store),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8755
https://github.com/broadinstitute/gatk/pull/8756:458,Performance,Load,Loading,458,"As scientists we must strive to standardize to the most rational units of measurement, including our measurement of the passage of time. That is why we, the GATK authors, would like to announce that we are updating the GATK display time to display the current system time stamped according to the French Revolutionary Calendar. Here is an example of the new logging outputs:. ```; Décadi, 10-Germinal-232, 7:74:79, L’outil:Couvoir INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/emeryj/hellbender/gatk/build/libs/gatk-package-4.5.0.0-20-g105b63e-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_compression.dylib; Décadi, 10-Germinal-232, 7:74:79, L’outil:Couvoir INFO CountReads - ------------------------------------------------------------; Décadi, 10-Germinal-232, 7:74:79, L’outil:Couvoir INFO CountReads - The Genome Analysis Toolkit (GATK) v4.5.0.0-20-g105b63e-SNAPSHOT; Décadi, 10-Germinal-232, 7:74:79, L’outil:Couvoir INFO CountReads - For support and documentation go to https://software.broadinstitute.org/gatk/; Décadi, 10-Germinal-232, 7:74:79, L’outil:Couvoir INFO CountReads - Executing as emeryj@wm85b-6ec on Mac OS X v13.2.1 x86_64; Décadi, 10-Germinal-232, 7:74:79, L’outil:Couvoir INFO CountReads - Java runtime: OpenJDK 64-Bit Server VM v17.0.6+10; Décadi, 10-Germinal-232, 7:74:79, L’outil:Couvoir INFO CountReads - Start Date/Time: March 29, 2024 at 2:35:42 PM EDT; Décadi, 10-Germinal-232, 7:74:79, L’outil:Couvoir INFO CountReads - ------------------------------------------------------------; Décadi, 10-Germinal-232, 7:74:79, L’outil:Couvoir INFO CountReads - ------------------------------------------------------------; Décadi, 10-Germinal-232, 7:74:79, L’outil:Couvoir INFO CountReads - HTSJDK Version: 4.1.0; Décadi, 10-Germinal-232, 7:74:79, L’outil:Couvoir INFO CountReads - Picard Version: 3.1.1; Décadi, 10-Germinal-232, 7:74:79, L’outil:Couvoir INFO CountReads - Built for Spark Version: 3.5.0; Décadi, 10-Germinal-232, 7:74:79, L’ou",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8756
https://github.com/broadinstitute/gatk/pull/8756:358,Testability,log,logging,358,"As scientists we must strive to standardize to the most rational units of measurement, including our measurement of the passage of time. That is why we, the GATK authors, would like to announce that we are updating the GATK display time to display the current system time stamped according to the French Revolutionary Calendar. Here is an example of the new logging outputs:. ```; Décadi, 10-Germinal-232, 7:74:79, L’outil:Couvoir INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/emeryj/hellbender/gatk/build/libs/gatk-package-4.5.0.0-20-g105b63e-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_compression.dylib; Décadi, 10-Germinal-232, 7:74:79, L’outil:Couvoir INFO CountReads - ------------------------------------------------------------; Décadi, 10-Germinal-232, 7:74:79, L’outil:Couvoir INFO CountReads - The Genome Analysis Toolkit (GATK) v4.5.0.0-20-g105b63e-SNAPSHOT; Décadi, 10-Germinal-232, 7:74:79, L’outil:Couvoir INFO CountReads - For support and documentation go to https://software.broadinstitute.org/gatk/; Décadi, 10-Germinal-232, 7:74:79, L’outil:Couvoir INFO CountReads - Executing as emeryj@wm85b-6ec on Mac OS X v13.2.1 x86_64; Décadi, 10-Germinal-232, 7:74:79, L’outil:Couvoir INFO CountReads - Java runtime: OpenJDK 64-Bit Server VM v17.0.6+10; Décadi, 10-Germinal-232, 7:74:79, L’outil:Couvoir INFO CountReads - Start Date/Time: March 29, 2024 at 2:35:42 PM EDT; Décadi, 10-Germinal-232, 7:74:79, L’outil:Couvoir INFO CountReads - ------------------------------------------------------------; Décadi, 10-Germinal-232, 7:74:79, L’outil:Couvoir INFO CountReads - ------------------------------------------------------------; Décadi, 10-Germinal-232, 7:74:79, L’outil:Couvoir INFO CountReads - HTSJDK Version: 4.1.0; Décadi, 10-Germinal-232, 7:74:79, L’outil:Couvoir INFO CountReads - Picard Version: 3.1.1; Décadi, 10-Germinal-232, 7:74:79, L’outil:Couvoir INFO CountReads - Built for Spark Version: 3.5.0; Décadi, 10-Germinal-232, 7:74:79, L’ou",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8756
https://github.com/broadinstitute/gatk/pull/8757:10,Deployability,integrat,integration,10,can i haz integration test,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8757
https://github.com/broadinstitute/gatk/pull/8757:10,Integrability,integrat,integration,10,can i haz integration test,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8757
https://github.com/broadinstitute/gatk/pull/8757:22,Testability,test,test,22,can i haz integration test,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8757
https://github.com/broadinstitute/gatk/pull/8758:137,Integrability,message,message,137,"Don't print the very long and misleading ""The following contigs are present in b37 and missing in the input VCF sequence dictionary"" log message when we're not even doing b37/hg19 conversion. Users are getting log messages like this even when they have an hg38 dictionary:. 16:21:32.764 INFO FuncotatorUtils - The following contigs are present in b37 and missing in the input VCF sequence dictionary:; 16:21:32.767 INFO FuncotatorUtils - 1 (len=249250621,assembly=GRCh37); 16:21:32.767 INFO FuncotatorUtils - 2 (len=243199373,assembly=GRCh37); 16:21:32.767 INFO FuncotatorUtils - 3 (len=198022430,assembly=GRCh37); 16:21:32.767 INFO FuncotatorUtils - 4 (len=191154276,assembly=GRCh37); 16:21:32.767 INFO FuncotatorUtils - 5 (len=180915260,assembly=GRCh37); 16:21:32.768 INFO FuncotatorUtils - 6 (len=171115067,assembly=GRCh37); 16:21:32.768 INFO FuncotatorUtils - 7 (len=159138663,assembly=GRCh37); 16:21:32.768 INFO FuncotatorUtils - 8 (len=146364022,assembly=GRCh37); 16:21:32.768 INFO FuncotatorUtils - 9 (len=141213431,assembly=GRCh37); 16:21:32.768 INFO FuncotatorUtils - 10 (len=135534747,assembly=GRCh37); 16:21:32.768 INFO FuncotatorUtils - 11 (len=135006516,assembly=GRCh37); 16:21:32.768 INFO FuncotatorUtils - 12 (len=133851895,assembly=GRCh37); 16:21:32.768 INFO FuncotatorUtils - 13 (len=115169878,assembly=GRCh37); 16:21:32.768 INFO FuncotatorUtils - 14 (len=107349540,assembly=GRCh37); 16:21:32.768 INFO FuncotatorUtils - 15 (len=102531392,assembly=GRCh37); 16:21:32.768 INFO FuncotatorUtils - 16 (len=90354753,assembly=GRCh37); 16:21:32.768 INFO FuncotatorUtils - 17 (len=81195210,assembly=GRCh37); 16:21:32.768 INFO FuncotatorUtils - 18 (len=78077248,assembly=GRCh37); 16:21:32.768 INFO FuncotatorUtils - 19 (len=59128983,assembly=GRCh37); 16:21:32.768 INFO FuncotatorUtils - 20 (len=63025520,assembly=GRCh37); 16:21:32.769 INFO FuncotatorUtils - 21 (len=48129895,assembly=GRCh37); 16:21:32.769 INFO FuncotatorUtils - 22 (len=51304566,assembly=GRCh37); .....",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8758
https://github.com/broadinstitute/gatk/pull/8758:214,Integrability,message,messages,214,"Don't print the very long and misleading ""The following contigs are present in b37 and missing in the input VCF sequence dictionary"" log message when we're not even doing b37/hg19 conversion. Users are getting log messages like this even when they have an hg38 dictionary:. 16:21:32.764 INFO FuncotatorUtils - The following contigs are present in b37 and missing in the input VCF sequence dictionary:; 16:21:32.767 INFO FuncotatorUtils - 1 (len=249250621,assembly=GRCh37); 16:21:32.767 INFO FuncotatorUtils - 2 (len=243199373,assembly=GRCh37); 16:21:32.767 INFO FuncotatorUtils - 3 (len=198022430,assembly=GRCh37); 16:21:32.767 INFO FuncotatorUtils - 4 (len=191154276,assembly=GRCh37); 16:21:32.767 INFO FuncotatorUtils - 5 (len=180915260,assembly=GRCh37); 16:21:32.768 INFO FuncotatorUtils - 6 (len=171115067,assembly=GRCh37); 16:21:32.768 INFO FuncotatorUtils - 7 (len=159138663,assembly=GRCh37); 16:21:32.768 INFO FuncotatorUtils - 8 (len=146364022,assembly=GRCh37); 16:21:32.768 INFO FuncotatorUtils - 9 (len=141213431,assembly=GRCh37); 16:21:32.768 INFO FuncotatorUtils - 10 (len=135534747,assembly=GRCh37); 16:21:32.768 INFO FuncotatorUtils - 11 (len=135006516,assembly=GRCh37); 16:21:32.768 INFO FuncotatorUtils - 12 (len=133851895,assembly=GRCh37); 16:21:32.768 INFO FuncotatorUtils - 13 (len=115169878,assembly=GRCh37); 16:21:32.768 INFO FuncotatorUtils - 14 (len=107349540,assembly=GRCh37); 16:21:32.768 INFO FuncotatorUtils - 15 (len=102531392,assembly=GRCh37); 16:21:32.768 INFO FuncotatorUtils - 16 (len=90354753,assembly=GRCh37); 16:21:32.768 INFO FuncotatorUtils - 17 (len=81195210,assembly=GRCh37); 16:21:32.768 INFO FuncotatorUtils - 18 (len=78077248,assembly=GRCh37); 16:21:32.768 INFO FuncotatorUtils - 19 (len=59128983,assembly=GRCh37); 16:21:32.768 INFO FuncotatorUtils - 20 (len=63025520,assembly=GRCh37); 16:21:32.769 INFO FuncotatorUtils - 21 (len=48129895,assembly=GRCh37); 16:21:32.769 INFO FuncotatorUtils - 22 (len=51304566,assembly=GRCh37); .....",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8758
https://github.com/broadinstitute/gatk/pull/8758:133,Testability,log,log,133,"Don't print the very long and misleading ""The following contigs are present in b37 and missing in the input VCF sequence dictionary"" log message when we're not even doing b37/hg19 conversion. Users are getting log messages like this even when they have an hg38 dictionary:. 16:21:32.764 INFO FuncotatorUtils - The following contigs are present in b37 and missing in the input VCF sequence dictionary:; 16:21:32.767 INFO FuncotatorUtils - 1 (len=249250621,assembly=GRCh37); 16:21:32.767 INFO FuncotatorUtils - 2 (len=243199373,assembly=GRCh37); 16:21:32.767 INFO FuncotatorUtils - 3 (len=198022430,assembly=GRCh37); 16:21:32.767 INFO FuncotatorUtils - 4 (len=191154276,assembly=GRCh37); 16:21:32.767 INFO FuncotatorUtils - 5 (len=180915260,assembly=GRCh37); 16:21:32.768 INFO FuncotatorUtils - 6 (len=171115067,assembly=GRCh37); 16:21:32.768 INFO FuncotatorUtils - 7 (len=159138663,assembly=GRCh37); 16:21:32.768 INFO FuncotatorUtils - 8 (len=146364022,assembly=GRCh37); 16:21:32.768 INFO FuncotatorUtils - 9 (len=141213431,assembly=GRCh37); 16:21:32.768 INFO FuncotatorUtils - 10 (len=135534747,assembly=GRCh37); 16:21:32.768 INFO FuncotatorUtils - 11 (len=135006516,assembly=GRCh37); 16:21:32.768 INFO FuncotatorUtils - 12 (len=133851895,assembly=GRCh37); 16:21:32.768 INFO FuncotatorUtils - 13 (len=115169878,assembly=GRCh37); 16:21:32.768 INFO FuncotatorUtils - 14 (len=107349540,assembly=GRCh37); 16:21:32.768 INFO FuncotatorUtils - 15 (len=102531392,assembly=GRCh37); 16:21:32.768 INFO FuncotatorUtils - 16 (len=90354753,assembly=GRCh37); 16:21:32.768 INFO FuncotatorUtils - 17 (len=81195210,assembly=GRCh37); 16:21:32.768 INFO FuncotatorUtils - 18 (len=78077248,assembly=GRCh37); 16:21:32.768 INFO FuncotatorUtils - 19 (len=59128983,assembly=GRCh37); 16:21:32.768 INFO FuncotatorUtils - 20 (len=63025520,assembly=GRCh37); 16:21:32.769 INFO FuncotatorUtils - 21 (len=48129895,assembly=GRCh37); 16:21:32.769 INFO FuncotatorUtils - 22 (len=51304566,assembly=GRCh37); .....",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8758
https://github.com/broadinstitute/gatk/pull/8758:210,Testability,log,log,210,"Don't print the very long and misleading ""The following contigs are present in b37 and missing in the input VCF sequence dictionary"" log message when we're not even doing b37/hg19 conversion. Users are getting log messages like this even when they have an hg38 dictionary:. 16:21:32.764 INFO FuncotatorUtils - The following contigs are present in b37 and missing in the input VCF sequence dictionary:; 16:21:32.767 INFO FuncotatorUtils - 1 (len=249250621,assembly=GRCh37); 16:21:32.767 INFO FuncotatorUtils - 2 (len=243199373,assembly=GRCh37); 16:21:32.767 INFO FuncotatorUtils - 3 (len=198022430,assembly=GRCh37); 16:21:32.767 INFO FuncotatorUtils - 4 (len=191154276,assembly=GRCh37); 16:21:32.767 INFO FuncotatorUtils - 5 (len=180915260,assembly=GRCh37); 16:21:32.768 INFO FuncotatorUtils - 6 (len=171115067,assembly=GRCh37); 16:21:32.768 INFO FuncotatorUtils - 7 (len=159138663,assembly=GRCh37); 16:21:32.768 INFO FuncotatorUtils - 8 (len=146364022,assembly=GRCh37); 16:21:32.768 INFO FuncotatorUtils - 9 (len=141213431,assembly=GRCh37); 16:21:32.768 INFO FuncotatorUtils - 10 (len=135534747,assembly=GRCh37); 16:21:32.768 INFO FuncotatorUtils - 11 (len=135006516,assembly=GRCh37); 16:21:32.768 INFO FuncotatorUtils - 12 (len=133851895,assembly=GRCh37); 16:21:32.768 INFO FuncotatorUtils - 13 (len=115169878,assembly=GRCh37); 16:21:32.768 INFO FuncotatorUtils - 14 (len=107349540,assembly=GRCh37); 16:21:32.768 INFO FuncotatorUtils - 15 (len=102531392,assembly=GRCh37); 16:21:32.768 INFO FuncotatorUtils - 16 (len=90354753,assembly=GRCh37); 16:21:32.768 INFO FuncotatorUtils - 17 (len=81195210,assembly=GRCh37); 16:21:32.768 INFO FuncotatorUtils - 18 (len=78077248,assembly=GRCh37); 16:21:32.768 INFO FuncotatorUtils - 19 (len=59128983,assembly=GRCh37); 16:21:32.768 INFO FuncotatorUtils - 20 (len=63025520,assembly=GRCh37); 16:21:32.769 INFO FuncotatorUtils - 21 (len=48129895,assembly=GRCh37); 16:21:32.769 INFO FuncotatorUtils - 22 (len=51304566,assembly=GRCh37); .....",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8758
https://github.com/broadinstitute/gatk/pull/8764:677,Deployability,integrat,integration,677,"Use Cromwell's [noAddress](https://support.terra.bio/hc/en-us/community/posts/360060020871-noAddress-true-results-in-stalling-jobs) feature to avoid the unnecessary use of external IPs on Cromwell worker VMs on wide scatters that cause us Google quota problems. For most of the GVS code this only involves adding `noAddress: true` to the existing runtime attributes. In the PGEN code this was slightly more work to change away from `docker: ""ubuntu:22.04""` which is implicitly pulled from Docker Hub. Since `noAddress: true` means the VM can only interact with Google services, we have to switch to a GCR-hosted image as Docker Hub has become unreachable. - [Mostly successful integration test](https://job-manager.dsde-prod.broadinstitute.org/jobs/32b9e2b5-3c56-4bf8-ab5e-66fa72c7cadb), delta some existing issues with cost discrepancies documented in VS-1324.; - [Successful PGEN extract](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/1f19bea2-a9b9-4ec5-b741-e9f64bbfa35a)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8764
https://github.com/broadinstitute/gatk/pull/8764:677,Integrability,integrat,integration,677,"Use Cromwell's [noAddress](https://support.terra.bio/hc/en-us/community/posts/360060020871-noAddress-true-results-in-stalling-jobs) feature to avoid the unnecessary use of external IPs on Cromwell worker VMs on wide scatters that cause us Google quota problems. For most of the GVS code this only involves adding `noAddress: true` to the existing runtime attributes. In the PGEN code this was slightly more work to change away from `docker: ""ubuntu:22.04""` which is implicitly pulled from Docker Hub. Since `noAddress: true` means the VM can only interact with Google services, we have to switch to a GCR-hosted image as Docker Hub has become unreachable. - [Mostly successful integration test](https://job-manager.dsde-prod.broadinstitute.org/jobs/32b9e2b5-3c56-4bf8-ab5e-66fa72c7cadb), delta some existing issues with cost discrepancies documented in VS-1324.; - [Successful PGEN extract](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/1f19bea2-a9b9-4ec5-b741-e9f64bbfa35a)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8764
https://github.com/broadinstitute/gatk/pull/8764:143,Safety,avoid,avoid,143,"Use Cromwell's [noAddress](https://support.terra.bio/hc/en-us/community/posts/360060020871-noAddress-true-results-in-stalling-jobs) feature to avoid the unnecessary use of external IPs on Cromwell worker VMs on wide scatters that cause us Google quota problems. For most of the GVS code this only involves adding `noAddress: true` to the existing runtime attributes. In the PGEN code this was slightly more work to change away from `docker: ""ubuntu:22.04""` which is implicitly pulled from Docker Hub. Since `noAddress: true` means the VM can only interact with Google services, we have to switch to a GCR-hosted image as Docker Hub has become unreachable. - [Mostly successful integration test](https://job-manager.dsde-prod.broadinstitute.org/jobs/32b9e2b5-3c56-4bf8-ab5e-66fa72c7cadb), delta some existing issues with cost discrepancies documented in VS-1324.; - [Successful PGEN extract](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/1f19bea2-a9b9-4ec5-b741-e9f64bbfa35a)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8764
https://github.com/broadinstitute/gatk/pull/8764:689,Testability,test,test,689,"Use Cromwell's [noAddress](https://support.terra.bio/hc/en-us/community/posts/360060020871-noAddress-true-results-in-stalling-jobs) feature to avoid the unnecessary use of external IPs on Cromwell worker VMs on wide scatters that cause us Google quota problems. For most of the GVS code this only involves adding `noAddress: true` to the existing runtime attributes. In the PGEN code this was slightly more work to change away from `docker: ""ubuntu:22.04""` which is implicitly pulled from Docker Hub. Since `noAddress: true` means the VM can only interact with Google services, we have to switch to a GCR-hosted image as Docker Hub has become unreachable. - [Mostly successful integration test](https://job-manager.dsde-prod.broadinstitute.org/jobs/32b9e2b5-3c56-4bf8-ab5e-66fa72c7cadb), delta some existing issues with cost discrepancies documented in VS-1324.; - [Successful PGEN extract](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/1f19bea2-a9b9-4ec5-b741-e9f64bbfa35a)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8764
https://github.com/broadinstitute/gatk/pull/8765:228,Deployability,pipeline,pipeline,228,"Reasoning:; - task will not recreate/overwrite table if it exists; - task does not take long, so unnecessary runs are not costly in time or $; - when not volatile, Beta users need to run with call-caching off if they re-run the pipeline. run where tables already existed: https://app.terra.bio/#workspaces/gvs-dev/RSA%20-%20GVS%20Quickstart%20V2%20/job_history/64782949-33dd-41ef-b3f7-5e88cc5a5dcc. integration run: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/9e79ef7f-9e64-46c7-8749-83909a5d423f (it failed the end tests, but the tables were created/populated as expected)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8765
https://github.com/broadinstitute/gatk/pull/8765:399,Deployability,integrat,integration,399,"Reasoning:; - task will not recreate/overwrite table if it exists; - task does not take long, so unnecessary runs are not costly in time or $; - when not volatile, Beta users need to run with call-caching off if they re-run the pipeline. run where tables already existed: https://app.terra.bio/#workspaces/gvs-dev/RSA%20-%20GVS%20Quickstart%20V2%20/job_history/64782949-33dd-41ef-b3f7-5e88cc5a5dcc. integration run: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/9e79ef7f-9e64-46c7-8749-83909a5d423f (it failed the end tests, but the tables were created/populated as expected)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8765
https://github.com/broadinstitute/gatk/pull/8765:399,Integrability,integrat,integration,399,"Reasoning:; - task will not recreate/overwrite table if it exists; - task does not take long, so unnecessary runs are not costly in time or $; - when not volatile, Beta users need to run with call-caching off if they re-run the pipeline. run where tables already existed: https://app.terra.bio/#workspaces/gvs-dev/RSA%20-%20GVS%20Quickstart%20V2%20/job_history/64782949-33dd-41ef-b3f7-5e88cc5a5dcc. integration run: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/9e79ef7f-9e64-46c7-8749-83909a5d423f (it failed the end tests, but the tables were created/populated as expected)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8765
https://github.com/broadinstitute/gatk/pull/8765:544,Testability,test,tests,544,"Reasoning:; - task will not recreate/overwrite table if it exists; - task does not take long, so unnecessary runs are not costly in time or $; - when not volatile, Beta users need to run with call-caching off if they re-run the pipeline. run where tables already existed: https://app.terra.bio/#workspaces/gvs-dev/RSA%20-%20GVS%20Quickstart%20V2%20/job_history/64782949-33dd-41ef-b3f7-5e88cc5a5dcc. integration run: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/9e79ef7f-9e64-46c7-8749-83909a5d423f (it failed the end tests, but the tables were created/populated as expected)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8765
https://github.com/broadinstitute/gatk/issues/8767:2429,Integrability,wrap,wrapAndCopyInto,2429," attr={AC=2, AF=0.500, AN=4, DP=155, ExcessHet=0.0000, FS=0.000, MLEAC=2, MLEAF=0.500, MQ=60.00, QD=27.97, SOR=1.204, VQSLOD=-3.754e-01, culprit=QD} GT=GT:AD:DP:GQ:PL 0/0:64,0:64:99:0,120,1800 1/1:0,89:89:99:2505,267,0 filters=; at org.broadinstitute.hellbender.engine.MultiVariantWalker.lambda$traverse$1(MultiVariantWalker.java:145); at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183); at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:197); at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:179); at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:197); at java.base/java.util.Iterator.forEachRemaining(Iterator.java:133); at java.base/java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1845); at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509); at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499); at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150); at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173); at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:596); at org.broadinstitute.hellbender.engine.MultiVariantWalker.traverse(MultiVariantWalker.java:136); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1098); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:149); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:198); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:217); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:166); at org.broadinstitute.hellbender.Main.mainEntr",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8767
https://github.com/broadinstitute/gatk/pull/8769:52,Testability,stub,stubs,52,This should reimport the messed up files as git lfs stubs.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8769
https://github.com/broadinstitute/gatk/pull/8770:11,Deployability,integrat,integration,11,successful integration run [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/802f9314-2b8b-4007-9c8d-6832a5687f22),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8770
https://github.com/broadinstitute/gatk/pull/8770:11,Integrability,integrat,integration,11,successful integration run [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/802f9314-2b8b-4007-9c8d-6832a5687f22),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8770
https://github.com/broadinstitute/gatk/pull/8773:221,Deployability,Integrat,Integration,221,Change extract so that when we filter at the genotype level (with FT) the VCF header has the filter definition in the comment/unstructured fields of the VCF Header.; Also minor renaming of ExtractCohort argument. Passing Integration Test (all chromosomes) [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/8e240d47-a75e-46b6-88f4-e95e7c1cf4e8); Passing Integration Test (chr20 and friends) [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/17ce6f1d-932d-4f4f-a2c8-f5f044bb1a67),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8773
https://github.com/broadinstitute/gatk/pull/8773:382,Deployability,Integrat,Integration,382,Change extract so that when we filter at the genotype level (with FT) the VCF header has the filter definition in the comment/unstructured fields of the VCF Header.; Also minor renaming of ExtractCohort argument. Passing Integration Test (all chromosomes) [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/8e240d47-a75e-46b6-88f4-e95e7c1cf4e8); Passing Integration Test (chr20 and friends) [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/17ce6f1d-932d-4f4f-a2c8-f5f044bb1a67),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8773
https://github.com/broadinstitute/gatk/pull/8773:221,Integrability,Integrat,Integration,221,Change extract so that when we filter at the genotype level (with FT) the VCF header has the filter definition in the comment/unstructured fields of the VCF Header.; Also minor renaming of ExtractCohort argument. Passing Integration Test (all chromosomes) [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/8e240d47-a75e-46b6-88f4-e95e7c1cf4e8); Passing Integration Test (chr20 and friends) [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/17ce6f1d-932d-4f4f-a2c8-f5f044bb1a67),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8773
https://github.com/broadinstitute/gatk/pull/8773:382,Integrability,Integrat,Integration,382,Change extract so that when we filter at the genotype level (with FT) the VCF header has the filter definition in the comment/unstructured fields of the VCF Header.; Also minor renaming of ExtractCohort argument. Passing Integration Test (all chromosomes) [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/8e240d47-a75e-46b6-88f4-e95e7c1cf4e8); Passing Integration Test (chr20 and friends) [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/17ce6f1d-932d-4f4f-a2c8-f5f044bb1a67),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8773
https://github.com/broadinstitute/gatk/pull/8773:233,Testability,Test,Test,233,Change extract so that when we filter at the genotype level (with FT) the VCF header has the filter definition in the comment/unstructured fields of the VCF Header.; Also minor renaming of ExtractCohort argument. Passing Integration Test (all chromosomes) [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/8e240d47-a75e-46b6-88f4-e95e7c1cf4e8); Passing Integration Test (chr20 and friends) [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/17ce6f1d-932d-4f4f-a2c8-f5f044bb1a67),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8773
https://github.com/broadinstitute/gatk/pull/8773:394,Testability,Test,Test,394,Change extract so that when we filter at the genotype level (with FT) the VCF header has the filter definition in the comment/unstructured fields of the VCF Header.; Also minor renaming of ExtractCohort argument. Passing Integration Test (all chromosomes) [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/8e240d47-a75e-46b6-88f4-e95e7c1cf4e8); Passing Integration Test (chr20 and friends) [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/17ce6f1d-932d-4f4f-a2c8-f5f044bb1a67),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8773
https://github.com/broadinstitute/gatk/issues/8776:598,Availability,error,error,598,"## Bug Report. ### Affected tool(s) or class(es); GATK PostprocessGermlineCNVCalls. ### Affected version(s); v4.4.0.0. ### Description ; Run GTAK on a batch of WES samples with `PostprocessGermlineCNVCalls` encountered: ""Records were not strictly sorted in dictionary order.""; I tried to detect germline CNV in cohort mode on 25 WES samples by the official tutorial. At first, I didn't perform scatter and the step `PostprocessGermlineCNVCalls` was very time-consuming but eventually worked. So I split the reference genome into 45 parts to save time. It's OK for the first sample but there was an error ""Records were not strictly sorted in dictionary order."" from the second sample. I was really annoyed by it. `03:12:39.275 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data/xiangxd/project/software/callers/gatk_4.4/gatk-package-4.4.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 03:12:39.467 INFO PostprocessGermlineCNVCalls - ------------------------------------------------------------; 03:12:39.473 INFO PostprocessGermlineCNVCalls - The Genome Analysis Toolkit (GATK) v4.4.0.0; 03:12:39.474 INFO PostprocessGermlineCNVCalls - For support and documentation go to https://software.broadinstitute.org/gatk/; 03:12:39.475 INFO PostprocessGermlineCNVCalls - Executing as xiangxd@cu07 on Linux v3.10.0-327.el7.x86_64 amd64; 03:12:39.475 INFO PostprocessGermlineCNVCalls - Java runtime: Java HotSpot(TM) 64-Bit Server VM v20.0.2+9-78; 03:12:39.477 INFO PostprocessGermlineCNVCalls - Start Date/Time: April 15, 2024, 3:12:39 AM CST; 03:12:39.477 INFO PostprocessGermlineCNVCalls - ------------------------------------------------------------; 03:12:39.478 INFO PostprocessGermlineCNVCalls - ------------------------------------------------------------; 03:12:39.495 INFO PostprocessGermlineCNVCalls - HTSJDK Version: 3.0.5; 03:12:39.496 INFO PostprocessGermlineCNVCalls - Picard Version: 3.0.0; 03:12:39.497 INFO PostprocessGermlineCNVCalls - Built for Spark Vers",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:7615,Availability,down,down,7615,"3:13:14.515 INFO PostprocessGermlineCNVCalls - Analyzing shard 43 / 45...; 03:13:14.652 INFO PostprocessGermlineCNVCalls - Analyzing shard 44 / 45...; 03:13:14.843 INFO PostprocessGermlineCNVCalls - Analyzing shard 45 / 45...; 03:13:15.144 INFO PostprocessGermlineCNVCalls - Generating segments...; 03:14:44.578 INFO PostprocessGermlineCNVCalls - Parsing Python output...; 03:14:44.593 INFO PostprocessGermlineCNVCalls - Writing segments VCF file to /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/output/R18002110LU01-XG3351_combined_segment_cohort.vcf...; 03:14:46.272 INFO PostprocessGermlineCNVCalls - Generating denoised copy ratios...; 03:14:47.231 INFO PostprocessGermlineCNVCalls - Writing denoised copy ratios to /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/output/R18002110LU01-XG3351_combined_ratio.txt...; 03:14:58.266 INFO PostprocessGermlineCNVCalls - PostprocessGermlineCNVCalls complete.; 03:14:58.268 INFO PostprocessGermlineCNVCalls - Shutting down engine; [April 15, 2024, 3:14:58 AM CST] org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls done. Elapsed time: 2.32 minutes.; Runtime.totalMemory()=1207959552; Using GATK jar /data/xiangxd/project/software/callers/gatk_4.4/gatk-package-4.4.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /data/xiangxd/project/software/callers/gatk_4.4/gatk-package-4.4.0.0-local.jar PostprocessGermlineCNVCalls --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_1-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_2-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_3-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_4-",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:20956,Availability,down,down,20956,"Picard Version: 3.0.0; 03:15:18.983 INFO PostprocessGermlineCNVCalls - Built for Spark Version: 3.3.1; 03:15:18.984 INFO PostprocessGermlineCNVCalls - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 03:15:18.985 INFO PostprocessGermlineCNVCalls - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 03:15:18.985 INFO PostprocessGermlineCNVCalls - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 03:15:18.986 INFO PostprocessGermlineCNVCalls - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 03:15:18.987 INFO PostprocessGermlineCNVCalls - Deflater: IntelDeflater; 03:15:18.988 INFO PostprocessGermlineCNVCalls - Inflater: IntelInflater; 03:15:18.988 INFO PostprocessGermlineCNVCalls - GCS max retries/reopens: 20; 03:15:18.989 INFO PostprocessGermlineCNVCalls - Requester pays: disabled; 03:15:18.990 INFO PostprocessGermlineCNVCalls - Initializing engine; 03:15:43.480 INFO PostprocessGermlineCNVCalls - Done initializing engine; 03:15:47.833 INFO PostprocessGermlineCNVCalls - Shutting down engine; [April 15, 2024, 3:15:47 AM CST] org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls done. Elapsed time: 0.49 minutes.; Runtime.totalMemory()=1207959552; java.lang.IllegalArgumentException: Records were not strictly sorted in dictionary order.; 	at org.broadinstitute.hellbender.tools.copynumber.arguments.CopyNumberArgumentValidationUtils.validateIntervals(CopyNumberArgumentValidationUtils.java:74); 	at org.broadinstitute.hellbender.tools.copynumber.formats.collections.AbstractLocatableCollection.getShardedCollectionSortOrder(AbstractLocatableCollection.java:142); 	at org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls.onTraversalStart(PostprocessGermlineCNVCalls.java:388); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1096); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:149); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseA",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:386,Performance,perform,perform,386,"## Bug Report. ### Affected tool(s) or class(es); GATK PostprocessGermlineCNVCalls. ### Affected version(s); v4.4.0.0. ### Description ; Run GTAK on a batch of WES samples with `PostprocessGermlineCNVCalls` encountered: ""Records were not strictly sorted in dictionary order.""; I tried to detect germline CNV in cohort mode on 25 WES samples by the official tutorial. At first, I didn't perform scatter and the step `PostprocessGermlineCNVCalls` was very time-consuming but eventually worked. So I split the reference genome into 45 parts to save time. It's OK for the first sample but there was an error ""Records were not strictly sorted in dictionary order."" from the second sample. I was really annoyed by it. `03:12:39.275 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data/xiangxd/project/software/callers/gatk_4.4/gatk-package-4.4.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 03:12:39.467 INFO PostprocessGermlineCNVCalls - ------------------------------------------------------------; 03:12:39.473 INFO PostprocessGermlineCNVCalls - The Genome Analysis Toolkit (GATK) v4.4.0.0; 03:12:39.474 INFO PostprocessGermlineCNVCalls - For support and documentation go to https://software.broadinstitute.org/gatk/; 03:12:39.475 INFO PostprocessGermlineCNVCalls - Executing as xiangxd@cu07 on Linux v3.10.0-327.el7.x86_64 amd64; 03:12:39.475 INFO PostprocessGermlineCNVCalls - Java runtime: Java HotSpot(TM) 64-Bit Server VM v20.0.2+9-78; 03:12:39.477 INFO PostprocessGermlineCNVCalls - Start Date/Time: April 15, 2024, 3:12:39 AM CST; 03:12:39.477 INFO PostprocessGermlineCNVCalls - ------------------------------------------------------------; 03:12:39.478 INFO PostprocessGermlineCNVCalls - ------------------------------------------------------------; 03:12:39.495 INFO PostprocessGermlineCNVCalls - HTSJDK Version: 3.0.5; 03:12:39.496 INFO PostprocessGermlineCNVCalls - Picard Version: 3.0.0; 03:12:39.497 INFO PostprocessGermlineCNVCalls - Built for Spark Vers",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:753,Performance,Load,Loading,753,"## Bug Report. ### Affected tool(s) or class(es); GATK PostprocessGermlineCNVCalls. ### Affected version(s); v4.4.0.0. ### Description ; Run GTAK on a batch of WES samples with `PostprocessGermlineCNVCalls` encountered: ""Records were not strictly sorted in dictionary order.""; I tried to detect germline CNV in cohort mode on 25 WES samples by the official tutorial. At first, I didn't perform scatter and the step `PostprocessGermlineCNVCalls` was very time-consuming but eventually worked. So I split the reference genome into 45 parts to save time. It's OK for the first sample but there was an error ""Records were not strictly sorted in dictionary order."" from the second sample. I was really annoyed by it. `03:12:39.275 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data/xiangxd/project/software/callers/gatk_4.4/gatk-package-4.4.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 03:12:39.467 INFO PostprocessGermlineCNVCalls - ------------------------------------------------------------; 03:12:39.473 INFO PostprocessGermlineCNVCalls - The Genome Analysis Toolkit (GATK) v4.4.0.0; 03:12:39.474 INFO PostprocessGermlineCNVCalls - For support and documentation go to https://software.broadinstitute.org/gatk/; 03:12:39.475 INFO PostprocessGermlineCNVCalls - Executing as xiangxd@cu07 on Linux v3.10.0-327.el7.x86_64 amd64; 03:12:39.475 INFO PostprocessGermlineCNVCalls - Java runtime: Java HotSpot(TM) 64-Bit Server VM v20.0.2+9-78; 03:12:39.477 INFO PostprocessGermlineCNVCalls - Start Date/Time: April 15, 2024, 3:12:39 AM CST; 03:12:39.477 INFO PostprocessGermlineCNVCalls - ------------------------------------------------------------; 03:12:39.478 INFO PostprocessGermlineCNVCalls - ------------------------------------------------------------; 03:12:39.495 INFO PostprocessGermlineCNVCalls - HTSJDK Version: 3.0.5; 03:12:39.496 INFO PostprocessGermlineCNVCalls - Picard Version: 3.0.0; 03:12:39.497 INFO PostprocessGermlineCNVCalls - Built for Spark Vers",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:18800,Performance,Load,Loading,18800,"rt_all/cohort_42-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_43-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_44-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_45-calls --contig-ploidy-calls /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/ploidy/ploidy-calls --allosomal-contig chrX --allosomal-contig chrY --sample-index 0 --output-genotyped-intervals /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/output/R18002110LU01-XG3351_combined_cohort.vcf --output-genotyped-segments /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/output/R18002110LU01-XG3351_combined_segment_cohort.vcf --output-denoised-copy-ratios /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/output/R18002110LU01-XG3351_combined_ratio.txt; 03:15:18.730 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data/xiangxd/project/software/callers/gatk_4.4/gatk-package-4.4.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 03:15:18.952 INFO PostprocessGermlineCNVCalls - ------------------------------------------------------------; 03:15:18.959 INFO PostprocessGermlineCNVCalls - The Genome Analysis Toolkit (GATK) v4.4.0.0; 03:15:18.959 INFO PostprocessGermlineCNVCalls - For support and documentation go to https://software.broadinstitute.org/gatk/; 03:15:18.960 INFO PostprocessGermlineCNVCalls - Executing as xiangxd@cu07 on Linux v3.10.0-327.el7.x86_64 amd64; 03:15:18.961 INFO PostprocessGermlineCNVCalls - Java runtime: Java HotSpot(TM) 64-Bit Server VM v20.0.2+9-78; 03:15:18.962 INFO PostprocessGermlineCNVCalls - Start Date/Time: April 15, 2024, 3:15:18 AM CST; 03:15:18.963 INFO PostprocessGermlineCNVCalls - ------------------------------------------------------------; 03:15:18.964 INFO PostprocessGermlineCNVCalls - ----------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:288,Safety,detect,detect,288,"## Bug Report. ### Affected tool(s) or class(es); GATK PostprocessGermlineCNVCalls. ### Affected version(s); v4.4.0.0. ### Description ; Run GTAK on a batch of WES samples with `PostprocessGermlineCNVCalls` encountered: ""Records were not strictly sorted in dictionary order.""; I tried to detect germline CNV in cohort mode on 25 WES samples by the official tutorial. At first, I didn't perform scatter and the step `PostprocessGermlineCNVCalls` was very time-consuming but eventually worked. So I split the reference genome into 45 parts to save time. It's OK for the first sample but there was an error ""Records were not strictly sorted in dictionary order."" from the second sample. I was really annoyed by it. `03:12:39.275 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data/xiangxd/project/software/callers/gatk_4.4/gatk-package-4.4.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 03:12:39.467 INFO PostprocessGermlineCNVCalls - ------------------------------------------------------------; 03:12:39.473 INFO PostprocessGermlineCNVCalls - The Genome Analysis Toolkit (GATK) v4.4.0.0; 03:12:39.474 INFO PostprocessGermlineCNVCalls - For support and documentation go to https://software.broadinstitute.org/gatk/; 03:12:39.475 INFO PostprocessGermlineCNVCalls - Executing as xiangxd@cu07 on Linux v3.10.0-327.el7.x86_64 amd64; 03:12:39.475 INFO PostprocessGermlineCNVCalls - Java runtime: Java HotSpot(TM) 64-Bit Server VM v20.0.2+9-78; 03:12:39.477 INFO PostprocessGermlineCNVCalls - Start Date/Time: April 15, 2024, 3:12:39 AM CST; 03:12:39.477 INFO PostprocessGermlineCNVCalls - ------------------------------------------------------------; 03:12:39.478 INFO PostprocessGermlineCNVCalls - ------------------------------------------------------------; 03:12:39.495 INFO PostprocessGermlineCNVCalls - HTSJDK Version: 3.0.5; 03:12:39.496 INFO PostprocessGermlineCNVCalls - Picard Version: 3.0.0; 03:12:39.497 INFO PostprocessGermlineCNVCalls - Built for Spark Vers",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:21332,Security,validat,validateIntervals,21332,"15:18.986 INFO PostprocessGermlineCNVCalls - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 03:15:18.987 INFO PostprocessGermlineCNVCalls - Deflater: IntelDeflater; 03:15:18.988 INFO PostprocessGermlineCNVCalls - Inflater: IntelInflater; 03:15:18.988 INFO PostprocessGermlineCNVCalls - GCS max retries/reopens: 20; 03:15:18.989 INFO PostprocessGermlineCNVCalls - Requester pays: disabled; 03:15:18.990 INFO PostprocessGermlineCNVCalls - Initializing engine; 03:15:43.480 INFO PostprocessGermlineCNVCalls - Done initializing engine; 03:15:47.833 INFO PostprocessGermlineCNVCalls - Shutting down engine; [April 15, 2024, 3:15:47 AM CST] org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls done. Elapsed time: 0.49 minutes.; Runtime.totalMemory()=1207959552; java.lang.IllegalArgumentException: Records were not strictly sorted in dictionary order.; 	at org.broadinstitute.hellbender.tools.copynumber.arguments.CopyNumberArgumentValidationUtils.validateIntervals(CopyNumberArgumentValidationUtils.java:74); 	at org.broadinstitute.hellbender.tools.copynumber.formats.collections.AbstractLocatableCollection.getShardedCollectionSortOrder(AbstractLocatableCollection.java:142); 	at org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls.onTraversalStart(PostprocessGermlineCNVCalls.java:388); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1096); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:149); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:217); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); Using GATK jar /data/xiangxd/project/software/callers/gatk_4.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:3337,Testability,test,test,3337,ssGermlineCNVCalls - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 03:12:39.502 INFO PostprocessGermlineCNVCalls - Deflater: IntelDeflater; 03:12:39.502 INFO PostprocessGermlineCNVCalls - Inflater: IntelInflater; 03:12:39.503 INFO PostprocessGermlineCNVCalls - GCS max retries/reopens: 20; 03:12:39.504 INFO PostprocessGermlineCNVCalls - Requester pays: disabled; 03:12:39.506 INFO PostprocessGermlineCNVCalls - Initializing engine; 03:13:03.198 INFO PostprocessGermlineCNVCalls - Done initializing engine; 03:13:06.533 INFO ProgressMeter - Starting traversal; 03:13:06.535 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 03:13:06.543 INFO ProgressMeter - unmapped 0.0 0 0.0; 03:13:06.544 INFO ProgressMeter - Traversal complete. Processed 0 total records in 0.0 minutes.; 03:13:06.545 INFO PostprocessGermlineCNVCalls - Generating intervals VCF file...; 03:13:08.009 INFO PostprocessGermlineCNVCalls - Writing intervals VCF file to /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/output/R18002110LU01-XG3351_combined_cohort.vcf...; 03:13:08.010 INFO PostprocessGermlineCNVCalls - Analyzing shard 1 / 45...; 03:13:08.540 INFO PostprocessGermlineCNVCalls - Analyzing shard 2 / 45...; 03:13:08.743 INFO PostprocessGermlineCNVCalls - Analyzing shard 3 / 45...; 03:13:08.898 INFO PostprocessGermlineCNVCalls - Analyzing shard 4 / 45...; 03:13:09.052 INFO PostprocessGermlineCNVCalls - Analyzing shard 5 / 45...; 03:13:09.237 INFO PostprocessGermlineCNVCalls - Analyzing shard 6 / 45...; 03:13:09.433 INFO PostprocessGermlineCNVCalls - Analyzing shard 7 / 45...; 03:13:09.557 INFO PostprocessGermlineCNVCalls - Analyzing shard 8 / 45...; 03:13:09.752 INFO PostprocessGermlineCNVCalls - Analyzing shard 9 / 45...; 03:13:09.885 INFO PostprocessGermlineCNVCalls - Analyzing shard 10 / 45...; 03:13:10.017 INFO PostprocessGermlineCNVCalls - Analyzing shard 11 / 45...; 03:13:10.170 INFO PostprocessGermlineCNVCalls - Analyzing shard 12 / 45...; ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:7088,Testability,test,test,7088,"3.601 INFO PostprocessGermlineCNVCalls - Analyzing shard 36 / 45...; 03:13:13.736 INFO PostprocessGermlineCNVCalls - Analyzing shard 37 / 45...; 03:13:13.884 INFO PostprocessGermlineCNVCalls - Analyzing shard 38 / 45...; 03:13:14.027 INFO PostprocessGermlineCNVCalls - Analyzing shard 39 / 45...; 03:13:14.156 INFO PostprocessGermlineCNVCalls - Analyzing shard 40 / 45...; 03:13:14.284 INFO PostprocessGermlineCNVCalls - Analyzing shard 41 / 45...; 03:13:14.385 INFO PostprocessGermlineCNVCalls - Analyzing shard 42 / 45...; 03:13:14.515 INFO PostprocessGermlineCNVCalls - Analyzing shard 43 / 45...; 03:13:14.652 INFO PostprocessGermlineCNVCalls - Analyzing shard 44 / 45...; 03:13:14.843 INFO PostprocessGermlineCNVCalls - Analyzing shard 45 / 45...; 03:13:15.144 INFO PostprocessGermlineCNVCalls - Generating segments...; 03:14:44.578 INFO PostprocessGermlineCNVCalls - Parsing Python output...; 03:14:44.593 INFO PostprocessGermlineCNVCalls - Writing segments VCF file to /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/output/R18002110LU01-XG3351_combined_segment_cohort.vcf...; 03:14:46.272 INFO PostprocessGermlineCNVCalls - Generating denoised copy ratios...; 03:14:47.231 INFO PostprocessGermlineCNVCalls - Writing denoised copy ratios to /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/output/R18002110LU01-XG3351_combined_ratio.txt...; 03:14:58.266 INFO PostprocessGermlineCNVCalls - PostprocessGermlineCNVCalls complete.; 03:14:58.268 INFO PostprocessGermlineCNVCalls - Shutting down engine; [April 15, 2024, 3:14:58 AM CST] org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls done. Elapsed time: 2.32 minutes.; Runtime.totalMemory()=1207959552; Using GATK jar /data/xiangxd/project/software/callers/gatk_4.4/gatk-package-4.4.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /data/xiangxd/pr",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:7377,Testability,test,test,7377,"d 39 / 45...; 03:13:14.156 INFO PostprocessGermlineCNVCalls - Analyzing shard 40 / 45...; 03:13:14.284 INFO PostprocessGermlineCNVCalls - Analyzing shard 41 / 45...; 03:13:14.385 INFO PostprocessGermlineCNVCalls - Analyzing shard 42 / 45...; 03:13:14.515 INFO PostprocessGermlineCNVCalls - Analyzing shard 43 / 45...; 03:13:14.652 INFO PostprocessGermlineCNVCalls - Analyzing shard 44 / 45...; 03:13:14.843 INFO PostprocessGermlineCNVCalls - Analyzing shard 45 / 45...; 03:13:15.144 INFO PostprocessGermlineCNVCalls - Generating segments...; 03:14:44.578 INFO PostprocessGermlineCNVCalls - Parsing Python output...; 03:14:44.593 INFO PostprocessGermlineCNVCalls - Writing segments VCF file to /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/output/R18002110LU01-XG3351_combined_segment_cohort.vcf...; 03:14:46.272 INFO PostprocessGermlineCNVCalls - Generating denoised copy ratios...; 03:14:47.231 INFO PostprocessGermlineCNVCalls - Writing denoised copy ratios to /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/output/R18002110LU01-XG3351_combined_ratio.txt...; 03:14:58.266 INFO PostprocessGermlineCNVCalls - PostprocessGermlineCNVCalls complete.; 03:14:58.268 INFO PostprocessGermlineCNVCalls - Shutting down engine; [April 15, 2024, 3:14:58 AM CST] org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls done. Elapsed time: 2.32 minutes.; Runtime.totalMemory()=1207959552; Using GATK jar /data/xiangxd/project/software/callers/gatk_4.4/gatk-package-4.4.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /data/xiangxd/project/software/callers/gatk_4.4/gatk-package-4.4.0.0-local.jar PostprocessGermlineCNVCalls --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_1-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/info",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:8223,Testability,test,test,8223,,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:8333,Testability,test,test,8333,,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:8443,Testability,test,test,8443,,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:8553,Testability,test,test,8553,,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:8663,Testability,test,test,8663,,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:8773,Testability,test,test,8773,,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:8883,Testability,test,test,8883,,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:8993,Testability,test,test,8993,,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:9103,Testability,test,test,9103,,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:9213,Testability,test,test,9213,project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_1-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_2-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_3-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_4-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_5-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_6-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_7-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_8-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_9-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_10-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_11-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_12-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_13-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_14-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_15-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_16-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_17-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_18-model --model-shard-path /data/xiangxd/project/test,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:9324,Testability,test,test,9324,roject/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_2-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_3-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_4-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_5-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_6-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_7-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_8-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_9-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_10-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_11-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_12-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_13-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_14-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_15-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_16-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_17-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_18-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_19-model --model-shard-path /data/xiangxd/project/test,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:9435,Testability,test,test,9435,oject/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_3-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_4-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_5-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_6-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_7-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_8-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_9-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_10-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_11-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_12-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_13-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_14-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_15-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_16-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_17-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_18-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_19-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_20-model --model-shard-path /data/xiangxd/project/test,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:9546,Testability,test,test,9546,ject/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_4-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_5-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_6-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_7-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_8-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_9-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_10-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_11-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_12-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_13-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_14-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_15-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_16-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_17-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_18-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_19-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_20-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_21-model --model-shard-path /data/xiangxd/project/test,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:9657,Testability,test,test,9657,ect/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_5-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_6-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_7-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_8-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_9-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_10-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_11-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_12-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_13-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_14-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_15-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_16-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_17-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_18-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_19-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_20-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_21-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_22-model --model-shard-path /data/xiangxd/project/test,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:9768,Testability,test,test,9768,ct/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_6-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_7-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_8-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_9-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_10-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_11-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_12-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_13-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_14-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_15-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_16-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_17-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_18-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_19-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_20-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_21-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_22-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_23-model --model-shard-path /data/xiangxd/project/test,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:9879,Testability,test,test,9879,t/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_7-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_8-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_9-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_10-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_11-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_12-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_13-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_14-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_15-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_16-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_17-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_18-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_19-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_20-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_21-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_22-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_23-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_24-model --model-shard-path /data/xiangxd/project/test,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:9990,Testability,test,test,9990,/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_8-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_9-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_10-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_11-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_12-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_13-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_14-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_15-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_16-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_17-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_18-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_19-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_20-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_21-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_22-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_23-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_24-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_25-model --model-shard-path /data/xiangxd/project/test,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:10101,Testability,test,test,10101,test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_9-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_10-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_11-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_12-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_13-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_14-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_15-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_16-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_17-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_18-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_19-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_20-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_21-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_22-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_23-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_24-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_25-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_26-model --model-shard-path /data/xiangxd/project/test,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:10212,Testability,test,test,10212,est/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_10-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_11-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_12-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_13-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_14-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_15-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_16-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_17-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_18-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_19-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_20-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_21-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_22-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_23-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_24-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_25-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_26-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_27-model --model-shard-path /data/xiangxd/project/test,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:10323,Testability,test,test,10323,est/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_11-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_12-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_13-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_14-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_15-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_16-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_17-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_18-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_19-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_20-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_21-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_22-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_23-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_24-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_25-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_26-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_27-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_28-model --model-shard-path /data/xiangxd/project/test,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:10434,Testability,test,test,10434,est/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_12-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_13-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_14-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_15-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_16-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_17-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_18-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_19-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_20-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_21-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_22-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_23-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_24-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_25-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_26-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_27-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_28-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_29-model --model-shard-path /data/xiangxd/project/test,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:10545,Testability,test,test,10545,est/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_13-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_14-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_15-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_16-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_17-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_18-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_19-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_20-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_21-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_22-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_23-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_24-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_25-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_26-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_27-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_28-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_29-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_30-model --model-shard-path /data/xiangxd/project/test,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:10656,Testability,test,test,10656,est/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_14-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_15-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_16-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_17-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_18-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_19-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_20-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_21-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_22-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_23-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_24-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_25-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_26-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_27-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_28-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_29-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_30-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_31-model --model-shard-path /data/xiangxd/project/test,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:10767,Testability,test,test,10767,est/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_15-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_16-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_17-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_18-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_19-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_20-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_21-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_22-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_23-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_24-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_25-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_26-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_27-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_28-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_29-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_30-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_31-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_32-model --model-shard-path /data/xiangxd/project/test,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:10878,Testability,test,test,10878,est/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_16-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_17-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_18-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_19-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_20-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_21-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_22-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_23-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_24-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_25-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_26-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_27-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_28-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_29-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_30-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_31-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_32-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_33-model --model-shard-path /data/xiangxd/project/test,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:10989,Testability,test,test,10989,est/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_17-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_18-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_19-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_20-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_21-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_22-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_23-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_24-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_25-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_26-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_27-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_28-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_29-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_30-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_31-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_32-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_33-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_34-model --model-shard-path /data/xiangxd/project/test,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:11100,Testability,test,test,11100,est/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_18-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_19-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_20-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_21-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_22-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_23-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_24-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_25-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_26-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_27-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_28-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_29-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_30-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_31-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_32-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_33-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_34-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_35-model --model-shard-path /data/xiangxd/project/test,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:11211,Testability,test,test,11211,est/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_19-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_20-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_21-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_22-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_23-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_24-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_25-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_26-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_27-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_28-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_29-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_30-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_31-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_32-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_33-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_34-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_35-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_36-model --model-shard-path /data/xiangxd/project/test,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:11322,Testability,test,test,11322,est/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_20-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_21-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_22-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_23-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_24-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_25-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_26-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_27-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_28-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_29-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_30-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_31-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_32-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_33-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_34-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_35-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_36-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_37-model --model-shard-path /data/xiangxd/project/test,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:11433,Testability,test,test,11433,est/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_21-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_22-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_23-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_24-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_25-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_26-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_27-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_28-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_29-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_30-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_31-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_32-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_33-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_34-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_35-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_36-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_37-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_38-model --model-shard-path /data/xiangxd/project/test,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:11544,Testability,test,test,11544,est/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_22-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_23-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_24-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_25-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_26-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_27-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_28-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_29-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_30-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_31-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_32-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_33-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_34-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_35-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_36-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_37-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_38-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_39-model --model-shard-path /data/xiangxd/project/test,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:11655,Testability,test,test,11655,est/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_23-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_24-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_25-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_26-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_27-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_28-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_29-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_30-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_31-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_32-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_33-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_34-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_35-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_36-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_37-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_38-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_39-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_40-model --model-shard-path /data/xiangxd/project/test,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:11766,Testability,test,test,11766,est/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_24-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_25-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_26-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_27-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_28-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_29-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_30-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_31-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_32-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_33-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_34-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_35-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_36-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_37-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_38-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_39-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_40-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_41-model --model-shard-path /data/xiangxd/project/test,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:11877,Testability,test,test,11877,est/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_25-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_26-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_27-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_28-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_29-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_30-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_31-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_32-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_33-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_34-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_35-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_36-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_37-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_38-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_39-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_40-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_41-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_42-model --model-shard-path /data/xiangxd/project/test,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:11988,Testability,test,test,11988,est/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_26-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_27-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_28-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_29-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_30-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_31-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_32-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_33-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_34-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_35-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_36-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_37-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_38-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_39-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_40-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_41-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_42-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_43-model --model-shard-path /data/xiangxd/project/test,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:12099,Testability,test,test,12099,est/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_27-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_28-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_29-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_30-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_31-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_32-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_33-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_34-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_35-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_36-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_37-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_38-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_39-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_40-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_41-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_42-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_43-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_44-model --model-shard-path /data/xiangxd/project/test,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:12210,Testability,test,test,12210,est/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_28-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_29-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_30-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_31-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_32-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_33-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_34-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_35-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_36-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_37-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_38-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_39-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_40-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_41-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_42-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_43-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_44-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_45-model --calls-shard-path /data/xiangxd/project/test,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:12321,Testability,test,test,12321,est/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_29-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_30-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_31-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_32-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_33-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_34-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_35-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_36-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_37-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_38-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_39-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_40-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_41-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_42-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_43-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_44-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_45-model --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_1-calls --calls-shard-path /data/xiangxd/project/test/,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:12432,Testability,test,test,12432,est/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_30-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_31-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_32-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_33-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_34-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_35-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_36-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_37-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_38-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_39-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_40-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_41-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_42-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_43-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_44-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_45-model --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_1-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_2-calls --calls-shard-path /data/xiangxd/project/test/P,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:12543,Testability,test,test,12543,est/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_31-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_32-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_33-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_34-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_35-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_36-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_37-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_38-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_39-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_40-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_41-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_42-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_43-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_44-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_45-model --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_1-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_2-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_3-calls --calls-shard-path /data/xiangxd/project/test/PD,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:12654,Testability,test,test,12654,est/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_32-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_33-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_34-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_35-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_36-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_37-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_38-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_39-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_40-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_41-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_42-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_43-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_44-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_45-model --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_1-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_2-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_3-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_4-calls --calls-shard-path /data/xiangxd/project/test/PD_,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:12765,Testability,test,test,12765,est/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_33-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_34-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_35-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_36-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_37-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_38-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_39-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_40-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_41-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_42-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_43-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_44-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_45-model --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_1-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_2-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_3-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_4-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_5-calls --calls-shard-path /data/xiangxd/project/test/PD_W,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:12876,Testability,test,test,12876,est/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_34-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_35-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_36-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_37-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_38-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_39-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_40-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_41-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_42-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_43-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_44-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_45-model --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_1-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_2-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_3-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_4-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_5-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_6-calls --calls-shard-path /data/xiangxd/project/test/PD_WE,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:12987,Testability,test,test,12987,est/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_35-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_36-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_37-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_38-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_39-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_40-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_41-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_42-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_43-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_44-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_45-model --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_1-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_2-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_3-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_4-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_5-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_6-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_7-calls --calls-shard-path /data/xiangxd/project/test/PD_WES,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:13098,Testability,test,test,13098,est/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_36-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_37-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_38-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_39-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_40-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_41-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_42-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_43-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_44-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_45-model --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_1-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_2-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_3-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_4-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_5-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_6-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_7-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_8-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:13209,Testability,test,test,13209,est/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_37-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_38-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_39-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_40-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_41-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_42-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_43-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_44-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_45-model --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_1-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_2-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_3-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_4-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_5-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_6-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_7-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_8-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_9-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_5,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:13319,Testability,test,test,13319,test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_38-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_39-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_40-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_41-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_42-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_43-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_44-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_45-model --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_1-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_2-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_3-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_4-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_5-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_6-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_7-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_8-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_9-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_10-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:13429,Testability,test,test,13429,/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_39-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_40-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_41-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_42-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_43-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_44-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_45-model --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_1-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_2-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_3-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_4-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_5-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_6-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_7-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_8-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_9-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_10-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_11-calls --calls-shard-path /data/xiangxd/project/test/PD_WES,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:13539,Testability,test,test,13539,t/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_40-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_41-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_42-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_43-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_44-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_45-model --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_1-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_2-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_3-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_4-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_5-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_6-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_7-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_8-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_9-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_10-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_11-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_12-calls --calls-shard-path /data/xiangxd/project/test/PD_WE,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:13649,Testability,test,test,13649,ct/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_41-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_42-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_43-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_44-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_45-model --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_1-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_2-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_3-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_4-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_5-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_6-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_7-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_8-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_9-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_10-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_11-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_12-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_13-calls --calls-shard-path /data/xiangxd/project/test/PD_W,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:13759,Testability,test,test,13759,ect/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_42-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_43-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_44-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_45-model --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_1-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_2-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_3-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_4-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_5-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_6-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_7-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_8-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_9-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_10-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_11-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_12-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_13-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_14-calls --calls-shard-path /data/xiangxd/project/test/PD_,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:13869,Testability,test,test,13869,ject/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_43-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_44-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_45-model --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_1-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_2-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_3-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_4-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_5-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_6-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_7-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_8-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_9-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_10-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_11-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_12-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_13-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_14-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_15-calls --calls-shard-path /data/xiangxd/project/test/PD,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:13979,Testability,test,test,13979,oject/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_44-model --model-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_45-model --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_1-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_2-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_3-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_4-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_5-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_6-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_7-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_8-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_9-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_10-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_11-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_12-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_13-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_14-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_15-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_16-calls --calls-shard-path /data/xiangxd/project/test/P,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:14089,Testability,test,test,14089,roject/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_45-model --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_1-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_2-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_3-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_4-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_5-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_6-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_7-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_8-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_9-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_10-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_11-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_12-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_13-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_14-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_15-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_16-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_17-calls --calls-shard-path /data/xiangxd/project/test/,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:14199,Testability,test,test,14199,project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_1-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_2-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_3-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_4-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_5-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_6-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_7-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_8-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_9-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_10-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_11-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_12-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_13-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_14-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_15-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_16-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_17-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_18-calls --calls-shard-path /data/xiangxd/project/test,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:14310,Testability,test,test,14310,roject/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_2-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_3-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_4-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_5-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_6-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_7-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_8-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_9-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_10-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_11-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_12-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_13-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_14-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_15-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_16-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_17-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_18-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_19-calls --calls-shard-path /data/xiangxd/project/test,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:14421,Testability,test,test,14421,oject/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_3-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_4-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_5-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_6-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_7-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_8-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_9-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_10-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_11-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_12-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_13-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_14-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_15-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_16-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_17-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_18-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_19-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_20-calls --calls-shard-path /data/xiangxd/project/test,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:14532,Testability,test,test,14532,ject/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_4-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_5-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_6-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_7-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_8-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_9-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_10-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_11-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_12-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_13-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_14-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_15-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_16-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_17-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_18-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_19-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_20-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_21-calls --calls-shard-path /data/xiangxd/project/test,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:14643,Testability,test,test,14643,ect/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_5-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_6-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_7-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_8-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_9-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_10-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_11-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_12-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_13-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_14-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_15-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_16-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_17-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_18-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_19-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_20-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_21-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_22-calls --calls-shard-path /data/xiangxd/project/test,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:14754,Testability,test,test,14754,ct/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_6-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_7-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_8-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_9-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_10-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_11-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_12-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_13-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_14-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_15-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_16-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_17-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_18-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_19-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_20-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_21-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_22-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_23-calls --calls-shard-path /data/xiangxd/project/test,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:14865,Testability,test,test,14865,t/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_7-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_8-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_9-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_10-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_11-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_12-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_13-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_14-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_15-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_16-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_17-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_18-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_19-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_20-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_21-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_22-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_23-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_24-calls --calls-shard-path /data/xiangxd/project/test,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:14976,Testability,test,test,14976,/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_8-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_9-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_10-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_11-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_12-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_13-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_14-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_15-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_16-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_17-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_18-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_19-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_20-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_21-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_22-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_23-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_24-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_25-calls --calls-shard-path /data/xiangxd/project/test,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:15087,Testability,test,test,15087,test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_9-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_10-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_11-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_12-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_13-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_14-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_15-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_16-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_17-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_18-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_19-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_20-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_21-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_22-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_23-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_24-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_25-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_26-calls --calls-shard-path /data/xiangxd/project/test,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:15198,Testability,test,test,15198,est/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_10-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_11-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_12-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_13-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_14-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_15-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_16-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_17-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_18-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_19-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_20-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_21-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_22-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_23-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_24-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_25-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_26-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_27-calls --calls-shard-path /data/xiangxd/project/test,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:15309,Testability,test,test,15309,est/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_11-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_12-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_13-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_14-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_15-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_16-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_17-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_18-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_19-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_20-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_21-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_22-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_23-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_24-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_25-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_26-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_27-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_28-calls --calls-shard-path /data/xiangxd/project/test,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:15420,Testability,test,test,15420,est/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_12-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_13-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_14-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_15-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_16-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_17-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_18-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_19-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_20-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_21-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_22-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_23-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_24-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_25-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_26-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_27-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_28-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_29-calls --calls-shard-path /data/xiangxd/project/test,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:15531,Testability,test,test,15531,est/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_13-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_14-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_15-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_16-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_17-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_18-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_19-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_20-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_21-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_22-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_23-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_24-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_25-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_26-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_27-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_28-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_29-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_30-calls --calls-shard-path /data/xiangxd/project/test,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:15642,Testability,test,test,15642,est/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_14-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_15-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_16-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_17-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_18-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_19-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_20-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_21-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_22-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_23-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_24-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_25-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_26-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_27-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_28-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_29-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_30-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_31-calls --calls-shard-path /data/xiangxd/project/test,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:15753,Testability,test,test,15753,est/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_15-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_16-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_17-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_18-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_19-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_20-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_21-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_22-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_23-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_24-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_25-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_26-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_27-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_28-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_29-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_30-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_31-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_32-calls --calls-shard-path /data/xiangxd/project/test,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:15864,Testability,test,test,15864,est/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_16-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_17-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_18-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_19-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_20-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_21-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_22-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_23-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_24-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_25-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_26-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_27-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_28-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_29-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_30-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_31-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_32-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_33-calls --calls-shard-path /data/xiangxd/project/test,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:15975,Testability,test,test,15975,est/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_17-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_18-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_19-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_20-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_21-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_22-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_23-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_24-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_25-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_26-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_27-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_28-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_29-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_30-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_31-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_32-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_33-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_34-calls --calls-shard-path /data/xiangxd/project/test,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:16086,Testability,test,test,16086,est/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_18-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_19-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_20-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_21-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_22-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_23-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_24-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_25-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_26-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_27-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_28-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_29-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_30-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_31-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_32-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_33-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_34-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_35-calls --calls-shard-path /data/xiangxd/project/test,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:16197,Testability,test,test,16197,est/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_19-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_20-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_21-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_22-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_23-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_24-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_25-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_26-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_27-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_28-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_29-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_30-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_31-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_32-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_33-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_34-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_35-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_36-calls --calls-shard-path /data/xiangxd/project/test,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:16308,Testability,test,test,16308,est/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_20-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_21-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_22-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_23-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_24-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_25-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_26-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_27-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_28-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_29-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_30-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_31-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_32-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_33-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_34-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_35-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_36-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_37-calls --calls-shard-path /data/xiangxd/project/test,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:16419,Testability,test,test,16419,est/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_21-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_22-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_23-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_24-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_25-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_26-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_27-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_28-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_29-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_30-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_31-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_32-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_33-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_34-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_35-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_36-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_37-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_38-calls --calls-shard-path /data/xiangxd/project/test,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:16530,Testability,test,test,16530,est/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_22-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_23-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_24-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_25-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_26-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_27-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_28-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_29-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_30-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_31-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_32-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_33-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_34-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_35-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_36-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_37-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_38-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_39-calls --calls-shard-path /data/xiangxd/project/test,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:16641,Testability,test,test,16641,est/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_23-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_24-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_25-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_26-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_27-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_28-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_29-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_30-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_31-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_32-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_33-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_34-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_35-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_36-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_37-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_38-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_39-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_40-calls --calls-shard-path /data/xiangxd/project/test,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:16752,Testability,test,test,16752,est/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_24-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_25-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_26-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_27-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_28-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_29-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_30-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_31-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_32-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_33-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_34-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_35-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_36-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_37-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_38-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_39-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_40-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_41-calls --calls-shard-path /data/xiangxd/project/test,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:16863,Testability,test,test,16863,est/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_25-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_26-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_27-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_28-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_29-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_30-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_31-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_32-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_33-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_34-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_35-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_36-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_37-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_38-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_39-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_40-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_41-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_42-calls --calls-shard-path /data/xiangxd/project/test,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:16974,Testability,test,test,16974,est/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_26-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_27-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_28-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_29-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_30-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_31-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_32-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_33-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_34-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_35-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_36-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_37-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_38-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_39-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_40-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_41-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_42-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_43-calls --calls-shard-path /data/xiangxd/project/test,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:17085,Testability,test,test,17085,est/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_27-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_28-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_29-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_30-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_31-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_32-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_33-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_34-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_35-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_36-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_37-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_38-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_39-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_40-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_41-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_42-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_43-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_44-calls --calls-shard-path /data/xiangxd/project/test,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:17196,Testability,test,test,17196,est/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_28-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_29-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_30-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_31-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_32-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_33-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_34-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_35-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_36-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_37-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_38-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_39-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_40-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_41-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_42-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_43-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_44-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_45-calls --contig-ploidy-calls /data/xiangxd/project/t,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:17307,Testability,test,test,17307,est/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_29-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_30-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_31-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_32-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_33-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_34-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_35-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_36-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_37-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_38-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_39-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_40-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_41-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_42-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_43-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_44-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_45-calls --contig-ploidy-calls /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/ploidy/ploidy-calls --allosomal-contig chrX --allosomal-contig chrY -,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:17418,Testability,test,test,17418,est/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_30-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_31-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_32-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_33-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_34-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_35-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_36-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_37-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_38-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_39-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_40-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_41-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_42-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_43-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_44-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_45-calls --contig-ploidy-calls /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/ploidy/ploidy-calls --allosomal-contig chrX --allosomal-contig chrY --sample-index 0 --output-genotyped-intervals /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/o,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:17529,Testability,test,test,17529,ohort_all/cohort_30-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_31-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_32-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_33-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_34-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_35-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_36-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_37-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_38-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_39-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_40-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_41-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_42-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_43-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_44-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_45-calls --contig-ploidy-calls /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/ploidy/ploidy-calls --allosomal-contig chrX --allosomal-contig chrY --sample-index 0 --output-genotyped-intervals /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/output/R18002110LU01-XG3351_combined_cohort.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:17640,Testability,test,test,17640,ohort_all/cohort_30-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_31-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_32-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_33-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_34-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_35-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_36-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_37-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_38-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_39-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_40-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_41-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_42-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_43-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_44-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_45-calls --contig-ploidy-calls /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/ploidy/ploidy-calls --allosomal-contig chrX --allosomal-contig chrY --sample-index 0 --output-genotyped-intervals /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/output/R18002110LU01-XG3351_combined_cohort.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:17751,Testability,test,test,17751,ohort_all/cohort_30-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_31-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_32-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_33-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_34-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_35-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_36-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_37-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_38-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_39-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_40-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_41-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_42-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_43-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_44-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_45-calls --contig-ploidy-calls /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/ploidy/ploidy-calls --allosomal-contig chrX --allosomal-contig chrY --sample-index 0 --output-genotyped-intervals /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/output/R18002110LU01-XG3351_combined_cohort.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:17862,Testability,test,test,17862,ohort_all/cohort_30-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_31-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_32-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_33-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_34-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_35-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_36-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_37-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_38-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_39-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_40-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_41-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_42-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_43-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_44-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_45-calls --contig-ploidy-calls /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/ploidy/ploidy-calls --allosomal-contig chrX --allosomal-contig chrY --sample-index 0 --output-genotyped-intervals /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/output/R18002110LU01-XG3351_combined_cohort.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:17973,Testability,test,test,17973,ohort_all/cohort_30-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_31-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_32-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_33-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_34-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_35-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_36-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_37-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_38-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_39-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_40-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_41-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_42-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_43-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_44-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_45-calls --contig-ploidy-calls /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/ploidy/ploidy-calls --allosomal-contig chrX --allosomal-contig chrY --sample-index 0 --output-genotyped-intervals /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/output/R18002110LU01-XG3351_combined_cohort.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:18084,Testability,test,test,18084,ohort_all/cohort_30-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_31-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_32-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_33-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_34-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_35-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_36-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_37-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_38-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_39-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_40-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_41-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_42-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_43-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_44-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_45-calls --contig-ploidy-calls /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/ploidy/ploidy-calls --allosomal-contig chrX --allosomal-contig chrY --sample-index 0 --output-genotyped-intervals /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/output/R18002110LU01-XG3351_combined_cohort.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:18198,Testability,test,test,18198,ohort_all/cohort_30-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_31-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_32-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_33-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_34-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_35-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_36-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_37-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_38-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_39-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_40-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_41-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_42-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_43-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_44-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_45-calls --contig-ploidy-calls /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/ploidy/ploidy-calls --allosomal-contig chrX --allosomal-contig chrY --sample-index 0 --output-genotyped-intervals /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/output/R18002110LU01-XG3351_combined_cohort.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:18377,Testability,test,test,18377,ohort_all/cohort_30-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_31-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_32-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_33-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_34-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_35-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_36-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_37-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_38-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_39-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_40-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_41-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_42-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_43-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_44-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_45-calls --contig-ploidy-calls /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/ploidy/ploidy-calls --allosomal-contig chrX --allosomal-contig chrY --sample-index 0 --output-genotyped-intervals /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/output/R18002110LU01-XG3351_combined_cohort.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:18518,Testability,test,test,18518,ES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_40-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_41-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_42-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_43-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_44-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_45-calls --contig-ploidy-calls /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/ploidy/ploidy-calls --allosomal-contig chrX --allosomal-contig chrY --sample-index 0 --output-genotyped-intervals /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/output/R18002110LU01-XG3351_combined_cohort.vcf --output-genotyped-segments /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/output/R18002110LU01-XG3351_combined_segment_cohort.vcf --output-denoised-copy-ratios /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/output/R18002110LU01-XG3351_combined_ratio.txt; 03:15:18.730 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data/xiangxd/project/software/callers/gatk_4.4/gatk-package-4.4.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 03:15:18.952 INFO PostprocessGermlineCNVCalls - ------------------------------------------------------------; 03:15:18.959 INFO PostprocessGermlineCNVCalls - The Genome Analysis Toolkit (GATK) v4.4.0.0; 03:15:18.959 INFO PostprocessGermlineCNVCalls - For support and documentation go to https://software.broadinstitute.org/gatk/; 03:15:18.960 INFO PostprocessGermlineCNVCalls - Executing as xiangxd@cu07 on Linux v3.10.0-327.el7.x86_64 amd64; 03:15:18.961 INFO PostprocessGermlineCNVCalls - Java runtime: Java HotSpot(TM) 64-Bit Server VM v20.0.2+9-78; 03:15:18.962 INFO ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:18669,Testability,test,test,18669,"ohort_all/cohort_41-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_42-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_43-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_44-calls --calls-shard-path /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/cohort_all/cohort_45-calls --contig-ploidy-calls /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/ploidy/ploidy-calls --allosomal-contig chrX --allosomal-contig chrY --sample-index 0 --output-genotyped-intervals /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/output/R18002110LU01-XG3351_combined_cohort.vcf --output-genotyped-segments /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/output/R18002110LU01-XG3351_combined_segment_cohort.vcf --output-denoised-copy-ratios /data/xiangxd/project/test/PD_WES_50/cnv_calling/GATK_gCNV/infos/output/R18002110LU01-XG3351_combined_ratio.txt; 03:15:18.730 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data/xiangxd/project/software/callers/gatk_4.4/gatk-package-4.4.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 03:15:18.952 INFO PostprocessGermlineCNVCalls - ------------------------------------------------------------; 03:15:18.959 INFO PostprocessGermlineCNVCalls - The Genome Analysis Toolkit (GATK) v4.4.0.0; 03:15:18.959 INFO PostprocessGermlineCNVCalls - For support and documentation go to https://software.broadinstitute.org/gatk/; 03:15:18.960 INFO PostprocessGermlineCNVCalls - Executing as xiangxd@cu07 on Linux v3.10.0-327.el7.x86_64 amd64; 03:15:18.961 INFO PostprocessGermlineCNVCalls - Java runtime: Java HotSpot(TM) 64-Bit Server VM v20.0.2+9-78; 03:15:18.962 INFO PostprocessGermlineCNVCalls - Start Date/Time: April 15, 2024, 3:15:18 AM CST; 03:15:18.963 INFO PostprocessGermlineCNVCalls - ------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
https://github.com/broadinstitute/gatk/issues/8776:22715,Testability,test,test,22715,,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776
